{
  "paper_id": "2403.04743v2",
  "title": "Speech Emotion Recognition Via Cnn-Transformer And Multidimensional Attention Mechanism",
  "published": "2024-03-07T18:49:29Z",
  "authors": [
    "Xiaoyu Tang",
    "Yixin Lin",
    "Ting Dang",
    "Yuanfang Zhang",
    "Jintao Cheng"
  ],
  "keywords": [
    "Speech emotion recognition",
    "temporal-channelspatial attention",
    "lightweight convolution transformer",
    "local global feature fusion"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech Emotion Recognition (SER) is crucial in human-machine interactions. Mainstream approaches utilize Convolutional Neural Networks or Recurrent Neural Networks to learn local energy feature representations of speech segments from speech information, but struggle with capturing global information such as the duration of energy in speech. Some use Transformers to capture global information, but there is room for improvement in terms of parameter count and performance. Furthermore, existing attention mechanisms focus on spatial or channel dimensions, hindering learning of important temporal information in speech. In this paper, to model local and global information at different levels of granularity in speech and capture temporal, spatial and channel dependencies in speech signals, we propose a Speech Emotion Recognition network based on CNN-Transformer and multi-dimensional attention mechanisms. Specifically, a stack of CNN blocks is dedicated to capturing local information in speech from a time-frequency perspective. In addition, a time-channel-space attention mechanism is used to enhance features across three dimensions. Moreover, we model local and global dependencies of feature sequences using large convolutional kernels with depthwise separable convolutions and lightweight Transformer modules. We evaluate the proposed method on IEMOCAP and Emo-DB datasets and show our approach significantly improves the performance over the stateof-the-art methods 1 .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "E MOTION recognition has significant importance in var- ious fields, especially in increasingly common humancomputer interaction systems  [1] , and speech emotion recognition (SER) has promising applications in areas such as mental health monitoring  [2] , educational assistance, personalized content recommendation, and customer service quality monitoring. Speech contains rich emotional information, and as Feature extraction of speech is a rather important and challenging task in speech emotion recognition, and the extraction of features directly affects the effectiveness of subsequent model training and the accuracy of the final algorithm for emotion recognition. Speech features can be categorized as acoustic-based features and deep learning-based features where acoustic-based features can be broadly classified into rhythmic features  [3] , spectral-based correlation features  [4] , and phonetic features  [5] . Among them, spectral-based correlation features reflect the characteristics of the signal in the frequency domain, where there are differences in the performance of different emotions in the frequency domain. Based on the spectral correlation features include linear spectrum  [6]  and inverse spectrum  [7] , Linear Prediction Cofficients (LPC), Log Frequency Power Coefficients (LFPC), etc.; Inverse spectrum includes Mel-Frequency Cepstrum Coefficients (MFCC), Linear Prediction Cepstrum Cofficients (LPCC), etc. Among them, MFCC is regarded as a low-level feature based on human knowledge, which is widely used in the field of speech.\n\nEarly SER algorithms mainly used acoustic-based features and combined with traditional machine learning algorithms, including hidden Markov models  [8] , Gaussian mixture models  [9] , and support vector machines  [10] . In recent years, deep learning-based neural networks have gradually become active in the field of speech emotion recognition  [11] ,  [12] , and compared with traditional models, deep learning-based models have shown better performance in speech emotion recognition. Deep learning-based features use neural networks to learn more advanced features from the original signal of speech or some low-dimensional acoustic features. Convolutional neural networks (CNNs) are effective in capturing local acoustic details in speech, while long short-term memory networks (LSTMs) are widely used in speech emotion recognition for modeling dynamic information and temporal dependencies in speech. Additionally, attention mechanisms are also a key factor in improving model performance, as they can adaptively focus on the importance of different features to obtain better speech features at the discourse level. For example, Qi et al.  [13]  proposed a hierarchical network based on static and dynamic features, which uses LSTM to encode dynamic and static features of speech and designs a gating model to fuse the features, an attention mechanism is used to acquire the discourse-level speech features. Liu et al.  [14]  proposed a local-global perceptual depth representation learning system. One module contains a multiscale CNN and a time-frequency CNN (TFCNN) to learn the local representation, and in the other module, a Capsule network with an improved routing algorithm is utilized to design a multi-block dense connection structure, which can learn both shallow and deep global information.\n\nAlthough speech emotion recognition (SER) models composed of CNNs exhibit better performance than traditional models, these networks can only extract local information in speech, such as the energy and rhythm of a particular segment of speech, while struggling to learn global information in speech features, such as the overall volume and speaking rate of the speaker, and the duration of energy, thus neglecting the global correlation of features  [15] . In recent years, the transformer  [16]  based on the self-attentive mechanism has been widely used in major deep learning tasks. Tarantino et al.  [17]  used transformer in combination with global windowing for speech emotion recognition and achieved better performance, but transformer is weak for local feature extraction. Some recent work has attempted to combine CNN and transformer to alleviate the limitations of using CNN and transformer alone. Wang et al.  [18]  stacked the transformer blocks after the CNN model to improve the global features of aggregation. A model combining the transformer and CNN is proposed in  [19] , enabling it to learn local information while capturing global dependencies. The original transformer tends to have a high number of parameters for computing multi-headed selfattention, which requires a lot of resources and poses some difficulties in the training of the network. In addition, this kind of work tends to stack transformers in the last part of the model or a simple combination of transformer and CNN, which makes it hard to obtain better local information of the speech.\n\nIn addition, attention mechanisms improve the effectiveness of task processing by selectively attending to features that are most relevant to the current task, and have received widespread attention in major fields. In recent years, several researchers have utilized deep learning methods for feature extraction and used attention mechanisms to improve performance. A lightweight self-attention module is proposed in  [20] , which uses MLP to extract channel information and a large perceptual field extended CNN to extract spatial contextual information. Guo et al.  [21]  proposed an attention mechanism based on time, frequency, and CNN channels to improve representation learning ability. However, temporal information is often embedded in speech, which reflects the dynamic changes of speech, such as pitch and energy variations over time. Temporal features can reflect the temporal context and evolution of emotion expression in speech. However, it falls short in capturing the temporal information present in speech, which represents the dynamics of speech and plays a crucial role in emotion recognition. This limitation is a common issue in both MLPs and CNNs.\n\nIn this paper, we investigate how to effectively combine transformer and CNN and apply them to SER to characterize local features and global features in speech signals, and propose the temporal-channel-space attention mechanism in the model for multiple dimensions of feature enhancement. Specifically, we first use a set of stacked CNNs to capture local information in speech and learn shallow features of speech for the transformer module for better training of the transformer module. In the stacked CNN module, two sets of convolutional filters of different shapes are used to capture both temporal and frequency domain contextual information. Specifically, after stacking the CNN modules, we introduced a temporalchannel-space attention mechanism that models the contextual emotional expression of features over time, and efficiently fuses the attention of the spatial and channel dimensions of the speech feature map through the Shuffle unit. Furthermore, a combination of transformer and CNN is used to model the local and global dependencies of feature sequences by a deep separable convolution with residuals and a lightweight transformer module. The main contributions of this work are summarized as follows:\n\n• A framework based on CNN and transformer is proposed for speech emotion recognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "In this section, we will briefly review the algorithms related to speech emotion recognition, namely convolutional and recurrent neural networks, attention mechanism, and transformer.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Convolutional Neural Networks And Recurrent Neural Networks",
      "text": "Speech is a continuous time-series signal, and CNN and RNN have been two main network structures for SER. Motivated by the studies of CNN in computer vision, AlexNet  [22]  and ResNet  [23]  show promising results in image classification tasks and therefore have been studies in SER. Zhu et al.  [24]  has proposed a new Global Aware Multi-scale (GLAM) neural network that utilizes a global perception fusion module to learn multi-scale feature representations, with a focus on emotional information. The multi-time-scale (MTS) method was introduced in  [25] , which extends the CNNs by scaling and resampling the convolutional kernel along the time axis to increase temporal flexibility. Liu et al.  [14]  proposed a local global-aware deep representation learning system that uses CNNs and Capsule Networks to learn local and deep global information.\n\nRNN can model the temporal information in speech and capture long-term dependencies in the speech signal more effectively. A new layered network HNSD was proposed  [13]  that can efficiently integrate static and dynamic features of SER, which uses LSTM to encode static and dynamic features and gated multi-features unit (GMU) for frame level feature fusion for the emotional intermediate representation. Xu et al.  [26]  proposed a hierarchical grained and feature model (HGFM) that uses recurrent neural networks to process both discourse-level and frame-level information of the speech. Since convolutional neural networks can capture local information of features, while recurrent neural networks take advantage of modeling temporal information, many works have combined these two approaches and achieved outstanding results. Li  [27]  extracted location information from MFCC features and VGGish features by bi-direction long short time memory (BiLSTM) neural network, and then fused these two features to predict emotions. Liu et al.  [28]  combined triplet loss and CNN-LSTM models to obtain more discriminative sentiment information, and the proposed framework yielded excellent results in experiments. Zou  [29]  et al. used CNN, BiLSTM, and wav2vec2 to extract different levels of speech information, including MFCC, spectrogram, and acoustic information, and fused these three features by an attention mechanism. Zhang  [30]  et al. used CNNs to learn segmentlevel features in spectrograms, using a deep LSTM model to capture temporal dependencies between speech segments.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Attention Mechanism",
      "text": "In recent years, attention mechanisms have received a lot of attention in major fields to improve the effectiveness of task processing by focusing on information that is more critical to the current task among the many inputs. A channel attention mechanism called Squeeze-and-Excitation (SE) was proposed in  [31] , which assigns weights to individual channels and adaptively recalibrates the feature responses of the channels. Woo et al.  [32]  proposed a convolutional block attention module that combines both spatial and channel dimensions to obtain attention with better results. In addition, some researchers have used deep learning methods for feature extraction of speech and enhancement of feature maps using attention mechanisms. An attention pooling-based approach was proposed in  [33] , compared to existing average and maximum pooling, it can combine both class-agnostic bottom-up attention maps and class-specific top-down attention maps in an effective manner. Mustaqeem et al.  [20]  proposed a self-attentive module (SAM) for SER systems,which uses a multilayer perceptron (MLP) to recognize global information of the channels and identifies spatial information using a special dilated CNN to generate an attentional map for both channels. SAM significantly reduces the computational and parametric overhead. A spectrotemporal-channel (STC) attention mechanism was proposed in  [21] , which acquires attention feature maps along three dimensions: time, frequency, and channel. Xi et al.  [34]  employed an attention mechanism based on the time and frequency domain to introduce long-distance contextual information.\n\nThe current attention mechanisms typically focus more on spatial or channel information in feature maps, often neglecting the temporal characteristics in speech. However, temporal features in speech are equally important for emotion recognition. Therefore, it is necessary to pay more attention to temporal information in attention mechanisms to better explore and utilize the temporal characteristics in speech signals.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Transformer",
      "text": "Transformer has been rapidly developing in the field of natural language processing (NLP) in recent years and has achieved great success. Due to its powerful ability to obtain global information, the transformer has been gradually extended to the fields of speech. An end-to-end speech emotion recognition model  [18]  was proposed to enhance the global feature representation of the model by using stacked transformer blocks at the end of the model. Hu et al.  [19]  took advantage of multiple models, improved the learning ability of the model by residual BLSTM, and proposed a convolutional neural network and E-transformer module to learn both local and global information. Recently, transformer-based selfsupervised methods have also been applied to speech, and some transformer-based models have achieved great success in automatic speech recognition (ASR), including wav2vec  [35] , VQ-wav2vec  [36] , and wav2vec2.0  [37] . There is also some work in speech emotion recognition that employs these models for migration learning. A pre-trained wav2vec2.0 model  [38]  is used as the input to the network and the outputs of multiple network layers of the pre-trained model are combined to produce a richer representation of speech features. Cai et al.  [39]  proposed a multi-task learning (MTL) framework that uses the wav2vec2.0 model for feature extraction and simultaneously training for speech emotion classification and text recognition. Among computer vision tasks, ViT  [40]  first applied transformer directly to image patch sequences which is groundbreaking in applying transformer structure to computer vision. ViT has a superior structure and reduced computational resource consumption compared to convolutional neural networks. There are many similar approaches in the field of speech. ViT was introduced to speech and improved based on the properties of spectrograms in  [41] , which proposed a separable transformer (SepTr) that uses the transformer to process tokens at the same time and the same frequency interval, respectively. In  [42] , a method to improve ViT was applied to infant cry recognition by combining the original log-Mel spectrogram, first-order time-frame and frequencybin differential log-Mels 3D features into ViT for infant cry recognition.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iii. Method",
      "text": "In this section, we describe the proposed model in detail. Our proposed model is shown in Fig.  1 , which consists of three parts, namely CNN Block, T-Sa attention mechanism module, and LCT Block, these three modules will be introduced in detail next.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Overview Of The Model",
      "text": "To take full advantage of convolutional neural networks and transformer to model the speech sequences and use attention mechanism to enhance the features in time, space and channel, based on which our model is designed. As shown in Fig.  1 , for a given input speech sequence, a series of preprocessing steps are performed. Specifically, we uniformly process different lengths of speech sequences into 1.8s, the longer sequences will be cropped into subsegments, and for shorter sequences, we process them using loop filling, after which MFCC features are extracted of speech as the input to the model. The local features of the speech first are obtained by a CNN block, where the irregular-sized time-frequency domain convolution is used to obtain the features in the time and frequency domains of the speech. The features are then enhanceed using a T-Sa attention mechanism block, which contains a bilstm attention module to model the features in the temporal order, followed by a spatial-channel attention mechanism to focus on the spatial and channel information. Finally, the global and local information of speech is learned interactively by an LCT Block, which enables the model to learn information at different scales. The three parts of the model in this paper are described in detail in the following sections.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Cnn Block",
      "text": "For a large model such as transformer, if MFCC features are directly input into transformer, it will bring a large number of parameters. And since the dataset of speech emotion recognition is generally small, using transformer directly for feature learning will make the model difficult to converge. Therefore, we introduce a CNN Block to pre-learn the local features in speech. As shown in Figure  1 , the CNN Block consists of a series of convolutional and pooling layers. For the input feature MFCC, the two dimensions of MFCC correspond to two dimensions in the temporal and frequency domains, respectively, for which we first use a pair of irregular convolutions to obtain the perceptual field in a specific range. For a convolution of size 3 × 1, we set the perceptual field in the time domain to 1, thus minimizing the effect in the time domain to learn information in the frequency domain, and for a convolution of size 1 × 3 which is a similar process, which in turn allows capturing a multi-scale representation in",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. T-Sa Attention Module",
      "text": "In this paper, inspired by  [43] , we adopt a novel attention module to combine temporal attention with spatial-channel attention, focusing on the temporal dynamics of speech features and spatial-channel information in the feature map, as shown in Fig.  2 , which is divided into two parts before and after. In the former part, the attention model enhances the temporal information in the features by Bilstm to model the current features temporally, based on the fact that speech information is temporal information and the order of features in time is of importance. The latter part follows the spatial and channel attention, which is a common concern in existing attention, and efficiently combines the attention of the spatial and channel dimensions of speech feature maps through the Shuffle unit. T-Sa attention module enhances the features in the model through three dimensions and with a small number of parameters and achieves better results in SER.\n\n1) Timing attention: After Pre-processing and CNN Block for feature extraction of speech, giving the feature map size of X ∈ R C×H×W for the input T-Sa attention module, where C, H and W denote the number of channels, spatial height and width, respectively. To model the temporal attention in speech features, a recurrent neural network is used to model the speech information which is bilstm. The input of bilstm is a two-dimensional sequence while our speech feature map is three-dimensional, so what we need is to process the feature map X. If we directly reshape the feature map, the input bilstm will bring a great number of parameters number. Therefore, average pooling is used to reduce the dimensionality of the channels, and the specific calculation is:\n\nAfter the feature passes through average pooling, the size of the feature map is X CAvgP ool ∈ R H×W . Feature map is adjusted to X CAvgP ool ∈ R W ×H by reshaping operation and then feeding it to the bilstm layer. By encoding long distances from front to back and from back to front, bilstm can better capture bidirectional feature dependencies. X CAvgP ool is encoded by bilstm as follows:\n\n← -\n\nTwo LSTMs in bilstm process the sequence forward and backward, respectively, and then concate the outputs of the two LSTMs together:\n\nH bilstm is then applied sigmoid activation and multiplied with the input feature map using the residual scheme to output temporal attention:\n\n2) Space-channel attention: Spatial attention and channel attention are widely used in computer vision. Most methods transform and aggregate features in these two directions, such as SE  [31] , CBAM  [32] , BAM  [44] , GCNet  [45] , but these methods do not make full use of the correlation between space and channel which are not efficient. Therefore, we adopted SA-Net  [43]  as our spatial-channel attention module.\n\nGiven the output X time of the temporal attention module, the input is first divided into G groups, and the size of each sub-feature map is X time ′ ∈ R C/G×H×W . Then, each group is split into two sub-branches in the channel dimension., X spatial and X channel , one of which obtains spatial attention and the other obtains channel attention.\n\nFor the channel attention branch, firstly, the global average pooling (GAP) operation is performed on the input of the branch to embed the global information. Then, a simple gating mechanism and sigmoid activation are used to perform adaptive learning of spatial features. A residual scheme is used to multiply the input channel branch feature map. The specific operation of the spatial attention module is as follows:\n\nW 1 and b 1 are learnable parameters. For the spatial attention branch, GN  [46]  operation is first performed on the input of the branch to embed spatial statistical information, and then a simple gating mechanism and sigmoid activation are used to perform adaptive learning of features on the channel, and the residual scheme is used to multiply the input channel branch feature map. The specific operation of the channel attention module is as follows:\n\nBoth W 2 and b 2 are learnable parameters, and then the outputs of the two branches are merged by concatenating them along the channel dimension:\n\nThe attention weights on space and channel are learned separately through two branches, and the corresponding residual scheme is multiplied with the respective input feature map to enhance the representations of space and channel. Finally, the channel shuffle  [47]  is employed to facilitate communication of information between different groups along the channel dimension. The information of the features interacts in the channel dimension, and the size of the output feature map is the same as the initial input.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "D. Lct Block",
      "text": "Inspired by  [48] , we proposed an LCT Block shown in Fig.  3 , which consists of three modules: lightweight local convolution (LLC), coordinate attention-lightweight multi-head attention mechanism (CA-LMAM) and SE Inverted Bottleneck feed forward network (SE-IBFFN). Through the combination of convolution and transformer, LCT Block obtains the information of features from the local and the whole, which is more efficient and has fewer parameters than the traditional transformer. LLC is a lightweight convolution module, which efficiently obtains local information of features. The CA-LMAM module can capture the long-distance dependencies of features and enhance the information in the time-frequency domain through the Coordinate Attention (CA)  [49]  module. SE-IBFFN introduces a nonlinear part through a feedforward network with inverted residuals, which enhances the performance of the model and can further capture the local information of features. These three modules will be introduced in detail below 1) LLC: In order to make up for the lack of local information in trasnformer, a convolution module is used to obtain local information in speech which called LLC. Some work such as CMT  [48]  also adopted a similar architecture, but the convolution module in CMT is relatively simple, only using a Depthwise convolution with residuals, which can not obtain effective local information. Inspired by  [50] , we used a large convolution kernel Depthwise convolution with residuals and a Pointwise convolution in the local feature extraction module, given an input feature X ∈ R C×H×W as follows:\n\nThe activation layer and batch normalization are omitted, and the same omission will be in subsequent formulas. P W Conv represents Pointwise convolution and DW Conv represents Depthwise. In P W Conv we used a large 7 × 7 convolution kernel is used to get a larger receptive field than a 3 × 3 convolution kernel. In addition, P W Conv has smaller parameters than DW Conv and ordinary convolution. Additionally, a residual structure is incorporated to address the issue of gradient dispersion.\n\n2) CA-LMAM: In transformer, multi-head attention is usually used to make the model pay more attention to the more noteworthy part of itself, which can obtain long-distance dependence in speech features. Given the output X ∈ R C×H×W in the LLC module, the input of multi-head attention is query Q, key K, and value V , respectively. If the original features are directly input into multi-head attention, it will often bring a large amount of calculation. The amount of calculation is related to the size of the input features. Therefore, 2 × 2 DW Conv is used to downsample the parts of K and V , as shown below:\n\nWhere\n\n2 , then the H and W dimensions are merged and input a linear layer respectively. Finally get\n\nFor Q, a CA module is used to enhance the time-frequency domain representation of speech features. This attention mechanism can obtain long-distance feature dependence along one direction and spatial dependence information in another direction. However, in speech, speech features have more special significance in the spatial dimension. The abscissa of speech features is the time axis, which represents the time domain information of speech while the ordinate of the speech feature represents the frequency information of the speech, so CA can better aggregate the dependent information in the time domain and frequency domain for speech features, which is more suitable for SER tasks.\n\nThe specific operation of CA is shown in Fig.  3 . Given the input X ∈ R C×H×W , pooling is performed in the time domain and frequency domain respectively:\n\nX T AvgP ool ∈ R C×H×1 and X F AvgP ool ∈ R C×1×W are the feature maps after aggregation in two directions of timefrequency domain. Then, they concatenate and perform convolution operations to encode the information: +H) , r is the reduction rate of the channel, and then separated into two separate features X t , X f along the spatial direction, where\n\n. The extraction of features is then performed by two separate convolutions, followed by activation using the σ activation function, and then multiplied by the initial output to learn the more critical information in the feature:\n\nWe adjust the dimension of Q, eventually\n\nEventually learning information about the model itself through multi-headed self-attention:\n\nWhere B is a learnable parameter, representing the relative position coding of multi-head self-attention, which is used to characterize the relative position relationship between tokens.\n\nIt is more flexible than the traditional absolute position coding, making transformer better model the relative position information of speech features.\n\n3) SE-IBFFN: In the original transformer, FFN is generally composed of two linear layers. In this paper, inspired by  [51] , a series of Pointwise convolution and Depthwise convolution make up our SE-IBFFN. Compared with the Transformer model traditionally composed of linear layers, this module can capture local information while learning channel information, and has a smaller number of convolution parameters than ordinary ones.\n\nThe structure of SE-IBFFN is shown in Fig.  3 . Given the input X ∈ R C×H×W , the specific calculation process is as follows:\n\nFirstly, the Pointwise convolution operation is performed on the input, and the number of channels is expanded to 4 times of the original to increase the feature size of the channels. Then a 7 × 7 large convolution kernel DW Conv is used to obtain the local information in the feature, and the large convolution kernel can provide a larger receptive field without increasing too many parameters. The feature map is then restored to its original size using P W Conv.\n\nThen, the SE module is to obtain the attention of the channel dimension of the feature map. Given the input X ∈ R C×H×W of SE, the specific calculation process is as follows:\n\nCompared with  [51] , we put the SE module after P W Conv, which makes the parameters of the model smaller. A residual structure is used to solve the problem of gradient dispersion.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Iv. Experiments Setup",
      "text": "In this section, we will introduce the dataset and experimental details used in our study, as well as the evaluation metrics used to assess the performance of our algorithm.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "A. Corpus Description",
      "text": "To verify the performance of our proposed algorithm, performance tests on two benchmark databases are conducted, on which we will evaluate our algorithm.\n\nActually, Interactive Emotional Dyadic Motion Capture (IEMOCAP)  [52]  is an action, multimodal, and multimodal database that contains data from 10 actors and actresses during an emotional binary interaction, with two speakers (one male and one female) speaking in each session. The IEMOCAP database has been annotated by several annotators with categorical labels and dimensional labels. The database combines discrete and dimensional sentiment models. In our work, the method used improvisational and scripted data, choose anger, happiness, neutral, sadness and excitement as basic emotions, and merge happy and excited into happy. We partitioned the IEMOCAP dataset into training and testing sets by randomly selecting 80% and 20% of the data, respectively.\n\nTo valid our method robustness, we tested our method in other datasets. The Berlin Emotional Database (Emo-DB)  [53]  is a German emotional speech database recorded by the Technical University of Berlin. The database includes recordings of ten actors, comprising of five male and five female, who simulate seven emotions, including neutral, anger, fear, joy, sadness, disgust, and boredom, on ten sentences (five short and five long), resulting in a total of 535 speech recordings (233 male and 302 female). It has high emotional freedom, adopts 16 kHz sampling, and 16-bit quantization, and saves files in WAV format. It is a discrete emotional language database, and the excitation method is performance type.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "B. Implementation Details",
      "text": "In the feature generation phase, the method uses the melfrequency cepstrum coefficients (MFCCs) extracted by 26 Mel filters as the feature input. Meanwhile it divided each input speech into 1.8 seconds of speech segments, and the overlap between segments is 1.6 seconds, which can generate a large number of speech samples to solve the problem of scarcity of data set samples in SER. To obtain the prediction result for a sentence in the test set, we take the average of the prediction results of all speech segments within that sentence. Our model trained a total of 150 epochs, used the cross entropy criterion as the objective function, and used the Adam optimizer. The weight decay rate is 10 -6 , the learning rate and the minibatch size are set to 0.001 and 128, respectively, and the multiplication factor 0.95 is exponentially decayed until the value reaches 10 -6 . Our experiment is carried out on Ubuntu 18.04 with a GeForce RTX 2080ti GPU, and we utilized Pytorch 1.7 as the training framework.\n\nIn addition, we use the method of mixup  [54]  to train, so as to improve the generalization ability of the system. This method constructs new training samples and labels by linear interpolation, and effectively smoothes the discrete data space into continuous space. In our proposed model, we set α to 0.2 for best performance.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "C. Evaluation Metrics",
      "text": "In this section, we'll describe in detail the criteria we use to evaluate the performance of our algorithms. For various categories of performance in the dataset, Precision, Recall, and F1-score are the general metrics to measure their performance. First of all, four concepts will be introduced: True Positive (TP), False Positive (FP), True Negative (TN), and False Negative (FN), where TP means actual positive and predicted positive, FP means actual positive and predicted negative, TN means actual negative and predicted positive, and FN means actual negative and predicted negative. The Precision, Recall, and F1-score can be expressed as:\n\nTo evaluate the overall performance of our model, weighted average accuracy (WA) and unweighted average accuracy (UA) will be used as evaluation metrics, where WA is the weighted average accuracy of different sentiment categories, and its weight is related to the number of sentiment categories, and UA is the average accuracy of different sentiment categories. The validity of the model is better evaluated in the context of unbalanced SER dataset samples. The calculation methods for these two metrics are as follows:\n\nAmong them, C represents the number of emotional categories, and N i represents the number of samples of class i.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "V. Results",
      "text": "In this section, we conduct extensive experiments to evaluate the performance of our method on two datasets. The section mainly compares our proposed model with state-of-the-art baselines, and then verify the effectiveness of our proposed modules through ablation experiments.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "A. Comparison With State-Of-The-Art",
      "text": "To compare with our proposed model, we evaluate the performance of the algorithm from the WA and UA perspective with a series of existing methods in IEMOCAP and Emo-DB datasets, as shown in Table  I  and Table  II . The proposed model is compared with several commonly used speech emotion recognition models, including algorithms based on the combination of CNN and transformer  [19] , CNN-based algorithms  [14]  [55], LSTM-based algorithms  [56]  [57], attention-based methods  [21]  [58], and some other algorithms.  To verify the performance of our proposed method, we compare it with other speech emotion recognition algorithms in IEMOCAP and Emo-DB datasets. The results are shown in Table  I  and Table  II .\n\nFor IEMOCAP dataset, as shown in Table  I , our proposed method outperforms other methods. In addition, compared with the simple splicing of transformer and CNN  [19] , our method achieves the best results through the ingenious design of local and global feature extraction. Compared to traditional spatial and channel attention  [21] , temporal attention information makes it more competitive. In addition, for the CNN or LSTM-based method  [14]  [57]  [56] [61] , our method introduces global information through a lightweight transformer module to bring more comprehensive features to the system, and also shows that our transformer module has a stronger ability to obtain global information than some capsule networks. Finally, our method is as competitive as the method using semi-supervised methods  [59]  and pre-trained models  [60] .\n\nFor Emo-DB dataset, as shown in Table  II , our proposed method outperforms several methods. Similar to the performance in IEMOCAP dataset, our attention mechanism and local-global model have significant advantages over traditional attention and CNN-based models  [58]  [63]  [55] . In addition, Emo-DB dataset is a smaller dataset. Compared with non-deep learning feature extraction and feature learning methods  [62]  [64], our overall end-to-end network based on deep learning also has a relatively better performance on this small dataset, indicating that our method still has excellent robustness on small datasets.\n\nIn summary, combining the performance of these two datasets proves the superiority of our proposed method.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "B. Results And Analysis",
      "text": "In this section, Table  III  and Table IV list the Precision, Recall, F1-score, and overall WA and UA for each sentiment category in IEMOCAP dataset and Emo-DB dataset, respectively. In addition, the confusion matrices are visualized of the two datasets in Fig.  4  and Fig.  5 , where the diagonal indicates that the sentiment is correctly classified, and other locations indicate that the sentiment is misclassified as other sentiments. The darker the color in the grid while higher the accuracy.  For IEMOCAP dataset, as shown in Table  IV  and Fig.  4 , our proposed model achieves good results on this dataset and has good accuracy for all four emotions, especially sadness, anger and happiness. Among these four emotions, anger has the highest recognition accuracy, while neutral has the lowest recognition accuracy. Sadness, anger and happiness will be misjudged as neutral in some cases. The result is similar to that in  [65] , which also verifies that neutral emotion is a defect of expression. This emotion is easily expressed as other emotions, which makes the model misjudged. Therefore, neutral emotions are easily confused with other emotions. For Emo-DB dataset, as shown in Table  IV  and Fig.  5 , our proposed model also achieved good results on this dataset, and has good accuracy for seven emotions. The three emotions achieved 100% accuracy, and Angry also achieved 96% accuracy. In addition, the wrong judgment in happiness is anger. We believe that this is because these two emotions have similar arousal, while the wrong judgment in disgust is fear. Finally, the accuracy of fear is lower than that of other emotion categories, but it also achieves relatively good results. In some cases, fear is easily misjudged as sad, angry and happy, this is because these four emotions have a high degree of arousal.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "C. Ablation Study",
      "text": "To explore the role of each part of our proposed model, Table  V  shows the results of a series of ablation experiments. The following are the modules for comparison. It can be seen from the table that when using the T-Sa module, our method has an absolute improvement of 1.2% and 1.54% in WA and UA, indicating that our attention mechanism module has a very significant effect and can aggregate the noteworthy parts of the features. In addition, when the temporal attention is removed, the model effect is also greatly reduced, indicating that temporal attention plays a non-important role in our model to enhance the temporal information in speech. We also directly removed the LCT module, which caused the performance of the model to be reduced by 4.52% and 3.79% on WA and UA, it clearly shows the importance of introducing global information into our LCT module.\n\nFor our LCT part, our experiments also verified the role of different modules within the LCT. Firstly, the LLC part is replaced with a 3 × 3 ordinary convolution, which reduces the performance of the model by 3.35% and 2.74% on WA and UA, and the number of parameters has also been improved. This shows that the wider receptive field brought by our large convolution kernel LLC module is very important, and it does not bring a larger number of parameters. In order to verify the role of our CA module, we removed the CA module, which caused the performance of the model to decrease by 2.35% and 2.6% on WA and UA, indicating that the time-frequency domain representation of the speech features enhanced by the CA module is of vital importance. Finally, the SE Attention part in our SE-IBFFN is removed, which reduces the performance of the model by 1.72% and 1.41% on WA and UA. It also proves that using the SE module to obtain the attention of the feature map channel dimension can improve the performance of the model to recognize emotions.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "D. Model Efficiency Analysis",
      "text": "In order to explore the size and efficiency of the proposed model, Table  VI    counts and accuracy between our model and other models.\n\nThe following are the modules we designed for comparison.\n\n• W/ Conv-LCT: This module replaces the LLC module in our LCT module with a 3 × 3 convolution and the other parts are retained.   VI , it can be seen that when the LLC module in LCT is replaced with a regular convolution, the number of parameters increases and the accuracy is lower than that of the LLC module. This indicates that LLC has better accuracy with fewer parameters. Additionally, when the first PW and DW convolutions in SE-IBFFN are replaced with regular convolutions, the number of parameters increases significantly, and the accuracy is still lower than the original model. We also reduced the size of the convolution kernel to 3 × 3, which improved the number of parameters, but it is still larger than the original SE-IBFFN, and the accuracy decreased. This suggests that the SE-IBFFN module did not bring a huge number of parameters while using a larger convolution kernel to achieve a larger receptive field, and the accuracy is still excellent.\n\nAdditionally, we attempted to replace CA-LMAM and SE-IBFFN in LCT with traditional transformers, and experimental results showed that the traditional transformer has a larger number of parameters and a decrease in accuracy compared to LCT, with a decrease of 2.23% and 2.29%, respectively. The entire LCT module was also replaced with other transformer models, including the MobileVit series which combines CNN and Vit and is a lightweight transformer model. The MobileVit block in each series was used to replace LCT, and the results showed that the parameter size of the MobileVitv3 block is smaller than LCT, but there is still a gap in accuracy. The performance of the two-layer MobileVitv1 block was the best, but it still did not reach the accuracy of LCT. Additionally, we found that models with a depth of 2 performed better than those with a depth of 3, due to the difficulty of fitting to small SER datasets as the number of parameters increases. In future work, in addition to MFCC features, we will try more hierarchical speech features and combine current CNN and transformer structures to improve the performance of Speech Emotion Recognition from multiple feature dimensions.",
      "page_start": 10,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: An illustration of our proposed speech emotion recognition framework consisting of three modules: i) CNN Block is used to extract local information in",
      "page": 4
    },
    {
      "caption": "Figure 1: , which consists of three",
      "page": 4
    },
    {
      "caption": "Figure 1: , the CNN Block",
      "page": 4
    },
    {
      "caption": "Figure 2: An illustration of our proposed T-Sa attention module consisting of two modules: Timing attention which enhances the current features temporally",
      "page": 5
    },
    {
      "caption": "Figure 2: , which is divided into two parts before and",
      "page": 5
    },
    {
      "caption": "Figure 3: An illustration of our proposed LCT Block attention module consisting of three modules: i) LLC, which efficiently captures the local information of the",
      "page": 6
    },
    {
      "caption": "Figure 3: , which consists of three modules: lightweight local con-",
      "page": 6
    },
    {
      "caption": "Figure 3: Given the",
      "page": 7
    },
    {
      "caption": "Figure 4: and Fig. 5, where the diagonal indicates",
      "page": 9
    },
    {
      "caption": "Figure 4: Visualization the confusion matrices of the proposed method: (a) Confusion matrix on IEMOCAP; (b) Confusion matrix on Emo-DB.",
      "page": 10
    },
    {
      "caption": "Figure 5: , our proposed model also achieved good results on this",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "human-machine\ninteractions. Mainstream approaches\nutilize",
          "one of the most basic human communication methods, speech": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "",
          "one of the most basic human communication methods, speech": "emotion recognition is particularly important for computers to"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "Convolutional Neural Networks or Recurrent Neural Networks",
          "one of the most basic human communication methods, speech": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "",
          "one of the most basic human communication methods, speech": "analyze and respond to the emotional state of human users and"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "to learn local energy feature representations of speech segments",
          "one of the most basic human communication methods, speech": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "",
          "one of the most basic human communication methods, speech": "respond to them accordingly. With the rapid development of"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "from speech\ninformation,\nbut\nstruggle with\ncapturing\nglobal",
          "one of the most basic human communication methods, speech": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "information such as the duration of energy in speech. Some use",
          "one of the most basic human communication methods, speech": "artificial\nintelligence, speech emotion recognition has received"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "Transformers\nto capture global\ninformation, but\nthere\nis\nroom",
          "one of the most basic human communication methods, speech": "extensive research attention. Human speech contains a wealth"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "for improvement\nin terms of parameter count and performance.",
          "one of the most basic human communication methods, speech": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "",
          "one of the most basic human communication methods, speech": "of\ninformation,\nincluding not only the language content but"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "Furthermore, existing attention mechanisms\nfocus on spatial or",
          "one of the most basic human communication methods, speech": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "",
          "one of the most basic human communication methods, speech": "also attributes\nsuch as gender\nand emotions of\nthe\nspeaker."
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "channel dimensions, hindering learning of\nimportant\ntemporal",
          "one of the most basic human communication methods, speech": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "",
          "one of the most basic human communication methods, speech": "It\nis\nof\ngreat\nsignificance\nto\naccurately\nidentify\nemotional"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "information in speech.\nIn this paper,\nto model\nlocal and global",
          "one of the most basic human communication methods, speech": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "information at different\nlevels of granularity in speech and cap-",
          "one of the most basic human communication methods, speech": "information from speech signals."
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "ture temporal, spatial and channel dependencies in speech signals,",
          "one of the most basic human communication methods, speech": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "",
          "one of the most basic human communication methods, speech": "Feature\nextraction\nof\nspeech\nis\na\nrather\nimportant\nand"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "we propose\na Speech Emotion Recognition network based on",
          "one of the most basic human communication methods, speech": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "",
          "one of the most basic human communication methods, speech": "challenging task in speech emotion recognition, and the ex-"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "CNN-Transformer and multi-dimensional attention mechanisms.",
          "one of the most basic human communication methods, speech": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "Specifically,\na\nstack of CNN blocks\nis dedicated to\ncapturing",
          "one of the most basic human communication methods, speech": "traction of\nfeatures directly affects the effectiveness of subse-"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "local\ninformation in speech from a time-frequency perspective.",
          "one of the most basic human communication methods, speech": "quent model\ntraining and the accuracy of\nthe final algorithm"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "In addition, a time-channel-space attention mechanism is used to",
          "one of the most basic human communication methods, speech": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "",
          "one of the most basic human communication methods, speech": "for emotion recognition. Speech features can be categorized"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "enhance features across\nthree dimensions. Moreover, we model",
          "one of the most basic human communication methods, speech": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "",
          "one of the most basic human communication methods, speech": "as\nacoustic-based features\nand deep learning-based features"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "local and global dependencies of\nfeature\nsequences using large",
          "one of the most basic human communication methods, speech": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "",
          "one of the most basic human communication methods, speech": "where acoustic-based features can be broadly classified into"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "convolutional kernels with depthwise separable convolutions and",
          "one of the most basic human communication methods, speech": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "lightweight Transformer modules. We\nevaluate\nthe\nproposed",
          "one of the most basic human communication methods, speech": "rhythmic features\n[3],\nspectral-based correlation features\n[4],"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "method\non\nIEMOCAP and Emo-DB datasets\nand\nshow our",
          "one of the most basic human communication methods, speech": "and phonetic features [5]. Among them, spectral-based corre-"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "approach significantly improves the performance over the state-",
          "one of the most basic human communication methods, speech": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "",
          "one of the most basic human communication methods, speech": "lation features\nreflect\nthe characteristics of\nthe signal\nin the"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "of-the-art methods1.",
          "one of the most basic human communication methods, speech": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "",
          "one of the most basic human communication methods, speech": "frequency domain, where there are differences\nin the perfor-"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "Index Terms—Speech emotion recognition,\ntemporal-channel-",
          "one of the most basic human communication methods, speech": "mance of different emotions in the frequency domain. Based"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "spatial attention, lightweight convolution transformer, local global",
          "one of the most basic human communication methods, speech": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "",
          "one of the most basic human communication methods, speech": "on the\nspectral\ncorrelation features\ninclude\nlinear\nspectrum"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "feature fusion.",
          "one of the most basic human communication methods, speech": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "",
          "one of the most basic human communication methods, speech": "[6]\nand\ninverse\nspectrum [7], Linear Prediction Cofficients"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "",
          "one of the most basic human communication methods, speech": "(LPC), Log Frequency Power Coefficients\n(LFPC),\netc.;\nIn-"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "I.\nINTRODUCTION",
          "one of the most basic human communication methods, speech": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "",
          "one of the most basic human communication methods, speech": "verse spectrum includes Mel-Frequency Cepstrum Coefficients"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "importance in var-",
          "one of the most basic human communication methods, speech": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "",
          "one of the most basic human communication methods, speech": "(MFCC), Linear Prediction Cepstrum Cofficients (LPCC), etc."
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "especially in increasingly common human-\nE MOTION recognition has significant",
          "one of the most basic human communication methods, speech": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "",
          "one of the most basic human communication methods, speech": "Among them, MFCC is regarded as a low-level feature based"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "computer interaction systems [1], and speech emotion recogni-",
          "one of the most basic human communication methods, speech": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "",
          "one of the most basic human communication methods, speech": "on human knowledge, which is widely used in the field of"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "tion (SER) has promising applications in areas such as mental",
          "one of the most basic human communication methods, speech": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "",
          "one of the most basic human communication methods, speech": "speech."
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "health monitoring\n[2],\neducational\nassistance,\npersonalized",
          "one of the most basic human communication methods, speech": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "",
          "one of the most basic human communication methods, speech": "Early SER algorithms mainly used acoustic-based features"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "content\nrecommendation, and customer\nservice quality mon-",
          "one of the most basic human communication methods, speech": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "",
          "one of the most basic human communication methods, speech": "and combined with traditional machine\nlearning algorithms,"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "itoring. Speech contains\nrich emotional\ninformation,\nand as",
          "one of the most basic human communication methods, speech": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "",
          "one of the most basic human communication methods, speech": "including hidden Markov models [8], Gaussian mixture mod-"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "Corresponding author: Xiaoyu Tang. E-mail address:\ntangxy@scnu.edu.cn.",
          "one of the most basic human communication methods, speech": "els\n[9],\nand support vector machines\n[10].\nIn recent years,"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "Xiaoyu Tang is with the School of Electronic and Information Engineering,",
          "one of the most basic human communication methods, speech": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "",
          "one of the most basic human communication methods, speech": "deep learning-based neural networks have gradually become"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "Faculty of Engineering, South China Normal University, Foshan, Guangdong",
          "one of the most basic human communication methods, speech": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "528225, China,\nand also with the School of Physics\nand Telecommunica-",
          "one of the most basic human communication methods, speech": "active in the field of speech emotion recognition [11], [12], and"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "tions Engineering, South China Normal University, Guangzhou, Guangdong",
          "one of the most basic human communication methods, speech": "compared with traditional models, deep learning-based models"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "510000, China.",
          "one of the most basic human communication methods, speech": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "",
          "one of the most basic human communication methods, speech": "have shown better performance in speech emotion recognition."
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "Yixin Lin is with the School of Electronic and Information Engineering,",
          "one of the most basic human communication methods, speech": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "Faculty of Engineering, South China Normal University, Foshan, Guangdong",
          "one of the most basic human communication methods, speech": "Deep\nlearning-based\nfeatures\nuse\nneural\nnetworks\nto\nlearn"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "528225, China.",
          "one of the most basic human communication methods, speech": "more advanced features from the original signal of speech or"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "Ting Dang is with the Nokia Bell Labs, Cambridge, UK.",
          "one of the most basic human communication methods, speech": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "",
          "one of the most basic human communication methods, speech": "some low-dimensional acoustic features. Convolutional neural"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "Yuanfang Zhang\nis with\nthe Autocity\n(Shenzhen) Autonomous Driving",
          "one of the most basic human communication methods, speech": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "Co.,ltd.",
          "one of the most basic human communication methods, speech": "networks\n(CNNs)\nare\neffective\nin\ncapturing\nlocal\nacoustic"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "Jintao Cheng is with the School of Physics and Telecommunications En-",
          "one of the most basic human communication methods, speech": "details\nin\nspeech, while\nlong\nshort-term memory\nnetworks"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "gineering, South China Normal University, Guangzhou, Guangdong 510000,",
          "one of the most basic human communication methods, speech": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "",
          "one of the most basic human communication methods, speech": "(LSTMs) are widely used in speech emotion recognition for"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "China.",
          "one of the most basic human communication methods, speech": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "",
          "one of the most basic human communication methods, speech": "modeling dynamic information and temporal dependencies in"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "1Our\ncode\nis\navailable\non\nhttps://github.com/SCNU-RISLAB/CNN-",
          "one of the most basic human communication methods, speech": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "Transformer-and-Multidimensional-Attention-Mechanism",
          "one of the most basic human communication methods, speech": "speech. Additionally,\nattention mechanisms\nare\nalso\na\nkey"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nis\ncrucial\nin": "0000–0000/00$00.00 © 2021 IEEE",
          "one of the most basic human communication methods, speech": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "factor in improving model performance, as they can adaptively",
          "2": "capturing the temporal\ninformation present\nin speech, which"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "focus on the importance of different\nfeatures to obtain better",
          "2": "represents the dynamics of speech and plays a crucial\nrole in"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "et\nspeech\nfeatures\nat\nthe\ndiscourse\nlevel. For\nexample, Qi",
          "2": "emotion recognition. This limitation is a common issue in both"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "al.\n[13] proposed a hierarchical network based on static and",
          "2": "MLPs and CNNs."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "dynamic features, which uses LSTM to encode dynamic and",
          "2": "In this paper, we\ninvestigate how to effectively combine"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "static features of\nspeech and designs a gating model\nto fuse",
          "2": "transformer and CNN and apply them to SER to characterize"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the\nfeatures,\nan attention mechanism is used to acquire\nthe",
          "2": "local\nfeatures\nand\nglobal\nfeatures\nin\nspeech\nsignals,\nand"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "et\nal.\ndiscourse-level\nspeech\nfeatures. Liu\n[14]\nproposed\na",
          "2": "propose\nthe\ntemporal-channel-space\nattention mechanism in"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "local-global perceptual depth representation learning system.",
          "2": "the model\nfor multiple dimensions of\nfeature\nenhancement."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "One module contains a multiscale CNN and a time-frequency",
          "2": "Specifically, we first use a set of stacked CNNs to capture local"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "CNN (TFCNN)\nto learn the local\nrepresentation, and in the",
          "2": "information in speech and learn shallow features of speech for"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "other module,\na Capsule network with an improved routing",
          "2": "the transformer module for better\ntraining of\nthe transformer"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "algorithm is utilized to design a multi-block dense connection",
          "2": "module. In the stacked CNN module, two sets of convolutional"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "structure, which\ncan\nlearn\nboth\nshallow and\ndeep\nglobal",
          "2": "filters of different\nshapes are used to capture both temporal"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "information.",
          "2": "and\nfrequency\ndomain\ncontextual\ninformation. Specifically,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Although speech emotion recognition (SER) models com-",
          "2": "after\nstacking the CNN modules, we introduced a temporal-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "posed\nof CNNs\nexhibit\nbetter\nperformance\nthan\ntraditional",
          "2": "channel-space attention mechanism that models the contextual"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "models,\nthese networks can only extract\nlocal\ninformation in",
          "2": "emotional\nexpression\nof\nfeatures\nover\ntime,\nand\nefficiently"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "speech, such as the energy and rhythm of a particular segment",
          "2": "fuses\nthe attention of\nthe spatial and channel dimensions of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "of\nspeech, while\nstruggling\nto\nlearn\nglobal\ninformation\nin",
          "2": "the speech feature map through the Shuffle unit. Furthermore,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "speech features, such as the overall volume and speaking rate",
          "2": "a\ncombination\nof\ntransformer\nand CNN is\nused\nto model"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "of the speaker, and the duration of energy,\nthus neglecting the",
          "2": "the local and global dependencies of\nfeature sequences by a"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "global correlation of\nfeatures [15].\nIn recent years,\nthe trans-",
          "2": "deep separable convolution with residuals and a lightweight"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "former\n[16] based on the self-attentive mechanism has been",
          "2": "transformer module. The main contributions of\nthis work are"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "widely used in major deep learning tasks. Tarantino et al. [17]",
          "2": "summarized as follows:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "used transformer\nin combination with global windowing for",
          "2": "• A framework based on CNN and transformer is proposed"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "speech emotion recognition and achieved better performance,",
          "2": "for\nspeech\nemotion\nrecognition. Our\nframework\nuses"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "but\ntransformer\nis weak\nfor\nlocal\nfeature\nextraction. Some",
          "2": "time-frequency domain convolution and stacked convolu-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "recent work has attempted to combine CNN and transformer",
          "2": "tion blocks to extract\ninitial\nlocal\nfeatures of speech and"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "to\nalleviate\nthe\nlimitations\nof\nusing CNN and\ntransformer",
          "2": "stacked CNN and transformer blocks are used to enhance"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "alone. Wang et al.\n[18]\nstacked the transformer blocks after",
          "2": "local and global\nfeatures."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the CNN model\nto improve the global features of aggregation.",
          "2": "• To enhance the finiteness of\nthe feature map and model"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "A model combining the transformer and CNN is proposed in",
          "2": "the temporal\ninformation of speech, a temporal-channel-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[19],\nenabling it\nto learn local\ninformation while\ncapturing",
          "2": "space attention mechanism called Time-Shuffle Attention"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "global dependencies. The original\ntransformer tends to have a",
          "2": "(T-Sa)\nis used in our model. T-Sa enhances\nthe feature"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "high number of parameters for computing multi-headed self-",
          "2": "map in multi-dimensions."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "attention, which requires a lot of\nresources and poses\nsome",
          "2": "• We propose a module based on deeply separable convo-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "difficulties\nin the\ntraining of\nthe network.\nIn addition,\nthis",
          "2": "lution and a lightweight\ntransformer called Lightweight"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "kind of work tends\nto stack transformers\nin the last part of",
          "2": "convolution\ntransformer(LCT).\nThis\nmodel\nemploys"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the model or a simple combination of\ntransformer and CNN,",
          "2": "lightweight\nconvolutional\nblocks\nto\nefficiently\nextract"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "which makes it hard to obtain better\nlocal\ninformation of\nthe",
          "2": "local\ninformation from features, and incorporates Coor-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "speech.",
          "2": "dinate Attention (CA)\ninto the multi-head self-attention"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "In addition, attention mechanisms improve the effectiveness",
          "2": "mechanism to capture\nlong-range dependencies\namong"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "of task processing by selectively attending to features that are",
          "2": "features while\nenhancing\ntheir\ntemporal\nand\nspectral"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "most relevant to the current task, and have received widespread",
          "2": "information."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "attention in major fields.\nIn recent years,\nseveral\nresearchers",
          "2": "• Extensive experiments of our proposed model on IEMO-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "have\nutilized\ndeep\nlearning methods\nfor\nfeature\nextraction",
          "2": "CAP and EMO-DB datasets demonstrate the effectiveness"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "and used attention mechanisms\nto improve performance. A",
          "2": "of\nthe model\nin SER tasks."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "lightweight\nself-attention module is proposed in [20], which",
          "2": "The\nrest\nof\nthe\npaper\nis\norganized\nas\nfollows.\nSection"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "uses MLP to extract channel information and a large perceptual",
          "2": "II\nbriefly\nreviews\nrelated work. Details\nof\nthe\nsystem are"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "field extended CNN to extract spatial contextual\ninformation.",
          "2": "presented in Section II. In Section IV,we present experimental"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Guo et al.\n[21] proposed an attention mechanism based on",
          "2": "results\nto showcase\nthe\neffectiveness of our model on two"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "time,\nfrequency,\nand CNN channels\nto improve\nrepresenta-",
          "2": "widely-used\nbenchmark\ndatasets. Section V concludes\nthis"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "tion learning ability. However,\ntemporal\ninformation is often",
          "2": "work."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "embedded in speech, which reflects\nthe dynamic changes of",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "II. RELATED WORK"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "speech,\nsuch as pitch and energy variations over\ntime. Tem-",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "poral\nfeatures can reflect\nthe temporal context and evolution",
          "2": "In this section, we will briefly review the algorithms related"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "of\nemotion expression in speech. However,\nit\nfalls\nshort\nin",
          "2": "to\nspeech\nemotion\nrecognition,\nnamely\nconvolutional\nand"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "recurrent\nneural\nnetworks,\nattention mechanism,\nand\ntrans-",
          "3": "that combines both spatial and channel dimensions to obtain"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "former.",
          "3": "attention with better results. In addition, some researchers have"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "used deep learning methods\nfor\nfeature extraction of\nspeech"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "and enhancement of feature maps using attention mechanisms."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "A. Convolutional neural networks and recurrent neural net-",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "An attention pooling-based approach was proposed in [33],"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "works",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "compared to existing average and maximum pooling,\nit can"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Speech is\na\ncontinuous\ntime-series\nsignal,\nand CNN and",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "combine\nboth\nclass-agnostic\nbottom-up\nattention maps\nand"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "RNN have been two main network structures for SER. Moti-",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "class-specific top-down attention maps in an effective manner."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "vated by the studies of CNN in computer vision, AlexNet [22]",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "Mustaqeem et al. [20] proposed a self-attentive module (SAM)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "and ResNet [23] show promising results in image classification",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "for SER systems,which uses a multilayer perceptron (MLP)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "et\nal.\ntasks\nand\ntherefore\nhave\nbeen\nstudies\nin SER. Zhu",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "to recognize global\ninformation of the channels and identifies"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[24] has proposed a new Global Aware Multi-scale (GLAM)",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "spatial\ninformation using a special dilated CNN to generate"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "neural network that utilizes a global perception fusion module",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "an attentional map for both channels. SAM significantly re-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "to learn multi-scale feature representations, with a focus on",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "duces the computational and parametric overhead. A spectro-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "emotional\ninformation. The multi-time-scale\n(MTS) method",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "temporal-channel (STC) attention mechanism was proposed in"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "was\nintroduced in [25], which extends\nthe CNNs by scaling",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "[21], which acquires attention feature maps along three dimen-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "and resampling the convolutional kernel along the time axis",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "sions: time, frequency, and channel. Xi et al. [34] employed an"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "et\nal.\nto\nincrease\ntemporal flexibility. Liu\n[14]\nproposed\na",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "attention mechanism based on the time and frequency domain"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "local global-aware deep representation learning system that",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "to introduce long-distance contextual\ninformation."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "uses CNNs\nand Capsule Networks\nto learn local\nand deep",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "The\ncurrent\nattention mechanisms\ntypically\nfocus more"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "global\ninformation.",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "on\nspatial\nor\nchannel\ninformation\nin\nfeature maps,\noften"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "RNN can model\nthe\ntemporal\ninformation in speech and",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "neglecting the\ntemporal\ncharacteristics\nin speech. However,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "capture\nlong-term dependencies\nin\nthe\nspeech\nsignal more",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "temporal features in speech are equally important for emotion"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "effectively. A new layered network HNSD was proposed [13]",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "recognition. Therefore,\nit\nis necessary to pay more attention to"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "that\ncan efficiently integrate\nstatic\nand dynamic\nfeatures of",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "temporal information in attention mechanisms to better explore"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "SER, which uses LSTM to encode static and dynamic features",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "and utilize the temporal characteristics in speech signals."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "and gated multi-features unit\n(GMU)\nfor\nframe level\nfeature",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "fusion for\nthe\nemotional\nintermediate\nrepresentation. Xu et",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "al.\n[26] proposed a hierarchical grained and feature model",
          "3": "C. Transformer"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "(HGFM)\nthat uses recurrent neural networks to process both",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "Transformer\nhas\nbeen\nrapidly\ndeveloping\nin\nthe field\nof"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "discourse-level\nand\nframe-level\ninformation\nof\nthe\nspeech.",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "natural\nlanguage processing (NLP)\nin recent years\nand has"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Since\nconvolutional\nneural\nnetworks\ncan\ncapture\nlocal\nin-",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "achieved great\nsuccess. Due to its powerful ability to obtain"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "formation of\nfeatures, while\nrecurrent neural networks\ntake",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "global\ninformation,\nthe\ntransformer\nhas\nbeen\ngradually\nex-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "advantage\nof modeling\ntemporal\ninformation, many works",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "tended to the fields of speech. An end-to-end speech emotion"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "have combined these two approaches and achieved outstanding",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "recognition model\n[18] was proposed to enhance the global"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "results. Li\n[27]\nextracted location information from MFCC",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "feature\nrepresentation of\nthe model by using stacked trans-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "features and VGGish features by bi-direction long short\ntime",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "former blocks at\nthe end of\nthe model. Hu et al.\n[19]\ntook"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "memory (BiLSTM) neural network, and then fused these two",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "advantage of multiple models,\nimproved the learning ability"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "features to predict emotions. Liu et al.\n[28] combined triplet",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "of\nthe model by residual BLSTM, and proposed a convolu-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "loss\nand CNN-LSTM models\nto obtain more discriminative",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "tional neural network and E-transformer module to learn both"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "sentiment\ninformation,\nand the proposed framework yielded",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "local and global information. Recently, transformer-based self-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "excellent\nresults\nin experiments. Zou [29] et al. used CNN,",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "supervised methods\nhave\nalso\nbeen\napplied\nto\nspeech,\nand"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "BiLSTM, and wav2vec2 to extract different\nlevels of\nspeech",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "some transformer-based models have achieved great success in"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "information,\nincluding MFCC,\nspectrogram, and acoustic in-",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "automatic speech recognition (ASR),\nincluding wav2vec [35],"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "formation,\nand\nfused\nthese\nthree\nfeatures\nby\nan\nattention",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "VQ-wav2vec [36], and wav2vec2.0 [37]. There is also some"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "mechanism. Zhang [30] et al. used CNNs\nto learn segment-",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "work in speech emotion recognition that employs these models"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "level\nfeatures in spectrograms, using a deep LSTM model\nto",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "for migration learning. A pre-trained wav2vec2.0 model\n[38]"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "capture temporal dependencies between speech segments.",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "is used as the input\nto the network and the outputs of multiple"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "network\nlayers\nof\nthe\npre-trained model\nare\ncombined\nto"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "B. Attention mechanism",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "et\nproduce\na\nricher\nrepresentation\nof\nspeech\nfeatures. Cai"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "In recent years, attention mechanisms have received a lot of",
          "3": "al.\n[39]\nproposed\na multi-task\nlearning\n(MTL)\nframework"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "attention in major fields\nto improve the effectiveness of\ntask",
          "3": "that\nuses\nthe wav2vec2.0 model\nfor\nfeature\nextraction\nand"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "processing by focusing on information that\nis more critical\nto",
          "3": "simultaneously training for speech emotion classification and"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the current\ntask among the many inputs. A channel attention",
          "3": "text\nrecognition. Among computer vision tasks, ViT [40] first"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "mechanism called Squeeze-and-Excitation (SE) was proposed",
          "3": "applied transformer directly to image patch sequences which is"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "in [31], which assigns weights to individual channels and adap-",
          "3": "groundbreaking in applying transformer structure to computer"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "tively recalibrates the feature responses of\nthe channels. Woo",
          "3": "vision. ViT has\na\nsuperior\nstructure\nand\nreduced\ncomputa-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "et al.\n[32] proposed a convolutional block attention module",
          "3": "tional resource consumption compared to convolutional neural"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 1. An illustration of our proposed speech emotion recognition framework consisting of three modules: i) CNN Block is used to extract local information in": "speech;\nii) T-Sa attention module enhances speech information in three dimensions:"
        },
        {
          "Fig. 1. An illustration of our proposed speech emotion recognition framework consisting of three modules: i) CNN Block is used to extract local information in": "in speech."
        },
        {
          "Fig. 1. An illustration of our proposed speech emotion recognition framework consisting of three modules: i) CNN Block is used to extract local information in": "networks. There are many similar approaches\nin the field of"
        },
        {
          "Fig. 1. An illustration of our proposed speech emotion recognition framework consisting of three modules: i) CNN Block is used to extract local information in": "speech. ViT was\nintroduced to speech and improved based"
        },
        {
          "Fig. 1. An illustration of our proposed speech emotion recognition framework consisting of three modules: i) CNN Block is used to extract local information in": "on\nthe\nproperties\nof\nspectrograms\nin\n[41], which\nproposed"
        },
        {
          "Fig. 1. An illustration of our proposed speech emotion recognition framework consisting of three modules: i) CNN Block is used to extract local information in": "a separable transformer\n(SepTr)\nthat uses\nthe transformer\nto"
        },
        {
          "Fig. 1. An illustration of our proposed speech emotion recognition framework consisting of three modules: i) CNN Block is used to extract local information in": "process\ntokens\nat\nthe\nsame\ntime\nand\nthe\nsame\nfrequency"
        },
        {
          "Fig. 1. An illustration of our proposed speech emotion recognition framework consisting of three modules: i) CNN Block is used to extract local information in": "interval,\nrespectively.\nIn [42], a method to improve ViT was"
        },
        {
          "Fig. 1. An illustration of our proposed speech emotion recognition framework consisting of three modules: i) CNN Block is used to extract local information in": "applied to infant\ncry recognition by combining the original"
        },
        {
          "Fig. 1. An illustration of our proposed speech emotion recognition framework consisting of three modules: i) CNN Block is used to extract local information in": "log-Mel\nspectrogram, first-order\ntime-frame\nand\nfrequency-"
        },
        {
          "Fig. 1. An illustration of our proposed speech emotion recognition framework consisting of three modules: i) CNN Block is used to extract local information in": "bin differential\nlog-Mels 3D features\ninto ViT for\ninfant cry"
        },
        {
          "Fig. 1. An illustration of our proposed speech emotion recognition framework consisting of three modules: i) CNN Block is used to extract local information in": "recognition."
        },
        {
          "Fig. 1. An illustration of our proposed speech emotion recognition framework consisting of three modules: i) CNN Block is used to extract local information in": ""
        },
        {
          "Fig. 1. An illustration of our proposed speech emotion recognition framework consisting of three modules: i) CNN Block is used to extract local information in": "III. METHOD"
        },
        {
          "Fig. 1. An illustration of our proposed speech emotion recognition framework consisting of three modules: i) CNN Block is used to extract local information in": "In this\nsection, we describe the proposed model\nin detail."
        },
        {
          "Fig. 1. An illustration of our proposed speech emotion recognition framework consisting of three modules: i) CNN Block is used to extract local information in": "Our proposed model is shown in Fig. 1, which consists of three"
        },
        {
          "Fig. 1. An illustration of our proposed speech emotion recognition framework consisting of three modules: i) CNN Block is used to extract local information in": ""
        },
        {
          "Fig. 1. An illustration of our proposed speech emotion recognition framework consisting of three modules: i) CNN Block is used to extract local information in": "parts, namely CNN Block, T-Sa attention mechanism module,"
        },
        {
          "Fig. 1. An illustration of our proposed speech emotion recognition framework consisting of three modules: i) CNN Block is used to extract local information in": ""
        },
        {
          "Fig. 1. An illustration of our proposed speech emotion recognition framework consisting of three modules: i) CNN Block is used to extract local information in": "and LCT Block,\nthese\nthree modules will be\nintroduced in"
        },
        {
          "Fig. 1. An illustration of our proposed speech emotion recognition framework consisting of three modules: i) CNN Block is used to extract local information in": ""
        },
        {
          "Fig. 1. An illustration of our proposed speech emotion recognition framework consisting of three modules: i) CNN Block is used to extract local information in": "detail next."
        },
        {
          "Fig. 1. An illustration of our proposed speech emotion recognition framework consisting of three modules: i) CNN Block is used to extract local information in": ""
        },
        {
          "Fig. 1. An illustration of our proposed speech emotion recognition framework consisting of three modules: i) CNN Block is used to extract local information in": ""
        },
        {
          "Fig. 1. An illustration of our proposed speech emotion recognition framework consisting of three modules: i) CNN Block is used to extract local information in": "A. Overview of\nthe model"
        },
        {
          "Fig. 1. An illustration of our proposed speech emotion recognition framework consisting of three modules: i) CNN Block is used to extract local information in": ""
        },
        {
          "Fig. 1. An illustration of our proposed speech emotion recognition framework consisting of three modules: i) CNN Block is used to extract local information in": "To take full advantage of convolutional neural networks and"
        },
        {
          "Fig. 1. An illustration of our proposed speech emotion recognition framework consisting of three modules: i) CNN Block is used to extract local information in": "transformer\nto model\nthe speech sequences and use attention"
        },
        {
          "Fig. 1. An illustration of our proposed speech emotion recognition framework consisting of three modules: i) CNN Block is used to extract local information in": "mechanism to enhance the features in time, space and channel,"
        },
        {
          "Fig. 1. An illustration of our proposed speech emotion recognition framework consisting of three modules: i) CNN Block is used to extract local information in": "based on which our model is designed. As shown in Fig. 1, for"
        },
        {
          "Fig. 1. An illustration of our proposed speech emotion recognition framework consisting of three modules: i) CNN Block is used to extract local information in": "a given input speech sequence, a series of preprocessing steps"
        },
        {
          "Fig. 1. An illustration of our proposed speech emotion recognition framework consisting of three modules: i) CNN Block is used to extract local information in": "are\nperformed. Specifically, we\nuniformly\nprocess\ndifferent"
        },
        {
          "Fig. 1. An illustration of our proposed speech emotion recognition framework consisting of three modules: i) CNN Block is used to extract local information in": "lengths of\nspeech sequences\ninto 1.8s,\nthe longer\nsequences"
        },
        {
          "Fig. 1. An illustration of our proposed speech emotion recognition framework consisting of three modules: i) CNN Block is used to extract local information in": "will be cropped into subsegments, and for shorter sequences,"
        },
        {
          "Fig. 1. An illustration of our proposed speech emotion recognition framework consisting of three modules: i) CNN Block is used to extract local information in": "we process them using loop filling, after which MFCC features"
        },
        {
          "Fig. 1. An illustration of our proposed speech emotion recognition framework consisting of three modules: i) CNN Block is used to extract local information in": "are extracted of\nspeech as\nthe input\nto the model. The local"
        },
        {
          "Fig. 1. An illustration of our proposed speech emotion recognition framework consisting of three modules: i) CNN Block is used to extract local information in": "features of the speech first are obtained by a CNN block, where"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "through BiLSTM, and Space-channel attention which enhances the features from the spatial and channel dimensions.",
          "two modules: Timing attention which enhances the current\nfeatures temporally": ""
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "the\ntemporal-frequency domain. The\nresults\nare\nthen fed to",
          "two modules: Timing attention which enhances the current\nfeatures temporally": "and\nthen\nfeeding\nit\nto\nthe\nbilstm layer. By\nencoding\nlong"
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "successive convolutional\nlayers and maxpooling layers, which",
          "two modules: Timing attention which enhances the current\nfeatures temporally": "distances from front to back and from back to front, bilstm can"
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "are used to further capture the local\nrepresentation in speech,",
          "two modules: Timing attention which enhances the current\nfeatures temporally": "better capture bidirectional\nfeature dependencies. X CAvgP ool"
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "the batch normalization (BN), and relu activation function are",
          "two modules: Timing attention which enhances the current\nfeatures temporally": "is encoded by bilstm as follows:"
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "applied after each convolutional\nlayer.",
          "two modules: Timing attention which enhances the current\nfeatures temporally": "−\n−\n−−−−−−→"
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "",
          "two modules: Timing attention which enhances the current\nfeatures temporally": "→h\nbilstm =\nBILST M (X CAvgP ool)\n(2)"
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "C. T-Sa attention module",
          "two modules: Timing attention which enhances the current\nfeatures temporally": ""
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "",
          "two modules: Timing attention which enhances the current\nfeatures temporally": "←\n←\n−−−−−−−"
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "",
          "two modules: Timing attention which enhances the current\nfeatures temporally": "−h\nbilstm =\nBILST M (X CAvgP ool)\n(3)"
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "In this paper,\ninspired by [43], we adopt a novel attention",
          "two modules: Timing attention which enhances the current\nfeatures temporally": ""
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "module\nto\ncombine\ntemporal\nattention with\nspatial-channel",
          "two modules: Timing attention which enhances the current\nfeatures temporally": ""
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "",
          "two modules: Timing attention which enhances the current\nfeatures temporally": "Two LSTMs\nin\nbilstm process\nthe\nsequence\nforward\nand"
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "attention,\nfocusing on the temporal dynamics of\nspeech fea-",
          "two modules: Timing attention which enhances the current\nfeatures temporally": ""
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "",
          "two modules: Timing attention which enhances the current\nfeatures temporally": "backward,\nrespectively,\nand then concate\nthe outputs of\nthe"
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "tures and spatial-channel\ninformation in the feature map, as",
          "two modules: Timing attention which enhances the current\nfeatures temporally": ""
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "",
          "two modules: Timing attention which enhances the current\nfeatures temporally": "two LSTMs together:"
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "shown in Fig. 2, which is divided into two parts before and",
          "two modules: Timing attention which enhances the current\nfeatures temporally": ""
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "",
          "two modules: Timing attention which enhances the current\nfeatures temporally": "−h\n←\nbilstm(cid:17)"
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "after.\nIn the\nformer part,\nthe\nattention model\nenhances\nthe",
          "two modules: Timing attention which enhances the current\nfeatures temporally": "h bilstm;\nH bilstm = Concatenate\n(4)"
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "temporal\ninformation in the features by Bilstm to model\nthe",
          "two modules: Timing attention which enhances the current\nfeatures temporally": ""
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "current\nfeatures\ntemporally,\nbased\non\nthe\nfact\nthat\nspeech",
          "two modules: Timing attention which enhances the current\nfeatures temporally": "H bilstm is then applied sigmoid activation and multiplied with"
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "information is temporal\ninformation and the order of\nfeatures",
          "two modules: Timing attention which enhances the current\nfeatures temporally": "the\ninput\nfeature map\nusing\nthe\nresidual\nscheme\nto\noutput"
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "in time is of\nimportance. The latter part\nfollows\nthe spatial",
          "two modules: Timing attention which enhances the current\nfeatures temporally": "temporal attention:"
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "and channel attention, which is a common concern in existing",
          "two modules: Timing attention which enhances the current\nfeatures temporally": ""
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "",
          "two modules: Timing attention which enhances the current\nfeatures temporally": "X time = σ (cid:0)H bilstm(cid:1) · X\n(5)"
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "attention, and efficiently combines the attention of\nthe spatial",
          "two modules: Timing attention which enhances the current\nfeatures temporally": ""
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "and channel dimensions of\nspeech feature maps\nthrough the",
          "two modules: Timing attention which enhances the current\nfeatures temporally": ""
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "",
          "two modules: Timing attention which enhances the current\nfeatures temporally": "2)\nSpace-channel attention:\nSpatial attention and channel"
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "Shuffle unit. T-Sa attention module enhances\nthe features\nin",
          "two modules: Timing attention which enhances the current\nfeatures temporally": ""
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "",
          "two modules: Timing attention which enhances the current\nfeatures temporally": "attention are widely used in computer vision. Most methods"
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "the model\nthrough three dimensions and with a small number",
          "two modules: Timing attention which enhances the current\nfeatures temporally": ""
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "",
          "two modules: Timing attention which enhances the current\nfeatures temporally": "transform and aggregate features in these two directions, such"
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "of parameters and achieves better\nresults in SER.",
          "two modules: Timing attention which enhances the current\nfeatures temporally": ""
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "",
          "two modules: Timing attention which enhances the current\nfeatures temporally": "as SE [31], CBAM [32], BAM [44], GCNet\n[45], but\nthese"
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "1) Timing attention: After Pre-processing and CNN Block",
          "two modules: Timing attention which enhances the current\nfeatures temporally": ""
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "",
          "two modules: Timing attention which enhances the current\nfeatures temporally": "methods do not make full use of the correlation between space"
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "for\nfeature extraction of\nspeech, giving the feature map size",
          "two modules: Timing attention which enhances the current\nfeatures temporally": ""
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "",
          "two modules: Timing attention which enhances the current\nfeatures temporally": "and channel which are not\nefficient. Therefore, we\nadopted"
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "of X ∈ RC×H×W for the input T-Sa attention module, where",
          "two modules: Timing attention which enhances the current\nfeatures temporally": ""
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "",
          "two modules: Timing attention which enhances the current\nfeatures temporally": "SA-Net\n[43] as our spatial-channel attention module."
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "C, H and W denote the number of channels,\nspatial height",
          "two modules: Timing attention which enhances the current\nfeatures temporally": ""
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "",
          "two modules: Timing attention which enhances the current\nfeatures temporally": "Given the output X time of\nthe temporal attention module,"
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "and width,\nrespectively. To model\nthe\ntemporal\nattention in",
          "two modules: Timing attention which enhances the current\nfeatures temporally": ""
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "",
          "two modules: Timing attention which enhances the current\nfeatures temporally": "the\ninput\nis\nfirst\ndivided\ninto G groups,\nand\nthe\nsize\nof"
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "speech features, a recurrent neural network is used to model",
          "two modules: Timing attention which enhances the current\nfeatures temporally": ""
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "",
          "two modules: Timing attention which enhances the current\nfeatures temporally": "∈ RC/G×H×W . Then, each\neach sub-feature map is X time′"
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "the speech information which is bilstm. The input of bilstm is",
          "two modules: Timing attention which enhances the current\nfeatures temporally": ""
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "",
          "two modules: Timing attention which enhances the current\nfeatures temporally": "group is split into two sub-branches in the channel dimension.,"
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "a two-dimensional sequence while our speech feature map is",
          "two modules: Timing attention which enhances the current\nfeatures temporally": ""
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "",
          "two modules: Timing attention which enhances the current\nfeatures temporally": "X spatial and X channel, one of which obtains spatial attention"
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "three-dimensional, so what we need is to process the feature",
          "two modules: Timing attention which enhances the current\nfeatures temporally": ""
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "",
          "two modules: Timing attention which enhances the current\nfeatures temporally": "and the other obtains channel attention."
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "map X. If we directly reshape the feature map, the input bilstm",
          "two modules: Timing attention which enhances the current\nfeatures temporally": ""
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "",
          "two modules: Timing attention which enhances the current\nfeatures temporally": "For\nthe channel attention branch, firstly,\nthe global average"
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "will bring a great number of parameters number. Therefore,",
          "two modules: Timing attention which enhances the current\nfeatures temporally": ""
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "",
          "two modules: Timing attention which enhances the current\nfeatures temporally": "pooling\n(GAP)\noperation\nis\nperformed\non\nthe\ninput\nof\nthe"
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "average pooling is used to reduce the dimensionality of\nthe",
          "two modules: Timing attention which enhances the current\nfeatures temporally": ""
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "",
          "two modules: Timing attention which enhances the current\nfeatures temporally": "branch\nto\nembed\nthe\nglobal\ninformation.\nThen,\na\nsimple"
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "channels, and the specific calculation is:",
          "two modules: Timing attention which enhances the current\nfeatures temporally": ""
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "",
          "two modules: Timing attention which enhances the current\nfeatures temporally": "gating mechanism and sigmoid activation are used to perform"
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "",
          "two modules: Timing attention which enhances the current\nfeatures temporally": "adaptive learning of spatial features. A residual scheme is used"
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "1 C\nC(cid:88) i\nX(i)\nX CAvgP ool =\n(1)",
          "two modules: Timing attention which enhances the current\nfeatures temporally": "to multiply the input channel branch feature map. The specific"
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "=1",
          "two modules: Timing attention which enhances the current\nfeatures temporally": "operation of\nthe spatial attention module is as follows:"
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "After\nthe feature passes\nthrough average pooling,\nthe size",
          "two modules: Timing attention which enhances the current\nfeatures temporally": ""
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "",
          "two modules: Timing attention which enhances the current\nfeatures temporally": "X channel′\n(cid:1) · X channel\n(6)\n= σ (cid:0)W1 · GAP (X channel) + b1"
        },
        {
          "Fig. 2.\nAn illustration of our proposed T-Sa attention module consisting of": "of\nthe\nfeature map\nis X CAvgP ool ∈ RH×W . Feature map",
          "two modules: Timing attention which enhances the current\nfeatures temporally": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 3. An illustration of our proposed LCT Block attention module consisting of three modules: i) LLC, which efficiently captures the local information of the": "features through a lightweight convolution module;\nii) CA-LMAM, which captures the long-range dependencies in the features and enhance the time-frequency"
        },
        {
          "Fig. 3. An illustration of our proposed LCT Block attention module consisting of three modules: i) LLC, which efficiently captures the local information of the": "domain features of speech through a CA module;\niii) SE-IBFFN, which introduces a nonlinear part\nthrough a feedforward network with inverse residuals to"
        },
        {
          "Fig. 3. An illustration of our proposed LCT Block attention module consisting of three modules: i) LLC, which efficiently captures the local information of the": "enhance the model\nthe expressiveness of\nthe model."
        },
        {
          "Fig. 3. An illustration of our proposed LCT Block attention module consisting of three modules: i) LLC, which efficiently captures the local information of the": "attention mechanism (CA-LMAM) and SE Inverted Bottleneck\nFor\nthe\nspatial\nattention\nbranch, GN [46]\noperation\nis"
        },
        {
          "Fig. 3. An illustration of our proposed LCT Block attention module consisting of three modules: i) LLC, which efficiently captures the local information of the": "feed forward network (SE-IBFFN). Through the combination\nfirst performed on the input of\nthe branch to embed spatial"
        },
        {
          "Fig. 3. An illustration of our proposed LCT Block attention module consisting of three modules: i) LLC, which efficiently captures the local information of the": "of\nconvolution and transformer, LCT Block obtains\nthe\nin-\nstatistical\ninformation,\nand then a\nsimple gating mechanism"
        },
        {
          "Fig. 3. An illustration of our proposed LCT Block attention module consisting of three modules: i) LLC, which efficiently captures the local information of the": "formation of\nfeatures from the local and the whole, which is\nand sigmoid activation are used to perform adaptive learning"
        },
        {
          "Fig. 3. An illustration of our proposed LCT Block attention module consisting of three modules: i) LLC, which efficiently captures the local information of the": "more efficient and has\nfewer parameters\nthan the traditional\nof features on the channel, and the residual scheme is used to"
        },
        {
          "Fig. 3. An illustration of our proposed LCT Block attention module consisting of three modules: i) LLC, which efficiently captures the local information of the": "transformer. LLC is a lightweight convolution module, which\nmultiply the input channel branch feature map. The specific"
        },
        {
          "Fig. 3. An illustration of our proposed LCT Block attention module consisting of three modules: i) LLC, which efficiently captures the local information of the": "efficiently\nobtains\nlocal\ninformation\nof\nfeatures. The CA-\noperation of\nthe channel attention module is as follows:"
        },
        {
          "Fig. 3. An illustration of our proposed LCT Block attention module consisting of three modules: i) LLC, which efficiently captures the local information of the": "LMAM module\ncan capture\nthe\nlong-distance dependencies"
        },
        {
          "Fig. 3. An illustration of our proposed LCT Block attention module consisting of three modules: i) LLC, which efficiently captures the local information of the": "X spatial′\n(cid:1) · X spatial\n(7)\n= σ (cid:0)W2 · GN (X spatial) + b2"
        },
        {
          "Fig. 3. An illustration of our proposed LCT Block attention module consisting of three modules: i) LLC, which efficiently captures the local information of the": "of features and enhance the information in the time-frequency"
        },
        {
          "Fig. 3. An illustration of our proposed LCT Block attention module consisting of three modules: i) LLC, which efficiently captures the local information of the": "domain through the Coordinate Attention (CA)\n[49] module."
        },
        {
          "Fig. 3. An illustration of our proposed LCT Block attention module consisting of three modules: i) LLC, which efficiently captures the local information of the": "Both W2 and b2 are learnable parameters, and then the outputs"
        },
        {
          "Fig. 3. An illustration of our proposed LCT Block attention module consisting of three modules: i) LLC, which efficiently captures the local information of the": "SE-IBFFN introduces a nonlinear part\nthrough a feedforward"
        },
        {
          "Fig. 3. An illustration of our proposed LCT Block attention module consisting of three modules: i) LLC, which efficiently captures the local information of the": "of\nthe two branches are merged by concatenating them along"
        },
        {
          "Fig. 3. An illustration of our proposed LCT Block attention module consisting of three modules: i) LLC, which efficiently captures the local information of the": "network with inverted residuals, which enhances\nthe perfor-"
        },
        {
          "Fig. 3. An illustration of our proposed LCT Block attention module consisting of three modules: i) LLC, which efficiently captures the local information of the": "the channel dimension:"
        },
        {
          "Fig. 3. An illustration of our proposed LCT Block attention module consisting of three modules: i) LLC, which efficiently captures the local information of the": "mance of\nthe model and can further capture the local\ninfor-"
        },
        {
          "Fig. 3. An illustration of our proposed LCT Block attention module consisting of three modules: i) LLC, which efficiently captures the local information of the": "X attention = Concatenate\nX channel′\n; X spatial′(cid:17)\n(8)"
        },
        {
          "Fig. 3. An illustration of our proposed LCT Block attention module consisting of three modules: i) LLC, which efficiently captures the local information of the": "mation of features. These three modules will be introduced in"
        },
        {
          "Fig. 3. An illustration of our proposed LCT Block attention module consisting of three modules: i) LLC, which efficiently captures the local information of the": "detail below"
        },
        {
          "Fig. 3. An illustration of our proposed LCT Block attention module consisting of three modules: i) LLC, which efficiently captures the local information of the": "The attention weights on space and channel are learned sep-"
        },
        {
          "Fig. 3. An illustration of our proposed LCT Block attention module consisting of three modules: i) LLC, which efficiently captures the local information of the": "1) LLC:\nIn order\nto make up for\nthe lack of\nlocal\ninfor-"
        },
        {
          "Fig. 3. An illustration of our proposed LCT Block attention module consisting of three modules: i) LLC, which efficiently captures the local information of the": "arately through two branches, and the corresponding residual"
        },
        {
          "Fig. 3. An illustration of our proposed LCT Block attention module consisting of three modules: i) LLC, which efficiently captures the local information of the": "mation in trasnformer, a convolution module is used to obtain"
        },
        {
          "Fig. 3. An illustration of our proposed LCT Block attention module consisting of three modules: i) LLC, which efficiently captures the local information of the": "scheme is multiplied with the respective input\nfeature map to"
        },
        {
          "Fig. 3. An illustration of our proposed LCT Block attention module consisting of three modules: i) LLC, which efficiently captures the local information of the": "local\ninformation in speech which called LLC. Some work"
        },
        {
          "Fig. 3. An illustration of our proposed LCT Block attention module consisting of three modules: i) LLC, which efficiently captures the local information of the": "enhance the representations of space and channel. Finally,\nthe"
        },
        {
          "Fig. 3. An illustration of our proposed LCT Block attention module consisting of three modules: i) LLC, which efficiently captures the local information of the": "such as CMT [48] also adopted a similar architecture, but\nthe"
        },
        {
          "Fig. 3. An illustration of our proposed LCT Block attention module consisting of three modules: i) LLC, which efficiently captures the local information of the": "channel shuffle [47]\nis employed to facilitate communication"
        },
        {
          "Fig. 3. An illustration of our proposed LCT Block attention module consisting of three modules: i) LLC, which efficiently captures the local information of the": "convolution module in CMT is\nrelatively simple, only using"
        },
        {
          "Fig. 3. An illustration of our proposed LCT Block attention module consisting of three modules: i) LLC, which efficiently captures the local information of the": "of\ninformation\nbetween\ndifferent\ngroups\nalong\nthe\nchannel"
        },
        {
          "Fig. 3. An illustration of our proposed LCT Block attention module consisting of three modules: i) LLC, which efficiently captures the local information of the": "a Depthwise convolution with residuals, which can not obtain"
        },
        {
          "Fig. 3. An illustration of our proposed LCT Block attention module consisting of three modules: i) LLC, which efficiently captures the local information of the": "dimension. The\ninformation of\nthe\nfeatures\ninteracts\nin the"
        },
        {
          "Fig. 3. An illustration of our proposed LCT Block attention module consisting of three modules: i) LLC, which efficiently captures the local information of the": "effective local\ninformation.\nInspired by [50], we used a large"
        },
        {
          "Fig. 3. An illustration of our proposed LCT Block attention module consisting of three modules: i) LLC, which efficiently captures the local information of the": "channel dimension, and the size of\nthe output\nfeature map is"
        },
        {
          "Fig. 3. An illustration of our proposed LCT Block attention module consisting of three modules: i) LLC, which efficiently captures the local information of the": "convolution kernel Depthwise convolution with residuals and"
        },
        {
          "Fig. 3. An illustration of our proposed LCT Block attention module consisting of three modules: i) LLC, which efficiently captures the local information of the": "the same as the initial\ninput."
        },
        {
          "Fig. 3. An illustration of our proposed LCT Block attention module consisting of three modules: i) LLC, which efficiently captures the local information of the": "a Pointwise convolution in the local feature extraction module,"
        },
        {
          "Fig. 3. An illustration of our proposed LCT Block attention module consisting of three modules: i) LLC, which efficiently captures the local information of the": "given an input\nfeature X ∈ RC×H×W as follows:"
        },
        {
          "Fig. 3. An illustration of our proposed LCT Block attention module consisting of three modules: i) LLC, which efficiently captures the local information of the": "D. LCT Block"
        },
        {
          "Fig. 3. An illustration of our proposed LCT Block attention module consisting of three modules: i) LLC, which efficiently captures the local information of the": "LLC (X) = P W Conv (DW Conv (X) + X)\n(9)"
        },
        {
          "Fig. 3. An illustration of our proposed LCT Block attention module consisting of three modules: i) LLC, which efficiently captures the local information of the": "Inspired by [48], we proposed an LCT Block shown in Fig."
        },
        {
          "Fig. 3. An illustration of our proposed LCT Block attention module consisting of three modules: i) LLC, which efficiently captures the local information of the": "The activation layer and batch normalization are omitted, and\n3, which\nconsists\nof\nthree modules:\nlightweight\nlocal\ncon-"
        },
        {
          "Fig. 3. An illustration of our proposed LCT Block attention module consisting of three modules: i) LLC, which efficiently captures the local information of the": "the same omission will be in subsequent formulas. P W Conv\nvolution\n(LLC),\ncoordinate\nattention-lightweight multi-head"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "represents Pointwise\nconvolution\nand DW Conv\nrepresents",
          "7": "RC/r×1×W . The extraction of\nfeatures\nis\nthen performed by"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Depthwise.\nIn P W Conv we used a large 7 × 7 convolution",
          "7": "two separate convolutions, followed by activation using the σ"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "kernel\nis used to get\na\nlarger\nreceptive field than a 3 × 3",
          "7": "activation function, and then multiplied by the initial output"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "convolution kernel. In addition, P W Conv has smaller param-",
          "7": "to learn the more critical\ninformation in the feature:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "eters than DW Conv and ordinary convolution. Additionally,",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "st = σ (cid:0)Conv (cid:0)X t(cid:1)(cid:1)\n(15)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "a\nresidual\nstructure\nis\nincorporated\nto\naddress\nthe\nissue\nof",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "gradient dispersion.",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "2) CA-LMAM:\nIn transformer, multi-head attention is usu-",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "sf = σ (cid:0)Conv (cid:0)X f (cid:1)(cid:1)\n(16)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ally used to make the model pay more attention to the more",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "noteworthy part of\nitself, which can obtain long-distance de-",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "pendence in speech features. Given the output X ∈ RC×H×W",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "Q = X · st · sf\n(17)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "in the LLC module,\nthe input of multi-head attention is query",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Q, key K, and value V ,\nrespectively.\nIf\nthe original\nfeatures",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "We adjust\nthe dimension of Q, eventually Q′ ∈ RN ×C, where"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "are directly input\ninto multi-head attention,\nit will often bring",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "N = H × W ."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "a large amount of calculation. The amount of calculation is",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "Eventually\nlearning\ninformation\nabout\nthe model\nitself"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "2 × 2\nrelated\nto\nthe\nsize\nof\nthe\ninput\nfeatures. Therefore,",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "through multi-headed self-attention:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "DW Conv is used to downsample the parts of K and V , as",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "(cid:19)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "shown below:",
          "7": "(cid:18) Q′K ′T"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "√\n+ B\nV ′\nLMAM(Q, K, V ) = Softmax\n(18)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "K = DW Conv (X)\n(10)",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "C"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "Where B is\na\nlearnable parameter,\nrepresenting the\nrelative"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "V = DW Conv (X)\n(11)",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "position coding of multi-head self-attention, which is used to"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "characterize the relative position relationship between tokens."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "2 × W\n2 × W\n, V ∈ RC× H\n,\nthen the H and W\nWhere K ∈ RC× H",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "It is more flexible than the traditional absolute position coding,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "dimensions are merged and input a linear\nlayer\nrespectively.",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "making transformer better model the relative position informa-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Finally get K ′ ∈ RN ′×C, V ′ ∈ RN ′×C, where N ′ = H",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "2 × W\n2 ,",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "tion of speech features."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "C is the dimension of\nthe linear\nlayer output.",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "3)\nSE-IBFFN:\nIn the original transformer, FFN is generally"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "For Q, a CA module is used to enhance the time-frequency",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "composed of two linear layers. In this paper,\ninspired by [51],"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "domain representation of speech features. This attention mech-",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "a series of Pointwise convolution and Depthwise convolution"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "anism can obtain long-distance feature dependence along one",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "make\nup\nour\nSE-IBFFN. Compared with\nthe Transformer"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "direction and spatial dependence information in another direc-",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "model traditionally composed of linear layers, this module can"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "tion. However,\nin speech,\nspeech features have more special",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "capture local\ninformation while learning channel\ninformation,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "significance in the spatial dimension. The abscissa of speech",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "and\nhas\na\nsmaller\nnumber\nof\nconvolution\nparameters\nthan"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "features\nis\nthe\ntime\naxis, which represents\nthe\ntime domain",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "ordinary ones."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "information of speech while the ordinate of the speech feature",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "The structure of SE-IBFFN is\nshown in Fig. 3. Given the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "represents\nthe\nfrequency information of\nthe\nspeech,\nso CA",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "input X ∈ RC×H×W ,\nthe specific calculation process\nis as"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "can better\naggregate\nthe dependent\ninformation in the\ntime",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "follows:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "domain and frequency domain for\nspeech features, which is",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "more suitable for SER tasks.",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "X conv = P W Conv (DW Conv (P W Conv (X)))\n(19)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "The\nspecific operation of CA is\nshown in Fig. 3. Given",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the input X ∈ RC×H×W , pooling is performed in the time",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "(20)\nSE-IBF F N (X) = SE (X conv) + X"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "domain and frequency domain respectively:",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "Firstly,\nthe Pointwise convolution operation is performed on"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "the input, and the number of channels is expanded to 4 times of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "1 W\nW(cid:88) i\nX T AvgP ool =\nX(i)\n(12)",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "=1",
          "7": "the original\nto increase the feature size of\nthe channels. Then"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "a 7 × 7 large convolution kernel DW Conv is used to obtain"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "the local\ninformation in the feature, and the large convolution"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "1 H\nH(cid:88) j\nX F AvgP ool =\nX(j)\n(13)",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "kernel can provide a larger\nreceptive field without\nincreasing"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "=1",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "too many parameters. The feature map is then restored to its"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "X T AvgP ool\n∈ RC×H×1\nand X F AvgP ool\n∈ RC×1×W are",
          "7": "original size using P W Conv."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the feature maps after aggregation in two directions of\ntime-",
          "7": "Then, the SE module is to obtain the attention of the channel"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "frequency domain. Then,\nthey concatenate and perform con-",
          "7": "dimension of the feature map. Given the input X ∈ RC×H×W"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "volution operations to encode the information:",
          "7": "of SE,\nthe specific calculation process is as follows:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "X tf = Conv (cid:0)concatenate (cid:0)X T AvgP ool; X F AvgP ool(cid:1)(cid:1)",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "SE (X) = σ (GAP (F C (F C (X)))) · X\n(21)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "(14)",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Where X tf ∈ RC/r×1×(W +H), r is the reduction rate of\nthe",
          "7": "Compared with [51], we put\nthe SE module after P W Conv,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "channel, and then separated into two separate features X t, X f",
          "7": "which makes the parameters of\nthe model smaller. A residual"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "along the\nspatial direction, where X t ∈ RC/r×H×1, X f ∈",
          "7": "structure is used to solve the problem of gradient dispersion."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "EXPERIMENTS SETUP\nIV.",
          "8": "interpolation, and effectively smoothes the discrete data space"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "into continuous space. In our proposed model, we set α to 0.2"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "In this section, we will introduce the dataset and experimen-",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "for best performance."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "tal details used in our study, as well as the evaluation metrics",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "used to assess the performance of our algorithm.",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "C. Evaluation Metrics"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "A. Corpus Description",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "In this\nsection, we’ll describe in detail\nthe criteria we use"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "To verify the performance of our proposed algorithm, per-",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "to evaluate\nthe performance of our\nalgorithms. For various"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "formance tests on two benchmark databases are conducted, on",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "categories of performance in the dataset, Precision, Recall, and"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "which we will evaluate our algorithm.",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "F1-score are the general metrics to measure their performance."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Actually,\nInteractive\nEmotional Dyadic Motion Capture",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "First of\nall,\nfour\nconcepts will be\nintroduced: True Positive"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "(IEMOCAP)\n[52]\nis\nan action, multimodal,\nand multimodal",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "(TP),\nFalse\nPositive\n(FP), True Negative\n(TN),\nand\nFalse"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "database\nthat\ncontains\ndata\nfrom 10\nactors\nand\nactresses",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "Negative (FN), where TP means actual positive and predicted"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "during\nan\nemotional\nbinary\ninteraction, with\ntwo\nspeakers",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "positive, FP means actual positive and predicted negative, TN"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "(one male\nand\none\nfemale)\nspeaking\nin\neach\nsession. The",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "means actual negative and predicted positive, and FN means"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "IEMOCAP database has been annotated by several annotators",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "actual negative and predicted negative. The Precision, Recall,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "with categorical\nlabels and dimensional\nlabels. The database",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "and F1-score can be expressed as:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "combines discrete and dimensional\nsentiment models.\nIn our",
          "8": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "A. Comparison with State-of-the-Art",
          "9": "local-global model have significant advantages over traditional"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "attention and CNN-based models [58]\n[63]\n[55].\nIn addition,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "To\ncompare with\nour\nproposed model, we\nevaluate\nthe",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "Emo-DB dataset is a smaller dataset. Compared with non-deep"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "performance of the algorithm from the WA and UA perspective",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "learning feature extraction and feature learning methods [62]"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "with a series of existing methods in IEMOCAP and Emo-DB",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "[64], our overall end-to-end network based on deep learning"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "datasets, as shown in Table I and Table II. The proposed model",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "also has a relatively better performance on this small dataset,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "is\ncompared with\nseveral\ncommonly\nused\nspeech\nemotion",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "indicating that our method still has\nexcellent\nrobustness on"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "recognition models,\nincluding algorithms based on the combi-",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "small datasets."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "nation of CNN and transformer\n[19], CNN-based algorithms",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "In\nsummary,\ncombining\nthe\nperformance\nof\nthese\ntwo"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[14]\n[55], LSTM-based algorithms\n[56]\n[57], attention-based",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "datasets proves the superiority of our proposed method."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "methods [21]\n[58], and some other algorithms.",
          "9": ""
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE I": ""
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "Comparative Methods"
        },
        {
          "TABLE I": "Latif et al.\n[59]"
        },
        {
          "TABLE I": "Hu et al.\n[19]"
        },
        {
          "TABLE I": "Guo et al.\n[21]"
        },
        {
          "TABLE I": "Gao et al.\n[60]"
        },
        {
          "TABLE I": "Wang et al.\n[57]"
        },
        {
          "TABLE I": "Latif et al.\n[56]"
        },
        {
          "TABLE I": "Liu et al.\n[14]"
        },
        {
          "TABLE I": "Dai et al.\n[61]"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "Proposed"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Liu et al.\n[14]": "Dai et al.\n[61]",
          "70.34": "65.40",
          "70.78": "66.90"
        },
        {
          "Liu et al.\n[14]": "",
          "70.34": "",
          "70.78": ""
        },
        {
          "Liu et al.\n[14]": "Proposed",
          "70.34": "71.64",
          "70.78": "72.72"
        },
        {
          "Liu et al.\n[14]": "",
          "70.34": "",
          "70.78": ""
        },
        {
          "Liu et al.\n[14]": "",
          "70.34": "",
          "70.78": ""
        },
        {
          "Liu et al.\n[14]": "TABLE II",
          "70.34": "",
          "70.78": ""
        },
        {
          "Liu et al.\n[14]": "",
          "70.34": "COMPARISON ON EMO-DB",
          "70.78": ""
        },
        {
          "Liu et al.\n[14]": "",
          "70.34": "",
          "70.78": ""
        },
        {
          "Liu et al.\n[14]": "Comparative Methods",
          "70.34": "WA",
          "70.78": "UA"
        },
        {
          "Liu et al.\n[14]": "Tuncer et al.\n[62]",
          "70.34": "90.09",
          "70.78": "89.47"
        },
        {
          "Liu et al.\n[14]": "Li et al.\n[58]",
          "70.34": "83.30",
          "70.78": "82.10"
        },
        {
          "Liu et al.\n[14]": "Zhong et al.\n[63]",
          "70.34": "85.76",
          "70.78": "86.12"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Kerkeni et al.": "Suganya et al.",
          "[64]": "[55]",
          "-": "-",
          "86.22": "85.62"
        },
        {
          "Kerkeni et al.": "",
          "[64]": "",
          "-": "",
          "86.22": ""
        },
        {
          "Kerkeni et al.": "Proposed",
          "[64]": "",
          "-": "90.65",
          "86.22": "89.51"
        },
        {
          "Kerkeni et al.": "performance",
          "[64]": "of",
          "-": "our",
          "86.22": "proposed method, we"
        },
        {
          "Kerkeni et al.": "",
          "[64]": "",
          "-": "",
          "86.22": ""
        },
        {
          "Kerkeni et al.": "",
          "[64]": "",
          "-": "",
          "86.22": ""
        },
        {
          "Kerkeni et al.": "",
          "[64]": "",
          "-": "",
          "86.22": ""
        },
        {
          "Kerkeni et al.": "",
          "[64]": "",
          "-": "",
          "86.22": ""
        },
        {
          "Kerkeni et al.": "",
          "[64]": "",
          "-": "",
          "86.22": ""
        },
        {
          "Kerkeni et al.": "",
          "[64]": "other methods.",
          "-": "In",
          "86.22": "addition,"
        },
        {
          "Kerkeni et al.": "",
          "[64]": "",
          "-": "",
          "86.22": ""
        },
        {
          "Kerkeni et al.": "splicing of",
          "[64]": "",
          "-": "transformer",
          "86.22": ""
        },
        {
          "Kerkeni et al.": "",
          "[64]": "",
          "-": "",
          "86.22": ""
        },
        {
          "Kerkeni et al.": "",
          "[64]": "",
          "-": "",
          "86.22": ""
        },
        {
          "Kerkeni et al.": "",
          "[64]": "",
          "-": "",
          "86.22": ""
        },
        {
          "Kerkeni et al.": "global\nfeature",
          "[64]": "",
          "-": "extraction. Compared",
          "86.22": ""
        },
        {
          "Kerkeni et al.": "",
          "[64]": "",
          "-": "",
          "86.22": ""
        },
        {
          "Kerkeni et al.": "and channel",
          "[64]": "",
          "-": "attention [21],",
          "86.22": "temporal"
        },
        {
          "Kerkeni et al.": "it more",
          "[64]": "",
          "-": "competitive.",
          "86.22": "In addition,"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "5,\nour\nproposed model\nalso\nachieved\ngood\nresults\non\nthis": "dataset, and has good accuracy for seven emotions. The three",
          "LCT module and preserves the rest.": "• W/o SE: This module removes the SE Attention part of"
        },
        {
          "5,\nour\nproposed model\nalso\nachieved\ngood\nresults\non\nthis": "emotions achieved 100% accuracy, and Angry also achieved",
          "LCT module and preserves the rest.": "our LCT module, and the other parts are retained."
        },
        {
          "5,\nour\nproposed model\nalso\nachieved\ngood\nresults\non\nthis": "96% accuracy. In addition, the wrong judgment in happiness is",
          "LCT module and preserves the rest.": ""
        },
        {
          "5,\nour\nproposed model\nalso\nachieved\ngood\nresults\non\nthis": "",
          "LCT module and preserves the rest.": "It\ncan be\nseen from the\ntable\nthat when using the T-Sa"
        },
        {
          "5,\nour\nproposed model\nalso\nachieved\ngood\nresults\non\nthis": "anger. We believe that\nthis is because these two emotions have",
          "LCT module and preserves the rest.": ""
        },
        {
          "5,\nour\nproposed model\nalso\nachieved\ngood\nresults\non\nthis": "",
          "LCT module and preserves the rest.": "module, our method has\nan absolute\nimprovement of 1.2%"
        },
        {
          "5,\nour\nproposed model\nalso\nachieved\ngood\nresults\non\nthis": "similar arousal, while the wrong judgment\nin disgust\nis fear.",
          "LCT module and preserves the rest.": ""
        },
        {
          "5,\nour\nproposed model\nalso\nachieved\ngood\nresults\non\nthis": "",
          "LCT module and preserves the rest.": "and\n1.54% in WA and UA,\nindicating\nthat\nour\nattention"
        },
        {
          "5,\nour\nproposed model\nalso\nachieved\ngood\nresults\non\nthis": "Finally, the accuracy of fear is lower than that of other emotion",
          "LCT module and preserves the rest.": ""
        },
        {
          "5,\nour\nproposed model\nalso\nachieved\ngood\nresults\non\nthis": "",
          "LCT module and preserves the rest.": "mechanism module\nhas\na\nvery\nsignificant\neffect\nand\ncan"
        },
        {
          "5,\nour\nproposed model\nalso\nachieved\ngood\nresults\non\nthis": "categories, but\nit also achieves relatively good results. In some",
          "LCT module and preserves the rest.": ""
        },
        {
          "5,\nour\nproposed model\nalso\nachieved\ngood\nresults\non\nthis": "",
          "LCT module and preserves the rest.": "aggregate\nthe noteworthy parts of\nthe\nfeatures.\nIn addition,"
        },
        {
          "5,\nour\nproposed model\nalso\nachieved\ngood\nresults\non\nthis": "cases,\nfear\nis easily misjudged as\nsad, angry and happy,\nthis",
          "LCT module and preserves the rest.": ""
        },
        {
          "5,\nour\nproposed model\nalso\nachieved\ngood\nresults\non\nthis": "",
          "LCT module and preserves the rest.": "when the temporal attention is\nremoved,\nthe model effect\nis"
        },
        {
          "5,\nour\nproposed model\nalso\nachieved\ngood\nresults\non\nthis": "is because these four emotions have a high degree of arousal.",
          "LCT module and preserves the rest.": ""
        },
        {
          "5,\nour\nproposed model\nalso\nachieved\ngood\nresults\non\nthis": "",
          "LCT module and preserves the rest.": "also greatly reduced,\nindicating that\ntemporal attention plays"
        },
        {
          "5,\nour\nproposed model\nalso\nachieved\ngood\nresults\non\nthis": "",
          "LCT module and preserves the rest.": "a non-important\nrole\nin our model\nto enhance\nthe\ntemporal"
        },
        {
          "5,\nour\nproposed model\nalso\nachieved\ngood\nresults\non\nthis": "TABLE V",
          "LCT module and preserves the rest.": ""
        },
        {
          "5,\nour\nproposed model\nalso\nachieved\ngood\nresults\non\nthis": "",
          "LCT module and preserves the rest.": "information\nin\nspeech. We\nalso\ndirectly\nremoved\nthe LCT"
        },
        {
          "5,\nour\nproposed model\nalso\nachieved\ngood\nresults\non\nthis": "PERFORMANCE WITH DIFFERENT EXPERIMENTAL SETTINGS",
          "LCT module and preserves the rest.": ""
        },
        {
          "5,\nour\nproposed model\nalso\nachieved\ngood\nresults\non\nthis": "",
          "LCT module and preserves the rest.": "module, which caused the performance of\nthe model\nto be"
        },
        {
          "5,\nour\nproposed model\nalso\nachieved\ngood\nresults\non\nthis": "",
          "LCT module and preserves the rest.": "reduced by 4.52% and 3.79% on WA and UA, it clearly shows"
        },
        {
          "5,\nour\nproposed model\nalso\nachieved\ngood\nresults\non\nthis": "Models\nWA\nUA",
          "LCT module and preserves the rest.": ""
        },
        {
          "5,\nour\nproposed model\nalso\nachieved\ngood\nresults\non\nthis": "",
          "LCT module and preserves the rest.": "the importance of introducing global information into our LCT"
        },
        {
          "5,\nour\nproposed model\nalso\nachieved\ngood\nresults\non\nthis": "W/o T-Sa\n69.92\n71.18",
          "LCT module and preserves the rest.": ""
        },
        {
          "5,\nour\nproposed model\nalso\nachieved\ngood\nresults\non\nthis": "",
          "LCT module and preserves the rest.": "module."
        },
        {
          "5,\nour\nproposed model\nalso\nachieved\ngood\nresults\non\nthis": "W/o lstm attention\n67.84\n69.62",
          "LCT module and preserves the rest.": ""
        },
        {
          "5,\nour\nproposed model\nalso\nachieved\ngood\nresults\non\nthis": "",
          "LCT module and preserves the rest.": "For our LCT part, our\nexperiments\nalso verified the\nrole"
        },
        {
          "5,\nour\nproposed model\nalso\nachieved\ngood\nresults\non\nthis": "W/o LCT\n67.12\n68.93",
          "LCT module and preserves the rest.": ""
        },
        {
          "5,\nour\nproposed model\nalso\nachieved\ngood\nresults\non\nthis": "",
          "LCT module and preserves the rest.": "of different modules within the LCT. Firstly,\nthe LLC part\nis"
        },
        {
          "5,\nour\nproposed model\nalso\nachieved\ngood\nresults\non\nthis": "W/ Conv-LCT\n68.29\n69.98",
          "LCT module and preserves the rest.": ""
        },
        {
          "5,\nour\nproposed model\nalso\nachieved\ngood\nresults\non\nthis": "",
          "LCT module and preserves the rest.": "replaced with a 3 × 3 ordinary convolution, which reduces the"
        },
        {
          "5,\nour\nproposed model\nalso\nachieved\ngood\nresults\non\nthis": "W/o CA\n69.29\n70.12",
          "LCT module and preserves the rest.": ""
        },
        {
          "5,\nour\nproposed model\nalso\nachieved\ngood\nresults\non\nthis": "",
          "LCT module and preserves the rest.": "performance of\nthe model by 3.35% and 2.74% on WA and"
        },
        {
          "5,\nour\nproposed model\nalso\nachieved\ngood\nresults\non\nthis": "W/o SE\n69.92\n71.31",
          "LCT module and preserves the rest.": ""
        },
        {
          "5,\nour\nproposed model\nalso\nachieved\ngood\nresults\non\nthis": "",
          "LCT module and preserves the rest.": "UA, and the number of parameters has also been improved."
        },
        {
          "5,\nour\nproposed model\nalso\nachieved\ngood\nresults\non\nthis": "71.64\n72.72\nProposed",
          "LCT module and preserves the rest.": ""
        },
        {
          "5,\nour\nproposed model\nalso\nachieved\ngood\nresults\non\nthis": "",
          "LCT module and preserves the rest.": "This shows that\nthe wider receptive field brought by our large"
        },
        {
          "5,\nour\nproposed model\nalso\nachieved\ngood\nresults\non\nthis": "",
          "LCT module and preserves the rest.": "convolution kernel LLC module is very important, and it does"
        },
        {
          "5,\nour\nproposed model\nalso\nachieved\ngood\nresults\non\nthis": "",
          "LCT module and preserves the rest.": "not bring a larger number of parameters.\nIn order\nto verify"
        },
        {
          "5,\nour\nproposed model\nalso\nachieved\ngood\nresults\non\nthis": "C. Ablation Study",
          "LCT module and preserves the rest.": ""
        },
        {
          "5,\nour\nproposed model\nalso\nachieved\ngood\nresults\non\nthis": "",
          "LCT module and preserves the rest.": "the\nrole\nof\nour CA module, we\nremoved\nthe CA module,"
        },
        {
          "5,\nour\nproposed model\nalso\nachieved\ngood\nresults\non\nthis": "To explore\nthe\nrole of\neach part of our proposed model,",
          "LCT module and preserves the rest.": ""
        },
        {
          "5,\nour\nproposed model\nalso\nachieved\ngood\nresults\non\nthis": "",
          "LCT module and preserves the rest.": "which\ncaused\nthe\nperformance\nof\nthe model\nto\ndecrease"
        },
        {
          "5,\nour\nproposed model\nalso\nachieved\ngood\nresults\non\nthis": "Table V shows the results of a series of ablation experiments.",
          "LCT module and preserves the rest.": ""
        },
        {
          "5,\nour\nproposed model\nalso\nachieved\ngood\nresults\non\nthis": "",
          "LCT module and preserves the rest.": "by\n2.35% and\n2.6% on WA and UA,\nindicating\nthat\nthe"
        },
        {
          "5,\nour\nproposed model\nalso\nachieved\ngood\nresults\non\nthis": "The following are the modules for comparison.",
          "LCT module and preserves the rest.": ""
        },
        {
          "5,\nour\nproposed model\nalso\nachieved\ngood\nresults\non\nthis": "",
          "LCT module and preserves the rest.": "time-frequency domain representation of\nthe speech features"
        },
        {
          "5,\nour\nproposed model\nalso\nachieved\ngood\nresults\non\nthis": "• W/o T-Sa: This module removes\nthe T-Sa module from",
          "LCT module and preserves the rest.": "enhanced by the CA module is of vital\nimportance. Finally,"
        },
        {
          "5,\nour\nproposed model\nalso\nachieved\ngood\nresults\non\nthis": "our model, using only the CNN Block and LCT Block",
          "LCT module and preserves the rest.": "the SE Attention part\nin our SE-IBFFN is\nremoved, which"
        },
        {
          "5,\nour\nproposed model\nalso\nachieved\ngood\nresults\non\nthis": "sections.",
          "LCT module and preserves the rest.": "reduces\nthe performance of\nthe model by 1.72% and 1.41%"
        },
        {
          "5,\nour\nproposed model\nalso\nachieved\ngood\nresults\non\nthis": "• W/o lstm attention: This module removes\nthe temporal",
          "LCT module and preserves the rest.": "on WA and UA.\nIt also proves\nthat using the SE module to"
        },
        {
          "5,\nour\nproposed model\nalso\nachieved\ngood\nresults\non\nthis": "attention lstm attention part\nin our T-Sa module, and the",
          "LCT module and preserves the rest.": "obtain the attention of the feature map channel dimension can"
        },
        {
          "5,\nour\nproposed model\nalso\nachieved\ngood\nresults\non\nthis": "other parts are retained.",
          "LCT module and preserves the rest.": "improve the performance of the model\nto recognize emotions."
        },
        {
          "5,\nour\nproposed model\nalso\nachieved\ngood\nresults\non\nthis": "• W/o LCT: This module removes the LCT module in our",
          "LCT module and preserves the rest.": ""
        },
        {
          "5,\nour\nproposed model\nalso\nachieved\ngood\nresults\non\nthis": "model and only uses the CNN Block and T-Sa parts.",
          "LCT module and preserves the rest.": ""
        },
        {
          "5,\nour\nproposed model\nalso\nachieved\ngood\nresults\non\nthis": "",
          "LCT module and preserves the rest.": "D. Model Efficiency Analysis"
        },
        {
          "5,\nour\nproposed model\nalso\nachieved\ngood\nresults\non\nthis": "• W/ Conv-LCT: This module replaces the LLC module in",
          "LCT module and preserves the rest.": ""
        },
        {
          "5,\nour\nproposed model\nalso\nachieved\ngood\nresults\non\nthis": "our LCT module with a 3 × 3 convolution, and the rest",
          "LCT module and preserves the rest.": "In order\nto explore the size and efficiency of\nthe proposed"
        },
        {
          "5,\nour\nproposed model\nalso\nachieved\ngood\nresults\non\nthis": "is preserved.",
          "LCT module and preserves the rest.": "model, Table VI\npresents\na\ncomparison\nof\nthe\nparameter"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "11": "number of parameters and a decrease in accuracy compared"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "11": "to LCT, with a decrease of 2.23% and 2.29%, respectively. The"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "11": "entire LCT module was also replaced with other\ntransformer"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Models",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "11": "models,\nincluding the MobileVit series which combines CNN"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "W/ Conv-LCT",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "11": "and Vit and is a lightweight transformer model. The MobileVit"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "W/ConvFFN-LCT-L",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "11": "block in each series was used to replace LCT, and the results"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "W/ConvFFN-LCT-S",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "11": "showed that\nthe parameter\nsize of\nthe MobileVitv3 block is"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "W/Transformer",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "11": "smaller\nthan LCT, but\nthere\nis\nstill\na gap in accuracy. The"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "W/MobileVitv1 block-depth2 [66]",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "11": "performance of the two-layer MobileVitv1 block was the best,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "W/MobileVitv1 block-depth3 [66]",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "11": "but\nit\nstill did not\nreach the accuracy of LCT. Additionally,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "W/MobileVitv2 block-depth2 [67]",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "11": "we found that models with a depth of 2 performed better than"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "W/MobileVitv2 block-depth3 [67]",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "11": "those with a depth of 3, due to the difficulty of fitting to small"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "W/MobileVitv3 block-depth2 [68]",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "11": "SER datasets as the number of parameters increases."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "W/MobileVitv3 block-depth3 [68]",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Proposed",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "11": "VI. CONCLUSION"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "SER datasets as the number of parameters increases.": ""
        },
        {
          "SER datasets as the number of parameters increases.": ""
        },
        {
          "SER datasets as the number of parameters increases.": "VI. CONCLUSION"
        },
        {
          "SER datasets as the number of parameters increases.": "In\nthis\nstudy,\nto\nbetter model\nlocal\nand\nglobal\nfeatures"
        },
        {
          "SER datasets as the number of parameters increases.": "of\nspeech\nsignals\nat\ndifferent\nlevels\nof\ngranularity\nin SER"
        },
        {
          "SER datasets as the number of parameters increases.": "and\ncapture\ntemporal,\nspatial\nand\nchannel\ndependencies\nin"
        },
        {
          "SER datasets as the number of parameters increases.": "speech signals, we propose\na Speech Emotion Recognition"
        },
        {
          "SER datasets as the number of parameters increases.": ""
        },
        {
          "SER datasets as the number of parameters increases.": "network based on CNN-Transformer\nand multi-dimensional"
        },
        {
          "SER datasets as the number of parameters increases.": ""
        },
        {
          "SER datasets as the number of parameters increases.": "attention mechanisms. The network consists of three modules."
        },
        {
          "SER datasets as the number of parameters increases.": ""
        },
        {
          "SER datasets as the number of parameters increases.": "First, a CNN block is used to model\ntime-frequency domain"
        },
        {
          "SER datasets as the number of parameters increases.": ""
        },
        {
          "SER datasets as the number of parameters increases.": "information in speech, capturing preliminary local information"
        },
        {
          "SER datasets as the number of parameters increases.": ""
        },
        {
          "SER datasets as the number of parameters increases.": "in\nspeech.\nSecond, we\npropose\na T-Sa\nnetwork\nto model"
        },
        {
          "SER datasets as the number of parameters increases.": ""
        },
        {
          "SER datasets as the number of parameters increases.": "the\nemotional\nexpression context of\nfeatures over\ntime\nand"
        },
        {
          "SER datasets as the number of parameters increases.": ""
        },
        {
          "SER datasets as the number of parameters increases.": "efficiently fuse the spatial and channel dimensions of speech"
        },
        {
          "SER datasets as the number of parameters increases.": ""
        },
        {
          "SER datasets as the number of parameters increases.": "feature maps through Shuffle units. Finally,\nto efficiently fuse"
        },
        {
          "SER datasets as the number of parameters increases.": ""
        },
        {
          "SER datasets as the number of parameters increases.": "local\ninformation and long-distance dependencies\nin speech,"
        },
        {
          "SER datasets as the number of parameters increases.": ""
        },
        {
          "SER datasets as the number of parameters increases.": "we propose\nan LCT module\nthat uses\nlightweight\nconvolu-"
        },
        {
          "SER datasets as the number of parameters increases.": ""
        },
        {
          "SER datasets as the number of parameters increases.": "tional modules and introduces Coordinate Attention into multi-"
        },
        {
          "SER datasets as the number of parameters increases.": ""
        },
        {
          "SER datasets as the number of parameters increases.": "head self-attention. This allows\nfor\nthe fusion of\nfeatures at"
        },
        {
          "SER datasets as the number of parameters increases.": ""
        },
        {
          "SER datasets as the number of parameters increases.": "different\nlevels of granularity while enhancing information in"
        },
        {
          "SER datasets as the number of parameters increases.": ""
        },
        {
          "SER datasets as the number of parameters increases.": "the time-frequency domain of\nfeatures without\nintroducing a"
        },
        {
          "SER datasets as the number of parameters increases.": ""
        },
        {
          "SER datasets as the number of parameters increases.": "high number of parameters."
        },
        {
          "SER datasets as the number of parameters increases.": ""
        },
        {
          "SER datasets as the number of parameters increases.": "In\nfuture work,\nin\naddition\nto MFCC features, we will"
        },
        {
          "SER datasets as the number of parameters increases.": ""
        },
        {
          "SER datasets as the number of parameters increases.": "try more\nhierarchical\nspeech\nfeatures\nand\ncombine\ncurrent"
        },
        {
          "SER datasets as the number of parameters increases.": ""
        },
        {
          "SER datasets as the number of parameters increases.": "CNN and transformer\nstructures\nto improve the performance"
        },
        {
          "SER datasets as the number of parameters increases.": ""
        },
        {
          "SER datasets as the number of parameters increases.": "of Speech Emotion Recognition from multiple feature dimen-"
        },
        {
          "SER datasets as the number of parameters increases.": ""
        },
        {
          "SER datasets as the number of parameters increases.": "sions."
        },
        {
          "SER datasets as the number of parameters increases.": ""
        },
        {
          "SER datasets as the number of parameters increases.": ""
        },
        {
          "SER datasets as the number of parameters increases.": "ACKNOWLEDGMENTS"
        },
        {
          "SER datasets as the number of parameters increases.": ""
        },
        {
          "SER datasets as the number of parameters increases.": "This work was\nsupported by the National Natural Science"
        },
        {
          "SER datasets as the number of parameters increases.": "Foundation of China\n(Grant No. 62001173),\nthe Project of"
        },
        {
          "SER datasets as the number of parameters increases.": "Special\nFunds\nfor\nthe Cultivation\nof Guangdong College"
        },
        {
          "SER datasets as the number of parameters increases.": "Students’ Scientific\nand Technological\nInnovation\n(”Climb-"
        },
        {
          "SER datasets as the number of parameters increases.": "ing\nProgram”\nSpecial\nFunds)\n(Grant No.\npdjh2022a0131,"
        },
        {
          "SER datasets as the number of parameters increases.": "pdjh2023b0141)."
        },
        {
          "SER datasets as the number of parameters increases.": ""
        },
        {
          "SER datasets as the number of parameters increases.": "REFERENCES"
        },
        {
          "SER datasets as the number of parameters increases.": ""
        },
        {
          "SER datasets as the number of parameters increases.": "[1] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech emotion"
        },
        {
          "SER datasets as the number of parameters increases.": "recognition: Features,\nclassification\nschemes,\nand\ndatabases,” Pattern"
        },
        {
          "SER datasets as the number of parameters increases.": "recognition, vol. 44, no. 3, pp. 572–587, 2011."
        },
        {
          "SER datasets as the number of parameters increases.": ""
        },
        {
          "SER datasets as the number of parameters increases.": "[2]\nL.-S. A. Low, N. C. Maddage, M. Lech, L. B. Sheeber, and N. B. Allen,"
        },
        {
          "SER datasets as the number of parameters increases.": ""
        },
        {
          "SER datasets as the number of parameters increases.": "“Detection of clinical depression in adolescents’\nspeech during family"
        },
        {
          "SER datasets as the number of parameters increases.": "interactions,” IEEE Transactions on Biomedical Engineering, vol. 58,"
        },
        {
          "SER datasets as the number of parameters increases.": "no. 3, pp. 574–586, 2010."
        },
        {
          "SER datasets as the number of parameters increases.": ""
        },
        {
          "SER datasets as the number of parameters increases.": "[3] A. Mahdhaoui, M. Chetouani, and C. Zong, “Motherese detection based"
        },
        {
          "SER datasets as the number of parameters increases.": ""
        },
        {
          "SER datasets as the number of parameters increases.": "on segmental and supra-segmental features,” in 2008 19th International"
        },
        {
          "SER datasets as the number of parameters increases.": "Conference on Pattern Recognition.\nIEEE, 2008, pp. 1–4."
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[4]\nS. E. Bou-Ghazale and J. H. Hansen, “A comparative study of traditional",
          "12": "2020 IEEE International Conference on Acoustics, Speech and Signal"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "and newly proposed features\nfor\nrecognition of\nspeech under\nstress,”",
          "12": "Processing (ICASSP).\nIEEE, 2020, pp. 6489–6493."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "IEEE Transactions on speech and audio processing, vol. 8, no. 4, pp.",
          "12": "[26] Y. Xu, H. Xu, and J. Zou, “Hgfm: A hierarchical grained and feature"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "429–442, 2000.",
          "12": "model\nfor acoustic emotion recognition,” in ICASSP 2020-2020 IEEE"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[5] C. Gobl and A. N. Chasaide, “The role of voice quality in communi-",
          "12": "International Conference on Acoustics, Speech and Signal Processing"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "cating emotion, mood and attitude,” Speech communication, vol. 40, no.",
          "12": "(ICASSP).\nIEEE, 2020, pp. 6499–6503."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "1-2, pp. 189–212, 2003.",
          "12": "[27] C. Li, “Robotic emotion recognition using two-level\nfeatures fusion in"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[6]\nJ. Hernando and C. Nadeu, “Linear prediction of the one-sided autocor-",
          "12": "audio signals of speech,” IEEE Sensors Journal, 2021."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "relation sequence for noisy speech recognition,” IEEE Transactions on",
          "12": "[28]\nJ. Liu and H. Wang, “A speech emotion recognition framework for better"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Speech and Audio Processing, vol. 5, no. 1, pp. 80–84, 1997.",
          "12": "discrimination of confusions,” in Interspeech, 2021, pp. 4483–4487."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[7]\nS. S. Barpanda, B. Majhi, P. K. Sa, A. K. Sangaiah, and S. Bakshi, “Iris",
          "12": "[29] H. Zou, Y. Si, C. Chen, D. Rajan, and E. S. Chng, “Speech emotion"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "feature extraction through wavelet mel-frequency cepstrum coefficients,”",
          "12": "recognition with co-attention based multi-level acoustic information,” in"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Optics & Laser Technology, vol. 110, pp. 13–23, 2019.",
          "12": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[8] Y. Q. Qin and X. Y. Zhang, “Hmm-based speaker emotional recognition",
          "12": "and Signal Processing (ICASSP).\nIEEE, 2022, pp. 7367–7371."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "technology for\nspeech signal,”\nin Advanced Materials Research, vol.",
          "12": "[30]\nS. Zhang, X. Zhao, and Q. Tian, “Spontaneous speech emotion recog-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "230.\nTrans Tech Publ, 2011, pp. 261–265.",
          "12": "nition using multiscale deep convolutional\nlstm,” IEEE Transactions on"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[9]\nJ. Pribil, A. Pribilova, and J. Matousek, “Artefact determination by gmm-",
          "12": "Affective Computing, 2019."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "based continuous detection of emotional changes in synthetic speech,” in",
          "12": "[31]\nJ. Hu, L. Shen, and G. Sun, “Squeeze-and-excitation networks,” 2018."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "2019 42nd International Conference on Telecommunications and Signal",
          "12": "[32]\nS. Woo,\nJ. Park,\nJ.-Y. Lee,\nand I. S. Kweon,\n“Cbam: Convolutional"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Processing (TSP).\nIEEE, 2019, pp. 45–48.",
          "12": "the European conference on\nblock attention module,” in Proceedings of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[10] B. Schuller, B. Vlasenko, F. Eyben, M. W¨ollmer, A. Stuhlsatz, A. Wen-",
          "12": "computer vision (ECCV), 2018, pp. 3–19."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "demuth,\nand G. Rigoll,\n“Cross-corpus\nacoustic\nemotion\nrecognition:",
          "12": "[33]\nP. Li, Y. Song,\nI. V. McLoughlin, W. Guo, and L.-R. Dai, “An atten-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Variances and strategies,” IEEE Transactions on Affective Computing,",
          "12": "tion pooling based representation learning method for\nspeech emotion"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "vol. 1, no. 2, pp. 119–131, 2010.",
          "12": "recognition,” 2018."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[11] B. Gupta\nand\nS. Dhawan,\n“Deep\nlearning\nresearch:\nScientometric",
          "12": "[34] Y.-X. Xi, Y. Song, L.-R. Dai,\nI. McLoughlin,\nand L. Liu,\n“Frontend"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "assessment\nof\nglobal\npublications\noutput\nduring\n2004-17,” Emerging",
          "12": "attributes disentanglement\nfor speech emotion recognition,” in ICASSP"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Science Journal, vol. 3, no. 1, pp. 23–32, 2019.",
          "12": "2022-2022 IEEE International Conference on Acoustics, Speech and"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[12]\nS. Kumar, T. Roshni, and D. Himayoun, “A comparison of emotional",
          "12": "Signal Processing (ICASSP).\nIEEE, 2022, pp. 7712–7716."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "neural network (enn) and artificial neural network (ann) approach for",
          "12": "[35]\nS.\nSchneider, A. Baevski, R. Collobert,\nand M. Auli,\n“wav2vec:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "rainfall-runoff modelling,” Civil Engineering Journal, vol. 5, no. 10, pp.",
          "12": "arXiv\npreprint\nUnsupervised\npre-training\nfor\nspeech\nrecognition,”"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "2120–2130, 2019.",
          "12": "arXiv:1904.05862, 2019."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[13] Q. Cao, M. Hou, B. Chen, Z. Zhang, and G. Lu, “Hierarchical network",
          "12": "[36] A.\nBaevski,\nS.\nSchneider,\nand\nM.\nAuli,\n“vq-wav2vec:\nSelf-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "based on the fusion of static and dynamic features for speech emotion",
          "12": "supervised learning of discrete speech representations,” arXiv preprint"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "recognition,” in ICASSP 2021-2021 IEEE International Conference on",
          "12": "arXiv:1910.05453, 2019."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Acoustics, Speech and Signal Processing (ICASSP).\nIEEE, 2021, pp.",
          "12": "[37] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, “wav2vec 2.0: A frame-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "6334–6338.",
          "12": "work for self-supervised learning of speech representations,” Advances"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[14]\nJ. Liu, Z. Liu, L. Wang, L. Guo,\nand\nJ. Dang,\n“Speech\nemotion",
          "12": "in Neural\nInformation Processing Systems, vol. 33, pp. 12 449–12 460,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "recognition with\nlocal-global\naware\ndeep\nrepresentation\nlearning,”\nin",
          "12": "2020."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech",
          "12": "[38]\nL. Pepino, P. Riera, and L. Ferrer, “Emotion recognition from speech"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "and Signal Processing (ICASSP).\nIEEE, 2020, pp. 7174–7178.",
          "12": "arXiv\npreprint\nusing wav2vec\n2.0\nembeddings,”\narXiv:2104.03502,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[15] R. A. Khalil, E. Jones, M. I. Babar, T. Jan, M. H. Zafar, and T. Alhussain,",
          "12": "2021."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "“Speech emotion recognition using deep learning techniques: A review,”",
          "12": "[39] X. Cai, J. Yuan, R. Zheng, L. Huang, and K. Church, “Speech emotion"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "IEEE Access, vol. 7, pp. 117 327–117 345, 2019.",
          "12": "recognition with multi-task learning,” in Interspeech, vol. 2021, 2021,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[16] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,",
          "12": "pp. 4508–4512."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "in\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances",
          "12": "[40] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "neural\ninformation processing systems, vol. 30, 2017.",
          "12": "T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al.,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[17]\nL. Tarantino, P. N. Garner, A. Lazaridis et al., “Self-attention for speech",
          "12": "“An image is worth 16x16 words: Transformers for\nimage recognition"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "emotion recognition,” in Interspeech, 2019, pp. 2578–2582.",
          "12": "at scale,” arXiv preprint arXiv:2010.11929, 2020."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[18] X. Wang, M. Wang, W. Qi, W. Su, X. Wang, and H. Zhou, “A novel",
          "12": "[41] N.\nRistea,\nR.\nIonescu,\nand\nF.\nKhan,\n“Septr:\nSeparable\ntrans-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "end-to-end speech emotion recognition network with stacked transformer",
          "12": "former\nfor audio spectrogram processing. arxiv 2022,” arXiv preprint"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "layers,” in ICASSP 2021-2021 IEEE International Conference on Acous-",
          "12": "arXiv:2203.09581."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "tics, Speech and Signal Processing (ICASSP).\nIEEE, 2021, pp. 6289–",
          "12": "[42] H.-t. Xu, J. Zhang, and L.-r. Dai, “Differential\ntime-frequency log-mel"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "6293.",
          "12": "spectrogram features\nfor vision transformer based infant\ncry recogni-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[19] D. Hu, X. Hu, and X. Xu, “Multiple enhancements to lstm for learning",
          "12": "tion,” Proc.\nInterspeech 2022, pp. 1963–1967, 2022."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Inter-\nemotion-salient\nfeatures\nin speech emotion recognition,” Proc.",
          "12": "[43] Q.-L. Zhang and Y.-B. Yang, “Sa-net: Shuffle attention for deep con-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "speech 2022, pp. 4720–4724, 2022.",
          "12": "volutional neural networks,” in ICASSP 2021-2021 IEEE International"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[20]\nS. Kwon et al., “Att-net: Enhanced emotion recognition system using",
          "12": "Conference\non Acoustics,\nSpeech\nand\nSignal Processing\n(ICASSP)."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "lightweight self-attention module,” Applied Soft Computing, vol. 102, p.",
          "12": "IEEE, 2021, pp. 2235–2239."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "107101, 2021.",
          "12": "[44]\nJ. Park, S. Woo, J.-Y. Lee, and I. S. Kweon, “Bam: Bottleneck attention"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[21]\nL. Guo, L. Wang, C. Xu, J. Dang, E. S. Chng, and H. Li, “Representation",
          "12": "module,” arXiv preprint arXiv:1807.06514, 2018."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "learning with\nspectro-temporal-channel\nattention\nfor\nspeech\nemotion",
          "12": "[45] Y. Cao, J. Xu, S. Lin, F. Wei, and H. Hu, “Gcnet: Non-local networks"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "recognition,” in ICASSP 2021-2021 IEEE International Conference on",
          "12": "the\nmeet\nsqueeze-excitation networks and beyond,” in Proceedings of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Acoustics, Speech and Signal Processing (ICASSP).\nIEEE, 2021, pp.",
          "12": "IEEE/CVF international\nconference\non\ncomputer\nvision workshops,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "6304–6308.",
          "12": "2019, pp. 0–0."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[22] A. Krizhevsky,\nI. Sutskever, and G. E. Hinton, “Imagenet classification",
          "12": "Proceedings\nof\nthe\n[46] Y. Wu\nand K. He,\n“Group\nnormalization,”\nin"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "with deep convolutional neural networks,” Communications of the ACM,",
          "12": "European conference on computer vision (ECCV), 2018, pp. 3–19."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "vol. 60, no. 6, pp. 84–90, 2017.",
          "12": "[47] N. Ma, X. Zhang, H.-T. Zheng, and J. Sun, “Shufflenet v2: Practical"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[23] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual\nlearning for image",
          "12": "the\nguidelines\nfor efficient cnn architecture design,” in Proceedings of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the IEEE conference on computer vision\nrecognition,” in Proceedings of",
          "12": "European conference on computer vision (ECCV), 2018, pp. 116–131."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "and pattern recognition, 2016, pp. 770–778.",
          "12": "[48]\nJ. Guo, K. Han, H. Wu, Y. Tang, X. Chen, Y. Wang,\nand C. Xu,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[24] W. Zhu and X. Li, “Speech emotion recognition with global-aware fusion",
          "12": "“Cmt: Convolutional\nneural\nnetworks meet\nvision\ntransformers,”\nin"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ICASSP 2022-2022\nIEEE\non multi-scale\nfeature\nrepresentation,”\nin",
          "12": "Proceedings\nof\nthe\nIEEE/CVF Conference\non Computer Vision\nand"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "International Conference on Acoustics, Speech and Signal Processing",
          "12": "Pattern Recognition, 2022, pp. 12 175–12 185."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "(ICASSP).\nIEEE, 2022, pp. 6437–6441.",
          "12": "[49] Q. Hou, D. Zhou,\nand\nJ.\nFeng,\n“Coordinate\nattention\nfor\nefficient"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[25]\nE. Guizzo, T. Weyde, and J. B. Leveson, “Multi-time-scale convolution",
          "12": "the IEEE/CVF conference\nmobile network design,” in Proceedings of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "for emotion recognition from speech audio signals,” in ICASSP 2020-",
          "12": "on computer vision and pattern recognition, 2021, pp. 13 713–13 722."
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[50] A. Trockman and J. Z. Kolter, “Patches are all you need?” arXiv preprint",
          "13": "Xiaoyu Tang\n(Member,\nIEEE)\nreceived\nthe B.S."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "arXiv:2201.09792, 2022.",
          "13": "degree from South China Normal University in 2003"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[51] A. Howard, M. Sandler, G. Chu, L.-C. Chen, B. Chen, M. Tan, W. Wang,",
          "13": "and the M.S. degree\nfrom Sun Yat-sen University"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Y\n. Zhu, R. Pang, V. Vasudevan et al.,\n“Searching\nfor mobilenetv3,”",
          "13": "in 2011. He is currently pursuing the Ph.D. degree"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the IEEE/CVF international conference on computer\nin Proceedings of",
          "13": "with South China Normal University. He is working"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "vision, 2019, pp. 1314–1324.",
          "13": "with the School of Physics and Telecommunication"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[52] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower, S. Kim, J. N.",
          "13": "Engineering, South China Normal University, where"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Chang, S. Lee, and S. S. Narayanan, “Iemocap:\nInteractive emotional",
          "13": "he engaged in information system development. His"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "dyadic motion capture database,” Language resources and evaluation,",
          "13": "research interests include machine vision,\nintelligent"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "vol. 42, no. 4, pp. 335–359, 2008.",
          "13": "control, and the Internet of Things. He is a member"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[53]\nF. Burkhardt, A. Paeschke, M. Rolfes, W. F. Sendlmeier, B. Weiss et al.,",
          "13": "of\nthe IEEE ICICSP Technical Committee."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "“A database of german emotional speech.” in Interspeech, vol. 5, 2005,",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "pp. 1517–1520.",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[54] H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz, “mixup: Beyond",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "empirical\nrisk minimization,” arXiv preprint arXiv:1710.09412, 2017.",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[55]\nS. Suganya\nand E. Charles,\n“Speech emotion recognition using deep",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "learning on audio recordings,” in 2019 19th International Conference",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "on Advances\nin ICT for Emerging Regions\n(ICTer), vol. 250.\nIEEE,",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "2019, pp. 1–6.",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "Yixin Lin\nreceived\nthe B.Eng.\ndegree\nfrom the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[56]\nS. Latif, R. Rana, S. Khalifa, R.\nJurdak,\nand B. W. Schuller,\n“Deep",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "School\nof\nPhysics\nand Telecommunication Engi-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "architecture\nenhancing\nrobustness\nto\nnoise,\nadversarial\nattacks,\nand",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "neering, South China Normal University,\nin 2021,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "arXiv\npreprint\ncross-corpus\nsetting\nfor\nspeech\nemotion\nrecognition,”",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "where\nhe\nis\ncurrently\npursuing\nthe M.E.\ndegree"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "arXiv:2005.08453, 2020.",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "with the Department of Electronics and Information"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[57]\nJ. Wang, M. Xue, R. Culhane, E. Diao, J. Ding, and V. Tarokh, “Speech",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "Engineering. His research interests include artificial"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "emotion recognition with dual-sequence lstm architecture,” in ICASSP",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "intelligence and speech emotion recognition."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "2020-2020 IEEE International Conference on Acoustics, Speech and",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Signal Processing (ICASSP).\nIEEE, 2020, pp. 6474–6478.",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[58]\nS. Li, X. Xing, W. Fan, B. Cai, P. Fordson, and X. Xu, “Spatiotem-",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "poral and frequential cascaded attention networks\nfor\nspeech emotion",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "recognition,” Neurocomputing, vol. 448, pp. 238–248, 2021.",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[59]\nS. Latif, R. Rana, S. Khalifa, R.\nJurdak,\nJ. Epps, and B. W. Schuller,",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "“Multi-task semi-supervised adversarial autoencoding for\nspeech emo-",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "tion recognition,” IEEE Transactions on Affective Computing, vol. 13,",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "no. 2, pp. 992–1004, 2022.",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[60] Y. Gao,\nJ. Liu, L. Wang, and J. Dang, “Metric learning based feature",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "representation with gated fusion model for speech emotion recognition.”",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "in Interspeech, 2021, pp. 4503–4507.",
          "13": "Ting Dang is a Senior Research Scientist at Nokia"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[61] D. Dai, Z. Wu, R. Li, X. Wu, J. Jia, and H. Meng, “Learning discrimi-",
          "13": "Bell Labs, Cambridge, UK. Before\njoining Nokia,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "native features from spectrograms using center\nloss for speech emotion",
          "13": "she worked as a Senior Research Associate at\nthe"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "recognition,” in ICASSP 2019-2019 IEEE International Conference on",
          "13": "University of Cambridge. Dr. Dang earned her Ph.D."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Acoustics, Speech and Signal Processing (ICASSP).\nIEEE, 2019, pp.",
          "13": "degree\nfrom the University\nof New South Wales"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "7405–7409.",
          "13": "in Sydney, Australia,\nand\nholds MEng\nand BEng"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[62]\nT. Tuncer, S. Dogan, and U. R. Acharya, “Automated accurate speech",
          "13": "degrees\nin Signal Processing from the Northwest-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "emotion\nrecognition\nsystem using\ntwine\nshuffle\npattern\nand\niterative",
          "13": "ern Polytechnical University in China. Her primary"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "neighborhood component analysis\ntechniques,” Knowledge-Based Sys-",
          "13": "research interests are on exploring the potential of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "tems, vol. 211, p. 106547, 2021.",
          "13": "audio signals\n(e.g.,\nspeech)\nfor mobile health,\ni.e.,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[63]\nS. Zhong, B. Yu, and H. Zhang, “Exploration of an independent\ntraining",
          "13": "automatic disease\nand mental\nstate prediction and"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "framework for\nspeech emotion recognition,” IEEE Access, vol. 8, pp.",
          "13": "monitoring (e.g., COVID-19, emotion) via mobile and wearable audio sensing."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "222 533–222 543, 2020.",
          "13": "Her work aims\nto develop generalised and interpretable machine\nlearning"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[64]\nL. Kerkeni, Y. Serrestou, K. Raoof, M. Mbarki, M. A. Mahjoub, and",
          "13": "models\nto improve healthcare delivery. She served as\nthe (senior) program"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "C. Cleder,\n“Automatic\nspeech\nemotion\nrecognition\nusing\nan\noptimal",
          "13": "committee\nand invited reviewer\nfor more\nthan 30 top-tier\nconferences\nand"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "combination of\nfeatures based on emd-tkeo,” Speech Communication,",
          "13": "journals, such as AAAI, NeurIPS, ICASSP, IEEE TAC, IEEE TASL etc. She"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "vol. 114, pp. 22–35, 2019.",
          "13": "has won the Asian Dean’s Forum (ADF) Rising Star Women in Engineering"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[65] M. Hou, Z. Zhang, Q. Cao, D. Zhang, and G. Lu, “Multi-view speech",
          "13": "Award 2022, IEEE Early Career Writing Retreat Grant 2019 and ISCA Grant"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "IEEE/ACM\nemotion\nrecognition\nvia\ncollective\nrelation\nconstruction,”",
          "13": "2017."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Transactions on Audio, Speech, and Language Processing, vol. 30, pp.",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "218–229, 2021.",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[66]\nS. Mehta\nand M.\nRastegari,\n“Mobilevit:\nlight-weight,\ngeneral-",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "arXiv\npreprint\npurpose,\nand\nmobile-friendly\nvision\ntransformer,”",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "arXiv:2110.02178, 2021.",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[67] Mehta, Sachin and Rastegari, Mohammad, “Separable self-attention for",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "mobile vision transformers,” arXiv preprint arXiv:2206.02680, 2022.",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[68]\nS. N. Wadekar and A. Chaurasia, “Mobilevitv3: Mobile-friendly vision",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "Yuanfang Zhang is\ncurrently principal\nresearcher"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "transformer with simple and effective fusion of\nlocal, global and input",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "at Autocity (Shenzhen) autonomous driving Co.,ltd."
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[50] A. Trockman and J. Z. Kolter, “Patches are all you need?” arXiv preprint",
          "13": "Xiaoyu Tang\n(Member,\nIEEE)\nreceived\nthe B.S."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "arXiv:2201.09792, 2022.",
          "13": "degree from South China Normal University in 2003"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[51] A. Howard, M. Sandler, G. Chu, L.-C. Chen, B. Chen, M. Tan, W. Wang,",
          "13": "and the M.S. degree\nfrom Sun Yat-sen University"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Y\n. Zhu, R. Pang, V. Vasudevan et al.,\n“Searching\nfor mobilenetv3,”",
          "13": "in 2011. He is currently pursuing the Ph.D. degree"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the IEEE/CVF international conference on computer\nin Proceedings of",
          "13": "with South China Normal University. He is working"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "vision, 2019, pp. 1314–1324.",
          "13": "with the School of Physics and Telecommunication"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[52] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower, S. Kim, J. N.",
          "13": "Engineering, South China Normal University, where"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Chang, S. Lee, and S. S. Narayanan, “Iemocap:\nInteractive emotional",
          "13": "he engaged in information system development. His"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "dyadic motion capture database,” Language resources and evaluation,",
          "13": "research interests include machine vision,\nintelligent"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "vol. 42, no. 4, pp. 335–359, 2008.",
          "13": "control, and the Internet of Things. He is a member"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[53]\nF. Burkhardt, A. Paeschke, M. Rolfes, W. F. Sendlmeier, B. Weiss et al.,",
          "13": "of\nthe IEEE ICICSP Technical Committee."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "“A database of german emotional speech.” in Interspeech, vol. 5, 2005,",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "pp. 1517–1520.",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[54] H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz, “mixup: Beyond",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "empirical\nrisk minimization,” arXiv preprint arXiv:1710.09412, 2017.",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[55]\nS. Suganya\nand E. Charles,\n“Speech emotion recognition using deep",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "learning on audio recordings,” in 2019 19th International Conference",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "on Advances\nin ICT for Emerging Regions\n(ICTer), vol. 250.\nIEEE,",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "2019, pp. 1–6.",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "Yixin Lin\nreceived\nthe B.Eng.\ndegree\nfrom the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[56]\nS. Latif, R. Rana, S. Khalifa, R.\nJurdak,\nand B. W. Schuller,\n“Deep",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "School\nof\nPhysics\nand Telecommunication Engi-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "architecture\nenhancing\nrobustness\nto\nnoise,\nadversarial\nattacks,\nand",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "neering, South China Normal University,\nin 2021,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "arXiv\npreprint\ncross-corpus\nsetting\nfor\nspeech\nemotion\nrecognition,”",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "where\nhe\nis\ncurrently\npursuing\nthe M.E.\ndegree"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "arXiv:2005.08453, 2020.",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "with the Department of Electronics and Information"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[57]\nJ. Wang, M. Xue, R. Culhane, E. Diao, J. Ding, and V. Tarokh, “Speech",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "Engineering. His research interests include artificial"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "emotion recognition with dual-sequence lstm architecture,” in ICASSP",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "intelligence and speech emotion recognition."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "2020-2020 IEEE International Conference on Acoustics, Speech and",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Signal Processing (ICASSP).\nIEEE, 2020, pp. 6474–6478.",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[58]\nS. Li, X. Xing, W. Fan, B. Cai, P. Fordson, and X. Xu, “Spatiotem-",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "poral and frequential cascaded attention networks\nfor\nspeech emotion",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "recognition,” Neurocomputing, vol. 448, pp. 238–248, 2021.",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[59]\nS. Latif, R. Rana, S. Khalifa, R.\nJurdak,\nJ. Epps, and B. W. Schuller,",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "“Multi-task semi-supervised adversarial autoencoding for\nspeech emo-",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "tion recognition,” IEEE Transactions on Affective Computing, vol. 13,",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "no. 2, pp. 992–1004, 2022.",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[60] Y. Gao,\nJ. Liu, L. Wang, and J. Dang, “Metric learning based feature",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "representation with gated fusion model for speech emotion recognition.”",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "in Interspeech, 2021, pp. 4503–4507.",
          "13": "Ting Dang is a Senior Research Scientist at Nokia"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[61] D. Dai, Z. Wu, R. Li, X. Wu, J. Jia, and H. Meng, “Learning discrimi-",
          "13": "Bell Labs, Cambridge, UK. Before\njoining Nokia,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "native features from spectrograms using center\nloss for speech emotion",
          "13": "she worked as a Senior Research Associate at\nthe"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "recognition,” in ICASSP 2019-2019 IEEE International Conference on",
          "13": "University of Cambridge. Dr. Dang earned her Ph.D."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Acoustics, Speech and Signal Processing (ICASSP).\nIEEE, 2019, pp.",
          "13": "degree\nfrom the University\nof New South Wales"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "7405–7409.",
          "13": "in Sydney, Australia,\nand\nholds MEng\nand BEng"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[62]\nT. Tuncer, S. Dogan, and U. R. Acharya, “Automated accurate speech",
          "13": "degrees\nin Signal Processing from the Northwest-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "emotion\nrecognition\nsystem using\ntwine\nshuffle\npattern\nand\niterative",
          "13": "ern Polytechnical University in China. Her primary"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "neighborhood component analysis\ntechniques,” Knowledge-Based Sys-",
          "13": "research interests are on exploring the potential of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "tems, vol. 211, p. 106547, 2021.",
          "13": "audio signals\n(e.g.,\nspeech)\nfor mobile health,\ni.e.,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[63]\nS. Zhong, B. Yu, and H. Zhang, “Exploration of an independent\ntraining",
          "13": "automatic disease\nand mental\nstate prediction and"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "framework for\nspeech emotion recognition,” IEEE Access, vol. 8, pp.",
          "13": "monitoring (e.g., COVID-19, emotion) via mobile and wearable audio sensing."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "222 533–222 543, 2020.",
          "13": "Her work aims\nto develop generalised and interpretable machine\nlearning"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[64]\nL. Kerkeni, Y. Serrestou, K. Raoof, M. Mbarki, M. A. Mahjoub, and",
          "13": "models\nto improve healthcare delivery. She served as\nthe (senior) program"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "C. Cleder,\n“Automatic\nspeech\nemotion\nrecognition\nusing\nan\noptimal",
          "13": "committee\nand invited reviewer\nfor more\nthan 30 top-tier\nconferences\nand"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "combination of\nfeatures based on emd-tkeo,” Speech Communication,",
          "13": "journals, such as AAAI, NeurIPS, ICASSP, IEEE TAC, IEEE TASL etc. She"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "vol. 114, pp. 22–35, 2019.",
          "13": "has won the Asian Dean’s Forum (ADF) Rising Star Women in Engineering"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[65] M. Hou, Z. Zhang, Q. Cao, D. Zhang, and G. Lu, “Multi-view speech",
          "13": "Award 2022, IEEE Early Career Writing Retreat Grant 2019 and ISCA Grant"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "IEEE/ACM\nemotion\nrecognition\nvia\ncollective\nrelation\nconstruction,”",
          "13": "2017."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Transactions on Audio, Speech, and Language Processing, vol. 30, pp.",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "218–229, 2021.",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[66]\nS. Mehta\nand M.\nRastegari,\n“Mobilevit:\nlight-weight,\ngeneral-",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "arXiv\npreprint\npurpose,\nand\nmobile-friendly\nvision\ntransformer,”",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "arXiv:2110.02178, 2021.",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[67] Mehta, Sachin and Rastegari, Mohammad, “Separable self-attention for",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "mobile vision transformers,” arXiv preprint arXiv:2206.02680, 2022.",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[68]\nS. N. Wadekar and A. Chaurasia, “Mobilevitv3: Mobile-friendly vision",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "Yuanfang Zhang is\ncurrently principal\nresearcher"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "transformer with simple and effective fusion of\nlocal, global and input",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "at Autocity (Shenzhen) autonomous driving Co.,ltd."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "features,” arXiv preprint arXiv:2209.15159, 2022.",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "He\ngot\nhis\ndual\nPh.D.\ndegrees with\nthe\nSchool"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "of Computer Science at Northwestern Polytechnical"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "His"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": ""
        }
      ],
      "page": 14
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "M Ayadi",
        "M Kamel",
        "F Karray"
      ],
      "year": "2011",
      "venue": "Pattern recognition"
    },
    {
      "citation_id": "2",
      "title": "Detection of clinical depression in adolescents' speech during family interactions",
      "authors": [
        "L.-S Low",
        "N Maddage",
        "M Lech",
        "L Sheeber",
        "N Allen"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "3",
      "title": "Motherese detection based on segmental and supra-segmental features",
      "authors": [
        "A Mahdhaoui",
        "M Chetouani",
        "C Zong"
      ],
      "year": "2008",
      "venue": "2008 19th International Conference on Pattern Recognition"
    },
    {
      "citation_id": "4",
      "title": "A comparative study of traditional and newly proposed features for recognition of speech under stress",
      "authors": [
        "S Bou-Ghazale",
        "J Hansen"
      ],
      "year": "2000",
      "venue": "IEEE Transactions on speech and audio processing"
    },
    {
      "citation_id": "5",
      "title": "The role of voice quality in communicating emotion, mood and attitude",
      "authors": [
        "C Gobl",
        "A Chasaide"
      ],
      "year": "2003",
      "venue": "Speech communication"
    },
    {
      "citation_id": "6",
      "title": "Linear prediction of the one-sided autocorrelation sequence for noisy speech recognition",
      "authors": [
        "J Hernando",
        "C Nadeu"
      ],
      "year": "1997",
      "venue": "IEEE Transactions on Speech and Audio Processing"
    },
    {
      "citation_id": "7",
      "title": "Iris feature extraction through wavelet mel-frequency cepstrum coefficients",
      "authors": [
        "S Barpanda",
        "B Majhi",
        "P Sa",
        "A Sangaiah",
        "S Bakshi"
      ],
      "year": "2019",
      "venue": "Optics & Laser Technology"
    },
    {
      "citation_id": "8",
      "title": "Hmm-based speaker emotional recognition technology for speech signal",
      "authors": [
        "Y Qin",
        "X Zhang"
      ],
      "year": "2011",
      "venue": "Advanced Materials Research"
    },
    {
      "citation_id": "9",
      "title": "Artefact determination by gmmbased continuous detection of emotional changes in synthetic speech",
      "authors": [
        "J Pribil",
        "A Pribilova",
        "J Matousek"
      ],
      "year": "2019",
      "venue": "2019 42nd International Conference on Telecommunications and Signal Processing"
    },
    {
      "citation_id": "10",
      "title": "Cross-corpus acoustic emotion recognition: Variances and strategies",
      "authors": [
        "B Schuller",
        "B Vlasenko",
        "F Eyben",
        "M Wöllmer",
        "A Stuhlsatz",
        "A Wendemuth",
        "G Rigoll"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "11",
      "title": "Deep learning research: Scientometric assessment of global publications output during 2004-17",
      "authors": [
        "B Gupta",
        "S Dhawan"
      ],
      "year": "2019",
      "venue": "Emerging Science Journal"
    },
    {
      "citation_id": "12",
      "title": "A comparison of emotional neural network (enn) and artificial neural network (ann) approach for rainfall-runoff modelling",
      "authors": [
        "S Kumar",
        "T Roshni",
        "D Himayoun"
      ],
      "year": "2019",
      "venue": "Civil Engineering Journal"
    },
    {
      "citation_id": "13",
      "title": "Hierarchical network based on the fusion of static and dynamic features for speech emotion recognition",
      "authors": [
        "Q Cao",
        "M Hou",
        "B Chen",
        "Z Zhang",
        "G Lu"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "14",
      "title": "Speech emotion recognition with local-global aware deep representation learning",
      "authors": [
        "J Liu",
        "Z Liu",
        "L Wang",
        "L Guo",
        "J Dang"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "15",
      "title": "Speech emotion recognition using deep learning techniques: A review",
      "authors": [
        "R Khalil",
        "E Jones",
        "M Babar",
        "T Jan",
        "M Zafar",
        "T Alhussain"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "16",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "17",
      "title": "Self-attention for speech emotion recognition",
      "authors": [
        "L Tarantino",
        "P Garner",
        "A Lazaridis"
      ],
      "year": "2019",
      "venue": "Self-attention for speech emotion recognition"
    },
    {
      "citation_id": "18",
      "title": "A novel end-to-end speech emotion recognition network with stacked transformer layers",
      "authors": [
        "X Wang",
        "M Wang",
        "W Qi",
        "W Su",
        "X Wang",
        "H Zhou"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "19",
      "title": "Multiple enhancements to lstm for learning emotion-salient features in speech emotion recognition",
      "authors": [
        "D Hu",
        "X Hu",
        "X Xu"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech 2022"
    },
    {
      "citation_id": "20",
      "title": "Att-net: Enhanced emotion recognition system using lightweight self-attention module",
      "authors": [
        "S Kwon"
      ],
      "year": "2021",
      "venue": "Applied Soft Computing"
    },
    {
      "citation_id": "21",
      "title": "Representation learning with spectro-temporal-channel attention for speech emotion recognition",
      "authors": [
        "L Guo",
        "L Wang",
        "C Xu",
        "J Dang",
        "E Chng",
        "H Li"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "22",
      "title": "Imagenet classification with deep convolutional neural networks",
      "authors": [
        "A Krizhevsky",
        "I Sutskever",
        "G Hinton"
      ],
      "year": "2017",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "23",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "24",
      "title": "Speech emotion recognition with global-aware fusion on multi-scale feature representation",
      "authors": [
        "W Zhu",
        "X Li"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "25",
      "title": "Multi-time-scale convolution for emotion recognition from speech audio signals",
      "authors": [
        "E Guizzo",
        "T Weyde",
        "J Leveson"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "26",
      "title": "Hgfm: A hierarchical grained and feature model for acoustic emotion recognition",
      "authors": [
        "Y Xu",
        "H Xu",
        "J Zou"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "27",
      "title": "Robotic emotion recognition using two-level features fusion in audio signals of speech",
      "authors": [
        "C Li"
      ],
      "year": "2021",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "28",
      "title": "A speech emotion recognition framework for better discrimination of confusions",
      "authors": [
        "J Liu",
        "H Wang"
      ],
      "year": "2021",
      "venue": "A speech emotion recognition framework for better discrimination of confusions"
    },
    {
      "citation_id": "29",
      "title": "Speech emotion recognition with co-attention based multi-level acoustic information",
      "authors": [
        "H Zou",
        "Y Si",
        "C Chen",
        "D Rajan",
        "E Chng"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "30",
      "title": "Spontaneous speech emotion recognition using multiscale deep convolutional lstm",
      "authors": [
        "S Zhang",
        "X Zhao",
        "Q Tian"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "31",
      "title": "Squeeze-and-excitation networks",
      "authors": [
        "J Hu",
        "L Shen",
        "G Sun"
      ],
      "year": "2018",
      "venue": "Squeeze-and-excitation networks"
    },
    {
      "citation_id": "32",
      "title": "Cbam: Convolutional block attention module",
      "authors": [
        "S Woo",
        "J Park",
        "J.-Y Lee",
        "I Kweon"
      ],
      "year": "2018",
      "venue": "Proceedings of the European conference on computer vision (ECCV)"
    },
    {
      "citation_id": "33",
      "title": "An attention pooling based representation learning method for speech emotion recognition",
      "authors": [
        "P Li",
        "Y Song",
        "I Mcloughlin",
        "W Guo",
        "L.-R Dai"
      ],
      "year": "2018",
      "venue": "An attention pooling based representation learning method for speech emotion recognition"
    },
    {
      "citation_id": "34",
      "title": "Frontend attributes disentanglement for speech emotion recognition",
      "authors": [
        "Y.-X Xi",
        "Y Song",
        "L.-R Dai",
        "I Mcloughlin",
        "L Liu"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "35",
      "title": "wav2vec: Unsupervised pre-training for speech recognition",
      "authors": [
        "S Schneider",
        "A Baevski",
        "R Collobert",
        "M Auli"
      ],
      "year": "2019",
      "venue": "wav2vec: Unsupervised pre-training for speech recognition",
      "arxiv": "arXiv:1904.05862"
    },
    {
      "citation_id": "36",
      "title": "vq-wav2vec: Selfsupervised learning of discrete speech representations",
      "authors": [
        "A Baevski",
        "S Schneider",
        "M Auli"
      ],
      "year": "2019",
      "venue": "vq-wav2vec: Selfsupervised learning of discrete speech representations",
      "arxiv": "arXiv:1910.05453"
    },
    {
      "citation_id": "37",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "38",
      "title": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "authors": [
        "L Pepino",
        "P Riera",
        "L Ferrer"
      ],
      "year": "2021",
      "venue": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "arxiv": "arXiv:2104.03502"
    },
    {
      "citation_id": "39",
      "title": "Speech emotion recognition with multi-task learning",
      "authors": [
        "X Cai",
        "J Yuan",
        "R Zheng",
        "L Huang",
        "K Church"
      ],
      "year": "2021",
      "venue": "Speech emotion recognition with multi-task learning"
    },
    {
      "citation_id": "40",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly"
      ],
      "year": "2020",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "arxiv": "arXiv:2010.11929"
    },
    {
      "citation_id": "41",
      "title": "Septr: Separable transformer for audio spectrogram processing",
      "authors": [
        "N Ristea",
        "R Ionescu",
        "F Khan"
      ],
      "venue": "Septr: Separable transformer for audio spectrogram processing",
      "arxiv": "arXiv:2203.09581"
    },
    {
      "citation_id": "42",
      "title": "Differential time-frequency log-mel spectrogram features for vision transformer based infant cry recognition",
      "authors": [
        "H.-T Xu",
        "J Zhang",
        "L.-R Dai"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech 2022"
    },
    {
      "citation_id": "43",
      "title": "Sa-net: Shuffle attention for deep convolutional neural networks",
      "authors": [
        "Q.-L Zhang",
        "Y.-B Yang"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "44",
      "title": "Bam: Bottleneck attention module",
      "authors": [
        "J Park",
        "S Woo",
        "J.-Y Lee",
        "I Kweon"
      ],
      "year": "2018",
      "venue": "Bam: Bottleneck attention module",
      "arxiv": "arXiv:1807.06514"
    },
    {
      "citation_id": "45",
      "title": "Gcnet: Non-local networks meet squeeze-excitation networks and beyond",
      "authors": [
        "Y Cao",
        "J Xu",
        "S Lin",
        "F Wei",
        "H Hu"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF international conference on computer vision workshops"
    },
    {
      "citation_id": "46",
      "title": "Group normalization",
      "authors": [
        "Y Wu",
        "K He"
      ],
      "year": "2018",
      "venue": "Proceedings of the European conference on computer vision (ECCV)"
    },
    {
      "citation_id": "47",
      "title": "Shufflenet v2: Practical guidelines for efficient cnn architecture design",
      "authors": [
        "N Ma",
        "X Zhang",
        "H.-T Zheng",
        "J Sun"
      ],
      "year": "2018",
      "venue": "Proceedings of the European conference on computer vision (ECCV)"
    },
    {
      "citation_id": "48",
      "title": "Cmt: Convolutional neural networks meet vision transformers",
      "authors": [
        "J Guo",
        "K Han",
        "H Wu",
        "Y Tang",
        "X Chen",
        "Y Wang",
        "C Xu"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "49",
      "title": "Coordinate attention for efficient mobile network design",
      "authors": [
        "Q Hou",
        "D Zhou",
        "J Feng"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "50",
      "title": "Patches are all you need?",
      "authors": [
        "A Trockman",
        "J Kolter"
      ],
      "year": "2022",
      "venue": "Patches are all you need?",
      "arxiv": "arXiv:2201.09792"
    },
    {
      "citation_id": "51",
      "title": "Searching for mobilenetv3",
      "authors": [
        "A Howard",
        "M Sandler",
        "G Chu",
        "L.-C Chen",
        "B Chen",
        "M Tan",
        "W Wang",
        "Y Zhu",
        "R Pang",
        "V Vasudevan"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF international conference on computer vision"
    },
    {
      "citation_id": "52",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "53",
      "title": "A database of german emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "Interspeech"
    },
    {
      "citation_id": "54",
      "title": "mixup: Beyond empirical risk minimization",
      "authors": [
        "H Zhang",
        "M Cisse",
        "Y Dauphin",
        "D Lopez-Paz"
      ],
      "year": "2017",
      "venue": "mixup: Beyond empirical risk minimization",
      "arxiv": "arXiv:1710.09412"
    },
    {
      "citation_id": "55",
      "title": "Speech emotion recognition using deep learning on audio recordings",
      "authors": [
        "S Suganya",
        "E Charles"
      ],
      "year": "2019",
      "venue": "2019 19th International Conference on Advances in ICT for Emerging Regions (ICTer)"
    },
    {
      "citation_id": "56",
      "title": "Deep architecture enhancing robustness to noise, adversarial attacks, and cross-corpus setting for speech emotion recognition",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "Deep architecture enhancing robustness to noise, adversarial attacks, and cross-corpus setting for speech emotion recognition",
      "arxiv": "arXiv:2005.08453"
    },
    {
      "citation_id": "57",
      "title": "Speech emotion recognition with dual-sequence lstm architecture",
      "authors": [
        "J Wang",
        "M Xue",
        "R Culhane",
        "E Diao",
        "J Ding",
        "V Tarokh"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "58",
      "title": "Spatiotemporal and frequential cascaded attention networks for speech emotion recognition",
      "authors": [
        "S Li",
        "X Xing",
        "W Fan",
        "B Cai",
        "P Fordson",
        "X Xu"
      ],
      "year": "2021",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "59",
      "title": "Multi-task semi-supervised adversarial autoencoding for speech emotion recognition",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "J Epps",
        "B Schuller"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "60",
      "title": "Metric learning based feature representation with gated fusion model for speech emotion recognition",
      "authors": [
        "Y Gao",
        "J Liu",
        "L Wang",
        "J Dang"
      ],
      "year": "2021",
      "venue": "Interspeech"
    },
    {
      "citation_id": "61",
      "title": "Learning discriminative features from spectrograms using center loss for speech emotion recognition",
      "authors": [
        "D Dai",
        "Z Wu",
        "R Li",
        "X Wu",
        "J Jia",
        "H Meng"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "62",
      "title": "Automated accurate speech emotion recognition system using twine shuffle pattern and iterative neighborhood component analysis techniques",
      "authors": [
        "T Tuncer",
        "S Dogan",
        "U Acharya"
      ],
      "year": "2021",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "63",
      "title": "Exploration of an independent training framework for speech emotion recognition",
      "authors": [
        "S Zhong",
        "B Yu",
        "H Zhang"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "64",
      "title": "Automatic speech emotion recognition using an optimal combination of features based on emd-tkeo",
      "authors": [
        "L Kerkeni",
        "Y Serrestou",
        "K Raoof",
        "M Mbarki",
        "M Mahjoub",
        "C Cleder"
      ],
      "year": "2019",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "65",
      "title": "Multi-view speech emotion recognition via collective relation construction",
      "authors": [
        "M Hou",
        "Z Zhang",
        "Q Cao",
        "D Zhang",
        "G Lu"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "66",
      "title": "Mobilevit: light-weight, generalpurpose, and mobile-friendly vision transformer",
      "authors": [
        "S Mehta",
        "M Rastegari"
      ],
      "year": "2021",
      "venue": "Mobilevit: light-weight, generalpurpose, and mobile-friendly vision transformer",
      "arxiv": "arXiv:2110.02178"
    },
    {
      "citation_id": "67",
      "title": "Separable self-attention for mobile vision transformers",
      "authors": [
        "Sachin Mehta",
        "Mohammad Rastegari"
      ],
      "year": "2022",
      "venue": "Separable self-attention for mobile vision transformers",
      "arxiv": "arXiv:2206.02680"
    },
    {
      "citation_id": "68",
      "title": "Mobilevitv3: Mobile-friendly vision transformer with simple and effective fusion of local, global and input features",
      "authors": [
        "S Wadekar",
        "A Chaurasia"
      ],
      "year": "2022",
      "venue": "Mobilevitv3: Mobile-friendly vision transformer with simple and effective fusion of local, global and input features",
      "arxiv": "arXiv:2209.15159"
    }
  ]
}