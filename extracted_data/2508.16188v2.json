{
  "paper_id": "2508.16188v2",
  "title": "Seeing Is Believing: Emotion-Aware Audio-Visual Language Modeling For Expressive Speech Generation",
  "published": "2025-08-22T08:08:45Z",
  "authors": [
    "Weiting Tan",
    "Jiachen Lian",
    "Hirofumi Inaguma",
    "Paden Tomasello",
    "Philipp Koehn",
    "Xutai Ma"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "We present an Audio-Visual Language Model (AVLM) for expressive speech generation by integrating full-face visual cues into a pre-trained expressive speech model. We explore multiple visual encoders and multimodal fusion strategies during pre-training to identify the most effective integration approach. Subsequent finetuning on emotion recognition and expressive dialogue tasks yields substantial gains over speech-only baselines (e.g., +5 F1 in emotion recognition). AVLM highlights the value of expressive visual information in guiding speech generation and offers a foundation for end-toend multimodal conversational systems. 1 * Work was done during an internship at Meta AI.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Expressive speech generation are supported by high-quality speech tokenization/codec  (Défossez et al., 2022; Jenrungrot et al., 2023; Du et al., 2024; Nguyen et al., 2024) , emotionally-nuanced speech datasets  (Chu et al., 2024; Huang et al., 2025; KimiTeam et al., 2025) , and effective emotionaware representation learning  (Wang et al., 2023; Tang et al., 2024b) . While recent advances in speech modeling have greatly improved the quality and controllability of synthesized speech, most methods rely solely on audio input and overlook the rich expressive cues available in the visual modality.\n\nHuman communication, however, is inherently multimodal. Visual signals-such as facial expressions, head gestures, and eye movements-are tightly intertwined with vocal expression, offering critical paralinguistic information that reflects emotion and intent. This raises a central question: can we enhance expressive speech generation by incorporating these visual cues, enabling the model to produce responses that are not only fluent but also emotionally resonant and contextually adaptive?\n\nTo address this question, we propose developing an emotion-aware audio-visual perceptual model for expressive speech generation. Although prior work has explored lip reading, audio-visual speech recognition (AVSR), and emotion recognition  (Shi et al., 2022; Hong et al., 2023; Han et al., 2024; Yeo et al., 2025a,b; Haliassos et al., 2024; Busso et al., 2008) , these efforts are often task-specific. AVSR methods, for example, improve transcription in noisy settings by focusing on the lip region, but they generally ignore non-semantic yet affective visual signals. Similarly, emotion recognition benchmarks like IEMOCAP  (Busso et al., 2008)  are designed for classification rather than generation, limiting their utility in expressive synthesis tasks. Consequently, current systems fall short in capturing the full emotional dynamics present in audio-visual communication.\n\nIn this work, we propose an Audio-Visual Language Model (AVLM) that integrates full-face visual cues into a pre-trained expressive Speech Language Model (SpeechLM) to support emotionally rich speech generation. Our training follows a twostage framework. First, we introduce a modality fusion module to align visual representations with SpeechLM's latent space, systematically evaluating visual encoders and fusion strategies during pre-training. Second, we fine-tune the AVLM for emotion recognition and dialogue generation using synthetic data derived from IEMOCAP. To construct expressive conversations, we rewrite dialogue responses using  GPT-4 (OpenAI et al., 2024)  and synthesize audio with Step-Audio  (Huang et al., 2025) , an expressive TTS system supporting voice cloning and natural language instruction.\n\nOur approach effectively incorporates a wider range of visual information beyond lip movements. It achieves lower perplexity during pre-training and improves AVSR performance by reducing Word Error Rate (WER) over 1 point under clean and noisy conditions. When fine-tuned for expressive speech generation and emotion recognition, our visually enhanced AVLM consistently outperforms the speech-only counterpart (by more than 5 F1score), demonstrating higher classification accuracy and generating more expressive outputs. In summary, we strengthen the connection between visual and audio modality for expressive generation and show that our novel AVLM could be a strong foundation for building emotionally intelligent endto-end conversational agents.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Audio-Visual Speech Recognition",
      "text": "AVSR leverages both audio and visual modalities for robust speech transcription. Datasets like LRS3  (Afouras et al., 2018) and VoxCeleb2 (Chung et al., 2018)  have driven recent progress. AV-HuBERT  (Shi et al., 2022)  aligns modalities via self-supervised learning, extended to multilingual settings by XLAVS-R  (Han et al., 2024) . Unified-Speech  (Haliassos et al., 2024)  incorporates auxiliary tasks to improve multi-modal alignment, while LLaMa-AVSR  (Cappellazzo et al., 2025)  and  MMS-LLaMa (Yeo et al., 2025b)  employ LLMs for improved transcription quality.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Audio-Visual Emotion Recognition",
      "text": "Multimodal emotion recognition has been supported by datasets such as IEMOCAP  (Busso et al., 2008) , RECOLA  (Ringeval et al., 2013) , CREMA-D  (Cao et al., 2014) , MSP-IMPROV  (Busso et al., 2017) , RAVDESS (Livingstone and Russo, 2018), and Aff-Wild  (Kollias et al., 2019) . Labels range from discrete classes to continuous dimensions like Valence, Arousal, and Dominance  (Mehrabian and Russell, 1974) . Based on these datasets, fusion methods  (Savchenko, 2022; Praveen et al., 2023; Ma et al., 2024a)  have been proposed to improve multimodal emotion recognition, though they are limited to classification or regression settings.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Speech Language Models",
      "text": "Speech Language Models (SpeechLMs) are language models trained on large text and speech datasets, typically pre-trained on text and fine-tuned with speech-text or speech-only data  (Wu et al., 2023; Yu et al., 2023; Tan et al., 2024; Zhang et al., 2023; Tang et al., 2024a; Chu et al., 2024) .\n\nMore recently, directly encoding speech as tokens and integrating them into pre-trained LLMs has gained traction for its scalability and ease of expressive speech synthesis  (Nguyen et al., 2024; Huang et al., 2025; Chen et al., 2025; KimiTeam et al., 2025) .  SpiritLM (Nguyen et al., 2024) , for instance, enhances expressiveness by incorporating style and pitch tokens  (Kharitonov et al., 2022; Duquenne et al., 2023) , while Step-Audio  (Huang et al., 2025)  leverages a linguistic tokenizer trained on Paraformer outputs  (Gao et al., 2022) . The focus on expressivity in these models makes them strong candidates for our visual integration.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Motivate The Integration Of Visual Cues: Audio-Visual Emotion Recognition",
      "text": "Before diving into Expressive Audio-Visual Language Modeling, we first motivate the study by showing why visual cues, besides its semantic correlation to speech in lip-reading, could be useful. We choose the emotion recognition benchmark, IEMOCAP  (Busso et al., 2008) , and compare the performance of existing audio-only baselines and our simple audio-visual classification model. IEMOCAP is made up of videos of two speakers' conversation with strong emotions. We follow prior benchmark EmoBox  (Ma et al., 2024b)  to train and evaluate models with their released data splits 2  . We follow the findings of EmoBox to use Whisper  (Radford et al., 2022)  as the audio encoder. For visual information, we directly encode frames with pre-trained Open-MAGVIT2  (Yu et al., 2024; Luo et al., 2025)   To train our classifier, we encode visual and speech input into features, adapt them through lightweight encoders, and feed them into a Transformer-Encoder model  (Vaswani et al., 2017) . The features are then pooled and passed into a Feedforward network to predict the emotion label. For details of our model setup, please refer to Appendix B. As shown in Table  1 , we compare our classification model with state-of-the-art speech-only models, EmoBox  (Ma et al., 2024b)  and SenseVoice  (An et al., 2024) . We find that, by incorporating visual modality, our model achieves superior performance.\n\nWe perform an ablation study by grouping samples based on the prediction outcomes of each modality. As shown in Table  2 , the fusion model makes more correct predictions via an ensembling mechanism. Notably, only a small number of samples are correctly predicted solely by the fusion model (119) or by a single-modality model alone (38). In most cases, the fusion model aligns with the correct single-modality prediction-646 times with speech-only and 537 times with visual-only model-indicating effective modality selection.\n\nThese findings indicate that the visual modality provides complementary non-semantic information. Effectively leveraging such visual signals holds promise for enhancing the expressiveness and emotional alignment of speech generation systems.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Audio-Visual Language Modeling",
      "text": "To support expressive speech generation, we propose Audio-Visual Language Modeling and explore modality fusion strategies in §4.1. We then discuss our fine-tuning strategy for emotion recognition and expressive speech generation in §4.2.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Avlm Pre-Training",
      "text": "To integrate visual modality into a pre-trained SpeechLM, we leverage self-supervised learning on raw video data (to distinguish this stage from future\n\nwe propose three audio-visual fusion strategies to obtain audiovisual representations h, as visualized in Fig.  2 .\n\nDIRECT CONCAT fuses modalities by simply concatenating the time-aligned audio and visual features, which are then projected through a two-layer feedforward network (FFN). The resulting feature is subsequently fed into SpeechLM to generate the speech token sequence s.\n\nBoth Q-FORMER INFILL and Q-FORMER PRE-FIX leverage the Q-Former module  (Li et al., 2023) , which uses a set of query latents to retrieve relevant information from the visual stream. These query latents are initialized as learnable parameters and refined via a cross-attention mechanism that attends to the visual features.\n\nFormally, given a visual stream v and a sequence of query latents q = (q 1 , q 2 , • • • , q |q| ), we first apply sinusoidal positional embeddings to the query latents. They are then passed through a cross-attention module, where each layer contains learnable pro-\n\nHere, d is the shared dimensionality of the query, visual, and speech representation spaces. The query representation is computed using the standard cross-attention formula:\n\nFor Q-FORMER INFILL, we set the number of query latents to match the length of the speech sequence, i.e., |z| = |q| = |s|, and randomly replace a portion of the speech representations with query latents. This approach is motivated by AV-HuBERT  (Shi et al., 2022) , where modality dropout encourages the model to learn robust audio-visual semantic alignment. In Q-FORMER PREFIX, we dynamically determine the number of query latents based on the number of visual frames and use them as a prefix prepended to the speech representations.  4  To ensure the model attends to the visual input, we additionally apply attention masking to some speech positions (see §5.1 for details).\n\nTo summarize, the audio-visual representation h is obtained with the following equations:\n\nwhere r i is 1 at position i where we replace the speech with a visual representation and 0 otherwise.\n\n• denotes concatenation along feature dimension.\n\nSubsequently, our AVLM is pre-trained with the negative log-likelihood (NLL) loss where parameters of the SpeechLM (θ lm ) and fusion module (ϕ fusion ) are both updated.\n\nIn our experiments, we find that Q-FORMER PRE-FIX achieves the best performance. Therefore, we adopt the Q-FORMER PREFIX AVLM as the base model for fine-tuning in the next section.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Avlm Fine-Tuning For Emotion Recognition And Expressive Generation",
      "text": "Given the pre-trained AVLM from §5.1, we finetune it on expressive audio-visual conversations annotated with an emotion label. The dataset is derived from IEMOCAP, providing us with an input audio-visual pair, an emotion label, and the output speech. The construction details are shown in the experiment sections and Appendix A, and some example data entries are provided in Table  9 . As shown in Fig.  3  and colorbox above, we use a multimodal prompt for expressive speech generation. Specifically, we extract visual query representations z = {z 1 , • • • , z N } from the input visual features and use them as a prefix to the input speech tokens s = {s 1 , • • • , s T }, with N < T due to the compression from Q-FORMER. We then insert the emotion label e to guide generation, followed by the target response speech tokens y = {y 1 , • • • , y M }.\n\nTo fine-tune the model, we minimize the negative log-likelihood (NLL) of the target response tokens y = {y 1 , . . . , y M }, conditioned on the multimodal prompt comprising the instruction  z, input speech s, and emotion label e:\n\n) Here, ∥ denotes the concatenation of input components along the sequence dimension.\n\nAuxiliary Emotion Recognition Task For expressive conversation synthesis using IEMOCAP, we ensure that both input and response speech share the same emotional tone. This is possible as IEMO-CAP uses scripted dialogue where speakers of each conversation share similar emotions.\n\nSince autoregressive emotion prediction is unreliable-due to limited data and hallucination issues of decoder-only models-we introduce an auxiliary emotion classification objective. As illustrated in Fig.  3 , we extract hidden states corresponding to (1) visual query representations and (2) style/pitch tokens from the SpiritLM-Expressive Tokenizer (details in §5.3). These states are passed through two feedforward networks, concatenated, pooled, and classified using a cross-entropy loss. A stopgradient is applied to prevent interference with AVLM training. During training, the true emotion label is used while at inference, the emotion label is predicted by the classifier.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Experiments",
      "text": "We begin with AVLM pre-training experiments ( §5.1), comparing different visual encoders and modality fusion strategies. Next, we fine-tune the AVLM on AVSR tasks ( §5.2) to validate the ef-fectiveness of our pre-training. Finally, we finetune AVLM for emotion recognition and expressive speech generation ( §5.3), achieving superior performance over SpeechLM by utilizing visual cues.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Avlm Pre-Training",
      "text": "Dataset and Preprocessing We use the LRS3 dataset  (Afouras et al., 2018) , which consists of 433 hours of transcribed audio-visual data, including a 30-hour clean subset. For pre-training, we utilize only the videos from the full 433-hour dataset.\n\nWe adopt the data pre-processing pipeline from MuAViC  (Anwar et al., 2023) , which segments videos into aligned audio-visual chunks of up to 15 seconds based on transcriptions. Speech tokens are extracted using SpiritLM's tokenizer, which produces interleaved tokens of three types: semantic tokens (24.99 fps), style tokens (1 fps), and pitch tokens (12.5 fps). The semantic tokens are discretized HuBERT representations  (Hsu et al., 2021) , the pitch tokens are derived from a VQ-VAE model trained on the F0 of speech  (Polyak et al., 2021) , and the style tokens are based on speechprop features  (Duquenne et al., 2023) . For further details, we refer readers to  Nguyen et al. (2024) .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Model Implementation Details",
      "text": "We compare three visual encoders: (1) Open-MAGVIT2  (Yu et al., 2024) , (2) VGG-Face2  (Cao et al., 2018) , and (3) SMIRK  (Retsinas et al., 2024) . Open-MAGVIT2 is a capable but expensive encoder as it is trained for image reconstruction. VGG-Face2 is more lightweight and focuses on facial regions through tasks like face recognition. SMIRK is a lightweight neural feature learned through 3D re- construction based on FLAME  (Li et al., 2017) .\n\nWe extract visual features using different encoders, adapt them with lightweight modules, and integrate them into the Q-FORMER for AVLM pre-training (visual encoder comparison details in Appendix C). For all three fusion modules, we use the same base model, SpiritLM, and apply LoRA fine-tuning  (Hu et al., 2021)  to the query, key, value, and output projection matrices, with r = 16, α = 32, and a dropout rate of 0.05. For Q-FORMER INFILL, we replace 50% of the speech tokens with visual query representations. For Q-FORMER PREFIX, we drop attention over a certain percentage of speech tokens (from 0% to 70% as shown in Table  4 ) to see if such masking helps AVLM become more robust. The details and hyperparameters of our three fusion architectures are provided in Appendix D. Results We evaluate pre-training performance on the LRS3 test set using perplexity (PPL). First, fixing the fusion method to Q-FORMER PREFIX, we ablate visual encoders. As shown in Table  3 , SMIRK achieves the lowest PPL, likely due to its disentangled expressive and jaw features being easier for the model to pick up. We therefore adopt SMIRK as our default visual encoder for future fine-tuning experiments.\n\nNext, using SMIRK, we compare fusion strategies. Q-FORMER PREFIX yields the lowest PPL, outperforming both SPEECH-ONLY and other fusion methods. In contrast, DIRECT CONCAT performs worst, suggesting that the direct concatenation and projection of audio-visual features complicate the model's adaptation to the representations.\n\nAmong Q-Former variants, Q-FORMER PRE-FIX consistently performs best. We further evaluate robustness by applying attention masking at inference (10-70%) and training additional Q-FORMER PREFIX variants with various attention masking ratios (Table  4 ). A 30% speech token dropout from attention computation during training yields the best overall results. Thus, we use the Q-FORMER PREFIX model trained with 30% masking as pre-trained base model for all fine-tuning tasks, including AVSR and expressive generation.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Avsr Fine-Tuning",
      "text": "To further evaluate our pre-trained AVLM, we finetune it on the Audio-Visual Speech Recognition (AVSR) task using the 30-hour clean subset of LRS3 and report Word Error Rate (WER). We compare against two baselines: (1) a SPEECH-ONLY Spir-itLM model directly fine-tuned on the same 30-hour subset (using speech prefix and text continuation), and (2) an AVLM model with the same architecture as the pre-trained one but fine-tuned from scratch. AVSR serves as an auxiliary benchmark to assess the quality of visual integration beyond perplexity (used during pre-training). Due to space constraints, we omit modeling details, which largely mirror those in expressive generation ( §4.2) by using text transcriptions rather than response speech as targets. Full implementation details are in Appendix E.\n\nWe evaluate all models under both clean and noisy conditions. We introduce noise using two methods: (1) SNR noise injection and (2) attention masking. For SNR noise injection, we follow previous studies  (Yeo et al., 2025b)  by adding white noise to create test audio with varying Signal-to-Noise Ratios (SNR). For attention masking, we randomly mask a certain percentage of speech tokens during attention computation. We also compare our results with previous topperforming models in Table  6 . Although models like  MMS-LLaMa (Yeo et al., 2025b)  and LLaMa-AVSR  (Cappellazzo et al., 2025)  report lower WER, the differences primarily stem from our choice of base model and visual features, rather than the modeling approach itself.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "As Demonstrated In",
      "text": "Both MMS-LLaMa and LLaMa-AVSR utilize audio encoders like Whisper and focus on visual features from the lip region, limiting their models to the AVSR task. In contrast, our base model, SpiritLM, supports expressive speech generation by encoding speech into units, which inevitably loses information from speech tokenization and result in higher WER. Moreover, our visual features encompass the full face, not just the lip region, to facilitate expressive generation but at the cost of less effective semantic alignment for tasks like AVSR.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Emotion Recognition And Expressive Speech Generation",
      "text": "Dataset Following the approach in §4.2, we fine-tune our pre-trained AVLM using multimodal prompts consisting of visual input, input and response audio, and an emotion label. The dataset is built from IEMOCAP  (Busso et al., 2008) , which provides expressive video dialogues and emotion labels. However, many original responses are very short (e.g., acknowledgments like \"yeah\"). To address this, we use GPT-4 (OpenAI et al., 2024) 5  to rewrite conversations into longer, more detailed responses. We then generate corresponding audio 5 We use the \"gpt-4o-2024-11-20\" snapshot using\n\nStep-Audio-TTS-3B  (Huang et al., 2025) , an expressive TTS model supporting voice cloning and style control. Additional dataset construction details are provided in Appendix A.\n\nModel Implementation We initialize our model from the pre-trained AVLM ( §5.1) using the Q-FORMER PREFIX architecture with the SMIRK visual encoder, and fine-tune it with LoRA using the same hyperparameters as pre-training. The emotion classifier is trained separately on hidden states from visual queries and style/pitch tokens posii ( §4.2). For the speech-only baseline, we fine-tune a pre-trained SpiritLM model with LoRA, excluding visual inputs. Its emotion classifier uses only the style and pitch token hidden states.\n\nEvaluation For emotion recognition, we evaluate performance by comparing the predicted and ground truth emotion labels, computing both accuracy and F1-score for four emotion categories: Happy, Sad, Angry, and Neutral. For expressive speech generation, we use fine-tuned model to generate speech tokens and synthesize speech through SpiritLM's Tokenizer. Subsequently, we use a thirdparty model, Qwen2-Audio  (Chu et al., 2024) , which excels at audio understanding and emotion recognition, to predict emotion labels from the generated speech (see Appendix D for details). We then compute accuracy and F1-score for the same four-way classification. For reference, we have also uploaded audio of some generated responses in our ARR supplementary material. AVLM model consistently surpasses the SPEECH-ONLY baseline, highlighting the benefit of visual guidance. In Fig.  4 , we further break down the F1scores by emotion category, observing a similar trend where AVLM always outperforms its speechonly counterpart. Among the emotions, Neutral and Sad are generally harder to predict than Happy and Angry, likely because they convey weaker emotional cues. On the contrary, Happy and Angry are associated with stronger facial expressions and higher vocal arousal, making it easier for the model to predict or generate speech of similar properties.",
      "page_start": 1,
      "page_end": 8
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "As Shown In",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Emotion Controllability Analysis",
      "text": "We investigated whether the emotion of generated speech could be controlled by modifying the predicted emotion label in the prompt. For example, if the original predicted emotion is \"angry\", we manually change it to \"happy\" and then evaluate the emotion of the newly generated speech. We measure the number of instances where the generated emotion successfully shifts from the original to the altered label and visualize the results in a heatmap. Ideally, we expect our AVLM to demonstrate strong controllability, generating speech that appropriately reflects the emotion specified in the prompt.\n\nAs shown in Fig.  5 , simply changing the emotion label in the prompt-without using in-context learning-rarely alters the emotional tone of the generated speech. This suggests that the model primarily relies on input speech and visual cues to guide emotion, rather than the emotion label itself. This behavior is expected, given the limited training data and the fact that input and response audio in our dataset are designed to share the same emotion.\n\nTo enhance controllability, we introduce incontext learning by including one demonstration per emotion in the prompt (see example in  Appendix D) . With this addition, AVLM becomes more responsive to changes in the emotion label, as shown in the right panel of Fig.  5 .\n\nThese findings indicate there is still large room for improving the emotion controllability, as the model after fine-tuning is not able to easily alter the speech style by conditioning on different emotion labels. We believe that enhancing data quality by creating larger and more diverse expressive audiovisual dialogue is crucial to addressing this issue, and we plan to explore this in future work.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "We present a framework for Audio-Visual Language Modeling that effectively incorporates visual cues into SpeechLM. Following empirical exploration of visual encoders and modeling architectures, we employ a prefix-based Query-Former with SMIRK visual features for AVLM. Our system, through leveraging visual modality, outperforms speechonly baselines in both Audio-Visual Speech Recognition and Expressive Speech Generation.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Limitations And Broader Impact",
      "text": "One limitation of our work is the limited availability of expressive audio-visual dialogue data for finetuning the pre-trained AVLM. Although we synthesize part of our dataset, the construction pipeline is based on IEMOCAP, which contains only a few hours of recordings. Collecting more diverse and expressive dialogue data could further enhance the controllability and quality of the generated speech.\n\nAnother limitation is that our evaluation primarily focuses on the emotional expressiveness of the generated speech, reflecting our goal of modeling emotion-aware speech generation from audio-visual inputs. While we demonstrate that visual cues can significantly improve emotional alignment, other aspects of speech quality-such as helpfulness, factual accuracy, and coherence-are not evaluated in this study. Finally, the decoding process in our Q-FORMER PREFIX architecture requires processing the visual stream before the speech stream, which can introduce latency and leave room for further efficiency improvements.\n\nEthical Consideration: While our work advances the generation of emotionally expressive and natural-sounding speech, it also raises potential risks of misuse. Specifically, models capable of synthesizing human-like emotional speech may be exploited for deceptive purposes, such as impersonation, social engineering scams, or the spread of misinformation. To mitigate such concerns, we encourage future work to incorporate safeguards such as synthetic speech watermarking, controlled access to fine-tuned models, and explicit labeling of machine-generated content. Responsible deployment and continuous risk assessment are essential as these technologies mature.\n\nOur experiments are conducted on publicly avail-able datasets under appropriate licenses. The LRS3 dataset is released under the Creative Commons Attribution 4.0 International License and is available for research purposes. The IEMOCAP dataset is also released for academic research use under its license agreement. We use both datasets in accordance with their respective terms to ensure compliance with data usage policies. The synthesized artifacts for fine-tuning are not currently distributed to avoid potential misuse and to respect the licensing constraints from IEMOCAP  (Busso et al., 2008) .\n\nWe are planning to release the dataset if permission is granted from the IEMOCAP data provider.\n\nWe believe responsible handling of such synthetic content is critical to ensuring ethical standards in expressive speech research. We also acknowledge the use of AI assistants (e.g., GitHub Copilot, ChatGPT) to support the development of our research. These tools were used to assist with code implementation, debugging, documentation, and drafting or refining written content. All substantive research contributions, model design decisions, and experimental results were generated and verified by the authors. We ensured that the use of AI tools complied with academic integrity standards and that any generated content was critically reviewed and edited to maintain originality and correctness.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "B Iemocap Emotion Recognition Experiment Setup",
      "text": "In §3, we demonstrate the benefit of incorporating visual modality for emotion recognition. Below, we detail our experimental setup:\n\nWe follow the data split protocol from EmoBox  (Ma et al., 2024b) , using the publicly available splits at https://github.com/emo-box/EmoBox/tree/main/data/iemocap. Visual frames are encoded using the Open-MAGVIT2 encoder, while speech signals are processed with Whisper-Large-V3. A lightweight adaptation module is then applied to map both speech and visual features into 512-dimensional representations for each frame.\n\nTo adapt the visual features from Open-MAGVIT2, which produce tensors of shape (Z = 18, H = 32, W = 32), we stack three residual blocks, each consisting of a normalization layer followed by a 2D convolutional neural network (CNN). The resulting output is flattened and projected into a 512-dimensional feature vector. Speech features are adapted in a similar manner, but using 1D-CNNs instead of 2D-CNNs.\n\nThe adapted speech and visual features are then concatenated and fed into a Transformer encoder consisting of 6 layers with a hidden size of 512. We experimented with concatenating along both the temporal and feature dimensions but observed no significant difference in performance. Therefore, we adopt feature-dimension concatenation to keep the sequence length shorter. The resulting features are mean-pooled and passed through a two-layer feedforward network to predict the emotion label. We train the model until convergence, using a learning rate of 3 × 10 -5 , a batch size of 16, and gradient accumulation over 4 steps.\n\nTo evaluate classifier performance, we follow prior work  (Ma et al., 2024b; An et al., 2024)  and report the unweighted average accuracy (UA), weighted average accuracy (WA), and macro F1 score, as shown in Table  1 . The UA, WA, and F1 metrics are computed using the accuracy_score and balanced_accuracy_score functions from the sklearn.metrics package.",
      "page_start": 15,
      "page_end": 16
    },
    {
      "section_name": "C Visual Encoder Comparison",
      "text": "We initially selected MAGVIT2 (using the public checkpoint from Open-MAGVIT2) for its rich visual representations. However, MAGVIT2 encodes each 256 × 256 frame into a high-dimensional feature of size 18 × 32 × 32 = 18,432, with 18 channels and spatial dimensions downsampled by a factor of 8. This high dimensionality introduces significant computational overhead and poses learning challenges, particularly given the limited size of our dataset (fewer than 500 hours from LRS3). To mitigate these issues, we explored more lightweight visual encoders: VGG-Face2 and SMIRK.\n\nVGG-Face2, trained for face recognition, encodes each frame into a 512-dimensional vector. SMIRK, trained for 3D face reconstruction using FLAME  (Li et al., 2017) , disentangles facial attributes into components such as shape, expression, jaw, eyelid, pose, and camera parameters. Among these, we use only the expressive (55-dimensional) and jaw (3-dimensional) features, which are most relevant for capturing emotion and lip movements.\n\nSince the outputs of these visual encoders differ in shape, we design specialized adapters before feeding the features into the Q-FORMER. For MAGVIT2, we follow the setup in Appendix B, stacking 2D convolutional layers with residual connections to reduce the spatial dimensions and project the encoding into a 512-dimensional feature.\n\nFor VGG-Face2, whose outputs are already 512-dimensional, we apply a simple two-layer feedforward network that first expands the feature to 1024 dimensions before projecting it back to 512 dimensions. For SMIRK, we linearly transform the expressive parameters into a 128-dimensional vector and the jaw parameters into a 32-dimensional vector. These are concatenated and passed through a two-layer feedforward network to produce a 256-dimensional visual representation.\n\nSince SMIRK features can sometimes be distorted, we leverage its pose parameters to filter out video clips with excessive horizontal head rotation. Let the predicted pose vector be r pose = (r x , r y , r z ), initially represented as a rotation vector. We convert it into a rotation matrix R using Rodrigues' rotation formula: R = I + sin(θ)K + (1 -cos(θ))K 2\n\n(5)",
      "page_start": 15,
      "page_end": 16
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Our Audio-Visual Language Model (AVLM)",
      "page": 1
    },
    {
      "caption": "Figure 2: DIRECT CONCAT fuses modalities by simply",
      "page": 3
    },
    {
      "caption": "Figure 2: Illustration of the modality fusion strategies explored in the AVLM pre-training stage. The fusion module",
      "page": 4
    },
    {
      "caption": "Figure 3: and colorbox above, we use",
      "page": 4
    },
    {
      "caption": "Figure 3: AVLM Multi-task Fine-tuning Objectives. The AVLM receives a multimodal input prompt and generates a",
      "page": 5
    },
    {
      "caption": "Figure 3: , we extract hidden states corresponding to",
      "page": 5
    },
    {
      "caption": "Figure 4: F1-score of each emotion class for AVLM and SPEECH-ONLY model.",
      "page": 8
    },
    {
      "caption": "Figure 4: , we further break down the F1-",
      "page": 8
    },
    {
      "caption": "Figure 5: , simply changing the emo-",
      "page": 8
    },
    {
      "caption": "Figure 5: Heatmaps of emotion controllability analysis",
      "page": 8
    },
    {
      "caption": "Figure 5: These findings indicate there is still large room",
      "page": 8
    },
    {
      "caption": "Figure 6: The instruction-tuning prompt used for AVSR adapts accordingly to reflect this new objective. For",
      "page": 17
    },
    {
      "caption": "Figure 6: Audio-Visual Speech Recognition Fine-tuning based on pre-trained AVLM model.",
      "page": 18
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Instruction-Tuning Prompt": "<Text\nInstruction>\nPerceive\nthe\n(x1, x2, · · · ):\ngiven\nvisual\nand\naudio\ninput.\nDetermine\nthe\nemotion\nof\nthe\nspeaker\nand\ncontinue\nthe\ndialogue with the same emotion.\n<Visual Input>: z1, z2, · · ·\n, zN\n<Input\nAudio>: s1, s2, · · ·\n, sT\nAngry,\n...)\n<Emotion>: e (Happy,\n<Speech\nResponse>: y1, y2, · · ·\n, yM"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4": "7",
          "6": "2"
        },
        {
          "4": "",
          "6": "2"
        },
        {
          "4": "6",
          "6": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "64\n19\n67\n22\n25": "39\n40",
          "33": "28",
          "35\n48\n18": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Conversation Rewriting Prompt": "<Instruction>:\nYour\ntask\nis\nto\ncontinue\nthe\nfollowing\nconversation\nnaturally,\ntaking\ninto\naccount\nboth\nthe\nconversation\nhistory\nand\nthe\nprovided\nemotional\ntone.\nYou\nwill\ngenerate\na\nresponse\nfrom\nthe\nperspective\nof\na\n[speaker_gender]\nspeaker,\nensuring\nthat\nyour\ntone\nreflects\na\n[emotion]\nemotion.\nAn\nexample\nresponse\nis\nalso\nprovided\nfor\nreference;\nif\nit\nis\nvery\nshort,\nplease\nexpand\nit\nto\none or two sentences.\n<Conversation History>:\n[Previous\nConversation]\n<Current\nSpeaker’s Emotion>: [Emotion\nLabel]\n<Example\nResponse>:\n[Original Response\nText]\nNow,\ngenerate\na\nnatural\nand\nconcise\nresponse\nthat\ncontinues\nthe\ndialogue\nusing\nall\nof\nthe\nabove\ninformation.\nKeep\nyour answer\nto\napproximately\ntwo\nsentences.\nOutput\nthe\nresponse\nonly."
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Zero-Shot Prompt": "<Text Instruction>: Perceive the given visual and audio input. Determine the emotion of the speaker\nand\ncontinue\nthe dialogue\nwith\nthe\nsame\nemotion.\n<Visual\nInput>: z1, z2, · · ·\n, zN\n<Input\nAudio>: s1, s2, · · ·\n, sT"
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "In-context Learning Prompt": "<Text Instruction>: Perceive the given visual and audio input. Determine the emotion of the speaker\nand\ncontinue\nthe dialogue\nwith\nthe\nsame\nemotion.\nBelow\nare some\nexamples:\n<Audio-Visual Conversation Input>:\nvisual\nqueries;input\nspeech\nunits\n<Emotion>:\nhappy\n<Response>: response speech\nunits\n<Audio-Visual Conversation Input>:\nvisual\nqueries;input\nspeech\nunits\n<Emotion>:\nsad\n<Response>: response speech\nunits\n<Audio-Visual Conversation Input>:\nvisual\nqueries;input\nspeech\nunits\n<Emotion>:\nangry\n<Response>: response speech\nunits\n<Audio-Visual Conversation Input>:\nvisual\nqueries;input\nspeech\nunits\n<Emotion>:\nneutral\n<Response>: response speech\nunits\nNow\ncontinue\nthe following Audio-Visual\nconversation\ninput with\nspecified\nemotion\n<Visual\nInput>: z1, z2, · · ·\n, zN\n<Input\nAudio>: s1, s2, · · ·\n, sT\n<Emotion>:\n{emotion label}\n<Response>:"
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Instruction-Tuning Prompt": "<Text\nInstruction>: Transcribe\nthe\nfollowing\naudio\nvisual clip:\n<Visual\nInput>: z1, z2, · · ·\n, zN\n<Audio\nInput>: s1, s2, · · ·\n, sT\n<Transcription>:\n(some text tokens)"
        }
      ],
      "page": 17
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Lrs3-ted: a large-scale dataset for visual speech recognition",
      "authors": [
        "Triantafyllos Afouras",
        "Son Chung",
        "Andrew Zisserman"
      ],
      "year": "2018",
      "venue": "Lrs3-ted: a large-scale dataset for visual speech recognition",
      "arxiv": "arXiv:1809.00496"
    },
    {
      "citation_id": "2",
      "title": "Funaudiollm: Voice understanding and generation foundation models for natural interaction between humans and llms",
      "authors": [
        "Qian Keyu An",
        "Chong Chen",
        "Zhihao Deng",
        "Changfeng Du",
        "Zhifu Gao",
        "Yue Gao",
        "Ting Gu",
        "Hangrui He",
        "Kai Hu",
        "Shengpeng Hu",
        "Yabin Ji",
        "Zerui Li",
        "Heng Li",
        "Haoneng Lu",
        "Xiang Luo",
        "Bin Lv",
        "Ziyang Ma",
        "Chongjia Ma",
        "Ni"
      ],
      "year": "2024",
      "venue": "Funaudiollm: Voice understanding and generation foundation models for natural interaction between humans and llms",
      "arxiv": "arXiv:2407.04051"
    },
    {
      "citation_id": "3",
      "title": "Muavic: A multilingual audio-visual corpus for robust speech recognition and robust speech-to-text translation",
      "authors": [
        "Mohamed Anwar",
        "Bowen Shi",
        "Vedanuj Goswami",
        "Wei-Ning Hsu",
        "Juan Pino",
        "Changhan Wang"
      ],
      "year": "2023",
      "venue": "Muavic: A multilingual audio-visual corpus for robust speech recognition and robust speech-to-text translation",
      "arxiv": "arXiv:2303.00628"
    },
    {
      "citation_id": "4",
      "title": "Iemocap: interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Emily Ebrahim (abe) Kazemzadeh",
        "Samuel Provost",
        "Jeannette Kim",
        "Sungbok Chang",
        "Shrikanth Lee",
        "Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "5",
      "title": "Msp-improv: An acted corpus of dyadic interactions to study emotion perception",
      "authors": [
        "Carlos Busso",
        "Srinivas Parthasarathy",
        "Alec Burmania",
        "Mohammed Abdelwahab",
        "Najmeh Sadoughi",
        "Emily Provost"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2016.2515617"
    },
    {
      "citation_id": "6",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "Houwei Cao",
        "David Cooper",
        "Michael Keutmann",
        "Ruben Gur",
        "Ani Nenkova",
        "Ragini Verma"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2014.2336244"
    },
    {
      "citation_id": "7",
      "title": "Vggface2: A dataset for recognising faces across pose and age",
      "authors": [
        "Qiong Cao",
        "Li Shen",
        "Weidi Xie",
        "M Omkar",
        "Andrew Parkhi",
        "Zisserman"
      ],
      "year": "2018",
      "venue": "Vggface2: A dataset for recognising faces across pose and age",
      "arxiv": "arXiv:1710.08092"
    },
    {
      "citation_id": "8",
      "title": "Large language models are strong audio-visual speech recognition learners",
      "authors": [
        "Umberto Cappellazzo",
        "Minsu Kim",
        "Honglie Chen",
        "Pingchuan Ma",
        "Stavros Petridis",
        "Daniele Falavigna",
        "Alessio Brutti",
        "Maja Pantic"
      ],
      "year": "2025",
      "venue": "Large language models are strong audio-visual speech recognition learners",
      "arxiv": "arXiv:2409.12319"
    },
    {
      "citation_id": "9",
      "title": "others. 2025. Minmo: A multimodal large language model for seamless voice interaction",
      "authors": [
        "Qian Chen",
        "Yafeng Chen",
        "Yanni Chen",
        "Mengzhe Chen",
        "Yingda Chen",
        "Chong Deng",
        "Zhihao Du",
        "Ruize Gao",
        "Changfeng Gao",
        "Zhifu Gao",
        "Yabin Li",
        "Xiang Lv",
        "Jiaqing Liu",
        "Haoneng Luo",
        "Bin Ma",
        "Chongjia Ni",
        "Xian Shi",
        "Jialong Tang",
        "Hui Wang"
      ],
      "venue": "others. 2025. Minmo: A multimodal large language model for seamless voice interaction",
      "arxiv": "arXiv:2501.06282"
    },
    {
      "citation_id": "10",
      "title": "Qwen2-audio technical report",
      "authors": [
        "Yunfei Chu",
        "Jin Xu",
        "Qian Yang",
        "Haojie Wei",
        "Xipin Wei",
        "Zhifang Guo",
        "Yichong Leng",
        "Yuanjun Lv",
        "Jinzheng He",
        "Junyang Lin",
        "Chang Zhou",
        "Jingren Zhou"
      ],
      "year": "2024",
      "venue": "Qwen2-audio technical report",
      "arxiv": "arXiv:2407.10759"
    },
    {
      "citation_id": "11",
      "title": "Voxceleb2: Deep speaker recognition",
      "authors": [
        "Son Joon",
        "Arsha Chung",
        "Andrew Nagrani",
        "Zisserman"
      ],
      "year": "2018",
      "venue": "Interspeech 2018",
      "doi": "10.21437/Interspeech.2018-1929"
    },
    {
      "citation_id": "12",
      "title": "Cosyvoice 2: Scalable streaming speech synthesis with large language models",
      "authors": [
        "Zhihao Du",
        "Yuxuan Wang",
        "Qian Chen",
        "Xian Shi",
        "Xiang Lv",
        "Tianyu Zhao",
        "Zhifu Gao",
        "Yexin Yang",
        "Changfeng Gao",
        "Hui Wang",
        "Fan Yu",
        "Huadai Liu",
        "Zhengyan Sheng",
        "Yue Gu",
        "Chong Deng",
        "Wen Wang",
        "Shiliang Zhang",
        "Zhijie Yan",
        "Jingren Zhou"
      ],
      "year": "2024",
      "venue": "Cosyvoice 2: Scalable streaming speech synthesis with large language models",
      "arxiv": "arXiv:2412.10117"
    },
    {
      "citation_id": "13",
      "title": "SONAR EXPRESSIVE: Zero-shot Expressive Speech-to-Speech Translation",
      "authors": [
        "Paul-Ambroise Duquenne",
        "Kevin Heffernan",
        "Alexandre Mourachko",
        "Benoît Sagot",
        "Holger Schwenk"
      ],
      "year": "2023",
      "venue": "SONAR EXPRESSIVE: Zero-shot Expressive Speech-to-Speech Translation"
    },
    {
      "citation_id": "14",
      "title": "High fidelity neural audio compression",
      "authors": [
        "Alexandre Défossez",
        "Jade Copet",
        "Gabriel Synnaeve",
        "Yossi Adi"
      ],
      "year": "2022",
      "venue": "High fidelity neural audio compression",
      "arxiv": "arXiv:2210.13438"
    },
    {
      "citation_id": "15",
      "title": "Paraformer: Fast and accurate parallel transformer for non-autoregressive end-to-end speech recognition",
      "authors": [
        "Zhifu Gao",
        "Shiliang Zhang",
        "Ian Mcloughlin",
        "Zhijie Yan"
      ],
      "year": "2022",
      "venue": "23rd Annual Conference of the International Speech Communication Association, Interspeech 2022",
      "doi": "10.21437/INTERSPEECH.2022-9996"
    },
    {
      "citation_id": "16",
      "title": "Unified speech recognition: A single model for auditory, visual, and audiovisual inputs",
      "authors": [
        "Alexandros Haliassos",
        "Rodrigo Mira",
        "Honglie Chen",
        "Zoe Landgraf",
        "Stavros Petridis",
        "Maja Pantic"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "17",
      "title": "XLAVS-R: Cross-lingual audio-visual speech representation learning for noise-robust speech perception",
      "authors": [
        "Hyojung Han",
        "Mohamed Anwar",
        "Juan Pino",
        "Wei-Ning Hsu",
        "Marine Carpuat",
        "Bowen Shi",
        "Changhan Wang"
      ],
      "year": "2024",
      "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2024.acl-long.697"
    },
    {
      "citation_id": "18",
      "title": "Watch or listen: Robust audiovisual speech recognition with visual corruption modeling and reliability scoring",
      "authors": [
        "Joanna Hong",
        "Minsu Kim",
        "Yun Choi",
        "Yong Ro"
      ],
      "year": "2023",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "19",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "Wei-Ning Hsu",
        "Benjamin Bolte",
        "Hubert Yao-Hung",
        "Kushal Tsai",
        "Ruslan Lakhotia",
        "Abdelrahman Salakhutdinov",
        "Mohamed"
      ],
      "year": "2021",
      "venue": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "arxiv": "arXiv:2106.07447"
    },
    {
      "citation_id": "20",
      "title": "Lora: Low-rank adaptation of large language models",
      "authors": [
        "Edward Hu",
        "Yelong Shen",
        "Phillip Wallis",
        "Zeyuan Allen-Zhu",
        "Yuanzhi Li",
        "Shean Wang",
        "Lu Wang",
        "Weizhu Chen"
      ],
      "year": "2021",
      "venue": "Lora: Low-rank adaptation of large language models",
      "arxiv": "arXiv:2106.09685"
    },
    {
      "citation_id": "21",
      "title": "Zheng Gong, Zixin Zhang, and 126 others. 2025. Step-audio: Unified understanding and generation in intelligent speech interaction",
      "authors": [
        "Ailin Huang",
        "Boyong Wu",
        "Bruce Wang",
        "Chao Yan",
        "Chen Hu",
        "Chengli Feng",
        "Fei Tian",
        "Feiyu Shen",
        "Jingbei Li",
        "Mingrui Chen",
        "Peng Liu",
        "Ruihang Miao",
        "Wang You",
        "Xi Chen",
        "Xuerui Yang",
        "Yechang Huang",
        "Yuxiang Zhang"
      ],
      "venue": "Zheng Gong, Zixin Zhang, and 126 others. 2025. Step-audio: Unified understanding and generation in intelligent speech interaction",
      "arxiv": "arXiv:2502.11946"
    },
    {
      "citation_id": "22",
      "title": "Lmcodec: A low bitrate speech codec with causal transformer models",
      "authors": [
        "Teerapat Jenrungrot",
        "W Michael Chinen",
        "Jan Bastiaan Kleijn",
        "Zalán Skoglund",
        "Neil Borsos",
        "Marco Zeghidour",
        "Tagliasacchi"
      ],
      "year": "2023",
      "venue": "Lmcodec: A low bitrate speech codec with causal transformer models",
      "arxiv": "arXiv:2303.12984"
    },
    {
      "citation_id": "23",
      "title": "Text-free prosodyaware generative spoken language modeling",
      "authors": [
        "Eugene Kharitonov",
        "Ann Lee",
        "Adam Polyak",
        "Yossi Adi",
        "Jade Copet",
        "Kushal Lakhotia",
        "Anh Tu",
        "Morgane Nguyen",
        "Abdelrahman Riviere",
        "Emmanuel Mohamed",
        "Wei-Ning Dupoux",
        "Hsu"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2022.acl-long.593"
    },
    {
      "citation_id": "24",
      "title": "others. 2025. Kimi-audio technical report",
      "authors": [
        "Ding Kimiteam",
        "Zeqian Ding",
        "Yichong Ju",
        "Songxiang Leng",
        "Tong Liu",
        "Zeyu Liu",
        "Kai Shang",
        "Wei Shen",
        "Xu Song",
        "Heyi Tan",
        "Zhengtao Tang",
        "Chu Wang",
        "Yifei Wei",
        "Xinran Xin",
        "Jianwei Xu",
        "Yutao Yu",
        "Xinyu Zhang",
        "Y Zhou",
        "Charles"
      ],
      "venue": "others. 2025. Kimi-audio technical report",
      "arxiv": "arXiv:2504.18425"
    },
    {
      "citation_id": "25",
      "title": "Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "Mihalis Nicolaou",
        "Athanasios Papaioannou",
        "Guoying Zhao",
        "Björn Schuller",
        "Irene Kotsia",
        "Stefanos Zafeiriou"
      ],
      "year": "2019",
      "venue": "International Journal of Computer Vision",
      "doi": "10.1007/s11263-019-01158-4"
    },
    {
      "citation_id": "26",
      "title": "Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models",
      "authors": [
        "Junnan Li",
        "Dongxu Li",
        "Silvio Savarese",
        "Steven Hoi"
      ],
      "year": "2023",
      "venue": "Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models",
      "arxiv": "arXiv:2301.12597"
    },
    {
      "citation_id": "27",
      "title": "Learning a model of facial shape and expression from 4D scans",
      "authors": [
        "Tianye Li",
        "Timo Bolkart",
        "Michael Black",
        "Hao Li",
        "Javier Romero"
      ],
      "year": "2017",
      "venue": "Proc. SIGGRAPH Asia)",
      "doi": "10.1145/3130800.3130813"
    },
    {
      "citation_id": "28",
      "title": "The ryerson audio-visual database of emotional and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "R Steven",
        "Frank Livingstone",
        "Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "29",
      "title": "Openmagvit2: An open-source project toward democratizing auto-regressive visual generation",
      "authors": [
        "Zhuoyan Luo",
        "Fengyuan Shi",
        "Yixiao Ge",
        "Yujiu Yang",
        "Limin Wang",
        "Ying Shan"
      ],
      "year": "2025",
      "venue": "Openmagvit2: An open-source project toward democratizing auto-regressive visual generation",
      "arxiv": "arXiv:2409.04410"
    },
    {
      "citation_id": "30",
      "title": "2024a. A transformer-based model with self-distillation for multimodal emotion recognition in conversations",
      "authors": [
        "Hui Ma",
        "Jian Wang",
        "Hongfei Lin",
        "Bo Zhang",
        "Yijia Zhang",
        "Bo Xu"
      ],
      "venue": "IEEE Transactions on Multimedia",
      "doi": "10.1109/TMM.2023.3271019"
    },
    {
      "citation_id": "31",
      "title": "2024b. Emobox: Multilingual multi-corpus speech emotion recognition toolkit and benchmark",
      "authors": [
        "Ziyang Ma",
        "Mingjie Chen",
        "Hezhao Zhang",
        "Zhisheng Zheng",
        "Wenxi Chen",
        "Xiquan Li",
        "Jiaxin Ye",
        "Xie Chen",
        "Thomas Hain"
      ],
      "venue": "2024b. Emobox: Multilingual multi-corpus speech emotion recognition toolkit and benchmark"
    },
    {
      "citation_id": "32",
      "title": "An Approach to Environmental Psychology",
      "authors": [
        "Albert Mehrabian",
        "James Russell"
      ],
      "year": "1974",
      "venue": "An Approach to Environmental Psychology"
    },
    {
      "citation_id": "33",
      "title": "Spirit-lm: Interleaved spoken and written language model",
      "authors": [
        "Anh Tu",
        "Benjamin Nguyen",
        "Bokai Muller",
        "Marta Yu",
        "Maha Costa-Jussa",
        "Sravya Elbayad",
        "Popuri"
      ],
      "year": "2024",
      "venue": "Spirit-lm: Interleaved spoken and written language model",
      "arxiv": "arXiv:2402.05755"
    },
    {
      "citation_id": "34",
      "title": "Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman",
      "authors": [
        "Josh Openai",
        "Steven Achiam",
        "Sandhini Adler",
        "Agarwal"
      ],
      "venue": "Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman",
      "arxiv": "arXiv:2303.08774"
    },
    {
      "citation_id": "35",
      "title": "Speech resynthesis from discrete disentangled self-supervised representations",
      "authors": [
        "Adam Polyak",
        "Yossi Adi",
        "Jade Copet",
        "Eugene Kharitonov",
        "Kushal Lakhotia",
        "Wei-Ning Hsu",
        "Abdelrahman Mohamed",
        "Emmanuel Dupoux"
      ],
      "year": "2021",
      "venue": "Speech resynthesis from discrete disentangled self-supervised representations",
      "arxiv": "arXiv:2104.00355"
    },
    {
      "citation_id": "36",
      "title": "Audio-visual fusion for emotion recognition in the valence-arousal space using joint cross-attention",
      "authors": [
        "R Gnana Praveen",
        "Patrick Cardinal",
        "Eric Granger"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Biometrics, Behavior, and Identity Science",
      "doi": "10.1109/TBIOM.2022.3233083"
    },
    {
      "citation_id": "37",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Tao Xu",
        "Greg Brockman",
        "Christine Mcleavey",
        "Ilya Sutskever"
      ],
      "year": "2022",
      "venue": "Robust speech recognition via large-scale weak supervision",
      "arxiv": "arXiv:2212.04356"
    },
    {
      "citation_id": "38",
      "title": "You only look once: Unified, real-time object detection",
      "authors": [
        "Joseph Redmon",
        "Santosh Divvala",
        "Ross Girshick",
        "Ali Farhadi"
      ],
      "year": "2016",
      "venue": "You only look once: Unified, real-time object detection",
      "arxiv": "arXiv:1506.02640"
    },
    {
      "citation_id": "39",
      "title": "Anastasios Roussos, Timo Bolkart, and Petros Maragos. 2024. 3d facial expressions through analysis-by-neural-synthesis",
      "authors": [
        "George Retsinas",
        "P Panagiotis",
        "Radek Filntisis",
        "Victoria Danecek",
        "Abrevaya"
      ],
      "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "40",
      "title": "Introducing the RECOLA multimodal corpus of remote collaborative and affective interactions",
      "authors": [
        "Fabien Ringeval",
        "Andreas Sonderegger",
        "Jürgen Sauer",
        "Denis Lalanne"
      ],
      "year": "2013",
      "venue": "Proceedings of the 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "41",
      "title": "Hsemotion: High-speed emotion recognition library",
      "authors": [
        "Andrey Savchenko"
      ],
      "year": "2022",
      "venue": "Software Impacts",
      "doi": "10.1016/j.simpa.2022.100433"
    },
    {
      "citation_id": "42",
      "title": "Learning audio-visual speech representation by masked multimodal cluster prediction",
      "authors": [
        "Bowen Shi",
        "Wei-Ning Hsu",
        "Kushal Lakhotia",
        "Abdelrahman Mohamed"
      ],
      "year": "2022",
      "venue": "Learning audio-visual speech representation by masked multimodal cluster prediction",
      "arxiv": "arXiv:2201.02184"
    },
    {
      "citation_id": "43",
      "title": "Ssr: Alignmentaware modality connector for speech language models",
      "authors": [
        "Weiting Tan",
        "Hirofumi Inaguma",
        "Ning Dong",
        "Paden Tomasello",
        "Xutai Ma"
      ],
      "year": "2024",
      "venue": "Ssr: Alignmentaware modality connector for speech language models",
      "arxiv": "arXiv:2410.00168"
    },
    {
      "citation_id": "44",
      "title": "2024a. Salmonn: Towards generic hearing abilities for large language models",
      "authors": [
        "Changli Tang",
        "Wenyi Yu",
        "Guangzhi Sun",
        "Xianzhao Chen",
        "Tian Tan",
        "Wei Li",
        "Lu Lu",
        "Zejun Ma",
        "Chao Zhang"
      ],
      "venue": "2024a. Salmonn: Towards generic hearing abilities for large language models",
      "arxiv": "arXiv:2310.13289"
    },
    {
      "citation_id": "45",
      "title": "Jing Xiao, and Jianzong Wang. 2024b. Ed-tts: Multi-scale emotion modeling using cross-domain emotion diarization for emotional speech synthesis",
      "authors": [
        "Haobin Tang",
        "Xulong Zhang",
        "Ning Cheng"
      ],
      "venue": "Jing Xiao, and Jianzong Wang. 2024b. Ed-tts: Multi-scale emotion modeling using cross-domain emotion diarization for emotional speech synthesis",
      "arxiv": "arXiv:2401.08166"
    },
    {
      "citation_id": "46",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Ł Ukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "47",
      "title": "Learning emotional representations from imbalanced speech data for speech emotion recognition and emotional text-to-speech",
      "authors": [
        "Shijun Wang",
        "Jón Guðnason",
        "Damian Borth"
      ],
      "year": "2023",
      "venue": "Learning emotional representations from imbalanced speech data for speech emotion recognition and emotional text-to-speech",
      "arxiv": "arXiv:2306.05709"
    },
    {
      "citation_id": "48",
      "title": "On decoder-only architecture for speech-to-text and large model integration",
      "authors": [
        "Jian Wu",
        "Yashesh Gaur",
        "Zhuo Chen",
        "Long Zhou",
        "Yimeng Zhu",
        "Tianrui Wang",
        "Jinyu Li",
        "Shujie Liu",
        "Bo Ren",
        "Linquan Liu",
        "Yu Wu"
      ],
      "year": "2023",
      "venue": "On decoder-only architecture for speech-to-text and large model integration",
      "arxiv": "arXiv:2307.03917"
    },
    {
      "citation_id": "49",
      "title": "Chae Won Kim, Stavros Petridis, and Yong Man Ro. 2025a. Zero-avsr: Zeroshot audio-visual speech recognition with llms by learning language-agnostic speech representations",
      "authors": [
        "Minsu Jeong Hun Yeo",
        "Kim"
      ],
      "venue": "Chae Won Kim, Stavros Petridis, and Yong Man Ro. 2025a. Zero-avsr: Zeroshot audio-visual speech recognition with llms by learning language-agnostic speech representations",
      "arxiv": "arXiv:2503.06273"
    },
    {
      "citation_id": "50",
      "title": "Se Jin Park, and Yong Man Ro. 2025b. Mms-llama: Efficient llm-based audio-visual speech recognition with minimal multimodal speech tokens",
      "authors": [
        "Hyeongseop Jeong Hun Yeo",
        "Rha"
      ],
      "venue": "Se Jin Park, and Yong Man Ro. 2025b. Mms-llama: Efficient llm-based audio-visual speech recognition with minimal multimodal speech tokens",
      "arxiv": "arXiv:2503.11315"
    },
    {
      "citation_id": "51",
      "title": "Language model beats diffusion -tokenizer is key to visual generation",
      "authors": [
        "Lijun Yu",
        "José Lezama",
        "B Nitesh",
        "Luca Gundavarapu",
        "Kihyuk Versari",
        "David Sohn",
        "Yong Minnen",
        "Vighnesh Cheng",
        "Agrim Birodkar",
        "Xiuye Gupta",
        "Alexander Gu",
        "Boqing Hauptmann",
        "Ming-Hsuan Gong",
        "Irfan Yang",
        "David Essa",
        "Lu Ross",
        "Jiang"
      ],
      "year": "2024",
      "venue": "Language model beats diffusion -tokenizer is key to visual generation",
      "arxiv": "arXiv:2310.05737"
    },
    {
      "citation_id": "52",
      "title": "Connecting speech encoder and large language model for asr",
      "authors": [
        "Wenyi Yu",
        "Changli Tang",
        "Guangzhi Sun",
        "Xianzhao Chen",
        "Tian Tan",
        "Wei Li",
        "Lu Lu",
        "Zejun Ma",
        "Chao Zhang"
      ],
      "year": "2023",
      "venue": "Connecting speech encoder and large language model for asr",
      "arxiv": "arXiv:2309.13963"
    },
    {
      "citation_id": "53",
      "title": "Speechgpt: Empowering large language models with intrinsic cross-modal conversational abilities",
      "authors": [
        "Dong Zhang",
        "Shimin Li",
        "Xin Zhang",
        "Jun Zhan",
        "Pengyu Wang",
        "Yaqian Zhou",
        "Xipeng Qiu"
      ],
      "year": "2023",
      "venue": "Speechgpt: Empowering large language models with intrinsic cross-modal conversational abilities",
      "arxiv": "arXiv:2305.11000"
    }
  ]
}