{
  "paper_id": "2306.08933v1",
  "title": "Towards Interpretability In Audio And Visual Affective Machine Learning: A Review",
  "published": "2023-06-15T08:16:01Z",
  "authors": [
    "David S. Johnson",
    "Olya Hakobyan",
    "Hanna Drimalla"
  ],
  "keywords": [
    "XAI",
    "Interpretability",
    "Affective Machine Learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Machine learning is frequently used in affective computing, but presents challenges due the opacity of stateof-the-art machine learning methods. Because of the impact affective machine learning systems may have on an individual's life, it is important that models be made transparent to detect and mitigate biased decision making. In this regard, affective machine learning could benefit from the recent advancements in explainable artificial intelligence (XAI) research. We perform a structured literature review to examine the use of interpretability in the context of affective machine learning. We focus on studies using audio, visual, or audiovisual data for model training and identified 29 research articles. Our findings show an emergence of the use of interpretability methods in the last five years. However, their use is currently limited regarding the range of methods used, the depth of evaluations, and the consideration of use-cases. We outline the main gaps in the research and provide recommendations for researchers that aim to implement interpretable methods for affective machine learning.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Humans communicate their affective state through a rich variety of nonverbal signals, such as facial expressions and vocal tone. These audiovisual signals can be analyzed using machine learning (ML) approaches for tasks like automated affect recognition  [1, 2, 3] . Such endeavors have implications for many areas of human life, offering the potential for improved life quality while also bearing risks. For instance, automatic recognition of affect and emotions from audiovisual data can enhance human-machine interactions  [4]  or help develop systems for the diagnosis and management of mental health  [3] . It has already been shown, however, that ML techniques often amplify existing societal biases  [5]  potentially leading to discriminatory interactions or incorrect mental health diagnoses. Given the need to provide fair benefits to all users and avoid potential unfair disadvantages, it is critical that decisions made by affective ML systems are transparent and trustworthy to both users of the systems and those affected by their decisions.\n\nEnsuring transparency in affective ML poses a challenge, especially with the rise of more complex and opaque methods in the field  [2] . For instance, deep-learning methods are becoming more popular due to their high-performance\n\nWe gratefully acknowledge funding by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation): TRR 318/1 2021 -438445824.\n\ncapabilities, yet they are very difficult to interpret. Research from the field of explainable artificial intelligence (XAI) aims to address this problem by facilitating the interpretation of models  [6] , either by developing inherently interpretable models or through post-hoc explanations  [7] . Despite their potential, the extent to which interpretability methods are applied in affective machine learning is not clear. Even with an awareness of interpretability, adapting these techniques to affective computing may not be straightforward due to different interpretability needs. This calls for a close evaluation of interpretability in affective ML, similar to the review of Tjoa and Guan  [8]  on XAI in the medical domain.\n\nTo our best knowledge, there are no reviews regarding interpretable and explainable affective computing. We aim to address this gap by contributing a comprehensive review of interpretability within affective ML for audio and/or visual data, analyzing the techniques employed to obtain interpretable models. Through this process, we identify the gaps and challenges in the research, and suggest potential research directions to establish interpretable affective ML.\n\nThe rest of the paper is organized as follows. In §II, we provide a brief introduction to the definitions and taxonomy of interpretability and explainability. We then introduce the methodology for our review in §III, before outlining the results in §IV. The findings are then discussed and summarized in §V and §VI, respectively. Finally, we discuss the ethical impact of our work in §VII.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Interpretable Ai",
      "text": "In this section, we provide a brief, non-exhaustive, introduction to the field of interpretable and explainable AI to establish the contextual basis for the relevant concepts. We refer interested readers to the survey of Barredo Arrieta et al.  [7]  for a more complete overview.\n\nInterpretable AI is an emerging and multidisciplinary field without universally accepted definitions of the terms interpretability and explainability, which are often used interchangeably. We follow the taxonomy of Graziani et al.  [9] , however, who consider interpretability as a broader term encompassing explainability, and define interpretable AI as a system that is able to communicate its inner workings and decisions in an understandable manner. Interpretability may then be achieved by either methods that are intepretable-bydesign or that generate explanations post-hoc. To this end, we consider interpretability as broader goal and explainability as one potential method to reach this goal.\n\nAI systems that are interpretable-by-design include transparent models that can be understood by humans. For example, low-dimensional, or sparse, linear models may be considered transparent since their low-dimensionality affords humanunderstandable feature weights. Additionally, simple decisiontrees that can be parsed by a human allowing them to follow the model's logic would also be considered transparent  [7] . Deep learning models with attention mechanisms are also considered by some to have built-in interpretability via their attention weights, although their effectiveness for interpretability is still a topic of debate  [10, 11] . Attention-based models are considered here to be interpretable-by-design since the attention weights are calculated directly by the model without the need for additional methods after inference to generate an interpretable representation.\n\nPost-hoc explainability, on the other hand, describes methods to generate explanations of decisions for models that are not already interpretable. Post-hoc methods examine the input data and corresponding predictions using varying techniques to extract what the model has learned and provide users with useful and understandable information about the model  [12] . One commonly used post-hoc method is feature attribution which generates scores for each feature, indicating their importance toward the final prediction.\n\nBoth interpretable-by-design and post-hoc methods can provide either global or local explanations  [7, 9] . Global interpretability approaches target an overall understanding of the model as a whole. For example, weights from simple linear models can indicate which features (e.g. facial expressions) have the largest influence on an emotion recognition system in general. By contrast, local interpretability approaches, such as feature attribution, aim to explain the output for a single instance, such as the role of different facial regions towards recognizing the emotion of a particular individual. Local explanations can often be aggregated over a group of instances to provide a global understanding of the model.\n\nThe choice of input representation also impacts interpretability  [7] , since input representations differ in their understandability. For example, high-dimensional vectors of extracted features are often not interpretable since they might not have human-understandable meaning or require expert knowledge to understand. Features such as facial action units (FAUs), however, offer a more interpretable representation because they correspond to facial movements understandable by the general population.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Iii. Methods",
      "text": "To identify research on interpretable AI in audio and visual tasks of affective ML, we employed a systematic search via the IEEE, PubMed, and Web of Science electronic databases. We searched each database without explicit start and end dates, on Nov. 11, 2022, for key terms in the title, abstract, and keywords using the query:\n\n(\"affective computing\" OR ((\"facial expression\" OR emotion * ) AND (recognition OR detection OR classification OR prediction)) OR (\"sentiment analysis\" AND multimodal)) AND (xai OR ((explainab * OR interpretab * ) AND (\"artificial intelligence\" OR \"AI\" OR model * OR \"machine learning\"))) NOT music. The \"*\" indicates wildcard search, while the quotations are used for exact search. The query used common terms for machine learning and affective computing, but may have missed some relevant work with different terms or that was not indexed by the included databases.\n\nAfter removing duplicates, the search resulted in N = 273 articles. Two of the authors reviewed and discussed the titles and abstracts of the articles to evaluate the quality of the search results and to finalize the selection criteria. The first inclusion criterion was that the studies employed machine learning methods for the recognition of affect, emotion, facial expression, or sentiment. Studies analyzing affective features (e.g. facial expressions) to predict other variables, such as psychological condition, were included as well. The second inclusion criterion was the use of audio or visual data for model training in an unimodal or multimodal setting. This means that sentiment analysis studies relying solely on textual data were not considered unless they employed multimodal models incorporating audiovisual data. Finally, the studies had to list interpretability as one contribution of their research. The exclusion of certain articles was necessary to maintain the focus of the paper, hence studies focusing on music emotion recognition or using exclusively physiological, EEG or text data were not included.\n\nAfter this consensus, N = 65 papers were included for further full-text review. For this iteration, two of the authors (A1 and A2) were randomly assigned half the papers for independent review. To ensure common ground for inclusion and exclusion in the round, the first 10 papers were individually reviewed by both authors and subsequently discussed. Papers that merely mentioned interpretability in their abstracts but only briefly touched on the topic in the main text were excluded during this round. In addition, the two authors eliminated papers which, despite including audiovisual data, did not consider behavioral signals (e.g. in a visual emotion analysis classifying the perceived emotional content of an image), focused on non-audiovisual features to explain model predictions (e.g. first impression prediction based on personality traits), or included intrusive sensors (e.g. motion-capture sensors). Finally, from the various papers that describe the NOVA framework  [40] , we chose to include the one published in a journal. This resulted in the exclusion of 36 further papers, for a total of N = 29 articles that employed interpretable methods for affective ML in the audio and visual domains. All authors then reviewed the final selection of papers to extract information regarding their contributions towards interpretable",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Local",
      "text": "The differentiable programming algorithm learned sparse representations of the face which were visualized to improve the interpretability of the model. The visualizations were used to qualitatively evaluate the model using a small set of local explanation.\n\naffective computing.\n\nFor each article we extracted the following information: the main affective ML task, the input modalities and representations, details regarding interpretability (method and scope) and a short description of the method and evaluation. The input modality is specified as the input representation accepted by the model rather than the datasets presented in the paper. For example, in some cases the articles used video datasets but only input individual frames (i.e., images), and not image sequences, into the model. Therefore, we consider these as the image modality. We classify the input representations as either raw input, features, or embeddings. Raw input represents unprocessed data such as images, videos or audio signals which are input directly to the model for end-to-end learning approaches. Feature representations are vectors of values that have been extracted from the data, usually through expertbased and domain-specific feature engineering. Embeddings refers to a form of extracted features that are the learned representations from a model pre-trained on a more general domain-related dataset, rather than being hand-crafted by human experts. In the case of features and embeddings, we list specific feature sets when they are considered well-known in affective computing research. In some cases, however, the diversity and novelty of methods used makes it challenging to list each method.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iv. Results",
      "text": "We identified 29 articles employing interpretability methods for affective ML. Although we did not include a date filter, the final selection resulted in publications from the last five years (2018-2022) indicating the emergence of interpretable affective AI. Of these works, 14 implemented interpretableby-design methods and 15 used post-hoc interpretability methods. Details of the articles for each category are presented in Table  I , interpretability-by-design and Table  II , post-hoc interpretability.\n\nThe affective ML tasks in the reviewed papers range from more traditional applications (affect or emotion recognition, sentiment analysis) to the detection of sarcasm, genuine expressions or depression. Nevertheless, the most common task was the recognition of facial expressions (N = 13). Most of the models were unimodal, focusing on the visual modality (N = 13 image, N = 7 video) with only one paper that relied exclusively on audio data (without the spoken content). There were N = 8 multimodal methods using the audio, video and language modalities, except in one case in which only speech data (i.e. audio and language) was included. For each modality, there is a range of different input representations that were employed. Typically, visual based methods used raw image or video data (N = 13) while others extracted features such as FAUs and facial landmarks (N = 8). The single audioonly model employed raw audio for model input. Multimodal methods, on the other hand, all relied on extracted features and embeddings.\n\nIn the domain of interpretability-by-design, Table  I , a few articles take the approach of transparent models (N = 3). Non-linear models such as deep neural networks with attention mechanisms were used more often (N = 5). In this case, the attention weights for an input sample provide the user a salient representation of the features most activated in the attention mechanism by that sample. Attention mechanisms can also be implemented as a multimodal fusion module to integrate the input modalities, affording insight into which modality is activated the most for a given decision  [21, 22, 23] . The remaining articles (N = 6) design models for interpretability using a variety of other novel methods. Most of these approaches provide local explanations (N = 11) while some generate global explanations (N = 6) (three articles include both global and local explanations).\n\nIn the domain of post-hoc explainability methods, Table  II , the most common approach (N = 13) was the implementation of feature attribution methods, such as Locally-Interpretable Model Agnostic Explanations (LIME)  [28, 31, 33, 40] , Shapley additive values (SHAP)  [30, 32, 33, 38, 39] , activation maps (CAM, GradCAM)  [36, 37] , layer-wise relevance propagation (LRP)  [31] , and integrated gradients  [27] . Of the remaining articles one proposed a method with Testing Concept Activation Vectors (TCAV)  [42]  and the other a Monte Carlo dropout method for calculating uncertainty  [29] . Similar to interpretability-by-design, the post-hoc methods mostly focus on local explanations (N = 13) but a few implement global explanations (N = 5) (three articles include both global and local explanations).\n\nRegardless of the interpretability approach, there was an over-reliance on using interpretability methods that visualize feature importance. In the area of interpretability-by-design this was done generating attentions maps from the weights of the attention mechanism (N = 5), visualizing the weights of transparent models (N = 2) or, in one case, visualizing sparse representations of the input. Of the post-hoc explainability methods, by far the most common approach was to visualize feature attribution scores as either bar charts or saliency maps, visualizing the importance of each pixel towards the model decision (N = 13).\n\nOur analysis of the research shows that it was taken for granted that the methods implemented were interpretable. In many cases, only a qualitative review of the explanation methods was provided based on a few example explanation visualizations, which were then briefly discussed with a subjective analysis. We identified only N = 4 works that included users in the evaluation of interpretability, and only one paper that evaluated the validity of the interpretability approach in a quantitative analysis without users.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "V. Discussion",
      "text": "We reviewed the use of interpretability methods in affective ML, focusing on tasks employing audio and/or visual data. Our results indicate the emergence of both interpretableby-design and post-hoc explainability methods over the last years, implemented for a variety of machine learning models with different input types. While these developments are encouraging, the current implementation of these methods is rather limited. We next discuss these limitations and potential solutions for overcoming them.\n\nIn the majority of research reviewed here, interpretability was limited to some variation of saliency maps showing which input features or modalities contributed the most to the decisions of the given model. In addition, we found that very few studies delved deeper into these methods beyond an anecdotal or qualitative review of just a small set of example explanations. This is problematic because the effectiveness of such visualizations for interpretability cannot be assessed without an explicit evaluation. In fact, it has been shown that relying exclusively on visualizations can be misleading and that many saliency methods perform similar to edge detectors that are model-and data-independent  [43] . Likewise, the reliability of attention weights for deriving model decisions has been questioned  [11] . Hence, we recommend performing systematic evaluation and comparison of different interpretability methods to ensure that the explanations are understandable and make sense for the given application.\n\nIt is worth noting that even a carefully chosen interpretability method may only show what features are important but not necessarily explain why these features have a greater impact on the model's decisions than the others, resulting in an interpretability gap that must be filled by the end-user  [44] . The severity of the interpretability gap can vary based on the type of input representation. For example, studies that used raw input for image data can generate salience maps that are more easily interpretable, compared to other input types, such as audio spectrograms which require expert knowledge to achieve a level of interpretability. Similarly, extracted features, and especially embeddings, are often not intuitive, calling for a careful consideration of input representations when developing interpretable affective computing systems. It may be advisable to consider interpretable features whenever possible or alternative methods to feature attribution. For instance, concept activation vectors (CAVs)  [35, 42]  provide explanations in terms of task specific high-level concepts that have been designed by the developer to be understood by the model's user group, instead of relying on low-level feature vectors. A more detailed description of alternative post-hoc explainability methods is available in the review of Barredo Arrieta et al.  [7] .\n\nOne aspect that may complicate relating input representations to model decisions is the use of multimodal approaches. Unlike in unimodal systems, understanding model decisions in these cases requires the consideration of a wider array of inputs from multiple modalities. Many of the reviewed methods focused on identifying dominant modalities, but largely neglected the interactions between the modalities. Therefore, we recommend analyzing the interactions between modalities similar to the methods proposed by Wang et al.  [39]  for M 2 Lens, indicating how different modalities can complement, dominate or conflict with each other.\n\nAnother challenge for multimodal scenarios is that availability of established interpretability methods differ between modalities. The methods used in the visual domain are more advanced in this regard compared to audio, where the interpretability research is still in its early stages. Therefore, more research into interpretability for audio modalities might be a promising direction, including the development of new interpretable features similar to FAUs for facial expressions.\n\nIn terms of scope, local explanations outnumbered global ones. While global explanations can be useful for identifying stereotypical features of affective expression, they may not capture finer variability between people and different situations. In particular, inter-personal level emotion expression can vary based on characteristics, such as gender or culture  [45] . At an intra-personal level, people's behavior might change depending on the situation, e.g. whether or not they are stressed. In this way, global explanations may be used to understand and identify biases and generalizations of models towards different groups, while local explanations may be used to provide an understanding of how intra-and inter-personal variations affect model decisions for specific individuals.\n\nSelecting an effective interpretability method for an explanation context requires an understanding of whom the explanation is for, but XAI researchers often fall short of considering the potential end-users  [46] . For this reason, it is recommended that XAI methods should be developed with the goal of being understandable to end-users and hence be evaluated with human behavioral studies. However, we found it to be rare that researchers even mentioned the intended recipients of the explanations. Furthermore, only four of the reviewed papers performed user-based evaluations  [13, 30, 39, 40] . In line with recent calls for more rigorous  [47]  and human-based  [46]  evaluation of interpretability, we suggest that affective computing researchers take a more human-centered approach and consider the contexts in which explanations will be situated, including both the application tasks and users. For example, explanations during an interaction between an affective social robot and a user with autism will have different needs than explanations geared towards an interviewer aiming to understand the behavioral analysis of an interview candidate.\n\nFollowing a human-centered approach  [46] , explanations can be seen as conversations and machine explanations should follow their maxims. This does not mean machine explanations have to be textual, but rather that they should have the properties found in human conversation. Rohlfing et al.  [48]  build on this and suggest that explanations are a social practice and should be co-constructed between system and user. This requires supporting a variety of explanation methods and knowledge of how they apply to different situations. In our analysis, only the work of Wang et al.  [39]  proposed interactive explanations, but this was limited to specific XAI methods and did not allow the system to adapt the explanation methods to the users needs. To this end, we suggest researchers should focus on implementing and evaluating a breadth of interpretability methods, beyond just feature importance, in different affective computing contexts. For example, researchers may want to consider counterfactual explanations as they are more congruent with human reasoning by considering how a model's decision change under different conditions  [46] .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "We welcome the use of interpretability methods within the field of affective machine learning. However, interpretable affective ML is still a young field with a limited breadth and evaluation of methods, calling for increased clarity regarding the context of the explanations and the effectiveness of different interpretability methods within stated contexts. Our analysis shows that affective ML could benefit from explicit consideration and discussion of the interpretability approach, such as the reliability of the used method, the interpretability of input representations and the choice of interpretability scope. Finally, the real end-users of affective ML systems should be considered when designing interpretable systems. While our work outlined the main developments and limitations, future work is needed for a more in-depth analysis of the use of interpretability methods for different contexts of different affective ML tasks.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Vii. Ethical Impact Statement",
      "text": "With our review and recommendations on the use of interpretability methods in affective ML, we hope to contribute to the development of more transparent and ethical systems. Nevertheless, it is important to approach our recommendations with caution. Specifically, we explicitly emphasize that implementing interpretable methods, even if approached in a rigorous manner, does not guarantee ethical systems. For example, providing explanations has been shown to increase automation bias leading to users over-trusting model decisions  [49]  potentially leading to adversarial implementations of interpretability. It is necessary then to consider the entire pipeline of the machine learning approach in an interdisciplinary setting, considering all steps from data collection to deployment.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Bielefeld, Germany": ""
        },
        {
          "Bielefeld, Germany": "Abstract—Machine\nlearning\nis\nfrequently\nused\nin\naffective"
        },
        {
          "Bielefeld, Germany": "computing,\nbut\npresents\nchallenges\ndue\nthe\nopacity\nof\nstate-"
        },
        {
          "Bielefeld, Germany": ""
        },
        {
          "Bielefeld, Germany": "of-the-art machine\nlearning methods. Because\nof\nthe\nimpact"
        },
        {
          "Bielefeld, Germany": ""
        },
        {
          "Bielefeld, Germany": "affective machine learning systems may have on an individual’s"
        },
        {
          "Bielefeld, Germany": ""
        },
        {
          "Bielefeld, Germany": "life,\nit\nis\nimportant\nthat models be made transparent\nto detect"
        },
        {
          "Bielefeld, Germany": "and mitigate biased decision making.\nIn this\nregard,\naffective"
        },
        {
          "Bielefeld, Germany": "machine learning could beneﬁt from the recent advancements in"
        },
        {
          "Bielefeld, Germany": "explainable artiﬁcial\nintelligence (XAI) research. We perform a"
        },
        {
          "Bielefeld, Germany": ""
        },
        {
          "Bielefeld, Germany": "structured literature review to examine the use of interpretability"
        },
        {
          "Bielefeld, Germany": ""
        },
        {
          "Bielefeld, Germany": "in the context of affective machine learning. We focus on studies"
        },
        {
          "Bielefeld, Germany": ""
        },
        {
          "Bielefeld, Germany": "using audio, visual, or audiovisual data for model\ntraining and"
        },
        {
          "Bielefeld, Germany": "identiﬁed 29 research articles. Our ﬁndings show an emergence"
        },
        {
          "Bielefeld, Germany": "of\nthe\nuse\nof\ninterpretability methods\nin\nthe\nlast\nﬁve\nyears."
        },
        {
          "Bielefeld, Germany": "However,\ntheir use\nis\ncurrently limited regarding the\nrange of"
        },
        {
          "Bielefeld, Germany": ""
        },
        {
          "Bielefeld, Germany": "methods used,\nthe depth of\nevaluations, and the\nconsideration"
        },
        {
          "Bielefeld, Germany": ""
        },
        {
          "Bielefeld, Germany": "of\nuse-cases. We\noutline\nthe main\ngaps\nin\nthe\nresearch\nand"
        },
        {
          "Bielefeld, Germany": ""
        },
        {
          "Bielefeld, Germany": "provide recommendations for researchers that aim to implement"
        },
        {
          "Bielefeld, Germany": "interpretable methods for affective machine learning."
        },
        {
          "Bielefeld, Germany": "Index Terms—XAI, Interpretability, Affective Machine Learn-"
        },
        {
          "Bielefeld, Germany": "ing"
        },
        {
          "Bielefeld, Germany": ""
        },
        {
          "Bielefeld, Germany": ""
        },
        {
          "Bielefeld, Germany": "I.\nINTRODUCTION"
        },
        {
          "Bielefeld, Germany": ""
        },
        {
          "Bielefeld, Germany": "Humans\ncommunicate their\naffective state\nthrough a\nrich"
        },
        {
          "Bielefeld, Germany": ""
        },
        {
          "Bielefeld, Germany": "variety of nonverbal\nsignals,\nsuch as\nfacial\nexpressions and"
        },
        {
          "Bielefeld, Germany": ""
        },
        {
          "Bielefeld, Germany": "vocal\ntone. These audiovisual signals can be analyzed using"
        },
        {
          "Bielefeld, Germany": ""
        },
        {
          "Bielefeld, Germany": "machine learning (ML) approaches\nfor\ntasks\nlike automated"
        },
        {
          "Bielefeld, Germany": ""
        },
        {
          "Bielefeld, Germany": "affect recognition [1, 2, 3]. Such endeavors have implications"
        },
        {
          "Bielefeld, Germany": ""
        },
        {
          "Bielefeld, Germany": "for many\nareas\nof\nhuman\nlife,\noffering\nthe\npotential\nfor"
        },
        {
          "Bielefeld, Germany": ""
        },
        {
          "Bielefeld, Germany": "improved life quality while also bearing risks. For\ninstance,"
        },
        {
          "Bielefeld, Germany": ""
        },
        {
          "Bielefeld, Germany": "automatic recognition of affect and emotions from audiovisual"
        },
        {
          "Bielefeld, Germany": "or\nhelp\ndata\ncan\nenhance\nhuman-machine\ninteractions\n[4]"
        },
        {
          "Bielefeld, Germany": ""
        },
        {
          "Bielefeld, Germany": "develop systems for the diagnosis and management of mental"
        },
        {
          "Bielefeld, Germany": ""
        },
        {
          "Bielefeld, Germany": "health\n[3].\nIt\nhas\nalready\nbeen\nshown,\nhowever,\nthat ML"
        },
        {
          "Bielefeld, Germany": ""
        },
        {
          "Bielefeld, Germany": "techniques often amplify existing societal biases\n[5] poten-"
        },
        {
          "Bielefeld, Germany": ""
        },
        {
          "Bielefeld, Germany": "tially leading to discriminatory interactions or incorrect mental"
        },
        {
          "Bielefeld, Germany": ""
        },
        {
          "Bielefeld, Germany": "health diagnoses. Given the need to provide fair beneﬁts to all"
        },
        {
          "Bielefeld, Germany": ""
        },
        {
          "Bielefeld, Germany": "users and avoid potential unfair disadvantages, it is critical that"
        },
        {
          "Bielefeld, Germany": ""
        },
        {
          "Bielefeld, Germany": "decisions made by affective ML systems are transparent and"
        },
        {
          "Bielefeld, Germany": ""
        },
        {
          "Bielefeld, Germany": "trustworthy to both users of the systems and those affected by"
        },
        {
          "Bielefeld, Germany": ""
        },
        {
          "Bielefeld, Germany": "their decisions."
        },
        {
          "Bielefeld, Germany": ""
        },
        {
          "Bielefeld, Germany": "Ensuring transparency in affective ML poses a challenge,"
        },
        {
          "Bielefeld, Germany": ""
        },
        {
          "Bielefeld, Germany": "especially with the rise of more complex and opaque meth-"
        },
        {
          "Bielefeld, Germany": ""
        },
        {
          "Bielefeld, Germany": "For\ninstance,\ndeep-learning methods\nods\nin\nthe\nﬁeld\n[2]."
        },
        {
          "Bielefeld, Germany": ""
        },
        {
          "Bielefeld, Germany": "are\nbecoming more\npopular\ndue\nto\ntheir\nhigh-performance"
        },
        {
          "Bielefeld, Germany": ""
        },
        {
          "Bielefeld, Germany": ""
        },
        {
          "Bielefeld, Germany": "We gratefully acknowledge\nfunding by the Deutsche Forschungsgemein-"
        },
        {
          "Bielefeld, Germany": "schaft\n(DFG, German Research Foundation): TRR 318/1 2021 – 438445824."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "consider\ninterpretability as broader goal and explainability as": "one potential method to reach this goal.",
          "(\"affective computing\" OR ((\"facial": "expression\" OR emotion*) AND"
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "AI\nsystems\nthat are interpretable-by-design include trans-",
          "(\"affective computing\" OR ((\"facial": "(recognition OR\ndetection OR"
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "parent models that can be understood by humans. For example,",
          "(\"affective computing\" OR ((\"facial": "classification OR prediction))"
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "low-dimensional,\nor\nsparse,\nlinear models may\nbe\nconsid-",
          "(\"affective computing\" OR ((\"facial": "OR (\"sentiment analysis\" AND"
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "ered transparent since their low-dimensionality affords human-",
          "(\"affective computing\" OR ((\"facial": "multimodal)) AND\n(xai OR"
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "understandable feature weights. Additionally, simple decision-",
          "(\"affective computing\" OR ((\"facial": "((explainab* OR\ninterpretab*) AND"
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "trees that can be parsed by a human allowing them to follow",
          "(\"affective computing\" OR ((\"facial": "(\"artificial intelligence\" OR \"AI\""
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "the model’s\nlogic would also be\nconsidered transparent\n[7].",
          "(\"affective computing\" OR ((\"facial": "OR model* OR \"machine learning\")))"
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "Deep\nlearning models with\nattention mechanisms\nare\nalso",
          "(\"affective computing\" OR ((\"facial": "NOT music."
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "considered by some to have built-in interpretability via their",
          "(\"affective computing\" OR ((\"facial": "The “*” indicates wildcard search, while the quotations are"
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "attention weights, although their effectiveness for interpretabil-",
          "(\"affective computing\" OR ((\"facial": "used\nfor\nexact\nsearch. The\nquery\nused\ncommon\nterms\nfor"
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "ity is still a topic of debate [10, 11]. Attention-based models",
          "(\"affective computing\" OR ((\"facial": "machine\nlearning\nand\naffective\ncomputing,\nbut may\nhave"
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "are\nconsidered here\nto\nbe\ninterpretable-by-design since\nthe",
          "(\"affective computing\" OR ((\"facial": "missed some relevant work with different\nterms or\nthat was"
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "attention weights are calculated directly by the model without",
          "(\"affective computing\" OR ((\"facial": "not\nindexed by the included databases."
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "the need for additional methods after inference to generate an",
          "(\"affective computing\" OR ((\"facial": "After\nremoving duplicates,\nthe search resulted in N = 273"
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "interpretable representation.",
          "(\"affective computing\" OR ((\"facial": "articles. Two of\nthe authors reviewed and discussed the titles"
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "Post-hoc explainability, on the other hand, describes meth-",
          "(\"affective computing\" OR ((\"facial": "and\nabstracts\nof\nthe\narticles\nto\nevaluate\nthe\nquality\nof\nthe"
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "ods to generate explanations of decisions for models that are",
          "(\"affective computing\" OR ((\"facial": "search results and to ﬁnalize the selection criteria. The ﬁrst"
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "not already interpretable. Post-hoc methods examine the input",
          "(\"affective computing\" OR ((\"facial": "inclusion\ncriterion was\nthat\nthe\nstudies\nemployed machine"
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "data and corresponding predictions using varying techniques to",
          "(\"affective computing\" OR ((\"facial": "learning methods for the recognition of affect, emotion, facial"
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "extract what the model has learned and provide users with use-",
          "(\"affective computing\" OR ((\"facial": "expression, or sentiment. Studies analyzing affective features"
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "ful and understandable information about the model [12]. One",
          "(\"affective computing\" OR ((\"facial": "(e.g.\nfacial\nexpressions)\nto\npredict other\nvariables,\nsuch\nas"
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "commonly used post-hoc method is feature attribution which",
          "(\"affective computing\" OR ((\"facial": "psychological condition, were included as well. The second"
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "generates scores for each feature,\nindicating their\nimportance",
          "(\"affective computing\" OR ((\"facial": "inclusion criterion was\nthe use of\naudio or visual data\nfor"
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "toward the ﬁnal prediction.",
          "(\"affective computing\" OR ((\"facial": "model\ntraining in an unimodal or multimodal\nsetting. This"
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "Both\ninterpretable-by-design\nand\npost-hoc methods\ncan",
          "(\"affective computing\" OR ((\"facial": "means that sentiment analysis studies relying solely on textual"
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "global\nlocal\nprovide\neither\nor\nexplanations\n[7,\n9]. Global",
          "(\"affective computing\" OR ((\"facial": "data were not\nconsidered unless\nthey employed multimodal"
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "interpretability approaches target an overall understanding of",
          "(\"affective computing\" OR ((\"facial": "models incorporating audiovisual data. Finally, the studies had"
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "the model as a whole. For example, weights from simple linear",
          "(\"affective computing\" OR ((\"facial": "to list\ninterpretability as one\ncontribution of\ntheir\nresearch."
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "models can indicate which features\n(e.g.\nfacial\nexpressions)",
          "(\"affective computing\" OR ((\"facial": "The exclusion of certain articles was necessary to maintain the"
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "have the largest\ninﬂuence on an emotion recognition system",
          "(\"affective computing\" OR ((\"facial": "focus of\nthe paper, hence studies focusing on music emotion"
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "in general. By contrast,\nlocal\ninterpretability approaches, such",
          "(\"affective computing\" OR ((\"facial": "recognition or using exclusively physiological, EEG or\ntext"
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "as\nfeature attribution, aim to explain the output\nfor a single",
          "(\"affective computing\" OR ((\"facial": "data were not\nincluded."
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "",
          "(\"affective computing\" OR ((\"facial": "After\nthis\nincluded for\nconsensus, N = 65 papers were"
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "instance,\nsuch as\nthe role of different\nfacial\nregions towards",
          "(\"affective computing\" OR ((\"facial": ""
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "",
          "(\"affective computing\" OR ((\"facial": "further\nfull-text\nreview. For\nthis\niteration,\ntwo of\nthe authors"
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "recognizing\nthe\nemotion\nof\na\nparticular\nindividual. Local",
          "(\"affective computing\" OR ((\"facial": ""
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "",
          "(\"affective computing\" OR ((\"facial": "(A1\nand A2) were\nrandomly\nassigned\nhalf\nthe\npapers\nfor"
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "explanations can often be aggregated over a group of instances",
          "(\"affective computing\" OR ((\"facial": ""
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "",
          "(\"affective computing\" OR ((\"facial": "independent\nreview. To ensure common ground for\ninclusion"
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "to provide a global understanding of\nthe model.",
          "(\"affective computing\" OR ((\"facial": ""
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "",
          "(\"affective computing\" OR ((\"facial": "and exclusion in the round,\nthe ﬁrst 10 papers were individ-"
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "input\nrepresentation\nThe\nchoice\nof\nalso\nimpacts\ninter-",
          "(\"affective computing\" OR ((\"facial": ""
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "",
          "(\"affective computing\" OR ((\"facial": "ually reviewed by both authors and subsequently discussed."
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "pretability\n[7],\nsince\ninput\nrepresentations\ndiffer\nin\ntheir",
          "(\"affective computing\" OR ((\"facial": ""
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "",
          "(\"affective computing\" OR ((\"facial": "Papers that merely mentioned interpretability in their abstracts"
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "understandability. For\nexample, high-dimensional vectors of",
          "(\"affective computing\" OR ((\"facial": ""
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "",
          "(\"affective computing\" OR ((\"facial": "but only brieﬂy touched on the topic in the main text were"
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "extracted features are often not\ninterpretable since they might",
          "(\"affective computing\" OR ((\"facial": ""
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "",
          "(\"affective computing\" OR ((\"facial": "excluded\nduring\nthis\nround.\nIn\naddition,\nthe\ntwo\nauthors"
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "not\nhave\nhuman-understandable meaning\nor\nrequire\nexpert",
          "(\"affective computing\" OR ((\"facial": ""
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "",
          "(\"affective computing\" OR ((\"facial": "eliminated papers which, despite including audiovisual data,"
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "knowledge to understand. Features such as facial action units",
          "(\"affective computing\" OR ((\"facial": ""
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "",
          "(\"affective computing\" OR ((\"facial": "did not consider behavioral signals\n(e.g.\nin a visual emotion"
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "(FAUs),\nhowever,\noffer\na more\ninterpretable\nrepresentation",
          "(\"affective computing\" OR ((\"facial": ""
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "",
          "(\"affective computing\" OR ((\"facial": "analysis\nclassifying\nthe\nperceived\nemotional\ncontent\nof\nan"
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "because they correspond to facial movements understandable",
          "(\"affective computing\" OR ((\"facial": ""
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "",
          "(\"affective computing\" OR ((\"facial": "image), focused on non-audiovisual features to explain model"
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "by the general population.",
          "(\"affective computing\" OR ((\"facial": ""
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "",
          "(\"affective computing\" OR ((\"facial": "predictions (e.g. ﬁrst\nimpression prediction based on person-"
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "",
          "(\"affective computing\" OR ((\"facial": "ality traits), or included intrusive sensors (e.g. motion-capture"
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "III. METHODS",
          "(\"affective computing\" OR ((\"facial": ""
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "",
          "(\"affective computing\" OR ((\"facial": "sensors). Finally,\nfrom the various papers\nthat describe\nthe"
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "To identify research on interpretable AI in audio and visual",
          "(\"affective computing\" OR ((\"facial": "NOVA framework [40], we chose to include the one published"
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "tasks of affective ML, we employed a systematic search via the",
          "(\"affective computing\" OR ((\"facial": "in a journal. This resulted in the exclusion of 36 further papers,"
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "IEEE, PubMed, and Web of Science electronic databases. We",
          "(\"affective computing\" OR ((\"facial": "for\na\nthat\nemployed interpretable\ntotal of N = 29 articles"
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "searched each database without explicit\nstart and end dates,",
          "(\"affective computing\" OR ((\"facial": "methods for affective ML in the audio and visual domains. All"
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "on Nov. 11, 2022,\nfor key terms\nin the\ntitle,\nabstract,\nand",
          "(\"affective computing\" OR ((\"facial": "authors then reviewed the ﬁnal selection of papers to extract"
        },
        {
          "consider\ninterpretability as broader goal and explainability as": "keywords using the query:",
          "(\"affective computing\" OR ((\"facial": "information regarding their contributions towards interpretable"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE I": ""
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "Scope"
        },
        {
          "TABLE I": "Global"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "Global"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "Local"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "Local"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "Global,"
        },
        {
          "TABLE I": "Local"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "Local"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "Local"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "Global,"
        },
        {
          "TABLE I": "Local"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "Local"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "Local"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "Global,"
        },
        {
          "TABLE I": "Local"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "Global"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "Local"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "Local"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "& Pain": "Recognition",
          "pretability of\nthe model. The visualizations were used to qualita-": "tively evaluate the model using a small set of\nlocal explanation."
        },
        {
          "& Pain": "",
          "pretability of\nthe model. The visualizations were used to qualita-": "input modality is speciﬁed as the input representation accepted"
        },
        {
          "& Pain": "",
          "pretability of\nthe model. The visualizations were used to qualita-": "by the model\nrather\nthan the datasets presented in the paper."
        },
        {
          "& Pain": "For each article we extracted the following information: the",
          "pretability of\nthe model. The visualizations were used to qualita-": ""
        },
        {
          "& Pain": "",
          "pretability of\nthe model. The visualizations were used to qualita-": "For example,\nin some cases\nthe articles used video datasets"
        },
        {
          "& Pain": "the",
          "pretability of\nthe model. The visualizations were used to qualita-": ""
        },
        {
          "& Pain": "",
          "pretability of\nthe model. The visualizations were used to qualita-": "but only input\nindividual frames (i.e.,\nimages), and not\nimage"
        },
        {
          "& Pain": "",
          "pretability of\nthe model. The visualizations were used to qualita-": ""
        },
        {
          "& Pain": "",
          "pretability of\nthe model. The visualizations were used to qualita-": "sequences,\ninto the model. Therefore, we\nconsider\nthese\nas"
        },
        {
          "& Pain": "short description of",
          "pretability of\nthe model. The visualizations were used to qualita-": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE II": ""
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": "Scope"
        },
        {
          "TABLE II": "Local"
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": "Global,"
        },
        {
          "TABLE II": "Local"
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": "Local"
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": "Local"
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": "Local"
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": "Global"
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": "Global,"
        },
        {
          "TABLE II": "Local"
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": "Local"
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": "Global"
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": "Local"
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": "Local"
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": "Local"
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": "Global,"
        },
        {
          "TABLE II": "Local"
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": "Local"
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": "Local"
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "the image modality. We classify the input\nrepresentations as": "either raw input, features, or embeddings. Raw input represents",
          "using a variety of other novel methods. Most of\nthese\nap-": "proaches provide\nlocal\nexplanations\nsome\n(N = 11) while"
        },
        {
          "the image modality. We classify the input\nrepresentations as": "unprocessed\ndata\nsuch\nas\nimages,\nvideos\nor\naudio\nsignals",
          "using a variety of other novel methods. Most of\nthese\nap-": "(three articles\ninclude\ngenerate global explanations (N = 6)"
        },
        {
          "the image modality. We classify the input\nrepresentations as": "which are input directly to the model\nfor end-to-end learning",
          "using a variety of other novel methods. Most of\nthese\nap-": "both global and local explanations)."
        },
        {
          "the image modality. We classify the input\nrepresentations as": "approaches. Feature representations are vectors of values that",
          "using a variety of other novel methods. Most of\nthese\nap-": "In the domain of post-hoc explainability methods, Table II,"
        },
        {
          "the image modality. We classify the input\nrepresentations as": "have been extracted from the data, usually through expert-",
          "using a variety of other novel methods. Most of\nthese\nap-": "the most common approach (N = 13) was the implementation"
        },
        {
          "the image modality. We classify the input\nrepresentations as": "based and domain-speciﬁc\nfeature\nengineering. Embeddings",
          "using a variety of other novel methods. Most of\nthese\nap-": "of\nfeature attribution methods,\nsuch as Locally-Interpretable"
        },
        {
          "the image modality. We classify the input\nrepresentations as": "refers\nto\na\nform of\nextracted features\nthat\nare\nthe\nlearned",
          "using a variety of other novel methods. Most of\nthese\nap-": "Model Agnostic Explanations (LIME) [28, 31, 33, 40], Shap-"
        },
        {
          "the image modality. We classify the input\nrepresentations as": "representations\nfrom a model pre-trained on a more general",
          "using a variety of other novel methods. Most of\nthese\nap-": "ley additive values\n(SHAP)\n[30, 32, 33, 38, 39],\nactivation"
        },
        {
          "the image modality. We classify the input\nrepresentations as": "domain-related\ndataset,\nrather\nthan\nbeing\nhand-crafted\nby",
          "using a variety of other novel methods. Most of\nthese\nap-": "layer-wise relevance prop-\nmaps (CAM, GradCAM) [36, 37],"
        },
        {
          "the image modality. We classify the input\nrepresentations as": "human experts.\nIn the case of\nfeatures and embeddings, we",
          "using a variety of other novel methods. Most of\nthese\nap-": "agation (LRP)\n[31], and integrated gradients [27]. Of\nthe re-"
        },
        {
          "the image modality. We classify the input\nrepresentations as": "list speciﬁc feature sets when they are considered well-known",
          "using a variety of other novel methods. Most of\nthese\nap-": "maining articles one proposed a method with Testing Concept"
        },
        {
          "the image modality. We classify the input\nrepresentations as": "in affective computing research.\nIn some cases, however,\nthe",
          "using a variety of other novel methods. Most of\nthese\nap-": "Activation Vectors (TCAV) [42] and the other a Monte Carlo"
        },
        {
          "the image modality. We classify the input\nrepresentations as": "diversity and novelty of methods used makes it challenging to",
          "using a variety of other novel methods. Most of\nthese\nap-": "dropout method for\ncalculating uncertainty [29]. Similar\nto"
        },
        {
          "the image modality. We classify the input\nrepresentations as": "list each method.",
          "using a variety of other novel methods. Most of\nthese\nap-": "interpretability-by-design,\nthe post-hoc methods mostly focus"
        },
        {
          "the image modality. We classify the input\nrepresentations as": "",
          "using a variety of other novel methods. Most of\nthese\nap-": "on local explanations (N = 13) but a few implement global"
        },
        {
          "the image modality. We classify the input\nrepresentations as": "IV. RESULTS",
          "using a variety of other novel methods. Most of\nthese\nap-": "(three articles include both global and\nexplanations (N = 5)"
        },
        {
          "the image modality. We classify the input\nrepresentations as": "",
          "using a variety of other novel methods. Most of\nthese\nap-": "local explanations)."
        },
        {
          "the image modality. We classify the input\nrepresentations as": "We identiﬁed 29 articles employing interpretability methods",
          "using a variety of other novel methods. Most of\nthese\nap-": "Regardless of\nthe\ninterpretability approach,\nthere was\nan"
        },
        {
          "the image modality. We classify the input\nrepresentations as": "for affective ML. Although we did not\ninclude a date ﬁlter,",
          "using a variety of other novel methods. Most of\nthese\nap-": "over-reliance on using interpretability methods that visualize"
        },
        {
          "the image modality. We classify the input\nrepresentations as": "the ﬁnal\nselection resulted in publications\nfrom the last ﬁve",
          "using a variety of other novel methods. Most of\nthese\nap-": "interpretability-by-design\nfeature\nimportance.\nIn the\narea of"
        },
        {
          "the image modality. We classify the input\nrepresentations as": "years\nindicating the\nemergence of\ninterpretable\n(2018-2022)",
          "using a variety of other novel methods. Most of\nthese\nap-": "this was done generating attentions maps from the weights of"
        },
        {
          "the image modality. We classify the input\nrepresentations as": "affective AI. Of",
          "using a variety of other novel methods. Most of\nthese\nap-": ""
        },
        {
          "the image modality. We classify the input\nrepresentations as": "these works, 14 implemented interpretable-",
          "using a variety of other novel methods. Most of\nthese\nap-": "the attention mechanism (N = 5), visualizing the weights of"
        },
        {
          "the image modality. We classify the input\nrepresentations as": "by-design methods and 15 used post-hoc interpretability meth-",
          "using a variety of other novel methods. Most of\nthese\nap-": "in one case, visualizing sparse\ntransparent models (N = 2) or,"
        },
        {
          "the image modality. We classify the input\nrepresentations as": "ods. Details of\nthe\narticles\nfor\neach category are presented",
          "using a variety of other novel methods. Most of\nthese\nap-": "post-hoc\nexplainability\nrepresentations of\nthe\ninput. Of\nthe"
        },
        {
          "the image modality. We classify the input\nrepresentations as": "I,\ninterpretability-by-design and Table\nII, post-hoc\nin Table",
          "using a variety of other novel methods. Most of\nthese\nap-": "methods, by far\nthe most common approach was to visualize"
        },
        {
          "the image modality. We classify the input\nrepresentations as": "interpretability.",
          "using a variety of other novel methods. Most of\nthese\nap-": "feature attribution scores as either bar charts or saliency maps,"
        },
        {
          "the image modality. We classify the input\nrepresentations as": "The affective ML tasks in the reviewed papers range from",
          "using a variety of other novel methods. Most of\nthese\nap-": "visualizing the\nimportance of each pixel\ntowards\nthe model"
        },
        {
          "the image modality. We classify the input\nrepresentations as": "more traditional applications\n(affect or\nemotion recognition,",
          "using a variety of other novel methods. Most of\nthese\nap-": "decision (N = 13)."
        },
        {
          "the image modality. We classify the input\nrepresentations as": "sentiment analysis)\nto the detection of\nsarcasm, genuine ex-",
          "using a variety of other novel methods. Most of\nthese\nap-": "Our\nanalysis of\nthe\nresearch shows\nthat\nit was\ntaken for"
        },
        {
          "the image modality. We classify the input\nrepresentations as": "pressions or depression. Nevertheless,\nthe most common task",
          "using a variety of other novel methods. Most of\nthese\nap-": "granted\nthat\nthe methods\nimplemented were\ninterpretable."
        },
        {
          "the image modality. We classify the input\nrepresentations as": "was the recognition of\nfacial expressions (N = 13). Most of",
          "using a variety of other novel methods. Most of\nthese\nap-": "In many cases, only a qualitative review of\nthe\nexplanation"
        },
        {
          "the image modality. We classify the input\nrepresentations as": "the models were unimodal,\nfocusing on the visual modality",
          "using a variety of other novel methods. Most of\nthese\nap-": "methods was provided based on a\nfew example explanation"
        },
        {
          "the image modality. We classify the input\nrepresentations as": "(N = 13 image, N = 7 video) with only one paper that relied",
          "using a variety of other novel methods. Most of\nthese\nap-": "visualizations, which were then brieﬂy discussed with a sub-"
        },
        {
          "the image modality. We classify the input\nrepresentations as": "exclusively on audio data (without\nthe spoken content). There",
          "using a variety of other novel methods. Most of\nthese\nap-": "jective analysis. We identiﬁed only N = 4 works that included"
        },
        {
          "the image modality. We classify the input\nrepresentations as": "were N = 8 multimodal methods using the audio, video and",
          "using a variety of other novel methods. Most of\nthese\nap-": "users in the evaluation of\ninterpretability, and only one paper"
        },
        {
          "the image modality. We classify the input\nrepresentations as": "language modalities, except\nin one case in which only speech",
          "using a variety of other novel methods. Most of\nthese\nap-": "that evaluated the validity of the interpretability approach in a"
        },
        {
          "the image modality. We classify the input\nrepresentations as": "data (i.e. audio and language) was included. For each modality,",
          "using a variety of other novel methods. Most of\nthese\nap-": "quantitative analysis without users."
        },
        {
          "the image modality. We classify the input\nrepresentations as": "there is\na\nrange of different\ninput\nrepresentations\nthat were",
          "using a variety of other novel methods. Most of\nthese\nap-": ""
        },
        {
          "the image modality. We classify the input\nrepresentations as": "",
          "using a variety of other novel methods. Most of\nthese\nap-": "V. DISCUSSION"
        },
        {
          "the image modality. We classify the input\nrepresentations as": "employed. Typically, visual based methods used raw image",
          "using a variety of other novel methods. Most of\nthese\nap-": ""
        },
        {
          "the image modality. We classify the input\nrepresentations as": "or video data (N = 13) while others extracted features such",
          "using a variety of other novel methods. Most of\nthese\nap-": "We reviewed the use of interpretability methods in affective"
        },
        {
          "the image modality. We classify the input\nrepresentations as": "as FAUs\nand facial\nlandmarks\nsingle\naudio-\n(N = 8). The",
          "using a variety of other novel methods. Most of\nthese\nap-": "ML,\nfocusing on tasks\nemploying audio and/or visual data."
        },
        {
          "the image modality. We classify the input\nrepresentations as": "only model employed raw audio for model\ninput. Multimodal",
          "using a variety of other novel methods. Most of\nthese\nap-": "interpretable-\nOur\nresults\nindicate\nthe\nemergence\nof\nboth"
        },
        {
          "the image modality. We classify the input\nrepresentations as": "methods, on the other hand, all\nrelied on extracted features",
          "using a variety of other novel methods. Most of\nthese\nap-": "by-design and post-hoc explainability methods over\nthe\nlast"
        },
        {
          "the image modality. We classify the input\nrepresentations as": "and embeddings.",
          "using a variety of other novel methods. Most of\nthese\nap-": "years,\nimplemented for a variety of machine learning models"
        },
        {
          "the image modality. We classify the input\nrepresentations as": "In the domain of\ninterpretability-by-design, Table I, a few",
          "using a variety of other novel methods. Most of\nthese\nap-": "with\ndifferent\ninput\ntypes. While\nthese\ndevelopments\nare"
        },
        {
          "the image modality. We classify the input\nrepresentations as": "articles\ntake\nthe\napproach of\ntransparent models\n(N = 3).",
          "using a variety of other novel methods. Most of\nthese\nap-": "encouraging,\nthe current\nimplementation of\nthese methods is"
        },
        {
          "the image modality. We classify the input\nrepresentations as": "Non-linear models such as deep neural networks with attention",
          "using a variety of other novel methods. Most of\nthese\nap-": "rather limited. We next discuss these limitations and potential"
        },
        {
          "the image modality. We classify the input\nrepresentations as": "In this case,\nthe\nmechanisms were used more often (N = 5).",
          "using a variety of other novel methods. Most of\nthese\nap-": "solutions for overcoming them."
        },
        {
          "the image modality. We classify the input\nrepresentations as": "attention weights for an input sample provide the user a salient",
          "using a variety of other novel methods. Most of\nthese\nap-": "In the majority of\nresearch reviewed here,\ninterpretability"
        },
        {
          "the image modality. We classify the input\nrepresentations as": "representation of\nthe features most activated in the attention",
          "using a variety of other novel methods. Most of\nthese\nap-": "was\nlimited\nto\nsome\nvariation\nof\nsaliency maps\nshowing"
        },
        {
          "the image modality. We classify the input\nrepresentations as": "mechanism by that\nsample. Attention mechanisms\ncan also",
          "using a variety of other novel methods. Most of\nthese\nap-": "which input\nfeatures or modalities\ncontributed the most\nto"
        },
        {
          "the image modality. We classify the input\nrepresentations as": "be implemented as a multimodal\nfusion module to integrate",
          "using a variety of other novel methods. Most of\nthese\nap-": "the decisions of\nthe given model.\nIn addition, we found that"
        },
        {
          "the image modality. We classify the input\nrepresentations as": "the\ninput modalities,\naffording insight\ninto which modality",
          "using a variety of other novel methods. Most of\nthese\nap-": "very few studies delved deeper into these methods beyond an"
        },
        {
          "the image modality. We classify the input\nrepresentations as": "is activated the most\nfor a given decision [21, 22, 23]. The",
          "using a variety of other novel methods. Most of\nthese\nap-": "anecdotal or qualitative review of\njust a small set of example"
        },
        {
          "the image modality. We classify the input\nrepresentations as": "interpretability\nremaining articles (N = 6) design models for",
          "using a variety of other novel methods. Most of\nthese\nap-": "explanations. This\nis\nproblematic because\nthe\neffectiveness"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "of\nsuch visualizations\nfor\ninterpretability cannot be assessed": "without\nan\nexplicit\nevaluation.\nIn\nfact,\nit\nhas\nbeen\nshown",
          "vary based on characteristics, such as gender or culture [45].": "At\nan\nintra-personal\nlevel,\npeople’s behavior might\nchange"
        },
        {
          "of\nsuch visualizations\nfor\ninterpretability cannot be assessed": "that\nrelying exclusively on visualizations can be misleading",
          "vary based on characteristics, such as gender or culture [45].": "depending\non\nthe\nsituation,\ne.g. whether\nor\nnot\nthey\nare"
        },
        {
          "of\nsuch visualizations\nfor\ninterpretability cannot be assessed": "and\nthat many\nsaliency methods\nperform similar\nto\nedge",
          "vary based on characteristics, such as gender or culture [45].": "In\nthis way,\nglobal\nexplanations may\nbe\nused\nto\nstressed."
        },
        {
          "of\nsuch visualizations\nfor\ninterpretability cannot be assessed": "detectors\nthat\nare model-\nand\ndata-independent\n[43]. Like-",
          "vary based on characteristics, such as gender or culture [45].": "understand and identify biases and generalizations of models"
        },
        {
          "of\nsuch visualizations\nfor\ninterpretability cannot be assessed": "wise,\nthe\nreliability of attention weights\nfor deriving model",
          "vary based on characteristics, such as gender or culture [45].": "towards different groups, while local explanations may be used"
        },
        {
          "of\nsuch visualizations\nfor\ninterpretability cannot be assessed": "recommend\ndecisions has been questioned [11]. Hence, we",
          "vary based on characteristics, such as gender or culture [45].": "to provide an understanding of how intra- and inter-personal"
        },
        {
          "of\nsuch visualizations\nfor\ninterpretability cannot be assessed": "performing systematic evaluation and comparison of different",
          "vary based on characteristics, such as gender or culture [45].": "variations affect model decisions for speciﬁc individuals."
        },
        {
          "of\nsuch visualizations\nfor\ninterpretability cannot be assessed": "interpretability methods\nto ensure\nthat\nthe\nexplanations are",
          "vary based on characteristics, such as gender or culture [45].": "Selecting an effective interpretability method for an expla-"
        },
        {
          "of\nsuch visualizations\nfor\ninterpretability cannot be assessed": "understandable and make sense for the given application.",
          "vary based on characteristics, such as gender or culture [45].": "nation context requires an understanding of whom the explana-"
        },
        {
          "of\nsuch visualizations\nfor\ninterpretability cannot be assessed": "It\nis worth noting that even a carefully chosen interpretabil-",
          "vary based on characteristics, such as gender or culture [45].": "tion is for, but XAI researchers often fall short of considering"
        },
        {
          "of\nsuch visualizations\nfor\ninterpretability cannot be assessed": "ity method may only show what\nfeatures are important but",
          "vary based on characteristics, such as gender or culture [45].": "the potential end-users [46]. For this reason, it is recommended"
        },
        {
          "of\nsuch visualizations\nfor\ninterpretability cannot be assessed": "not\nnecessarily\nexplain why\nthese\nfeatures\nhave\na\ngreater",
          "vary based on characteristics, such as gender or culture [45].": "that XAI methods\nshould\nbe\ndeveloped with\nthe\ngoal\nof"
        },
        {
          "of\nsuch visualizations\nfor\ninterpretability cannot be assessed": "impact on the model’s decisions than the others,\nresulting in",
          "vary based on characteristics, such as gender or culture [45].": "being understandable to end-users and hence be evaluated with"
        },
        {
          "of\nsuch visualizations\nfor\ninterpretability cannot be assessed": "an interpretability gap that must be ﬁlled by the end-user [44].",
          "vary based on characteristics, such as gender or culture [45].": "human behavioral\nstudies. However, we\nfound it\nto be\nrare"
        },
        {
          "of\nsuch visualizations\nfor\ninterpretability cannot be assessed": "The severity of\nthe interpretability gap can vary based on the",
          "vary based on characteristics, such as gender or culture [45].": "that researchers even mentioned the intended recipients of the"
        },
        {
          "of\nsuch visualizations\nfor\ninterpretability cannot be assessed": "type of\ninput\nrepresentation. For example,\nstudies\nthat used",
          "vary based on characteristics, such as gender or culture [45].": "explanations. Furthermore, only four of\nthe reviewed papers"
        },
        {
          "of\nsuch visualizations\nfor\ninterpretability cannot be assessed": "raw input\nfor\nimage\ndata\ncan\ngenerate\nsalience maps\nthat",
          "vary based on characteristics, such as gender or culture [45].": "performed user-based evaluations [13, 30, 39, 40]. In line with"
        },
        {
          "of\nsuch visualizations\nfor\ninterpretability cannot be assessed": "are more easily interpretable, compared to other\ninput\ntypes,",
          "vary based on characteristics, such as gender or culture [45].": "and\nhuman-based [46]\nrecent\ncalls\nfor more\nrigorous\n[47]"
        },
        {
          "of\nsuch visualizations\nfor\ninterpretability cannot be assessed": "such as audio spectrograms which require expert knowledge to",
          "vary based on characteristics, such as gender or culture [45].": "suggest\nthat affective com-\nevaluation of\ninterpretability, we"
        },
        {
          "of\nsuch visualizations\nfor\ninterpretability cannot be assessed": "achieve a level of interpretability. Similarly, extracted features,",
          "vary based on characteristics, such as gender or culture [45].": "puting researchers take a more human-centered approach and"
        },
        {
          "of\nsuch visualizations\nfor\ninterpretability cannot be assessed": "and especially embeddings, are often not intuitive, calling for a",
          "vary based on characteristics, such as gender or culture [45].": "consider the contexts in which explanations will be situated,"
        },
        {
          "of\nsuch visualizations\nfor\ninterpretability cannot be assessed": "careful consideration of input representations when developing",
          "vary based on characteristics, such as gender or culture [45].": "including both the application tasks and users. For example,"
        },
        {
          "of\nsuch visualizations\nfor\ninterpretability cannot be assessed": "interpretable affective computing systems. It may be advisable",
          "vary based on characteristics, such as gender or culture [45].": "explanations during an interaction between an affective social"
        },
        {
          "of\nsuch visualizations\nfor\ninterpretability cannot be assessed": "to consider interpretable features whenever possible or alter-",
          "vary based on characteristics, such as gender or culture [45].": "robot and a user with autism will have different needs than ex-"
        },
        {
          "of\nsuch visualizations\nfor\ninterpretability cannot be assessed": "native methods\nto feature attribution. For\ninstance,\nconcept",
          "vary based on characteristics, such as gender or culture [45].": "planations geared towards an interviewer aiming to understand"
        },
        {
          "of\nsuch visualizations\nfor\ninterpretability cannot be assessed": "42]\nprovide\nexplanations\nin\nactivation vectors\n(CAVs)\n[35,",
          "vary based on characteristics, such as gender or culture [45].": "the behavioral analysis of an interview candidate."
        },
        {
          "of\nsuch visualizations\nfor\ninterpretability cannot be assessed": "terms\nof\ntask\nspeciﬁc\nhigh-level\nconcepts\nthat\nhave\nbeen",
          "vary based on characteristics, such as gender or culture [45].": "Following a\nhuman-centered approach [46],\nexplanations"
        },
        {
          "of\nsuch visualizations\nfor\ninterpretability cannot be assessed": "designed by the developer\nto be understood by the model’s",
          "vary based on characteristics, such as gender or culture [45].": "can be seen as conversations and machine explanations should"
        },
        {
          "of\nsuch visualizations\nfor\ninterpretability cannot be assessed": "user group,\ninstead of\nrelying on low-level feature vectors. A",
          "vary based on characteristics, such as gender or culture [45].": "follow their maxims. This does not mean machine explanations"
        },
        {
          "of\nsuch visualizations\nfor\ninterpretability cannot be assessed": "more detailed description of alternative post-hoc explainability",
          "vary based on characteristics, such as gender or culture [45].": "have\nto\nbe\ntextual,\nbut\nrather\nthat\nthey\nshould\nhave\nthe"
        },
        {
          "of\nsuch visualizations\nfor\ninterpretability cannot be assessed": "methods is available in the review of Barredo Arrieta et al. [7].",
          "vary based on characteristics, such as gender or culture [45].": "properties found in human conversation. Rohlﬁng et al.\n[48]"
        },
        {
          "of\nsuch visualizations\nfor\ninterpretability cannot be assessed": "One aspect\nthat may complicate relating input\nrepresenta-",
          "vary based on characteristics, such as gender or culture [45].": "build on this and suggest that explanations are a social practice"
        },
        {
          "of\nsuch visualizations\nfor\ninterpretability cannot be assessed": "tions to model decisions is the use of multimodal approaches.",
          "vary based on characteristics, such as gender or culture [45].": "and should be co-constructed between system and user. This"
        },
        {
          "of\nsuch visualizations\nfor\ninterpretability cannot be assessed": "Unlike in unimodal\nsystems, understanding model decisions",
          "vary based on characteristics, such as gender or culture [45].": "requires\nsupporting\na\nvariety\nof\nexplanation methods\nand"
        },
        {
          "of\nsuch visualizations\nfor\ninterpretability cannot be assessed": "in these cases\nrequires the consideration of a wider array of",
          "vary based on characteristics, such as gender or culture [45].": "knowledge of how they apply to different\nsituations.\nIn our"
        },
        {
          "of\nsuch visualizations\nfor\ninterpretability cannot be assessed": "inputs from multiple modalities. Many of\nthe reviewed meth-",
          "vary based on characteristics, such as gender or culture [45].": "analysis, only the work of Wang et al. [39] proposed interactive"
        },
        {
          "of\nsuch visualizations\nfor\ninterpretability cannot be assessed": "ods\nfocused on identifying dominant modalities, but\nlargely",
          "vary based on characteristics, such as gender or culture [45].": "explanations, but this was limited to speciﬁc XAI methods and"
        },
        {
          "of\nsuch visualizations\nfor\ninterpretability cannot be assessed": "neglected the interactions between the modalities. Therefore,",
          "vary based on characteristics, such as gender or culture [45].": "did not allow the system to adapt\nthe explanation methods to"
        },
        {
          "of\nsuch visualizations\nfor\ninterpretability cannot be assessed": "we recommend analyzing the interactions between modalities",
          "vary based on characteristics, such as gender or culture [45].": "researchers\nshould\nthe users needs. To this end, we suggest"
        },
        {
          "of\nsuch visualizations\nfor\ninterpretability cannot be assessed": "similar\nto\nthe methods\nproposed\nby Wang\net\nal.\n[39]\nfor",
          "vary based on characteristics, such as gender or culture [45].": "focus\non\nimplementing\nand\nevaluating\na\nbreadth\nof\ninter-"
        },
        {
          "of\nsuch visualizations\nfor\ninterpretability cannot be assessed": "M2Lens,\nindicating how different modalities can complement,",
          "vary based on characteristics, such as gender or culture [45].": "pretability methods, beyond just\nfeature\nimportance,\nin dif-"
        },
        {
          "of\nsuch visualizations\nfor\ninterpretability cannot be assessed": "dominate or conﬂict with each other.",
          "vary based on characteristics, such as gender or culture [45].": "ferent affective computing contexts. For example,\nresearchers"
        },
        {
          "of\nsuch visualizations\nfor\ninterpretability cannot be assessed": "Another challenge for multimodal\nscenarios\nis\nthat\navail-",
          "vary based on characteristics, such as gender or culture [45].": "may want\nto consider counterfactual explanations as they are"
        },
        {
          "of\nsuch visualizations\nfor\ninterpretability cannot be assessed": "ability of established interpretability methods differ between",
          "vary based on characteristics, such as gender or culture [45].": "more congruent with human reasoning by considering how a"
        },
        {
          "of\nsuch visualizations\nfor\ninterpretability cannot be assessed": "modalities. The methods used in the visual domain are more",
          "vary based on characteristics, such as gender or culture [45].": "model’s decision change under different conditions [46]."
        },
        {
          "of\nsuch visualizations\nfor\ninterpretability cannot be assessed": "advanced in\nthis\nregard compared to\naudio, where\nthe\nin-",
          "vary based on characteristics, such as gender or culture [45].": ""
        },
        {
          "of\nsuch visualizations\nfor\ninterpretability cannot be assessed": "",
          "vary based on characteristics, such as gender or culture [45].": "VI. CONCLUSION"
        },
        {
          "of\nsuch visualizations\nfor\ninterpretability cannot be assessed": "terpretability research is\nstill\nin its\nearly stages. Therefore,",
          "vary based on characteristics, such as gender or culture [45].": ""
        },
        {
          "of\nsuch visualizations\nfor\ninterpretability cannot be assessed": "more research into interpretability for audio modalities might",
          "vary based on characteristics, such as gender or culture [45].": "We welcome the use of\ninterpretability methods within the"
        },
        {
          "of\nsuch visualizations\nfor\ninterpretability cannot be assessed": "be a promising direction,\nincluding the development of new",
          "vary based on characteristics, such as gender or culture [45].": "ﬁeld\nof\naffective machine\nlearning. However,\ninterpretable"
        },
        {
          "of\nsuch visualizations\nfor\ninterpretability cannot be assessed": "interpretable features similar to FAUs for facial expressions.",
          "vary based on characteristics, such as gender or culture [45].": "affective ML is still a young ﬁeld with a limited breadth and"
        },
        {
          "of\nsuch visualizations\nfor\ninterpretability cannot be assessed": "In terms of\nscope,\nlocal explanations outnumbered global",
          "vary based on characteristics, such as gender or culture [45].": "evaluation of methods,\ncalling for\nincreased clarity regard-"
        },
        {
          "of\nsuch visualizations\nfor\ninterpretability cannot be assessed": "ones. While global explanations can be useful\nfor\nidentifying",
          "vary based on characteristics, such as gender or culture [45].": "ing the\ncontext of\nthe\nexplanations and the\neffectiveness of"
        },
        {
          "of\nsuch visualizations\nfor\ninterpretability cannot be assessed": "stereotypical\nfeatures of\naffective\nexpression,\nthey may not",
          "vary based on characteristics, such as gender or culture [45].": "different\ninterpretability methods within stated contexts. Our"
        },
        {
          "of\nsuch visualizations\nfor\ninterpretability cannot be assessed": "capture ﬁner variability between people\nand different\nsitua-",
          "vary based on characteristics, such as gender or culture [45].": "analysis shows\nthat affective ML could beneﬁt\nfrom explicit"
        },
        {
          "of\nsuch visualizations\nfor\ninterpretability cannot be assessed": "tions. In particular, inter-personal level emotion expression can",
          "vary based on characteristics, such as gender or culture [45].": "consideration and discussion of\nthe interpretability approach,"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "and N. Navarin,\n“Face\nthe Truth:\nInterpretable Emotion Genuineness"
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "of\ninput\nrepresentations\nand\nthe\nchoice\nof\ninterpretability",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": ""
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "Detection,” in 2022 International Joint Conference on Neural Networks"
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "scope. Finally,\nthe\nreal\nend-users\nof\naffective ML systems",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": ""
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "(IJCNN).\nPadua,\nItaly:\nIEEE, Jul. 2022, pp. 01–08."
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "should be\nconsidered when designing interpretable systems.",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "[15] Y. Jiao, Y. Niu, Y. Zhang, F. Li, C. Zou, and G. Shi, “Facial Attention"
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "based Convolutional Neural Network\nfor\n2D+3D Facial Expression"
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "While our work outlined the main developments and limita-",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": ""
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "and Image Pro-\nRecognition,”\nin 2019 IEEE Visual Communications"
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "tions,\nfuture work is needed for a more in-depth analysis of",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": ""
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "cessing (VCIP).\nSydney, Australia:\nIEEE, Dec. 2019, pp. 1–4."
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "the use of\ninterpretability methods\nfor different\ncontexts of",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "[16]\nJ. Zhou, X. Zhang, Y. Liu, and X. Lan, “Facial Expression Recognition"
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "Using Spatial-Temporal Semantic Graph Network,” in 2020 IEEE Inter-"
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "different affective ML tasks.",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": ""
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "national Conference on Image Processing (ICIP).\nAbu Dhabi, United"
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "Arab Emirates:\nIEEE, Oct. 2020, pp. 1961–1965."
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "VII. ETHICAL IMPACT STATEMENT",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "[17] A. Anand, S. Negi, and N. Narendra, “Filters Know How You Feel: Ex-"
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "plaining Intermediate Speech Emotion Classiﬁcation Representations,”"
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "With our\nreview and recommendations on the use of\nin-",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "2021 Asia-Paciﬁc\nSignal\nand\nInformation Processing Association\nin"
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "Annual Summit and Conference\n(APSIPA ASC), Dec. 2021, pp. 756–"
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "terpretability methods in affective ML, we hope to contribute",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": ""
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "761."
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "to the development of more transparent and ethical\nsystems.",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": ""
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "[18]\nZ. Yang, Y. Zhang, and J. Luo, “Human-Centered Emotion Recognition"
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "Nevertheless,\nit\nis\nimportant\nto approach our\nrecommenda-",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "in Animated GIFs,”\nin 2019 IEEE International Conference on Mul-"
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "timedia\nand Expo (ICME).\nShanghai, China:\nIEEE,\nJul. 2019,\npp."
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "tions with caution. Speciﬁcally, we explicitly emphasize that",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": ""
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "1090–1095."
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "implementing interpretable methods,\neven if\napproached in",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": ""
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "[19] V. Pandit, M. Schmitt, N. Cummins, and B. Schuller, “I\nsee it\nin your"
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "a\nrigorous manner, does not guarantee\nethical\nsystems. For",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "eyes: Training the shallowest-possible CNN to recognise emotions and"
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "pain\nfrom muted web-assisted\nin-the-wild\nvideo-chats\nin\nreal-time,”"
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "example, providing explanations has been shown to increase",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": ""
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "Information Processing & Management, vol. 57, no. 6, p. 102347, Nov."
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "automation bias\nleading\nto\nusers\nover-trusting model\ndeci-",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": ""
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "2020."
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "sions\n[49] potentially leading to adversarial\nimplementations",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "[20]\nJ. Wu, S. Mai, and H. Hu, “Interpretable Multimodal Capsule Fusion,”"
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,"
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "of\ninterpretability.\nIt\nis necessary then to consider\nthe entire",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": ""
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "vol. 30, pp. 1815–1826, 2022."
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "pipeline of\nthe machine\nlearning approach in an interdisci-",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": ""
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "[21] Y. Wu, Y. Zhao, X. Lu, B. Qin, Y. Wu, J. Sheng, and J. Li, “Modeling"
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "plinary setting, considering all\nsteps\nfrom data collection to",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”"
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "IEEE MultiMedia, vol. 28, no. 2, pp. 86–95, Apr. 2021."
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "deployment.",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": ""
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "[22] Y. Gu, K. Yang, S. Fu, S. Chen, X. Li,\nand I. Marsic,\n“Multimodal"
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "Affective Analysis Using Hierarchical Attention Strategy with Word-"
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "REFERENCES",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "the 56th Annual Meeting of\nthe\nLevel Alignment,”\nin Proceedings of"
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "Association\nfor Computational Linguistics\n(Volume\n1: Long Papers)."
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "[1] Y. Wang et al., “A systematic review on affective computing: Emotion",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": ""
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "Melbourne, Australia: Association for Computational Linguistics,\nJul."
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "models, databases, and recent advances,” Information Fusion, vol. 83–",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": ""
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "2018, pp. 2225–2235."
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "84, pp. 19–52, 2022.",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": ""
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "[23]\nL. Hemamou, A. Guillon, J.-C. Martin, and C. Clavel, “Multimodal Hi-"
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "[2]\nS. Poria, E. Cambria, R. Bajpai, and A. Hussain, “A review of affective",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": ""
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "erarchical Attention Neural Network: Looking for Candidates Behaviour"
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "computing: From unimodal analysis to multimodal\nfusion,” Information",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": ""
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "IEEE Transactions\non Affective\nwhich\nImpact Recruiter’s Decision,”"
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "Fusion, vol. 37, pp. 98–125, Sep. 2017.",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": ""
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "Computing, pp. 1–1, 2021."
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "[3]\nJ. Han, Z. Zhang, C. Mascolo, E. André,\nJ. Tao, Z. Zhao, and B. W.",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": ""
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "[24]\nE. Morales-Vargas, C. A. Reyes-García, and H. Peregrina-Barreto, “On"
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "Schuller, “Deep learning for mobile mental health: Challenges and recent",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": ""
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "the use of action units and fuzzy explanatory models for facial expression"
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "advances,” IEEE Signal Processing Magazine, vol. 38, pp. 96–105, Nov.",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": ""
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "recognition,” PLOS ONE, vol. 14, no. 10, Oct. 2019."
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "2021.",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": ""
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "[25] B. Browatzki and C. Wallraven, “Robust Discrimination and Generation"
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "[4] B. W. Schuller, R. Picard, E. André, J. Gratch, and J. Tao, “Intelligent",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": ""
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "of Faces using Compact, Disentangled Embeddings,” in 2019 IEEE/CVF"
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "signal processing for affective computing [from the guest editors],” IEEE",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": ""
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "International\nConference\non Computer\nVision Workshop\n(ICCVW)."
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "Signal Processing Magazine, vol. 38, no. 6, pp. 9–11, Nov. 2021.",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": ""
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "Seoul, Korea (South):\nIEEE, Oct. 2019, pp. 579–588."
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "[5]\nJ. Buolamwini\nand T. Gebru, “Gender\nshades:\nIntersectional\naccuracy",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": ""
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "[26] R. Zhi, M. Wan, and X. Hu, “SGS2Net: Deep representation of\nfacial"
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "disparities in commercial gender classiﬁcation,” in Conference on Fair-",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": ""
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "Journal of Electronic\nexpression by graph-preserving\nsparse\ncoding,”"
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "ness, Accountability and Transparency.\nPMLR, Jan. 2018, pp. 77–91.",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": ""
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "Imaging, vol. 29, no. 06, Dec. 2020."
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "[6] A. Adadi\nand M. Berrada,\n“Peeking\ninside\nthe black-box: A survey",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": ""
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "[27]\nL. P. Carlini, L. A. Ferreira, G. A. S. Coutrin, V. V. Varoto, T. M."
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "on explainable\nartiﬁcial\nintelligence\n(XAI),”\nIEEE Access, vol. 6, pp.",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": ""
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "Heiderich, R. C. X. Balda, M. C. M. Barros, R. Guinsburg, and C. E."
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "52 138–52 160, 2018.",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": ""
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "Thomaz, “A Convolutional Neural Network-based Mobile Application"
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "et\n[7] A. Barredo Arrieta\nal.,\n“Explainable\nartiﬁcial\nintelligence\n(XAI):",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": ""
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "to Bedside Neonatal Pain Assessment,” in 2021 34th SIBGRAPI Con-"
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "Concepts,\ntaxonomies, opportunities and challenges\ntoward responsible",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": ""
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "ference on Graphics, Patterns and Images (SIBGRAPI).\nGramado, Rio"
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "AI,” Information Fusion, vol. 58, pp. 82–115, Jun. 2020.",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": ""
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "Grande do Sul, Brazil:\nIEEE, Oct. 2021, pp. 394–401."
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "[8]\nE. Tjoa\nand C. Guan,\n“A survey on explainable\nartiﬁcial\nintelligence",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": ""
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "[28] M. Deramgozin, S.\nJovanovic, H. Rabah, and N. Ramzan, “A Hybrid"
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "(XAI): Toward medical XAI,” IEEE Transactions on Neural Networks",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": ""
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "Explainable AI Framework Applied to Global and Local Facial Expres-"
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "and Learning Systems, vol. 32, no. 11, pp. 4793–4813, Nov. 2021.",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": ""
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "sion Recognition,” in 2021 IEEE International Conference on Imaging"
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "[9] M. Graziani et al., “A global\ntaxonomy of interpretable AI: Unifying the",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": ""
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "Systems and Techniques\n(IST).\nKaohsiung, Taiwan:\nIEEE, Aug. 2021,"
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "terminology for the technical and social sciences,” Artiﬁcial Intelligence",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": ""
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "pp. 1–5."
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "Review, Sep. 2022.",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": ""
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "[29] A. Ghandeharioun, B. Eoff, B.\nJou,\nand R. Picard,\n“Characterizing"
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "[10] A. de Santana Correia and E. L. Colombini, “Attention, please! A survey",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": ""
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "Sources of Uncertainty to Proxy Calibration and Disambiguate Anno-"
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "Intelligence\nof\nneural\nattention models\nin\ndeep\nlearning,” Artiﬁcial",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": ""
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "tator and Data Bias,” in 2019 IEEE/CVF International Conference on"
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "Review, vol. 55, no. 8, pp. 6037–6124, Dec. 2022.",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": ""
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "Computer Vision Workshop (ICCVW).\nSeoul, Korea\n(South):\nIEEE,"
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "[11]\nZ. Niu, G. Zhong, and H. Yu, “A review on the attention mechanism of",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": ""
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "Oct. 2019, pp. 4202–4206."
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "deep learning,” Neurocomputing, vol. 452, pp. 48–62, Sep. 2021.",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": ""
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "[30] K. ter Burg and H. Kaya, “Comparing Approaches for Explaining DNN-"
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "[12]\nZ. C. Lipton, “The mythos of model\ninterpretability,” Communications",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": ""
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "Based Facial Expression Classiﬁcations,” Algorithms, vol. 15, no. 10, p."
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "of\nthe ACM, vol. 61, no. 10, pp. 36–43, Sep. 2018.",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": ""
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "367, Oct. 2022."
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "[13] D. Seuss, T. Hassan, A. Dieckmann, M. Unfried, K. R. R. Scherer,",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": ""
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "[31] K. Weitz, T. Hassan, U. Schmid, and J.-U. Garbas, “Deep-learned faces"
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "M. Mortillaro, and J.-U. Garbas, “Automatic Estimation of Action Unit",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": ""
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "of pain and emotions: Elucidating the differences of\nfacial expressions"
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "Intensities and Inference of Emotional Appraisals,” IEEE Transactions",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": ""
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": "with the help of\nexplainable AI methods,”\ntm - Technisches Messen,"
        },
        {
          "such as the reliability of\nthe used method,\nthe interpretability": "on Affective Computing, pp. 1–1, May 2021.",
          "[14] M. Cardaioli, A. Miolla, M. Conti, G. Sartori, M. Monaro, C. Scarpazza,": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": "IEEE MultiMedia, vol. 28, no. 2, pp. 86–95, Apr. 2021."
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": ""
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": "[22] Y. Gu, K. Yang, S. Fu, S. Chen, X. Li,\nand I. Marsic,\n“Multimodal"
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": "Affective Analysis Using Hierarchical Attention Strategy with Word-"
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": "the 56th Annual Meeting of\nthe\nLevel Alignment,”\nin Proceedings of"
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": "Association\nfor Computational Linguistics\n(Volume\n1: Long Papers)."
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": ""
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": "Melbourne, Australia: Association for Computational Linguistics,\nJul."
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": ""
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": "2018, pp. 2225–2235."
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": ""
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": "[23]\nL. Hemamou, A. Guillon, J.-C. Martin, and C. Clavel, “Multimodal Hi-"
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": ""
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": "erarchical Attention Neural Network: Looking for Candidates Behaviour"
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": ""
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": "IEEE Transactions\non Affective\nwhich\nImpact Recruiter’s Decision,”"
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": ""
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": "Computing, pp. 1–1, 2021."
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": ""
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": "[24]\nE. Morales-Vargas, C. A. Reyes-García, and H. Peregrina-Barreto, “On"
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": ""
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": "the use of action units and fuzzy explanatory models for facial expression"
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": ""
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": "recognition,” PLOS ONE, vol. 14, no. 10, Oct. 2019."
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": ""
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": "[25] B. Browatzki and C. Wallraven, “Robust Discrimination and Generation"
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": ""
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": "of Faces using Compact, Disentangled Embeddings,” in 2019 IEEE/CVF"
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": ""
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": "International\nConference\non Computer\nVision Workshop\n(ICCVW)."
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": ""
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": "Seoul, Korea (South):\nIEEE, Oct. 2019, pp. 579–588."
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": ""
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": "[26] R. Zhi, M. Wan, and X. Hu, “SGS2Net: Deep representation of\nfacial"
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": ""
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": "Journal of Electronic\nexpression by graph-preserving\nsparse\ncoding,”"
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": ""
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": "Imaging, vol. 29, no. 06, Dec. 2020."
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": ""
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": "[27]\nL. P. Carlini, L. A. Ferreira, G. A. S. Coutrin, V. V. Varoto, T. M."
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": ""
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": "Heiderich, R. C. X. Balda, M. C. M. Barros, R. Guinsburg, and C. E."
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": ""
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": "Thomaz, “A Convolutional Neural Network-based Mobile Application"
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": ""
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": "to Bedside Neonatal Pain Assessment,” in 2021 34th SIBGRAPI Con-"
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": ""
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": "ference on Graphics, Patterns and Images (SIBGRAPI).\nGramado, Rio"
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": ""
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": "Grande do Sul, Brazil:\nIEEE, Oct. 2021, pp. 394–401."
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": ""
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": "[28] M. Deramgozin, S.\nJovanovic, H. Rabah, and N. Ramzan, “A Hybrid"
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": ""
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": "Explainable AI Framework Applied to Global and Local Facial Expres-"
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": ""
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": "sion Recognition,” in 2021 IEEE International Conference on Imaging"
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": ""
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": "Systems and Techniques\n(IST).\nKaohsiung, Taiwan:\nIEEE, Aug. 2021,"
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": ""
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": "pp. 1–5."
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": ""
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": "[29] A. Ghandeharioun, B. Eoff, B.\nJou,\nand R. Picard,\n“Characterizing"
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": ""
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": "Sources of Uncertainty to Proxy Calibration and Disambiguate Anno-"
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": ""
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": "tator and Data Bias,” in 2019 IEEE/CVF International Conference on"
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": ""
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": "Computer Vision Workshop (ICCVW).\nSeoul, Korea\n(South):\nIEEE,"
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": ""
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": "Oct. 2019, pp. 4202–4206."
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": ""
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": "[30] K. ter Burg and H. Kaya, “Comparing Approaches for Explaining DNN-"
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": ""
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": "Based Facial Expression Classiﬁcations,” Algorithms, vol. 15, no. 10, p."
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": ""
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": "367, Oct. 2022."
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": ""
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": "[31] K. Weitz, T. Hassan, U. Schmid, and J.-U. Garbas, “Deep-learned faces"
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": ""
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": "of pain and emotions: Elucidating the differences of\nfacial expressions"
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": ""
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": "with the help of\nexplainable AI methods,”\ntm - Technisches Messen,"
        },
        {
          "Incongruity\nbetween Modalities\nfor Multimodal Sarcasm Detection,”": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "[32] Y. Zhou, X. Yao, W. Han, Y. Wang, Z. Li, and Y. Li, “Distinguishing"
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "apathy and depression in older adults with mild cognitive impairment us-"
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "ing text, audio, and video based on multiclass classiﬁcation and shapely"
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "International\nJournal\nadditive\nexplanations,”\nof Geriatric Psychiatry,"
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "vol. 37, no. 11, p. gps.5827, Nov. 2022."
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "[33] D. K. Jain, A. Rahate, G. Joshi, R. Walambe, and K. Kotecha, “Employ-"
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "ing Co-Learning to Evaluate the Explainability of Multimodal Sentiment"
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "Analysis,” IEEE Transactions on Computational Social Systems, pp. 1–"
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "8, 2022."
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "[34] M. P. S. Lorente, E. M. Lopez, L. A. Florez, A. L. Espino,\nJ. A.\nI."
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "Martínez, and A. S. de Miguel, “Explaining Deep Learning-Based Driver"
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "Models,” Applied Sciences, vol. 11, no. 8, p. 3321, Apr. 2021."
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "[35] A. R. Asokan, N. Kumar, A. V. Ragam, and S. S. Shylaja, “Interpretabil-"
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "ity\nfor Multimodal\nEmotion Recognition\nusing Concept Activation"
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "Vectors,” in 2022 International\nJoint Conference on Neural Networks."
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "Padua,\nItaly:\nIEEE, Jul. 2022, pp. 01–08."
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "[36] M. Gund, A. R. Bharadwaj,\nand\nI. Nwogu,\n“Interpretable Emotion"
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "2020\n25th\nClassiﬁcation Using Temporal Convolutional Models,”\nin"
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "International Conference on Pattern Recognition (ICPR). Milan,\nItaly:"
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "IEEE, Jan. 2021, pp. 6367–6374."
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "[37] M. Rathod, C. Dalvi, K. Kaur, S. Patil, S. Gite, P. Kamat, K. Kotecha,"
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "A. Abraham, and L. A. Gabralla,\n“Kids’ Emotion Recognition Using"
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "Various Deep-Learning Models with Explainable AI,” Sensors, vol. 22,"
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "no. 20, p. 8066, Oct. 2022."
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "[38] D. Boulanger, M. A. A. Dewan, V. S. Kumar, and F. Lin, “Lightweight"
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "and Interpretable Detection of Affective Engagement\nfor Online Learn-"
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "ers,”\nin 2021 IEEE Intl Conf on Dependable, Autonomic and Secure"
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "Computing,\nIntl Conf\non Pervasive\nIntelligence\nand Computing,\nIntl"
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "Conf on Cloud and Big Data Computing,\nIntl Conf on Cyber Science"
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech).\nAB,"
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "Canada:\nIEEE, Oct. 2021, pp. 176–184."
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "[39] X. Wang,\nJ. He, Z.\nJin, M. Yang, Y. Wang,\nand H. Qu,\n“M2Lens:"
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "Visualizing and Explaining Multimodal Models for Sentiment Analysis,”"
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "IEEE Transactions on Visualization and Computer Graphics, vol. 28,"
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "no. 1, pp. 802–812, Jan. 2022."
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "[40] A. Heimerl, K. Weitz, T. Baur, and E. Andre, “Unraveling ML Models"
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "of Emotion With NOVA: Multi-Level Explainable AI for Non-Experts,”"
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "IEEE Transactions on Affective Computing, vol. 13, no. 3, pp. 1155–"
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "1167, Jul. 2022."
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "[41] X. Zhou, K.\nJin, Y. Shang, and G. Guo, “Visually Interpretable Rep-"
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "resentation Learning for Depression Recognition from Facial\nImages,”"
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "IEEE Transactions on Affective Computing, vol. 11, no. 3, pp. 542–552,"
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "Jul. 2020."
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "[42] B. Kim, M. Wattenberg,\nJ. Gilmer, C. Cai,\nJ. Wexler, F. Viegas,\nand"
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "R. Sayres,\n“Interpretability Beyond Feature Attribution: Quantitative"
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "Testing with Concept Activation Vectors\n(TCAV),”\nin Proceedings of"
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "the 35th International Conference on Machine Learning.\nPMLR, Jul."
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "2018, pp. 2668–2677."
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "[43]\nJ. Adebayo,\nJ. Gilmer, M. Muelly,\nI. Goodfellow, M. Hardt,\nand"
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "the 32nd\nB. Kim, “Sanity checks for saliency maps,” in Proceedings of"
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "International Conference on Neural Information Processing Systems, ser."
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "NIPS’18.\nRed Hook, NY, USA: Curran Associates\nInc., Dec. 2018,"
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "pp. 9525–9536."
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "[44] M. Ghassemi, L. Oakden-Rayner, and A. L. Beam, “The false hope of"
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "current approaches\nto explainable artiﬁcial\nintelligence in health care,”"
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "The Lancet Digital Health, vol. 3, no. 11, pp. e745–e750, Nov. 2021."
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "[45]\nJ. A. Hall, T. G. Horgan, and N. A. Murphy, “Nonverbal Communica-"
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "tion,” Annual Review of Psychology, vol. 70, no. 1, pp. 271–294, 2019."
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "[46]\nT. Miller, P. Howe,\nand L. Sonenberg,\n“Explainable AI: Beware\nof"
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "Inmates Running the Asylum Or: How I Learnt\nto Stop Worrying and"
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "Love the Social and Behavioural Sciences,” arXiv:1712.00547 [cs], Dec."
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "2017."
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "[47]\nF. Doshi-Velez\nand B. Kim,\n“Towards A Rigorous Science\nof\nInter-"
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "pretable Machine Learning,” Mar. 2017."
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "et\n[48] K.\nJ. Rohlﬁng\nal.,\n“Explanation\nas\na\nsocial\npractice:\nToward\na"
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "IEEE\nconceptual\nframework\nfor\nthe\nsocial\ndesign\nof AI\nsystems,”"
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "Transactions on Cognitive and Developmental Systems, pp. 1–1, 2020."
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "[49] H. Kaur, H. Nori, S.\nJenkins, R. Caruana, H. Wallach,\nand J. Wort-"
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "man Vaughan,\n“Interpreting\nInterpretability: Understanding Data Sci-"
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "entists’ Use of\nInterpretability Tools\nfor Machine Learning,”\nin Pro-"
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "ceedings of\nthe 2020 CHI Conference on Human Factors in Computing"
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "Systems.\nNew York, NY, USA: Association for Computing Machinery,"
        },
        {
          "vol. 86, no. 7-8, pp. 404–412, Jul. 2019.": "Apr. 2020, pp. 1–14."
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A systematic review on affective computing: Emotion models, databases, and recent advances",
      "authors": [
        "Y Wang"
      ],
      "year": "2022",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "2",
      "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
      "authors": [
        "S Poria",
        "E Cambria",
        "R Bajpai",
        "A Hussain"
      ],
      "year": "2017",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "3",
      "title": "Deep learning for mobile mental health: Challenges and recent advances",
      "authors": [
        "J Han",
        "Z Zhang",
        "C Mascolo",
        "E André",
        "J Tao",
        "Z Zhao",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "4",
      "title": "Intelligent signal processing for affective computing",
      "authors": [
        "B Schuller",
        "R Picard",
        "E André",
        "J Gratch",
        "J Tao"
      ],
      "year": "2021",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "5",
      "title": "Gender shades: Intersectional accuracy disparities in commercial gender classification",
      "authors": [
        "J Buolamwini",
        "T Gebru"
      ],
      "year": "2018",
      "venue": "Conference on Fairness, Accountability and Transparency"
    },
    {
      "citation_id": "6",
      "title": "Peeking inside the black-box: A survey on explainable artificial intelligence (XAI)",
      "authors": [
        "A Adadi",
        "M Berrada"
      ],
      "year": "2018",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "7",
      "title": "Explainable artificial intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI",
      "authors": [
        "A Arrieta"
      ],
      "year": "2020",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "8",
      "title": "A survey on explainable artificial intelligence (XAI): Toward medical XAI",
      "authors": [
        "E Tjoa",
        "C Guan"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "9",
      "title": "A global taxonomy of interpretable AI: Unifying the terminology for the technical and social sciences",
      "authors": [
        "M Graziani"
      ],
      "year": "2022",
      "venue": "Artificial Intelligence Review"
    },
    {
      "citation_id": "10",
      "title": "Attention, please! A survey of neural attention models in deep learning",
      "authors": [
        "A De Santana Correia",
        "E Colombini"
      ],
      "year": "2022",
      "venue": "Artificial Intelligence Review"
    },
    {
      "citation_id": "11",
      "title": "A review on the attention mechanism of deep learning",
      "authors": [
        "Z Niu",
        "G Zhong",
        "H Yu"
      ],
      "year": "2021",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "12",
      "title": "The mythos of model interpretability",
      "authors": [
        "Z Lipton"
      ],
      "year": "2018",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "13",
      "title": "Automatic Estimation of Action Unit Intensities and Inference of Emotional Appraisals",
      "authors": [
        "D Seuss",
        "T Hassan",
        "A Dieckmann",
        "M Unfried",
        "K Scherer",
        "M Mortillaro",
        "J.-U Garbas"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "14",
      "title": "Face the Truth: Interpretable Emotion Genuineness Detection",
      "authors": [
        "M Cardaioli",
        "A Miolla",
        "M Conti",
        "G Sartori",
        "M Monaro",
        "C Scarpazza",
        "N Navarin"
      ],
      "year": "2022",
      "venue": "2022 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "15",
      "title": "Facial Attention based Convolutional Neural Network for 2D+3D Facial Expression Recognition",
      "authors": [
        "Y Jiao",
        "Y Niu",
        "Y Zhang",
        "F Li",
        "C Zou",
        "G Shi"
      ],
      "year": "2019",
      "venue": "2019 IEEE Visual Communications and Image Processing (VCIP)"
    },
    {
      "citation_id": "16",
      "title": "Facial Expression Recognition Using Spatial-Temporal Semantic Graph Network",
      "authors": [
        "J Zhou",
        "X Zhang",
        "Y Liu",
        "X Lan"
      ],
      "year": "2020",
      "venue": "2020 IEEE International Conference on Image Processing (ICIP)"
    },
    {
      "citation_id": "17",
      "title": "Filters Know How You Feel: Explaining Intermediate Speech Emotion Classification Representations",
      "authors": [
        "A Anand",
        "S Negi",
        "N Narendra"
      ],
      "year": "2021",
      "venue": "2021 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)"
    },
    {
      "citation_id": "18",
      "title": "Human-Centered Emotion Recognition in Animated GIFs",
      "authors": [
        "Z Yang",
        "Y Zhang",
        "J Luo"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "19",
      "title": "I see it in your eyes: Training the shallowest-possible CNN to recognise emotions and pain from muted web-assisted in-the-wild video-chats in real-time",
      "authors": [
        "V Pandit",
        "M Schmitt",
        "N Cummins",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "Information Processing & Management"
    },
    {
      "citation_id": "20",
      "title": "Interpretable Multimodal Capsule Fusion",
      "authors": [
        "J Wu",
        "S Mai",
        "H Hu"
      ],
      "year": "2022",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "21",
      "title": "Modeling Incongruity between Modalities for Multimodal Sarcasm Detection",
      "authors": [
        "Y Wu",
        "Y Zhao",
        "X Lu",
        "B Qin",
        "Y Wu",
        "J Sheng",
        "J Li"
      ],
      "year": "2021",
      "venue": "IEEE MultiMedia"
    },
    {
      "citation_id": "22",
      "title": "Multimodal Affective Analysis Using Hierarchical Attention Strategy with Word-Level Alignment",
      "authors": [
        "Y Gu",
        "K Yang",
        "S Fu",
        "S Chen",
        "X Li",
        "I Marsic"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "23",
      "title": "Multimodal Hierarchical Attention Neural Network: Looking for Candidates Behaviour which Impact Recruiter's Decision",
      "authors": [
        "L Hemamou",
        "A Guillon",
        "J.-C Martin",
        "C Clavel"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "24",
      "title": "On the use of action units and fuzzy explanatory models for facial expression recognition",
      "authors": [
        "E Morales-Vargas",
        "C Reyes-García",
        "H Peregrina",
        "Barreto"
      ],
      "year": "2019",
      "venue": "PLOS ONE"
    },
    {
      "citation_id": "25",
      "title": "Robust Discrimination and Generation of Faces using Compact, Disentangled Embeddings",
      "authors": [
        "B Browatzki",
        "C Wallraven"
      ],
      "year": "2019",
      "venue": "2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)"
    },
    {
      "citation_id": "26",
      "title": "SGS2Net: Deep representation of facial expression by graph-preserving sparse coding",
      "authors": [
        "R Zhi",
        "M Wan",
        "X Hu"
      ],
      "year": "2020",
      "venue": "Journal of Electronic Imaging"
    },
    {
      "citation_id": "27",
      "title": "A Convolutional Neural Network-based Mobile Application to Bedside Neonatal Pain Assessment",
      "authors": [
        "L Carlini",
        "L Ferreira",
        "G Coutrin",
        "V Varoto",
        "T Heiderich",
        "R Balda",
        "M Barros",
        "R Guinsburg",
        "C Thomaz"
      ],
      "year": "2021",
      "venue": "2021 34th SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI)"
    },
    {
      "citation_id": "28",
      "title": "A Hybrid Explainable AI Framework Applied to Global and Local Facial Expression Recognition",
      "authors": [
        "M Deramgozin",
        "S Jovanovic",
        "H Rabah",
        "N Ramzan"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Conference on Imaging Systems and Techniques (IST)"
    },
    {
      "citation_id": "29",
      "title": "Characterizing Sources of Uncertainty to Proxy Calibration and Disambiguate Annotator and Data Bias",
      "authors": [
        "A Ghandeharioun",
        "B Eoff",
        "B Jou",
        "R Picard"
      ],
      "year": "2019",
      "venue": "2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)"
    },
    {
      "citation_id": "30",
      "title": "Comparing Approaches for Explaining DNN-Based Facial Expression Classifications",
      "authors": [
        "K Burg",
        "H Kaya"
      ],
      "year": "2022",
      "venue": "Algorithms"
    },
    {
      "citation_id": "31",
      "title": "Deep-learned faces of pain and emotions: Elucidating the differences of facial expressions with the help of explainable AI methods",
      "authors": [
        "K Weitz",
        "T Hassan",
        "U Schmid",
        "J.-U Garbas"
      ],
      "year": "2019",
      "venue": "tm -Technisches Messen"
    },
    {
      "citation_id": "32",
      "title": "Distinguishing apathy and depression in older adults with mild cognitive impairment using text, audio, and video based on multiclass classification and shapely additive explanations",
      "authors": [
        "Y Zhou",
        "X Yao",
        "W Han",
        "Y Wang",
        "Z Li",
        "Y Li"
      ],
      "year": "2022",
      "venue": "International Journal of Geriatric Psychiatry"
    },
    {
      "citation_id": "33",
      "title": "Employing Co-Learning to Evaluate the Explainability of Multimodal Sentiment Analysis",
      "authors": [
        "D Jain",
        "A Rahate",
        "G Joshi",
        "R Walambe",
        "K Kotecha"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Computational Social Systems"
    },
    {
      "citation_id": "34",
      "title": "Explaining Deep Learning-Based Driver Models",
      "authors": [
        "M Lorente",
        "E Lopez",
        "L Florez",
        "A Espino",
        "J Martínez",
        "A De Miguel"
      ],
      "year": "2021",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "35",
      "title": "Interpretability for Multimodal Emotion Recognition using Concept Activation Vectors",
      "authors": [
        "A Asokan",
        "N Kumar",
        "A Ragam",
        "S Shylaja"
      ],
      "year": "2022",
      "venue": "2022 International Joint Conference on Neural Networks"
    },
    {
      "citation_id": "36",
      "title": "Interpretable Emotion Classification Using Temporal Convolutional Models",
      "authors": [
        "M Gund",
        "A Bharadwaj",
        "I Nwogu"
      ],
      "year": "2021",
      "venue": "2020 25th International Conference on Pattern Recognition (ICPR)"
    },
    {
      "citation_id": "37",
      "title": "Kids' Emotion Recognition Using Various Deep-Learning Models with Explainable AI",
      "authors": [
        "M Rathod",
        "C Dalvi",
        "K Kaur",
        "S Patil",
        "S Gite",
        "P Kamat",
        "K Kotecha",
        "A Abraham",
        "L Gabralla"
      ],
      "year": "2022",
      "venue": "Sensors"
    },
    {
      "citation_id": "38",
      "title": "Lightweight and Interpretable Detection of Affective Engagement for Online Learners",
      "authors": [
        "D Boulanger",
        "M Dewan",
        "V Kumar",
        "F Lin"
      ],
      "year": "2021",
      "venue": "2021 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress"
    },
    {
      "citation_id": "39",
      "title": "M2Lens: Visualizing and Explaining Multimodal Models for Sentiment Analysis",
      "authors": [
        "X Wang",
        "J He",
        "Z Jin",
        "M Yang",
        "Y Wang",
        "H Qu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Visualization and Computer Graphics"
    },
    {
      "citation_id": "40",
      "title": "Unraveling ML Models of Emotion With NOVA: Multi-Level Explainable AI for Non-Experts",
      "authors": [
        "A Heimerl",
        "K Weitz",
        "T Baur",
        "E Andre"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "41",
      "title": "Visually Interpretable Representation Learning for Depression Recognition from Facial Images",
      "authors": [
        "X Zhou",
        "K Jin",
        "Y Shang",
        "G Guo"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "42",
      "title": "Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)",
      "authors": [
        "B Kim",
        "M Wattenberg",
        "J Gilmer",
        "C Cai",
        "J Wexler",
        "F Viegas",
        "R Sayres"
      ],
      "year": "2018",
      "venue": "Proceedings of the 35th International Conference on Machine Learning"
    },
    {
      "citation_id": "43",
      "title": "Sanity checks for saliency maps",
      "authors": [
        "J Adebayo",
        "J Gilmer",
        "M Muelly",
        "I Goodfellow",
        "M Hardt",
        "B Kim"
      ],
      "year": "2018",
      "venue": "Proceedings of the 32nd International Conference on Neural Information Processing Systems, ser. NIPS'18"
    },
    {
      "citation_id": "44",
      "title": "The false hope of current approaches to explainable artificial intelligence in health care",
      "authors": [
        "M Ghassemi",
        "L Oakden-Rayner",
        "A Beam"
      ],
      "year": "2021",
      "venue": "The Lancet Digital Health"
    },
    {
      "citation_id": "45",
      "title": "Nonverbal Communication",
      "authors": [
        "J Hall",
        "T Horgan",
        "N Murphy"
      ],
      "year": "2019",
      "venue": "Annual Review of Psychology"
    },
    {
      "citation_id": "46",
      "title": "Explainable AI: Beware of Inmates Running the Asylum Or: How I Learnt to Stop Worrying and Love the Social and Behavioural Sciences",
      "authors": [
        "T Miller",
        "P Howe",
        "L Sonenberg"
      ],
      "year": "2017",
      "venue": "Explainable AI: Beware of Inmates Running the Asylum Or: How I Learnt to Stop Worrying and Love the Social and Behavioural Sciences",
      "arxiv": "arXiv:1712.00547"
    },
    {
      "citation_id": "47",
      "title": "Towards A Rigorous Science of Interpretable Machine Learning",
      "authors": [
        "F Doshi-Velez",
        "B Kim"
      ],
      "year": "2017",
      "venue": "Towards A Rigorous Science of Interpretable Machine Learning"
    },
    {
      "citation_id": "48",
      "title": "Explanation as a social practice: Toward a conceptual framework for the social design of AI systems",
      "authors": [
        "K Rohlfing"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "49",
      "title": "Interpreting Interpretability: Understanding Data Scientists' Use of Interpretability Tools for Machine Learning",
      "authors": [
        "H Kaur",
        "H Nori",
        "S Jenkins",
        "R Caruana",
        "H Wallach",
        "J Vaughan"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems"
    }
  ]
}