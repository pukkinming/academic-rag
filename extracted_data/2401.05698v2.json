{
  "paper_id": "2401.05698v2",
  "title": "Hicmae: Hierarchical Contrastive Masked Autoencoder For Self-Supervised Audio-Visual Emotion Recognition",
  "published": "2024-01-11T07:00:07Z",
  "authors": [
    "Licai Sun",
    "Zheng Lian",
    "Bin Liu",
    "Jianhua Tao"
  ],
  "keywords": [
    "Audio-Visual Emotion Recognition",
    "Self-Supervised Learning",
    "Masked Autoencoder",
    "Contrastive Learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Audio-Visual Emotion Recognition (AVER) has garnered increasing attention in recent years for its critical role in creating emotionaware intelligent machines. Previous efforts in this area are dominated by the supervised learning paradigm. Despite significant progress, supervised learning is meeting its bottleneck due to the longstanding data scarcity issue in AVER. Motivated by recent advances in self-supervised learning, we propose Hierarchical Contrastive Masked Autoencoder (HiCMAE), a novel self-supervised framework that leverages large-scale self-supervised pre-training on vast unlabeled audio-visual data to promote the advancement of AVER. Following prior arts in self-supervised audio-visual representation learning, HiCMAE adopts two primary forms of selfsupervision for pre-training, namely masked data modeling and contrastive learning. Unlike them which focus exclusively on top-layer representations while neglecting explicit guidance of intermediate layers, HiCMAE develops a three-pronged strategy to foster hierarchical audio-visual feature learning and improve the overall quality of learned representations. Firstly, it incorporates hierarchical skip connections between the encoder and decoder to encourage intermediate layers to learn more meaningful representations and bolster masked audio-visual reconstruction. Secondly, hierarchical cross-modal contrastive learning is also exerted on intermediate representations to narrow the audio-visual modality gap progressively and facilitate subsequent cross-modal fusion. Finally, during downstream fine-tuning, HiCMAE employs hierarchical feature fusion to comprehensively integrate multi-level features from different layers. To verify the effectiveness of HiCMAE, we conduct extensive experiments on 9 datasets covering both categorical and dimensional AVER tasks. Experimental results show that our method significantly outperforms state-of-the-art supervised and self-supervised audio-visual methods, which indicates that HiCMAE is a powerful audio-visual emotion representation learner. Codes and models are publicly available at https://github.com/sunlicai/HiCMAE.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "\"The question is not whether intelligent machines can have any emotions, but whether machines can be intelligent without any emotions.\"",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "-Marvin Minsky",
      "text": "Emotions are fundamental to the multifaceted spectrum of human experience, influencing our cognition, decision-making, and interpersonal interactions  [1] . They also play a pivotal role in developing intelligent machines and achieving the ultimate goal of emotional artificial intelligence, as Marvin Minsky, a pioneer of artificial intelligence, highlighted above  [2] . Typically, people express and perceive emotions through multiple modalities. Previous psychological studies have demonstrated that the language modality (i.e., verbal information) only contributes 7% to the perception of emotions in our daily communication, while the audio (e.g., tone and intonation) and visual (e.g., facial expressions) modalities significantly predominate, contributing to 38% and 55% respectively  [3, 4] . As a result, the last two decades have witnessed a surge of interest in automatic Audio-Visual Emotion Recognition (AVER) in the affective computing community. The task of AVER is to extract emotion-related representations from audio-visual signals and then integrate them to identify the subject's emotional state.\n\nSeveral concrete examples are depicted in Fig.  1 .\n\nEarly studies on AVER concentrate on developing various hand-engineering features for audio and video modalities  [6, 4] . With the advent of the deep learning era, a new trend has emerged towards learning features directly from raw audiovisual data by training deep supervised neural networks in an end-to-end manner  [7, 8, 9] . Despite considerable advancements, supervised learning is heavily constrained by its reliance on large amounts of labeled data to achieve satisfactory performance. This reliance significantly hampers further progress of supervised methods due to the longstanding issue of data scarcity in AVER  [10, 11] .\n\nRecently, another deep learning paradigm, i.e., selfsupervised learning, which can learn powerful representations from vast unlabeled data, has revolutionized many research areas  [12, 13, 14, 15, 16, 17] . In particular, masked data modeling (e.g., MAE  [17] ) and contrastive learning (e.g., CLIP  [16] ) are two prominent methods and have demonstrated great success in self-supervised visual representation learning. Specifically, masked data modeling aims to reconstruct the raw data from masked inputs, while contrastive learning encourages the semantic alignment of different modalities. They are also combined and extended to learn generic audio-visual representations in recent studies, such as CAV-MAE  [18]  and MAViL  [19] . However, due to the domain gap between upstream pretraining and downstream AVER tasks, the learned representations are typically not suitable for AVER. More recently, Sadok et al.  [20]  developed a vector-quantized masked autoencoder (VQ-MAE-AV) for AVER. Although achieving promising results, VQ-MAE-AV requires pre-trained variational autoencoders for vector quantization and thus cannot be pre-trained in a single stage. More importantly, several studies on masked image modeling have shown that intermediate layers are essential to self-supervised visual representation learning and explicitly guiding them can improve the quality of learned representations  [21, 22] . However, the aforementioned methods fail to achieve that goal as they solely operate on representations from the top layer and neglect explicit guidance on intermediate layers, thus impeding feature evolution across layers and leading to suboptimal results in downstream tasks.\n\nTo address the above challenges, this paper builds on top of MAE and contrastive learning to propose Hierarchical Contrastive Masked Autoencoder (HiCMAE), a novel selfsupervised framework tailored for AVER. As shown in Fig.  3 , it is mainly composed of two audio-visual encoders, a crossmodal fusion encoder, and two lightweight audio-visual decoders. To foster hierarchical audio-visual feature learning and improve the overall quality of learned representations, HiC-MAE develops a three-pronged strategy: (1) Drawing inspiration from the architecture design in U-Net  [26] , HiCMAE introduces hierarchical skip connections between the encoder We present Pearson correlation coefficient on Werewolf-XL  [23]  and weighted F1-score on AVCAffe  [24]  and MER-MULTI  [25] . For other datasets, we show weighted average recall (WAR). and decoder to drive intermediate encoder layers to learn more useful representations and aid the decoder in accomplishing the task of masked audio-visual reconstruction. (  2 ) Considering the natural audio-visual correspondences in videos, hierarchical cross-modal contrastive learning is also applied to intermediate representations of audio-visual encoders to reduce heterogeneous modality gap in a progressive manner and enhance cross-modal fusion in subsequent layers. (3) Since different layers typically capture distinct levels of information, HiCMAE performs hierarchical feature fusion during downstream finetuning to comprehensively integrate multi-level features from various encoder layers. To demonstrate the effectiveness of HiCMAE, we perform large-scale self-supervised pre-training on VoxCeleb2  [27]  and evaluate the pre-trained model on nine datasets encompassing both categorical and dimensional AVER tasks. As illustrated in Fig.  2 , our HiCMAE significantly outperforms state-of-the-art supervised or self-supervised audiovisual methods. For example, HiCMAE surpasses the bestperforming VQ-MAE-AV  [20]  by +4.49% WAR on CREMA-D (6-class)  [28]  and beats the state-of-the-art T-MEP  [9]  by +5.02% WAR on MAFW (11-class)  [5]  and +6.16% WAR on DFEW  [29] . Moreover, extensive ablation studies also justify various design choices in HiCMAE.\n\nIn summary, our main contributions are three-fold:\n\n• We present HiCMAE, a novel self-supervised framework for AVER, as an early endeavor to leverage large-scale self-supervised pre-training to address the dilemma of supervised methods and promote the development of AVER.\n\n• Unlike previous methods, HiCMAE introduces a threepronged approach to foster hierarchical audio-visual feature learning and the ablation studies verify its efficacy.\n\n• Comprehensive experiments across 9 datasets covering both categorical and dimensional AVER tasks demonstrate that HiCMAE beats state-of-the-art audio-visual methods by significant margins, indicating that HiCMAE is a powerful audio-visual emotion representation learner.",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Audio-Visual Emotion Recognition",
      "text": "Most studies on Audio-Visual Emotion Recognition (AVER) fall into the supervised learning paradigm. They mainly focus on two important aspects: unimodal feature extraction and audio-visual information fusion. As for the first aspect, researchers have developed and exploited numerous features in the past two decades  [6, 4, 10, 30] . Early studies concentrate on various handcrafted features for two modalities, such as IS13  [31]  and eGeMAPS  [32]  for audio, LBP-TOP  [33]  and HOG  [34]  for video. With the advent of deep learning, a large amount of deep supervised models trained on large audio and image/video datasets have emerged as powerful audiovisual feature extractors  [35, 36, 37, 38, 39, 25] , such as PANNs  [40]  and VGGish  [41]  for audio, VGGFace  [42]  and C3D  [43]  for video. There are also lots of attempts to train endto-end deep neural networks on raw audio and video emotion data  [44, 8, 45, 29, 46] . In recent years, plenty of large selfsupervised pre-trained models have demonstrated great success in audio (e.g., Wav2vec2.0  [47]  and HuBERT  [48] ) or video (e.g., SVFAP  [49]  and MAE-DFER  [50] ) emotion recognition. After unimodal feature extraction, the next crucial step is audio-visual information fusion. Current fusion strategies can be roughly divided into three categories, i.e., early fusion, late fusion, and model-level fusion  [6, 4] . Early fusion typically combines audio-visual features at the input level  [36, 39] . In contrast, late fusion integrates audio-visual predictions at the decision level  [37, 38] . The most commonly used strategy is model-level fusion  [7, 51, 52, 53, 54, 55, 56, 57, 9] . For example, MulT  [51]  utilizes cross-modal attention to capture dense interactions in unaligned audio-visual feature sequences. EMT  [57]  improves MulT by introducing the global multimodal context and employing it to interact with local unimodal features to achieve efficient cross-modal information exchange. The recent T-MEP  [9]  adopts a similar strategy with MulT to fuse finegrained audio-visual tokens by combining self-attention and cross-attention mechanisms in an interleaving manner.\n\nDespite promising results, the aforementioned methods are mostly supervised learning methods. They either train from scratch or rely on pre-trained models from other different tasks for initialization, which are thus severely constrained by the data scarcity issue in AVER or suffer from substantial domain shifts. In contrast, this paper presents an early attempt to leverage large-scale self-supervised audio-visual pre-training on massive unlabeled data to largely advance the development of audio-visual emotion recognition.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Self-Supervised Audio-Visual Representation Learning",
      "text": "In general, there are three kinds of methods for learning generic audio-visual representations through self-supervision, namely contrastive learning, masked data modeling, and the hybrid method. Contrastive learning exploits the natural audiovisual correspondence in videos as a free signal for selfsupervision  [58, 59, 60] . In contrast, the goal of masked data modeling is to reconstruct the original data from its masked input. Motivated by its recent success in the image and video domain  [17, 61, 62] , Georgescu et al.  [63]  extend it to the audio-visual domain and achieve significant improvements over previous methods. Recently, a few studies have made attempts to combine the former two kinds of methods, resulting in the hybrid method. In particular, CAV-MAE  [18]  integrates MAE and cross-modal contrastive learning and shows that they are complementary. MAViL  [19]  further introduces intra-modal contrastive learning and masked self-training on contextualized features to improve audio-visual pre-training. Although these methods have demonstrated great success in general audiovisual tasks, the learned representations are typically not suitable for AVER because they are trained on general scene or action videos instead of facial videos in AVER. Recently, VQ-MAE-AV  [20]  introduces a vector-quantized MAE for AVER. Despite promising results, it requires two-stage pre-training. In addition, the more important issue is that the aforementioned audio-visual masked autoencoders fail to promote hierarchical audio-visual feature learning as they focus exclusively on toplayer representations while neglecting explicit guidance of intermediate layers, thus impeding feature evolution across layers and leading to sub-optimal performance  [21, 22] . Therefore, our HiCAME presents a three-pronged approach to foster hierarchical audio-visual feature learning and demonstrates improved performance.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Method",
      "text": "In this section, we elaborate on Hierarchical Contrastive Masked AutoEncoder (HiCMAE), a novel self-supervised audio-visual emotion representation learning framework for AVER. The training process of HiCMAE includes two steps, i.e., self-supervised pre-training (Section 3.1-3.3) on largescale unlabeled AVER data and downstream fine-tuning (Section 3.4) on limited labeled AVER data. Specifically, the selfsupervised pre-training pipeline of HiCMAE is illustrated in Fig.  3 . It mainly consists of two modality-specific encoders, a cross-modal fusion encoder, and two lightweight modalityspecific decoders. HiCMAE adopts two primary forms of self-supervision for pre-training: masked data modeling (i.e., masked audio-visual reconstruction) and contrastive learning. Moreover, it introduces a three-pronged strategy to promote hierarchical audio-visual feature learning during both pre-training and fine-tuning, including hierarchical skip connections between the encoder and decoder (Section 3.1), hierarchical crossmodal contrastive learning (Section 3.2), and hierarchical feature fusion for downstream fine-tuning (Section 3.4).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Masked Audio-Visual Reconstruction With Hierarchical",
      "text": "Skip Connections As depicted in Fig.  3 , HiCMAE follows MAE  [17]  to adopt an asymmetric encoder-decoder architecture for efficient self-  supervised audio-visual pre-training. The raw audio and video inputs are first embedded into tokens and then masked to filter out a substantial proportion of tokens. Next, the visible (i.e., unmasked) audio and video tokens are processed by their modality-specific encoders and a cross-modal fusion encoder. After feature encoding, the visible audio and video tokens are padded with learnable masked tokens and then passed through lightweight modality-specific decoders for final audio and video reconstruction. Note that, unlike conventional masked autoencoders, hierarchical skip connections are added between audio-visual encoders and decoders to better guide the encoder feature learning at different levels and promote masked audio-visual reconstruction by providing the decoder with multi-level features.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Data Embedding And Token Masking",
      "text": "Since we use Transformer  [64]  as the main component of the encoder, it is necessary to embed audio and video inputs into a sequence of discrete tokens. Formally, we denote the facial video frames as X v ∈ R T v ×H×W×3 (T v is the number of frames, H and W denote the height and width, and 3 represents RGB channels) and the audio spectrogram as X a ∈ R T a ×F (T a is the temporal length and F denotes the number of frequency channels). We utilize a cube embedding layer with a size of 2 × 16 × 16 to split X v into video tokens\n\n16 is the number of video tokens, C is the number of feature channels). As for X a , we employ a patch embedding layer with a size of 16 × 16 to split it into audio tokens X ′ a ∈ R N a ×C (N a = T a 16 • F 16 is the number of audio tokens). After data embedding, we mask out a large proportion of audio and video tokens to make audio-visual reconstruction a non-trivial self-supervised task and significantly reduce the pretraining cost at the same time. Considering the high temporal redundancy and correlation in video data, we adopt the tube masking (i.e., each temporal slice has the same masking pattern) strategy  [61] . For audio, we simply use random masking  [65] . The masking ratios for audio and video tokens are set to ρ a = 80% and ρ v = 90% respectively  [61, 65] . After token masking, only the visible video tokens\n\n• N a ) will be processed by the encoder introduced next.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Encoder",
      "text": "The encoder in HiCMAE comprises two modality-specific encoders and a cross-modal fusion encoder. The former respects the diversity of audio-visual information and aims to learn unique characteristics in each modality, while the latter is used to capture meaningful cross-modal interactions and reinforce the representation of one modality with the supplementary information from the other modality for better masked audiovisual reconstruction.\n\nModality-Specific Encoder. As a general self-supervised pre-training framework, there can be many choices for the architecture of modality-specific encoders. For simplicity, we follow MAE  [17]  and VideoMAE  [61]  to employ standard Transformer  [64]  as audio and video encoders. Other efficient architectures such as LGI-Former  [50]  can be explored in future work. The audio and video encoders consist of N s Transformer layers. Each Transformer layer is mainly composed of Multi-Head Self-Attention (MHSA) and Feed-Forward Network (FFN):\n\nwhere E 0 m = X ′′ m , m ∈ {a, v} denotes the audio or video modality, j ∈ {1, ..., N s } is the layer index, and LN stands for layer normalization  [64] . For MHSA in Eq. (  1 ), it calculates dotproduct attention to learn intrinsic dependency relationships in input audio/video tokens:\n\nwhere\n\nH is the number of attention heads, and d h = C/H is the feature dimension in each attention head. For FFN in Eq. (  1 ), it comprises two linear projection layers with a GELU  [66]  activation function in between:\n\nwhere\n\n, and b 2 ∈ R C are learnable parameters.\n\nCross-Modal Fusion Encoder. After unimodal feature encoding, HiCMAE employs a cross-modal fusion encoder to capture meaningful interactions in audio-visual modalities and enable cross-modal reinforcement  [51, 57] . We mainly utilize multi-head cross-attention (MHCA) to implement this module. MHCA shares the same spirit with MHSA. The main difference is that MHCA accepts two modalities as input, with one as the target modality and the other as the source modality. The formulation of MHCA is given as follows:\n\nwhere the notation is similar to that in Eq.  (2) . By combining MHCA, MHSA, and FFN, we obtain one half of the fusion encoder which can achieve cross-modal reinforcement from audio to video: where j ∈ {1, ..., N f } is the layer index, N f is the number of fusion encoder layers, E 0 a→v = E N s v is the output of video encoder, and E 0 v→a = E N s a is the output of audio encoder. Similarly, the other half of the fusion encoder enables cross-modal reinforcement from video to audio:",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Decoder",
      "text": "After feature encoding, HiCMAE utilizes two modalityspecific decoders for final masked audio-visual reconstruction. Previous studies have demonstrated that a small-capacity model is enough to reconstruct masked information  [17, 61, 65] . Therefore, we follow them to employ a lightweight Transformer-based model as the decoder, whose number of layers is much less than that of the encoder, to largely reduce the computation burden during self-supervised pre-training.\n\nTo begin with, we denote audio and video tokens output by the last fusion encoder layer as E N f v→a and E N f a→v respectively. Then we pad them with learnable mask tokens M m (m ∈ {a, v}), and fixed sinusoidal positional embeddings PE d are added to retain positional information. After that, we obtain the input tokens for audio and video decoders, i.e., D 0\n\nAs shown in Fig.  4 , unlike previous audio-visual masked autoencoders which solely operate on encoder representation from the last layer and neglect explicit guidance on other layers  [18, 63, 19, 20] , HiCMAE incorporates hierarchical skip connections between intermediate encoder and decoder layers to explicitly steer encoder feature learning of different levels and assist the decoder to com-plete the task of masked audio-visual reconstruction. Specifically, HiCMAE adds an MHCA layer before each (except for the first) Transformer layer in the decoder. Given the tokens D k m (m ∈ {a, v}) output by the kth Transformer layer, the skip connection constructed by MHCA enables it to directly attend to the intermediate encoder representations E j m with different levels of information (from high-level coarse-grained to lowlevel fine-grained) and extract useful information to reinforce itself. Then, as usual, a standard Transformer layer is followed to exploit global self-attention to infer masked information. The whole process can be formulated as follows:\n\nwhere k ∈ {2, ..., N d } is the decoder layer index, j ∈ {q 1 , ..., q N c } is the selected encoder layer index, N c is the number of skip connections, and q i ∈ {1, ..., N s }. Reconstruction Loss. Finally, the decoder output D N d m is passed through a linear projection layer to predict the masked information in each modality. We calculate the mean squared error between the ground truth and reconstructed data in the masked positions for each modality and sum them to get the reconstruction loss:\n\nwhere X m (m ∈ {a, v}) denotes the original input, Xm is the reconstructed data, M m denotes the masked positions. Note that, for the visual modality, we follow  [50]  to reconstruct both spatial appearance and temporal motion information (i.e., frame difference signals).",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Hierarchical Cross-Modal Contrastive Learning",
      "text": "The natural audio-visual correspondences in videos offer a free and useful signal for learning self-supervised representations. Therefore, in addition to masked reconstruction, HiCMAE also utilizes Hierarchical Cross-Modal Contrastive Learning (HCMCL) as a supplement for improved selfsupervised audio-visual pre-training (Fig.  3 ). The main benefits of HCMCL are two-fold: 1) HCMCL can narrow the representation gap between audio and video modalities; 2) HCMCL enables better audio-visual information fusion in the subsequent cross-modal encoder. Note that, as shown in Fig.  3 , different from conventional contrastive learning which is only applied to high-level (i.e., the last layer) features, HCMCL is imposed on multiple intermediate (including both high-level and low-level) features in audio and video encoders to achieve latent representation alignment in a progressive manner.\n\nTo align with hierarchical skip connections in Section 3.1.3, we use the same N c selected encoder layers (i.e., {q 1 , ..., q N c }, q i ∈ {1, ..., N s }) for HCMCL. Since HCMCL is conducted within a batch of samples, for convenience, we add an additional subscript i to the notation of encoder tokens (i.e., E j i,m ) to indicate the sample index in this batch. Given a batch of N audio tokens {E j 1,a , ..., E j N,a } and N video tokens {E j 1,v , ..., E j N,v } from jth ( j ∈ {q 1 , ..., q N c }) encoder layer, we first perform global average pooling to obtain the sample-level features, i.e., e j i,a = AvgPool(E j i,a ) and e j i,v = AvgPool(E j i,v ). After that, we utilize symmetric InfoNCE loss  [67]  for HCMCL. For a batch of audio features e j a = {e j 1,a , ..., e j N,a } and video features e j v = {e j 1,v , ..., e j N,v }, the symmetric InfoNCE loss maximizes the cosine similarity of N paired audio-visual features (i.e., from the same sample) in the batch while minimizing the cosine similarity of features of N(N -1) incorrect pairings (i.e., from different samples):\n\nL(e j a , e j v ) = -\n\nwhere sim(x, y) = x ⊤ y ||x||•||y|| is the cosine similarity between x and y, τ is the temperature factor. Finally, we sum the InfoNCE loss across N c selected encoder layers to obtain the overall contrastive loss for HCMCL:\n\na , e j v ).\n\n(10)",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Overall Pre-Training Loss",
      "text": "By combining masked audio-visual reconstruction and hierarchical cross-modal contrastive learning, we obtain the overall loss for self-supervised audio-visual pre-training as follows:\n\nwhere λ is the weight factor for balancing the hierarchical contrastive loss.\n\nDuring self-supervised pre-training on massive unlabeled AVER data, the audio-visual emotion semantics are implicitly modeled by L in Eq.  (11) . Despite implicit modeling, the visualization analysis of masked audio-visual reconstruction in Section 4.6.1 shows that the emotion-related information (e.g., smiles in video frames and harmonics in the audio spectrogram) can be well restored by reasoning in limited visible contexts, indicating that HiCMAE can capture audio-visual emotion semantics via large-scale self-supervised pre-training.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Hierarchical Feature Fusion For Downstream Fine-Tuning",
      "text": "After self-supervised pre-training, we discard the lightweight decoders and only use the encoders in HiCMAE for downstream fine-tuning on limited labeled AVER data. To benefit downstream emotion recognition tasks, we utilize both crossmodal and unimodal features from the encoders. The crossmodal features come from the cross-modal fusion encoder. We simply perform global average pooling to output tokens from the last fusion layer. The unimodal features come from modality-specific encoders. To fully exploit features of different levels, we use learnable weights to combine features from different audio/video encoder layers. Thus, the overall feature can be obtained as follows:\n\nv→a , e a , e v ),\n\nwhere e\n\nAfter obtaining the overall feature e, we simply use a linear layer to project it to get the final emotion prediction ŷ. For the classification task, we utilize the classic cross-entropy loss for model fine-tuning:\n\nwhere ŷ = [ŷ 1 , ..., ŷK ] ∈ R K denotes the prediction, y = [y 1 , ..., y K ] ∈ R K is the target, and K is the number of emotion categories. For the regression task, we compute the mean square error between the target and the prediction:\n\nwhere ŷ ∈ R D , ŷ ∈ R D , and D is the number of emotion dimensions. During downstream fine-tuning, the audio-visual emotion semantics are explicitly modeled by L CLS in Eq. (  13 ) or L REG in Eq. (  14 ).",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Implementation Details",
      "text": "We develop three versions of HiCMAE (i.e., base: HiCMAE-B, small: HiCMAE-S, tiny: HiCMAE-T) to meet various needs in real-world applications. Their main difference is the size of hidden units (C = 512, C = 384, and C = 256, respectively) in the encoder. For three models, we use N s = 10 layers in modality-specific encoders, N f = 2 layers in the crossmodal fusion encoder, and N d = 4 layers in the lightweight decoder. We introduce three hierarchical skip connections between the modality-specific encoder and the decoder, specifically between the 4th encoder layer and the 2nd decoder layer, the 7th encoder layer and 3rd decoder layer, as well as between the 10th encoder layer and the 4th decoder layer. The hierarchical cross-modal contrastive learning is also applied to these selected audio-visual encoder layers.\n\nWe pre-train HiCMAE on a very large audio-visual dataset VoxCeleb2  [27] . It contains more than one million video clips from over six thousand celebrities. VoxCeleb2 is split into a development set and a test set. In this paper, we only use its development set which has 1,092,009 video clips for self-supervised pre-training. For each video clip, we sample 16 consecutive frames with a temporal stride of 4 frames and follow  [50]  to extract 160 × 160 patch in each frame to obtain the video input with a size of 16 × 160 × 160 × 3. The corresponding audio waveform (2.56s) is also extracted and converted into a 128dimensional log Mel filterbank feature sequence using a 25ms Hanning window with a hop length of 10ms  [18] , resulting in the audio spectrogram with a size of 256 × 128. The loss weight in Eq. (  11 ) is set to λ = 0.0025. The temperature factor for contrastive learning is fixed to τ = 0.07. We pre-train HiC-MAE for 100 epochs using four Nvidia Tesla V100 GPUs with a total batch size of 160 and a base learning rate of 3e -4. It takes about five days to complete the pre-training. For downstream tasks, we fine-tune the pre-trained model for 50 or 100 epochs with a total batch size of 56 and a base learning rate of 1e -3. During inference, we follow  [50]  to uniformly sample two clips from each video and calculate their mean score as the final prediction. Other hyper-parameters for pre-training and fine-tuning can refer to  [61, 65, 18]  for details.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Categorical Audio-Visual Emotion Recognition:",
      "text": "In-the-Wild Setting",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Datasets",
      "text": "The basic dataset information is summarized in Table  1 .\n\nMAFW  [5]  is a large-scale multimodal compound in-thewild affective dataset. It consists of 10,045 video clips annotated with 11 common emotions (including seven basic emotions, contempt, anxiety, helplessness, and disappointment). Each video clip is also accompanied by several textual sentences to describe the subject's affective behaviors. The dataset provides an 11-class single-labeled set (9,172 video clips) and a 43-class compound set (8,996 video clips). For model evaluation, we follow the original paper to adopt a 5-fold crossvalidation protocol.\n\nDFEW  [29]  comprises 16,372 video clips which are extracted from over 1,500 high-definition movies. This dataset presents several challenging characteristics, such as extreme illumination and occlusion. The video clips are annotated with seven basic emotions (i.e., happy, sad, neutral, anger, surprise, disgust, and fear). To align with previous work  [29, 50, 9] , we perform 5-fold cross-validation on 11,697 single-labeled clips for evaluation.\n\nMER-MULTI  [25]  provides 3,373 training video clips originating from Chinese TV series and movies. The dataset is annotated with six emotions, including neutral, anger, happiness, sadness, worry, and surprise. We follow the original paper to conduct 5-fold cross-validation on 3,373 video clips for hyperparameter tuning and evaluate the model on a held-out test set with 411 video clips.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Results On Mafw",
      "text": "We first present the results of 11 single-labeled emotions on MAFW  [5]  in Table  2 . Compared with state-of-the-art and RoBERTa  [81] ) as unimodal encoders and shows degraded performance without using them. Therefore, these comparison results greatly demonstrate the powerful learning capacity of our method and the superiority of large-scale self-supervised pre-training over traditional supervised learning.\n\nIn addition to the overall performance, we also provide the detailed results of each emotion in Table  2 . As can be seen, our method achieves superior performance across most emotions, such as anger, disgust, sadness, contempt, and disappointment. It is worth noting that the samples of contempt and disappointment account for only 2.57% and 1.98% of the total. Some baseline methods completely fail to recognize these samples due to the imbalanced distribution, while our method improves the previous best result by +7.18% for contempt and +8.14% for disappointment.\n\nAs for unimodal results, our method still shows strong performance when compared with state-of-the-art unimodal baselines. Specifically, when only fine-tuning the audio encoder, we achieve competitive or even better results than three cuttingedge large pre-trained speech models (i.e., Wav2Vec2  [47] , Hu-BERT  [48] , and WavLM-Plus  [71] ), while requiring significantly fewer parameters and computational costs (e.g., 8M parameters and 1G FLOPS in HiCMAE-T versus 95M parameters and 18G FLOPs in HuBERT). For the visual modality, the proposed method achieves the best trade-off between model performance and model size. For example, HiCMAE-B outperforms the previous state-of-the-art self-supervised method MAE-DFER  [50]  which is also pre-trained on VoxCeleb2 by +0.48% UAR and +0.53% WAR, while being 2.6× smaller and using 36% fewer FLOPs.\n\nThe results of 43 compound emotions on MAFW are shown in Table  3 . This task is quite challenging due to the high difficulty of distinguishing similar compound classes (e.g., 'anger anxiety', 'anger disgust', and 'anger disgust anxiety') and extremely imbalanced distribution  [5] . For audio-visual modalities, when compared with state-of-the-art T-MEP  [9] , our method achieves competitive or slightly better performance in terms of UAR and WAR. When evaluating macro-averaged F1-score and AUC, our largest model shows significant improvement over the best-performing T-ESFL  [5]  (i.e., 12.16% versus 8.44% and 85.30% versus 74.13%). For audio modality, our method achieves consistently moderate improvement over three strong baselines. Finally, for visual modality, our method brings similar performance gains when compared with the previous best results.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Results On Dfew And Mer-Multi",
      "text": "We present the performance comparison with state-of-theart methods on DFEW  [29]  in Table  4 . In the audio-visual setting, our HiCMAE-S significantly outperforms the previous best-supervised method T-MEP by +5.89% UAR and +5.48% WAR, while having 25% fewer parameters and similar computational costs. In unimodal settings, although our models underperform state-of-the-art unimodal baselines, they still achieve very competitive performance. It should also be noted that our method has significantly fewer parameters and FLOPs, thus making it more suitable in resource-constrained scenarios.\n\nThe results on a Chinese dataset MER-MULTI  [25]  are shown in Table  5 . Our HiCMAE-B slightly outperforms the previous best method which utilizes MA-Net  [93]  (supervised on a facial expression dataset) and a powerful HuBERT-CH model (pre-trained on 10k+ hours of Chinese speech data  [91] ). The audio-only result of our method has a similar performance with three self-supervised models which are also pre-trained mostly on English speech data, while it is largely inferior to the state-of-the-art HuBERT-CH. We argue that this is mainly due to the large domain gap between pre-training and fine-tuning   [50, 94]  to conduct 5-fold cross-validation in a subject-independent manner. We also conduct experiments on a subset of four emotions (i.e., happiness, sadness, anger, and neutral state) and only report the result of the last fold in this setup to align with  [94] .\n\nMSP-IMPROV  [68]  is an acted audiovisual corpus to explore emotional behaviors during conversational dyadic interactions. The conversations are designed to elicit spontaneous emotions. The dataset contains 8,438 video clips recorded in six sessions from 12 actors. Following  [94] , we only use samples of four emotion categories (i.e., anger, happiness, neutral state, and sadness) and conduct 6-fold cross-validation in a sessionindependent manner. RAVDESS  [69]  is an audio-visual dataset that includes emotional speech and song. It comprises 2,880 video clips featuring 24 professional actors, each labeled with one of eight emotions (i.e., seven basic emotions and calm). In this paper, we only use the speech part consisting of 1,440 video clips. We adopt a subject-independent 6-fold cross-validation protocol for evaluation  [95, 96, 50] .\n\nIEMOCAP  [70]  contains about 12 hours of videos from 10 subjects recorded in five sessions. In this paper, we use 5,531 samples of five classes (i.e., anger, neutral state, happiness, excitement, and sadness) and follow the common practice of merging excitement into happiness to formulate a four-emotion classification task  [97, 98] . We conduct 5-fold cross-validation in a session-independent manner.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Results On Crema-D",
      "text": "The performance comparison with state-of-the-art methods on CREMA-D (6-class)  [28]  is presented in Table  6 . In the audio-visual setting, we first find that our method outperforms two self-supervised baselines (MulT Base and Large  [55] ) by substantial margins (+13% WAR). It is worth noting that they are audio-visual Transformers pre-trained on VoxCeleb2 too. However, they rely on features extracted from other models instead of the raw data as input. Thus, critical information might be lost during feature extraction, leading to significantly inferior results. VQ-MAE-AV  [20]  is another strong selfsupervised baseline. Similar to us, it is based on masked autoencoder (but requires two-stage pre-training) and also pretrained on VoxCeleb2. When compared with two versions of VQ-MAE-AV, our method shows large performance improvement over them and can be trained in a single stage. Specif- ically, the smallest HiCMAE-T surpasses the best VQ-MAE-AV by +3.34% WAR while using 33% fewer parameters. With the increase in model size, HiCMAE-B pushes the performance gap to even larger (+4.49% WAR), establishing a new stateof-the-art result on this dataset. These results demonstrate the effectiveness of the three-pronged strategy to promote hierarchical feature learning in HiCMAE. Finally, our method also outperforms advanced supervised baselines (e.g., Ladder Networks  [105] ).\n\nIn the unimodal setting, we observe that our method still maintains competitive performance. For example, for visual modality, HiCMAE-B is slightly inferior (<0.2% UAR and WAR) to state-of-the-art MAE-DFER  [50] , but requires significantly fewer parameters (62%) and computational costs (36% FLOPs). For audio modality, although the performance gap between HiCMAE-B and WavLM-Plus is larger (about 2% UAR and WAR), it is still acceptable as the latter is 3× larger than the former and has 4.5× more FLOPs.\n\nIn addition to the default six emotions on this dataset, we also conduct experiments on a subset with four emotions. The audio-visual results are shown in Table  7 . Among these baselines, most are self-supervised methods and the HuBERT series except for AV-HuBERT  [106]  (i.e., FAV-HuBERT  [94] , TAPT-HuBERT  [94] , CTAPT-HuBERT  [94] , and AW-HuBERT  [94] ) are pre-trained on VoxCeleb2 with industry-level computation resources (32 Tesla-V100 GPUs) for approximately 10 days. When compared with the best-performing AW-HuBERT, our HiCMAE-B still shows slight improvement (+0.35% UAR and +0.48% WAR), while being 21% smaller and training-friendly (we only need 4 Tesla-V100 GPUs to pre-train the model for about 5 days). It should also be noted that AW-HuBERT is a semi-supervised method that requires another labeled dataset and unlabeled samples from VoxCeleb2 for affective adaptation to obtain improved performance. Thus, these results amply demonstrate the superiority of the proposed method.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Results On Msp-Improv, Ravdess, And Iemocap",
      "text": "The audio-visual results of MSP-IMPROV  [68]  are shown in Table  8 . We observe that, when compared with the state-of-theart AW-HuBERT, HiCMAE-B achieves similar UAR but shows a large improvement (+3.15%) in terms of WAR. Two smaller versions of HiCMAE have lower UAR. However, their WAR is still higher than AW-HuBERT.\n\nIn Table  9 , we compare the proposed method with state-ofthe-art methods on RAVDESS  [69] . It can be seen that HiC-MAE brings large gains over the previous best method. In specific, our HiCMAE-B achieves a gain of +3.19% WAR over VQ-MAE-AV+Query2Emo  [20] . HiCMAE-T still outperforms it by +1.31% WAR, while using 33% fewer parameters. The unimodal results are less satisfactory on this dataset, probably due to the much smaller model size.\n\nThe performance comparison on IEMOCAP  [70]  is presented in Table  10 . Most baseline results are from AV-SUPERB  [98] . We find that HiCMAE achieves substantial improvement over state-of-the-art generic audio-visual representation learner (e.g., MAViL  [19]  and AVBERT  [112] ) in the audio-visual setting. For example, our HiCMAE-S surpasses AVBERT by +5.62% WAR while having a similar model size. HiCMAE-T outperforms MAViL which is also built upon masked autoencoders by +11.68% WAR while being 9× smaller, indicating the importance of reducing domain shift and the benefit of hierarchical feature learning to improve the quality of the learned representations. When evaluating visual-only performance, our method also demonstrates great success. However, the audio-only results are inferior to state-of-the-art large pretrained speech models, leaving much room for future performance improvement.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Dimensional Audio-Visual Emotion Recognition",
      "text": "",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Datasets",
      "text": "Werewolf-XL  [23]  is an audio-visual database for studying spontaneous emotions during competitive group interactions in Werewolf games. It contains a total of about 15 hours of audio-visual recordings. In this paper, we use 14,632 speakers' samples with dimensional annotations (i.e., arousal, valence, and dominance) and conduct subject-independent 5-fold crossvalidation for model evaluation.\n\nAVCAffe  [24]  is a large-scale audio-visual affect dataset simulating remote work scenarios. It consists of a total of 108 hours of videos (more than 58,000 video clips) along with selfreported labels for cognitive load and affect (i.e., arousal, and valence). Note that the arousal and valence scores are given on a scale of 1-4 and we follow the original paper to formulate their prediction as a classification task instead of a regression one. This dataset provides an official split (86 subjects for training and 20 subjects for test) for model evaluation.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Results",
      "text": "In Table  11 , we compare HiCMAE with state-of-the-art methods on Werewolf-XL  [23] . It can be seen that our method outperforms baselines by large margins in terms of Pearson correlation coefficient (PCC) and concordance correlation coefficient (CCC). In the audio-visual setting, for example, HiCAME-B surpasses the previous best by +17% PCC and +4% CCC in arousal and +6% PCC and +4% CCC in valence. In the unimodal setting, HiCMAE also achieves competitive or even better results. Specifically, for the video modality, HiCMAE-B has similar performance with the state-of-the-art SVFAP while having significantly fewer parameters (32M versus 78M) and FLOPs (32G versus 44G). For the audio modality, HiCMAE brings large performance gains over the bestperforming baselines on two evaluation metrics in three emotion dimensions except for CCC in arousal. The audio-visual performance comparison on AVCAffe  [24]  is presented in Table  12 . We find that HiCMAE-B outperforms the previous best results by +2.68% weighted F1-score in arousal and +3.50% weighted F1-score in valence. As for the smaller model HiCMAE-S, although we see some performance drop, it still beats the baselines moderately in two dimensions while having similar parameters with them. Finally, HiCMAE-T typically lags behind the previous supervised methods, probably due to its too-small model capacity.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Ablation Studies",
      "text": "In this section, we conduct in-depth ablation studies to investigate several key design factors in HiCMAE. By default, we present the results of HiCMAE-B on two representative datasets, i.e., MAFW  (11-class)  and CREMA-D (6-class).  hierarchical skip connections between the encoder and decoder, hierarchical cross-modal contrastive learning, and hierarchical feature fusion for downstream fine-tuning. To this end, we sequentially remove one of them and evaluate the corresponding variant in downstream tasks. The ablation results are shown in Table  13 . We observe that the removal of any module will lead to degraded performance and the model achieves the worst performance when all three modules are removed (i.e., the vanilla audio-visual masked autoencoder), which verifies their effectiveness in fostering hierarchical representation learning and improving the overall equality of the learned audio-visual representations in downstream tasks. We also notice that hierarchical skip connections between the encoder and decoder contribute the most among the three modules, followed by hierarchical cross-modal contrastive learning, and finally hierarchical feature fusion for downstream fine-tuning.",
      "page_start": 13,
      "page_end": 17
    },
    {
      "section_name": "Loss Weight",
      "text": "We then explore the role of contrastive loss weight λ in Eq.  (11) . As presented in Table  14 , we have the following observations: 1) when λ = 0, the model achieves sub-optimal performance on both datasets, which indicates the necessity of hierarchical cross-modal contrastive learning during self-supervised pre-training. 2) when λ = 0.1, the model achieves significantly worse performance. This implies that masked reconstruction loss is more essential than hierarchical cross-modal contrastive loss in HiCMAE. Large λ will overemphasize the latter too much and undermine the former, thus leading to a significant performance decline. 3) HiCMAE works reasonably well when λ ∈ [0.001, 0.01] and it generally achieves the best performance when λ = 0.0025.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Layers In Modality-Specific And Cross-Modal Fusion",
      "text": "Encoder Next, we ablate the choice of the number of layers in modality-specific encoders (N s ) and the cross-modal fusion en-   coder (N f ) by keeping their sum fixed. The ablation results are shown in Table  15 . We first find that the model achieves the worst performance when the cross-modal fusion encoder is removed (i.e., N f = 0). This observation demonstrates the crucial role of the cross-modal fusion encoder in integrating heterogeneous audio-visual information. Besides, only one fusion layer is also beneficial to improve the result. Finally, maintaining enough layers in modality-specific encoders is also necessary since too small N s will also hurt model performance.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Information Flow In Cross-Modal Fusion Encoder",
      "text": "We then investigate the effect of different types of information flow in the cross-modal fusion encoder. We develop three variants of the default information flow in Eq. (5-6) and show their differences in Fig.  5 . Specifically, for the raw-input variant, tokens of one modality in each fusion layer always attend to the raw input tokens of the other modality  [51] , instead of updated tokens from the last layer. For the video-first variant, video tokens first update themselves via audio information from the last fusion layer and then audio tokens attend to the updated video tokens. The audio-first variant is just the reverse of the video-first variant. The ablation results are presented in Table  16 . We observe that the model performance is not sensitive to different types of information flow in the cross-modal fusion encoder. Besides, in general, the default information flow works best, followed by the video-first and audio-first variants, and finally the raw-input variant.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Crema-D",
      "text": "Figure  7 : Ablation study on model scales.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Pre-Training Epochs",
      "text": "In this part, we explore the effect of pre-training epochs on downstream fine-tuning. The results are shown in Fig.  6 . As can be seen, we find that longer pre-training epochs lead to improved fine-tuning performance, which is consistent with our expectations. Besides, the model performance begins to saturate around 80 epochs. Due to limited computation resources and expensive time costs, we stop pre-training at 100 epochs. Nevertheless, we believe that continual pre-training will bring further performance gains and we encourage other researchers who have access to industry-level computation resources to conduct follow-up studies.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Model Scales",
      "text": "Finally, we show the impact of model scales on downstream fine-tuning. We present the results in Fig.  7 . Although these numbers are shown in previous tables, we want to offer a more clear and intuitive understanding. As shown in Fig.  7 , we observe that the larger model typically beats the smaller one in     downstream fine-tuning. Moreover, we notice that the performance saturation is less pronounced than that shown in pretraining epochs (especially for MAFW), indicating that further scaling up the model size can yield better results. Thus, we also encourage other researchers to continue this exploration.\n\n4.6. Visualization Analysis 4.6.1. Masked Audio-Visual Reconstruction We first show the reconstruction ability of HiCMAE under different masking rates. We randomly select video clips of four celebrities from the unseen test set of VoxCeleb2 for evaluation. The results of masked audio-visual reconstruction are shown in Fig.  8 . For each video clip, we display the original audio spectrogram and 8 facial frame images in the first row, the masked input in medium level (60% for audio and 75% for video) along with the corresponding reconstructed data in the second and third row, and the highly masked input (80% for audio and 90% for video) along with the corresponding reconstructed data in the last two rows. From the figure, we observe that, although some fine-grained details are lost, the global structures of the audio spectrogram (e.g., harmonics) and visual frames (e.g., smiles) are well recovered under both medium and high masking rates. These satisfactory reconstruction results indicate that HiCMAE is capable of fully exploiting limited intra-modal and cross-modal contextual information to infer the missing audiovisual data. We believe this capability learned during selfsupervised pre-training has laid the foundation for its superior downstream fine-tuning performance.",
      "page_start": 14,
      "page_end": 15
    },
    {
      "section_name": "Embedding Space",
      "text": "In this part, we utilize t-SNE  [114]  to visualize the learned feature embedding space of HiCMAE on CREMA-D (6-class). To demonstrate the effect of audio-visual fusion, we present both unimodal and multimodal embedding space. The results are shown in Fig.  9 . Each row presents unimodal or audiovisual embedding space and each column denotes one of five folds on this dataset. From the figure, we find that both audioonly and video-only models have learned good embedding space for distinguishing different kinds of emotions. Moreover, the multimodal embedding space is more discriminative than the unimodal ones, as evidenced by its more compact intraclass and more separated inter-class distributions. Therefore, this comparison result qualitatively verifies the effectiveness of multimodal fusion for audio-visual emotion recognition.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Error Analysis",
      "text": "We present the confusion matrices on MAFW  (11-class)  in Fig.  10 . The aggregated confusion matrix across five folds is shown in Fig.  10a  and the confusion matrix of each fold is shown in Fig.  10b-10f . As can be seen, although achieving significant improvement over previous methods as stated in Section 4.2.2, the performance of HiCMAE on several rare emotions (such as contempt, disappointment, and disgust) is not satisfactory. This is mainly attributed to the imbalanced emotion distribution in real-world scenarios. The imbalanced learning strategies (e.g., resampling techniques and cost-sensitive learning) can be utilized to address this problem and we leave it in future work. We also notice that, among all emotions, neutral emotion is the one most easily confused with other emotions. We believe that the reason is that the boundary between neutral emotion and other emotions is typically less clear than those between other emotion combinations (e.g., happiness and anger). Besides, we observe that fear is often misclassified as surprise. This is consistent with our expectation as their difference in terms of facial expressions is very subtle (both featuring wide-open eyes and raised eyebrows).",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we have presented a novel self-supervised framework (HiCMAE), as an early attempt to leverage largescale self-supervised pre-training to address the dilemma faced by current supervised methods and largely promote the development of AVER. HiCMAE is built on top of two primary forms of self-supervision, namely masked data modeling and contrastive learning. Moreover, to facilitate hierarchical audiovisual feature learning, it introduces a three-pronged approach, including hierarchical skip connections between the encoder and decoder, hierarchical cross-modal contrastive learning, and hierarchical feature fusion for downstream fine-tuning. Comprehensive experiments across 9 AVER datasets covering both categorical and dimensional tasks demonstrate that HiCMAE outperforms state-of-the-art audio-visual methods by significant margins, indicating that HiCMAE is a powerful audiovisual emotion representation learner. Extensive ablation studies and visualization analysis also verify the efficacy of HiC-MAE. We hope this work can provide some insight into the development of AVER and inspire more relevant studies.\n\nIt should be noted that, due to limited computational resources, we cannot afford the pre-training of larger models with more training time and data. Therefore, we plan to tackle these issues in future work and also encourage other researchers to conduct follow-up studies. Besides, although HiCMAE has achieved exceptional performance on many datasets, this paper has conducted limited exploration into its internal mechanisms. Therefore, it is worth investigating and enhancing the interpretability of HiCMAE in future work. Possible interpretation tools include attention visualization  [73] , relevancy map  [115] , and Grad-CAM  [116] . Finally, it is also interesting to apply HiCMAE to other audio-visual tasks (e.g., active speaker detection, deepfake detection, and talking face generation).",
      "page_start": 17,
      "page_end": 17
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Early studies on AVER concentrate on developing various",
      "page": 1
    },
    {
      "caption": "Figure 1: Several samples selected from the MAFW [5] dataset.",
      "page": 2
    },
    {
      "caption": "Figure 2: Comparison with state-of-the-art audio-visual methods on 9 datasets.",
      "page": 2
    },
    {
      "caption": "Figure 2: , our HiCMAE significantly out-",
      "page": 2
    },
    {
      "caption": "Figure 3: It mainly consists of two modality-specific encoders,",
      "page": 3
    },
    {
      "caption": "Figure 3: , HiCMAE follows MAE [17] to adopt",
      "page": 3
    },
    {
      "caption": "Figure 3: The overall pre-training pipeline of HiCMAE. HiCMAE mainly adopts an asymmetric encoder-decoder architecture with hierarchical skip connections in",
      "page": 4
    },
    {
      "caption": "Figure 4: The illustration of hierarchical skip connections between the encoder",
      "page": 5
    },
    {
      "caption": "Figure 3: ). The main benefits",
      "page": 6
    },
    {
      "caption": "Figure 3: , different",
      "page": 6
    },
    {
      "caption": "Figure 5: Different types of information flow in cross-modal fusion encoder.",
      "page": 14
    },
    {
      "caption": "Figure 5: Specifically, for the raw-input vari-",
      "page": 14
    },
    {
      "caption": "Figure 6: Ablation study on pre-training epochs.",
      "page": 14
    },
    {
      "caption": "Figure 7: Ablation study on model scales.",
      "page": 14
    },
    {
      "caption": "Figure 7: Although these",
      "page": 14
    },
    {
      "caption": "Figure 8: Reconstruction visualization of four unseen celebrities from the test set of VoxCeleb2. For each subject, we sequentially show the original audio",
      "page": 15
    },
    {
      "caption": "Figure 9: Unimodal and multimodal embedding space visualization on CREMA-D (6-class).",
      "page": 15
    },
    {
      "caption": "Figure 10: Confusion matrices on MAFW (11-class). AN: anger. DI: disgust. FE: fear. HA: happiness. NE: neutral. SA: sadness. SU: surprise. CO: contempt.",
      "page": 16
    },
    {
      "caption": "Figure 8: For each video clip, we display the original audio spec-",
      "page": 16
    },
    {
      "caption": "Figure 9: Each row presents unimodal or audio-",
      "page": 16
    },
    {
      "caption": "Figure 10: The aggregated confusion matrix across five folds is",
      "page": 16
    },
    {
      "caption": "Figure 10: a and the confusion matrix of each fold is",
      "page": 16
    },
    {
      "caption": "Figure 10: b-10f. As can be seen, although achieving sig-",
      "page": 16
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "dBeijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China": "Abstract"
        },
        {
          "dBeijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China": "Audio-Visual Emotion Recognition (AVER) has garnered increasing attention in recent years for its critical role in creating emotion-"
        },
        {
          "dBeijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China": "aware intelligent machines. Previous efforts in this area are dominated by the supervised learning paradigm. Despite significant"
        },
        {
          "dBeijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China": "progress, supervised learning is meeting its bottleneck due to the longstanding data scarcity issue in AVER. Motivated by recent ad-"
        },
        {
          "dBeijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China": "vances in self-supervised learning, we propose Hierarchical Contrastive Masked Autoencoder (HiCMAE), a novel self-supervised"
        },
        {
          "dBeijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China": "framework that leverages large-scale self-supervised pre-training on vast unlabeled audio-visual data to promote the advancement"
        },
        {
          "dBeijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China": "of AVER. Following prior arts in self-supervised audio-visual representation learning, HiCMAE adopts two primary forms of self-"
        },
        {
          "dBeijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China": "supervision for pre-training, namely masked data modeling and contrastive learning. Unlike them which focus exclusively on"
        },
        {
          "dBeijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China": "top-layer representations while neglecting explicit guidance of intermediate layers, HiCMAE develops a three-pronged strategy to"
        },
        {
          "dBeijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China": "foster hierarchical audio-visual feature learning and improve the overall quality of learned representations. Firstly, it incorporates"
        },
        {
          "dBeijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China": "hierarchical skip connections between the encoder and decoder to encourage intermediate layers to learn more meaningful represen-"
        },
        {
          "dBeijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China": "tations and bolster masked audio-visual reconstruction. Secondly, hierarchical cross-modal contrastive learning is also exerted on"
        },
        {
          "dBeijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China": "intermediate representations to narrow the audio-visual modality gap progressively and facilitate subsequent cross-modal fusion."
        },
        {
          "dBeijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China": "Finally, during downstream fine-tuning, HiCMAE employs hierarchical\nfeature fusion to comprehensively integrate multi-level"
        },
        {
          "dBeijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China": "features from different\nlayers. To verify the effectiveness of HiCMAE, we conduct extensive experiments on 9 datasets covering"
        },
        {
          "dBeijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China": "both categorical and dimensional AVER tasks. Experimental results show that our method significantly outperforms state-of-the-art"
        },
        {
          "dBeijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China": "supervised and self-supervised audio-visual methods, which indicates that HiCMAE is a powerful audio-visual emotion represen-"
        },
        {
          "dBeijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China": "tation learner. Codes and models are publicly available at https://github.com/sunlicai/HiCMAE."
        },
        {
          "dBeijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China": "Keywords: Audio-Visual Emotion Recognition; Self-Supervised Learning; Masked Autoencoder; Contrastive Learning"
        },
        {
          "dBeijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China": "1.\nIntroduction\nthat\nthe language modality (i.e., verbal\ninformation) only con-"
        },
        {
          "dBeijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China": "tributes 7% to the perception of emotions in our daily commu-"
        },
        {
          "dBeijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China": "“The question is not whether intelligent machines can\nnication, while the audio (e.g.,\ntone and intonation) and visual"
        },
        {
          "dBeijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China": "have any emotions, but whether machines can be in-\n(e.g.,\nfacial expressions) modalities significantly predominate,"
        },
        {
          "dBeijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China": "telligent without any emotions.”\ncontributing to 38% and 55% respectively [3, 4]. As a result,"
        },
        {
          "dBeijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China": "the last\ntwo decades have witnessed a surge of\ninterest\nin au-"
        },
        {
          "dBeijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China": "tomatic Audio-Visual Emotion Recognition (AVER) in the af-\n—Marvin Minsky"
        },
        {
          "dBeijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China": "fective computing community. The task of AVER is to extract"
        },
        {
          "dBeijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China": "emotion-related representations from audio-visual signals and"
        },
        {
          "dBeijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China": "Emotions are fundamental\nto the multifaceted spectrum of"
        },
        {
          "dBeijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China": "then integrate them to identify the subject’s emotional\nstate."
        },
        {
          "dBeijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China": "human experience, influencing our cognition, decision-making,"
        },
        {
          "dBeijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China": "Several concrete examples are depicted in Fig. 1."
        },
        {
          "dBeijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China": "and interpersonal interactions [1]. They also play a pivotal role"
        },
        {
          "dBeijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China": "in developing intelligent machines and achieving the ultimate"
        },
        {
          "dBeijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China": "Early studies on AVER concentrate on developing various"
        },
        {
          "dBeijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China": "goal of emotional artificial\nintelligence, as Marvin Minsky, a"
        },
        {
          "dBeijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China": "hand-engineering features for audio and video modalities [6, 4]."
        },
        {
          "dBeijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China": "pioneer of artificial\nintelligence, highlighted above [2]. Typi-"
        },
        {
          "dBeijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China": "With the\nadvent of\nthe deep learning era,\na new trend has"
        },
        {
          "dBeijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China": "cally, people express and perceive emotions through multiple"
        },
        {
          "dBeijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China": "emerged towards\nlearning features directly from raw audio-"
        },
        {
          "dBeijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China": "modalities. Previous psychological studies have demonstrated"
        },
        {
          "dBeijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China": "visual data by training deep supervised neural networks in an"
        },
        {
          "dBeijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China": "end-to-end manner\n[7, 8, 9].\nDespite considerable advance-"
        },
        {
          "dBeijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China": "ments, supervised learning is heavily constrained by its reliance"
        },
        {
          "dBeijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China": "∗Corresponding authors: Biu Liu and Jianhua Tao."
        },
        {
          "dBeijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China": "on large amounts of\nlabeled data to achieve satisfactory per-"
        },
        {
          "dBeijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China": "Email addresses: sunlicai2019@ia.ac.cn (Licai Sun),"
        },
        {
          "dBeijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China": "formance. This reliance significantly hampers further progress"
        },
        {
          "dBeijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China": "lianzheng2016@ia.ac.cn (Zheng Lian), liubin@nlpr.ia.ac.cn (Bin"
        },
        {
          "dBeijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China": "Liu), jhtao@tsinghua.edu.cn (Jianhua Tao)\nof\nsupervised methods due to the longstanding issue of data"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "DFEW [29]. Moreover, extensive ablation studies also justify": ""
        },
        {
          "DFEW [29]. Moreover, extensive ablation studies also justify": "various design choices in HiCMAE."
        },
        {
          "DFEW [29]. Moreover, extensive ablation studies also justify": ""
        },
        {
          "DFEW [29]. Moreover, extensive ablation studies also justify": "In summary, our main contributions are three-fold:"
        },
        {
          "DFEW [29]. Moreover, extensive ablation studies also justify": ""
        },
        {
          "DFEW [29]. Moreover, extensive ablation studies also justify": ""
        },
        {
          "DFEW [29]. Moreover, extensive ablation studies also justify": "• We present HiCMAE, a novel self-supervised framework"
        },
        {
          "DFEW [29]. Moreover, extensive ablation studies also justify": ""
        },
        {
          "DFEW [29]. Moreover, extensive ablation studies also justify": "for AVER, as an early endeavor\nto leverage large-scale"
        },
        {
          "DFEW [29]. Moreover, extensive ablation studies also justify": ""
        },
        {
          "DFEW [29]. Moreover, extensive ablation studies also justify": "self-supervised pre-training to address the dilemma of su-"
        },
        {
          "DFEW [29]. Moreover, extensive ablation studies also justify": ""
        },
        {
          "DFEW [29]. Moreover, extensive ablation studies also justify": "pervised methods and promote the development of AVER."
        },
        {
          "DFEW [29]. Moreover, extensive ablation studies also justify": ""
        },
        {
          "DFEW [29]. Moreover, extensive ablation studies also justify": "• Unlike previous methods, HiCMAE introduces a three-"
        },
        {
          "DFEW [29]. Moreover, extensive ablation studies also justify": "pronged approach to foster hierarchical audio-visual fea-"
        },
        {
          "DFEW [29]. Moreover, extensive ablation studies also justify": "ture learning and the ablation studies verify its efficacy."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 1: Several samples selected from the MAFW [5] dataset.": "scarcity in AVER [10, 11]."
        },
        {
          "Figure 1: Several samples selected from the MAFW [5] dataset.": "Recently,\nanother\ndeep\nlearning\nparadigm,\ni.e.,\nself-"
        },
        {
          "Figure 1: Several samples selected from the MAFW [5] dataset.": "supervised learning, which can learn powerful representations"
        },
        {
          "Figure 1: Several samples selected from the MAFW [5] dataset.": "from vast unlabeled data, has revolutionized many research ar-"
        },
        {
          "Figure 1: Several samples selected from the MAFW [5] dataset.": ""
        },
        {
          "Figure 1: Several samples selected from the MAFW [5] dataset.": "eas [12, 13, 14, 15, 16, 17]. In particular, masked data modeling"
        },
        {
          "Figure 1: Several samples selected from the MAFW [5] dataset.": ""
        },
        {
          "Figure 1: Several samples selected from the MAFW [5] dataset.": "(e.g., MAE [17]) and contrastive learning (e.g., CLIP [16]) are"
        },
        {
          "Figure 1: Several samples selected from the MAFW [5] dataset.": ""
        },
        {
          "Figure 1: Several samples selected from the MAFW [5] dataset.": "two prominent methods and have demonstrated great success"
        },
        {
          "Figure 1: Several samples selected from the MAFW [5] dataset.": "in self-supervised visual representation learning. Specifically,"
        },
        {
          "Figure 1: Several samples selected from the MAFW [5] dataset.": "masked data modeling aims to reconstruct\nthe raw data from"
        },
        {
          "Figure 1: Several samples selected from the MAFW [5] dataset.": "masked inputs, while contrastive learning encourages the se-"
        },
        {
          "Figure 1: Several samples selected from the MAFW [5] dataset.": "mantic alignment of different modalities. They are also com-"
        },
        {
          "Figure 1: Several samples selected from the MAFW [5] dataset.": "bined and extended to learn generic audio-visual\nrepresenta-"
        },
        {
          "Figure 1: Several samples selected from the MAFW [5] dataset.": "tions\nin recent\nstudies,\nsuch as CAV-MAE [18] and MAViL"
        },
        {
          "Figure 1: Several samples selected from the MAFW [5] dataset.": "[19]. However, due to the domain gap between upstream pre-"
        },
        {
          "Figure 1: Several samples selected from the MAFW [5] dataset.": "training and downstream AVER tasks,\nthe learned representa-"
        },
        {
          "Figure 1: Several samples selected from the MAFW [5] dataset.": "tions are typically not suitable for AVER. More recently, Sadok"
        },
        {
          "Figure 1: Several samples selected from the MAFW [5] dataset.": "et al.\n[20] developed a vector-quantized masked autoencoder"
        },
        {
          "Figure 1: Several samples selected from the MAFW [5] dataset.": "(VQ-MAE-AV)\nfor AVER. Although achieving promising re-"
        },
        {
          "Figure 1: Several samples selected from the MAFW [5] dataset.": "sults, VQ-MAE-AV requires\npre-trained\nvariational\nautoen-"
        },
        {
          "Figure 1: Several samples selected from the MAFW [5] dataset.": "coders for vector quantization and thus cannot be pre-trained in"
        },
        {
          "Figure 1: Several samples selected from the MAFW [5] dataset.": "a single stage. More importantly, several studies on masked im-"
        },
        {
          "Figure 1: Several samples selected from the MAFW [5] dataset.": "age modeling have shown that intermediate layers are essential"
        },
        {
          "Figure 1: Several samples selected from the MAFW [5] dataset.": "to self-supervised visual representation learning and explicitly"
        },
        {
          "Figure 1: Several samples selected from the MAFW [5] dataset.": "guiding them can improve the quality of learned representations"
        },
        {
          "Figure 1: Several samples selected from the MAFW [5] dataset.": "[21, 22]. However, the aforementioned methods fail to achieve"
        },
        {
          "Figure 1: Several samples selected from the MAFW [5] dataset.": "that goal as they solely operate on representations from the top"
        },
        {
          "Figure 1: Several samples selected from the MAFW [5] dataset.": "layer and neglect explicit guidance on intermediate layers, thus"
        },
        {
          "Figure 1: Several samples selected from the MAFW [5] dataset.": "impeding feature evolution across\nlayers and leading to sub-"
        },
        {
          "Figure 1: Several samples selected from the MAFW [5] dataset.": "optimal results in downstream tasks."
        },
        {
          "Figure 1: Several samples selected from the MAFW [5] dataset.": ""
        },
        {
          "Figure 1: Several samples selected from the MAFW [5] dataset.": "To address\nthe above challenges,\nthis paper builds on top"
        },
        {
          "Figure 1: Several samples selected from the MAFW [5] dataset.": ""
        },
        {
          "Figure 1: Several samples selected from the MAFW [5] dataset.": "of MAE and\ncontrastive\nlearning\nto\npropose Hierarchical"
        },
        {
          "Figure 1: Several samples selected from the MAFW [5] dataset.": ""
        },
        {
          "Figure 1: Several samples selected from the MAFW [5] dataset.": "Contrastive Masked Autoencoder\n(HiCMAE),\na\nnovel\nself-"
        },
        {
          "Figure 1: Several samples selected from the MAFW [5] dataset.": "supervised framework tailored for AVER. As shown in Fig. 3,"
        },
        {
          "Figure 1: Several samples selected from the MAFW [5] dataset.": ""
        },
        {
          "Figure 1: Several samples selected from the MAFW [5] dataset.": "it\nis mainly composed of\ntwo audio-visual encoders, a cross-"
        },
        {
          "Figure 1: Several samples selected from the MAFW [5] dataset.": ""
        },
        {
          "Figure 1: Several samples selected from the MAFW [5] dataset.": "modal\nfusion encoder,\nand two lightweight audio-visual de-"
        },
        {
          "Figure 1: Several samples selected from the MAFW [5] dataset.": ""
        },
        {
          "Figure 1: Several samples selected from the MAFW [5] dataset.": "coders. To foster hierarchical audio-visual feature learning and"
        },
        {
          "Figure 1: Several samples selected from the MAFW [5] dataset.": ""
        },
        {
          "Figure 1: Several samples selected from the MAFW [5] dataset.": "improve the overall quality of\nlearned representations, HiC-"
        },
        {
          "Figure 1: Several samples selected from the MAFW [5] dataset.": "MAE develops a three-pronged strategy:\n(1) Drawing inspi-"
        },
        {
          "Figure 1: Several samples selected from the MAFW [5] dataset.": "ration from the architecture design in U-Net\n[26], HiCMAE"
        },
        {
          "Figure 1: Several samples selected from the MAFW [5] dataset.": "introduces hierarchical skip connections between the encoder"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "• Comprehensive\nexperiments\nacross 9 datasets\ncovering": "both categorical and dimensional AVER tasks demonstrate",
          "namely contrastive learning, masked data modeling, and the hy-": "brid method. Contrastive learning exploits the natural audio-"
        },
        {
          "• Comprehensive\nexperiments\nacross 9 datasets\ncovering": "that HiCMAE beats state-of-the-art audio-visual methods",
          "namely contrastive learning, masked data modeling, and the hy-": "visual\ncorrespondence\nin\nvideos\nas\na\nfree\nsignal\nfor\nself-"
        },
        {
          "• Comprehensive\nexperiments\nacross 9 datasets\ncovering": "by significant margins, indicating that HiCMAE is a pow-",
          "namely contrastive learning, masked data modeling, and the hy-": "supervision [58, 59, 60].\nIn contrast,\nthe goal of masked data"
        },
        {
          "• Comprehensive\nexperiments\nacross 9 datasets\ncovering": "erful audio-visual emotion representation learner.",
          "namely contrastive learning, masked data modeling, and the hy-": "modeling is to reconstruct the original data from its masked in-"
        },
        {
          "• Comprehensive\nexperiments\nacross 9 datasets\ncovering": "",
          "namely contrastive learning, masked data modeling, and the hy-": "put. Motivated by its recent success in the image and video"
        },
        {
          "• Comprehensive\nexperiments\nacross 9 datasets\ncovering": "",
          "namely contrastive learning, masked data modeling, and the hy-": "domain [17, 61, 62], Georgescu et al.\n[63] extend it\nto the"
        },
        {
          "• Comprehensive\nexperiments\nacross 9 datasets\ncovering": "2. Related Work",
          "namely contrastive learning, masked data modeling, and the hy-": ""
        },
        {
          "• Comprehensive\nexperiments\nacross 9 datasets\ncovering": "",
          "namely contrastive learning, masked data modeling, and the hy-": "audio-visual domain and achieve significant improvements over"
        },
        {
          "• Comprehensive\nexperiments\nacross 9 datasets\ncovering": "2.1. Audio-Visual Emotion Recognition",
          "namely contrastive learning, masked data modeling, and the hy-": "previous methods. Recently, a few studies have made attempts"
        },
        {
          "• Comprehensive\nexperiments\nacross 9 datasets\ncovering": "Most studies on Audio-Visual Emotion Recognition (AVER)",
          "namely contrastive learning, masked data modeling, and the hy-": "to combine the former\ntwo kinds of methods,\nresulting in the"
        },
        {
          "• Comprehensive\nexperiments\nacross 9 datasets\ncovering": "fall\ninto the supervised learning paradigm.\nThey mainly fo-",
          "namely contrastive learning, masked data modeling, and the hy-": "hybrid method.\nIn particular, CAV-MAE [18] integrates MAE"
        },
        {
          "• Comprehensive\nexperiments\nacross 9 datasets\ncovering": "cus on two important aspects: unimodal feature extraction and",
          "namely contrastive learning, masked data modeling, and the hy-": "and cross-modal contrastive learning and shows that\nthey are"
        },
        {
          "• Comprehensive\nexperiments\nacross 9 datasets\ncovering": "audio-visual\ninformation fusion.\nAs\nfor\nthe first aspect,\nre-",
          "namely contrastive learning, masked data modeling, and the hy-": "complementary. MAViL [19]\nfurther\nintroduces\nintra-modal"
        },
        {
          "• Comprehensive\nexperiments\nacross 9 datasets\ncovering": "searchers have developed and exploited numerous features in",
          "namely contrastive learning, masked data modeling, and the hy-": "contrastive learning and masked self-training on contextualized"
        },
        {
          "• Comprehensive\nexperiments\nacross 9 datasets\ncovering": "the past\ntwo decades\n[6, 4, 10, 30].\nEarly studies\nconcen-",
          "namely contrastive learning, masked data modeling, and the hy-": "features to improve audio-visual pre-training. Although these"
        },
        {
          "• Comprehensive\nexperiments\nacross 9 datasets\ncovering": "trate on various handcrafted features for\ntwo modalities, such",
          "namely contrastive learning, masked data modeling, and the hy-": "methods have demonstrated great\nsuccess\nin general\naudio-"
        },
        {
          "• Comprehensive\nexperiments\nacross 9 datasets\ncovering": "as\nIS13 [31] and eGeMAPS [32]\nfor audio, LBP-TOP [33]",
          "namely contrastive learning, masked data modeling, and the hy-": "visual\ntasks,\nthe learned representations are typically not suit-"
        },
        {
          "• Comprehensive\nexperiments\nacross 9 datasets\ncovering": "and HOG [34]\nfor video. With the advent of deep learning,",
          "namely contrastive learning, masked data modeling, and the hy-": "able for AVER because they are trained on general\nscene or"
        },
        {
          "• Comprehensive\nexperiments\nacross 9 datasets\ncovering": "a large amount of deep supervised models trained on large au-",
          "namely contrastive learning, masked data modeling, and the hy-": "action videos instead of facial videos in AVER. Recently, VQ-"
        },
        {
          "• Comprehensive\nexperiments\nacross 9 datasets\ncovering": "dio and image/video datasets have emerged as powerful audio-",
          "namely contrastive learning, masked data modeling, and the hy-": "MAE-AV [20] introduces a vector-quantized MAE for AVER."
        },
        {
          "• Comprehensive\nexperiments\nacross 9 datasets\ncovering": "visual feature extractors [35, 36, 37, 38, 39, 25], such as PANNs",
          "namely contrastive learning, masked data modeling, and the hy-": "Despite promising results, it requires two-stage pre-training. In"
        },
        {
          "• Comprehensive\nexperiments\nacross 9 datasets\ncovering": "[40]\nand VGGish [41]\nfor\naudio, VGGFace\n[42]\nand C3D",
          "namely contrastive learning, masked data modeling, and the hy-": "addition,\nthe more important\nissue is that\nthe aforementioned"
        },
        {
          "• Comprehensive\nexperiments\nacross 9 datasets\ncovering": "[43]\nfor video.\nThere are also lots of attempts\nto train end-",
          "namely contrastive learning, masked data modeling, and the hy-": "audio-visual masked autoencoders fail\nto promote hierarchical"
        },
        {
          "• Comprehensive\nexperiments\nacross 9 datasets\ncovering": "to-end deep neural networks on raw audio and video emotion",
          "namely contrastive learning, masked data modeling, and the hy-": "audio-visual feature learning as they focus exclusively on top-"
        },
        {
          "• Comprehensive\nexperiments\nacross 9 datasets\ncovering": "data [44, 8, 45, 29, 46].\nIn recent years, plenty of\nlarge self-",
          "namely contrastive learning, masked data modeling, and the hy-": "layer representations while neglecting explicit guidance of in-"
        },
        {
          "• Comprehensive\nexperiments\nacross 9 datasets\ncovering": "supervised pre-trained models have demonstrated great success",
          "namely contrastive learning, masked data modeling, and the hy-": "termediate layers,\nthus impeding feature evolution across lay-"
        },
        {
          "• Comprehensive\nexperiments\nacross 9 datasets\ncovering": "in audio (e.g., Wav2vec2.0 [47] and HuBERT [48]) or video",
          "namely contrastive learning, masked data modeling, and the hy-": "ers and leading to sub-optimal performance [21, 22].\nThere-"
        },
        {
          "• Comprehensive\nexperiments\nacross 9 datasets\ncovering": "(e.g., SVFAP [49]\nand MAE-DFER [50])\nemotion recogni-",
          "namely contrastive learning, masked data modeling, and the hy-": "fore, our HiCAME presents a three-pronged approach to foster"
        },
        {
          "• Comprehensive\nexperiments\nacross 9 datasets\ncovering": "tion. After unimodal feature extraction,\nthe next crucial step is",
          "namely contrastive learning, masked data modeling, and the hy-": "hierarchical audio-visual feature learning and demonstrates im-"
        },
        {
          "• Comprehensive\nexperiments\nacross 9 datasets\ncovering": "audio-visual\ninformation fusion. Current fusion strategies can",
          "namely contrastive learning, masked data modeling, and the hy-": "proved performance."
        },
        {
          "• Comprehensive\nexperiments\nacross 9 datasets\ncovering": "be roughly divided into three categories,\ni.e., early fusion,\nlate",
          "namely contrastive learning, masked data modeling, and the hy-": ""
        },
        {
          "• Comprehensive\nexperiments\nacross 9 datasets\ncovering": "fusion, and model-level\nfusion [6, 4].\nEarly fusion typically",
          "namely contrastive learning, masked data modeling, and the hy-": ""
        },
        {
          "• Comprehensive\nexperiments\nacross 9 datasets\ncovering": "",
          "namely contrastive learning, masked data modeling, and the hy-": "3. Method"
        },
        {
          "• Comprehensive\nexperiments\nacross 9 datasets\ncovering": "combines audio-visual\nfeatures at\nthe input\nlevel\n[36, 39].\nIn",
          "namely contrastive learning, masked data modeling, and the hy-": ""
        },
        {
          "• Comprehensive\nexperiments\nacross 9 datasets\ncovering": "contrast,\nlate fusion integrates audio-visual predictions at\nthe",
          "namely contrastive learning, masked data modeling, and the hy-": "In this\nsection, we\nelaborate on Hierarchical Contrastive"
        },
        {
          "• Comprehensive\nexperiments\nacross 9 datasets\ncovering": "decision level\n[37, 38]. The most commonly used strategy is",
          "namely contrastive learning, masked data modeling, and the hy-": "Masked AutoEncoder\n(HiCMAE),\na\nnovel\nself-supervised"
        },
        {
          "• Comprehensive\nexperiments\nacross 9 datasets\ncovering": "model-level fusion [7, 51, 52, 53, 54, 55, 56, 57, 9]. For exam-",
          "namely contrastive learning, masked data modeling, and the hy-": "audio-visual\nemotion\nrepresentation\nlearning\nframework\nfor"
        },
        {
          "• Comprehensive\nexperiments\nacross 9 datasets\ncovering": "ple, MulT [51] utilizes cross-modal attention to capture dense",
          "namely contrastive learning, masked data modeling, and the hy-": "AVER. The training process of HiCMAE includes two steps,"
        },
        {
          "• Comprehensive\nexperiments\nacross 9 datasets\ncovering": "interactions in unaligned audio-visual feature sequences. EMT",
          "namely contrastive learning, masked data modeling, and the hy-": "i.e.,\nself-supervised\npre-training\n(Section\n3.1-3.3)\non\nlarge-"
        },
        {
          "• Comprehensive\nexperiments\nacross 9 datasets\ncovering": "[57] improves MulT by introducing the global multimodal con-",
          "namely contrastive learning, masked data modeling, and the hy-": "scale unlabeled AVER data and downstream fine-tuning (Sec-"
        },
        {
          "• Comprehensive\nexperiments\nacross 9 datasets\ncovering": "text and employing it to interact with local unimodal features to",
          "namely contrastive learning, masked data modeling, and the hy-": "tion 3.4) on limited labeled AVER data. Specifically,\nthe self-"
        },
        {
          "• Comprehensive\nexperiments\nacross 9 datasets\ncovering": "achieve efficient cross-modal information exchange. The recent",
          "namely contrastive learning, masked data modeling, and the hy-": "supervised pre-training pipeline of HiCMAE is\nillustrated in"
        },
        {
          "• Comprehensive\nexperiments\nacross 9 datasets\ncovering": "T-MEP [9] adopts a similar strategy with MulT to fuse fine-",
          "namely contrastive learning, masked data modeling, and the hy-": "Fig. 3.\nIt mainly consists of\ntwo modality-specific encoders,"
        },
        {
          "• Comprehensive\nexperiments\nacross 9 datasets\ncovering": "grained audio-visual\ntokens by combining self-attention and",
          "namely contrastive learning, masked data modeling, and the hy-": "a cross-modal\nfusion encoder, and two lightweight modality-"
        },
        {
          "• Comprehensive\nexperiments\nacross 9 datasets\ncovering": "cross-attention mechanisms in an interleaving manner.",
          "namely contrastive learning, masked data modeling, and the hy-": "specific decoders.\nHiCMAE adopts\ntwo primary forms of"
        },
        {
          "• Comprehensive\nexperiments\nacross 9 datasets\ncovering": "Despite promising results,\nthe aforementioned methods are",
          "namely contrastive learning, masked data modeling, and the hy-": "self-supervision for pre-training: masked data modeling (i.e.,"
        },
        {
          "• Comprehensive\nexperiments\nacross 9 datasets\ncovering": "mostly supervised learning methods.\nThey either\ntrain from",
          "namely contrastive learning, masked data modeling, and the hy-": "masked audio-visual\nreconstruction) and contrastive learning."
        },
        {
          "• Comprehensive\nexperiments\nacross 9 datasets\ncovering": "scratch or rely on pre-trained models from other different tasks",
          "namely contrastive learning, masked data modeling, and the hy-": "Moreover, it introduces a three-pronged strategy to promote hi-"
        },
        {
          "• Comprehensive\nexperiments\nacross 9 datasets\ncovering": "for\ninitialization, which are thus\nseverely constrained by the",
          "namely contrastive learning, masked data modeling, and the hy-": "erarchical audio-visual feature learning during both pre-training"
        },
        {
          "• Comprehensive\nexperiments\nacross 9 datasets\ncovering": "data\nscarcity issue\nin AVER or\nsuffer\nfrom substantial do-",
          "namely contrastive learning, masked data modeling, and the hy-": "and fine-tuning,\nincluding hierarchical\nskip connections be-"
        },
        {
          "• Comprehensive\nexperiments\nacross 9 datasets\ncovering": "main shifts.\nIn contrast,\nthis paper presents an early attempt",
          "namely contrastive learning, masked data modeling, and the hy-": "tween the encoder and decoder (Section 3.1), hierarchical cross-"
        },
        {
          "• Comprehensive\nexperiments\nacross 9 datasets\ncovering": "to leverage large-scale self-supervised audio-visual pre-training",
          "namely contrastive learning, masked data modeling, and the hy-": "modal contrastive learning (Section 3.2), and hierarchical fea-"
        },
        {
          "• Comprehensive\nexperiments\nacross 9 datasets\ncovering": "on massive unlabeled data to largely advance the development",
          "namely contrastive learning, masked data modeling, and the hy-": "ture fusion for downstream fine-tuning (Section 3.4)."
        },
        {
          "• Comprehensive\nexperiments\nacross 9 datasets\ncovering": "of audio-visual emotion recognition.",
          "namely contrastive learning, masked data modeling, and the hy-": ""
        },
        {
          "• Comprehensive\nexperiments\nacross 9 datasets\ncovering": "",
          "namely contrastive learning, masked data modeling, and the hy-": "3.1. Masked Audio-Visual Reconstruction with Hierarchical"
        },
        {
          "• Comprehensive\nexperiments\nacross 9 datasets\ncovering": "2.2.\nSelf-Supervised Audio-Visual Representation Learning",
          "namely contrastive learning, masked data modeling, and the hy-": "Skip Connections"
        },
        {
          "• Comprehensive\nexperiments\nacross 9 datasets\ncovering": "In general,\nthere\nare\nthree kinds of methods\nfor\nlearning",
          "namely contrastive learning, masked data modeling, and the hy-": "As depicted in Fig. 3, HiCMAE follows MAE [17] to adopt"
        },
        {
          "• Comprehensive\nexperiments\nacross 9 datasets\ncovering": "generic audio-visual\nrepresentations through self-supervision,",
          "namely contrastive learning, masked data modeling, and the hy-": "an asymmetric encoder-decoder architecture for efficient self-"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Audio": "Decoder"
        },
        {
          "Audio": "Audio Reconstruction"
        },
        {
          "Audio": "Figure 3: The overall pre-training pipeline of HiCMAE. HiCMAE mainly adopts an asymmetric encoder-decoder architecture with hierarchical skip connections in"
        },
        {
          "Audio": "between for masked audio-visual reconstruction. Besides, hierarchical cross-modal contrastive learning is employed at intermediate audio-visual encoder layers to"
        },
        {
          "Audio": "reduce the modality gap in a progressive manner and facilitate cross-modal fusion in subsequent layers."
        },
        {
          "Audio": "supervised audio-visual pre-training. The raw audio and video"
        },
        {
          "Audio": "inputs are first embedded into tokens and then masked to fil-"
        },
        {
          "Audio": "ter out\na\nsubstantial proportion of\ntokens.\nNext,\nthe visi-"
        },
        {
          "Audio": ""
        },
        {
          "Audio": "ble (i.e., unmasked) audio and video tokens are processed by"
        },
        {
          "Audio": "their modality-specific encoders and a cross-modal fusion en-"
        },
        {
          "Audio": "coder. After feature encoding,\nthe visible audio and video to-"
        },
        {
          "Audio": "kens are padded with learnable masked tokens and then passed"
        },
        {
          "Audio": "through lightweight modality-specific decoders\nfor final\nau-"
        },
        {
          "Audio": "dio and video reconstruction. Note that, unlike conventional"
        },
        {
          "Audio": "masked autoencoders, hierarchical skip connections are added"
        },
        {
          "Audio": "between audio-visual encoders and decoders\nto better guide"
        },
        {
          "Audio": "the\nencoder\nfeature\nlearning at different\nlevels\nand promote"
        },
        {
          "Audio": ""
        },
        {
          "Audio": "masked audio-visual\nreconstruction by providing the decoder"
        },
        {
          "Audio": "with multi-level features."
        },
        {
          "Audio": ""
        },
        {
          "Audio": "3.1.1. Data Embedding and Token Masking"
        },
        {
          "Audio": "Since we use Transformer\n[64] as\nthe main component of"
        },
        {
          "Audio": "the encoder,\nit\nis necessary to embed audio and video inputs"
        },
        {
          "Audio": "into a sequence of discrete tokens.\nFormally, we denote the"
        },
        {
          "Audio": "is the number of\nfacial video frames as Xv ∈ RTv×H×W×3\n(Tv"
        },
        {
          "Audio": "frames, H and W denote the height and width, and 3 represents"
        },
        {
          "Audio": "RGB channels) and the audio spectrogram as Xa ∈ RTa×F (Ta"
        },
        {
          "Audio": "is the temporal\nlength and F denotes the number of frequency"
        },
        {
          "Audio": "channels). We utilize a cube embedding layer with a size of"
        },
        {
          "Audio": "into video tokens X′"
        },
        {
          "Audio": "2 × 16 × 16 to split Xv\nv ∈ RNv×C (Nv ="
        },
        {
          "Audio": "T2\n· H\n· W\nis the number of video tokens, C is the number of"
        },
        {
          "Audio": "16\n16"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "pre-training framework,\nthere can be many choices for the ar-": "chitecture of modality-specific encoders. For simplicity, we fol-"
        },
        {
          "pre-training framework,\nthere can be many choices for the ar-": "low MAE [17] and VideoMAE [61] to employ standard Trans-"
        },
        {
          "pre-training framework,\nthere can be many choices for the ar-": "former\n[64] as audio and video encoders. Other efficient ar-"
        },
        {
          "pre-training framework,\nthere can be many choices for the ar-": ""
        },
        {
          "pre-training framework,\nthere can be many choices for the ar-": "chitectures\nsuch as LGI-Former\n[50] can be explored in fu-"
        },
        {
          "pre-training framework,\nthere can be many choices for the ar-": "ture work. The audio and video encoders consist of Ns Trans-"
        },
        {
          "pre-training framework,\nthere can be many choices for the ar-": ""
        },
        {
          "pre-training framework,\nthere can be many choices for the ar-": ""
        },
        {
          "pre-training framework,\nthere can be many choices for the ar-": "former\nlayers.\nEach Transformer\nlayer\nis mainly composed"
        },
        {
          "pre-training framework,\nthere can be many choices for the ar-": "of Multi-Head Self-Attention (MHSA) and Feed-Forward Net-"
        },
        {
          "pre-training framework,\nthere can be many choices for the ar-": ""
        },
        {
          "pre-training framework,\nthere can be many choices for the ar-": "work (FFN):"
        },
        {
          "pre-training framework,\nthere can be many choices for the ar-": ""
        },
        {
          "pre-training framework,\nthere can be many choices for the ar-": "E j−1′\n,\n= MHSA(LN(E j−1\n)) + E j−1"
        },
        {
          "pre-training framework,\nthere can be many choices for the ar-": ""
        },
        {
          "pre-training framework,\nthere can be many choices for the ar-": "(1)"
        },
        {
          "pre-training framework,\nthere can be many choices for the ar-": "E j\n,\n)) + E j−1′\nm = FFN(LN(E j−1′"
        },
        {
          "pre-training framework,\nthere can be many choices for the ar-": ""
        },
        {
          "pre-training framework,\nthere can be many choices for the ar-": ""
        },
        {
          "pre-training framework,\nthere can be many choices for the ar-": "= X′′\nwhere E0\nm\nm, m ∈ {a, v} denotes the audio or video modal-"
        },
        {
          "pre-training framework,\nthere can be many choices for the ar-": "ity,\nis the layer\nindex, and LN stands for\nlayer\nj ∈ {1, ..., Ns}"
        },
        {
          "pre-training framework,\nthere can be many choices for the ar-": "normalization [64].\nFor MHSA in Eq.\n(1),\nit calculates dot-"
        },
        {
          "pre-training framework,\nthere can be many choices for the ar-": "product attention to learn intrinsic dependency relationships in"
        },
        {
          "pre-training framework,\nthere can be many choices for the ar-": "input audio/video tokens:"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "normalization [64].\nFor MHSA in Eq.\n(1),\nit calculates dot-": "product attention to learn intrinsic dependency relationships in"
        },
        {
          "normalization [64].\nFor MHSA in Eq.\n(1),\nit calculates dot-": "input audio/video tokens:"
        },
        {
          "normalization [64].\nFor MHSA in Eq.\n(1),\nit calculates dot-": "MHSA(E) = Concat(head1, ..., headH)WO,"
        },
        {
          "normalization [64].\nFor MHSA in Eq.\n(1),\nit calculates dot-": "QhK⊤\nh"
        },
        {
          "normalization [64].\nFor MHSA in Eq.\n(1),\nit calculates dot-": "√\n(2)\n)Vh, h = 1, ..., H,\nheadh = Softmax("
        },
        {
          "normalization [64].\nFor MHSA in Eq.\n(1),\nit calculates dot-": "dh"
        },
        {
          "normalization [64].\nFor MHSA in Eq.\n(1),\nit calculates dot-": "Qh = EWQ\nh , Vh = EWV\nh ,\nh , Kh = EWK"
        },
        {
          "normalization [64].\nFor MHSA in Eq.\n(1),\nit calculates dot-": ""
        },
        {
          "normalization [64].\nFor MHSA in Eq.\n(1),\nit calculates dot-": "where WQ\n∈ RC×dh , WK\n∈ RC×dh , WV\n∈ RC×dh , WO ∈ RC×C,"
        },
        {
          "normalization [64].\nFor MHSA in Eq.\n(1),\nit calculates dot-": "h\nh\nh"
        },
        {
          "normalization [64].\nFor MHSA in Eq.\n(1),\nit calculates dot-": "H is the number of attention heads, and dh = C/H is the feature"
        },
        {
          "normalization [64].\nFor MHSA in Eq.\n(1),\nit calculates dot-": "dimension in each attention head. For FFN in Eq.\n(1),\nit com-"
        },
        {
          "normalization [64].\nFor MHSA in Eq.\n(1),\nit calculates dot-": "prises two linear projection layers with a GELU [66] activation"
        },
        {
          "normalization [64].\nFor MHSA in Eq.\n(1),\nit calculates dot-": "function in between:"
        },
        {
          "normalization [64].\nFor MHSA in Eq.\n(1),\nit calculates dot-": ""
        },
        {
          "normalization [64].\nFor MHSA in Eq.\n(1),\nit calculates dot-": "(3)\nFFN(E) = GELU(EW1 + b1)W2 + b2,"
        },
        {
          "normalization [64].\nFor MHSA in Eq.\n(1),\nit calculates dot-": ""
        },
        {
          "normalization [64].\nFor MHSA in Eq.\n(1),\nit calculates dot-": "where W1 ∈ RC×4C, b1 ∈ R4C, W2 ∈ R4C×C, and b2 ∈ RC are"
        },
        {
          "normalization [64].\nFor MHSA in Eq.\n(1),\nit calculates dot-": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "plete the task of masked audio-visual reconstruction. Specifi-": "cally, HiCMAE adds an MHCA layer before each (except for",
          "∈\nfor HCMCL. Since HCMCL is\nconducted\nqi\n{1, ..., Ns})": "within a batch of samples,\nfor convenience, we add an addi-"
        },
        {
          "plete the task of masked audio-visual reconstruction. Specifi-": "",
          "∈\nfor HCMCL. Since HCMCL is\nconducted\nqi\n{1, ..., Ns})": "i\ntokens (i.e., E j"
        },
        {
          "plete the task of masked audio-visual reconstruction. Specifi-": "",
          "∈\nfor HCMCL. Since HCMCL is\nconducted\nqi\n{1, ..., Ns})": "i,m)"
        },
        {
          "plete the task of masked audio-visual reconstruction. Specifi-": "Dk\nlayer,\nthe skip\nm (m ∈ {a, v}) output by the kth Transformer",
          "∈\nfor HCMCL. Since HCMCL is\nconducted\nqi\n{1, ..., Ns})": "to indicate the sample index in this batch. Given a batch of N"
        },
        {
          "plete the task of masked audio-visual reconstruction. Specifi-": "connection constructed by MHCA enables it\nto directly attend",
          "∈\nfor HCMCL. Since HCMCL is\nconducted\nqi\n{1, ..., Ns})": "N,a} and N video tokens {E j\nN,v}\n1,a, ..., E j\n1,v, ..., E j"
        },
        {
          "plete the task of masked audio-visual reconstruction. Specifi-": "to the intermediate encoder\nrepresentations E j\nm with different",
          "∈\nfor HCMCL. Since HCMCL is\nconducted\nqi\n{1, ..., Ns})": "∈\nfrom jth ( j\nencoder\nlayer, we first perform"
        },
        {
          "plete the task of masked audio-visual reconstruction. Specifi-": "",
          "∈\nfor HCMCL. Since HCMCL is\nconducted\nqi\n{1, ..., Ns})": "{q1, ..., qNc })"
        },
        {
          "plete the task of masked audio-visual reconstruction. Specifi-": "levels of\ninformation (from high-level coarse-grained to low-",
          "∈\nfor HCMCL. Since HCMCL is\nconducted\nqi\n{1, ..., Ns})": "global average pooling to obtain the sample-level features, i.e.,"
        },
        {
          "plete the task of masked audio-visual reconstruction. Specifi-": "level fine-grained) and extract useful\ninformation to reinforce",
          "∈\nfor HCMCL. Since HCMCL is\nconducted\nqi\n{1, ..., Ns})": "e j\n= AvgPool(E j\nand e j\n= AvgPool(E j"
        },
        {
          "plete the task of masked audio-visual reconstruction. Specifi-": "",
          "∈\nfor HCMCL. Since HCMCL is\nconducted\nqi\n{1, ..., Ns})": "i,a\ni,a)\ni,v\ni,v)."
        },
        {
          "plete the task of masked audio-visual reconstruction. Specifi-": "itself. Then, as usual, a standard Transformer layer is followed",
          "∈\nfor HCMCL. Since HCMCL is\nconducted\nqi\n{1, ..., Ns})": "we utilize symmetric InfoNCE loss\n[67]\nfor HCMCL. For a"
        },
        {
          "plete the task of masked audio-visual reconstruction. Specifi-": "",
          "∈\nfor HCMCL. Since HCMCL is\nconducted\nqi\n{1, ..., Ns})": "batch of audio features e j"
        },
        {
          "plete the task of masked audio-visual reconstruction. Specifi-": "to exploit global self-attention to infer masked information. The",
          "∈\nfor HCMCL. Since HCMCL is\nconducted\nqi\n{1, ..., Ns})": "a = {e j\nN,a} and video features\n1,a, ..., e j"
        },
        {
          "plete the task of masked audio-visual reconstruction. Specifi-": "whole process can be formulated as follows:",
          "∈\nfor HCMCL. Since HCMCL is\nconducted\nqi\n{1, ..., Ns})": ""
        },
        {
          "plete the task of masked audio-visual reconstruction. Specifi-": "",
          "∈\nfor HCMCL. Since HCMCL is\nconducted\nqi\n{1, ..., Ns})": "e j"
        },
        {
          "plete the task of masked audio-visual reconstruction. Specifi-": "",
          "∈\nfor HCMCL. Since HCMCL is\nconducted\nqi\n{1, ..., Ns})": "N,v}, the symmetric InfoNCE loss maximizes the\n1,v, ..., e j"
        },
        {
          "plete the task of masked audio-visual reconstruction. Specifi-": "",
          "∈\nfor HCMCL. Since HCMCL is\nconducted\nqi\n{1, ..., Ns})": "cosine similarity of N paired audio-visual\nfeatures (i.e.,\nfrom"
        },
        {
          "plete the task of masked audio-visual reconstruction. Specifi-": "Dk′\n,\n= MHCA(LN(Dk−1\n), LN(E j\nm)) + Dk−1\nm\nm",
          "∈\nfor HCMCL. Since HCMCL is\nconducted\nqi\n{1, ..., Ns})": ""
        },
        {
          "plete the task of masked audio-visual reconstruction. Specifi-": "",
          "∈\nfor HCMCL. Since HCMCL is\nconducted\nqi\n{1, ..., Ns})": "the same sample) in the batch while minimizing the cosine sim-"
        },
        {
          "plete the task of masked audio-visual reconstruction. Specifi-": "Dk′′\n= MHSA(LN(Dk′",
          "∈\nfor HCMCL. Since HCMCL is\nconducted\nqi\n{1, ..., Ns})": ""
        },
        {
          "plete the task of masked audio-visual reconstruction. Specifi-": "(7)\nm\nm)) + Dk′\nm,",
          "∈\nfor HCMCL. Since HCMCL is\nconducted\nqi\n{1, ..., Ns})": "ilarity of features of N(N − 1) incorrect pairings (i.e., from dif-"
        },
        {
          "plete the task of masked audio-visual reconstruction. Specifi-": "",
          "∈\nfor HCMCL. Since HCMCL is\nconducted\nqi\n{1, ..., Ns})": "ferent samples):"
        },
        {
          "plete the task of masked audio-visual reconstruction. Specifi-": "Dk\n= FFN(LN(Dk′′\nm\nm )) + Dk′′\nm ,",
          "∈\nfor HCMCL. Since HCMCL is\nconducted\nqi\n{1, ..., Ns})": ""
        },
        {
          "plete the task of masked audio-visual reconstruction. Specifi-": "where k ∈ {2, ..., Nd} is the decoder layer index,\nj ∈ {q1, ..., qNc }",
          "∈\nfor HCMCL. Since HCMCL is\nconducted\nqi\n{1, ..., Ns})": ""
        },
        {
          "plete the task of masked audio-visual reconstruction. Specifi-": "",
          "∈\nfor HCMCL. Since HCMCL is\nconducted\nqi\n{1, ..., Ns})": "1 2\n[L(e j\nLInfoNCE(e j\na, e j\nv) + L(e j\nv, e j\na)],\na, e j\nv) = −"
        },
        {
          "plete the task of masked audio-visual reconstruction. Specifi-": "is the selected encoder\nlayer\nis the number of skip\nindex, Nc",
          "∈\nfor HCMCL. Since HCMCL is\nconducted\nqi\n{1, ..., Ns})": ""
        },
        {
          "plete the task of masked audio-visual reconstruction. Specifi-": "",
          "∈\nfor HCMCL. Since HCMCL is\nconducted\nqi\n{1, ..., Ns})": "N\nexp (sim(e j"
        },
        {
          "plete the task of masked audio-visual reconstruction. Specifi-": "connections, and qi ∈ {1, ..., Ns}.",
          "∈\nfor HCMCL. Since HCMCL is\nconducted\nqi\n{1, ..., Ns})": "(cid:88)\ni,a, e j\ni,v)/τ)"
        },
        {
          "plete the task of masked audio-visual reconstruction. Specifi-": "",
          "∈\nfor HCMCL. Since HCMCL is\nconducted\nqi\n{1, ..., Ns})": "1 N\n,\nL(e j\nlog\na, e j\nv) = −"
        },
        {
          "plete the task of masked audio-visual reconstruction. Specifi-": "Reconstruction Loss.\nFinally,\nthe decoder output DNd\nis",
          "∈\nfor HCMCL. Since HCMCL is\nconducted\nqi\n{1, ..., Ns})": "(cid:80)N"
        },
        {
          "plete the task of masked audio-visual reconstruction. Specifi-": "",
          "∈\nfor HCMCL. Since HCMCL is\nconducted\nqi\n{1, ..., Ns})": "(9)"
        },
        {
          "plete the task of masked audio-visual reconstruction. Specifi-": "",
          "∈\nfor HCMCL. Since HCMCL is\nconducted\nqi\n{1, ..., Ns})": "q,v)/τ)\ni=1\nq=1 exp (sim(e j\ni,a, e j"
        },
        {
          "plete the task of masked audio-visual reconstruction. Specifi-": "passed through a linear projection layer to predict\nthe masked",
          "∈\nfor HCMCL. Since HCMCL is\nconducted\nqi\n{1, ..., Ns})": ""
        },
        {
          "plete the task of masked audio-visual reconstruction. Specifi-": "",
          "∈\nfor HCMCL. Since HCMCL is\nconducted\nqi\n{1, ..., Ns})": "N\nexp (sim(e j"
        },
        {
          "plete the task of masked audio-visual reconstruction. Specifi-": "information in each modality. We calculate the mean squared",
          "∈\nfor HCMCL. Since HCMCL is\nconducted\nqi\n{1, ..., Ns})": "(cid:88)\ni,v, e j\ni,a)/τ)"
        },
        {
          "plete the task of masked audio-visual reconstruction. Specifi-": "",
          "∈\nfor HCMCL. Since HCMCL is\nconducted\nqi\n{1, ..., Ns})": "1 N\n,\nlog\nL(e j\nv, e j\na) = −"
        },
        {
          "plete the task of masked audio-visual reconstruction. Specifi-": "error between the ground truth and reconstructed data in the",
          "∈\nfor HCMCL. Since HCMCL is\nconducted\nqi\n{1, ..., Ns})": "(cid:80)N"
        },
        {
          "plete the task of masked audio-visual reconstruction. Specifi-": "",
          "∈\nfor HCMCL. Since HCMCL is\nconducted\nqi\n{1, ..., Ns})": "q,a)/τ)\ni=1\nq=1 exp (sim(e j\ni,v, e j"
        },
        {
          "plete the task of masked audio-visual reconstruction. Specifi-": "masked positions for each modality and sum them to get\nthe",
          "∈\nfor HCMCL. Since HCMCL is\nconducted\nqi\n{1, ..., Ns})": ""
        },
        {
          "plete the task of masked audio-visual reconstruction. Specifi-": "reconstruction loss:",
          "∈\nfor HCMCL. Since HCMCL is\nconducted\nqi\n{1, ..., Ns})": "x⊤y"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 2: Compared with state-of-the-art",
      "data": [
        {
          "downstream emotion recognition tasks, we utilize both cross-": "modal and unimodal\nfeatures from the encoders.\nThe cross-",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": "velopment set and a test set. In this paper, we only use its devel-"
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "modal\nfeatures\ncome\nfrom the\ncross-modal\nfusion encoder.",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": "opment set which has 1,092,009 video clips for self-supervised"
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "We simply perform global average pooling to output\ntokens",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": "pre-training.\nFor each video clip, we sample 16 consecutive"
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "from the last\nfusion layer. The unimodal\nfeatures come from",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": "frames with a temporal stride of 4 frames and follow [50]\nto"
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "modality-specific encoders. To fully exploit features of differ-",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": "extract 160 × 160 patch in each frame to obtain the video input"
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "ent\nlevels, we use learnable weights to combine features from",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": "with a size of 16 × 160 × 160 × 3. The corresponding audio"
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "different audio/video encoder layers. Thus,\nthe overall feature",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": "waveform (2.56s)\nis also extracted and converted into a 128-"
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "can be obtained as follows:",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": "dimensional\nlog Mel filterbank feature sequence using a 25ms"
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": "Hanning window with a hop length of 10ms [18], resulting in"
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "e = Concat(eN f\na→v, eN f\nv→a, ea, ev),",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": ""
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": "the audio spectrogram with a size of 256 × 128. The loss weight"
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": "in Eq.\n(11)\nis set\nto λ = 0.0025. The temperature factor\nfor"
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "(12)\nNs(cid:88)\nNs(cid:88)",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": ""
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "α j\nα j\nea =\nae j\na, ev =\nve j\nv,",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": "contrastive learning is fixed to τ = 0.07. We pre-train HiC-"
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "j=1\nj=1",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": "MAE for 100 epochs using four Nvidia Tesla V100 GPUs with"
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": "a total batch size of 160 and a base learning rate of 3e − 4.\nIt"
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "where eN f\nm =\na→v = AvgPool(EN f\na→v), eN f\nv→a = AvgPool(EN f\nv→a), e j",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": "takes about five days to complete the pre-training. For down-"
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "AvgPool(E j\nm) (m ∈ {a, v}), (cid:80)Ns",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": ""
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "m = 1.\nj=1 α j",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": "stream tasks, we fine-tune the pre-trained model for 50 or 100"
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "After obtaining the overall feature e, we simply use a linear",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": "epochs with a total batch size of 56 and a base learning rate of"
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "layer to project it to get the final emotion prediction ˆy. For the",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": "1e − 3. During inference, we follow [50] to uniformly sample"
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "classification task, we utilize the classic cross-entropy loss for",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": "two clips from each video and calculate their mean score as the"
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "model fine-tuning:",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": "final prediction. Other hyper-parameters for pre-training and"
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": "fine-tuning can refer to [61, 65, 18] for details."
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "K",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": ""
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "(cid:88)",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": ""
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "(13)\nyk log ˆyK,\nLCLS = −",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": ""
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": "4.2. Categorical Audio-Visual Emotion Recognition:\nIn-the-"
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "k=1",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": ""
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": "Wild Setting"
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "=\n=\n∈ RK",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": ""
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "y\ny\nwhere\ndenotes\nthe prediction,\n[ˆy1, ..., ˆyK]",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": ""
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": "4.2.1. Datasets"
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "[y1, ..., yK] ∈ RK is the target, and K is the number of emo-",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": ""
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": "The basic dataset information is summarized in Table 1."
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "tion categories. For the regression task, we compute the mean",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": ""
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": "MAFW [5]\nis a large-scale multimodal compound in-the-"
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "square error between the target and the prediction:",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": ""
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": "wild affective dataset.\nIt consists of 10,045 video clips anno-"
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": "tated with 11 common emotions (including seven basic emo-"
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "(14)\nLREG = ||y − ˆy||2.",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": ""
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": "tions,\ncontempt,\nanxiety,\nhelplessness,\nand disappointment)."
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": "Each video clip is also accompanied by several\ntextual\nsen-"
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "where ˆy ∈ RD, ˆy ∈ RD, and D is the number of emotion dimen-",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": ""
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": "tences to describe the subject’s affective behaviors. The dataset"
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "sions. During downstream fine-tuning,\nthe audio-visual emo-",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": ""
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": "provides an 11-class single-labeled set (9,172 video clips) and"
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "(13) or\ntion semantics are explicitly modeled by LCLS in Eq.",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": ""
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": "a 43-class compound set (8,996 video clips). For model eval-"
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "LREG in Eq. (14).",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": ""
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": "uation, we follow the original paper\nto adopt a 5-fold cross-"
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": "validation protocol."
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "4. Experiments",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": "DFEW [29]\ncomprises 16,372 video clips which are\nex-"
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": "tracted from over 1,500 high-definition movies.\nThis dataset"
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "4.1.\nImplementation Details",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": "presents several challenging characteristics, such as extreme il-"
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": "lumination and occlusion. The video clips are annotated with"
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "We develop three versions of HiCMAE (i.e., base: HiCMAE-",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": ""
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": "seven basic emotions (i.e., happy, sad, neutral, anger, surprise,"
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "B, small: HiCMAE-S, tiny: HiCMAE-T) to meet various needs",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": ""
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": "disgust, and fear). To align with previous work [29, 50, 9], we"
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "in real-world applications.\nTheir main difference is\nthe size",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": ""
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": "perform 5-fold cross-validation on 11,697 single-labeled clips"
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "of hidden units (C = 512, C = 384, and C = 256,\nrespec-",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": ""
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": "for evaluation."
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "tively) in the encoder. For three models, we use Ns = 10 lay-",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": ""
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": "MER-MULTI [25] provides 3,373 training video clips orig-"
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "ers in modality-specific encoders, N f = 2 layers in the cross-",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": ""
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": "inating from Chinese TV series and movies. The dataset is an-"
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "modal\nfusion encoder, and Nd = 4 layers in the lightweight",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": ""
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": "notated with six emotions,\nincluding neutral, anger, happiness,"
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "decoder. We introduce three hierarchical skip connections be-",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": ""
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": "sadness, worry, and surprise. We follow the original paper to"
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "tween the modality-specific encoder and the decoder, specifi-",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": ""
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": "conduct 5-fold cross-validation on 3,373 video clips for hyper-"
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "cally between the 4th encoder layer and the 2nd decoder layer,",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": ""
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": "parameter tuning and evaluate the model on a held-out\ntest set"
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "the 7th encoder layer and 3rd decoder layer, as well as between",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": ""
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": "with 411 video clips."
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "the 10th encoder layer and the 4th decoder layer. The hierar-",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": ""
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "chical cross-modal contrastive learning is also applied to these",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": ""
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "selected audio-visual encoder layers.",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": "4.2.2. Results on MAFW"
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "We pre-train HiCMAE on a very large audio-visual dataset",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": "We first present\nthe results of 11 single-labeled emotions"
        },
        {
          "downstream emotion recognition tasks, we utilize both cross-": "VoxCeleb2 [27].\nIt contains more than one million video clips",
          "from over six thousand celebrities. VoxCeleb2 is split into a de-": "on MAFW [5]\nin Table 2.\nCompared with state-of-the-art"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 1: Basic information of nine AVER datasets used in this paper.": "#Subjects"
        },
        {
          "Table 1: Basic information of nine AVER datasets used in this paper.": "N/A"
        },
        {
          "Table 1: Basic information of nine AVER datasets used in this paper.": ""
        },
        {
          "Table 1: Basic information of nine AVER datasets used in this paper.": "N/A"
        },
        {
          "Table 1: Basic information of nine AVER datasets used in this paper.": "N/A"
        },
        {
          "Table 1: Basic information of nine AVER datasets used in this paper.": "N/A"
        },
        {
          "Table 1: Basic information of nine AVER datasets used in this paper.": "91"
        },
        {
          "Table 1: Basic information of nine AVER datasets used in this paper.": ""
        },
        {
          "Table 1: Basic information of nine AVER datasets used in this paper.": "91"
        },
        {
          "Table 1: Basic information of nine AVER datasets used in this paper.": "12"
        },
        {
          "Table 1: Basic information of nine AVER datasets used in this paper.": "24"
        },
        {
          "Table 1: Basic information of nine AVER datasets used in this paper.": "10"
        },
        {
          "Table 1: Basic information of nine AVER datasets used in this paper.": "129"
        },
        {
          "Table 1: Basic information of nine AVER datasets used in this paper.": "106"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 2: Comparison with state-of-the-art methods on MAFW (11-class). SSL: self-supervised learning method or not. AN: anger. DI: disgust. FE: fear. HA:": "happiness. NE: neutral. SA: sadness. SU: surprise. CO: contempt. AX: anxiety. HL: helplessness. DS: disappointment. UAR: unweighted average recall. WAR:"
        },
        {
          "Table 2: Comparison with state-of-the-art methods on MAFW (11-class). SSL: self-supervised learning method or not. AN: anger. DI: disgust. FE: fear. HA:": "weighted average recall. *: do not use pre-trained models for initialization. Throughout the paper, we highlight the best result in bold and underline the second best."
        },
        {
          "Table 2: Comparison with state-of-the-art methods on MAFW (11-class). SSL: self-supervised learning method or not. AN: anger. DI: disgust. FE: fear. HA:": ""
        },
        {
          "Table 2: Comparison with state-of-the-art methods on MAFW (11-class). SSL: self-supervised learning method or not. AN: anger. DI: disgust. FE: fear. HA:": "Method"
        },
        {
          "Table 2: Comparison with state-of-the-art methods on MAFW (11-class). SSL: self-supervised learning method or not. AN: anger. DI: disgust. FE: fear. HA:": ""
        },
        {
          "Table 2: Comparison with state-of-the-art methods on MAFW (11-class). SSL: self-supervised learning method or not. AN: anger. DI: disgust. FE: fear. HA:": ""
        },
        {
          "Table 2: Comparison with state-of-the-art methods on MAFW (11-class). SSL: self-supervised learning method or not. AN: anger. DI: disgust. FE: fear. HA:": "Wav2Vec2.0 [47]"
        },
        {
          "Table 2: Comparison with state-of-the-art methods on MAFW (11-class). SSL: self-supervised learning method or not. AN: anger. DI: disgust. FE: fear. HA:": "HuBERT [48]"
        },
        {
          "Table 2: Comparison with state-of-the-art methods on MAFW (11-class). SSL: self-supervised learning method or not. AN: anger. DI: disgust. FE: fear. HA:": "WavLM-Plus [71]"
        },
        {
          "Table 2: Comparison with state-of-the-art methods on MAFW (11-class). SSL: self-supervised learning method or not. AN: anger. DI: disgust. FE: fear. HA:": "HiCMAE-T"
        },
        {
          "Table 2: Comparison with state-of-the-art methods on MAFW (11-class). SSL: self-supervised learning method or not. AN: anger. DI: disgust. FE: fear. HA:": "HiCMAE-S"
        },
        {
          "Table 2: Comparison with state-of-the-art methods on MAFW (11-class). SSL: self-supervised learning method or not. AN: anger. DI: disgust. FE: fear. HA:": "HiCMAE-B"
        },
        {
          "Table 2: Comparison with state-of-the-art methods on MAFW (11-class). SSL: self-supervised learning method or not. AN: anger. DI: disgust. FE: fear. HA:": "ResNet-18 [72]"
        },
        {
          "Table 2: Comparison with state-of-the-art methods on MAFW (11-class). SSL: self-supervised learning method or not. AN: anger. DI: disgust. FE: fear. HA:": "ViT [73]"
        },
        {
          "Table 2: Comparison with state-of-the-art methods on MAFW (11-class). SSL: self-supervised learning method or not. AN: anger. DI: disgust. FE: fear. HA:": "C3D [43]"
        },
        {
          "Table 2: Comparison with state-of-the-art methods on MAFW (11-class). SSL: self-supervised learning method or not. AN: anger. DI: disgust. FE: fear. HA:": "ResNet-18+LSTM [5]"
        },
        {
          "Table 2: Comparison with state-of-the-art methods on MAFW (11-class). SSL: self-supervised learning method or not. AN: anger. DI: disgust. FE: fear. HA:": "ViT+LSTM [5]"
        },
        {
          "Table 2: Comparison with state-of-the-art methods on MAFW (11-class). SSL: self-supervised learning method or not. AN: anger. DI: disgust. FE: fear. HA:": "C3D+LSTM [5]"
        },
        {
          "Table 2: Comparison with state-of-the-art methods on MAFW (11-class). SSL: self-supervised learning method or not. AN: anger. DI: disgust. FE: fear. HA:": "Former-DFER [74]"
        },
        {
          "Table 2: Comparison with state-of-the-art methods on MAFW (11-class). SSL: self-supervised learning method or not. AN: anger. DI: disgust. FE: fear. HA:": "T-ESFL [5]"
        },
        {
          "Table 2: Comparison with state-of-the-art methods on MAFW (11-class). SSL: self-supervised learning method or not. AN: anger. DI: disgust. FE: fear. HA:": "T-MEP [9]"
        },
        {
          "Table 2: Comparison with state-of-the-art methods on MAFW (11-class). SSL: self-supervised learning method or not. AN: anger. DI: disgust. FE: fear. HA:": "DFER-CLIP [75]"
        },
        {
          "Table 2: Comparison with state-of-the-art methods on MAFW (11-class). SSL: self-supervised learning method or not. AN: anger. DI: disgust. FE: fear. HA:": "SVFAP [49]"
        },
        {
          "Table 2: Comparison with state-of-the-art methods on MAFW (11-class). SSL: self-supervised learning method or not. AN: anger. DI: disgust. FE: fear. HA:": "MAE-DFER [50]"
        },
        {
          "Table 2: Comparison with state-of-the-art methods on MAFW (11-class). SSL: self-supervised learning method or not. AN: anger. DI: disgust. FE: fear. HA:": "HiCMAE-T"
        },
        {
          "Table 2: Comparison with state-of-the-art methods on MAFW (11-class). SSL: self-supervised learning method or not. AN: anger. DI: disgust. FE: fear. HA:": "HiCMAE-S"
        },
        {
          "Table 2: Comparison with state-of-the-art methods on MAFW (11-class). SSL: self-supervised learning method or not. AN: anger. DI: disgust. FE: fear. HA:": "HiCMAE-B"
        },
        {
          "Table 2: Comparison with state-of-the-art methods on MAFW (11-class). SSL: self-supervised learning method or not. AN: anger. DI: disgust. FE: fear. HA:": "ResNet-18+LSTM [5]"
        },
        {
          "Table 2: Comparison with state-of-the-art methods on MAFW (11-class). SSL: self-supervised learning method or not. AN: anger. DI: disgust. FE: fear. HA:": "C3D+LSTM [5]"
        },
        {
          "Table 2: Comparison with state-of-the-art methods on MAFW (11-class). SSL: self-supervised learning method or not. AN: anger. DI: disgust. FE: fear. HA:": "AMH [76]"
        },
        {
          "Table 2: Comparison with state-of-the-art methods on MAFW (11-class). SSL: self-supervised learning method or not. AN: anger. DI: disgust. FE: fear. HA:": "T-ESFL [5]"
        },
        {
          "Table 2: Comparison with state-of-the-art methods on MAFW (11-class). SSL: self-supervised learning method or not. AN: anger. DI: disgust. FE: fear. HA:": "T-MEP* [9]"
        },
        {
          "Table 2: Comparison with state-of-the-art methods on MAFW (11-class). SSL: self-supervised learning method or not. AN: anger. DI: disgust. FE: fear. HA:": "T-MEP [9]"
        },
        {
          "Table 2: Comparison with state-of-the-art methods on MAFW (11-class). SSL: self-supervised learning method or not. AN: anger. DI: disgust. FE: fear. HA:": "HiCMAE-T"
        },
        {
          "Table 2: Comparison with state-of-the-art methods on MAFW (11-class). SSL: self-supervised learning method or not. AN: anger. DI: disgust. FE: fear. HA:": "HiCMAE-S"
        },
        {
          "Table 2: Comparison with state-of-the-art methods on MAFW (11-class). SSL: self-supervised learning method or not. AN: anger. DI: disgust. FE: fear. HA:": "HiCMAE-B"
        },
        {
          "Table 2: Comparison with state-of-the-art methods on MAFW (11-class). SSL: self-supervised learning method or not. AN: anger. DI: disgust. FE: fear. HA:": "ResNet18+MDRE [77]"
        },
        {
          "Table 2: Comparison with state-of-the-art methods on MAFW (11-class). SSL: self-supervised learning method or not. AN: anger. DI: disgust. FE: fear. HA:": "AMH [76]"
        },
        {
          "Table 2: Comparison with state-of-the-art methods on MAFW (11-class). SSL: self-supervised learning method or not. AN: anger. DI: disgust. FE: fear. HA:": "Rajan et al. [78]"
        },
        {
          "Table 2: Comparison with state-of-the-art methods on MAFW (11-class). SSL: self-supervised learning method or not. AN: anger. DI: disgust. FE: fear. HA:": "T-ESFL [5]"
        },
        {
          "Table 2: Comparison with state-of-the-art methods on MAFW (11-class). SSL: self-supervised learning method or not. AN: anger. DI: disgust. FE: fear. HA:": "T-MEP* [9]"
        },
        {
          "Table 2: Comparison with state-of-the-art methods on MAFW (11-class). SSL: self-supervised learning method or not. AN: anger. DI: disgust. FE: fear. HA:": "T-MEP [9]"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 4: In the audio-visual",
      "data": [
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": ""
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": "self-supervised learning method or not."
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": ""
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": "WAR: weighted average recall. MF1: macro-averaged F1-score. AUC: area"
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": "under curve. *: do not use pre-trained models for initialization."
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": ""
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": "SSL"
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": "✓"
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": ""
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": "✓"
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": ""
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": "✓"
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": "✓"
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": ""
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": "✓"
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": "✓"
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": ""
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": "×"
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": "×"
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": ""
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": "×"
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": "×"
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": "×"
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": "×"
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": ""
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": "×"
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": ""
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": "×"
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": "×"
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": "✓"
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": ""
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": "✓"
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": "✓"
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": ""
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": ""
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": "×"
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": "×"
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": "×"
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": "×"
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": ""
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": "×"
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": ""
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": ""
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": "✓"
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": ""
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": "✓"
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": ""
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": "×"
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": ""
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": "×"
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": ""
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": "×"
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": "×"
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": "×"
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": ""
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": "×"
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": ""
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": ""
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": ""
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": "brings similar performance gains when compared with the pre-"
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": ""
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": ""
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": ""
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": ""
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": "4.2.3. Results on DFEW and MER-MULTI"
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": "the performance comparison with state-of-the-"
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": "art methods on DFEW [29]"
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": "setting, our HiCMAE-S significantly outperforms the previous"
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": "best-supervised method T-MEP by +5.89% UAR and +5.48%"
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": "WAR, while having 25% fewer parameters and similar compu-"
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": "tational costs. In unimodal settings, although our models under-"
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": "perform state-of-the-art unimodal baselines,"
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": "very competitive performance."
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": "our method has significantly fewer parameters and FLOPs, thus"
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": "making it more suitable in resource-constrained scenarios."
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": "a Chinese"
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": "shown in Table 5. Our HiCMAE-B slightly outperforms the"
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": "previous best method which utilizes MA-Net [93] (supervised"
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": "on a facial expression dataset) and a powerful HuBERT-CH"
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": "model (pre-trained on 10k+ hours of Chinese speech data [91])."
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": "The audio-only result of our method has a similar performance"
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": "with three self-supervised models which are also pre-trained"
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": "mostly on English speech data, while it is largely inferior to the"
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": "state-of-the-art HuBERT-CH. We argue that"
        },
        {
          "Table 3: Comparison with state-of-the-art methods on MAFW (43-class). SSL:": "to the large domain gap between pre-training and fine-tuning"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 4: Comparison with state-of-the-art methods on DFEW. SSL: self- Table5:Comparisonwithstate-of-the-artmethodsonMER-MULTI.SSL:self-",
      "data": [
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "weighted average recall. *: do not use pre-trained models for initialization.",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": ""
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": ""
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "Method",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": "SSL"
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": ""
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "Wav2Vec2.0 [47]",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": "×"
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "HuBERT [48]",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": "×"
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": ""
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "WavLM-Plus [71]",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": "✓"
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": ""
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "HiCMAE-T",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": "✓"
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": ""
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "HiCMAE-S",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": "✓"
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": ""
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "HiCMAE-B",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": "✓"
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": ""
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": "✓"
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "C3D [43]",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": ""
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": "✓"
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "R(2+1)D-18 [82]",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": ""
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": "✓"
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "3D ResNet-18 [83]",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": ""
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "EC-STFL [29]",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": "×"
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "ResNet-18+LSTM [74]",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": "×"
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "ResNet-18+GRU [74]",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": "×"
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "Former-DFER [74]",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": "×"
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "CEFLNet [84]",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": "×"
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": ""
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "EST [85]",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": "✓"
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": ""
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "STT [86]",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": "✓"
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": ""
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "NR-DFERNet [87]",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": "✓"
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": ""
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "DPCNet [88]",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": ""
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": ""
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "IAL [89]",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": "✓"
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": ""
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "M3DFEL [90]",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": ""
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": ""
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": ""
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "T-MEP [9]",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": ""
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": ""
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "DFER-CLIP [75]",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": ""
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": "✓"
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "SVFAP [49]",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": ""
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": "✓"
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "MAE-DFER [50]",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": ""
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": "✓"
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "HiCMAE-T",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": ""
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "HiCMAE-S",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": ""
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "HiCMAE-B",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": ""
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "ResNet-18+LSTM [9]",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": ""
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": "RAVDESS [69] is an audio-visual dataset that includes emo-"
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "C3D+LSTM [9]",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": ""
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "AMH [76]",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": "tional speech and song. It comprises 2,880 video clips featuring"
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": ""
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "T-MEP* [9]",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": "24 professional actors, each labeled with one of eight emotions"
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "T-MEP [9]",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": ""
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": "(i.e., seven basic emotions and calm)."
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "HiCMAE-T",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": ""
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "HiCMAE-S",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": "use the speech part consisting of 1,440 video clips. We adopt a"
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": ""
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "HiCMAE-B",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": "subject-independent 6-fold cross-validation protocol for evalu-"
        },
        {
          "supervised learning method or not. UAR: unweighted average recall. WAR:": "",
          "supervised learning method or not. UAR: unweighted average recall. WF1:": ""
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class). ically, the smallest HiCMAE-T surpasses the best VQ-MAE-",
      "data": [
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "SSL: self-supervised learning method or not. UAR: unweighted average recall.",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "AV by +3.34% WAR while using 33% fewer parameters. With"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "WAR: weighted average recall.",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "the increase in model size, HiCMAE-B pushes the performance"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "gap to even larger\n(+4.49% WAR), establishing a new state-"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "Method",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "of-the-art result on this dataset. These results demonstrate the"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "AuxFormer [53]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "effectiveness of\nthe three-pronged strategy to promote hierar-"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "LR+eGeMAPS [99, 32]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "chical\nfeature learning in HiCMAE. Finally, our method also"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "LR+wav2vec [99, 47]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "Wav2Vec2.0 [47]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "outperforms advanced supervised baselines (e.g., Ladder Net-"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "HuBERT [48]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "works [105])."
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "WavLM-Plus [71]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "In the unimodal\nsetting, we observe that our method still"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "HiCMAE-T",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "HiCMAE-S",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "maintains competitive performance.\nFor example,\nfor visual"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "HiCMAE-B",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "modality, HiCMAE-B is\nslightly inferior\n(<0.2% UAR and"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "AuxFormer [53]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "WAR) to state-of-the-art MAE-DFER [50], but requires signif-"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "VO-LSTM [100]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "icantly fewer parameters (62%) and computational costs (36%"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "Goncalves et al. [101]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "Lei et al. [102]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "FLOPs). For audio modality, although the performance gap be-"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "SVFAP [49]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "tween HiCMAE-B and WavLM-Plus is larger (about 2% UAR"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "MAE-DFER [50]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "and WAR), it is still acceptable as the latter is 3× larger than the"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "HiCMAE-T",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "HiCMAE-S",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "former and has 4.5× more FLOPs."
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "HiCMAE-B",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "In addition to the default six emotions on this dataset, we"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "EF-GRU [55]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "also conduct experiments on a subset with four emotions. The"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "LF-GRU [55]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "TFN [103]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "audio-visual results are shown in Table 7. Among these base-"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "MATER [104]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "lines, most are self-supervised methods and the HuBERT series"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "MulT Base [55]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "except for AV-HuBERT [106] (i.e., FAV-HuBERT [94], TAPT-"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "MulT Large [55]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "AuxFormer [53]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "HuBERT [94], CTAPT-HuBERT [94], and AW-HuBERT [94])"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "AV-LSTM [100]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "are pre-trained on VoxCeleb2 with industry-level computation"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "AV-Gating [100]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "resources (32 Tesla-V100 GPUs)\nfor approximately 10 days."
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "Goncalves et al. [101]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "Ladder Networks [105]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "When compared with the best-performing AW-HuBERT, our"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "VQ-MAE-AV+",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "HiCMAE-B still shows slight improvement (+0.35% UAR and"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "Attn. Pooling [20]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "VQ-MAE-AV+",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "+0.48% WAR), while being 21% smaller and training-friendly"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "Query2Emo [20]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "(we only need 4 Tesla-V100 GPUs to pre-train the model\nfor"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "HiCMAE-T",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "about 5 days).\nIt should also be noted that AW-HuBERT is a"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "HiCMAE-S",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "HiCMAE-B",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "semi-supervised method that\nrequires another\nlabeled dataset"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "and unlabeled samples\nfrom VoxCeleb2 for affective adapta-"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "tion to obtain improved performance. Thus, these results amply"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "demonstrate the superiority of the proposed method."
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "Method",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "4.3.3. Results on MSP-IMPROV, RAVDESS, and IEMOCAP"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "Tran et al. [55]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "The audio-visual results of MSP-IMPROV [68] are shown in"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "AuxFormer [53]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "AV-HuBERT [106]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "Table 8. We observe that, when compared with the state-of-the-"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "FAV-HuBERT [94]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "art AW-HuBERT, HiCMAE-B achieves similar UAR but shows"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "TAPT-HuBERT [94]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "a large improvement (+3.15%) in terms of WAR. Two smaller"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "CTAPT-HuBERT [94]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "AW-HuBERT [94]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "versions of HiCMAE have lower UAR. However, their WAR is"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "HiCMAE-T",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "still higher than AW-HuBERT."
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "HiCMAE-S",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "In Table 9, we compare the proposed method with state-of-"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "HiCMAE-B",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "the-art methods on RAVDESS [69].\nIt can be seen that HiC-"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "MAE brings large gains over the previous best method. In spe-"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "cific, our HiCMAE-B achieves a gain of +3.19% WAR over"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "VQ-MAE-AV+Query2Emo [20]. HiCMAE-T still outperforms"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "Method",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "it by +1.31% WAR, while using 33% fewer parameters. The"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "Tran et al. [55]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "unimodal results are less satisfactory on this dataset, probably"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "AuxFormer",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "due to the much smaller model size."
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "AV-HuBERT [106]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "FAV-HuBERT [94]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "The performance\ncomparison on IEMOCAP [70]\nis pre-"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "TAPT-HuBERT [94]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "sented in Table 10. Most baseline results are from AV-SUPERB"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "CTAPT-HuBERT [94]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "[98]. We find that HiCMAE achieves substantial improvement"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "AW-HuBERT [94]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "HiCMAE-T",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "over state-of-the-art generic audio-visual representation learner"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "HiCMAE-S",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "(e.g., MAViL [19] and AVBERT [112]) in the audio-visual set-"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "HiCMAE-B",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "ting.\nFor\nexample,\nour HiCMAE-S surpasses AVBERT by"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class). ically, the smallest HiCMAE-T surpasses the best VQ-MAE-",
      "data": [
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "SSL: self-supervised learning method or not. UAR: unweighted average recall.",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "AV by +3.34% WAR while using 33% fewer parameters. With"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "WAR: weighted average recall.",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "the increase in model size, HiCMAE-B pushes the performance"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "gap to even larger\n(+4.49% WAR), establishing a new state-"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "Method",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "of-the-art result on this dataset. These results demonstrate the"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "AuxFormer [53]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "effectiveness of\nthe three-pronged strategy to promote hierar-"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "LR+eGeMAPS [99, 32]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "chical\nfeature learning in HiCMAE. Finally, our method also"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "LR+wav2vec [99, 47]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "Wav2Vec2.0 [47]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "outperforms advanced supervised baselines (e.g., Ladder Net-"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "HuBERT [48]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "works [105])."
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "WavLM-Plus [71]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "In the unimodal\nsetting, we observe that our method still"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "HiCMAE-T",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "HiCMAE-S",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "maintains competitive performance.\nFor example,\nfor visual"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "HiCMAE-B",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "modality, HiCMAE-B is\nslightly inferior\n(<0.2% UAR and"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "AuxFormer [53]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "WAR) to state-of-the-art MAE-DFER [50], but requires signif-"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "VO-LSTM [100]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "icantly fewer parameters (62%) and computational costs (36%"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "Goncalves et al. [101]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "Lei et al. [102]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "FLOPs). For audio modality, although the performance gap be-"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "SVFAP [49]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "tween HiCMAE-B and WavLM-Plus is larger (about 2% UAR"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "MAE-DFER [50]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "and WAR), it is still acceptable as the latter is 3× larger than the"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "HiCMAE-T",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "HiCMAE-S",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "former and has 4.5× more FLOPs."
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "HiCMAE-B",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "In addition to the default six emotions on this dataset, we"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "EF-GRU [55]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "also conduct experiments on a subset with four emotions. The"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "LF-GRU [55]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "TFN [103]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "audio-visual results are shown in Table 7. Among these base-"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "MATER [104]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "lines, most are self-supervised methods and the HuBERT series"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "MulT Base [55]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "except for AV-HuBERT [106] (i.e., FAV-HuBERT [94], TAPT-"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "MulT Large [55]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "AuxFormer [53]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "HuBERT [94], CTAPT-HuBERT [94], and AW-HuBERT [94])"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "AV-LSTM [100]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "are pre-trained on VoxCeleb2 with industry-level computation"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "AV-Gating [100]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "resources (32 Tesla-V100 GPUs)\nfor approximately 10 days."
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "Goncalves et al. [101]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "Ladder Networks [105]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "When compared with the best-performing AW-HuBERT, our"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "VQ-MAE-AV+",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "HiCMAE-B still shows slight improvement (+0.35% UAR and"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "Attn. Pooling [20]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "VQ-MAE-AV+",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "+0.48% WAR), while being 21% smaller and training-friendly"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "Query2Emo [20]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "(we only need 4 Tesla-V100 GPUs to pre-train the model\nfor"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "HiCMAE-T",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "about 5 days).\nIt should also be noted that AW-HuBERT is a"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "HiCMAE-S",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "HiCMAE-B",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "semi-supervised method that\nrequires another\nlabeled dataset"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "and unlabeled samples\nfrom VoxCeleb2 for affective adapta-"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "tion to obtain improved performance. Thus, these results amply"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "demonstrate the superiority of the proposed method."
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "Method",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "4.3.3. Results on MSP-IMPROV, RAVDESS, and IEMOCAP"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "Tran et al. [55]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "The audio-visual results of MSP-IMPROV [68] are shown in"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "AuxFormer [53]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "AV-HuBERT [106]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "Table 8. We observe that, when compared with the state-of-the-"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "FAV-HuBERT [94]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "art AW-HuBERT, HiCMAE-B achieves similar UAR but shows"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "TAPT-HuBERT [94]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "a large improvement (+3.15%) in terms of WAR. Two smaller"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "CTAPT-HuBERT [94]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "AW-HuBERT [94]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "versions of HiCMAE have lower UAR. However, their WAR is"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "HiCMAE-T",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "still higher than AW-HuBERT."
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "HiCMAE-S",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "In Table 9, we compare the proposed method with state-of-"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "HiCMAE-B",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "the-art methods on RAVDESS [69].\nIt can be seen that HiC-"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "MAE brings large gains over the previous best method. In spe-"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "cific, our HiCMAE-B achieves a gain of +3.19% WAR over"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "VQ-MAE-AV+Query2Emo [20]. HiCMAE-T still outperforms"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "Method",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "it by +1.31% WAR, while using 33% fewer parameters. The"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "Tran et al. [55]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "unimodal results are less satisfactory on this dataset, probably"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "AuxFormer",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "due to the much smaller model size."
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "AV-HuBERT [106]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "FAV-HuBERT [94]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "The performance\ncomparison on IEMOCAP [70]\nis pre-"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "TAPT-HuBERT [94]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "sented in Table 10. Most baseline results are from AV-SUPERB"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "CTAPT-HuBERT [94]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "[98]. We find that HiCMAE achieves substantial improvement"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "AW-HuBERT [94]",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "HiCMAE-T",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "over state-of-the-art generic audio-visual representation learner"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "HiCMAE-S",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "(e.g., MAViL [19] and AVBERT [112]) in the audio-visual set-"
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "HiCMAE-B",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": ""
        },
        {
          "Table 6: Comparison with state-of-the-art methods on CREMA-D (6-class).": "",
          "ically,\nthe smallest HiCMAE-T surpasses the best VQ-MAE-": "ting.\nFor\nexample,\nour HiCMAE-S surpasses AVBERT by"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 11: Comparison with state-of-the-art methods on Werewolf-XL. SSL:",
      "data": [
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": ""
        },
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": "supervised learning method or not. UAR: unweighted average recall. WAR:"
        },
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": ""
        },
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": "weighted average recall."
        },
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": ""
        },
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": ""
        },
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": "Method"
        },
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": ""
        },
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": ""
        },
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": "LR+eGeMAPS [99, 32]"
        },
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": ""
        },
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": "LR+wav2vec [99, 47]"
        },
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": ""
        },
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": "Wav2Vec2.0 [47]"
        },
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": ""
        },
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": "HuBERT [48]"
        },
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": ""
        },
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": "WavLM-Plus [71]"
        },
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": ""
        },
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": "HiCMAE-T"
        },
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": ""
        },
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": "HiCMAE-S"
        },
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": ""
        },
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": "HiCMAE-B"
        },
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": ""
        },
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": "VO-LSTM [100]"
        },
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": "3D ResNeXt-50 [95]"
        },
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": "SVFAP [49]"
        },
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": ""
        },
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": "MAE-DFER [50]"
        },
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": ""
        },
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": "HiCMAE-T"
        },
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": ""
        },
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": "HiCMAE-S"
        },
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": ""
        },
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": "HiCMAE-B"
        },
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": "AV-LSTM [100]"
        },
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": "AV-Gating [100]"
        },
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": "MCBP [107]"
        },
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": ""
        },
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": "MMTM [108]"
        },
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": "MSAF [95]"
        },
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": ""
        },
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": "ERANNs [109]"
        },
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": "CFN-SR [96]"
        },
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": "MATER [104]"
        },
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": ""
        },
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": "MulT [51]"
        },
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": "AVT [110]"
        },
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": "VQ-MAE-AV+"
        },
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": ""
        },
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": "Attn. Pooling [20]"
        },
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": ""
        },
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": "VQ-MAE-AV+"
        },
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": ""
        },
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": "Query2Emo [20]"
        },
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": ""
        },
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": "HiCMAE-T"
        },
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": "HiCMAE-S"
        },
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": "HiCMAE-B"
        },
        {
          "Table 9: Comparison with state-of-the-art methods on RAVDESS. SSL: self-": ""
        }
      ],
      "page": 12
    },
    {
      "caption": "Table 11: Comparison with state-of-the-art methods on Werewolf-XL. SSL:",
      "data": [
        {
          "4.4.1. Datasets": ""
        },
        {
          "4.4.1. Datasets": ""
        },
        {
          "4.4.1. Datasets": "spontaneous"
        },
        {
          "4.4.1. Datasets": "in Werewolf games."
        },
        {
          "4.4.1. Datasets": ""
        },
        {
          "4.4.1. Datasets": ""
        },
        {
          "4.4.1. Datasets": ""
        },
        {
          "4.4.1. Datasets": ""
        },
        {
          "4.4.1. Datasets": ""
        },
        {
          "4.4.1. Datasets": ""
        },
        {
          "4.4.1. Datasets": ""
        },
        {
          "4.4.1. Datasets": ""
        },
        {
          "4.4.1. Datasets": ""
        },
        {
          "4.4.1. Datasets": ""
        },
        {
          "4.4.1. Datasets": ""
        },
        {
          "4.4.1. Datasets": ""
        },
        {
          "4.4.1. Datasets": ""
        },
        {
          "4.4.1. Datasets": ""
        },
        {
          "4.4.1. Datasets": ""
        },
        {
          "4.4.1. Datasets": ""
        },
        {
          "4.4.1. Datasets": ""
        },
        {
          "4.4.1. Datasets": ""
        },
        {
          "4.4.1. Datasets": ""
        },
        {
          "4.4.1. Datasets": ""
        },
        {
          "4.4.1. Datasets": ""
        },
        {
          "4.4.1. Datasets": ""
        },
        {
          "4.4.1. Datasets": ""
        },
        {
          "4.4.1. Datasets": ""
        },
        {
          "4.4.1. Datasets": ""
        },
        {
          "4.4.1. Datasets": ""
        },
        {
          "4.4.1. Datasets": ""
        },
        {
          "4.4.1. Datasets": ""
        },
        {
          "4.4.1. Datasets": ""
        },
        {
          "4.4.1. Datasets": ""
        },
        {
          "4.4.1. Datasets": "4.4.2. Results"
        },
        {
          "4.4.1. Datasets": ""
        },
        {
          "4.4.1. Datasets": "In Table"
        },
        {
          "4.4.1. Datasets": ""
        },
        {
          "4.4.1. Datasets": "methods"
        },
        {
          "4.4.1. Datasets": ""
        },
        {
          "4.4.1. Datasets": ""
        },
        {
          "4.4.1. Datasets": ""
        },
        {
          "4.4.1. Datasets": ""
        },
        {
          "4.4.1. Datasets": ""
        },
        {
          "4.4.1. Datasets": ""
        },
        {
          "4.4.1. Datasets": ""
        }
      ],
      "page": 12
    },
    {
      "caption": "Table 13: Ablation study on three-pronged strategy for hierarchical feature 10 2 42.65 56.17 84.91 84.89",
      "data": [
        {
          "Table 12: Comparison with state-of-the-art methods on AVCAffe. SSL: self-": "supervised learning method or not. The evaluation metric for arousal and va-",
          "Table 14: Ablation study on loss weight for self-supervised pre-training.": ""
        },
        {
          "Table 12: Comparison with state-of-the-art methods on AVCAffe. SSL: self-": "lence is weighted F1-score.",
          "Table 14: Ablation study on loss weight for self-supervised pre-training.": ""
        },
        {
          "Table 12: Comparison with state-of-the-art methods on AVCAffe. SSL: self-": "",
          "Table 14: Ablation study on loss weight for self-supervised pre-training.": ""
        },
        {
          "Table 12: Comparison with state-of-the-art methods on AVCAffe. SSL: self-": "",
          "Table 14: Ablation study on loss weight for self-supervised pre-training.": "Loss weight λ"
        },
        {
          "Table 12: Comparison with state-of-the-art methods on AVCAffe. SSL: self-": "",
          "Table 14: Ablation study on loss weight for self-supervised pre-training.": ""
        },
        {
          "Table 12: Comparison with state-of-the-art methods on AVCAffe. SSL: self-": "",
          "Table 14: Ablation study on loss weight for self-supervised pre-training.": ""
        },
        {
          "Table 12: Comparison with state-of-the-art methods on AVCAffe. SSL: self-": "Method",
          "Table 14: Ablation study on loss weight for self-supervised pre-training.": ""
        },
        {
          "Table 12: Comparison with state-of-the-art methods on AVCAffe. SSL: self-": "",
          "Table 14: Ablation study on loss weight for self-supervised pre-training.": "0"
        },
        {
          "Table 12: Comparison with state-of-the-art methods on AVCAffe. SSL: self-": "",
          "Table 14: Ablation study on loss weight for self-supervised pre-training.": "0.001"
        },
        {
          "Table 12: Comparison with state-of-the-art methods on AVCAffe. SSL: self-": "VGG-16+",
          "Table 14: Ablation study on loss weight for self-supervised pre-training.": ""
        },
        {
          "Table 12: Comparison with state-of-the-art methods on AVCAffe. SSL: self-": "",
          "Table 14: Ablation study on loss weight for self-supervised pre-training.": ""
        },
        {
          "Table 12: Comparison with state-of-the-art methods on AVCAffe. SSL: self-": "MC3-18 [24]",
          "Table 14: Ablation study on loss weight for self-supervised pre-training.": "0.0025"
        },
        {
          "Table 12: Comparison with state-of-the-art methods on AVCAffe. SSL: self-": "VGG-16+",
          "Table 14: Ablation study on loss weight for self-supervised pre-training.": "0.005"
        },
        {
          "Table 12: Comparison with state-of-the-art methods on AVCAffe. SSL: self-": "",
          "Table 14: Ablation study on loss weight for self-supervised pre-training.": ""
        },
        {
          "Table 12: Comparison with state-of-the-art methods on AVCAffe. SSL: self-": "3D ResNet-18 [24]",
          "Table 14: Ablation study on loss weight for self-supervised pre-training.": "0.01"
        },
        {
          "Table 12: Comparison with state-of-the-art methods on AVCAffe. SSL: self-": "VGG-16+",
          "Table 14: Ablation study on loss weight for self-supervised pre-training.": "0.1"
        },
        {
          "Table 12: Comparison with state-of-the-art methods on AVCAffe. SSL: self-": "",
          "Table 14: Ablation study on loss weight for self-supervised pre-training.": ""
        },
        {
          "Table 12: Comparison with state-of-the-art methods on AVCAffe. SSL: self-": "R(2+1)D-18 [24]",
          "Table 14: Ablation study on loss weight for self-supervised pre-training.": ""
        },
        {
          "Table 12: Comparison with state-of-the-art methods on AVCAffe. SSL: self-": "ResNet-18+",
          "Table 14: Ablation study on loss weight for self-supervised pre-training.": ""
        },
        {
          "Table 12: Comparison with state-of-the-art methods on AVCAffe. SSL: self-": "",
          "Table 14: Ablation study on loss weight for self-supervised pre-training.": ""
        },
        {
          "Table 12: Comparison with state-of-the-art methods on AVCAffe. SSL: self-": "MC3-18 [24]",
          "Table 14: Ablation study on loss weight for self-supervised pre-training.": "Table 15: Ablation study on layers in different encoders. Ns: number of layers"
        },
        {
          "Table 12: Comparison with state-of-the-art methods on AVCAffe. SSL: self-": "ResNet-18+",
          "Table 14: Ablation study on loss weight for self-supervised pre-training.": "in modality-specific encoders."
        },
        {
          "Table 12: Comparison with state-of-the-art methods on AVCAffe. SSL: self-": "",
          "Table 14: Ablation study on loss weight for self-supervised pre-training.": ""
        },
        {
          "Table 12: Comparison with state-of-the-art methods on AVCAffe. SSL: self-": "3D ResNet-18 [24]",
          "Table 14: Ablation study on loss weight for self-supervised pre-training.": ""
        },
        {
          "Table 12: Comparison with state-of-the-art methods on AVCAffe. SSL: self-": "ResNet-18+",
          "Table 14: Ablation study on loss weight for self-supervised pre-training.": ""
        },
        {
          "Table 12: Comparison with state-of-the-art methods on AVCAffe. SSL: self-": "",
          "Table 14: Ablation study on loss weight for self-supervised pre-training.": ""
        },
        {
          "Table 12: Comparison with state-of-the-art methods on AVCAffe. SSL: self-": "R(2+1)D-18 [24]",
          "Table 14: Ablation study on loss weight for self-supervised pre-training.": ""
        },
        {
          "Table 12: Comparison with state-of-the-art methods on AVCAffe. SSL: self-": "",
          "Table 14: Ablation study on loss weight for self-supervised pre-training.": ""
        },
        {
          "Table 12: Comparison with state-of-the-art methods on AVCAffe. SSL: self-": "",
          "Table 14: Ablation study on loss weight for self-supervised pre-training.": ""
        },
        {
          "Table 12: Comparison with state-of-the-art methods on AVCAffe. SSL: self-": "HiCMAE-T",
          "Table 14: Ablation study on loss weight for self-supervised pre-training.": "Ns\nN f"
        },
        {
          "Table 12: Comparison with state-of-the-art methods on AVCAffe. SSL: self-": "",
          "Table 14: Ablation study on loss weight for self-supervised pre-training.": ""
        },
        {
          "Table 12: Comparison with state-of-the-art methods on AVCAffe. SSL: self-": "HiCMAE-S",
          "Table 14: Ablation study on loss weight for self-supervised pre-training.": ""
        },
        {
          "Table 12: Comparison with state-of-the-art methods on AVCAffe. SSL: self-": "HiCMAE-B",
          "Table 14: Ablation study on loss weight for self-supervised pre-training.": ""
        },
        {
          "Table 12: Comparison with state-of-the-art methods on AVCAffe. SSL: self-": "",
          "Table 14: Ablation study on loss weight for self-supervised pre-training.": "12\n0"
        },
        {
          "Table 12: Comparison with state-of-the-art methods on AVCAffe. SSL: self-": "",
          "Table 14: Ablation study on loss weight for self-supervised pre-training.": "11\n1"
        },
        {
          "Table 12: Comparison with state-of-the-art methods on AVCAffe. SSL: self-": "Table 13: Ablation study on three-pronged strategy for hierarchical",
          "Table 14: Ablation study on loss weight for self-supervised pre-training.": "10\n2"
        },
        {
          "Table 12: Comparison with state-of-the-art methods on AVCAffe. SSL: self-": "learning.",
          "Table 14: Ablation study on loss weight for self-supervised pre-training.": ""
        },
        {
          "Table 12: Comparison with state-of-the-art methods on AVCAffe. SSL: self-": "",
          "Table 14: Ablation study on loss weight for self-supervised pre-training.": "8\n4"
        },
        {
          "Table 12: Comparison with state-of-the-art methods on AVCAffe. SSL: self-": "modal contrastive learning. HFF: hierarchical feature fusion.",
          "Table 14: Ablation study on loss weight for self-supervised pre-training.": ""
        }
      ],
      "page": 13
    },
    {
      "caption": "Table 13: Ablation study on three-pronged strategy for hierarchical feature 10 2 42.65 56.17 84.91 84.89",
      "data": [
        {
          "HSP: hierarchical": "",
          "skip connections.": "",
          "HCMCL: hierarchical cross-": ""
        },
        {
          "HSP: hierarchical": "",
          "skip connections.": "",
          "HCMCL: hierarchical cross-": ""
        },
        {
          "HSP: hierarchical": "",
          "skip connections.": "",
          "HCMCL: hierarchical cross-": "CREMA-D"
        },
        {
          "HSP: hierarchical": "HCMCL",
          "skip connections.": "",
          "HCMCL: hierarchical cross-": ""
        },
        {
          "HSP: hierarchical": "",
          "skip connections.": "UAR",
          "HCMCL: hierarchical cross-": "UAR"
        },
        {
          "HSP: hierarchical": "",
          "skip connections.": "",
          "HCMCL: hierarchical cross-": ""
        },
        {
          "HSP: hierarchical": "×",
          "skip connections.": "41.06",
          "HCMCL: hierarchical cross-": "83.76"
        },
        {
          "HSP: hierarchical": "",
          "skip connections.": "",
          "HCMCL: hierarchical cross-": ""
        },
        {
          "HSP: hierarchical": "×",
          "skip connections.": "41.82",
          "HCMCL: hierarchical cross-": "84.30"
        },
        {
          "HSP: hierarchical": "",
          "skip connections.": "",
          "HCMCL: hierarchical cross-": ""
        },
        {
          "HSP: hierarchical": "✓",
          "skip connections.": "42.48",
          "HCMCL: hierarchical cross-": "84.73"
        },
        {
          "HSP: hierarchical": "✓",
          "skip connections.": "",
          "HCMCL: hierarchical cross-": ""
        },
        {
          "HSP: hierarchical": "",
          "skip connections.": "42.65",
          "HCMCL: hierarchical cross-": "84.91"
        },
        {
          "HSP: hierarchical": "",
          "skip connections.": "",
          "HCMCL: hierarchical cross-": ""
        }
      ],
      "page": 13
    },
    {
      "caption": "Table 15: We first find that the model achieves the",
      "data": [
        {
          "MAFW": ""
        },
        {
          "MAFW": "56.06% 56.17%"
        },
        {
          "MAFW": "55.87%"
        },
        {
          "MAFW": ""
        },
        {
          "MAFW": "55.46%"
        },
        {
          "MAFW": ""
        },
        {
          "MAFW": ""
        },
        {
          "MAFW": ""
        },
        {
          "MAFW": ""
        },
        {
          "MAFW": ""
        },
        {
          "MAFW": ""
        },
        {
          "MAFW": ""
        },
        {
          "MAFW": "40\n60\n80\n100"
        },
        {
          "MAFW": "Pre-training Epochs"
        },
        {
          "MAFW": ""
        },
        {
          "MAFW": "CREMA-D"
        },
        {
          "MAFW": ""
        },
        {
          "MAFW": "84.76% 84.89%"
        },
        {
          "MAFW": "84.62%"
        },
        {
          "MAFW": "84.34%"
        },
        {
          "MAFW": ""
        },
        {
          "MAFW": ""
        },
        {
          "MAFW": ""
        },
        {
          "MAFW": ""
        },
        {
          "MAFW": ""
        },
        {
          "MAFW": "40\n60\n80\n100"
        },
        {
          "MAFW": "Pre-training Epochs"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table 15: We first find that the model achieves the",
      "data": [
        {
          "audio-first\n42.44\n56.00\n84.60\n84.57": "",
          "CREMA-D": "86"
        },
        {
          "audio-first\n42.44\n56.00\n84.60\n84.57": "",
          "CREMA-D": "84.91%\n84.89%\n85"
        },
        {
          "audio-first\n42.44\n56.00\n84.60\n84.57": "",
          "CREMA-D": "84.38%"
        },
        {
          "audio-first\n42.44\n56.00\n84.60\n84.57": "",
          "CREMA-D": "WAR"
        },
        {
          "audio-first\n42.44\n56.00\n84.60\n84.57": "coder (N f ) by keeping their sum fixed. The ablation results are",
          "CREMA-D": "84\n83.74%"
        },
        {
          "audio-first\n42.44\n56.00\n84.60\n84.57": "shown in Table 15. We first find that\nthe model achieves the",
          "CREMA-D": ""
        },
        {
          "audio-first\n42.44\n56.00\n84.60\n84.57": "",
          "CREMA-D": "83"
        },
        {
          "audio-first\n42.44\n56.00\n84.60\n84.57": "worst performance when the cross-modal fusion encoder is re-",
          "CREMA-D": ""
        },
        {
          "audio-first\n42.44\n56.00\n84.60\n84.57": "moved (i.e., N f = 0). This observation demonstrates the crucial",
          "CREMA-D": "82\nBase\nTiny\nSmall\nBase"
        },
        {
          "audio-first\n42.44\n56.00\n84.60\n84.57": "",
          "CREMA-D": "Model Scales"
        },
        {
          "audio-first\n42.44\n56.00\n84.60\n84.57": "role of the cross-modal fusion encoder in integrating heteroge-",
          "CREMA-D": ""
        },
        {
          "audio-first\n42.44\n56.00\n84.60\n84.57": "neous audio-visual information. Besides, only one fusion layer",
          "CREMA-D": ""
        },
        {
          "audio-first\n42.44\n56.00\n84.60\n84.57": "",
          "CREMA-D": "Figure 7: Ablation study on model scales."
        },
        {
          "audio-first\n42.44\n56.00\n84.60\n84.57": "is also beneficial\nto improve the result.\nFinally, maintaining",
          "CREMA-D": ""
        },
        {
          "audio-first\n42.44\n56.00\n84.60\n84.57": "enough layers in modality-specific encoders is also necessary",
          "CREMA-D": ""
        },
        {
          "audio-first\n42.44\n56.00\n84.60\n84.57": "since too small Ns will also hurt model performance.",
          "CREMA-D": ""
        },
        {
          "audio-first\n42.44\n56.00\n84.60\n84.57": "",
          "CREMA-D": ""
        },
        {
          "audio-first\n42.44\n56.00\n84.60\n84.57": "",
          "CREMA-D": "In this part, we explore the effect of pre-training epochs on"
        },
        {
          "audio-first\n42.44\n56.00\n84.60\n84.57": "4.5.4.\nInformation Flow in Cross-modal Fusion Encoder",
          "CREMA-D": "6. As"
        },
        {
          "audio-first\n42.44\n56.00\n84.60\n84.57": "",
          "CREMA-D": "can be seen, we find that longer pre-training epochs lead to im-"
        },
        {
          "audio-first\n42.44\n56.00\n84.60\n84.57": "We then investigate the effect of different\ntypes of informa-",
          "CREMA-D": ""
        },
        {
          "audio-first\n42.44\n56.00\n84.60\n84.57": "",
          "CREMA-D": "proved fine-tuning performance, which is consistent with our"
        },
        {
          "audio-first\n42.44\n56.00\n84.60\n84.57": "tion flow in the cross-modal fusion encoder. We develop three",
          "CREMA-D": ""
        },
        {
          "audio-first\n42.44\n56.00\n84.60\n84.57": "",
          "CREMA-D": "the model performance begins to satu-"
        },
        {
          "audio-first\n42.44\n56.00\n84.60\n84.57": "variants of the default\ninformation flow in Eq.\n(5-6) and show",
          "CREMA-D": ""
        },
        {
          "audio-first\n42.44\n56.00\n84.60\n84.57": "",
          "CREMA-D": "rate around 80 epochs. Due to limited computation resources"
        },
        {
          "audio-first\n42.44\n56.00\n84.60\n84.57": "their differences in Fig. 5. Specifically, for the raw-input vari-",
          "CREMA-D": ""
        },
        {
          "audio-first\n42.44\n56.00\n84.60\n84.57": "",
          "CREMA-D": "and expensive time costs, we stop pre-training at 100 epochs."
        },
        {
          "audio-first\n42.44\n56.00\n84.60\n84.57": "ant,\ntokens of one modality in each fusion layer always attend",
          "CREMA-D": ""
        },
        {
          "audio-first\n42.44\n56.00\n84.60\n84.57": "",
          "CREMA-D": "Nevertheless, we believe that continual pre-training will bring"
        },
        {
          "audio-first\n42.44\n56.00\n84.60\n84.57": "to the raw input\ntokens of\nthe other modality [51],\ninstead of",
          "CREMA-D": ""
        },
        {
          "audio-first\n42.44\n56.00\n84.60\n84.57": "",
          "CREMA-D": "further performance gains and we encourage other researchers"
        },
        {
          "audio-first\n42.44\n56.00\n84.60\n84.57": "updated tokens from the last\nlayer. For the video-first variant,",
          "CREMA-D": ""
        },
        {
          "audio-first\n42.44\n56.00\n84.60\n84.57": "",
          "CREMA-D": "to industry-level\ncomputation resources\nto"
        },
        {
          "audio-first\n42.44\n56.00\n84.60\n84.57": "video tokens first update themselves via audio information from",
          "CREMA-D": ""
        },
        {
          "audio-first\n42.44\n56.00\n84.60\n84.57": "",
          "CREMA-D": ""
        },
        {
          "audio-first\n42.44\n56.00\n84.60\n84.57": "the last fusion layer and then audio tokens attend to the updated",
          "CREMA-D": ""
        },
        {
          "audio-first\n42.44\n56.00\n84.60\n84.57": "video tokens. The audio-first variant\nis just\nthe reverse of the",
          "CREMA-D": ""
        },
        {
          "audio-first\n42.44\n56.00\n84.60\n84.57": "video-first variant. The ablation results are presented in Table",
          "CREMA-D": ""
        },
        {
          "audio-first\n42.44\n56.00\n84.60\n84.57": "16. We observe that\nthe model performance is not sensitive to",
          "CREMA-D": "Finally, we show the impact of model scales on downstream"
        },
        {
          "audio-first\n42.44\n56.00\n84.60\n84.57": "different types of information flow in the cross-modal fusion en-",
          "CREMA-D": "the results in Fig.\n7. Although these"
        },
        {
          "audio-first\n42.44\n56.00\n84.60\n84.57": "coder. Besides,\nin general,\nthe default\ninformation flow works",
          "CREMA-D": "numbers are shown in previous tables, we want to offer a more"
        },
        {
          "audio-first\n42.44\n56.00\n84.60\n84.57": "best,\nfollowed by the video-first and audio-first variants, and",
          "CREMA-D": "clear and intuitive understanding. As shown in Fig. 7, we ob-"
        },
        {
          "audio-first\n42.44\n56.00\n84.60\n84.57": "finally the raw-input variant.",
          "CREMA-D": "typically beats the smaller one in"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ANG": "DIS"
        },
        {
          "ANG": "FEA"
        },
        {
          "ANG": "HAP"
        },
        {
          "ANG": "NEU"
        },
        {
          "ANG": "SAD"
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "80": ""
        },
        {
          "80": ""
        },
        {
          "80": ""
        },
        {
          "80": "70"
        },
        {
          "80": ""
        },
        {
          "80": ""
        },
        {
          "80": "60"
        },
        {
          "80": ""
        },
        {
          "80": ""
        },
        {
          "80": "50"
        },
        {
          "80": ""
        },
        {
          "80": "40"
        },
        {
          "80": ""
        },
        {
          "80": ""
        },
        {
          "80": "30"
        },
        {
          "80": ""
        },
        {
          "80": ""
        },
        {
          "80": "20"
        },
        {
          "80": ""
        },
        {
          "80": "10"
        },
        {
          "80": ""
        },
        {
          "80": ""
        },
        {
          "80": "0"
        },
        {
          "80": ""
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "HL\n3.44": "",
          "8.40": "",
          "0.76": "",
          "9.92 20.99 12.98 5.73": "",
          "2.29 18.70 8.02": "",
          "8.78": "",
          "10": "",
          "HL\n0.00": "",
          "0.00": "",
          "5.66 39.62 7.55": "",
          "9.43": "",
          "3.77 22.64 7.55": "",
          "3.77": "",
          "HL\n5.66": "",
          "1.89 16.98 39.62 13.21 1.89": "",
          "1.89": "",
          "5.66": ""
        },
        {
          "HL\n3.44": "DS\n3.85",
          "8.40": "6.59",
          "0.76": "0.00",
          "9.92 20.99 12.98 5.73": "3.30 17.03 18.13 9.34",
          "2.29 18.70 8.02": "2.75 15.93 9.34 13.74",
          "8.78": "",
          "10": "",
          "HL\n0.00": "DS\n0.00",
          "0.00": "5.41",
          "5.66 39.62 7.55": "2.70 51.35 13.51 0.00",
          "9.43": "5.41 13.51 13.51",
          "3.77 22.64 7.55": "5.41",
          "3.77": "",
          "HL\n5.66": "DS\n8.11",
          "1.89 16.98 39.62 13.21 1.89": "",
          "1.89": "2.70",
          "5.66": ""
        },
        {
          "HL\n3.44": "",
          "8.40": "",
          "0.76": "",
          "9.92 20.99 12.98 5.73": "",
          "2.29 18.70 8.02": "",
          "8.78": "",
          "10": "0",
          "HL\n0.00": "",
          "0.00": "",
          "5.66 39.62 7.55": "",
          "9.43": "",
          "3.77 22.64 7.55": "",
          "3.77": "",
          "HL\n5.66": "",
          "1.89 16.98 39.62 13.21 1.89": "",
          "1.89": "",
          "5.66": ""
        },
        {
          "HL\n3.44": "AN",
          "8.40": "DI",
          "0.76": "FE",
          "9.92 20.99 12.98 5.73": "NE",
          "2.29 18.70 8.02": "AX",
          "8.78": "DS",
          "10": "",
          "HL\n0.00": "AN",
          "0.00": "DI",
          "5.66 39.62 7.55": "NE",
          "9.43": "HL",
          "3.77 22.64 7.55": "AX",
          "3.77": "DS",
          "HL\n5.66": "AN",
          "1.89 16.98 39.62 13.21 1.89": "HA",
          "1.89": "SA\nSU\nCO",
          "5.66": "AX"
        },
        {
          "HL\n3.44": "",
          "8.40": "",
          "0.76": "",
          "9.92 20.99 12.98 5.73": "",
          "2.29 18.70 8.02": "",
          "8.78": "",
          "10": "",
          "HL\n0.00": "",
          "0.00": "",
          "5.66 39.62 7.55": "",
          "9.43": "",
          "3.77 22.64 7.55": "",
          "3.77": "",
          "HL\n5.66": "",
          "1.89 16.98 39.62 13.21 1.89": "",
          "1.89": "(c) Fold 2",
          "5.66": ""
        },
        {
          "HL\n3.44": "AN",
          "8.40": "66.91 6.83",
          "0.76": "1.44",
          "9.92 20.99 12.98 5.73": "1.80",
          "2.29 18.70 8.02": "2.16\n6.47",
          "8.78": "0.72",
          "10": "",
          "HL\n0.00": "AN",
          "0.00": "",
          "5.66 39.62 7.55": "1.08",
          "9.43": "",
          "3.77 22.64 7.55": "0.00\n2.88",
          "3.77": "",
          "HL\n5.66": "",
          "1.89 16.98 39.62 13.21 1.89": "",
          "1.89": "",
          "5.66": ""
        },
        {
          "HL\n3.44": "",
          "8.40": "",
          "0.76": "",
          "9.92 20.99 12.98 5.73": "",
          "2.29 18.70 8.02": "",
          "8.78": "",
          "10": "",
          "HL\n0.00": "",
          "0.00": "",
          "5.66 39.62 7.55": "",
          "9.43": "",
          "3.77 22.64 7.55": "",
          "3.77": "",
          "HL\n5.66": "",
          "1.89 16.98 39.62 13.21 1.89": "",
          "1.89": "",
          "5.66": ""
        },
        {
          "HL\n3.44": "",
          "8.40": "",
          "0.76": "",
          "9.92 20.99 12.98 5.73": "",
          "2.29 18.70 8.02": "",
          "8.78": "",
          "10": "",
          "HL\n0.00": "",
          "0.00": "75.18 4.32",
          "5.66 39.62 7.55": "",
          "9.43": "0.72",
          "3.77 22.64 7.55": "",
          "3.77": "0.36",
          "HL\n5.66": "AN",
          "1.89 16.98 39.62 13.21 1.89": "0.36",
          "1.89": "5.76\n3.96\n0.00",
          "5.66": "5.04"
        },
        {
          "HL\n3.44": "DI",
          "8.40": "17.97 38.28 1.56",
          "0.76": "",
          "9.92 20.99 12.98 5.73": "3.91",
          "2.29 18.70 8.02": "8.59 14.84 1.56",
          "8.78": "2.34",
          "10": "",
          "HL\n0.00": "DI",
          "0.00": "",
          "5.66 39.62 7.55": "",
          "9.43": "",
          "3.77 22.64 7.55": "0.78 14.84 0.00",
          "3.77": "",
          "HL\n5.66": "",
          "1.89 16.98 39.62 13.21 1.89": "",
          "1.89": "",
          "5.66": ""
        },
        {
          "HL\n3.44": "",
          "8.40": "",
          "0.76": "",
          "9.92 20.99 12.98 5.73": "",
          "2.29 18.70 8.02": "",
          "8.78": "",
          "10": "",
          "HL\n0.00": "",
          "0.00": "",
          "5.66 39.62 7.55": "",
          "9.43": "",
          "3.77 22.64 7.55": "",
          "3.77": "",
          "HL\n5.66": "",
          "1.89 16.98 39.62 13.21 1.89": "",
          "1.89": "",
          "5.66": ""
        },
        {
          "HL\n3.44": "",
          "8.40": "",
          "0.76": "",
          "9.92 20.99 12.98 5.73": "",
          "2.29 18.70 8.02": "",
          "8.78": "",
          "10": "",
          "HL\n0.00": "",
          "0.00": "18.90 30.71 2.36",
          "5.66 39.62 7.55": "",
          "9.43": "",
          "3.77 22.64 7.55": "",
          "3.77": "1.57",
          "HL\n5.66": "DI",
          "1.89 16.98 39.62 13.21 1.89": "4.72",
          "1.89": "4.72 10.24 10.24 1.57 14.17 0.79",
          "5.66": ""
        },
        {
          "HL\n3.44": "FE\n3.20",
          "8.40": "",
          "0.76": "0.00 53.60 0.00",
          "9.92 20.99 12.98 5.73": "",
          "2.29 18.70 8.02": "1.60",
          "8.78": "0.00",
          "10": "",
          "HL\n0.00": "FE",
          "0.00": "",
          "5.66 39.62 7.55": "",
          "9.43": "",
          "3.77 22.64 7.55": "4.80",
          "3.77": "",
          "HL\n5.66": "",
          "1.89 16.98 39.62 13.21 1.89": "",
          "1.89": "",
          "5.66": ""
        },
        {
          "HL\n3.44": "",
          "8.40": "",
          "0.76": "",
          "9.92 20.99 12.98 5.73": "",
          "2.29 18.70 8.02": "",
          "8.78": "",
          "10": "",
          "HL\n0.00": "",
          "0.00": "",
          "5.66 39.62 7.55": "",
          "9.43": "0.80",
          "3.77 22.64 7.55": "",
          "3.77": "0.00",
          "HL\n5.66": "FE\n4.00",
          "1.89 16.98 39.62 13.21 1.89": "",
          "1.89": "0.80 20.00 26.40 0.00",
          "5.66": "4.00"
        },
        {
          "HL\n3.44": "HA\n0.83",
          "8.40": "2.48",
          "0.76": "",
          "9.92 20.99 12.98 5.73": "",
          "2.29 18.70 8.02": "1.24\n0.41",
          "8.78": "0.83",
          "10": "",
          "HL\n0.00": "HA",
          "0.00": "",
          "5.66 39.62 7.55": "",
          "9.43": "",
          "3.77 22.64 7.55": "0.00\n0.00",
          "3.77": "",
          "HL\n5.66": "",
          "1.89 16.98 39.62 13.21 1.89": "",
          "1.89": "",
          "5.66": ""
        },
        {
          "HL\n3.44": "",
          "8.40": "",
          "0.76": "",
          "9.92 20.99 12.98 5.73": "",
          "2.29 18.70 8.02": "",
          "8.78": "",
          "10": "",
          "HL\n0.00": "",
          "0.00": "0.81 10.08 0.00 66.53 2.42",
          "5.66 39.62 7.55": "",
          "9.43": "3.63",
          "3.77 22.64 7.55": "",
          "3.77": "0.00",
          "HL\n5.66": "HA",
          "1.89 16.98 39.62 13.21 1.89": "",
          "1.89": "2.02\n8.87\n4.44",
          "5.66": "1.21"
        },
        {
          "HL\n3.44": "NE\n7.02",
          "8.40": "6.58",
          "0.76": "0.88",
          "9.92 20.99 12.98 5.73": "4.82 56.14 9.21",
          "2.29 18.70 8.02": "1.32\n7.02",
          "8.78": "0.00",
          "10": "",
          "HL\n0.00": "NE",
          "0.00": "",
          "5.66 39.62 7.55": "8.44 70.67 4.00",
          "9.43": "",
          "3.77 22.64 7.55": "1.78\n1.78",
          "3.77": "",
          "HL\n5.66": "",
          "1.89 16.98 39.62 13.21 1.89": "",
          "1.89": "",
          "5.66": ""
        },
        {
          "HL\n3.44": "",
          "8.40": "",
          "0.76": "",
          "9.92 20.99 12.98 5.73": "",
          "2.29 18.70 8.02": "",
          "8.78": "",
          "10": "",
          "HL\n0.00": "",
          "0.00": "2.64 15.86 0.44 20.70 36.56 4.41",
          "5.66 39.62 7.55": "",
          "9.43": "5.73",
          "3.77 22.64 7.55": "",
          "3.77": "1.76",
          "HL\n5.66": "NE",
          "1.89 16.98 39.62 13.21 1.89": "",
          "1.89": "5.29\n1.76",
          "5.66": "4.85"
        },
        {
          "HL\n3.44": "SA\n3.40",
          "8.40": "1.02",
          "0.76": "1.70",
          "9.92 20.99 12.98 5.73": "",
          "2.29 18.70 8.02": "0.00\n5.78",
          "8.78": "0.00",
          "10": "",
          "HL\n0.00": "SA",
          "0.00": "",
          "5.66 39.62 7.55": "",
          "9.43": "",
          "3.77 22.64 7.55": "0.00\n2.39",
          "3.77": "",
          "HL\n5.66": "",
          "1.89 16.98 39.62 13.21 1.89": "",
          "1.89": "",
          "5.66": ""
        },
        {
          "HL\n3.44": "",
          "8.40": "",
          "0.76": "",
          "9.92 20.99 12.98 5.73": "",
          "2.29 18.70 8.02": "",
          "8.78": "",
          "10": "",
          "HL\n0.00": "",
          "0.00": "1.02",
          "5.66 39.62 7.55": "",
          "9.43": "0.00",
          "3.77 22.64 7.55": "",
          "3.77": "0.00",
          "HL\n5.66": "SA\n2.72",
          "1.89 16.98 39.62 13.21 1.89": "0.68",
          "1.89": "0.00",
          "5.66": "6.12"
        },
        {
          "HL\n3.44": "",
          "8.40": "",
          "0.76": "",
          "9.92 20.99 12.98 5.73": "",
          "2.29 18.70 8.02": "",
          "8.78": "",
          "10": "",
          "HL\n0.00": "",
          "0.00": "",
          "5.66 39.62 7.55": "",
          "9.43": "",
          "3.77 22.64 7.55": "",
          "3.77": "",
          "HL\n5.66": "",
          "1.89 16.98 39.62 13.21 1.89": "",
          "1.89": "",
          "5.66": ""
        },
        {
          "HL\n3.44": "SU\n4.21",
          "8.40": "6.07",
          "0.76": "3.74",
          "9.92 20.99 12.98 5.73": "0.93",
          "2.29 18.70 8.02": "4.21",
          "8.78": "0.47",
          "10": "",
          "HL\n0.00": "SU",
          "0.00": "",
          "5.66 39.62 7.55": "0.93",
          "9.43": "",
          "3.77 22.64 7.55": "1.87",
          "3.77": "",
          "HL\n5.66": "",
          "1.89 16.98 39.62 13.21 1.89": "",
          "1.89": "",
          "5.66": ""
        },
        {
          "HL\n3.44": "",
          "8.40": "",
          "0.76": "",
          "9.92 20.99 12.98 5.73": "",
          "2.29 18.70 8.02": "",
          "8.78": "",
          "10": "",
          "HL\n0.00": "",
          "0.00": "",
          "5.66 39.62 7.55": "",
          "9.43": "0.00",
          "3.77 22.64 7.55": "",
          "3.77": "0.00",
          "HL\n5.66": "SU\n5.61",
          "1.89 16.98 39.62 13.21 1.89": "",
          "1.89": "3.27 67.76 0.47",
          "5.66": "5.61"
        },
        {
          "HL\n3.44": "",
          "8.40": "",
          "0.76": "",
          "9.92 20.99 12.98 5.73": "",
          "2.29 18.70 8.02": "",
          "8.78": "",
          "10": "",
          "HL\n0.00": "",
          "0.00": "",
          "5.66 39.62 7.55": "",
          "9.43": "",
          "3.77 22.64 7.55": "",
          "3.77": "",
          "HL\n5.66": "",
          "1.89 16.98 39.62 13.21 1.89": "",
          "1.89": "",
          "5.66": ""
        },
        {
          "HL\n3.44": "CO",
          "8.40": "8.51 14.89 0.00 34.04 29.79 2.13",
          "0.76": "",
          "9.92 20.99 12.98 5.73": "",
          "2.29 18.70 8.02": "2.13\n4.26",
          "8.78": "2.13",
          "10": "",
          "HL\n0.00": "CO",
          "0.00": "",
          "5.66 39.62 7.55": "",
          "9.43": "",
          "3.77 22.64 7.55": "",
          "3.77": "",
          "HL\n5.66": "",
          "1.89 16.98 39.62 13.21 1.89": "",
          "1.89": "",
          "5.66": ""
        },
        {
          "HL\n3.44": "",
          "8.40": "",
          "0.76": "",
          "9.92 20.99 12.98 5.73": "",
          "2.29 18.70 8.02": "",
          "8.78": "",
          "10": "",
          "HL\n0.00": "",
          "0.00": "10.64 23.40 2.13 10.64 10.64 8.51",
          "5.66 39.62 7.55": "",
          "9.43": "6.38 10.64",
          "3.77 22.64 7.55": "",
          "3.77": "",
          "HL\n5.66": "CO",
          "1.89 16.98 39.62 13.21 1.89": "",
          "1.89": "4.26\n4.26",
          "5.66": "8.51"
        },
        {
          "HL\n3.44": "AX",
          "8.40": "11.48 12.02 2.73",
          "0.76": "",
          "9.92 20.99 12.98 5.73": "",
          "2.29 18.70 8.02": "3.28 34.97 4.37",
          "8.78": "2.19",
          "10": "",
          "HL\n0.00": "AX",
          "0.00": "",
          "5.66 39.62 7.55": "",
          "9.43": "",
          "3.77 22.64 7.55": "",
          "3.77": "",
          "HL\n5.66": "",
          "1.89 16.98 39.62 13.21 1.89": "",
          "1.89": "",
          "5.66": ""
        },
        {
          "HL\n3.44": "",
          "8.40": "",
          "0.76": "",
          "9.92 20.99 12.98 5.73": "",
          "2.29 18.70 8.02": "",
          "8.78": "",
          "10": "",
          "HL\n0.00": "",
          "0.00": "4.37",
          "5.66 39.62 7.55": "",
          "9.43": "",
          "3.77 22.64 7.55": "",
          "3.77": "0.00",
          "HL\n5.66": "AX\n9.84",
          "1.89 16.98 39.62 13.21 1.89": "",
          "1.89": "",
          "5.66": ""
        },
        {
          "HL\n3.44": "HL",
          "8.40": "1.92 21.15 0.00 23.08 9.62",
          "0.76": "",
          "9.92 20.99 12.98 5.73": "",
          "2.29 18.70 8.02": "0.00 17.31 7.69",
          "8.78": "7.69",
          "10": "",
          "HL\n0.00": "HL",
          "0.00": "5.77",
          "5.66 39.62 7.55": "",
          "9.43": "",
          "3.77 22.64 7.55": "",
          "3.77": "",
          "HL\n5.66": "HL\n5.77",
          "1.89 16.98 39.62 13.21 1.89": "",
          "1.89": "",
          "5.66": "1.92 21.15 9.62 15.38"
        },
        {
          "HL\n3.44": "",
          "8.40": "",
          "0.76": "",
          "9.92 20.99 12.98 5.73": "",
          "2.29 18.70 8.02": "",
          "8.78": "",
          "10": "",
          "HL\n0.00": "",
          "0.00": "",
          "5.66 39.62 7.55": "",
          "9.43": "",
          "3.77 22.64 7.55": "",
          "3.77": "",
          "HL\n5.66": "",
          "1.89 16.98 39.62 13.21 1.89": "",
          "1.89": "",
          "5.66": ""
        },
        {
          "HL\n3.44": "DS",
          "8.40": "2.78 13.89 0.00",
          "0.76": "",
          "9.92 20.99 12.98 5.73": "",
          "2.29 18.70 8.02": "",
          "8.78": "",
          "10": "",
          "HL\n0.00": "DS",
          "0.00": "8.33",
          "5.66 39.62 7.55": "",
          "9.43": "",
          "3.77 22.64 7.55": "",
          "3.77": "",
          "HL\n5.66": "DS\n5.56",
          "1.89 16.98 39.62 13.21 1.89": "5.56",
          "1.89": "8.33 16.67 11.11 0.00 25.00 5.56 13.89",
          "5.66": ""
        },
        {
          "HL\n3.44": "",
          "8.40": "",
          "0.76": "",
          "9.92 20.99 12.98 5.73": "",
          "2.29 18.70 8.02": "",
          "8.78": "",
          "10": "",
          "HL\n0.00": "",
          "0.00": "",
          "5.66 39.62 7.55": "",
          "9.43": "",
          "3.77 22.64 7.55": "",
          "3.77": "",
          "HL\n5.66": "",
          "1.89 16.98 39.62 13.21 1.89": "",
          "1.89": "",
          "5.66": ""
        },
        {
          "HL\n3.44": "AN",
          "8.40": "DI",
          "0.76": "FE",
          "9.92 20.99 12.98 5.73": "NE",
          "2.29 18.70 8.02": "CO\nAX",
          "8.78": "DS",
          "10": "",
          "HL\n0.00": "AN",
          "0.00": "DI",
          "5.66 39.62 7.55": "NE",
          "9.43": "HL",
          "3.77 22.64 7.55": "AX",
          "3.77": "DS",
          "HL\n5.66": "AN",
          "1.89 16.98 39.62 13.21 1.89": "HA",
          "1.89": "SA\nSU\nCO",
          "5.66": "AX"
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(d) Fold 3\n(e) Fold 4\n(f) Fold 5": "Figure 10: Confusion matrices on MAFW (11-class). AN: anger. DI: disgust. FE: fear. HA: happiness. NE: neutral. SA: sadness. SU: surprise. CO: contempt."
        },
        {
          "(d) Fold 3\n(e) Fold 4\n(f) Fold 5": "AX: anxiety. HL: helplessness. DS: disappointment."
        },
        {
          "(d) Fold 3\n(e) Fold 4\n(f) Fold 5": "4.6.2. Embedding Space\ndownstream fine-tuning. Moreover, we notice that\nthe perfor-"
        },
        {
          "(d) Fold 3\n(e) Fold 4\n(f) Fold 5": "mance saturation is\nless pronounced than that\nshown in pre-\nIn this part, we utilize t-SNE [114] to visualize the learned"
        },
        {
          "(d) Fold 3\n(e) Fold 4\n(f) Fold 5": "training epochs (especially for MAFW), indicating that further\nfeature embedding space of HiCMAE on CREMA-D (6-class)."
        },
        {
          "(d) Fold 3\n(e) Fold 4\n(f) Fold 5": "scaling up the model size can yield better results. Thus, we also\nTo demonstrate the effect of audio-visual\nfusion, we present"
        },
        {
          "(d) Fold 3\n(e) Fold 4\n(f) Fold 5": "encourage other researchers to continue this exploration.\nboth unimodal and multimodal embedding space. The results"
        },
        {
          "(d) Fold 3\n(e) Fold 4\n(f) Fold 5": "are shown in Fig.\n9.\nEach row presents unimodal or audio-"
        },
        {
          "(d) Fold 3\n(e) Fold 4\n(f) Fold 5": "visual embedding space and each column denotes one of five\n4.6. Visualization Analysis"
        },
        {
          "(d) Fold 3\n(e) Fold 4\n(f) Fold 5": "folds on this dataset. From the figure, we find that both audio-"
        },
        {
          "(d) Fold 3\n(e) Fold 4\n(f) Fold 5": "4.6.1. Masked Audio-Visual Reconstruction"
        },
        {
          "(d) Fold 3\n(e) Fold 4\n(f) Fold 5": "only\nand\nvideo-only models\nhave\nlearned\ngood\nembedding"
        },
        {
          "(d) Fold 3\n(e) Fold 4\n(f) Fold 5": "We first show the reconstruction ability of HiCMAE under"
        },
        {
          "(d) Fold 3\n(e) Fold 4\n(f) Fold 5": "space for distinguishing different kinds of emotions. Moreover,"
        },
        {
          "(d) Fold 3\n(e) Fold 4\n(f) Fold 5": "different masking rates. We randomly select video clips of four"
        },
        {
          "(d) Fold 3\n(e) Fold 4\n(f) Fold 5": "the multimodal embedding space is more discriminative than"
        },
        {
          "(d) Fold 3\n(e) Fold 4\n(f) Fold 5": "celebrities from the unseen test set of VoxCeleb2 for evaluation."
        },
        {
          "(d) Fold 3\n(e) Fold 4\n(f) Fold 5": "the unimodal ones,\nas evidenced by its more compact\nintra-"
        },
        {
          "(d) Fold 3\n(e) Fold 4\n(f) Fold 5": "The results of masked audio-visual reconstruction are shown in"
        },
        {
          "(d) Fold 3\n(e) Fold 4\n(f) Fold 5": "class and more separated inter-class distributions.\nTherefore,"
        },
        {
          "(d) Fold 3\n(e) Fold 4\n(f) Fold 5": "Fig. 8. For each video clip, we display the original audio spec-"
        },
        {
          "(d) Fold 3\n(e) Fold 4\n(f) Fold 5": "this comparison result qualitatively verifies the effectiveness of"
        },
        {
          "(d) Fold 3\n(e) Fold 4\n(f) Fold 5": "trogram and 8 facial frame images in the first row,\nthe masked"
        },
        {
          "(d) Fold 3\n(e) Fold 4\n(f) Fold 5": "multimodal fusion for audio-visual emotion recognition."
        },
        {
          "(d) Fold 3\n(e) Fold 4\n(f) Fold 5": "input in medium level (60% for audio and 75% for video) along"
        },
        {
          "(d) Fold 3\n(e) Fold 4\n(f) Fold 5": "with the corresponding reconstructed data in the second and"
        },
        {
          "(d) Fold 3\n(e) Fold 4\n(f) Fold 5": "4.7. Error Analysis"
        },
        {
          "(d) Fold 3\n(e) Fold 4\n(f) Fold 5": "third row, and the highly masked input (80% for audio and 90%"
        },
        {
          "(d) Fold 3\n(e) Fold 4\n(f) Fold 5": "We present\nthe confusion matrices on MAFW (11-class) in\nfor video) along with the corresponding reconstructed data in"
        },
        {
          "(d) Fold 3\n(e) Fold 4\n(f) Fold 5": "Fig. 10. The aggregated confusion matrix across five folds is\nthe last\ntwo rows. From the figure, we observe that, although"
        },
        {
          "(d) Fold 3\n(e) Fold 4\n(f) Fold 5": "shown in Fig. 10a and the confusion matrix of each fold is\nsome fine-grained details are lost,\nthe global structures of\nthe"
        },
        {
          "(d) Fold 3\n(e) Fold 4\n(f) Fold 5": "shown in Fig. 10b-10f. As can be seen, although achieving sig-\naudio spectrogram (e.g., harmonics) and visual\nframes\n(e.g.,"
        },
        {
          "(d) Fold 3\n(e) Fold 4\n(f) Fold 5": "nificant\nimprovement over previous methods as stated in Sec-\nsmiles) are well recovered under both medium and high mask-"
        },
        {
          "(d) Fold 3\n(e) Fold 4\n(f) Fold 5": "ing rates. These satisfactory reconstruction results indicate that\ntion 4.2.2,\nthe performance of HiCMAE on several rare emo-"
        },
        {
          "(d) Fold 3\n(e) Fold 4\n(f) Fold 5": "tions (such as contempt, disappointment, and disgust) is not sat-\nHiCMAE is capable of fully exploiting limited intra-modal and"
        },
        {
          "(d) Fold 3\n(e) Fold 4\n(f) Fold 5": "isfactory. This is mainly attributed to the imbalanced emotion\ncross-modal contextual information to infer the missing audio-"
        },
        {
          "(d) Fold 3\n(e) Fold 4\n(f) Fold 5": "distribution in real-world scenarios. The imbalanced learning\nvisual data. We believe\nthis\ncapability learned during self-"
        },
        {
          "(d) Fold 3\n(e) Fold 4\n(f) Fold 5": "strategies (e.g., resampling techniques and cost-sensitive learn-\nsupervised pre-training has laid the foundation for its superior"
        },
        {
          "(d) Fold 3\n(e) Fold 4\n(f) Fold 5": "ing) can be utilized to address this problem and we leave it\nin\ndownstream fine-tuning performance."
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "future work. We also notice that, among all emotions, neu-": "tral emotion is the one most easily confused with other emo-",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "tions. We believe that\nthe reason is that\nthe boundary between",
          "References": "[1] N. Schwarz, et al., Emotion, cognition, and decision making, Cognition"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "neutral emotion and other emotions is typically less clear than",
          "References": "& emotion 14 (4) (2000) 433–440."
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "[2] M. Minsky, Society of mind, Simon and Schuster, 1988."
        },
        {
          "future work. We also notice that, among all emotions, neu-": "those between other emotion combinations (e.g., happiness and",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "[3] A. MEHRABIAN, Communication without words, Psychol. Today 2 (4)"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "anger). Besides, we observe that fear is often misclassified as",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "(1968) 53–56."
        },
        {
          "future work. We also notice that, among all emotions, neu-": "surprise. This is consistent with our expectation as their differ-",
          "References": "[4] C.-H. Wu, J.-C. Lin, W.-L. Wei, Survey on audiovisual emotion recog-"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "ence in terms of facial expressions is very subtle (both featuring",
          "References": "nition: databases, features, and data fusion strategies, APSIPA transac-"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "tions on signal and information processing 3 (2014) e12."
        },
        {
          "future work. We also notice that, among all emotions, neu-": "wide-open eyes and raised eyebrows).",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "[5] Y. Liu, W. Dai, C. Feng, W. Wang, G. Yin,\nJ. Zeng, S. Shan, Mafw:"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "A large-scale, multi-modal, compound affective database for dynamic"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "facial expression recognition in the wild,\nin: Proceedings of\nthe 30th"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "ACM International Conference on Multimedia, 2022, pp. 24–32."
        },
        {
          "future work. We also notice that, among all emotions, neu-": "5. Conclusion",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "[6]\nZ. Zeng, M. Pantic, G.\nI. Roisman, T. S. Huang, A survey of affect"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "recognition methods: Audio, visual, and spontaneous expressions, IEEE"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "Transactions on Pattern Analysis and Machine Intelligence 31 (1) (2008)"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "In this paper, we have presented a novel\nself-supervised",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "39–58."
        },
        {
          "future work. We also notice that, among all emotions, neu-": "framework (HiCMAE), as an early attempt\nto leverage large-",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "[7]\nS. Zhang, S. Zhang, T. Huang, W. Gao, Q. Tian, Learning affective"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "scale self-supervised pre-training to address the dilemma faced",
          "References": "features with a hybrid deep model\nfor audio–visual emotion recogni-"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "by current supervised methods and largely promote the devel-",
          "References": "tion, IEEE Transactions on Circuits and Systems for Video Technology"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "28 (10) (2017) 3030–3043."
        },
        {
          "future work. We also notice that, among all emotions, neu-": "opment of AVER. HiCMAE is built on top of\ntwo primary",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "[8]\nP. Tzirakis, G. Trigeorgis, M. A. Nicolaou, B. W. Schuller, S. Zafeiriou,"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "forms of self-supervision, namely masked data modeling and",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "End-to-end multimodal\nemotion\nrecognition\nusing\ndeep\nneural\nnet-"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "contrastive learning. Moreover, to facilitate hierarchical audio-",
          "References": "works, IEEE Journal of selected topics in signal processing 11 (8) (2017)"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "visual feature learning, it introduces a three-pronged approach,",
          "References": "1301–1309."
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "[9] X. Zhang, M. Li, S. Lin, H. Xu, G. Xiao, Transformer-based multimodal"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "including hierarchical\nskip connections between the encoder",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "emotional perception for dynamic facial expression recognition in the"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "and decoder, hierarchical cross-modal contrastive learning, and",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "wild, IEEE Transactions on Circuits and Systems for Video Technology"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "hierarchical\nfeature fusion for downstream fine-tuning. Com-",
          "References": "(2023)."
        },
        {
          "future work. We also notice that, among all emotions, neu-": "prehensive experiments across 9 AVER datasets covering both",
          "References": "[10]\nS. Zhang, Y. Yang, C. Chen, X. Zhang, Q. Leng, X. Zhao, Deep"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "learning-based multimodal emotion recognition from audio, visual, and"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "categorical and dimensional\ntasks demonstrate that HiCMAE",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "text modalities: A systematic review of recent advancements and future"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "outperforms\nstate-of-the-art audio-visual methods by signifi-",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "prospects, Expert Systems with Applications (2023) 121692."
        },
        {
          "future work. We also notice that, among all emotions, neu-": "cant margins,\nindicating that HiCMAE is a powerful audio-",
          "References": "[11] G. Pei, H. Li, Y. Lu, Y. Wang, S. Hua, T. Li, Affective computing: Recent"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "advances, challenges, and future trends, Intelligent Computing 3 (2024)"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "visual emotion representation learner. Extensive ablation stud-",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "0076."
        },
        {
          "future work. We also notice that, among all emotions, neu-": "ies and visualization analysis also verify the efficacy of HiC-",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "[12] R. Balestriero, M. Ibrahim, V. Sobal, A. Morcos, S. Shekhar, T. Gold-"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "MAE. We hope this work can provide some insight\ninto the",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "stein, F. Bordes, A. Bardes, G. Mialon, Y. Tian, et al., A cookbook of"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "development of AVER and inspire more relevant studies.",
          "References": "self-supervised learning, arXiv preprint arXiv:2304.12210 (2023)."
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "[13]\nJ. Devlin, M.-W. Chang, K. Lee, K. Toutanova, Bert:\nPre-training"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "It\nshould be noted that,\ndue\nto limited computational\nre-",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "of deep bidirectional\ntransformers\nfor\nlanguage understanding,\narXiv"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "sources, we cannot afford the pre-training of larger models with",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "preprint arXiv:1810.04805 (2018)."
        },
        {
          "future work. We also notice that, among all emotions, neu-": "more training time and data. Therefore, we plan to tackle these",
          "References": "[14] R. Mao, Q. Liu, K. He, W. Li, E. Cambria, The biases of pre-trained"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "issues in future work and also encourage other\nresearchers to",
          "References": "language models: An empirical study on prompt-based sentiment anal-"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "ysis and emotion detection, IEEE Transactions on Affective Computing"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "conduct\nfollow-up studies.\nBesides,\nalthough HiCMAE has",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "(2022)."
        },
        {
          "future work. We also notice that, among all emotions, neu-": "achieved exceptional performance on many datasets,\nthis pa-",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "[15] W. Li, L. Zhu, R. Mao, E. Cambria, Skier: A symbolic knowledge inte-"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "per has conducted limited exploration into its internal mecha-",
          "References": "grated model for conversational emotion recognition, in: Proceedings of"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "the AAAI Conference on Artificial Intelligence, 2023."
        },
        {
          "future work. We also notice that, among all emotions, neu-": "nisms. Therefore,\nit\nis worth investigating and enhancing the",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "[16] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "interpretability of HiCMAE in future work. Possible interpre-",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "G. Sastry, A. Askell, P. Mishkin, J. Clark, et al., Learning transferable"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "tation tools include attention visualization [73], relevancy map",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "visual models from natural language supervision,\nin:\nInternational con-"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "[115], and Grad-CAM [116].\nFinally,\nit\nis also interesting to",
          "References": "ference on machine learning, PMLR, 2021, pp. 8748–8763."
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "[17] K. He, X. Chen, S. Xie, Y. Li, P. Doll´ar, R. Girshick, Masked autoen-"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "apply HiCMAE to other audio-visual tasks (e.g., active speaker",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "coders are scalable vision learners,\nin: Proceedings of\nthe IEEE/CVF"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "detection, deepfake detection, and talking face generation).",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "Conference on Computer Vision and Pattern Recognition, 2022, pp."
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "16000–16009."
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "[18] Y. Gong, A. Rouditchenko, A. H. Liu, D. Harwath, L. Karlinsky,"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "H. Kuehne, J. R. Glass, Contrastive audio-visual masked autoencoder,"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "Acknowledgements",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "in: The Eleventh International Conference on Learning Representations,"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "2023."
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "[19]\nP.-Y. Huang, V. Sharma, H. Xu, C. Ryali, H. Fan, Y. Li, S.-W. Li,"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "This work is\nsupported by the National Natural Science",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "G. Ghosh,\nJ. Malik, C. Feichtenhofer, MAVil: Masked audio-video"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "Foundation of China\n(NSFC)\n(No.61831022, No.62276259,",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "learners, in: Thirty-seventh Conference on Neural Information Process-"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "No.62201572,\nNo.U21B2010),\nBeijing Municipal\nScience",
          "References": "ing Systems, 2023."
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "[20]\nS. Sadok, S. Leglaive, R. S´eguier, A vector quantized masked au-"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "& Technology Commission, Administrative Commission of",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "toencoder\nfor audiovisual\nspeech emotion recognition, arXiv preprint"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "Zhongguancun Science Park (No.Z211100004821013), Open",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "arXiv:2305.03568 (2023)."
        },
        {
          "future work. We also notice that, among all emotions, neu-": "Research Projects of Zhejiang Lab (No.2021KH0AB06), and",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "[21] H. Wang, Y. Tang, Y. Wang, J. Guo, Z.-H. Deng, K. Han, Masked image"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "CCF-Baidu Open Fund (No.OF2022025).",
          "References": "modeling with local multi-scale reconstruction,\nin: Proceedings of the"
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "future work. We also notice that, among all emotions, neu-": "tral emotion is the one most easily confused with other emo-",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "tions. We believe that\nthe reason is that\nthe boundary between",
          "References": "[1] N. Schwarz, et al., Emotion, cognition, and decision making, Cognition"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "neutral emotion and other emotions is typically less clear than",
          "References": "& emotion 14 (4) (2000) 433–440."
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "[2] M. Minsky, Society of mind, Simon and Schuster, 1988."
        },
        {
          "future work. We also notice that, among all emotions, neu-": "those between other emotion combinations (e.g., happiness and",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "[3] A. MEHRABIAN, Communication without words, Psychol. Today 2 (4)"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "anger). Besides, we observe that fear is often misclassified as",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "(1968) 53–56."
        },
        {
          "future work. We also notice that, among all emotions, neu-": "surprise. This is consistent with our expectation as their differ-",
          "References": "[4] C.-H. Wu, J.-C. Lin, W.-L. Wei, Survey on audiovisual emotion recog-"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "ence in terms of facial expressions is very subtle (both featuring",
          "References": "nition: databases, features, and data fusion strategies, APSIPA transac-"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "tions on signal and information processing 3 (2014) e12."
        },
        {
          "future work. We also notice that, among all emotions, neu-": "wide-open eyes and raised eyebrows).",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "[5] Y. Liu, W. Dai, C. Feng, W. Wang, G. Yin,\nJ. Zeng, S. Shan, Mafw:"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "A large-scale, multi-modal, compound affective database for dynamic"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "facial expression recognition in the wild,\nin: Proceedings of\nthe 30th"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "ACM International Conference on Multimedia, 2022, pp. 24–32."
        },
        {
          "future work. We also notice that, among all emotions, neu-": "5. Conclusion",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "[6]\nZ. Zeng, M. Pantic, G.\nI. Roisman, T. S. Huang, A survey of affect"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "recognition methods: Audio, visual, and spontaneous expressions, IEEE"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "Transactions on Pattern Analysis and Machine Intelligence 31 (1) (2008)"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "In this paper, we have presented a novel\nself-supervised",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "39–58."
        },
        {
          "future work. We also notice that, among all emotions, neu-": "framework (HiCMAE), as an early attempt\nto leverage large-",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "[7]\nS. Zhang, S. Zhang, T. Huang, W. Gao, Q. Tian, Learning affective"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "scale self-supervised pre-training to address the dilemma faced",
          "References": "features with a hybrid deep model\nfor audio–visual emotion recogni-"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "by current supervised methods and largely promote the devel-",
          "References": "tion, IEEE Transactions on Circuits and Systems for Video Technology"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "28 (10) (2017) 3030–3043."
        },
        {
          "future work. We also notice that, among all emotions, neu-": "opment of AVER. HiCMAE is built on top of\ntwo primary",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "[8]\nP. Tzirakis, G. Trigeorgis, M. A. Nicolaou, B. W. Schuller, S. Zafeiriou,"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "forms of self-supervision, namely masked data modeling and",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "End-to-end multimodal\nemotion\nrecognition\nusing\ndeep\nneural\nnet-"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "contrastive learning. Moreover, to facilitate hierarchical audio-",
          "References": "works, IEEE Journal of selected topics in signal processing 11 (8) (2017)"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "visual feature learning, it introduces a three-pronged approach,",
          "References": "1301–1309."
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "[9] X. Zhang, M. Li, S. Lin, H. Xu, G. Xiao, Transformer-based multimodal"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "including hierarchical\nskip connections between the encoder",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "emotional perception for dynamic facial expression recognition in the"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "and decoder, hierarchical cross-modal contrastive learning, and",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "wild, IEEE Transactions on Circuits and Systems for Video Technology"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "hierarchical\nfeature fusion for downstream fine-tuning. Com-",
          "References": "(2023)."
        },
        {
          "future work. We also notice that, among all emotions, neu-": "prehensive experiments across 9 AVER datasets covering both",
          "References": "[10]\nS. Zhang, Y. Yang, C. Chen, X. Zhang, Q. Leng, X. Zhao, Deep"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "learning-based multimodal emotion recognition from audio, visual, and"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "categorical and dimensional\ntasks demonstrate that HiCMAE",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "text modalities: A systematic review of recent advancements and future"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "outperforms\nstate-of-the-art audio-visual methods by signifi-",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "prospects, Expert Systems with Applications (2023) 121692."
        },
        {
          "future work. We also notice that, among all emotions, neu-": "cant margins,\nindicating that HiCMAE is a powerful audio-",
          "References": "[11] G. Pei, H. Li, Y. Lu, Y. Wang, S. Hua, T. Li, Affective computing: Recent"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "advances, challenges, and future trends, Intelligent Computing 3 (2024)"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "visual emotion representation learner. Extensive ablation stud-",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "0076."
        },
        {
          "future work. We also notice that, among all emotions, neu-": "ies and visualization analysis also verify the efficacy of HiC-",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "[12] R. Balestriero, M. Ibrahim, V. Sobal, A. Morcos, S. Shekhar, T. Gold-"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "MAE. We hope this work can provide some insight\ninto the",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "stein, F. Bordes, A. Bardes, G. Mialon, Y. Tian, et al., A cookbook of"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "development of AVER and inspire more relevant studies.",
          "References": "self-supervised learning, arXiv preprint arXiv:2304.12210 (2023)."
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "[13]\nJ. Devlin, M.-W. Chang, K. Lee, K. Toutanova, Bert:\nPre-training"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "It\nshould be noted that,\ndue\nto limited computational\nre-",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "of deep bidirectional\ntransformers\nfor\nlanguage understanding,\narXiv"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "sources, we cannot afford the pre-training of larger models with",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "preprint arXiv:1810.04805 (2018)."
        },
        {
          "future work. We also notice that, among all emotions, neu-": "more training time and data. Therefore, we plan to tackle these",
          "References": "[14] R. Mao, Q. Liu, K. He, W. Li, E. Cambria, The biases of pre-trained"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "issues in future work and also encourage other\nresearchers to",
          "References": "language models: An empirical study on prompt-based sentiment anal-"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "ysis and emotion detection, IEEE Transactions on Affective Computing"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "conduct\nfollow-up studies.\nBesides,\nalthough HiCMAE has",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "(2022)."
        },
        {
          "future work. We also notice that, among all emotions, neu-": "achieved exceptional performance on many datasets,\nthis pa-",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "[15] W. Li, L. Zhu, R. Mao, E. Cambria, Skier: A symbolic knowledge inte-"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "per has conducted limited exploration into its internal mecha-",
          "References": "grated model for conversational emotion recognition, in: Proceedings of"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "the AAAI Conference on Artificial Intelligence, 2023."
        },
        {
          "future work. We also notice that, among all emotions, neu-": "nisms. Therefore,\nit\nis worth investigating and enhancing the",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "[16] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "interpretability of HiCMAE in future work. Possible interpre-",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "G. Sastry, A. Askell, P. Mishkin, J. Clark, et al., Learning transferable"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "tation tools include attention visualization [73], relevancy map",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "visual models from natural language supervision,\nin:\nInternational con-"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "[115], and Grad-CAM [116].\nFinally,\nit\nis also interesting to",
          "References": "ference on machine learning, PMLR, 2021, pp. 8748–8763."
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "[17] K. He, X. Chen, S. Xie, Y. Li, P. Doll´ar, R. Girshick, Masked autoen-"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "apply HiCMAE to other audio-visual tasks (e.g., active speaker",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "coders are scalable vision learners,\nin: Proceedings of\nthe IEEE/CVF"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "detection, deepfake detection, and talking face generation).",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "Conference on Computer Vision and Pattern Recognition, 2022, pp."
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "16000–16009."
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "[18] Y. Gong, A. Rouditchenko, A. H. Liu, D. Harwath, L. Karlinsky,"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "H. Kuehne, J. R. Glass, Contrastive audio-visual masked autoencoder,"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "Acknowledgements",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "in: The Eleventh International Conference on Learning Representations,"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "2023."
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "[19]\nP.-Y. Huang, V. Sharma, H. Xu, C. Ryali, H. Fan, Y. Li, S.-W. Li,"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "This work is\nsupported by the National Natural Science",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "G. Ghosh,\nJ. Malik, C. Feichtenhofer, MAVil: Masked audio-video"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "Foundation of China\n(NSFC)\n(No.61831022, No.62276259,",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "learners, in: Thirty-seventh Conference on Neural Information Process-"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "No.62201572,\nNo.U21B2010),\nBeijing Municipal\nScience",
          "References": "ing Systems, 2023."
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "[20]\nS. Sadok, S. Leglaive, R. S´eguier, A vector quantized masked au-"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "& Technology Commission, Administrative Commission of",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "toencoder\nfor audiovisual\nspeech emotion recognition, arXiv preprint"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "Zhongguancun Science Park (No.Z211100004821013), Open",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "arXiv:2305.03568 (2023)."
        },
        {
          "future work. We also notice that, among all emotions, neu-": "Research Projects of Zhejiang Lab (No.2021KH0AB06), and",
          "References": ""
        },
        {
          "future work. We also notice that, among all emotions, neu-": "",
          "References": "[21] H. Wang, Y. Tang, Y. Wang, J. Guo, Z.-H. Deng, K. Han, Masked image"
        },
        {
          "future work. We also notice that, among all emotions, neu-": "CCF-Baidu Open Fund (No.OF2022025).",
          "References": "modeling with local multi-scale reconstruction,\nin: Proceedings of the"
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "2023, pp. 2122–2131.",
          "2352.": "[40] Q. Kong, Y. Cao, T. Iqbal, Y. Wang, W. Wang, M. D. Plumbley, Panns:"
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "[22] Y. Liu, S. Zhang,\nJ. Chen, Z. Yu, K. Chen, D. Lin,\nImproving pixel-",
          "2352.": "Large-scale pretrained audio neural networks for audio pattern recog-"
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "based mim by reducing wasted modeling capability,\nin: Proceedings of",
          "2352.": "nition, IEEE/ACM Transactions on Audio, Speech, and Language Pro-"
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "the IEEE/CVF International Conference on Computer Vision, 2023, pp.",
          "2352.": "cessing 28 (2020) 2880–2894."
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "5361–5372.",
          "2352.": "[41]\nS. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C."
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "[23] K. Zhang, X. Wu, X. Xie, X. Zhang, H. Zhang, X. Chen, L. Sun,",
          "2352.": "Moore, M. Plakal, D. Platt, R. A. Saurous, B. Seybold, et al., Cnn archi-"
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "Werewolf-xl: A database\nfor\nidentifying spontaneous affect\nin large",
          "2352.": "tectures for large-scale audio classification,\nin: 2017 ieee international"
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "competitive group interactions,\nIEEE Transactions on Affective Com-",
          "2352.": "conference on acoustics, speech and signal processing (icassp),\nIEEE,"
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "puting 14 (02) (2023) 1201–1214.",
          "2352.": "2017, pp. 131–135."
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "[24]\nP. Sarkar, A. Posen, A. Etemad, Avcaffe: A large scale audio-visual",
          "2352.": "[42] Q. Cao, L. Shen, W. Xie, O. M. Parkhi, A. Zisserman, Vggface2: A"
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "dataset of cognitive load and affect\nfor\nremote work,\narXiv preprint",
          "2352.": "dataset for recognising faces across pose and age,\nin: 2018 13th IEEE"
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "arXiv:2205.06887 (2022).",
          "2352.": "international conference on automatic face & gesture recognition (FG"
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "[25]\nZ. Lian, H. Sun, L. Sun, K. Chen, M. Xu, K. Wang, K. Xu, Y. He, Y. Li,",
          "2352.": "2018), IEEE, 2018, pp. 67–74."
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "J. Zhao, et al., Mer 2023: Multi-label learning, modality robustness, and",
          "2352.": "[43] D. Tran, L. Bourdev, R. Fergus, L. Torresani, M. Paluri, Learning spa-"
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "semi-supervised learning, in: Proceedings of the 31st ACM International",
          "2352.": "tiotemporal features with 3d convolutional networks, in: Proceedings of"
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "Conference on Multimedia, 2023, pp. 9610–9614.",
          "2352.": "the IEEE international conference on computer vision, 2015, pp. 4489–"
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "[26] O. Ronneberger, P. Fischer, T. Brox, U-net: Convolutional networks",
          "2352.": "4497."
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "for biomedical\nimage\nsegmentation,\nin: Medical\nImage Computing",
          "2352.": "[44] G. Trigeorgis, F. Ringeval, R. Brueckner, E. Marchi, M. A. Nicolaou,"
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "and Computer-Assisted Intervention–MICCAI 2015: 18th International",
          "2352.": "B. Schuller, S. Zafeiriou, Adieu features?\nend-to-end speech emotion"
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III",
          "2352.": "recognition using a deep convolutional\nrecurrent network,\nin:\n2016"
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "18, Springer, 2015, pp. 234–241.",
          "2352.": "IEEE international conference on acoustics, speech and signal process-"
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "[27]\nJ. S. Chung, A. Nagrani, A. Zisserman, Voxceleb2: Deep speaker recog-",
          "2352.": "ing (ICASSP), IEEE, 2016, pp. 5200–5204."
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "nition, Proc. Interspeech 2018 (2018) 1086–1090.",
          "2352.": "[45]\nJ. Huang, Y. Li, J. Tao, Z. Lian, J. Yi, End-to-end continuous emotion"
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "[28] H. Cao, D. G. Cooper, M. K. Keutmann, R. C. Gur, A. Nenkova,",
          "2352.": "recognition from video using 3d convlstm networks,\nin:\n2018 IEEE"
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "R. Verma,\nCrema-d:\nCrowd-sourced\nemotional multimodal\nactors",
          "2352.": "International Conference on Acoustics, Speech and Signal Processing"
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "dataset, IEEE transactions on affective computing 5 (4) (2014) 377–390.",
          "2352.": "(ICASSP), IEEE, 2018, pp. 6837–6841."
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "[29] X. Jiang, Y. Zong, W. Zheng, C. Tang, W. Xia, C. Lu,\nJ. Liu, Dfew:",
          "2352.": "[46] Y. Wang, Y. Sun, Y. Huang, Z. Liu, S. Gao, W. Zhang, W. Ge, W. Zhang,"
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "A large-scale database for\nrecognizing dynamic facial expressions\nin",
          "2352.": "Ferv39k: A large-scale multi-scene dataset for facial expression recog-"
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "the wild,\nin:\nProceedings of\nthe 28th ACM International Conference",
          "2352.": "nition in videos, in: Proceedings of the IEEE/CVF Conference on Com-"
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "on Multimedia, 2020, pp. 2881–2889.",
          "2352.": "puter Vision and Pattern Recognition, 2022, pp. 20922–20931."
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "[30]\nZ. Lian, L. Sun, Y. Ren, H. Gu, H. Sun, L. Chen, B. Liu, J. Tao, Mer-",
          "2352.": "[47] A. Baevski, Y. Zhou, A. Mohamed, M. Auli, wav2vec 2.0: A frame-"
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "bench: A unified evaluation benchmark for multimodal emotion recog-",
          "2352.": "work for self-supervised learning of speech representations, Advances"
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "nition, arXiv preprint arXiv:2401.03429 (2024).",
          "2352.": "in neural information processing systems 33 (2020) 12449–12460."
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "[31] B. Schuller, S. Steidl, A. Batliner, A. Vinciarelli, K. Scherer, F. Ringeval,",
          "2352.": "[48] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov,"
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "M. Chetouani, F. Weninger, F. Eyben, E. Marchi, et al., The interspeech",
          "2352.": "A. Mohamed, Hubert:\nSelf-supervised speech representation learning"
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "2013 computational paralinguistics challenge: Social signals, conflict,",
          "2352.": "by masked prediction of hidden units, IEEE/ACM Transactions on Au-"
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "emotion, autism,\nin: Proceedings INTERSPEECH 2013, 14th Annual",
          "2352.": "dio, Speech, and Language Processing 29 (2021) 3451–3460."
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "Conference of\nthe International Speech Communication Association,",
          "2352.": "[49]\nL. Sun, Z. Lian, K. Wang, Y. He, M. Xu, H. Sun, B. Liu,\nJ. Tao,"
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "Lyon, France, 2013.",
          "2352.": "Svfap:\nSelf-supervised\nvideo\nfacial\naffect\nperceiver,\narXiv\npreprint"
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "[32]\nF. Eyben, K. R. Scherer, B. W. Schuller, J. Sundberg, E. Andr´e, C. Busso,",
          "2352.": "arXiv:2401.00416 (2023)."
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "L. Y. Devillers, J. Epps, P. Laukka, S. S. Narayanan, et al., The geneva",
          "2352.": "[50]\nL. Sun, Z. Lian, B. Liu, J. Tao, Mae-dfer: Efficient masked autoencoder"
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "minimalistic acoustic parameter\nset\n(gemaps)\nfor voice research and",
          "2352.": "for self-supervised dynamic facial expression recognition,\nin: Proceed-"
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "affective computing,\nIEEE transactions on affective computing 7 (2)",
          "2352.": "ings of\nthe 31st ACM International Conference on Multimedia, 2023,"
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "(2015) 190–202.",
          "2352.": "pp. 6110–6121."
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "[33] G. Zhao, M. Pietikainen, Dynamic texture recognition using local binary",
          "2352.": "[51] Y.-H. H. Tsai,\nS. Bai,\nP.\nP. Liang,\nJ. Z. Kolter,\nL.-P. Morency,"
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "patterns with an application to facial expressions, IEEE transactions on",
          "2352.": "R. Salakhutdinov, Multimodal\ntransformer\nfor unaligned multimodal"
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "pattern analysis and machine intelligence 29 (6) (2007) 915–928.",
          "2352.": "language\nsequences,\nin:\nProceedings of\nthe\nconference. Association"
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "[34] N. Dalal, B. Triggs, Histograms of oriented gradients for human detec-",
          "2352.": "for Computational Linguistics. Meeting, Vol. 2019, NIH Public Access,"
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "tion,\nin:\n2005 IEEE computer society conference on computer vision",
          "2352.": "2019, p. 6558."
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "and pattern recognition (CVPR’05), Vol. 1, Ieee, 2005, pp. 886–893.",
          "2352.": "[52]\nJ. Huang, J. Tao, B. Liu, Z. Lian, M. Niu, Multimodal\ntransformer fu-"
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "[35] Y. Fan, X. Lu, D. Li, Y. Liu, Video-based emotion recognition using",
          "2352.": "sion for continuous emotion recognition,\nin:\nICASSP 2020-2020 IEEE"
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "cnn-rnn and c3d hybrid networks,\nin:\nProceedings of\nthe 18th ACM",
          "2352.": "International Conference on Acoustics, Speech and Signal Processing"
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "international conference on multimodal interaction, 2016, pp. 445–450.",
          "2352.": "(ICASSP), IEEE, 2020, pp. 3507–3511."
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "[36]\nS. Chen, Q. Jin, J. Zhao, S. Wang, Multimodal multi-task learning for",
          "2352.": "[53]\nL. Goncalves, C. Busso, Auxformer: Robust approach to audiovisual"
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "dimensional and continuous emotion recognition, in: Proceedings of the",
          "2352.": "emotion recognition,\nin:\nICASSP 2022-2022 IEEE International Con-"
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "7th Annual Workshop on Audio/Visual Emotion Challenge, 2017, pp.",
          "2352.": "ference on Acoustics, Speech and Signal Processing (ICASSP), IEEE,"
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "19–26.",
          "2352.": "2022, pp. 7357–7361."
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "[37]\nL. Sun, Z. Lian,\nJ. Tao, B. Liu, M. Niu, Multi-modal continuous di-",
          "2352.": "[54]\nZ. Lian, B. Liu, J. Tao, Ctnet: Conversational\ntransformer network for"
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "mensional emotion recognition using recurrent neural network and self-",
          "2352.": "emotion recognition,\nIEEE/ACM Transactions on Audio, Speech, and"
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "attention mechanism,\nin: Proceedings of\nthe 1st\ninternational on mul-",
          "2352.": "Language Processing 29 (2021) 985–1000."
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "timodal sentiment analysis in real-life media challenge and workshop,",
          "2352.": "[55] M. Tran, M. Soleymani, A pre-trained audio-visual transformer for emo-"
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "2020, pp. 27–34.",
          "2352.": "tion recognition, in: ICASSP 2022-2022 IEEE International Conference"
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "[38]\nL. Sun, M. Xu, Z. Lian, B. Liu, J. Tao, M. Wang, Y. Cheng, Multimodal",
          "2352.": "on Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2022, pp."
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "emotion recognition and sentiment analysis via attention enhanced re-",
          "2352.": "4698–4702."
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "current model,\nin:\nProceedings of\nthe 2nd on Multimodal Sentiment",
          "2352.": "[56]\nJ.-H. Hsu, C.-H. Wu, Applying segment-level attention on bi-modal"
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "Analysis Challenge, 2021, pp. 15–20.",
          "2352.": "transformer encoder for audio-visual emotion recognition, IEEE Trans-"
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "[39]\nL. Meng, Y. Liu, X. Liu, Z. Huang, W. Jiang, T. Zhang, C. Liu, Q. Jin,",
          "2352.": "actions on Affective Computing (2023)."
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "Valence and arousal estimation based on multimodal\ntemporal-aware",
          "2352.": "[57]\nL. Sun, Z. Lian, B. Liu, J. Tao, Efficient multimodal\ntransformer with"
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "features for videos in the wild,\nin: Proceedings of the IEEE/CVF Con-",
          "2352.": "dual-level feature restoration for robust multimodal sentiment analysis,"
        },
        {
          "IEEE/CVF Conference on Computer Vision and Pattern Recognition,": "ference on Computer Vision and Pattern Recognition, 2022, pp. 2345–",
          "2352.": "IEEE Transactions on Affective Computing (2023)."
        }
      ],
      "page": 18
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "of the IEEE international conference on computer vision, 2017, pp. 609–",
          "arXiv:2310.16640 (2023).": "[80] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, H. J´egou,"
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "617.",
          "arXiv:2310.16640 (2023).": "Training data-efficient\nimage transformers & distillation through atten-"
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "[59] A. Owens, A. A. Efros, Audio-visual scene analysis with self-supervised",
          "arXiv:2310.16640 (2023).": "tion, in: International conference on machine learning, PMLR, 2021, pp."
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "multisensory features,\nin: Proceedings of\nthe European conference on",
          "arXiv:2310.16640 (2023).": "10347–10357."
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "computer vision (ECCV), 2018, pp. 631–648.",
          "arXiv:2310.16640 (2023).": "[81] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,"
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "[60]\nP. Morgado, N. Vasconcelos,\nI. Misra, Audio-visual\ninstance discrimi-",
          "arXiv:2310.16640 (2023).": "L. Zettlemoyer, V. Stoyanov, Roberta: A robustly optimized bert pre-"
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "nation with cross-modal agreement,\nin: Proceedings of the IEEE/CVF",
          "arXiv:2310.16640 (2023).": "training approach, arXiv preprint arXiv:1907.11692 (2019)."
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "Conference on Computer Vision and Pattern Recognition, 2021, pp.",
          "arXiv:2310.16640 (2023).": "[82] D. Tran, H. Wang, L. Torresani, J. Ray, Y. LeCun, M. Paluri, A closer"
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "12475–12486.",
          "arXiv:2310.16640 (2023).": "look at spatiotemporal convolutions for action recognition, in: Proceed-"
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "[61]\nZ. Tong, Y. Song, J. Wang, L. Wang, VideoMAE: Masked autoencoders",
          "arXiv:2310.16640 (2023).": "ings of the IEEE conference on Computer Vision and Pattern Recogni-"
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "are data-efficient learners for self-supervised video pre-training, in: Ad-",
          "arXiv:2310.16640 (2023).": "tion, 2018, pp. 6450–6459."
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "vances in Neural Information Processing Systems, 2022.",
          "arXiv:2310.16640 (2023).": "[83] K. Hara, H. Kataoka, Y. Satoh, Can spatiotemporal 3d cnns retrace the"
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "[62] C. Feichtenhofer, H. Fan, Y. Li, K. He, Masked autoencoders as spa-",
          "arXiv:2310.16640 (2023).": "history of 2d cnns and imagenet?,\nin:\nProceedings of\nthe IEEE con-"
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "tiotemporal learners, arXiv preprint arXiv:2205.09113 (2022).",
          "arXiv:2310.16640 (2023).": "ference on Computer Vision and Pattern Recognition, 2018, pp. 6546–"
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "[63] M.-I. Georgescu, E. Fonseca, R. T.\nIonescu, M. Lucic, C. Schmid,",
          "arXiv:2310.16640 (2023).": "6555."
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "A. Arnab, Audiovisual masked autoencoders,\nin:\nProceedings of\nthe",
          "arXiv:2310.16640 (2023).": "[84] Y. Liu, C. Feng, X. Yuan, L. Zhou, W. Wang, J. Qin, Z. Luo, Clip-aware"
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "IEEE/CVF International Conference on Computer Vision,\n2023,\npp.",
          "arXiv:2310.16640 (2023).": "expressive feature learning for video-based facial expression recogni-"
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "16144–16154.",
          "arXiv:2310.16640 (2023).": "tion, Information Sciences 598 (2022) 182–195."
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "[64] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,",
          "arXiv:2310.16640 (2023).": "[85] Y. Liu, W. Wang, C. Feng, H. Zhang, Z. Chen, Y. Zhan, Expression"
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "Ł. Kaiser, I. Polosukhin, Attention is all you need, Advances in neural",
          "arXiv:2310.16640 (2023).": "snippet transformer for robust video-based facial expression recognition,"
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "information processing systems 30 (2017).",
          "arXiv:2310.16640 (2023).": "Pattern Recognition 138 (2023) 109368."
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "[65]\nP.-Y. Huang, H. Xu, J. Li, A. Baevski, M. Auli, W. Galuba, F. Metze,",
          "arXiv:2310.16640 (2023).": "[86]\nF. Ma, B. Sun, S. Li, Spatio-temporal\ntransformer\nfor dynamic facial"
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "C. Feichtenhofer, Masked autoencoders that\nlisten, Advances in Neural",
          "arXiv:2310.16640 (2023).": "expression recognition in the wild,\narXiv preprint\narXiv:2205.04749"
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "Information Processing Systems 35 (2022) 28708–28720.",
          "arXiv:2310.16640 (2023).": "(2022)."
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "[66] D. Hendrycks, K. Gimpel, Gaussian error\nlinear units\n(gelus),\narXiv",
          "arXiv:2310.16640 (2023).": "[87] H. Li, M. Sui, Z. Zhu, et al., Nr-dfernet: Noise-robust network for dy-"
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "preprint arXiv:1606.08415 (2016).",
          "arXiv:2310.16640 (2023).": "namic facial expression recognition, arXiv preprint arXiv:2206.04975"
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "[67] A. v. d. Oord, Y. Li, O. Vinyals, Representation learning with contrastive",
          "arXiv:2310.16640 (2023).": "(2022)."
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "predictive coding, arXiv preprint arXiv:1807.03748 (2018).",
          "arXiv:2310.16640 (2023).": "[88] Y. Wang, Y. Sun, W. Song, S. Gao, Y. Huang, Z. Chen, W. Ge, W. Zhang,"
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "[68] C. Busso, S. Parthasarathy, A. Burmania, M. AbdelWahab, N. Sadoughi,",
          "arXiv:2310.16640 (2023).": "Dpcnet: Dual path multi-excitation collaborative network for facial ex-"
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "E. M. Provost, Msp-improv: An acted corpus of dyadic interactions to",
          "arXiv:2310.16640 (2023).": "pression representation learning in videos,\nin: Proceedings of the 30th"
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "study emotion perception,\nIEEE Transactions on Affective Computing",
          "arXiv:2310.16640 (2023).": "ACM International Conference on Multimedia, 2022, pp. 101–110."
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "8 (1) (2016) 67–80.",
          "arXiv:2310.16640 (2023).": "[89] H. Li, H. Niu, Z. Zhu, F. Zhao, Intensity-aware loss for dynamic facial"
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "[69]\nS. R. Livingstone, F. A. Russo, The ryerson audio-visual database of",
          "arXiv:2310.16640 (2023).": "expression recognition in the wild,\nin: Proceedings of the AAAI Con-"
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "emotional\nspeech and song (ravdess): A dynamic, multimodal\nset of",
          "arXiv:2310.16640 (2023).": "ference on Artificial Intelligence, Vol. 37, 2023, pp. 67–75."
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "facial and vocal expressions in north american english, PloS one 13 (5)",
          "arXiv:2310.16640 (2023).": "[90] H. Wang, B. Li, S. Wu, S. Shen, F. Liu, S. Ding, A. Zhou, Rethink-"
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "(2018) e0196391.",
          "arXiv:2310.16640 (2023).": "ing the learning paradigm for dynamic facial expression recognition, in:"
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "[70] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower, S. Kim, J. N.",
          "arXiv:2310.16640 (2023).": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pat-"
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "Chang, S. Lee, S. S. Narayanan, Iemocap:\nInteractive emotional dyadic",
          "arXiv:2310.16640 (2023).": "tern Recognition, 2023, pp. 17958–17968."
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "motion capture database, Language resources and evaluation 42 (2008)",
          "arXiv:2310.16640 (2023).": "[91] B. Zhang, H. Lv, P. Guo, Q. Shao, C. Yang, L. Xie, X. Xu, H. Bu,"
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "335–359.",
          "arXiv:2310.16640 (2023).": "X. Chen, C. Zeng, et al., Wenetspeech: A 10000+ hours multi-domain"
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "[71]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,",
          "arXiv:2310.16640 (2023).": "mandarin corpus for speech recognition,\nin:\nICASSP 2022-2022 IEEE"
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "T. Yoshioka, X. Xiao, et al., Wavlm: Large-scale self-supervised pre-",
          "arXiv:2310.16640 (2023).": "International Conference on Acoustics, Speech and Signal Processing"
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "training for full stack speech processing, IEEE Journal of Selected Top-",
          "arXiv:2310.16640 (2023).": "(ICASSP), IEEE, 2022, pp. 6182–6186."
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "ics in Signal Processing 16 (6) (2022) 1505–1518.",
          "arXiv:2310.16640 (2023).": "[92]\nJ. Hu, L. Shen, G. Sun, Squeeze-and-excitation networks,\nin: Proceed-"
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "[72] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recog-",
          "arXiv:2310.16640 (2023).": "ings of the IEEE conference on computer vision and pattern recognition,"
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "nition,\nin: Proceedings of the IEEE conference on computer vision and",
          "arXiv:2310.16640 (2023).": "2018, pp. 7132–7141."
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "pattern recognition, 2016, pp. 770–778.",
          "arXiv:2310.16640 (2023).": "[93]\nZ. Zhao, Q. Liu, S. Wang, Learning deep global multi-scale and local"
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "[73] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,",
          "arXiv:2310.16640 (2023).": "attention features\nfor\nfacial expression recognition in the wild,\nIEEE"
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al.,",
          "arXiv:2310.16640 (2023).": "Transactions on Image Processing 30 (2021) 6544–6556."
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "An image is worth 16x16 words: Transformers for image recognition at",
          "arXiv:2310.16640 (2023).": "[94] M. Tran, Y. Kim, C.-C. Su, C.-H. Kuo, M. Soleymani, Saaml: A frame-"
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "scale, arXiv preprint arXiv:2010.11929 (2020).",
          "arXiv:2310.16640 (2023).": "work for semi-supervised affective adaptation via metric learning,\nin:"
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "[74]\nZ. Zhao, Q. Liu, Former-dfer: Dynamic facial expression recognition",
          "arXiv:2310.16640 (2023).": "Proceedings of the 31st ACM International Conference on Multimedia,"
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "transformer, in: Proceedings of the 29th ACM International Conference",
          "arXiv:2310.16640 (2023).": "2023, pp. 6004–6015."
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "on Multimedia, 2021, pp. 1553–1561.",
          "arXiv:2310.16640 (2023).": "[95]\nL. Su, C. Hu, G. Li, D. Cao, Msaf: Multimodal split attention fusion,"
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "[75]\nZ. Zhao,\nI. Patras, Prompting visual-language models for dynamic fa-",
          "arXiv:2310.16640 (2023).": "arXiv preprint arXiv:2012.07175 (2020)."
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "cial\nexpression recognition,\nin:\nBritish Machine Vision Conference",
          "arXiv:2310.16640 (2023).": "[96]\nZ. Fu, F. Liu, H. Wang, J. Qi, X. Fu, A. Zhou, Z. Li, A cross-modal"
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "(BMVC), 2023, pp. 1–14.",
          "arXiv:2310.16640 (2023).": "fusion network based on self-attention and residual structure for multi-"
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "[76]\nS. Yoon, S. Dey, H. Lee, K. Jung, Attentive modality hopping mech-",
          "arXiv:2310.16640 (2023).": "modal emotion recognition, arXiv preprint arXiv:2111.02172 (2021)."
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "anism for\nspeech emotion recognition,\nin:\nICASSP 2020-2020 IEEE",
          "arXiv:2310.16640 (2023).": "[97]\nL.-W. Chen, A. Rudnicky, Exploring wav2vec 2.0 fine tuning for\nim-"
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "International Conference on Acoustics, Speech and Signal Processing",
          "arXiv:2310.16640 (2023).": "proved speech emotion recognition,\nin:\nICASSP 2023-2023 IEEE In-"
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "(ICASSP), IEEE, 2020, pp. 3362–3366.",
          "arXiv:2310.16640 (2023).": "ternational Conference on Acoustics, Speech and Signal Processing"
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "[77]\nS. Yoon, S. Byun, K. Jung, Multimodal speech emotion recognition us-",
          "arXiv:2310.16640 (2023).": "(ICASSP), IEEE, 2023, pp. 1–5."
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "ing audio and text, in: 2018 IEEE Spoken Language Technology Work-",
          "arXiv:2310.16640 (2023).": "[98] Y. Tseng, L. Berry, Y.-T. Chen,\nI. Chiu, H.-H. Lin, M. Liu, P. Peng,"
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "shop (SLT), IEEE, 2018, pp. 112–118.",
          "arXiv:2310.16640 (2023).": "Y\n.-J. Shih, H.-Y. Wang, H. Wu, et al., Av-superb: A multi-task evalu-"
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "[78] V. Rajan, A. Brutti, A. Cavallaro,\nIs cross-attention preferable to self-",
          "arXiv:2310.16640 (2023).": "ation benchmark for audio-visual representation models, arXiv preprint"
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "attention for multi-modal emotion recognition?, in: ICASSP 2022-2022",
          "arXiv:2310.16640 (2023).": "arXiv:2309.10787 (2023)."
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "IEEE International Conference on Acoustics, Speech and Signal Pro-",
          "arXiv:2310.16640 (2023).": "[99] A. Keesing, Y. S. Koh, V. Yogarajan, M. Witbrock, Emotion recognition"
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "cessing (ICASSP), IEEE, 2022, pp. 4693–4697.",
          "arXiv:2310.16640 (2023).": "toolkit (ertk): Standardising tools for emotion recognition research,\nin:"
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "[79] N. M. Foteinopoulou,\nI. Patras, Emoclip: A vision-language method",
          "arXiv:2310.16640 (2023).": "Proceedings of the 31st ACM International Conference on Multimedia,"
        },
        {
          "[58] R. Arandjelovic, A. Zisserman, Look,\nlisten and learn,\nin: Proceedings": "for\nzero-shot\nvideo\nfacial\nexpression\nrecognition,\narXiv\npreprint",
          "arXiv:2310.16640 (2023).": "2023, pp. 9693–9696."
        }
      ],
      "page": 19
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[100]\nE. Ghaleb, M. Popa, S. Asteriadis, Multimodal and temporal perception": "of audio-visual cues for emotion recognition, in: 2019 8th International"
        },
        {
          "[100]\nE. Ghaleb, M. Popa, S. Asteriadis, Multimodal and temporal perception": "Conference on Affective Computing and Intelligent Interaction (ACII),"
        },
        {
          "[100]\nE. Ghaleb, M. Popa, S. Asteriadis, Multimodal and temporal perception": "IEEE, 2019, pp. 552–558."
        },
        {
          "[100]\nE. Ghaleb, M. Popa, S. Asteriadis, Multimodal and temporal perception": "[101]\nL. Goncalves, C. Busso, Robust audiovisual emotion recognition: Align-"
        },
        {
          "[100]\nE. Ghaleb, M. Popa, S. Asteriadis, Multimodal and temporal perception": "ing modalities, capturing temporal\ninformation, and handling missing"
        },
        {
          "[100]\nE. Ghaleb, M. Popa, S. Asteriadis, Multimodal and temporal perception": "features,\nIEEE Transactions on Affective Computing 13 (04)\n(2022)"
        },
        {
          "[100]\nE. Ghaleb, M. Popa, S. Asteriadis, Multimodal and temporal perception": "2156–2170."
        },
        {
          "[100]\nE. Ghaleb, M. Popa, S. Asteriadis, Multimodal and temporal perception": "[102] Y. Lei, H. Cao, Audio-visual emotion recognition with preference learn-"
        },
        {
          "[100]\nE. Ghaleb, M. Popa, S. Asteriadis, Multimodal and temporal perception": "ing based on intended and multi-modal perceived labels, IEEE Transac-"
        },
        {
          "[100]\nE. Ghaleb, M. Popa, S. Asteriadis, Multimodal and temporal perception": "tions on Affective Computing (2023) 1–16."
        },
        {
          "[100]\nE. Ghaleb, M. Popa, S. Asteriadis, Multimodal and temporal perception": "[103] A. Zadeh, M. Chen, S. Poria, E. Cambria, L.-P. Morency, Tensor\nfu-"
        },
        {
          "[100]\nE. Ghaleb, M. Popa, S. Asteriadis, Multimodal and temporal perception": "sion network for multimodal sentiment analysis,\nin: Proceedings of the"
        },
        {
          "[100]\nE. Ghaleb, M. Popa, S. Asteriadis, Multimodal and temporal perception": "2017 Conference on Empirical Methods in Natural Language Process-"
        },
        {
          "[100]\nE. Ghaleb, M. Popa, S. Asteriadis, Multimodal and temporal perception": "ing, 2017, pp. 1103–1114."
        },
        {
          "[100]\nE. Ghaleb, M. Popa, S. Asteriadis, Multimodal and temporal perception": "[104]\nE. Ghaleb, J. Niehues, S. Asteriadis, Multimodal attention-mechanism"
        },
        {
          "[100]\nE. Ghaleb, M. Popa, S. Asteriadis, Multimodal and temporal perception": "for temporal emotion recognition,\nin: 2020 IEEE International Confer-"
        },
        {
          "[100]\nE. Ghaleb, M. Popa, S. Asteriadis, Multimodal and temporal perception": "ence on Image Processing (ICIP), IEEE, 2020, pp. 251–255."
        },
        {
          "[100]\nE. Ghaleb, M. Popa, S. Asteriadis, Multimodal and temporal perception": "[105]\nL. Goncalves, C. Busso, Learning cross-modal audiovisual representa-"
        },
        {
          "[100]\nE. Ghaleb, M. Popa, S. Asteriadis, Multimodal and temporal perception": "tions with ladder networks for emotion recognition,\nin:\nICASSP 2023-"
        },
        {
          "[100]\nE. Ghaleb, M. Popa, S. Asteriadis, Multimodal and temporal perception": "2023 IEEE International Conference on Acoustics, Speech and Signal"
        },
        {
          "[100]\nE. Ghaleb, M. Popa, S. Asteriadis, Multimodal and temporal perception": "Processing (ICASSP), IEEE, 2023, pp. 1–5."
        },
        {
          "[100]\nE. Ghaleb, M. Popa, S. Asteriadis, Multimodal and temporal perception": "[106] B. Shi, W.-N. Hsu, K. Lakhotia, A. Mohamed, Learning audio-visual"
        },
        {
          "[100]\nE. Ghaleb, M. Popa, S. Asteriadis, Multimodal and temporal perception": "speech representation by masked multimodal cluster prediction,\nin:\nIn-"
        },
        {
          "[100]\nE. Ghaleb, M. Popa, S. Asteriadis, Multimodal and temporal perception": "ternational Conference on Learning Representations, 2022."
        },
        {
          "[100]\nE. Ghaleb, M. Popa, S. Asteriadis, Multimodal and temporal perception": "[107] A. Fukui, D. H. Park, D. Yang, A. Rohrbach, T. Darrell, M. Rohrbach,"
        },
        {
          "[100]\nE. Ghaleb, M. Popa, S. Asteriadis, Multimodal and temporal perception": "Multimodal compact bilinear pooling for visual question answering and"
        },
        {
          "[100]\nE. Ghaleb, M. Popa, S. Asteriadis, Multimodal and temporal perception": "visual grounding, in: Proceedings of the 2016 Conference on Empirical"
        },
        {
          "[100]\nE. Ghaleb, M. Popa, S. Asteriadis, Multimodal and temporal perception": "Methods in Natural Language Processing, 2016, pp. 457–468."
        },
        {
          "[100]\nE. Ghaleb, M. Popa, S. Asteriadis, Multimodal and temporal perception": "[108] H. R. V. Joze, A. Shaban, M. L. Iuzzolino, K. Koishida, Mmtm: Multi-"
        },
        {
          "[100]\nE. Ghaleb, M. Popa, S. Asteriadis, Multimodal and temporal perception": "modal transfer module for cnn fusion, in: Proceedings of the IEEE/CVF"
        },
        {
          "[100]\nE. Ghaleb, M. Popa, S. Asteriadis, Multimodal and temporal perception": "conference\non\ncomputer\nvision\nand\npattern\nrecognition,\n2020,\npp."
        },
        {
          "[100]\nE. Ghaleb, M. Popa, S. Asteriadis, Multimodal and temporal perception": "13289–13299."
        },
        {
          "[100]\nE. Ghaleb, M. Popa, S. Asteriadis, Multimodal and temporal perception": "[109]\nS. Verbitskiy, V. Berikov, V. Vyshegorodtsev, Eranns: Efficient residual"
        },
        {
          "[100]\nE. Ghaleb, M. Popa, S. Asteriadis, Multimodal and temporal perception": "audio neural networks for audio pattern recognition, Pattern Recognition"
        },
        {
          "[100]\nE. Ghaleb, M. Popa, S. Asteriadis, Multimodal and temporal perception": "Letters 161 (2022) 38–44."
        },
        {
          "[100]\nE. Ghaleb, M. Popa, S. Asteriadis, Multimodal and temporal perception": "[110] K. Chumachenko, A. Iosifidis, M. Gabbouj, Self-attention fusion for au-"
        },
        {
          "[100]\nE. Ghaleb, M. Popa, S. Asteriadis, Multimodal and temporal perception": "diovisual emotion recognition with incomplete data,\nin: 2022 26th In-"
        },
        {
          "[100]\nE. Ghaleb, M. Popa, S. Asteriadis, Multimodal and temporal perception": "ternational Conference on Pattern Recognition (ICPR), IEEE, 2022, pp."
        },
        {
          "[100]\nE. Ghaleb, M. Popa, S. Asteriadis, Multimodal and temporal perception": "2822–2828."
        },
        {
          "[100]\nE. Ghaleb, M. Popa, S. Asteriadis, Multimodal and temporal perception": "[111] H. Mittal, P. Morgado, U. Jain, A. Gupta, Learning state-aware visual"
        },
        {
          "[100]\nE. Ghaleb, M. Popa, S. Asteriadis, Multimodal and temporal perception": "representations from audible interactions, Advances in Neural Informa-"
        },
        {
          "[100]\nE. Ghaleb, M. Popa, S. Asteriadis, Multimodal and temporal perception": "tion Processing Systems 35 (2022) 23765–23779."
        },
        {
          "[100]\nE. Ghaleb, M. Popa, S. Asteriadis, Multimodal and temporal perception": "[112]\nS. Lee, Y. Yu, G. Kim, T. Breuel, J. Kautz, Y. Song, Parameter efficient"
        },
        {
          "[100]\nE. Ghaleb, M. Popa, S. Asteriadis, Multimodal and temporal perception": "multimodal\ntransformers for video representation learning,\nin:\nInterna-"
        },
        {
          "[100]\nE. Ghaleb, M. Popa, S. Asteriadis, Multimodal and temporal perception": "tional Conference on Learning Representations, 2021."
        },
        {
          "[100]\nE. Ghaleb, M. Popa, S. Asteriadis, Multimodal and temporal perception": "[113] O. Parkhi, A. Vedaldi, A. Zisserman, Deep face recognition, in: BMVC"
        },
        {
          "[100]\nE. Ghaleb, M. Popa, S. Asteriadis, Multimodal and temporal perception": "2015-Proceedings\nof\nthe British Machine Vision Conference\n2015,"
        },
        {
          "[100]\nE. Ghaleb, M. Popa, S. Asteriadis, Multimodal and temporal perception": "British Machine Vision Association, 2015."
        },
        {
          "[100]\nE. Ghaleb, M. Popa, S. Asteriadis, Multimodal and temporal perception": "[114]\nL. Van der Maaten, G. Hinton, Visualizing data using t-sne., Journal of"
        },
        {
          "[100]\nE. Ghaleb, M. Popa, S. Asteriadis, Multimodal and temporal perception": "machine learning research 9 (11) (2008)."
        },
        {
          "[100]\nE. Ghaleb, M. Popa, S. Asteriadis, Multimodal and temporal perception": "[115] H. Chefer, S. Gur, L. Wolf, Transformer interpretability beyond attention"
        },
        {
          "[100]\nE. Ghaleb, M. Popa, S. Asteriadis, Multimodal and temporal perception": "visualization, in: Proceedings of the IEEE/CVF conference on computer"
        },
        {
          "[100]\nE. Ghaleb, M. Popa, S. Asteriadis, Multimodal and temporal perception": "vision and pattern recognition, 2021, pp. 782–791."
        },
        {
          "[100]\nE. Ghaleb, M. Popa, S. Asteriadis, Multimodal and temporal perception": "[116] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, D. Batra,"
        },
        {
          "[100]\nE. Ghaleb, M. Popa, S. Asteriadis, Multimodal and temporal perception": "Grad-cam: Visual explanations from deep networks via gradient-based"
        },
        {
          "[100]\nE. Ghaleb, M. Popa, S. Asteriadis, Multimodal and temporal perception": "localization,\nin: Proceedings of\nthe IEEE international conference on"
        },
        {
          "[100]\nE. Ghaleb, M. Popa, S. Asteriadis, Multimodal and temporal perception": "computer vision, 2017, pp. 618–626."
        }
      ],
      "page": 20
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion, cognition, and decision making",
      "authors": [
        "N Schwarz"
      ],
      "year": "2000",
      "venue": "Cognition & emotion"
    },
    {
      "citation_id": "2",
      "title": "Society of mind, Simon and Schuster",
      "authors": [
        "M Minsky"
      ],
      "year": "1988",
      "venue": "Society of mind, Simon and Schuster"
    },
    {
      "citation_id": "3",
      "title": "Communication without words",
      "authors": [
        "A Mehrabian"
      ],
      "year": "1968",
      "venue": "Psychol. Today"
    },
    {
      "citation_id": "4",
      "title": "Survey on audiovisual emotion recognition: databases, features, and data fusion strategies",
      "authors": [
        "C.-H Wu",
        "J.-C Lin",
        "W.-L Wei"
      ],
      "year": "2014",
      "venue": "APSIPA transactions on signal and information processing"
    },
    {
      "citation_id": "5",
      "title": "Mafw: A large-scale, multi-modal, compound affective database for dynamic facial expression recognition in the wild",
      "authors": [
        "Y Liu",
        "W Dai",
        "C Feng",
        "W Wang",
        "G Yin",
        "J Zeng",
        "S Shan"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "6",
      "title": "A survey of affect recognition methods: Audio, visual, and spontaneous expressions",
      "authors": [
        "Z Zeng",
        "M Pantic",
        "G Roisman",
        "T Huang"
      ],
      "year": "2008",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "7",
      "title": "Learning affective features with a hybrid deep model for audio-visual emotion recognition",
      "authors": [
        "S Zhang",
        "S Zhang",
        "T Huang",
        "W Gao",
        "Q Tian"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "8",
      "title": "End-to-end multimodal emotion recognition using deep neural networks",
      "authors": [
        "P Tzirakis",
        "G Trigeorgis",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2017",
      "venue": "IEEE Journal of selected topics in signal processing"
    },
    {
      "citation_id": "9",
      "title": "Transformer-based multimodal emotional perception for dynamic facial expression recognition in the wild",
      "authors": [
        "X Zhang",
        "M Li",
        "S Lin",
        "H Xu",
        "G Xiao"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "10",
      "title": "Deep learning-based multimodal emotion recognition from audio, visual, and text modalities: A systematic review of recent advancements and future prospects",
      "authors": [
        "S Zhang",
        "Y Yang",
        "C Chen",
        "X Zhang",
        "Q Leng",
        "X Zhao"
      ],
      "year": "2023",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "11",
      "title": "Affective computing: Recent advances, challenges, and future trends",
      "authors": [
        "G Pei",
        "H Li",
        "Y Lu",
        "Y Wang",
        "S Hua",
        "T Li"
      ],
      "year": "2024",
      "venue": "Affective computing: Recent advances, challenges, and future trends"
    },
    {
      "citation_id": "12",
      "title": "A cookbook of self-supervised learning",
      "authors": [
        "R Balestriero",
        "M Ibrahim",
        "V Sobal",
        "A Morcos",
        "S Shekhar",
        "T Goldstein",
        "F Bordes",
        "A Bardes",
        "G Mialon",
        "Y Tian"
      ],
      "year": "2023",
      "venue": "A cookbook of self-supervised learning",
      "arxiv": "arXiv:2304.12210"
    },
    {
      "citation_id": "13",
      "title": "Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2018",
      "venue": "Pre-training of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "14",
      "title": "The biases of pre-trained language models: An empirical study on prompt-based sentiment analysis and emotion detection",
      "authors": [
        "R Mao",
        "Q Liu",
        "K He",
        "W Li",
        "E Cambria"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "15",
      "title": "Skier: A symbolic knowledge integrated model for conversational emotion recognition",
      "authors": [
        "W Li",
        "L Zhu",
        "R Mao",
        "E Cambria"
      ],
      "year": "2023",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "16",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "C Hallacy",
        "A Ramesh",
        "G Goh",
        "S Agarwal",
        "G Sastry",
        "A Askell",
        "P Mishkin",
        "J Clark"
      ],
      "year": "2021",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "17",
      "title": "Masked autoencoders are scalable vision learners",
      "authors": [
        "K He",
        "X Chen",
        "S Xie",
        "Y Li",
        "P Dollár",
        "R Girshick"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "18",
      "title": "Contrastive audio-visual masked autoencoder",
      "authors": [
        "Y Gong",
        "A Rouditchenko",
        "A Liu",
        "D Harwath",
        "L Karlinsky",
        "H Kuehne",
        "J Glass"
      ],
      "year": "2023",
      "venue": "The Eleventh International Conference on Learning Representations"
    },
    {
      "citation_id": "19",
      "title": "Masked audio-video learners",
      "authors": [
        "P.-Y Huang",
        "V Sharma",
        "H Xu",
        "C Ryali",
        "H Fan",
        "Y Li",
        "S.-W Li",
        "G Ghosh",
        "J Malik",
        "C Feichtenhofer",
        "Mavil"
      ],
      "year": "2023",
      "venue": "Thirty-seventh Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "20",
      "title": "A vector quantized masked autoencoder for audiovisual speech emotion recognition",
      "authors": [
        "S Sadok",
        "S Leglaive",
        "R Séguier"
      ],
      "year": "2023",
      "venue": "A vector quantized masked autoencoder for audiovisual speech emotion recognition",
      "arxiv": "arXiv:2305.03568"
    },
    {
      "citation_id": "21",
      "title": "Masked image modeling with local multi-scale reconstruction",
      "authors": [
        "H Wang",
        "Y Tang",
        "Y Wang",
        "J Guo",
        "Z.-H Deng",
        "K Han"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "22",
      "title": "Improving pixelbased mim by reducing wasted modeling capability",
      "authors": [
        "Y Liu",
        "S Zhang",
        "J Chen",
        "Z Yu",
        "K Chen",
        "D Lin"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "23",
      "title": "Werewolf-xl: A database for identifying spontaneous affect in large competitive group interactions",
      "authors": [
        "K Zhang",
        "X Wu",
        "X Xie",
        "X Zhang",
        "H Zhang",
        "X Chen",
        "L Sun"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "24",
      "title": "Avcaffe: A large scale audio-visual dataset of cognitive load and affect for remote work",
      "authors": [
        "P Sarkar",
        "A Posen",
        "A Etemad"
      ],
      "year": "2022",
      "venue": "Avcaffe: A large scale audio-visual dataset of cognitive load and affect for remote work",
      "arxiv": "arXiv:2205.06887"
    },
    {
      "citation_id": "25",
      "title": "Multi-label learning, modality robustness, and semi-supervised learning",
      "authors": [
        "Z Lian",
        "H Sun",
        "L Sun",
        "K Chen",
        "M Xu",
        "K Wang",
        "K Xu",
        "Y He",
        "Y Li",
        "J Zhao"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "26",
      "title": "U-net: Convolutional networks for biomedical image segmentation",
      "authors": [
        "O Ronneberger",
        "P Fischer",
        "T Brox"
      ],
      "year": "2015",
      "venue": "18th International Conference"
    },
    {
      "citation_id": "27",
      "title": "Deep speaker recognition",
      "authors": [
        "J Chung",
        "A Nagrani",
        "A Zisserman"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "28",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "Crema-d: Crowd-sourced emotional multimodal actors dataset"
    },
    {
      "citation_id": "29",
      "title": "Dfew: A large-scale database for recognizing dynamic facial expressions in the wild",
      "authors": [
        "X Jiang",
        "Y Zong",
        "W Zheng",
        "C Tang",
        "W Xia",
        "C Lu",
        "J Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "30",
      "title": "Merbench: A unified evaluation benchmark for multimodal emotion recognition",
      "authors": [
        "Z Lian",
        "L Sun",
        "Y Ren",
        "H Gu",
        "H Sun",
        "L Chen",
        "B Liu",
        "J Tao"
      ],
      "year": "2024",
      "venue": "Merbench: A unified evaluation benchmark for multimodal emotion recognition",
      "arxiv": "arXiv:2401.03429"
    },
    {
      "citation_id": "31",
      "title": "The interspeech 2013 computational paralinguistics challenge: Social signals, conflict, emotion, autism",
      "authors": [
        "B Schuller",
        "S Steidl",
        "A Batliner",
        "A Vinciarelli",
        "K Scherer",
        "F Ringeval",
        "M Chetouani",
        "F Weninger",
        "F Eyben",
        "E Marchi"
      ],
      "year": "2013",
      "venue": "Proceedings INTERSPEECH 2013, 14th Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "32",
      "title": "The geneva minimalistic acoustic parameter set (gemaps) for voice research and affective computing",
      "authors": [
        "F Eyben",
        "K Scherer",
        "B Schuller",
        "J Sundberg",
        "E André",
        "C Busso",
        "L Devillers",
        "J Epps",
        "P Laukka",
        "S Narayanan"
      ],
      "year": "2015",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "33",
      "title": "Dynamic texture recognition using local binary patterns with an application to facial expressions",
      "authors": [
        "G Zhao",
        "M Pietikainen"
      ],
      "year": "2007",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "34",
      "title": "Histograms of oriented gradients for human detection",
      "authors": [
        "N Dalal",
        "B Triggs"
      ],
      "year": "2005",
      "venue": "IEEE computer society conference on computer vision and pattern recognition (CVPR'05)"
    },
    {
      "citation_id": "35",
      "title": "Video-based emotion recognition using cnn-rnn and c3d hybrid networks",
      "authors": [
        "Y Fan",
        "X Lu",
        "D Li",
        "Y Liu"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM international conference on multimodal interaction"
    },
    {
      "citation_id": "36",
      "title": "Multimodal multi-task learning for dimensional and continuous emotion recognition",
      "authors": [
        "S Chen",
        "Q Jin",
        "J Zhao",
        "S Wang"
      ],
      "year": "2017",
      "venue": "Proceedings of the 7th Annual Workshop on Audio/Visual Emotion Challenge"
    },
    {
      "citation_id": "37",
      "title": "Multi-modal continuous dimensional emotion recognition using recurrent neural network and selfattention mechanism",
      "authors": [
        "L Sun",
        "Z Lian",
        "J Tao",
        "B Liu",
        "M Niu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 1st international on multimodal sentiment analysis in real-life media challenge and workshop"
    },
    {
      "citation_id": "38",
      "title": "Multimodal emotion recognition and sentiment analysis via attention enhanced recurrent model",
      "authors": [
        "L Sun",
        "M Xu",
        "Z Lian",
        "B Liu",
        "J Tao",
        "M Wang",
        "Y Cheng"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2nd on Multimodal Sentiment Analysis Challenge"
    },
    {
      "citation_id": "39",
      "title": "Valence and arousal estimation based on multimodal temporal-aware features for videos in the wild",
      "authors": [
        "L Meng",
        "Y Liu",
        "X Liu",
        "Z Huang",
        "W Jiang",
        "T Zhang",
        "C Liu",
        "Q Jin"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "40",
      "title": "Panns: Large-scale pretrained audio neural networks for audio pattern recognition",
      "authors": [
        "Q Kong",
        "Y Cao",
        "T Iqbal",
        "Y Wang",
        "W Wang",
        "M Plumbley"
      ],
      "year": "2020",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "41",
      "title": "Cnn architectures for large-scale audio classification",
      "authors": [
        "S Hershey",
        "S Chaudhuri",
        "D Ellis",
        "J Gemmeke",
        "A Jansen",
        "R Moore",
        "M Plakal",
        "D Platt",
        "R Saurous",
        "B Seybold"
      ],
      "year": "2017",
      "venue": "2017 ieee international conference on acoustics, speech and signal processing"
    },
    {
      "citation_id": "42",
      "title": "Vggface2: A dataset for recognising faces across pose and age",
      "authors": [
        "Q Cao",
        "L Shen",
        "W Xie",
        "O Parkhi",
        "A Zisserman"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE international conference on automatic face & gesture recognition (FG 2018)"
    },
    {
      "citation_id": "43",
      "title": "Learning spatiotemporal features with 3d convolutional networks",
      "authors": [
        "D Tran",
        "L Bourdev",
        "R Fergus",
        "L Torresani",
        "M Paluri"
      ],
      "year": "2015",
      "venue": "Learning spatiotemporal features with 3d convolutional networks"
    },
    {
      "citation_id": "44",
      "title": "Adieu features? end-to-end speech emotion recognition using a deep convolutional recurrent network",
      "authors": [
        "G Trigeorgis",
        "F Ringeval",
        "R Brueckner",
        "E Marchi",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2016",
      "venue": "IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "45",
      "title": "End-to-end continuous emotion recognition from video using 3d convlstm networks",
      "authors": [
        "J Huang",
        "Y Li",
        "J Tao",
        "Z Lian",
        "J Yi"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "46",
      "title": "Ferv39k: A large-scale multi-scene dataset for facial expression recognition in videos",
      "authors": [
        "Y Wang",
        "Y Sun",
        "Y Huang",
        "Z Liu",
        "S Gao",
        "W Zhang",
        "W Ge",
        "W Zhang"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "47",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "48",
      "title": "Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "49",
      "title": "Svfap: Self-supervised video facial affect perceiver",
      "authors": [
        "L Sun",
        "Z Lian",
        "K Wang",
        "Y He",
        "M Xu",
        "H Sun",
        "B Liu",
        "J Tao"
      ],
      "year": "2023",
      "venue": "Svfap: Self-supervised video facial affect perceiver",
      "arxiv": "arXiv:2401.00416"
    },
    {
      "citation_id": "50",
      "title": "Mae-dfer: Efficient masked autoencoder for self-supervised dynamic facial expression recognition",
      "authors": [
        "L Sun",
        "Z Lian",
        "B Liu",
        "J Tao"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "51",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Y.-H Tsai",
        "S Bai",
        "P Liang",
        "J Kolter",
        "L.-P Morency",
        "R Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the conference. Association for Computational Linguistics. Meeting"
    },
    {
      "citation_id": "52",
      "title": "Multimodal transformer fusion for continuous emotion recognition",
      "authors": [
        "J Huang",
        "J Tao",
        "B Liu",
        "Z Lian",
        "M Niu"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "53",
      "title": "Auxformer: Robust approach to audiovisual emotion recognition",
      "authors": [
        "L Goncalves",
        "C Busso"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "54",
      "title": "Ctnet: Conversational transformer network for emotion recognition",
      "authors": [
        "Z Lian",
        "B Liu",
        "J Tao"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "55",
      "title": "A pre-trained audio-visual transformer for emotion recognition",
      "authors": [
        "M Tran",
        "M Soleymani"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "56",
      "title": "Applying segment-level attention on bi-modal transformer encoder for audio-visual emotion recognition",
      "authors": [
        "J.-H Hsu",
        "C.-H Wu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "57",
      "title": "Efficient multimodal transformer with dual-level feature restoration for robust multimodal sentiment analysis",
      "authors": [
        "L Sun",
        "Z Lian",
        "B Liu",
        "J Tao"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "58",
      "title": "Look, listen and learn",
      "authors": [
        "R Arandjelovic",
        "A Zisserman"
      ],
      "year": "2017",
      "venue": "Look, listen and learn"
    },
    {
      "citation_id": "59",
      "title": "Audio-visual scene analysis with self-supervised multisensory features",
      "authors": [
        "A Owens",
        "A Efros"
      ],
      "year": "2018",
      "venue": "Proceedings of the European conference on computer vision (ECCV)"
    },
    {
      "citation_id": "60",
      "title": "Audio-visual instance discrimination with cross-modal agreement",
      "authors": [
        "P Morgado",
        "N Vasconcelos",
        "I Misra"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "61",
      "title": "Masked autoencoders are data-efficient learners for self-supervised video pre-training",
      "authors": [
        "Z Tong",
        "Y Song",
        "J Wang",
        "L Wang",
        "Videomae"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "62",
      "title": "Masked autoencoders as spatiotemporal learners",
      "authors": [
        "C Feichtenhofer",
        "H Fan",
        "Y Li",
        "K He"
      ],
      "year": "2022",
      "venue": "Masked autoencoders as spatiotemporal learners",
      "arxiv": "arXiv:2205.09113"
    },
    {
      "citation_id": "63",
      "title": "Audiovisual masked autoencoders",
      "authors": [
        "M.-I Georgescu",
        "E Fonseca",
        "R Ionescu",
        "M Lucic",
        "C Schmid",
        "A Arnab"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "64",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "65",
      "title": "Masked autoencoders that listen",
      "authors": [
        "P.-Y Huang",
        "H Xu",
        "J Li",
        "A Baevski",
        "M Auli",
        "W Galuba",
        "F Metze",
        "C Feichtenhofer"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "66",
      "title": "Gaussian error linear units (gelus)",
      "authors": [
        "D Hendrycks",
        "K Gimpel"
      ],
      "year": "2016",
      "venue": "Gaussian error linear units (gelus)",
      "arxiv": "arXiv:1606.08415"
    },
    {
      "citation_id": "67",
      "title": "Representation learning with contrastive predictive coding",
      "authors": [
        "A Oord",
        "Y Li",
        "O Vinyals"
      ],
      "year": "2018",
      "venue": "Representation learning with contrastive predictive coding",
      "arxiv": "arXiv:1807.03748"
    },
    {
      "citation_id": "68",
      "title": "Msp-improv: An acted corpus of dyadic interactions to study emotion perception",
      "authors": [
        "C Busso",
        "S Parthasarathy",
        "A Burmania",
        "M Abdelwahab",
        "N Sadoughi",
        "E Provost"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "69",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "70",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "71",
      "title": "Wavlm: Large-scale self-supervised pretraining for full stack speech processing",
      "authors": [
        "S Chen",
        "C Wang",
        "Z Chen",
        "Y Wu",
        "S Liu",
        "Z Chen",
        "J Li",
        "N Kanda",
        "T Yoshioka",
        "X Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "72",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Deep residual learning for image recognition"
    },
    {
      "citation_id": "73",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly"
      ],
      "year": "2020",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "arxiv": "arXiv:2010.11929"
    },
    {
      "citation_id": "74",
      "title": "Former-dfer: Dynamic facial expression recognition transformer",
      "authors": [
        "Z Zhao",
        "Q Liu"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "75",
      "title": "Prompting visual-language models for dynamic facial expression recognition",
      "authors": [
        "Z Zhao",
        "I Patras"
      ],
      "year": "2023",
      "venue": "British Machine Vision Conference (BMVC)"
    },
    {
      "citation_id": "76",
      "title": "Attentive modality hopping mechanism for speech emotion recognition",
      "authors": [
        "S Yoon",
        "S Dey",
        "H Lee",
        "K Jung"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "77",
      "title": "Multimodal speech emotion recognition using audio and text",
      "authors": [
        "S Yoon",
        "S Byun",
        "K Jung"
      ],
      "year": "2018",
      "venue": "IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "78",
      "title": "Is cross-attention preferable to selfattention for multi-modal emotion recognition?",
      "authors": [
        "V Rajan",
        "A Brutti",
        "A Cavallaro"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "79",
      "title": "Emoclip: A vision-language method for zero-shot video facial expression recognition",
      "authors": [
        "N Foteinopoulou",
        "I Patras"
      ],
      "year": "2023",
      "venue": "Emoclip: A vision-language method for zero-shot video facial expression recognition",
      "arxiv": "arXiv:2310.16640"
    },
    {
      "citation_id": "80",
      "title": "Training data-efficient image transformers & distillation through attention",
      "authors": [
        "H Touvron",
        "M Cord",
        "M Douze",
        "F Massa",
        "A Sablayrolles",
        "H Jégou"
      ],
      "year": "2021",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "81",
      "title": "A robustly optimized bert pretraining approach",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal",
        "J Du",
        "M Joshi",
        "D Chen",
        "O Levy",
        "M Lewis",
        "L Zettlemoyer",
        "V Stoyanov"
      ],
      "year": "2019",
      "venue": "A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "82",
      "title": "A closer look at spatiotemporal convolutions for action recognition",
      "authors": [
        "D Tran",
        "H Wang",
        "L Torresani",
        "J Ray",
        "Y Lecun",
        "M Paluri"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "83",
      "title": "Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet?",
      "authors": [
        "K Hara",
        "H Kataoka",
        "Y Satoh"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "84",
      "title": "Clip-aware expressive feature learning for video-based facial expression recognition",
      "authors": [
        "Y Liu",
        "C Feng",
        "X Yuan",
        "L Zhou",
        "W Wang",
        "J Qin",
        "Z Luo"
      ],
      "year": "2022",
      "venue": "Information Sciences"
    },
    {
      "citation_id": "85",
      "title": "Expression snippet transformer for robust video-based facial expression recognition",
      "authors": [
        "Y Liu",
        "W Wang",
        "C Feng",
        "H Zhang",
        "Z Chen",
        "Y Zhan"
      ],
      "year": "2023",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "86",
      "title": "Spatio-temporal transformer for dynamic facial expression recognition in the wild",
      "authors": [
        "F Ma",
        "B Sun",
        "S Li"
      ],
      "year": "2022",
      "venue": "Spatio-temporal transformer for dynamic facial expression recognition in the wild",
      "arxiv": "arXiv:2205.04749"
    },
    {
      "citation_id": "87",
      "title": "Nr-dfernet: Noise-robust network for dynamic facial expression recognition",
      "authors": [
        "H Li",
        "M Sui",
        "Z Zhu"
      ],
      "year": "2022",
      "venue": "Nr-dfernet: Noise-robust network for dynamic facial expression recognition",
      "arxiv": "arXiv:2206.04975"
    },
    {
      "citation_id": "88",
      "title": "Dpcnet: Dual path multi-excitation collaborative network for facial expression representation learning in videos",
      "authors": [
        "Y Wang",
        "Y Sun",
        "W Song",
        "S Gao",
        "Y Huang",
        "Z Chen",
        "W Ge",
        "W Zhang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "89",
      "title": "Intensity-aware loss for dynamic facial expression recognition in the wild",
      "authors": [
        "H Li",
        "H Niu",
        "Z Zhu",
        "F Zhao"
      ],
      "year": "2023",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "90",
      "title": "Rethinking the learning paradigm for dynamic facial expression recognition",
      "authors": [
        "H Wang",
        "B Li",
        "S Wu",
        "S Shen",
        "F Liu",
        "S Ding",
        "A Zhou"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "91",
      "title": "Wenetspeech: A 10000+ hours multi-domain mandarin corpus for speech recognition",
      "authors": [
        "B Zhang",
        "H Lv",
        "P Guo",
        "Q Shao",
        "C Yang",
        "L Xie",
        "X Xu",
        "H Bu",
        "X Chen",
        "C Zeng"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "92",
      "title": "Squeeze-and-excitation networks",
      "authors": [
        "J Hu",
        "L Shen",
        "G Sun"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "93",
      "title": "Learning deep global multi-scale and local attention features for facial expression recognition in the wild",
      "authors": [
        "Z Zhao",
        "Q Liu",
        "S Wang"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "94",
      "title": "Saaml: A framework for semi-supervised affective adaptation via metric learning",
      "authors": [
        "M Tran",
        "Y Kim",
        "C.-C Su",
        "C.-H Kuo",
        "M Soleymani"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "95",
      "title": "Multimodal split attention fusion",
      "authors": [
        "L Su",
        "C Hu",
        "G Li",
        "D Cao",
        "Msaf"
      ],
      "year": "2020",
      "venue": "Multimodal split attention fusion",
      "arxiv": "arXiv:2012.07175"
    },
    {
      "citation_id": "96",
      "title": "A cross-modal fusion network based on self-attention and residual structure for multimodal emotion recognition",
      "authors": [
        "Z Fu",
        "F Liu",
        "H Wang",
        "J Qi",
        "X Fu",
        "A Zhou",
        "Z Li"
      ],
      "year": "2021",
      "venue": "A cross-modal fusion network based on self-attention and residual structure for multimodal emotion recognition",
      "arxiv": "arXiv:2111.02172"
    },
    {
      "citation_id": "97",
      "title": "Exploring wav2vec 2.0 fine tuning for improved speech emotion recognition",
      "authors": [
        "L.-W Chen",
        "A Rudnicky"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "98",
      "title": "Av-superb: A multi-task evaluation benchmark for audio-visual representation models",
      "authors": [
        "Y Tseng",
        "L Berry",
        "Y.-T Chen",
        "I Chiu",
        "H.-H Lin",
        "M Liu",
        "P Peng",
        "Y.-J Shih",
        "H.-Y Wang",
        "H Wu"
      ],
      "year": "2023",
      "venue": "Av-superb: A multi-task evaluation benchmark for audio-visual representation models",
      "arxiv": "arXiv:2309.10787"
    },
    {
      "citation_id": "99",
      "title": "Emotion recognition toolkit (ertk): Standardising tools for emotion recognition research",
      "authors": [
        "A Keesing",
        "Y Koh",
        "V Yogarajan",
        "M Witbrock"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "100",
      "title": "Multimodal and temporal perception of audio-visual cues for emotion recognition",
      "authors": [
        "E Ghaleb",
        "M Popa",
        "S Asteriadis"
      ],
      "year": "2019",
      "venue": "2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "101",
      "title": "Robust audiovisual emotion recognition: Aligning modalities, capturing temporal information, and handling missing features",
      "authors": [
        "L Goncalves",
        "C Busso"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "102",
      "title": "Audio-visual emotion recognition with preference learning based on intended and multi-modal perceived labels",
      "authors": [
        "Y Lei",
        "H Cao"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "103",
      "title": "Tensor fusion network for multimodal sentiment analysis",
      "authors": [
        "A Zadeh",
        "M Chen",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "104",
      "title": "Multimodal attention-mechanism for temporal emotion recognition",
      "authors": [
        "E Ghaleb",
        "J Niehues",
        "S Asteriadis"
      ],
      "year": "2020",
      "venue": "2020 IEEE International Conference on Image Processing"
    },
    {
      "citation_id": "105",
      "title": "Learning cross-modal audiovisual representations with ladder networks for emotion recognition",
      "authors": [
        "L Goncalves",
        "C Busso"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "106",
      "title": "Learning audio-visual speech representation by masked multimodal cluster prediction",
      "authors": [
        "B Shi",
        "W.-N Hsu",
        "K Lakhotia",
        "A Mohamed"
      ],
      "year": "2022",
      "venue": "ternational Conference on Learning Representations"
    },
    {
      "citation_id": "107",
      "title": "Multimodal compact bilinear pooling for visual question answering and visual grounding",
      "authors": [
        "A Fukui",
        "D Park",
        "D Yang",
        "A Rohrbach",
        "T Darrell",
        "M Rohrbach"
      ],
      "year": "2016",
      "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "108",
      "title": "Multimodal transfer module for cnn fusion",
      "authors": [
        "H Joze",
        "A Shaban",
        "M Iuzzolino",
        "K Koishida",
        "Mmtm"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "109",
      "title": "Eranns: Efficient residual audio neural networks for audio pattern recognition",
      "authors": [
        "S Verbitskiy",
        "V Berikov",
        "V Vyshegorodtsev"
      ],
      "year": "2022",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "110",
      "title": "Self-attention fusion for audiovisual emotion recognition with incomplete data",
      "authors": [
        "K Chumachenko",
        "A Iosifidis",
        "M Gabbouj"
      ],
      "year": "2022",
      "venue": "2022 26th International Conference on Pattern Recognition (ICPR)"
    },
    {
      "citation_id": "111",
      "title": "Learning state-aware visual representations from audible interactions",
      "authors": [
        "H Mittal",
        "P Morgado",
        "U Jain",
        "A Gupta"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "112",
      "title": "Parameter efficient multimodal transformers for video representation learning",
      "authors": [
        "S Lee",
        "Y Yu",
        "G Kim",
        "T Breuel",
        "J Kautz",
        "Y Song"
      ],
      "year": "2021",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "113",
      "title": "Deep face recognition",
      "authors": [
        "O Parkhi",
        "A Vedaldi",
        "A Zisserman"
      ],
      "year": "2015",
      "venue": "BMVC 2015-Proceedings of the British Machine Vision Conference 2015, British Machine Vision Association"
    },
    {
      "citation_id": "114",
      "title": "Visualizing data using t-sne",
      "authors": [
        "L Van Der Maaten",
        "G Hinton"
      ],
      "year": "2008",
      "venue": "Journal of machine learning research"
    },
    {
      "citation_id": "115",
      "title": "Transformer interpretability beyond attention visualization",
      "authors": [
        "H Chefer",
        "S Gur",
        "L Wolf"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "116",
      "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
      "authors": [
        "R Selvaraju",
        "M Cogswell",
        "A Das",
        "R Vedantam",
        "D Parikh",
        "D Batra"
      ],
      "year": "2017",
      "venue": "Grad-cam: Visual explanations from deep networks via gradient-based localization"
    }
  ]
}