{
  "paper_id": "2405.16701v1",
  "title": "Detail-Enhanced Intra-And Inter-Modal Interaction For Audio-Visual Emotion Recognition",
  "published": "2024-05-26T21:31:59Z",
  "authors": [
    "Tong Shi",
    "Xuri Ge",
    "Joemon M. Jose",
    "Nicolas Pugeault",
    "Paul Henderson"
  ],
  "keywords": [
    "Audio-visual emotion recognition",
    "Optical flow information",
    "Intra-and Inter-modal modeling",
    "Transformers ùëâ ùêæ"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Capturing complex temporal relationships between video and audio modalities is vital for Audio-Visual Emotion Recognition (AVER). However, existing methods lack attention to local details, such as facial state changes between video frames, which can reduce the discriminability of features and thus lower recognition accuracy. In this paper, we propose a Detail-Enhanced Intra-and Inter-modal Interaction network (DE-III) for AVER, incorporating several novel aspects. We introduce optical flow information to enrich video representations with texture details that better capture facial state changes. A fusion module integrates the optical flow estimation with the corresponding video frames to enhance the representation of facial texture variations. We also design attentive intra-and inter-modal feature enhancement modules to further improve the richness and discriminability of video and audio representations. A detailed quantitative evaluation shows that our proposed model outperforms all existing methods on three benchmark datasets for both concrete and continuous emotion recognition. To encourage further research and ensure replicability, we will release our full code upon acceptance.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion perception is attracting ever-increasing research attention due to its wide range of applications, such as affective computing  [32] , human-computer interaction  [3] , and social robotics  [34] . Multi-modal emotion recognition, especially integrating audio and video (i.e. AVER), is particularly important since it makes use of the information present in two modalities that are vital to human communication. Unlike single-modal emotion recognition, multi-modal emotion recognition has access to different representations of the same emotion from different modalities. This improves feature representation capabilities and distinguishability, leading to improved recognition accuracy  [36, 8] .\n\nHowever, there are still two challenges that are the focus of ongoing research in AVER: (i) how to enhance the representation of fine details within modalities, such as tiny details of facial motion (e.g. due to micro-expressions), and (ii) how »ß # »ß ( »ß )\n\n. . .\n\nPooling Pooling Fig.  1 . Overview of our proposed method DE-III. Given video frames vi and audio fragments ai, we extract features and pass these through separate Conformer encoders. We introduce explicit information about facial motions -captured by optical flow oi -to enhance video feature representations, with a new pair-wise O-V attention fusion module that effectively integrates the information from optical flow and video frames. We propose an inter-modal feature enhancement module (large boxes near top) to attentively fuse the associated audio and video representations in both directions, i.e. audio-to-video and video-to-audio. During training, the final emotion predictions are calculated independently from three sets of features: the video features albeit with audio information fused (i.e. without the model components in the chequered box); the converse using the audio features; and finally using both sets of features after a further fusion stage. During inference, we use the prediction head that performed best on validation data.\n\nto better leverage inter-modal associations to fully exploit the complementary information from different modalities. Solving both will enable learning better feature representations, and improve emotion recognition accuracy\n\nWhen learning features from one modality, intra-modal temporal relationship mining  [44, 11, 42]  and feature detail enhancement  [39]  are important ways to make features more discriminative. For instance,  [42]  proposed an adaptive graph attention network to explore the relationship between frames of videos for microexpression recognition, while  [39]  introduced optical flow to replace face images for micro-expression recognition based on a multi-scale feature representation. However, these methods focus on the single-modal setting, and cannot exploit information from multiple modalities.  [44]  used self-attention  [37]  within each modality to enhance their representation and then fused them by a linear-based function to classify; however this cannot fully account for the complex, nonlinear relationships between audio and video.\n\nMulti-modal approaches have recently become mainstream  [24, 28, 41]  since considering both audio and video further improves representations, by fusing information in associated video frames and audio fragments. For example,  [8]  explored the effectiveness of different variants of transformer-based inter-modal attention mechanisms for AVER and showed inter-modal interaction can significantly improve performance. However, although inter-modal interaction improves recognition, these methods do not investigate modeling temporal relationships within each modality.  [13]  adopts a multi-branch joint auxiliary training method, designing independent audio and video branches and multi-modal fusion to enhance feature relationships, which greatly improves recognition performance.  [16, 15]  used a network shared across modalities to encourage consistency of the multi-modal feature space. However, since different modalities have different feature distributions and properties, a shared network may not fully capture the unique characteristics of each modality, resulting in information loss.\n\nMost of the relationship modeling strategies mentioned above  [35, 17, 16, 15 ] model temporal relationships based on implicit appearance representation of video frames and audio fragments, but ignore an inherent challenge of AVER -that in video features the frame-to-frame variations of faces are much weaker than in audio. For example, there may be significant changes in content and intonation between two audio fragments, while there is little difference between video frames. It is clear that these missing explicit details, especially state changes between face frames of videos, may lead to reduced discriminability of feature representations during the relationship modeling process, thereby affecting the accuracy of AVER.\n\nWe address these issues by introducing a multi-modal interaction network (Figure  1 ) that incorporates an explicit representation of visual detail changes between frames, and which can better fuse the complementary information from video and audio. Different from methods that directly model relationships between local regions of a facial sequence  [30, 12, 19, 10] , optical flow is a simple and effective way to represent the state changes between the facial frames. Optical flow can enhance the discriminability of visual representations by directly highlighting significant detail differences between frames, especially those texture changes that can express facial emotions  [39] . To this end, we propose a novel detail-enhanced intra-and inter-modal interactions network (called DE-III) for AVER, which integrates explicit optical flow information into an end-to-end multi-modal interaction framework. In addition, two independent multi-modal interaction fusion mechanisms and multiple residual connections further alleviate the information loss problem in existing shared interaction strategies  [16, 15] . Our main contributions are as follows:\n\nwe explicitly capture detail changes between video frames using optical flow, and integrate this information using a lightweight attentive fusion module; we design novel detail-enhanced intra-and inter-modal interaction modules for the video and audio modalities, which can effectively fuse associated information of one modality into the other modality and reduce information loss by residual connections.\n\nWe evaluate the resulting model and several variants on three widely used benchmarks and obtain highly competitive results including a new state-of-the-art on multiple metrics, e.g. 83.7% F1-Micro score on CREMA-D, 82.7% accuracy on RAVDESS and the highest scores on MSP-IMPROV with 89.3%, 88.7% and 85.8% for valence, arousal and dominance.",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "Related Work",
      "text": "Emotion recognition has received a significant amount of attention in the computer vision community. Numerous methods  [38, 43, 31, 22]  have been proposed to solve this task by using different data modalities, such as images, speech and text. These methods can be divided into two main kinds: unimodal methods (that input just one modality), and multi-modal methods (that input two or more modalities). Our proposed DE-III belongs to the latter category, combining audio and video modalities to improve the performance of emotion recognition.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Unimodal Emotion Recognition",
      "text": "Unimodal emotion recognition methods  [38, 1, 43, 31, 22]  focus on application scenarios where only one kind of data is available; they design feature enhancement and interaction methods based on the inherent properties of the corresponding modality. The most common methods are text-based [40,9,1] and image-based  [38, 43, 31] . For example for text,  [1]  present a BERT-based model to explore the importance of context extraction in texts for emotion recognition. One work by  [33]  proposed one sequence-based convolutional neural network to detect human emotion from big data. However, it is harder to to accurately predict human emotions from a text transcription compared to using richer modalities such as images or videos. For image data,  [31]  proposed feature decomposition and reconstruction learning for effective facial image expression recognition.  [27]  introduced the image depth information to improve the context information of images, which improved the representation capability and thus recognition accuracy. Moving to video,  [2]  introduced facial micro-expression analysis methods that can improve emotion recognition by capturing richer contextual sequence information than static images. Although unimodal emotion recognition has achieved substantial progress and delivers promising results, it is inherently limited by having less information available than multi-modal approaches.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Multi-Modal Emotion Recognition",
      "text": "Recently, multi-modal emotion recognition has become mainstream  [26, 7, 13, 17, 15, 16, 8, 14, 36]  due to its ability to fully exploit the complementary information present in different modalities. For instance,  [8]  explored the effectiveness of different variants of transformer-based inter-modal attention mechanisms for audio-video emotion recognition and showed that inter-modal interaction can significantly improve performance.  [25]  showed that combining audio and with a corresponding text transcription improves the representation ability of features, since audio captures details of intonation, while text captures semantics more explicitly. Moreover,  [7]  fused three modalities (audio, text and vision), further improving recognition accuracy. The above works indicate that combining multiple modalities can significantly enhance the discrimination ability of fused representations and thus the recognition performance. In this work, we study multimodal-based emotion recognition, specifically for audio-video emotion recognition (AVER). The most similar works to ours are  [16, 15] , both of which used a transformer-based architecture that is shared across video and audio modalities to encourage consistency of the multi-modal feature space. However, their proposed shared network cannot fully capture the unique feature distributions of each modality, such as explicit facial state changes between face video frames, resulting in the loss of information during the multimodal relationship modeling process. Unlike  [13, 14, 36] , which adopt attention-based neural network to effectively process and integrate audio modalities, our model not only learns the intra-relationships within video feature representations but also models the inter-relationships when attentively fuses the audio representation. Our proposed model augments video features with optical flow information before fusing with the audio features. Unlike traditional methods  [21]  that directly combine the optical flow features with visual representations, we use Conformer  [17]  networks to extract context-aware features, and design a novel pairwise O-V attention fusion module to combine them.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Proposed Model",
      "text": "The overall framework of our proposed model DE-III is shown in Figure  1 . We first extract video and audio features, then enhance their representative power through temporal relationship modelling within their respective modalities, also fusing optical flow information with the video features to better capture detail changes. Then, the inter-modal feature enhancement module performs attentionweighted fusion of each modality's information with the other modality.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Audio Self-Enhancement Module",
      "text": "To represent the information in audio, we use a pre-trained wav2vec model  [18]  to embed the extracted audio fragments. Specifically, we split a given audio clip into a sequence of m fragments A = {a 1 , a 2 , . . . , a m } using a sliding window.\n\nThen we use the wav2vec-large-robust model to extract corresponding fragmentlevel representations »¶ = { »ß1 , »ß2 , . . . , »ßm }. Next, a Conformer encoder  [17]  (a transformer-based model with convolutions to improve temporally-local information processing) is used to obtain enhanced audio-fragment representations ƒÄ = {ƒÅ 1 , ƒÅ2 , . . . , ƒÅm } that account for (intra-modal) local and global temporal relationships.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Video Pairwise Attention Enhancement Module",
      "text": "Different from the audio features where contextual semantics are clear, i.e. there is clear semantic content and significant intonation changes, in the video, subtle yet important changes in facial texture tend to be lost during feature extraction.\n\nWe therefore use a pre-trained optical flow model  [20]  to extract the flow o i between adjacent pairs of video frames {v i-1 , v i }, where i ‚àà {1, . . . , n} and n is the number of video frames; this can explicitly represent fine-grained changes of facial texture such as micro-expressions. Then, we employ the widely-used EfficientNet-B2 model  [32] , which has been fine-tuned on VGGface2  [5]  dataset, to extract representations for video frames and their corresponding optical flow maps; we denote these features by V = { v1 , v2 , . . . , vn } and »Æ = { »Ø1 , »Ø2 , . . . , »Øn } respectively. To further enhance the representational ability of these visual features, we use two independent Conformer encoders  [17]  to embed them into the same dimensional space as the audio modality. This also allows for subsequent intermodal interaction. We next propose a simple and efficient pairwise O-V attention fusion module to combine the features of frames and optical flow into a joint embedding space. Specifically, we use a fully-connected (FC) layer to map the features at each time-point to two channels, then apply a softmax function  [6]  and interpret these values as weights for the frame and flow features respectively.\n\nWe finally obtain the detailed-enhanced video representation ov i by a weighted sum of linearly-projected frame features and corresponding flow features. Thus, we set\n\nwhere [ : ] denotes concatenation along the channel dimension, W o and W v are the linear projection parameters, and\n\nWe refer to the two conformers followed by the OV-fusion as the pair-wise attention enhancement (PAE) module.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Inter-Modal Feature Enhancement Module",
      "text": "Inspired by the attention mechanisms  [41, 13] , we next design an inter-modal feature enhancement module (IFE) that allows each modality to attend to the other and integrate relevant information. For simplicity we describe only the audio-tovideo fusion (IFE-Video); however a similar approach is used for video-to-audio. We want to allow the enhanced video frame features ov i to attend to features of relevant audio fragments ƒÄ = {ƒÅ 1 , ƒÅ2 , . . . , ƒÅm }. Different from traditional selfattention  [37]  and cross-attention  [36] , we take the target video frame ov i as the query to calculate the attention weights, with the audio fragment defining the keys and values after the linear projections. Attentive fusion from another modality allows relevant modality information to be extracted and integrated, thereby improving the distinguishability of target modality representation. Finally, we obtain the video representations √ñV = { √∂ v i } after IFE by adding a residual connection, and passing through a feed-forward block (FFB) which contains two linear layers. In summary, we set\n\nwhere W ov , W a and Wa are linear projection parameters. Similarly, we obtain the attention-aware video fragment representations of each audio fragment and combine them with an audio residual operation to give the final audio representations √Ñ = {√§ 1 , √§2 , ..., √§m }.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Feature Aggregation And Objective Function",
      "text": "Since we want to make a single prediction for an entire video, we max-pool the features along the temporal axis, yielding a video-centric feature vector √∂v * from √ñV , and audio-centric feature vector √§ * from √Ñ (note that √∂v * still incorporates information fused from the audio modality as described in Section 3.3, and viceversa). We use three independent emotion prediction heads (each a multi-layer perceptron) with corresponding losses to jointly optimize different branches the model -the video-cross loss L V (using √∂v * as input to the MLP), audio-cross loss L A (using √§ * ) and audio-visual fusion loss L F (using √∂v * concatenated with √§ * ). The overall objective function is the sum of the three losses. We use multi-class cross-entropy for datasets with discrete emotion class labels, and concordance correlation coefficient (CCC) for datasets with continuous labels. Specifically, CCC is given by\n\nwhere ¬µ x and ¬µ y are the mean of the predicted result ≈∑ and the label y, respectively, œÉ x and œÉ y are their standard deviations, and œÅ is their Pearson correlation coefficient (a œÅ value close to ¬±1 suggests a strong linear relationship, while a value of 0 signifies the absence of any linear correlation). During inference we can use predictions from any of the three heads; for our main experiments we use the prediction head that performed best on the validation data.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experimental Setup",
      "text": "Datasets and Metrics. To verify the effectiveness of our proposed approach, we evaluate it on three popular AVER datasets: CREMA-D  [4] , MSP-IMPROV  [3]     [36, 8, 16, 14]  we use only the continuous labels. We adhered to the protocol in  [8, 13] , with 5 separate folds where each fold divides the data into training, validation, and test sets with non-overlapping actor identities. We evaluate based on the most commonly-used metrics for each dataset -F1-Macro and F1-Micro for CREMA-D  [4] , Accuracy for RAVDESS  [23]  and CCC for MSP-IMPROV  [3] .\n\nImplementation Details. All models were trained for up to 20 epochs using early stopping on the validation set, and we report our results on the test set. We choose hyper-parameters based on validation set performance. We use AdamW for optimization with a learning rate of 5 √ó 10 -6 and weight decay of 5 √ó 10 -2 . The face images are extracted from each frame of every video clip and resized to 224 √ó 224 pixels. We generate optical flow maps using  [20]  and normalize their magnitude by a standard deviation calculated from the local optical flow magnitude at every pixel position within an entire video clip. We use the pre-trained EfficientNet-B2 from  [32]  to extract features from the video frames and optical flow maps. The audio features are extracted using wav2vec2-large-robust  [18] . Separate Conformer encoders for video and audio map the extracted features to vectors of 1408-dimension each. Each Conformer block has a hidden dimensionality of 512, with 8 attention heads. The number of blocks in the acoustic, visual, and optical flow Conformers were set to 3, 3, and 2, respectively. For the prediction heads, we use MLPs with hidden dimensionality of 512. Our IFE module (Section 3.3) uses single-head attention  [37]  with the linear feed-forward block and the highlighted fusion feature dimensions remain unchanged. Our model was implemented in PyTorch and trained on 2 NVIDIA RTX A5000 GPUs, taking 1 hour.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Quantitative Comparison",
      "text": "In Table  1  we present quantitative results for our method and several existing works: 1) Multi  [36] , a transformer-based cross-modal attention fusion method; 2) MMER  [8] , with multiple self-attention fusion mechanisms; 3) UAVM  [16] , a transformer-based feature enhancement model with a shared audio-visual encoder; 4) AuxFormer  [13] , a transformer framework with two independent auxiliary branches; 5) LADDER  [14] , a transformer-based cross-attention framework with auxiliary reconstruction tasks. We see that compared with the previous best method LADDER  [14]  on CREMA-D, our DE-III achieves higher performance in terms of F1-Micro score, 83.7% vs. 80.3%. On MSP-IMPROV, our DE-III attains excellent CCC values of 89.3% for valence (Val.), 88.7% for arousal (Aro.), and 85.8% for dominance (Dom.), establishing a new state-of-the-art for this dataset. Moreover, we also achieve a better accuracy (Acc.) score on RAVDESS compared with the SOTA method, 82.7% for DE-III vs. 81.6% for MMER.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Ablation Studies",
      "text": "In this section, we evaluate the performance benefit due to various components and design decisions in our model.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Effects Of Inter-Modal Feature Enhancement (Ife).",
      "text": "In the IFE block, we define video attending to audio as V-cross and audio attending to video as A-cross. We first experiment with removing the IFE module (i.e. without any inter-modality fusion, only RGB images and flow maps, denoted None-IFE). In Table  2 , we see a large performance drop in this setting -compared with the best output (from IFE-Video), the F1-Macro and F1-Micro scores decrease by 3.7% and 5.1% on the CREMA-D test set, respectively. This suggests that intermodality fusion plays an important role in improving AVER capabilities. Recall that our model has three prediction heads: IFE-Audio (i.e. using features √§ * ), Effects of Video Pairwise Attention Enhancement (PAE) Module. To demonstrate our ablations on pair-wise attention enhancement (PAE) Module, we categorize different settings as \"Fuse when?\", \"Visual input\", \"sequential model\", and \"Fuse how?\". Results on CREMA-D are given in Table  3 , all using the IFE-Video prediction head. We first present results when trained with only one part of the video information, i.e. RGB images only (IFE-V-F), or optical flow maps only (IFE-V-O). We see that IFE-V-O achieves 55.4% F1-macro and 64.9% F1-micro. The result shows optical flow information present low capability to distinguish emotions, and it is much weaker than using RGB images only. When combining optical flow maps with RGB images in the full model (IFE-Video), there is a remarkable performance improvement vs. IFE-V-F. It indicates that the flow maps indeed augment the video feature representations. Next, we replace our PAE with one single conformer followed by one OV-fusion block. To pass the image and optical flow features together into the conformer, we attempt several alternative operations-temporal concatenation (IFE-V-FOSC), channelwise concatenation (IFE-V-FODC), and summation (IFE-V-FODS). We see (Table  3 ) that our PAE module achieves the highest recognition performance, with 1.0% improvement over IFE-V-FOSC on F1-macro and 1.1% improvement over IFE-V-FODC on F1-Micro. These observations indicate that our PAE module is a more effective fusion method for combining visual features and optical flow features. Finally, we explore early fusion and late fusion strategies. We find that by moving OV-fusion block before the Conformer (IFE-V-Early), accuracy decreases slightly vs. having OV-fusion after the Conformer (IFE-Video), by 0.3% F1-Macro and 0.7% F1-Micro. We hypothesise that this is because the additional computation performed beforehand by the Conformer is beneficial in helping the OV-fusion module to determine whether to focus on image or flow information for each time-point. Additionally, we compare our method by replacing the conformer to the vanilla transformer  [37] , the accuracy decreases slightly by 1.6% and 1.1%, this demonstrates that the conformer is superior to the vanilla transformer at the image level in capturing changes in facial details from feature representations.\n\nEffects of optical-flow extraction variants. We next experiment with using different sliding window lengths and strides when extracting the optical flow from the videos. Firstly, we vary the window length while keeping the stride fixed to 1 (i.e. moving frame by frame). Secondly, we vary both the window length and the stride together (i.e. non-overlapping windows). The results in Table  4  show that using a window length of 1 with a stride of 1 performs best. Increased window lengths, with fixed or increasing strides, show consistent drops in performance, with the worst-performing variant having window length of 5 and stride of 1 (achieving 74.3% F1-Macro, versus 79.5% for window length and stride of 1). This indicates that temporally-fine-grained information is valuable in increasing the accuracy of emotion recognition. We also experiment with using a different backbone feature extractor for the optical flow, since face images and flow-maps are quite different domains. We choose DINOv2  [29] , which has been shown to be robust across many image domains, and fix the window length and stride to 1 (i.e. the best-performing setting). However, we find it performs worse than using EffcientNet pre-trained on a large face images dataset, dropping from 79.5% to 76.8% F1-Macro and from 83.7% to 82.6% F1-Micro.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Qualitative Analysis",
      "text": "To better understand the behavior of our model, we visualize the inter-modal fusion weights Œ± ij for IFE-Audio and IFE-Video (see Section 3.3) in Figure  2 . The brightness of each location in the heatmap represents the strength with which the modality on the horizontal axis is attending to that on the vertical axis, at that particular time-point. The pattern of attention varies considerably for different points along the horizontal axis, showing that the model does not attend to fixed, specific points in the other modality, but adapts depending on the current features, and presumably the varying emotional states depicted in the video. Notably, the heatmaps do not exhibit a bright diagonal line; this indicates that time-points generally attend not to the corresponding time-point in the other modality, but to other (presumably relevant or informative) timepoints. Overall these results suggest that our inter-modal feature enhancement module can selectively fuse the useful information from each modality into the other.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Conclusion",
      "text": "We have presented a new model, DE-III, for audio-visual emotion recognition, which combines intra-and inter-model feature enhancement in a unified framework. DE-III introduces a pair-wise attention fusion method that integrates explicit facial detail changes between video frames, captured by optical flow. It not only improves the distinguishability of features within each visual modality, but also further increases the effectiveness of subsequent inter-modal feature interactions. Our results demonstrate that DE-III enhances emotion recognition by optimally fusing the information available in different modalities. Indeed, our model achieves state-of-the-art performance on three popular datasets, for both concrete and continuous emotion labels.",
      "page_start": 12,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview of our proposed method DE-III. Given video frames vi and audio",
      "page": 2
    },
    {
      "caption": "Figure 1: ) that incorporates an explicit representation of visual detail changes",
      "page": 3
    },
    {
      "caption": "Figure 2: Heatmaps showing inter-modality attention weights calculated by IFE-Audio",
      "page": 12
    },
    {
      "caption": "Figure 2: The brightness of each location in the heatmap represents the strength with",
      "page": 12
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ùëâ ùêæ": "Pro."
        }
      ],
      "page": 2
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Transformer models for textbased emotion detection: a review of bert-based approaches",
      "authors": [
        "F Acheampong",
        "H Nunoo-Mensah",
        "W Chen"
      ],
      "year": "2021",
      "venue": "Artificial Intelligence Review"
    },
    {
      "citation_id": "2",
      "title": "Videobased facial micro-expression analysis: A survey of datasets, features and algorithms",
      "authors": [
        "X Ben",
        "Y Ren",
        "J Zhang",
        "S Wang",
        "K Kpalma",
        "W Meng",
        "Y Liu"
      ],
      "year": "2021",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "3",
      "title": "MSP-IMPROV: An acted corpus of dyadic interactions to study emotion perception",
      "authors": [
        "C Busso",
        "S Parthasarathy",
        "A Burmania",
        "M Abdelwahab",
        "N Sadoughi",
        "E Provost"
      ],
      "year": "2016",
      "venue": "Trans. Affect. Comput"
    },
    {
      "citation_id": "4",
      "title": "CREMA-D: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "Trans. Affect. Comput"
    },
    {
      "citation_id": "5",
      "title": "VGGFace2: A dataset for recognising faces across pose and age",
      "authors": [
        "Q Cao",
        "L Shen",
        "W Xie",
        "O Parkhi",
        "A Zisserman"
      ],
      "year": "2018",
      "venue": "International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "6",
      "title": "Attention-based models for speech recognition",
      "authors": [
        "J Chorowski",
        "D Bahdanau",
        "D Serdyuk",
        "K Cho",
        "Y Bengio"
      ],
      "year": "2015",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "7",
      "title": "M2fnet: Multi-modal fusion network for emotion recognition in conversation",
      "authors": [
        "V Chudasama",
        "P Kar",
        "A Gudmalwar",
        "N Shah",
        "P Wasnik",
        "N Onoe"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "8",
      "title": "Self-attention fusion for audiovisual emotion recognition with incomplete data",
      "authors": [
        "K Chumachenko",
        "A Iosifidis",
        "M Gabbouj"
      ],
      "year": "2022",
      "venue": "Self-attention fusion for audiovisual emotion recognition with incomplete data"
    },
    {
      "citation_id": "9",
      "title": "A survey of textual emotion recognition and its challenges",
      "authors": [
        "J Deng",
        "F Ren"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "10",
      "title": "Algrnet: Multi-relational adaptive facial action unit modelling for face representation and relevant recognitions",
      "authors": [
        "X Ge",
        "J Jose",
        "P Wang",
        "A Iyer",
        "X Liu",
        "H Han"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Biometrics, Behavior, and Identity Science"
    },
    {
      "citation_id": "11",
      "title": "Mgrr-net: Multi-level graph relational reasoning network for facial action unit detection",
      "authors": [
        "X Ge",
        "J Jose",
        "S Xu",
        "X Liu",
        "H Han"
      ],
      "year": "2024",
      "venue": "ACM Transactions on Intelligent Systems and Technology"
    },
    {
      "citation_id": "12",
      "title": "Local global relational network for facial action units recognition",
      "authors": [
        "X Ge",
        "P Wan",
        "H Han",
        "J Jose",
        "Z Ji",
        "Z Wu",
        "X Liu"
      ],
      "year": "2021",
      "venue": "Local global relational network for facial action units recognition"
    },
    {
      "citation_id": "13",
      "title": "AuxFormer: Robust approach to audiovisual emotion recognition",
      "authors": [
        "L Goncalves",
        "C Busso"
      ],
      "year": "2022",
      "venue": "ICASSP"
    },
    {
      "citation_id": "14",
      "title": "Learning cross-modal audiovisual representations with ladder networks for emotion recognition",
      "authors": [
        "L Goncalves",
        "C Busso"
      ],
      "year": "2023",
      "venue": "ICASSP"
    },
    {
      "citation_id": "15",
      "title": "Versatile audiovisual learning for handling single and multi modalities in emotion regression and classification tasks",
      "authors": [
        "L Goncalves",
        "S Leem",
        "W Lin",
        "B Sisman",
        "C Busso"
      ],
      "year": "2023",
      "venue": "Versatile audiovisual learning for handling single and multi modalities in emotion regression and classification tasks",
      "arxiv": "arXiv:2305.07216"
    },
    {
      "citation_id": "16",
      "title": "Uavm: Towards unifying audio and visual models",
      "authors": [
        "Y Gong",
        "A Liu",
        "A Rouditchenko",
        "J Glass"
      ],
      "year": "2022",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "17",
      "title": "Conformer: Convolution-augmented transformer for speech recognition",
      "authors": [
        "A Gulati",
        "J Qin",
        "C Chiu",
        "N Parmar",
        "Y Zhang",
        "J Yu",
        "W Han",
        "S Wang",
        "Z Zhang",
        "Y Wu"
      ],
      "year": "2021",
      "venue": "ICASSP"
    },
    {
      "citation_id": "18",
      "title": "Robust wav2vec 2.0: Analyzing domain shift in self-supervised pre-training",
      "authors": [
        "W Hsu",
        "A Sriram",
        "A Baevski",
        "T Likhomanenko",
        "Q Xu",
        "V Pratap",
        "J Kahn",
        "A Lee",
        "R Collobert",
        "G Synnaeve"
      ],
      "year": "2021",
      "venue": "Robust wav2vec 2.0: Analyzing domain shift in self-supervised pre-training",
      "arxiv": "arXiv:2104.01027"
    },
    {
      "citation_id": "19",
      "title": "A spatio-temporal integrated model based on local and global features for video expression recognition",
      "authors": [
        "M Hu",
        "P Ge",
        "X Wang",
        "H Lin",
        "F Ren"
      ],
      "year": "2021",
      "venue": "The Visual Computer"
    },
    {
      "citation_id": "20",
      "title": "Perceiver IO: A general architecture for structured inputs & outputs",
      "authors": [
        "A Jaegle",
        "S Borgeaud",
        "J Alayrac",
        "C Doersch",
        "C Ionescu",
        "D Ding",
        "S Koppula",
        "D Zoran",
        "A Brock",
        "E Shelhamer"
      ],
      "year": "2021",
      "venue": "Perceiver IO: A general architecture for structured inputs & outputs",
      "arxiv": "arXiv:2107.14795"
    },
    {
      "citation_id": "21",
      "title": "Automatic facial expression recognition under partial occlusion based on motion reconstruction using a denoising autoencoder",
      "authors": [
        "A Kemmou",
        "A El Makrani",
        "I El Azami",
        "M Aabidi"
      ],
      "year": "2024",
      "venue": "Indonesian Journal of Electrical Engineering and Computer Science"
    },
    {
      "citation_id": "22",
      "title": "Eeg based emotion recognition: A tutorial and review",
      "authors": [
        "X Li",
        "Y Zhang",
        "P Tiwari",
        "D Song",
        "B Hu",
        "M Yang",
        "Z Zhao",
        "N Kumar",
        "P Marttinen"
      ],
      "year": "2022",
      "venue": "ACM Computing Surveys"
    },
    {
      "citation_id": "23",
      "title": "The Ryerson audio-visual database of emotional speech and song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "24",
      "title": "An end-to-end learning approach for multimodal emotion recognition: Extracting common and private information",
      "authors": [
        "F Ma",
        "W Zhang",
        "Y Li",
        "S Huang",
        "L Zhang"
      ],
      "year": "2019",
      "venue": "An end-to-end learning approach for multimodal emotion recognition: Extracting common and private information"
    },
    {
      "citation_id": "25",
      "title": "Multimodal emotion recognition based on deep temporal features using cross-modal transformer and self-attention",
      "authors": [
        "B Maji",
        "M Swain",
        "R Guha",
        "A Routray"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "26",
      "title": "M3er: Multiplicative multimodal emotion recognition using facial, textual, and speech cues",
      "authors": [
        "T Mittal",
        "U Bhattacharya",
        "R Chandra",
        "A Bera",
        "D Manocha"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "27",
      "title": "Emoticon: Context-aware multimodal emotion recognition using frege's principle",
      "authors": [
        "T Mittal",
        "P Guhan",
        "U Bhattacharya",
        "R Chandra",
        "A Bera",
        "D Manocha"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "28",
      "title": "C-GCN: Correlation based graph convolutional network for audio-video emotion recognition",
      "authors": [
        "W Nie",
        "M Ren",
        "J Nie",
        "S Zhao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "29",
      "title": "Learning robust visual features without supervision",
      "authors": [
        "M Oquab",
        "T Darcet",
        "T Moutakanni",
        "H Vo",
        "M Szafraniec",
        "V Khalidov",
        "P Fernandez",
        "D Haziza",
        "F Massa",
        "A El-Nouby"
      ],
      "year": "2023",
      "venue": "Learning robust visual features without supervision",
      "arxiv": "arXiv:2304.07193"
    },
    {
      "citation_id": "30",
      "title": "Facial expression recognition in videos using dynamic kernels",
      "authors": [
        "N Perveen",
        "D Roy",
        "K Chalavadi"
      ],
      "year": "2020",
      "venue": "Trans. Image Process"
    },
    {
      "citation_id": "31",
      "title": "Feature decomposition and reconstruction learning for effective facial expression recognition",
      "authors": [
        "D Ruan",
        "Y Yan",
        "S Lai",
        "Z Chai",
        "C Shen",
        "H Wang"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "32",
      "title": "Facial expression and attributes recognition based on multi-task learning of lightweight neural networks",
      "authors": [
        "A Savchenko"
      ],
      "year": "2021",
      "venue": "Facial expression and attributes recognition based on multi-task learning of lightweight neural networks"
    },
    {
      "citation_id": "33",
      "title": "An effective approach for emotion detection in multimedia text data using sequence based convolutional neural network",
      "authors": [
        "K Shrivastava",
        "S Kumar",
        "D Jain"
      ],
      "year": "2019",
      "venue": "Multimedia tools and applications"
    },
    {
      "citation_id": "34",
      "title": "Emotion recognition for human-robot interaction: Recent advances and future perspectives",
      "authors": [
        "M Spezialetti",
        "G Placidi",
        "S Rossi"
      ],
      "year": "2020",
      "venue": "Frontiers in Robotics and AI",
      "doi": "10.3389/frobt.2020.532279"
    },
    {
      "citation_id": "35",
      "title": "Self-attention for speech emotion recognition",
      "authors": [
        "L Tarantino",
        "P Garner",
        "A Lazaridis"
      ],
      "year": "2019",
      "venue": "Self-attention for speech emotion recognition"
    },
    {
      "citation_id": "36",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Y Tsai",
        "S Bai",
        "P Liang",
        "J Kolter",
        "L Morency",
        "R Salakhutdinov"
      ],
      "year": "2019",
      "venue": "ACL"
    },
    {
      "citation_id": "37",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "≈Å Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "38",
      "title": "Suppressing uncertainties for largescale facial expression recognition",
      "authors": [
        "K Wang",
        "X Peng",
        "J Yang",
        "S Lu",
        "Y Qiao"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "39",
      "title": "Micro-expression recognition based on multi-scale attention fusion",
      "authors": [
        "M Wang"
      ],
      "year": "2021",
      "venue": "Micro-expression recognition based on multi-scale attention fusion"
    },
    {
      "citation_id": "40",
      "title": "Emotion correlation mining through deep learning models on natural language text",
      "authors": [
        "X Wang",
        "L Kou",
        "V Sugumaran",
        "X Luo",
        "H Zhang"
      ],
      "year": "2020",
      "venue": "IEEE transactions on cybernetics"
    },
    {
      "citation_id": "41",
      "title": "Msa-gcn: Multiscale adaptive graph convolution network for gait emotion recognition",
      "authors": [
        "Y Yin",
        "L Jing",
        "F Huang",
        "G Yang",
        "Z Wang"
      ],
      "year": "2023",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "42",
      "title": "Adaptive graph attention network with temporal fusion for micro-expressions recognition",
      "authors": [
        "Y Zhang",
        "H Wang",
        "Y Xu",
        "X Mao",
        "T Xu",
        "S Zhao",
        "E Chen"
      ],
      "year": "2023",
      "venue": "Adaptive graph attention network with temporal fusion for micro-expressions recognition"
    },
    {
      "citation_id": "43",
      "title": "Relative uncertainty learning for facial expression recognition",
      "authors": [
        "Y Zhang",
        "C Wang",
        "W Deng"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "44",
      "title": "Exploring emotion features and fusion strategies for audio-video emotion recognition",
      "authors": [
        "H Zhou",
        "D Meng",
        "Y Zhang",
        "X Peng",
        "J Du",
        "K Wang",
        "Y Qiao"
      ],
      "year": "2019",
      "venue": "Exploring emotion features and fusion strategies for audio-video emotion recognition"
    }
  ]
}