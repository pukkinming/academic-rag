{
  "paper_id": "2211.09273v4",
  "title": "Privacy Against Real-Time Speech Emotion Detection Via Acoustic Adversarial Evasion Of Machine Learning",
  "published": "2022-11-17T00:25:05Z",
  "authors": [
    "Brian Testa",
    "Yi Xiao",
    "Harshit Sharma",
    "Avery Gump",
    "Asif Salekin"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Smart speaker voice assistants (VAs) such as Amazon Echo and Google Home have been widely adopted due to their seamless integration with smart home devices and the Internet of Things (IoT) technologies. These VA services raise privacy concerns, especially due to their access to our speech. This work considers one such use case: the unaccountable and unauthorized surveillance of a user's emotion via speech emotion recognition (SER). This paper presents DARE-GP, a solution that creates additive noise to mask users' emotional information while preserving the transcription-relevant portions of their speech. DARE-GP does this by using a constrained genetic programming approach to learn the spectral frequency traits that depict target users' emotional content, and then generating a universal adversarial audio perturbation that provides this privacy protection. Unlike existing works, DARE-GP provides: a) real-time protection of previously unheard utterances, b) against previously unseen black-box SER classifiers, c) while protecting speech transcription, and d) does so in a realistic, acoustic environment. Further, this evasion is robust against defenses employed by a knowledgeable adversary. The evaluations in this work culminate with acoustic evaluations against two off-the-shelf commercial smart speakers using a small-form-factor (raspberry pi) integrated with a wake-word system to evaluate the efficacy of its real-world, real-time deployment. CCS Concepts: â€¢ Computing methodologies â†’ Supervised learning by classification; Genetic programming; â€¢ Security and privacy â†’ Privacy-preserving protocols; â€¢ Human-centered computing â†’ Sound-based input / output.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Motivation: Smart speaker voice assistants (VAs) such as Amazon Echo and Google Home have been widely adopted due to their seamless integration with smart home devices and the Internet of Things (IoT) technologies  [18, 95, 103] . However, VA services raise privacy concerns, especially due to their access to our speech, which big tech companies may leverage for monetization, a practice called surveillance capitalism  [17, 56, 121] . In the context of smart speaker VA use, users have no right or control over their speech recordings once the command speech is uploaded to the cloud  [52, 117] . One possible use for this speech data is surveillance of users' emotional data.\n\nLarge tech companies have demonstrated interest in capturing affect information from speech through both explicit service offerings (e.g. -Amazon Halo  [52] ), and through research and registered patents for technologies developed for affect assessment from speech acoustic, including Amazon  [38, 48] , Microsoft  [89] , Apple  [53] , and Spotify  [106] . This investment in speech emotion recognition (SER) indicates two things: 1) these companies have a motivation for collecting users' emotion data, and 2) these companies cannot simply inspect the transcripts of our smart speaker VA interactions to get this information. This second point is supported by a study published by Mozilla and Yahoo  [12] , the primary uses of the Google Home and Amazon Alexa, i.e., smart speaker VAs are task commands, and conversational interactions are relatively rare (< 10%). This point is further supported by a short analysis of the emotional content in the transcripts of common smart speaker commands. The results of this analysis are provided in Appendix A.\n\nAs for companies' motivation to collect this information, the work of Lerner et al.  [61]  and many others  [37, 51, 58] , emotion has a profound impact upon decision making. Authors at The Atlantic noted these trends in speech emotion recognition patents amongst technology companies and came to a reasonable conclusion: these technologies could be applied for targeted advertising based upon a user's emotions  [38] . Such exploitation of users' emotional information is not innocuous. Studies have shown that consumers' affective states and stress positively affect their impulsive  [85]  and compulsive  [84, 120]  buying behaviors. Notably, increasing impulsive buying may cause compulsive buying disorder  [27, 36] , which may result in substantial debts, legal problems, and personal distress  [55, 110] . Furthermore, recent studies have shown  [13]  that personalized advertisement positively affects impulse buying; hence it can be assumed that personalized advertisements based on users' affective states or stress would exploit the vulnerability of individuals suffering from such disorders and may even contribute to causing them.\n\nUnauthorized, unaccountable exposure of emotion information is a real concern beyond just commercial use cases. In government, affective computing is beginning to influence law enforcement activities  [43, 70, 86] . In recent years, United States law enforcement has dramatically increased subpoenas for user interaction recordings from smart speaker companies to assess individuals' mental states  [92, 112] . These law enforcement use cases are especially troubling in the context of significant bias and fairness concerns in affective state recognition  [57, 74, 87, 116] , which may create disparity against the minority populations.\n\nMany individuals are concerned by the exploitation of their private data by companies or the government. In the US  [14]  and the UK  [70, 76]  many have significant reservations about sharing smart speaker VA audio with external stakeholders such as advertisers and law-enforcement agencies. In the UK, about 52% of the population has chosen not to partake in VA services due to privacy concerns  [76] . In the United States, concerns regarding the potential abuses of affective computing  [32, 52] , have escalated as high as the United States Senate. On the world stage, the United Nations Human Rights Council has weighed in  [76] , acknowledging \"...emotion recognition technologies as an emergent priority in the global right to privacy in the digital age\".\n\nThe purpose of this work is to empower smart speaker VA users to protect their private emotion information. Put another way, this work addresses the question: Can users exploit the utility of smart speaker VA services while limiting the inadvertent disclosure of emotional information depicted through speakers' acoustic attributes? Any answer to this question must address all of the following research questions:\n\n(1) Given a set of users, can an approach deceive a (i) previously unseen black-box SER classifier (ii) without compromising the speech-to-text transcription on (iii) previously unheard utterances (i.e., commands)? (2) How does the performance of this approach compare with state-of-the-art (SotA) audio evasion techniques?\n\n(3) Can a knowledgeable SER operator defend against this technique? (4) Can such protection be granted in an acoustic, real-world scenario with: closed, off-the-shelf (OTS) smart speakers, variable user location related to the smart speaker, and space weight and power (SWAP)  [94]  constraints?\n\nA 'closed' smart speaker is one that does not provide an API for audio processing before the audio is uploaded to the cloud (discussed in Section 2). This is a common characteristic across many commercial smart speakers, including Amazon Echo  [65, 107] .\n\nApproach: This paper presents an approach called Defeating Acoustic Recognition of Emotion via Genetic Programming (DARE-GP). Given a set of users, DARE-GP uses constrained genetic programming (GP) to generate a universal adversarial audio perturbation (UAAP)  [2] . For the respective users' speech (even for previously unheard VA commands, i.e., not used during UAAP generation), that single UAAP causes misclassification on previously-unseen black-box SER classifiers while maintaining audio transcription utility (i.e., the constraint); these universal spectral perturbations are called Emotion Obfuscating Noises (EONs). DARE-GP approach is motivated by the insights of Weninger et al.  [111]  and BÄƒlan et al.  [16] , establishing the presence of the spectralemotional content relationship in speech and SER classifiers' utilization of such relationship in emotion detection. Furthermore, studies  [5, 7, 8]  have shown that the paralinguistic emotional content can be disentangled from the linguistic transcription content in speech, and that SER inference from spectral information of emotionally neutral utterances is quite feasible  [15, 31, 44, 47, 67, 72] . Leveraging constrained GP for a target set of users, DARE-GP identifies the spectral attributes through which they most commonly depict their speech emotions and are not related to the depiction of linguistic transcription content. Finally, it generates a single EON that masks such spectral attributes. Hence, while playing simultaneously with users' speech (even the previously unheard ones), the respective EON masks speakers' emotional content while preserving its transcription utility.\n\nAn effective, real-world emotion protection mechanism should demonstrate all of the following attributes:\n\nâ€¢ Audio Utility Preservation -If a solution disrupts the transcription of audio, then it is not a usable solution; it would make the smart speaker effectively useless. DARE-GP's EON development explicitly conserves the smart speaker's ability to transcribe user audio by imposing constraints on genetic programming. â€¢ Transferable to Previously Unseen Black-Box Classifiers -As described in Section 2, DARE-GP does not have query access  [29]  to the target SER classifier, making it impossible to directly develop a surrogate SER classifier, mimicking the target one. DARE-GP generates noise in the spectral domain, which intrinsically makes an EON transferable to previously unseen SER classifiers without modification. â€¢ Supports Closed, OTS Smart Speaker VA Interaction -The closed nature of smart speaker processing  [65]  precludes changes to hardware-implemented features like beamforming or software-based audio processing before the audio is provided to cloud services for processing  [107] ; this limits the scope of possible approaches to deceive a back-end SER classifier. An EON is played as additive noise simultaneously with user speech; this supports real-world integration without requiring any specific smart speaker/VA-specific APIs, interfaces, etc. â€¢ Real-Time for Previously Unheard Utterances -Smart speakers' appeal is their ease of use; introducing latency to or requiring replay of user commands would significantly degrade a user's smart speaker experience.\n\nAs discussed above, DARE-GP precomputes a single EON, which is universal in nature for a set of users. Meaning, this additive noise (i.e., EON) can be played in real-time to cause misclassification of previously unheard utterances without any new, on the fly, utterance-specific processing (similar to  [64] ). â€¢ Effective in an Acoustic Environment -A solution cannot assume feature-level access to the SER model; as described in Section 2, interaction with the smart speaker is limited to existing, user-facing interfaces. EONs are developed explicitly for acoustic environments, and do not suffer the issues encountered when attempting to manifest feature-space-developed evasive samples in an acoustic setting  [63, 88]  (see Section 6.2). â€¢ Robust versus Knowledgeable Defender -A solution should assume that the SER operator will have knowledge of its implementation and will attempt to perform accurate emotion inference despite the solution; details about such a defender's expected knowledge and resources are provided in Section 2. EONs have been demonstrated to be robust versus multiple possible defenses (see Section 6.3). â€¢ Noninvasive -Any solution should not be disruptive to a user's daily life. The impact of an EON to a user is discussed in Section 6.4. In addition, in a recent user survey 89.5% of respondents indicated that the additive noise generated by DARE-GP was either Noninvasive or Completely Inaudible. Additional details of this survey are provided in Section 7 and Appendix D. While there are multiple state-of-the-art (SotA) audio evasion approaches, all of them are lacking in one or more of the areas discussed above. Details comparing DARE-GP to SotA audio evasion techniques are provided in Section 8. DARE-GP's ability to simultaneously satisfy all of these attributes makes it the first approach to be deployable in a real-world environment. Figure  1  shows this deployment scenario where a DARE-GP device is used in tandem with a smart speaker to play a pre-generated EON acoustically in real-time.\n\nContributions: DARE-GP is a first-of-its-kind work to deceive previously unseen black-box SER classifiers while preserving the transcription-relevant content of the speech (Section 6.1). Unlike previous GP-based audio evasion works  [10, 63, 102, 114] , DARE-GP: a) performed constrained optimization to protect audio transcription during evasion, and b) DARE-GP used GP to generate Universal Adversarial Audio Perturbations (UAAPs)  [2]  that can be played as additive noise that causes misclassification of previously unheard utterances in real-time. Further, by leveraging the strong connection between spectral components of speech and emotional content (backed by discussed above literature), DARE-GP evasions transfer to previously unseen black box models without requiring query access to the models, and without any a priori knowledge of the models' features, topologies, or class labels. Extensive evaluations demonstrate DARE-GP's: superior performance against state-of-the-art SER evasion techniques (Section 6.2), robustness versus a knowledgeable adversary (Section 6.3), and performance in a realworld, over-air deployment scenario, in terms of both SER evasion and transcription utility protection using two off-the-shelf smart speakers. (Section 6.4).\n\nThe remainder of this paper is organized as follows: Section 2 -Attack Model, Section 3 -Problem Statement, Section 4 -Technical Approach, Section 5 -Datasets, Section 6 -Evaluation, Section 7 -Discussion, Section 8 -Related Works, and Section 9 -Conclusion.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Attack Model",
      "text": "The following sections provide the context under which DARE-GP was developed, deployed, and evaluated.\n\nâ€¢ Attack Goal: DARE-GP's goal is to degrade inference of a user's emotional state based upon the spectral components of that user's interactions with a smart speaker's voice assistant (VA). This obfuscation of the user's emotional state should not come at the expense of usability of the smart speaker's VA; the solution must: (1) minimize transcription errors, and (2) support real-time, on-demand use.\n\nâ€¢ Target System: A black-box SER classifier operated by the smart-speaker VA provider, whose purpose is to infer a user's emotional state based upon the spectral components (not the transcript) of a user's speech.\n\nâ€¢ Proposed System's Access Level: DARE-GP did not require any privileged access to the smart speaker; it could not rely upon any smart speaker side-channels, undocumented hardware features, or custom hardware/firmware updates. Further, the initial audio processing on the smart speaker was treated as a 'closed' system; any custom code/skills on the smart speaker could not preempt upload of the audio to the cloud. In addition, DARE-GP made no assumptions about the black-box SER classifier; DARE-GP's interactions with the SER classifier were limited to the same audio channel used for user command processing. DARE-GP could not rely upon any access to the black-box SER classifier to train a surrogate classifier  [22, 23, 46] . In addition, DARE-GP could not assume a specific feature representations, topology or set of class labels.\n\nâ€¢ Knowledgeable Adversary: DARE-GP was developed considering that an SER provider would be considered an adversary who wanted to ensure that the SER classifier's emotion inference was correct. As the SER provider, this adversary would have access to audio samples mixed with an EON but not access to the user's unmodified speech, nor would they have access to ground truth labels for the user's speech. This is important because, without this level of access, the SER provider would not be able to use adversarial training  [41]  to defend against DARE-GP. Further, EONs are different for different households, i.e., targeted set of users (see Section 4 and Section 7), we assumed that the adversary did not have access to the precise parameters of the EON. DARE-GP's robustness against various SotA audio evasion defenses are presented in Section 6.3.\n\nâ€¢ Assumptions: The primary assumption was that the SER classifier made emotion inference based upon the spectral information of speech. There were two reasons for this assumption. First, if a user's interaction transcript did contain emotion information, there is no solution without interfering with the smart speaker's primary functionality: execution of commands. Secondly, smart-speaker command transcripts do not typically contain emotion information  [12] ; Appendix A contains the results of an experiment that validate this assumption.\n\nAnother important assumption during DARE-GP development related to the speech-to-text implementation used by the smart speaker. Since many smart speakers do not provide APIs to access command transcripts, DARE-GP could not rely upon the smart speaker's speech-to-text utilities. DARE-GP was developed using an open-source vosk/kaldi  [80]  transcription service which we assumed would be less tolerant of noise than the commercially-supported speech-to-text implementations within smart speakers. This assumption proved to be true, as DARE-GP was significantly more successful when evaluated against real smart speakers (Section 6.4).\n\nâ€¢ Outcome of the Attack: The desired outcome of DARE-GP's attack was degraded SER prediction accuracy of the user's smart-speaker interactions while preserving transcription accuracy. Further, this outcome should be robust versus defenses deployed by a knowledgeable SER Provider.\n\nâ€¢ Non-Invasivenes: DARE-GP uses additive noise to degrade SER inference; such perturbations had the potential to be invasive if they were too loud, or if the frequencies used had other negative consequences. For example, frequencies outside of human hearing have been used in previous audio evasion works  [119] ; regular exposure to frequencies beyond the range of human hearing can cause health-related side effects  [60] . To address this, DARE-GP's audio perturbations were constrained to typical speech frequency ranges and were constrained on amplitude to prevent impacts on the runtime environment.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Problem Statement",
      "text": "To formalize the research questions described in the introduction: Given: A smart speaker executing speech-to-text transcription system S and a black-box SER classifier ð¶, within environment E. As is a common practice, the SER classifier ð¶ has been trained to improve classification performance on user population U  [99] .\n\nGenerate: A single Emotion Obfuscating Noise (EON), P, that increases the misclassification rate of C on utterances generated by users in U within the confines of E without impacting the effectiveness of S. Definitions of some important terms in this section are provided in Table  1 . The Technical Approach (Section 4) and Evaluation (Section 6) address the following challenges associated with answering the research questions:\n\nâ€¢ Implement a constrained optimization approach that, given a surrogate SER classifier C* and a digital environment ÃŠ, develops an EON PË†that does not degrade the effectiveness of S (constrain) and increases misclassification in C* that transfers to black-box SER classifiers C (optimization objective). Details of the approach to address this are provided in Section 4 and empirical evaluation demonstrating the effectiveness of the generated EON is presented in Section 6.1. â€¢ Given a SER Provider with knowledge of DARE-GP, assess their ability to infer the true class of user utterances perturbed by PË†. Details regarding these defenses and their effectiveness are provided in Section 6.3. â€¢ Assess the efficacy of a digitally-developed EON PË†when played in the original real environment E; that is, assess PË†â†’ P. Extensive evaluations are provided in Section 6.4.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Technical Approach",
      "text": "EON Description DARE-GP uses GP to generate an EON. Sound waveforms have three basic physical attributes: frequency, amplitude, and temporal variation  [30] . Following the natural sound characteristic, each EON is a mixture of ð‘ different tones, where each tone has a different frequency, corresponding amplitude, and temporal variation through different start-time and duration (Table  1 ). Since EONs are hearable sound, they are additive in nature; meaning can be played simultaneously with the target users' ð‘ˆ 's speech in real-time to inject noise. One of the most important characteristics of EONs is that they are universal within the set of users ð‘ˆ . A single EON can be used to alter the utterances from the set of users; separate EONs are not required for new utterances. The number of tones ð‘ of EON is a hyperparameter derived empirically in our evaluation.\n\nHow Does an EON Cause Misclassification? To explain how an EON cause misclassification we leverage a machine learning prediction explanation technique: Kernel SHAP  [69] . Kernel SHAP approximates the conditional expectations of Shapley values (i.e., the importance of input attributes) in deep learning models by perturbing different portions of the input and observing the impact on output classes. Since we are interested in the spectral attributes of speech that depict emotional information to an SER classifier, the Shapley values are calculated by masking specific frequency bands to measure significance.\n\nFigure  2  visualizes the spectral attributes on which an SER classifier focuses to make an inference and how DARE-GP spectral perturbation inject noise on those attributes to cause misclassification. Figure  2A   spectrogram for a user utterance, and the associated Kernel SHAP explanation  [69]  for the prediction of class label: angry. The blue bars to the right and left indicate frequency bands that contribute the correct and incorrect label, respectively. In this instance, the SER classifier correctly predicted a class label of angry. According to the explanation, the frequency bands corresponding to the second (F2) and fourth (F4) formants and high-frequency information (above 4k Hz) are important for correct emotion prediction, which is in line with literature  [33, 93] . Figure  2B  is the spectrogram of a DARE-GP-generated spectral perturbation composed of three tones. Each tone represents a frequency, an amplitude, and a duration, which injects noise in a specific frequency band of the target set of users' speech through which they commonly depict emotion. The number of tones is a hyperparameter that we determined through empirical analysis. Each tone's frequency, amplitude, and duration are learnable parameters that we learned through GP. Notably, Figure  2B 's tones correspond to the above-discussed important frequency bands for SER's correct classification.\n\nFinally, Figure  2C  shows the spectrogram of the user utterance when played simultaneously with the spectral perturbation and again presented to the SER classifier. Notably, according to the Kernel SHAP explanation regarding the angry class, the frequency bands affected by the spectral perturbation strongly correlate with misclassification. These frequency bands were positively impacting SER's correct angry class prediction (rightside bars in Figure  2A ); however, due to the injection of noise, they are negatively impacting SER's angry class prediction (changed to left-side bars in Figure  2C ), causing misclassification to happy.\n\nUse of Genetic Programming (GP): Using constrained GP, DARE-GP can develop an EON for a target set of users that causes misclassification while constraining them to limit transcription errors. This is done by incorporating both evasion and transcription accuracy into the fitness function used in the GP evolution process. Most importantly, EON development does not rely on the specifics of the feature representation used by a target SER classifier. Rather than trying to replicate a specific SER classifier's gradients, the GP fitness function guides the evolution of perturbations that mask frequency bands through which a given set of users depict their emotional information. By injecting noise in a target users' emotion-relevant spectral traits the EON a) transfers to a diverse set of previously unseen black-box SER classifiers, b) is universal, i.e., effective on previously unseen new utterance, and c) functions well acoustically.\n\nGenetic Programming (GP) Workflow: Figure  3  shows the high-level GP approach used to generate an EON. The approach requires a labeled set of audio samples from the users. There are two constraints on this dataset: samples need to be correctly classified by the surrogate SER classifier, and the transcription service needs to be able to correctly extract text transcripts from these samples. Naturally misclassified samples are not Fig.  3 . Genetic Programming Adversarial Evasion Workflow interesting because they are already misclassified. Likewise, audio samples that cannot be transcribed correctly are not useful because they do not support constraining the EONs to limit transcription errors.\n\nAs the first step of GP workflow in Figure  3 , a population of individuals (Table  1 ), each of which has the parameters necessary to generate an EON, is initialized. These individuals are then assigned fitness scores by generating their EONs, combining those EONs with audio samples from the training set to generate potentially evasive audio samples, and then assessing the fitness of those samples with respect to the classifier C* and the utility of those samples with respect to the transcription system S. A new set of individuals, i.e., a new population is generated by selecting the strongest (i.e., higher fitness) individuals and creating new variations from them through crossover and mutation. This process is iterated over multiple generations (i.e., GP iterations), with each generation's individuals generating slightly better EONs (i.e., having higher fitness).\n\nSpecific details of the GP operations are provided below:\n\nFitness -The fitness calculation ranks each individual in the population. For DARE-GP, this ranking is based upon the ability of an EON to mislead the surrogate SER classifier C*, and its ability to do so without compromising the underlying audio's transcription (see Equations 1-3).\n\nSelection -This step selects a subset of individuals from a population to carry forward into the next generation before crossover and mutation. Selection is performed using a tournament selection method  [54]  with a tournament size of ð‘›ð‘†ð‘’ð‘™. This guaranteed that at least the ð‘›ð‘†ð‘’ð‘™ -1 weakest individuals were eliminated from each generation. Crossover -In GP crossover, offspring \". . . are created by exchanging the genes of parents among themselves\"  [73] . This step is a method to generate new individuals (i.e., offspring) from previous selected ones (i.e., parents), thus generating new EONs, by combining the parameters of two existing individuals to create two new individuals. Crossover is performed by exchanging EON-parameters between the selected parents  [73]  with probability ð‘ð¶ð‘‹ . Mutation -Used to prevent population stagnation  [83] . New individuals are generated by randomly modifying select individuals' EON-generation-parameters. Mutation introduces the greatest amount of variability in the population; a given individual can undergo any number of changes, leading to significant improvement or degradation. It is performed by randomly shuffling EON parameters (scaled to [0,1]) with probability ð‘ð‘€ð‘ˆ ð‘‹ .\n\nFinal EON Selection -After iterating for ð¾ generations, the EON with the highest evasion success rate (ESR) when mixed with the validation dataset.\n\nHyperparameters -ð‘›ð‘†ð‘’ð‘™, ð‘ð¶ð‘‹ , and ð‘ð‘€ð‘ˆ ð‘‹ are hyperparameters, identified empirically through grid-search.\n\nImportant questions pertaining to the approach are answered below:\n\nâ€¢ Are EONs developed with access to the black-box SER classifiers? No. The development of EONs exclusively leverages C*, a surrogate SER classifier created for this purpose. It is important to emphasize that, while the topology and weights of this classifier are available during EON development, the only privileged characteristics of C* used are the scores from its softmax layer during fitness evaluation.\n\nâ€¢ How are EONs constrained to prevent degraded transcription? As mentioned previously, DARE-GP addresses a constrained optimization problem where SER classifier deception needs to balance against audio utility. The balance is maintained by incorporating a transcription correctness score (Equation  2 ) into the EON fitness function (Equation  3 ). DARE-GP used an open-source audio transcription library backed with a kaldi  [80]  speech model to calculate the transcription correctness score. DARE-GP was developed under the assumption that a commercial transcription solution would be at least as robust to background noise as the open-source implementation used for these experiments. This assumption was validated with the experiments in Section 6.4.\n\nâ€¢ How is the EON fitness calculated? The fitness score measures the relative efficacy of individuals in a population and is calculated using a fitness function. For DARE-GP, this fitness function is composed of two components: a deception score and a transcription score. The deception score, given in Equation  1 , measures the extent to which the EON generated by an individual fools the surrogate classifier C*.\n\nwhere: The first term of the deception score is the mean decrease in the actual class' score from the softmax in our surrogate classifier. The second term is a bonus applied for each misclassification; the purpose of this term is to prioritize increasing the number of misclassifications rather than increasing the confidence of existing misclassifications. The bonus value used in this work was 50 and was found empirically using a grid search. The transcription score in Equation 2 penalizes transcription errors:\n\nwhere: The transcription score goes to 0 if any of the audio samples when mixed with an individual's EON, cannot be transcribed correctly. In addition, if any of the transcription confidence scores fall below a specified confidence threshold, then the transcription score goes to 0. Otherwise, the score is 1 minus the mean confidence decrease as a result of applying the EON to audio samples in S. Confidence increases never occurred during the experiments for this work, but to accommodate this possibility a confidence increase would be replaced by 0 when calculating this mean. The fitness score is calculated as the product of the deception(ind) which is a real number, and the transcription(ind) which is a real number in [0, 1], where a higher value indicates better preservation of the audio transcripts.\n\nTaking the product of the deception and transcription scores allows for incremental improvements to classifier deception while harshly penalizing transcription errors. This approach was taken due to an obvious complication; a sufficiently loud EON could drown out the speech in the audio samples, thus providing near-perfect evasive quality while rendering the audio effectively useless.\n\nThis fitness function exclusively uses the surrogate SER classifier C*. In our evaluation, the surrogate SER classifier is a simple DNN that demonstrates state-of-the-art performance using Mel-frequency cepstral coefficients (MFCCs) as the feature representation; MFCCs are a widely used feature in many audio classification use cases  [19, 26, 34, 68, 82, 113] . Additional details of this classifier are provided in Table  4 .\n\nâ€¢ How will DARE-GP generate an EON for a set of users in a real environment? As shown in Figure  1 , the DARE-GP system will include a microphone that listens for a wake word and a speaker that will physically touch the smart speaker and play the 'final EON' once the wake word is heard. By touching the smart speaker, the DARE-GP speaker can play the EON with minimal loudness (i.e., minimally invasive) since the EON acoustic will propagate both via air and physical (i.e., smart speaker material) modalities.\n\nThe real-world deployment process is presented in Figure  4 . As the initial step (step A), DARE-GP will digitally train/develop a set of EONs on existing independent datasets (i.e., canonical data). These are \"factory default\" generic EONs without any tailoring for the target users U and target environment E. By initializing these EONs, the amount of in-home fine-tuning is significantly reduced. To adapt the final single EON for the target users U, DARE-GP will record some speech samples from them in the target environment E (details provided in Appendix C) and digitally fine-tune the \"factory default\" generic EONs to the users' speech through 'ð‘—' iterations/generations of the GP (step B). The final generation of GP will result in 'ð‘¡' EONs with the highest fitness in the digital environment. However, to identify which of the final 't' EONs perform best over-air for the environment E, DARE-GP plays and records the 'ð‘¡' EONs with different loudnesses (step C) and again digitally evaluates their fitness with the users' U's recorded speech. Finally (step D), the highest fitness EON-and-loudness pair out of all possible candidates is selected as the EON for the target users U to deploy for the target environment E.\n\nStep E is the evaluation step of the final EON by simultaneously playing it with users' speech (i.e., acoustic mixing). As discussed above, the EON to propagates via conductance  [108] ; since the EON is not presented to the smart speaker microphone from only one direction, the built-in beamforming is not able to single out the EON (i.e., noise).",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Datasets",
      "text": "The experiments described in this paper were performed using two standard audio datasets: the Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)  [67] , and the Toronto Emotional Speech Set (TESS)  [31]  datasets. Details of both of these datasets are provided in Table  2 .\n\nThe spoken part of the RAVDESS dataset (no sung content) consists of 1, 440 samples from 24 speakers. The speakers were actors, split evenly between males and females. The speakers performed two separate utterances while demonstrating 8 different emotions. The subset of the TESS dataset used in this paper consisted of 1800 audio samples generated by two actresses. These samples spanned five emotions (neutral, angry, happy, sad, and fearful) and included 200 utterances. This dataset was utilized in assessing the extensibility of an EON to multiple, previously unheard utterances.\n\nThe counts of audio samples for these datasets are provided in Table  2 , but not all of these samples were usable for the experiments in this work. Before performing any experiments, specific samples were removed if a) the speech-to-text library was unable to extract a correct transcript or b) the surrogate SER classifier misclassified the sample before any EONs were applied. This process ensured that EONs were not penalized for audio samples that did not normally transcribe correctly, and evasion scores were not inflated due to natural misclassifications from the surrogate classifier.\n\nTwo additional datasets were implicitly used in this work: Interactive Emotional Dyadic Motion Capture (IEMOCAP)  [20]  and Surrey Audio-Visual Expressed Emotion (SAVEE)  [47] . These datasets were part of the training datasets for several of the black-box SER models used in the evaluations of DARE-GP. These datasets were not used during EON development or evaluation.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Evaluation",
      "text": "The following experiments address the following research questions:\n\n(1) Given a set of users, can an approach deceive a (i) previously unseen black-box SER classifier (ii) without compromising the speech-to-text transcription on (iii) previously unheard utterances? (Section 6.1) (2) How does the performance of this approach compare with SotA audio evasion techniques? (Section 6.2) (3) Can a knowledgeable SER operator defend against this technique? (Section 6.3) The previous research questions were evaluated digitally; user utterances and EONs were mixed in code. The following question considers the real-world over-air deployment of digitally-generated EONs.\n\n(4) Can such protection be granted in an acoustic, real-world scenario with: closed, off-the-shelf (OTS) smart speakers, variable user location related to the smart speaker, and SWAP  [94]  constraints? (Section 6.4) Fig.  5 . A high-level view of how the datasets from Section 5 were used in these evaluations. All RAVDESS data was used for EON training, as was a 10% slice of TESS data. An additional 10% of TESS data was reserved for additional training on any black-box classifiers that underperformed on the original TESS data. The remaining 80% of TESS data was used for evaluation. All TESS data splits were balanced between the two speakers in the dataset.\n\nTo evaluate the success of DARE-GP at answering these questions, the following metrics were used:\n\nEvasion Success Rate (ESR): The fraction of evaluation samples that both fool the target SER classifier and transcribe correctly. This metric ensures that a solution both protects emotional privacy and utility of the audio modified by the solution.\n\nFalse Label Independence: Relationship between an utterance's actual emotion label and the fake label caused by DARE-GP. For example, if calm audio samples, when perturbed, always resulted in samples with the false emotion happy, it would be trivial to discern the true emotion. This metric assesses the strength of the relationship between the true and the false emotion/class. Two information theoretic measures are used for this metric: Normalized Mutual Information (NMI)  [35]  and Matthews Correlation Coefficient (MCC)  [24] .\n\nDataset Split for Black Box Evaluation: All evaluations performed in the following sections used EONs that were trained on RAVDESS data for 40 generations, tailored on 10% of the TESS data for 10 generations, and evaluated using the 80% of TESS data dedicated for evaluation (see Figure  5 ). RAVDESS was used to train \"factory default\" EONs without any insight into the target environment or users. The TESS data (the portion used to train & select the EON) acted as user-supplied audio samples to tailor the \"factory default\" EONs to the target users.\n\nNotably, the black box SER classifiers that DARE-GP aims to deceive, are taken from git-repositories or trained by us on a third independent dataset, IEMOCAP  [20] . The other 10% of TESS data was reserved for further training black box SER classifiers, if their performance was too low without any TESS data fine-tuning, which is a known limitation of SER models  [99] . Such a split ensures, the EON development process, Black box SER training process, and evaluation process use completely disjoint data; prohibiting any data leakage.\n\nAll splits on TESS data were balanced with respect to the two speakers in the dataset, and were based upon utterance ID; utterances were disjoint between the EON training, evaluation and SER training sets. Such a split ensures that the evaluations are performed on previously unseen utterances/commands.\n\nMetaparameters: The metaparameter values (discussed in Section 4) are provided in Table  3 . These parameters were found empirically based on the desired level of diversity and EON effectiveness. The EON Frequency Range was set to limit EON frequencies to ranges within the typical human speech. The EON amplitude range was determined based on some initial assessments of EON loudness on transcription scores. The range of amplitude values was limited to protect transcription accuracy and prevent EONs from becoming irritating to users. The EON duration and offset were introduced to prevent EONs from being a single, monotonous tone. This was empirically demonstrated to be more effective and would be more difficult to detect. Similar to the perturbations in  [64] , EONs did not require precise alignment with user utterances in order to generate adversarial samples. Further refinement of the metaparameters presented here is left as future work.\n\nMost importantly, all metaparameter values were derived without any access to the black-box SER classifiers and these values were invariant across all of the evaluations presented in this work. This consistency prevented the introduction of any hidden bias based on knowledge of the black-box classifiers' inner workings.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Evasion Success And Transferability",
      "text": "This section evaluates the effectiveness of the EON developed utilizing a surrogate SER classifier C* and evaluated against (1) previously-unseen, black-box classifiers, C on (2) previously unheard utterances. As discussed above, the EON was not tailored for the black-box classifiers in any way. The details of these black-box SER classifiers are provided in Table  4 . The measure of effectiveness used was ESR (Section 2). These classifiers used different combinations of Mel-frequency cepstral coefficients (MFCCs)  [78] , mean Chroma  [90]  and mean Mel-scaled spectrogram  [97]  values for their features. In addition, the class labels and network topologies were different between classifiers. Since DARE-GP is oblivious to these details, application to these other classifiers was completely transparent. The results of applying the EON is presented in Table  5 . It is important to note that the ESR was calculated on samples that the classifiers originally classified correctly.\n\nThe ESR against the SEC, TDNN, wav2vec, and RESNET classifiers were particularly significant; these classifiers used Mel-scaled spectrograms for their feature representation and were the least similar to the surrogate classifier in both topology and feature representation. These results underscore the appeal of DARE-GP; since EON is generated to create spectral noise, it is transferable to previously unseen SER classifiers and unheard utterances.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Comparison With Other Evasion Attacks",
      "text": "The evaluations in this section compare DARE-GP with state-of-the-art audio evasion attacks. Recently, Gao et al.  [39]  evaded SER classifiers using three spectral envelope attacks: Vocal Tract Length Normalization (VTLN)  [59] , the McAdams transformation  [75] , and Modulation Spectrum Smoothing (MSS)  [101] . To compare gradientbased approach's efficacy for this paper's attack model, evaluations were also performed using two white-box, gradient-based attacks: Fast Gradient Sign Method (FGSM)  [41]  and Projected Gradient Descent (PGD)  [71] . It is important to note that none of these attacks are executable in a real-time, acoustic environment (see Section 8.3); hence are not the true baseline or competitors of this paper. However, during the time of writing, these were the most comparable attacks. The evasion results are provided in Table  5 .\n\nExisting literature  [104, 105]  shows that the speech spectral attributes, such as the third and fourth formants convey the emotional content of speech, and the three spectral envelope attacks distort the spectral attributes, including temporal content and the formant information; hence can deceive the SER classifiers. However, these transformations are generic and applied to all frequency ranges and harmonics, distorting speech's transcription utility as well, resulting in a relatively low ESR. Moreover, spectral envelope attacks perform relatively well on SER classifiers that take spectrogram directly as input (i.e., SEC, TDNN, RESNET, and wav2vec). They are less effective on the classifiers that apply MFCC on the acoustic data (i.e., DNN, Flair, SEA, CNN-MAA); this transformation filters out some of the distorted information during the feature extraction phase. In contrast, DARE-GP only introduces additive noise on the specific spectral aspects (through generated tones for specific frequencies and amplitudes), which convey emotional information and does not interrupt the speech transcription-relevant Table  5 . ESR values for various attacks against the black-box SER classifiers. VTLN, McAdams, and MSS are universal adversarial attacks performed on all audio samples proposed by Gao, et. al.  [39] . FGSM and PGD attacks were performed as usual, generating bespoke transformations for each audio sample. Multiple values of ðœ– were evaluated; ðœ– = 0.01 was the optimal value for both attacks. The DARE-GP ESR was calculated using an EON generated using the surrogate classifier. information, nor does it modify the overall spectral attributes; hence outperforming the spectral envelope attacks and more importantly performs well against all SER classifiers. These evaluations further demonstrate DARE-GP's generalizability and efficacy against different SER classifiers, even outperforming the attacks that need off-line spectral transformations  [39] . It is important to note that the gradient based attacks, PGD  [71]  and FGSM  [41] , were quite ineffective. Both were able to generate evasive audio samples in feature space for more than 90% of the evaluation audio samples. However, when converted back to audio time series from feature space, the resulting audio was too garbled to transcribe correctly. These transcription errors are what drove the ESR in each trial to 0 for these attacks.",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "Classifier Vtln Mcadams",
      "text": "",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Defenses Against Dare-Gp",
      "text": "DARE-GP is an evasion attack against an unknown SER classifier. This attack's effectiveness has been shown above, but this begs the question regarding whether or not a knowledgeable SER Provider, one familiar with the details of DARE-GP, could deploy countermeasures to allow them to perform accurate emotion inference. This question is considered from two different vantage points: active and passive defenses. The datasets and EON used in these evaluations were the same used in Section 6.2. 6.3.1 Active Defense. Active defenses attempt to mitigate the impacts of DARE-GP before attempting to infer the true class. One common approach, adversarial training, is not viable because the SER Provider would not have access to the ground truth emotional content associated with EON-modified audio samples. Table  6  shows the evaluation results of several different active defenses against the DARE-GP.\n\nTwo of the defense methods considered were first published in WaveGuard  [45] : Audio Resampling and Mel Spectrogram Extraction and Inversion. These methods can be applied as a pre-processing step on any inputs to the SER classifier. Audio resampling was ineffective; the tones used to comprise EONs are constant over their duration and are present regardless of the sample rate. Mel spectrogram extraction and inversion was also ineffective. The surrogate classifier and several of the black-box classifiers use MFCCs as their feature representations; if EONs were significantly degraded during the MFCC extraction process, then they would not have been effective against any of these MFCC-based classifiers. One interesting result here was that several applications of defenses actually improved ESR against Data Flair and wav2vec. For example, Data Flair uses 40 MFCCs as the feature representation; the Mel Spectrogram Defense used 22 MFCCs to be consistent with  [39] . This lossy preprocessing negatively impacted Data Flair's performance on a few audio samples, which degraded the performance and improved the ESR.\n\nIn addition to these defenses, if an SER Provider knew the specifics of DARE-GP, and could further capture the specific frequency bands masked by a specific EON, then it would be possible for the provider to implement an EON Band Pass filter. This is not the same as inverting EON application; since, the SER Provider would not have ground truth with respect to the user's speech before EON application. Bandpass filters are \"a linear transformation of the data that leaves intact the components of the data within a specified band of frequencies and eliminates all other components\"  [25] . A bandpass filter is an effective means for removing spurious audio data outside frequency ranges of interest (e.g., those of human speech). In this case, bandpass filters were applied to remove within +/-10Hz of the EON frequencies. Again, due to the interspersion of EON frequencies with human speech, removing these bands severely impacted the SER classifiers. Passive defenses attempt to infer the true class of an audio sample without modifying the audio. The SER Provider could exploit strong relationships between false and true classes to defeat DARE-GP. As mentioned in Section 2, the evaluation metrics used to assess the relationships between the true emotion and the false labels due to DARE-GP are MCC  [24]  and NMI  [35] .\n\nThe MCC is a special case of the Pearson Correlation Coefficient used for classification problems. As with the Pearson Correlation Coefficient, the score is in the range [-1, 1], where 1, -1 and 0 are strong positive correlation, strong negative correlation and uncorrelated. NMI is scaled in the range [0, 1]; a value of 1 indicates that one can create a perfect one-to-one mapping from predicted class labels to actual class labels. A value of 0 indicates that there is no dependency between the predicted and actual labels. MCC and NMI values for the black-box classifiers were calculated for unperturbed audio samples and when samples were perturbed by an EON; these values are shown in Table  7 .\n\nThe application of an EON significantly degraded both the MCC and NMI between true and predicted classes for all of the classifiers. These values of NMI and MCC indicate very weak correlations between the true and predicted classes. In the case of CNN MAA, over 68% of the misclassifications were predicted as angry or surprised. The SEC classifier misclassified over 80% of the EON-perturbed audio samples to sad or angry. The RESNET and SEC classifiers, on the other hand, strongly favored one class for misclassification (sad and happy, respectively). The Data Flair, SEA, TDNN SER classifiers, on the other hand, did not strongly favor any class for misclassification; these classifiers' misclassifications were spread out fairly evenly across all possible classes. Finally, in wav2vec misclassifications were split between happy, angry, and neutral with rates of 52%, 28%, and 27%, respectively.\n\nOur empirical evaluations did not identify any significant inter-emotion relationship across the various classifiers used for evaluation. In addition, no clear relationship was observed between a user's true emotion and the misclassified emotion for a specific classifier. Further investigation into why these classifiers' misclassifications fell into these distributions is left as future work. The main takeaway from these results is that the EON-driven misclassifications do not reveal information that would allow the SER Provider to reverse engineer the actual classes from these forced misclassifications.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "A B C",
      "text": "Fig.  6 . DARE-GP (A) evaluation setup with Amazon Echo. (B) evaluation setup with Amazon Dot. (C) evaluation setup with Amazon Dot and DARE-GP device implemented with a Raspberry Pi. Direction markings 1-8 used for EON direction evaluation.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Acoustic Eon Evaluations",
      "text": "The final set of evaluations assessed whether or not a digitally-generated EON could be effective as over-air, playable sounds. ESR is the metric used for these evaluations. Since these evaluations involved commercial smart speakers, the ESR calculation used the Alexa-based speech-to-text implementation rather than the vosk/kaldi libraries used in the rest of this work. Evaluations were performed against two commercial smart speakers as shown in Figures  6 (A ) and (B). In all of these configurations, a DARE-GP/EON Speaker was placed against the smart speaker; whenever a user's speech was presented to the smart speaker, the DARE-GP/EON speaker played the EON. This configuration allowed an EON to propagate to the smart speaker via conductance of the materials in the smart speaker rather than only through the acoustics of the room, which can cause digitally-developed evasions to fail in an acoustic setting  [63] . The evaluations using real smart speakers presented in this section are:\n\nâ€¢ Real-time playback of EONs with user utterances from different distances (Section 6.4.1) â€¢ Real-time playback of EONs when the user is speaking from different directions (Section 6.4.2) â€¢ Automated, real-time EON invocation triggered by the smart speaker's wake word (Section 6.4.3) 6.4.1 Acoustic Evaluation on Commercial Smart Speakers. The first set of evaluations considers the effectiveness of a digitally-derived EON when adapted and played in an over-air acoustic setting for target users U. User utterances were played via a speaker at varying distances from the smart speaker (see Figure  6 (A) ). These evaluations were performed using two different Alexa-based smart speakers. Recordings were captured by the smart speaker (Amazon Alexa) using a custom Alexa skill that treated each of the EON or audio sample recordings as an interaction. These audio files and corresponding Alexa transcripts were manually downloaded from the Alexa console  [11]  (additional details provided in Appendix C). The ESR was then calculated for the final EON by counting any misclassified sample as a successful evasion if the transcript extracted by Alexa was correct.\n\nTable  8  shows the evaluation results for the configurations shown in Figure  6  (A) and (B) when the \"simulateduser (i.e., User Speaker)\" was 1ft away from the smart speaker. Both had comparable SER classifier evasion rates (around 0.9), but with low word error rate (WER; a measure of transcription accuracy), likely due to the robust microphone arrays present in smart speakers. This low WER indicates that the EONs are not overpowering the user utterances. Since these were the first trials to play EONs acoustically, this was the first opportunity to assess how invasive the EONs were. During these evaluations, the mean background noise in the room was 46dB. The loudest EON generated in these experiments was 50dB which is below the normal conversation range  [1] . These evaluations were repeated with the \"simulated-user\" (i.e., User Speaker) at different distances. The EONs and loudness were kept constant at each distance to assess how well these EONs would work if the user were interacting with the smart speaker from different locations in the room. As shown in Table  8 , even at a distance of 9ft, there is minimal degradation in the EON's effectiveness. The reason for such robustness is the DARE-GP's position with respect to the smart speaker. Irrespective of \"user's\" distance from the speaker, the EONs are always played from a device touching the smart speaker; this means that the smart speaker only needs to be robust with respect to capturing the user's speech from different ranges, which is a primary requirement of these devices. 6.4.2 Evaluation of EON Robustness from Different Directions. In addition to distance, the direction from which a user speaks could also impact the effectiveness of an EON due to considerations like beamforming. To evaluate this, the same audio samples used in Section 6.4.1 were played at 5ft and 9ft from the smart speaker at each of the directions shown in Figure  6  (C). The EON was played from the small speaker adjacent to the smart speaker as shown in Figure  6  (C), with no change to EON or utterance loudness at the respective speakers. The resulting ESR of this EON at each distance/direction combination is provided in Table  9 .\n\nUser direction with respect to the smart speaker and the DARE-GP device does play some role in EON effectiveness. The trials in Table  9  showed a slight degradation when the user was aligned closely with the DARE-GP speaker. This added relatively higher disruption on the speech signal, causing higher degradation on transcription efficacy, resulting in lower ESR. The effect was much more pronounced in the 9ft test, which was not surprising considering the larger variance observed at 9ft in the distance evaluation (see Table  8 ). That said, this trial demonstrates that EONs are robust to user direction even when the smart speaker uses beamforming (the Echo Dot) to focus the microphone array on the direction from which the user is speaking. We believe this robustness is due to the DARE-GP speaker's physical contact with the smart speaker, enabling propagation of EON via conductance of the materials in the smart speaker. EON is not being presented to the smart speaker microphone over the air only from one direction, debilitating any directional noise masking by the beamforming.",
      "page_start": 16,
      "page_end": 17
    },
    {
      "section_name": "Automated Eon Invocation.",
      "text": "The previous evaluations demonstrated that DARE-GP is effective in an acoustic setting. To deploy a pre-trained DARE-GP system (one in which an EON has already been developed) three components are required: a small processor, a microphone, and a speaker. The processor is responsible for listening (i.e., detecting) for the smart speaker's wake word via the microphone and then playing the precalculated EON at a pre-determined volume through the speaker. This deployment configuration should introduce a small amount of lag between when the user starts to speak and when the EON starts to play; this evaluation measured the impact of this lag on EON effectiveness.\n\nTo demonstrate this setup, the final evaluation used a Raspberry Pi 3, Model B with a Quad Core 1.2GHz Broadcom BCM2837 64bit CPU and 1GB of RAM, along with an off-the-shelf speaker and microphone (see Figure  6 (D) ). An off-the-shelf wake word detection engine  [77]  was used to listen for \"Alexa\" before playing the EON. In this configuration, DARE-GP consumes less than 3.7 W of power during peak usage. The EON used for this evaluation used the same EON and volume as the first trial at 1ft using the Amazon Dot from Section 6.4.1.\n\nThe original ESR for this trial was 0.76. When executing the same EON in a fully automated manner, the resulting ESR was 0.738. The average time between wake word detection and the start of EON playback was 0.15 seconds. This evaluation, along with the previous ones, demonstrate that DARE-GP's digitally-generated EONs can transition successfully to an acoustic setting with multiple smart speakers in a realistic setting. In fact, this approach benefits from characteristics of the realistic setting (i.e. -the superior microphones and speech-to-text utilities in the smart speakers), resulting in a very effective solution. Further, it demonstrates that this approach's use of a pre-generated, universal EON allows real-time deployment on a very small form factor.",
      "page_start": 17,
      "page_end": 18
    },
    {
      "section_name": "Discussion",
      "text": "There were some important experiment design choices and constraints that are worth discussing in more detail:\n\nUse of Recorded Sound and Off-the-Shelf SER Classifiers: For this work, evaluations used previouslyrecorded audio. This approach provided a benchmark against a set of known datasets, rather than possibly introducing some inadvertent bias that would lead to unrealistic results. Likewise, the off-the-shelf SER classifiers, were used for a similar reason. The use of existing data and existing SER classifiers demonstrates the ability of DARE-GP to generalize.\n\nSurrogate SER Classifier: The surrogate SER classifier used to train DARE-GP is significant because of its distinction from the black-box classifiers. This classifier is a simple DNN that uses MFCC features. While the .877 accuracy is acceptable for learning the emotion-relevant spectral speech attributes, it is not as high as some of the black-box classifiers. Additionally, it has different classes than Flair and SEA, and different features than Flair, SEC, RESNET and TDNN. This is important concerning the utility of DARE-GP; by targeting an underlying attribute of a user's speech, the spectral components, DARE-GP was effective despite the differences between the surrogate and black-box SER classifiers' implementation and performance.\n\nTargeted versus Untargeted Evasion: DARE-GP is an example of untargeted evasion; an evasive sample has fooled the SER classifier as long as the predicted class is different from the actual class. Based on the objectives of DARE-GP, untargeted evasion was sufficient. As demonstrated in Section 6.3.2, the actual and false classes are sufficiently independent of one another, which would prevent the SER Provider from inferring the actual emotion associated with a given utterance. This means, for example, that no successful targeted advertising is possible utilizing the emotional content of any speaker's utterance.\n\nDifferences Across EONs: The final EON for a given set of users is tailored based on the specific users' common emotion-relevant spectral traits. Figure  7  shows two spectrograms trained for different sets of users, varied in demographics from the RAVDESS dataset. Figure  7 (A)  shows an EON that overlaps formant ð¹ 3 and ð¹ 6 and one tone in a higher register. In Figure  7  (B), the EON masks formant ð¹ 4 and also masks two higher frequency bands. This figure provides an example that DARE-GP learns different EONs for different sets of users. In this example, two groups (i.e., sets of users) with a demographic difference may have differences in their common emotion-depiction-relevant spectral traits. Hence the generated EONs are different.\n\nNotably, initial attempts to generate EONs that would work for previously unseen users were unsuccessful; cross-dataset evaluations where RAVDESS-trained EONs were applied to TESS audio samples showed that some tailoring was required. While this approach does require labeled audio samples for the end users, using a A B pre-trained model to label unlabeled user data, along with approaches to measure the adaptation performance of such an untrained model  [40]  could decrease the impact of this issue. Such investigation is left as future work.\n\nUser Survey: In addition to the evaluations performed in Section 6, a survey was performed with 19 undergraduate and graduate students. This survey aimed to 1) assess interest in emotion protection, 2) get user feedback on the user-friendliness of DARE-GP, and 3) the user-friendliness of a notional system that implements a recordâ†’perturbâ†’replay design. When asked about the level of concern over companies using their emotion data for targeted ads, the mean level of concern was 3.16 out of a maximum of 4.0, with only two respondents rating their concern as Neutral or Unconcerned. Only 1 out of the 19 respondents rated the EON used for evaluation in Section 6.4.3 as Somewhat Distracting. Most of the SotA approaches discussed in Section 8 would require recording a user's utterance, modification of the utterance to generate an evasive variant, and then playback to the smart speaker. To simulate this, a script was used to record the user's utterance after hearing a wake word. Then, the sample was modified slightly, and then played after playing a pre-recorded wake word for the smart speaker. 36.8% of the respondents stated that they would be unwilling to use this system. This survey provided further evidence that DARE-GP is both relevant and more usable than SotA systems. Additional details are provided in Appendix D.\n\nBeamforming Countermeasure: The current approach addresses beamforming via physical contact between the DARE-GP speaker and the smart speaker. Future smart speakers, if manufactured to limit the impact of external vibrations, could potentially improve the effectiveness of beamforming by limiting EON presentation via conductance. Since no commercial smart speakers currently advertise this feature, evaluating the effectiveness of such a defense is beyond the scope of this paper and is left as future work.\n\nExtended, In-the-wild Evaluation: Evaluations were performed in laboratory environments and in two home settings; exhaustive in-the-wild evaluation was out of scope. Future work would benefit from sampling data from a broader range of in-the-wild situations to determine DARE-GPs effectiveness and any long-term EON invasiveness concerns.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Related Works",
      "text": "There are three classes of related works relevant to DARE-GP: (1) works that protect against unanticipated recording, (2) GP-based ML evasion, and (3) audio evasion works.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Protection Of Audio Against Unanticipated Recording",
      "text": "There are several works that address protection from unanticipated recording. These works include both protection from unauthorized devices and cases where the user wants to prevent known devices from unauthorized recording. Patronus  [62]  is an approach to perform real-time audio jamming that could only be decoded by devices with the\n\nâœ“ SMACK  [118]  âœ“ âœ“ Practical (RIR)  [63]  Partial âœ“ Imperio  [88]  âœ“ Semi-Black-Box  [114]  Partial Targeted Black-Box  [102]  Did You Hear That?  [10]  correct \"key. \" MicShield  [98]  prevents smart speakers from recording audio outside the window of a command being directed at the smart speaker. In both cases, the solutions presented prevent access to any audio by unanticipated devices at unplanned times. DARE-GP is complementary to these approaches by providing protection with respect to emotional content for devices intended to hear a given command.",
      "page_start": 19,
      "page_end": 20
    },
    {
      "section_name": "Evasion Of Black-Box Classifiers Using Genetic Programming",
      "text": "Works related to evading classifiers without requiring detailed insight into the inner-workings of the evaded classifiers are generally referred to as black-box adversarial evasion  [100] . The use of genetic programming (GP) in DARE-GP was inspired by previous applications of GP towards black-box adversarial evasion in multiple domains, including: PDF malware  [109] , WiFi behavior detection  [66] , and audio speech recognition (ASR)  [10, 102] . The primary difference between DARE-GP and these existing works is that DARE-GP uses GP to generate universal adversarial perturbations (UAPs); that is, the EON generated by DARE-GP causes misclassification in previously unseen inputs (utterances). All of these other works used GP to generate unique perturbations for each evasive sample. Unlike these existig approaches, by precomputing a single, universal adversarial perturbation, DARE-GP is able to operate in real-time for previously unheard utterances. Additionally, compared to existing GP-based audio evasion approaches  [10, 114] , DARE-GP leveraged constrained GP to protect the speech-to-text transcription utility, making it practically deployable for the target task against commercial smart speakers.",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "Adversarial Evasion Of Audio Classifiers",
      "text": "There are many works that consider evasion of audio classifiers, including speaker recognition  [2, 7, 50, 64] , automated speech recognition (ASR)  [2, 3, 10, 21, 64, 88, 102, 114, 118] , and speech emotion recognition (SER)  [6, 7, 39] . Since nothing in these approaches intrinsically precludes application to other audio domains, these were all considered when comparing DARE-GP to SotA techniques. However, as shown in Table  10 , there are several key attributes that differentiate these works from DARE-GP.\n\nAdvpulse  [64]  presented an approach for targeted evasion of speaker recognition and automated speech recognition classifiers. Of the considered works, Advpulse has multiple similarities with DARE-GP. Advpulse generates additive noise in the form of 0.5 second pulses; these pulses are universal adversarial audio perturbations, much like DARE-GP's EONs. As a result, like DARE-GP, Advpulse can be used for real-time evasion of previously unheard utterances in an acoustic setting. The authors were concerned that the additive noise might only be effective if played at exactly the right time to cause misclassification with an utterance. To address this, the authors built in variable delays in their training data to address this concern. DARE-GP did not address this explicitly; mainly because the use case presented provides a simple way to align EONs with user speech: the wake word for the smart speaker's VA. Details on how this worked are provided in Section 6.4.3. Advpulse also considered the possibility of a knowledgeable adversary attempting to thwart their evasion approach; the defenses presented are very similar to those presented in Section 6.3. There are two major differences between DARE-GP and Advpulse: transcription protection and black-box transferability. Advpulse did not consider inadvertent impacts on speech transcription from their approach. More importantly, Advpulse is a white-box evasion system; the approach required detailed access to the target model's gradients and weights in order to develop their additive noise. Both of these limitations of this work make Advpulse unsuitable for the use case presented in this paper.\n\nEDGY  [7]  considers the privacy of a user's speech as a challenge of separating the text of what a user says (linguistic) from the other sensitive user information carried in speech like the user's gender or emotion (paralinguistic). To address this, the authors learned separate, disentangled representations (embeddings) of the linguistic and paralinguistic parts of a user's speech. A significant part of this work was devoted to how to perform this separation at edge devices in a resource-constrained environment. One major benefit of this work is that it does not require access to a target classifier to develop these embeddings. The authors developed several classifiers for paralinguistic information based upon a TRIpLet Loss network (TRILL)  [91]  and demonstrated that using the linguistic embedding could significantly degrade the classification of multiple private user attributes, including age, emotion, accent, and gender. The most significant difference between this work and DARE-GP is the employment scenario. DARE-GP EONs are additive noise that can be played in real-time when a user speaks. EDGY, like many adversarial audio works  [39, 63, 88, 114, 118] : 1) requires the user to speak, 2) processes the audio to generate the adversarial sample, and 3) uses a vocoder or similar capability to generate synthetic speech to evade the classifier. This impacts both usability of the system and can degrade audio quality, as shown in the impacts on word error rate (WER) for the generated audio.\n\nGao et al.  [39]  demonstrated that three spectral envelope-based attacks could defeat several previously unseen SER classifiers. Like DARE-GP, these attacks were computed without any insight into the target classifiers. These attacks were not executable in real-time; unlike DARE-GP's EONs, the spectral envelope filters applied (vocal tract length normalization, modulation spectrum smoothing, and McAdams transformation) cannot be represented as additive noise that can be played when a user speaks. To deploy these filters in a real-world scenario, the users' utterances would need to be recorded, modified, and then replayed within range of the target smart speaker. Further, these filters introduced significant transcription errors, measured as Word Error Rate (WER). Evaluations comparing DARE-GP's performance to these methods are presented in Section 6.2.\n\nSMACK  [118]  performed evasion against several automated speech recognition (ASR) and speaker recognition (SR) classifiers by modifing the prosody of speech to disrupt these classifiers while attempting to maintain \"natural sounding\" speech. Like several other state-of-the-art (SotA) works  [63, 64, 88, 118] , SMACK employed gradient-based methods in feature space to generate adversarial samples. Like multiple SotA works, SMACK leveraged room impulse response (RIR)  [96]  to model a room's acoustics in order to improve the success rate of these digitally-created evasive samples when playing them in acoustically. Like  [39]  and  [7] , SMACK is not real-time and would require preprocesing of a user's audio before presenting it to the smart speaker's VA.\n\nThe remaining works are similar to those presented above. Imperio  [88]  evades a white-box ASR classifier, also using RIR as an approach to improve effectiveness of evasive samples when played acoustically. Wu, Yi, et al.  [114]  also evaded and ASR classifier, but was able to do so without requiring the gradients and weights of the target model. This work used GP to generate unique adversarial perturbations for each sample, but could not operate in real-time and did not evaluate the efficacy of these samples when played acoustically. Similarly, Alzantot et al.  [10]  also evaded ASR systems using GP, but with black-box access to the target model (hard label access). Finally, Taori et al.  [102]  also used GP to perform targeted evasion against an ASR system. This work used a new mutation operation based on gradient estimation to speed convergence. Like other SotA GP-based black-box evasion works, this work required hard-label access to the target classifier and generated unique perturbations for each utterance in a non-real-time scenario.\n\nEach of these existing works manifests some of the characteristics of a usable system to address the challenges laid out in this paper. That said, each of them falls short in one or more area that would make the system unusable for the use case described in this work.\n\nThe objective of this work was to degrade the privacy loss concerning speaker emotions incurred by using a smart speaker VA service (by an SER Provider). The paper presents DARE-GP, which for a set of users, precomputes an EON (i.e., universal audio perturbation) that disrupts emotion detection by adding spectral distortions to the target users' speech, hence effective against previously unheard utterances and transferable to broad set of black-box SER classifiers with heterogeneous classes, topologies, and feature representations. Generated EON is additive, meaning that simultaneously playing an EON with the user's speech can evade SER classifiers over the air in real-time. Our extensive evaluation shows DARE-GP's superior performance compared to state-of-the-art SER attacks and robustness against defenses from a knowledgeable SER Provider. Finally, DARE-GP was demonstrated effective in a real-time, real-world scenario against commercial smart speakers, where a SWaP-efficient form factor using wake word detection could automatically deploy an EON whenever a user interacted with a smart speaker, demonstrating an ESR of 0.738 against an Amazon Dot.",
      "page_start": 20,
      "page_end": 22
    },
    {
      "section_name": "A B",
      "text": "Fig.  8 . (A) Test setup with Amazon Dot and 3rd-party microphone for EON and utterance capture. This configuration was used exclusively for collection of data for EON fine-tuning and was not used for evaluation. (B) Alexa Review Voice History page. For evaluation, a script was used to retrieve the audio recordings from this page so that evaluations would be performed using Alexa-captured audio.\n\n(4) Action: Prompt the user to say the \"other\" system wake word and a smart speaker command. This will record the user's speech, digitally modify the audio, and play back the user's command with a smart speaker wake word.\n\n(5) Question 2: How distracting was the noise played by the first system? The respondents' answers to these questions are summarized in Table  12 . There are a few key takeaways from the survey responses. First, nearly all respondents were ate least somewhat concerned about a company's use of emotional data from targeted advertisement, which supports the motivation for this work. Second, the EON played by a DARE-GP system was either noninvasive or completely inaudible for nearly all respondents. Moreover, none of the respondents stated that they would be unwilling to DARE-GP. Finally, the record/playback solution, which is an analog for how any of the non-real-time systems would need to be deployed, was significantly less appealing; nearly half of the respondents stated that they would be unwilling to use this system.",
      "page_start": 23,
      "page_end": 29
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: DARE-GP employment scenario. An adjacent DARE-GP device can be used to prevent reliable inference by a smart",
      "page": 4
    },
    {
      "caption": "Figure 1: shows this deployment scenario where a DARE-GP",
      "page": 4
    },
    {
      "caption": "Figure 2: visualizes the spectral attributes on which an SER classifier focuses to make an inference and how",
      "page": 6
    },
    {
      "caption": "Figure 2: (A) Spectrogram of a user utterance recorded at a speaker distance of 1ft from Echo DOT and associated Kernel Shap",
      "page": 7
    },
    {
      "caption": "Figure 2: B is the spectrogram of a DARE-GP-generated spectral perturbation composed of three tones. Each tone",
      "page": 7
    },
    {
      "caption": "Figure 2: Bâ€™s tones correspond to the above-discussed important",
      "page": 7
    },
    {
      "caption": "Figure 2: C shows the spectrogram of the user utterance when played simultaneously with the spectral",
      "page": 7
    },
    {
      "caption": "Figure 2: A); however, due to the injection of noise, they are negatively impacting SERâ€™s angry class",
      "page": 7
    },
    {
      "caption": "Figure 2: C), causing misclassification to happy.",
      "page": 7
    },
    {
      "caption": "Figure 3: shows the high-level GP approach used to generate an",
      "page": 7
    },
    {
      "caption": "Figure 3: Genetic Programming Adversarial Evasion Workflow",
      "page": 8
    },
    {
      "caption": "Figure 3: , a population of individuals (Table 1), each of which has the",
      "page": 8
    },
    {
      "caption": "Figure 4: As the initial step (step A), DARE-GP will digitally",
      "page": 10
    },
    {
      "caption": "Figure 4: (A) Pre-train a generic population using the â€œcanonicalâ€ dataset. (B) Starting with this pre-trained population, generate",
      "page": 11
    },
    {
      "caption": "Figure 5: A high-level view of how the datasets from Section 5 were used in these evaluations. All RAVDESS data was used",
      "page": 12
    },
    {
      "caption": "Figure 5: ). RAVDESS was used to train \"factory",
      "page": 12
    },
    {
      "caption": "Figure 6: DARE-GP (A) evaluation setup with Amazon Echo. (B) evaluation setup with Amazon Dot. (C) evaluation setup",
      "page": 16
    },
    {
      "caption": "Figure 6: (A)). These",
      "page": 16
    },
    {
      "caption": "Figure 6: (A) and (B) when the \"simulated-",
      "page": 16
    },
    {
      "caption": "Figure 6: (C) for details.",
      "page": 17
    },
    {
      "caption": "Figure 6: (C). The EON was played from the small speaker adjacent to the smart speaker as",
      "page": 17
    },
    {
      "caption": "Figure 6: (C), with no change to EON or utterance loudness at the respective speakers. The resulting",
      "page": 17
    },
    {
      "caption": "Figure 6: (D)). An off-the-shelf wake word detection engine [77] was used to listen for \"Alexa\" before playing the EON.",
      "page": 18
    },
    {
      "caption": "Figure 7: shows two spectrograms trained for different sets of users,",
      "page": 18
    },
    {
      "caption": "Figure 7: (A) shows an EON that overlaps formant ð¹3 and ð¹6",
      "page": 18
    },
    {
      "caption": "Figure 7: (B), the EON masks formant ð¹4 and also masks two higher frequency",
      "page": 18
    },
    {
      "caption": "Figure 7: Spectrograms of sample user utterances mixed with EONs generated for (A) a group of three men and two women,",
      "page": 19
    },
    {
      "caption": "Figure 8: B), we used an off-the-shelf microphone (an MXI PoConSeries 1 AC-404USB) to capture these training",
      "page": 28
    },
    {
      "caption": "Figure 8: A. This microphone was touching the smart",
      "page": 28
    },
    {
      "caption": "Figure 8: (A) Test setup with Amazon Dot and 3rd-party microphone for EON and utterance capture. This configuration was",
      "page": 29
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "SER Provider": "Audio Sample",
          "Stakeholder responsible for operating and training the SER classifier; the purpose of DARE-GP is to prevent this\nprincipleâ€™s SER classifier from performing accurate inferences about a userâ€™s emotions.": "Speech sample with a valid transcript and classification by a target emotion classifier. These are the samples for\nwhich we want to create evasive variants."
        },
        {
          "SER Provider": "Evasive Audio Sample",
          "Stakeholder responsible for operating and training the SER classifier; the purpose of DARE-GP is to prevent this\nprincipleâ€™s SER classifier from performing accurate inferences about a userâ€™s emotions.": "Modified version of an audio sample that a) the target classifier cannot determine the actual class and b) the\ntranscript is still correct."
        },
        {
          "SER Provider": "Emotion Obfuscating Noise (EON)",
          "Stakeholder responsible for operating and training the SER classifier; the purpose of DARE-GP is to prevent this\nprincipleâ€™s SER classifier from performing accurate inferences about a userâ€™s emotions.": "A combination of ð‘ tones that can be combined with an audio sample to generate a potentially evasive sample."
        },
        {
          "SER Provider": "Tone",
          "Stakeholder responsible for operating and training the SER classifier; the purpose of DARE-GP is to prevent this\nprincipleâ€™s SER classifier from performing accurate inferences about a userâ€™s emotions.": "An acoustic signal specified by a single frequency, offset, duration and amplitude."
        },
        {
          "SER Provider": "Generation (of GP)",
          "Stakeholder responsible for operating and training the SER classifier; the purpose of DARE-GP is to prevent this\nprincipleâ€™s SER classifier from performing accurate inferences about a userâ€™s emotions.": "A single iteration of a genetic program (GP)."
        },
        {
          "SER Provider": "Population",
          "Stakeholder responsible for operating and training the SER classifier; the purpose of DARE-GP is to prevent this\nprincipleâ€™s SER classifier from performing accurate inferences about a userâ€™s emotions.": "Collection of individuals resulting from a generation."
        },
        {
          "SER Provider": "Individual",
          "Stakeholder responsible for operating and training the SER classifier; the purpose of DARE-GP is to prevent this\nprincipleâ€™s SER classifier from performing accurate inferences about a userâ€™s emotions.": "A member of the population whose parameters can be used to generate an Emotion Obfuscating Noise (EON)."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Parameter": "EON Frequency",
          "Value": "[100,4000] Hz"
        },
        {
          "Parameter": "EON Duration",
          "Value": "[2.5, 4.0] s"
        },
        {
          "Parameter": "EON Offset",
          "Value": "[0.0, 0.5] s"
        },
        {
          "Parameter": "EON Amplitude",
          "Value": "[0.0067, 0.04]"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Classifier": "Surrogate DNN",
          "Topology": "7-layer DNN",
          "Features": "MFCC",
          "Classes": "8",
          "Accuracy (Benign)": "0.877"
        },
        {
          "Classifier": "Data Flair [28]",
          "Topology": "2-layer DNN",
          "Features": "MFCC, Chroma",
          "Classes": "4",
          "Accuracy (Benign)": "0.665"
        },
        {
          "Classifier": "Speech Emotion Analyzer\n(SEA) [81]",
          "Topology": "18-layer CNN",
          "Features": "MFCC",
          "Classes": "10",
          "Accuracy (Benign)": "0.921"
        },
        {
          "Classifier": "Speech Emotion Classification\n(SEC) [81]",
          "Topology": "Bi-Dir LSTM\nand CNN",
          "Features": "Mel Spectrogram",
          "Classes": "8",
          "Accuracy (Benign)": "0.916"
        },
        {
          "Classifier": "CNN with Multi-scale Area Attention\n(CNN-MAA) [39, 115]",
          "Topology": "CNN-MAA",
          "Features": "Mel Spectrogram",
          "Classes": "8",
          "Accuracy (Benign)": "0.952"
        },
        {
          "Classifier": "Time Delay Neural Network\n(TDNN)",
          "Topology": "TDNN",
          "Features": "Mel Spectrogram",
          "Classes": "8",
          "Accuracy (Benign)": "0.678"
        },
        {
          "Classifier": "Residual Neural Network\n(RESNET)",
          "Topology": "RESNET",
          "Features": "Mel Spectrogram",
          "Classes": "8",
          "Accuracy (Benign)": "0.773"
        },
        {
          "Classifier": "wav2vec [15]",
          "Topology": "CNN and DNN",
          "Features": "wav2vec",
          "Classes": "4",
          "Accuracy (Benign)": "0.963"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Classifier": "Data Flair",
          "VTLN": "0.455",
          "McAdams": "0.063",
          "MSS": "0.0",
          "FGSM": "0.0",
          "PGD": "0.0",
          "DARE-GP": "0.854"
        },
        {
          "Classifier": "SEA",
          "VTLN": "0.230",
          "McAdams": "0.071",
          "MSS": "0.023",
          "FGSM": "0.0",
          "PGD": "0.0",
          "DARE-GP": "0.675"
        },
        {
          "Classifier": "SEC",
          "VTLN": "0.338",
          "McAdams": "0.077",
          "MSS": "0.061",
          "FGSM": "0.0",
          "PGD": "0.0",
          "DARE-GP": "0.688"
        },
        {
          "Classifier": "CNN MAA",
          "VTLN": "0.302",
          "McAdams": "0.043",
          "MSS": "0.044",
          "FGSM": "0.0",
          "PGD": "0.0",
          "DARE-GP": "0.668"
        },
        {
          "Classifier": "TDNN",
          "VTLN": "0.251",
          "McAdams": "0.065",
          "MSS": "0.082",
          "FGSM": "0.0",
          "PGD": "0.0",
          "DARE-GP": "0.69"
        },
        {
          "Classifier": "RESNET",
          "VTLN": "0.327",
          "McAdams": "0.082",
          "MSS": "0.003",
          "FGSM": "0.0",
          "PGD": "0.0",
          "DARE-GP": "0.683"
        },
        {
          "Classifier": "wav2vec",
          "VTLN": "0.187",
          "McAdams": "0.058",
          "MSS": "0.001",
          "FGSM": "0.0",
          "PGD": "0.0",
          "DARE-GP": "0.304"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ESR with\nNo Defense": "0.839",
          "Î” after applying the defenses\nAudio\nMel Spectrogram\nEON Band\nResmpling\nExtraction\nPass": "0.000\n+0.011\n0.000"
        },
        {
          "ESR with\nNo Defense": "0.675",
          "Î” after applying the defenses\nAudio\nMel Spectrogram\nEON Band\nResmpling\nExtraction\nPass": "0.000\n-0.011\n-0.001"
        },
        {
          "ESR with\nNo Defense": "0.686",
          "Î” after applying the defenses\nAudio\nMel Spectrogram\nEON Band\nResmpling\nExtraction\nPass": "0.000\n-0.004\n-0.040"
        },
        {
          "ESR with\nNo Defense": "0.680",
          "Î” after applying the defenses\nAudio\nMel Spectrogram\nEON Band\nResmpling\nExtraction\nPass": "-0.001\n-0.005\n-0.076"
        },
        {
          "ESR with\nNo Defense": "0.690",
          "Î” after applying the defenses\nAudio\nMel Spectrogram\nEON Band\nResmpling\nExtraction\nPass": "-0.093\n-0.067\n-0.056"
        },
        {
          "ESR with\nNo Defense": "0.656",
          "Î” after applying the defenses\nAudio\nMel Spectrogram\nEON Band\nResmpling\nExtraction\nPass": "-0.018\n-0.051\n-0.056"
        },
        {
          "ESR with\nNo Defense": "0.358",
          "Î” after applying the defenses\nAudio\nMel Spectrogram\nEON Band\nResmpling\nExtraction\nPass": "+0.084\n0.000\n-0.082"
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Classifier": "Data Flair",
          "Mean MCC - EON": "0.093",
          "Mean NMI - EON": "0.174"
        },
        {
          "Classifier": "SEA",
          "Mean MCC - EON": "0.290",
          "Mean NMI - EON": "0.241"
        },
        {
          "Classifier": "SEC",
          "Mean MCC - EON": "0.144",
          "Mean NMI - EON": "0.089"
        },
        {
          "Classifier": "CNN MAA",
          "Mean MCC - EON": "0.220",
          "Mean NMI - EON": "0.243"
        },
        {
          "Classifier": "TDNN",
          "Mean MCC - EON": "-0.135",
          "Mean NMI - EON": "0.150"
        },
        {
          "Classifier": "RESNET",
          "Mean MCC - EON": "-0.121",
          "Mean NMI - EON": "0.108"
        },
        {
          "Classifier": "wav2vec",
          "Mean MCC - EON": "-0.289",
          "Mean NMI - EON": "0.361"
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Transferable to\nBlack Box Model": "âœ“",
          "Protects\nTranscription": "âœ“",
          "Effective\nAcoustically": "âœ“",
          "Real-Time for Previously\nUnheard Utterances": "âœ“",
          "Natural Interaction\nwith Smart Speaker": "âœ“",
          "Robust vs.\nDefenses": "âœ“"
        },
        {
          "Transferable to\nBlack Box Model": "",
          "Protects\nTranscription": "",
          "Effective\nAcoustically": "âœ“",
          "Real-Time for Previously\nUnheard Utterances": "âœ“",
          "Natural Interaction\nwith Smart Speaker": "âœ“",
          "Robust vs.\nDefenses": "âœ“"
        },
        {
          "Transferable to\nBlack Box Model": "âœ“",
          "Protects\nTranscription": "Partial",
          "Effective\nAcoustically": "âœ“",
          "Real-Time for Previously\nUnheard Utterances": "âœ“",
          "Natural Interaction\nwith Smart Speaker": "",
          "Robust vs.\nDefenses": ""
        },
        {
          "Transferable to\nBlack Box Model": "âœ“",
          "Protects\nTranscription": "",
          "Effective\nAcoustically": "",
          "Real-Time for Previously\nUnheard Utterances": "",
          "Natural Interaction\nwith Smart Speaker": "",
          "Robust vs.\nDefenses": ""
        },
        {
          "Transferable to\nBlack Box Model": "",
          "Protects\nTranscription": "",
          "Effective\nAcoustically": "âœ“",
          "Real-Time for Previously\nUnheard Utterances": "",
          "Natural Interaction\nwith Smart Speaker": "",
          "Robust vs.\nDefenses": "âœ“"
        },
        {
          "Transferable to\nBlack Box Model": "",
          "Protects\nTranscription": "Partial",
          "Effective\nAcoustically": "âœ“",
          "Real-Time for Previously\nUnheard Utterances": "",
          "Natural Interaction\nwith Smart Speaker": "",
          "Robust vs.\nDefenses": ""
        },
        {
          "Transferable to\nBlack Box Model": "",
          "Protects\nTranscription": "",
          "Effective\nAcoustically": "âœ“",
          "Real-Time for Previously\nUnheard Utterances": "",
          "Natural Interaction\nwith Smart Speaker": "",
          "Robust vs.\nDefenses": ""
        },
        {
          "Transferable to\nBlack Box Model": "",
          "Protects\nTranscription": "Partial",
          "Effective\nAcoustically": "",
          "Real-Time for Previously\nUnheard Utterances": "",
          "Natural Interaction\nwith Smart Speaker": "",
          "Robust vs.\nDefenses": ""
        },
        {
          "Transferable to\nBlack Box Model": "",
          "Protects\nTranscription": "",
          "Effective\nAcoustically": "",
          "Real-Time for Previously\nUnheard Utterances": "",
          "Natural Interaction\nwith Smart Speaker": "",
          "Robust vs.\nDefenses": ""
        },
        {
          "Transferable to\nBlack Box Model": "",
          "Protects\nTranscription": "",
          "Effective\nAcoustically": "",
          "Real-Time for Previously\nUnheard Utterances": "",
          "Natural Interaction\nwith Smart Speaker": "",
          "Robust vs.\nDefenses": ""
        }
      ],
      "page": 20
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Classifier": "Simple DNN (the surrogate classifier)",
          "Description": "A 7-layer, densely connected DNN constructed for use in\nthe EON fitness function. Uses MFCC features.",
          "Classes": "8: neutral, calm, happy, sad,\nangry,\nfearful, disgust, sur-\nprised",
          "Accuracy": ".877"
        },
        {
          "Classifier": "Data Flair [28]",
          "Description": "2-Layer, multi-Layer perceptron classifier published as part\nof the data-flair machine learning training program. Uses\nMFCC and Chroma features.",
          "Classes": "4: calm, happy,\nfearful, dis-\ngust",
          "Accuracy": ".665"
        },
        {
          "Classifier": "Speech Emotion Analyzer (SEA) [81]",
          "Description": "An 18-Layer, convolutional neural network (CNN) that clas-\nsifies based upon gender and emotion.",
          "Classes": "10:\nangry,\ncalm,\nfearful,\nhappy,\nsad\nfor\nboth male\nand female",
          "Accuracy": ".921"
        },
        {
          "Classifier": "Speech Emotion Classification - CNN-\nLSTM (SEC) [49]",
          "Description": "Bi-directional LSTM attention mechanism and a CNN using\nMel Spectrogram features.",
          "Classes": "8: neutral, calm, happy, sad,\nangry,\nfearful, disgust, sur-\nprised",
          "Accuracy": ".916"
        },
        {
          "Classifier": "Conv. Neural Network with Multi-scale\nArea Attention (CNN-MAA) [39]",
          "Description": "Based upon implementation by Gao et al. [39] as a target for\nadversarial evasion, based upon Xu et al. [115]. Uses MFCC\nfeatures.",
          "Classes": "8: neutral, calm, happy, sad,\nangry,\nfearful, disgust, sur-\nprised",
          "Accuracy": "0.952"
        },
        {
          "Classifier": "Time delay Neural Network (TDNN)",
          "Description": "A Time Delay Neural Network (TDNN) SER classifier built\nspecifically for this work. The input to this classifier is a 3s\nspectrogram.",
          "Classes": "8: neutral, calm, happy, sad,\nangry,\nfearful, disgust, sur-\nprised",
          "Accuracy": "0.678"
        },
        {
          "Classifier": "Residual Neural Network (RESNET)",
          "Description": "A RESNET SER classifier built specifically for this work. The\ninput to this classifier is a 3s spectrogram.",
          "Classes": "8: neutral, calm, happy, sad,\nangry,\nfearful, disgust, sur-\nprised",
          "Accuracy": "0.773"
        },
        {
          "Classifier": "wav2vec",
          "Description": "A wav2vec SER classifier built specifically for this work. The\ninput to this classifier is a 3s spectrogram.",
          "Classes": "4: neutral, happy, sad, angry",
          "Accuracy": "0.963"
        }
      ],
      "page": 28
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Question 1": "Question 2",
          "Very Unconcerned\n0": "Very Distracting\n0",
          "Somewhat Unconcerned\n1": "Somewhat Distracting\n1",
          "Neutral\n1": "Neutral\n1",
          "Somewhat Concerned\n8": "Noninvasive\n8",
          "Very Concerned\n9": "Completely Inaudible\n9"
        },
        {
          "Question 1": "Question 3",
          "Very Unconcerned\n0": "Very Unwilling\n0",
          "Somewhat Unconcerned\n1": "Somewhat Unwilling\n0",
          "Neutral\n1": "Neutral\n0",
          "Somewhat Concerned\n8": "Somewhat Willing\n9",
          "Very Concerned\n9": "Very Willing\n10"
        },
        {
          "Question 1": "Question 4",
          "Very Unconcerned\n0": "Very Unwilling\n2",
          "Somewhat Unconcerned\n1": "Somewhat Unwilling\n7",
          "Neutral\n1": "Neutral\n3",
          "Somewhat Concerned\n8": "Somewhat Willing\n4",
          "Very Concerned\n9": "Very Willing\n3"
        }
      ],
      "page": 30
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Normal conversation loudness",
      "year": "2022",
      "venue": "Normal conversation loudness"
    },
    {
      "citation_id": "2",
      "title": "Universal adversarial audio perturbations",
      "authors": [
        "Sajjad Abdoli",
        "Jerome Luiz G Hafemann",
        "Ismail Rony",
        "Patrick Ben Ayed",
        "Alessandro Cardinal",
        "Koerich"
      ],
      "year": "2019",
      "venue": "Universal adversarial audio perturbations",
      "arxiv": "arXiv:1908.03173"
    },
    {
      "citation_id": "3",
      "title": "Sok: The faults in our asrs: An overview of attacks against automatic speech recognition and speaker identification systems",
      "authors": [
        "Hadi Abdullah",
        "Kevin Warren",
        "Vincent Bindschaedler",
        "Nicolas Papernot",
        "Patrick Traynor"
      ],
      "year": "2021",
      "venue": "2021 IEEE symposium on security and privacy (SP)"
    },
    {
      "citation_id": "4",
      "title": "Ultimate Alexa Command Guide: 200+ Voice Commands You Need to Know for Your Echo",
      "authors": [
        "Nelson Aguilar"
      ],
      "year": "2022",
      "venue": "Ultimate Alexa Command Guide: 200+ Voice Commands You Need to Know for Your Echo"
    },
    {
      "citation_id": "5",
      "title": "Emotion filtering at the edge",
      "authors": [
        "Ranya Aloufi",
        "Hamed Haddadi",
        "David Boyle"
      ],
      "year": "2019",
      "venue": "Proceedings of the 1st Workshop on Machine Learning on Edge in Sensor Systems"
    },
    {
      "citation_id": "6",
      "title": "Emotionless: Privacy-preserving speech analysis for voice assistants",
      "authors": [
        "Ranya Aloufi",
        "Hamed Haddadi",
        "David Boyle"
      ],
      "year": "2019",
      "venue": "Emotionless: Privacy-preserving speech analysis for voice assistants",
      "arxiv": "arXiv:1908.03632"
    },
    {
      "citation_id": "7",
      "title": "Paralinguistic privacy protection at the edge",
      "authors": [
        "Ranya Aloufi",
        "Hamed Haddadi",
        "David Boyle"
      ],
      "year": "2020",
      "venue": "ACM Transactions on Privacy and Security"
    },
    {
      "citation_id": "8",
      "title": "Privacy-preserving voice analysis via disentangled representations",
      "authors": [
        "Ranya Aloufi",
        "Hamed Haddadi",
        "David Boyle"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 ACM SIGSAC Conference on Cloud Computing Security Workshop"
    },
    {
      "citation_id": "9",
      "title": "A survey of state-of-the-art approaches for emotion recognition in text",
      "authors": [
        "Nourah Alswaidan",
        "Bachir Menai"
      ],
      "year": "2020",
      "venue": "Knowledge and Information Systems"
    },
    {
      "citation_id": "10",
      "title": "Did you hear that? adversarial examples against automatic speech recognition",
      "authors": [
        "Moustafa Alzantot",
        "Bharathan Balaji",
        "Mani Srivastava"
      ],
      "year": "2018",
      "venue": "Did you hear that? adversarial examples against automatic speech recognition",
      "arxiv": "arXiv:1801.00554"
    },
    {
      "citation_id": "11",
      "title": "What Is Automatic Speech Recognition?",
      "authors": [
        "Amazon"
      ],
      "year": "2022",
      "venue": "What Is Automatic Speech Recognition?"
    },
    {
      "citation_id": "12",
      "title": "Music, Search, and IoT: How People (Really) Use Voice Assistants",
      "authors": [
        "Tawfiq Ammari",
        "Jofish Kaye",
        "Janice Tsai",
        "Frank Bentley"
      ],
      "year": "2019",
      "venue": "ACM Trans. Comput. Hum. Interact"
    },
    {
      "citation_id": "13",
      "title": "Impact of Personalized Social Media Advertising on Online Impulse Buying Behavior",
      "authors": [
        "Huzaifa Aslam",
        "Muhammad Rashid",
        "Nouman Chaudhary"
      ],
      "year": "2021",
      "venue": "SEISENSE Business Review"
    },
    {
      "citation_id": "14",
      "title": "Americans and privacy: Concerned, confused and feeling lack of control over their personal information",
      "authors": [
        "Brooke Auxier",
        "Lee Rainie",
        "Monica Anderson",
        "Andrew Perrin",
        "Madhu Kumar",
        "Erica Turner"
      ],
      "year": "2019",
      "venue": "Americans and privacy: Concerned, confused and feeling lack of control over their personal information"
    },
    {
      "citation_id": "15",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "16",
      "title": "Emotion classification based on biophysical signals and machine learning techniques",
      "authors": [
        "Oana BÄƒlan",
        "Gabriela Moise",
        "Livia Petrescu",
        "Alin Moldoveanu",
        "Marius Leordeanu",
        "Florica Moldoveanu"
      ],
      "year": "2019",
      "venue": "Symmetry"
    },
    {
      "citation_id": "17",
      "title": "What is surveillance capitalism? -definition from whatis",
      "authors": [
        "Nick Barney",
        "Ivy Wigmore"
      ],
      "year": "2022",
      "venue": "What is surveillance capitalism? -definition from whatis"
    },
    {
      "citation_id": "18",
      "title": "Understanding the long-term use of smart speaker assistants",
      "authors": [
        "Frank Bentley",
        "Chris Luvogt",
        "Max Silverman",
        "Rushani Wirasinghe",
        "Brooke White",
        "Danielle Lottridge"
      ],
      "year": "2018",
      "venue": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies"
    },
    {
      "citation_id": "19",
      "title": "Method and article of manufacture for content-based analysis, storage, retrieval, and segmentation of audio information",
      "authors": [
        "Douglas Thomas L Blum",
        "James Keislar",
        "Wheaton",
        "Erling H Wold"
      ],
      "year": "1999",
      "venue": "US Patent"
    },
    {
      "citation_id": "20",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "21",
      "title": "Audio adversarial examples: Targeted attacks on speech-to-text",
      "authors": [
        "Nicholas Carlini",
        "David Wagner"
      ],
      "year": "2018",
      "venue": "2018 IEEE security and privacy workshops (SPW)"
    },
    {
      "citation_id": "22",
      "title": "Query-efficient hard-label black-box attack: An optimization-based approach",
      "authors": [
        "Minhao Cheng",
        "Thong Le",
        "Pin-Yu Chen",
        "Jinfeng Yi",
        "Huan Zhang",
        "Cho-Jui Hsieh"
      ],
      "year": "2018",
      "venue": "Query-efficient hard-label black-box attack: An optimization-based approach",
      "arxiv": "arXiv:1807.04457"
    },
    {
      "citation_id": "23",
      "title": "Sign-opt: A query-efficient hard-label adversarial attack",
      "authors": [
        "Minhao Cheng",
        "Simranjit Singh",
        "Patrick Chen",
        "Pin-Yu Chen",
        "Sijia Liu",
        "Cho-Jui Hsieh"
      ],
      "year": "2019",
      "venue": "Sign-opt: A query-efficient hard-label adversarial attack",
      "arxiv": "arXiv:1909.10773"
    },
    {
      "citation_id": "24",
      "title": "The advantages of the Matthews correlation coefficient (MCC) over F1 score and accuracy in binary classification evaluation",
      "authors": [
        "Davide Chicco",
        "Giuseppe Jurman"
      ],
      "year": "2020",
      "venue": "BMC genomics"
    },
    {
      "citation_id": "25",
      "title": "The band pass filter",
      "authors": [
        "J Lawrence",
        "Terry Fitzgerald"
      ],
      "year": "2003",
      "venue": "international economic review"
    },
    {
      "citation_id": "26",
      "title": "Comparison of techniques for environmental sound recognition",
      "authors": [
        "Michael Cowling",
        "Renate Sitte"
      ],
      "year": "2003",
      "venue": "Pattern recognition letters"
    },
    {
      "citation_id": "27",
      "title": "How impulse buying influences compulsive buying: The central role of consumer anxiety and escapism",
      "authors": [
        "Mahmoud Aadel A Darrat",
        "Douglas Darrat",
        "Amyx"
      ],
      "year": "2016",
      "venue": "Journal of Retailing and Consumer Services"
    },
    {
      "citation_id": "28",
      "title": "Python Mini Project -Speech Emotion Recognition with librosa",
      "year": "2019",
      "venue": "Python Mini Project -Speech Emotion Recognition with librosa"
    },
    {
      "citation_id": "29",
      "title": "Why do adversarial attacks transfer? explaining transferability of evasion and poisoning attacks",
      "authors": [
        "Ambra Demontis",
        "Marco Melis",
        "Maura Pintor",
        "Jagielski Matthew",
        "Battista Biggio",
        "Oprea Alina",
        "Nita-Rotaru Cristina",
        "Fabio Roli"
      ],
      "year": "2019",
      "venue": "28th USENIX security symposium. USENIX Association"
    },
    {
      "citation_id": "30",
      "title": "Basics of sound, the ear, and hearing",
      "authors": [
        "Susan Robert A Dobie",
        "National Van Hemel",
        "Council"
      ],
      "year": "2004",
      "venue": "Hearing Loss: Determining Eligibility for Social Security Benefits"
    },
    {
      "citation_id": "31",
      "title": "Toronto emotional speech set (TESS)-Younger talker_Happy",
      "authors": [
        "Kate Dupuis",
        "Kathleen Pichora-Fuller"
      ],
      "year": "2010",
      "venue": "Toronto emotional speech set (TESS)-Younger talker_Happy"
    },
    {
      "citation_id": "32",
      "title": "Why President Biden should ban affective computing in federal law enforcement",
      "authors": [
        "Alex Engler"
      ],
      "year": "2021",
      "venue": "Why President Biden should ban affective computing in federal law enforcement"
    },
    {
      "citation_id": "33",
      "title": "A cross-linguistic comparison of perception to formant frequency cues in emotional speech",
      "authors": [
        "Donna Erickson",
        "Albert Rilliard",
        "Takaaki Shochi",
        "Jonghye Han",
        "Hideki Kawahara",
        "Sakakibara"
      ],
      "year": "2008",
      "venue": "COCOSDA"
    },
    {
      "citation_id": "34",
      "title": "Audio-based context recognition",
      "authors": [
        "Antti J Eronen",
        "Vesa T Peltonen",
        "T Juha",
        "Tuomi",
        "P Anssi",
        "Seppo Klapuri",
        "Timo Fagerlund",
        "GaÃ«tan Sorsa",
        "Jyri Lorho",
        "Huopaniemi"
      ],
      "year": "2005",
      "venue": "IEEE Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "35",
      "title": "Normalized mutual information feature selection",
      "authors": [
        "Pablo EstÃ©vez",
        "Michel Tesmer",
        "Claudio Perez",
        "Jacek Zurada"
      ],
      "year": "2009",
      "venue": "IEEE Transactions on neural networks"
    },
    {
      "citation_id": "36",
      "title": "Compulsive buying disorder",
      "authors": [
        "Tatiana Zambrano",
        "Hermano Tavares"
      ],
      "year": "2021",
      "venue": "Textbook of Addiction Treatment: International Perspectives"
    },
    {
      "citation_id": "37",
      "title": "The Laws of Emotion",
      "authors": [
        "H Nico",
        "Frijda"
      ],
      "year": "1988",
      "venue": "The Laws of Emotion"
    },
    {
      "citation_id": "38",
      "title": "Alexa wants to know how you're feeling today",
      "authors": [
        "Sidney Fussell"
      ],
      "year": "2019",
      "venue": "Alexa wants to know how you're feeling today"
    },
    {
      "citation_id": "39",
      "title": "Black-box adversarial attacks through speech distortion for speech emotion recognition",
      "authors": [
        "Jinxing Gao",
        "Diqun Yan",
        "Mingyu Dong"
      ],
      "year": "2022",
      "venue": "EURASIP Journal on Audio, Speech, and Music Processing"
    },
    {
      "citation_id": "40",
      "title": "DAPPER: Label-Free Performance Estimation after Personalization for Heterogeneous Mobile Sensing",
      "authors": [
        "Taesik Gong",
        "Yewon Kim",
        "Adiba Orzikulova",
        "Yunxin Liu",
        "Sung Hwang",
        "Jinwoo Shin",
        "Sung-Ju Lee"
      ],
      "year": "2023",
      "venue": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies"
    },
    {
      "citation_id": "41",
      "title": "Explaining and harnessing adversarial examples",
      "authors": [
        "Ian Goodfellow",
        "Jonathon Shlens",
        "Christian Szegedy"
      ],
      "year": "2014",
      "venue": "Explaining and harnessing adversarial examples",
      "arxiv": "arXiv:1412.6572"
    },
    {
      "citation_id": "42",
      "title": "Analyzing Sentiment | Cloud Natural Language API | Google Cloud",
      "authors": [
        "Google"
      ],
      "year": "2022",
      "venue": "Analyzing Sentiment | Cloud Natural Language API | Google Cloud"
    },
    {
      "citation_id": "43",
      "title": "Police facial recognition robot identifies anger and distress",
      "authors": [
        "Fiona Hamilton"
      ],
      "year": "2020",
      "venue": "Police facial recognition robot identifies anger and distress"
    },
    {
      "citation_id": "44",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "Wei-Ning Hsu",
        "Benjamin Bolte",
        "Hubert Yao-Hung",
        "Kushal Tsai",
        "Ruslan Lakhotia",
        "Abdelrahman Salakhutdinov",
        "Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "45",
      "title": "2021. {WaveGuard}: Understanding and Mitigating Audio Adversarial Examples",
      "authors": [
        "Shehzeen Hussain",
        "Paarth Neekhara",
        "Shlomo Dubnov",
        "Julian Mcauley",
        "Farinaz Koushanfar"
      ],
      "venue": "30th USENIX Security Symposium (USENIX Security 21"
    },
    {
      "citation_id": "46",
      "title": "Black-box adversarial attacks with limited queries and information",
      "authors": [
        "Andrew Ilyas",
        "Logan Engstrom",
        "Anish Athalye",
        "Jessy Lin"
      ],
      "year": "2018",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "47",
      "title": "Surrey audio-visual expressed emotion (savee) database",
      "authors": [
        "Philip Jackson",
        "Sjuosg Haq"
      ],
      "year": "2014",
      "venue": "Surrey audio-visual expressed emotion (savee) database"
    },
    {
      "citation_id": "48",
      "title": "Voice-based determination of physical and emotional characteristics of users",
      "authors": [
        "Huafeng Jin",
        "Shuo Wang"
      ],
      "year": "2018",
      "venue": "US Patent"
    },
    {
      "citation_id": "49",
      "title": "GitHub -Data-Science-kosta/Speech-Emotion-Classification-with-PyTorch",
      "authors": [
        "Kosta Jovanovic"
      ],
      "year": "2021",
      "venue": "GitHub -Data-Science-kosta/Speech-Emotion-Classification-with-PyTorch"
    },
    {
      "citation_id": "50",
      "title": "A study of intentional voice modifications for evading automatic speaker recognition",
      "authors": [
        "Harry Sachin S Kajarekar",
        "Elizabeth Bratt",
        "Rafael Shriberg",
        "Leon De"
      ],
      "year": "2006",
      "venue": "IEEE Odyssey-The Speaker and Language Recognition Workshop. IEEE"
    },
    {
      "citation_id": "51",
      "title": "Understanding emotions",
      "authors": [
        "Dacher Keltner",
        "Keith Oatley",
        "Jennifer Jenkins"
      ],
      "year": "2014",
      "venue": "Understanding emotions"
    },
    {
      "citation_id": "52",
      "title": "Following Privacy Concerns Surrounding Amazon Halo, Klobuchar Urges Administration to Take Action to Protect Personal Health Data",
      "authors": [
        "Amy Klobuchar"
      ],
      "year": "2020",
      "venue": "Following Privacy Concerns Surrounding Amazon Halo, Klobuchar Urges Administration to Take Action to Protect Personal Health Data"
    },
    {
      "citation_id": "53",
      "title": "Detecting emotion primitives from speech and their use in discerning categorical emotions",
      "authors": [
        "Vasudha Kowtha",
        "Vikramjit Mitra",
        "Chris Bartels",
        "Erik Marchi",
        "Sue Booker",
        "William Caruso",
        "Sachin Kajarekar",
        "Devang Naik"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "54",
      "title": "Genetic programming",
      "authors": [
        "R John",
        "Riccardo Koza",
        "Poli"
      ],
      "year": "2005",
      "venue": "Search methodologies"
    },
    {
      "citation_id": "55",
      "title": "Impulse buying and post-purchase regret: a study of shopping behavior for the purchase of grocery products. Abhishek Kumar, Sumana Chaudhuri, Aparna Bhardwaj and Pallavi Mishra, Emotional Intelligence and its Impact on Team Building through Mediation of Leadership Effectiveness",
      "authors": [
        "Abhishek Kumar",
        "Dr Chaudhuri",
        "Dr Bhardwaj",
        "Pallavi Mishra"
      ],
      "year": "2020",
      "venue": "International Journal of Management"
    },
    {
      "citation_id": "56",
      "title": "Harvard professor says surveillance capitalism is undermining democracy",
      "authors": [
        "John Laidler"
      ],
      "year": "2019",
      "venue": "Harvard professor says surveillance capitalism is undermining democracy"
    },
    {
      "citation_id": "57",
      "title": "Emotion-reading tech fails the racial bias test",
      "authors": [
        "Rhue Lauren"
      ],
      "year": "2019",
      "venue": "Emotion-reading tech fails the racial bias test"
    },
    {
      "citation_id": "58",
      "title": "Emotion and adaptation",
      "authors": [
        "Lazarus Richard"
      ],
      "year": "1991",
      "venue": "Emotion and adaptation"
    },
    {
      "citation_id": "59",
      "title": "A frequency warping approach to speaker normalization",
      "authors": [
        "Li Lee",
        "Richard Rose"
      ],
      "year": "1998",
      "venue": "IEEE Transactions on speech and audio processing"
    },
    {
      "citation_id": "60",
      "title": "Are some people suffering as a result of increasing mass exposure of the public to ultrasound in air?",
      "authors": [
        "Timothy G Leighton"
      ],
      "year": "2016",
      "venue": "Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences"
    },
    {
      "citation_id": "61",
      "title": "Emotion and decision making",
      "authors": [
        "Jennifer Lerner",
        "Ye Li",
        "Piercarlo Valdesolo",
        "Karim Kassam"
      ],
      "year": "2015",
      "venue": "Annual review of psychology"
    },
    {
      "citation_id": "62",
      "title": "Patronus: Preventing unauthorized speech recordings with support for selective unscrambling",
      "authors": [
        "Lingkun Li",
        "Manni Liu",
        "Yuguang Yao",
        "Fan Dang",
        "Zhichao Cao",
        "Yunhao Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 18th Conference on Embedded Networked Sensor Systems"
    },
    {
      "citation_id": "63",
      "title": "Practical adversarial attacks against speaker recognition systems",
      "authors": [
        "Zhuohang Li",
        "Cong Shi",
        "Yi Xie",
        "Jian Liu",
        "Bo Yuan",
        "Yingying Chen"
      ],
      "year": "2020",
      "venue": "Proceedings of the 21st international workshop on mobile computing systems and applications"
    },
    {
      "citation_id": "64",
      "title": "Advpulse: Universal, synchronization-free, and targeted audio adversarial attacks via subsecond perturbations",
      "authors": [
        "Zhuohang Li",
        "Yi Wu",
        "Jian Liu",
        "Yingying Chen",
        "Bo Yuan"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security"
    },
    {
      "citation_id": "65",
      "title": "Council post: How can we make the smart speaker feel at home?",
      "authors": [
        "Mark Lippett"
      ],
      "year": "2023",
      "venue": "Council post: How can we make the smart speaker feel at home?"
    },
    {
      "citation_id": "66",
      "title": "Physical-World Attack towards WiFi-based Behavior Recognition",
      "authors": [
        "Jianwei Liu",
        "Yinghui He",
        "Chaowei Xiao",
        "Jinsong Han",
        "Le Cheng",
        "Kui Ren"
      ],
      "year": "2022",
      "venue": "IEEE INFOCOM 2022-IEEE Conference on Computer Communications"
    },
    {
      "citation_id": "67",
      "title": "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English",
      "authors": [
        "R Steven",
        "Frank Livingstone",
        "Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "68",
      "title": "Mel frequency cepstral coefficients for music modeling",
      "authors": [
        "Beth Logan"
      ],
      "year": "2000",
      "venue": "International Symposium on Music Information Retrieval"
    },
    {
      "citation_id": "69",
      "title": "A unified approach to interpreting model predictions",
      "authors": [
        "M Scott",
        "Su-In Lundberg",
        "Lee"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "70",
      "title": "British police to trial facial recognition system that detects your mood",
      "authors": [
        "Thomas Macaulay"
      ],
      "year": "2020",
      "venue": "British police to trial facial recognition system that detects your mood"
    },
    {
      "citation_id": "71",
      "title": "Towards deep learning models resistant to adversarial attacks",
      "authors": [
        "Aleksander Madry",
        "Aleksandar Makelov",
        "Ludwig Schmidt",
        "Dimitris Tsipras",
        "Adrian Vladu"
      ],
      "year": "2017",
      "venue": "Towards deep learning models resistant to adversarial attacks",
      "arxiv": "arXiv:1706.06083"
    },
    {
      "citation_id": "72",
      "title": "Multimodal emotion recognition with high-level speech and text features",
      "authors": [
        "Mariana Rodrigues Makiuchi",
        "Kuniaki Uto",
        "Koichi Shinoda"
      ],
      "year": "2021",
      "venue": "2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)"
    },
    {
      "citation_id": "73",
      "title": "Introduction to Genetic Algorithms -Including Example Code",
      "authors": [
        "Vijini Mallawaarachchi"
      ],
      "year": "2017",
      "venue": "Introduction to Genetic Algorithms -Including Example Code"
    },
    {
      "citation_id": "74",
      "title": "Facial Expression Recognition: Impact of Gender on Fairness and Expressions*",
      "authors": [
        "Cristina Manresa-Yee",
        "Silvia Ramis Guarinos",
        "Jose Maria",
        "Buades Rubio"
      ],
      "year": "2022",
      "venue": "Proceedings of the XXII International Conference on Human Computer Interaction"
    },
    {
      "citation_id": "75",
      "title": "Spectral fusion, spectral parsing and the formation of auditory images",
      "authors": [
        "Stephen Edward"
      ],
      "year": "1984",
      "venue": "Spectral fusion, spectral parsing and the formation of auditory images"
    },
    {
      "citation_id": "76",
      "title": "Emotional AI, soft biometrics and the surveillance of emotional life: An unusual consensus on privacy",
      "authors": [
        "Andrew Mcstay"
      ],
      "year": "2020",
      "venue": "Big Data & Society"
    },
    {
      "citation_id": "77",
      "title": "GitHub -Picovoice/porcupine: On-device wake word detection powered by deep learning",
      "authors": [
        "Eric Mikulin"
      ],
      "year": "2022",
      "venue": "GitHub -Picovoice/porcupine: On-device wake word detection powered by deep learning"
    },
    {
      "citation_id": "78",
      "title": "Automatic speech recognition: An auditory perspective",
      "authors": [
        "Nelson Mogran",
        "HervÃ© Bourlard",
        "Hynek Hermansky"
      ],
      "year": "2004",
      "venue": "Speech processing in the auditory system"
    },
    {
      "citation_id": "79",
      "title": "Sentiment Analysis API | DeepAI",
      "authors": [
        "Nlp Stanford"
      ],
      "year": "2022",
      "venue": "Sentiment Analysis API | DeepAI"
    },
    {
      "citation_id": "80",
      "title": "The Kaldi speech recognition toolkit",
      "authors": [
        "Daniel Povey",
        "Arnab Ghoshal",
        "Gilles Boulianne",
        "Lukas Burget",
        "Ondrej Glembek",
        "Nagendra Goel",
        "Mirko Hannemann",
        "Petr Motlicek",
        "Yanmin Qian",
        "Petr Schwarz"
      ],
      "year": "2011",
      "venue": "IEEE 2011 workshop on automatic speech recognition and understanding"
    },
    {
      "citation_id": "81",
      "title": "GitHub -MiteshPuthran/Speech-Emotion-Analyzer: The neural network model is capable of detecting five different male/female emotions from audio speeches. (Deep Learning, NLP, Python)",
      "authors": [
        "Mitesh Puthran"
      ],
      "year": "2021",
      "venue": "GitHub -MiteshPuthran/Speech-Emotion-Analyzer: The neural network model is capable of detecting five different male/female emotions from audio speeches. (Deep Learning, NLP, Python)"
    },
    {
      "citation_id": "82",
      "title": "Towards an optimal feature set for robustness improvement of sounds classification in a HMM-based classifier adapted to real world background noise",
      "authors": [
        "Asma Rabaoui",
        "Zied Lachiri",
        "Noureddine Ellouze"
      ],
      "year": "2007",
      "venue": "Proc. 4th Int. Multi-Conf. Systems, Signals & Devices"
    },
    {
      "citation_id": "83",
      "title": "Stagnation detection with randomized local search",
      "authors": [
        "Amirhossein Rajabi",
        "Carsten Witt"
      ],
      "year": "2021",
      "venue": "European Conference on Evolutionary Computation in Combinatorial Optimization (Part of EvoStar)"
    },
    {
      "citation_id": "84",
      "title": "Stress, gender and compulsive buying among early adolescents",
      "authors": [
        "A James",
        "Camille Roberts",
        "Roberts"
      ],
      "year": "2012",
      "venue": "Stress, gender and compulsive buying among early adolescents"
    },
    {
      "citation_id": "85",
      "title": "Factors affecting impulse buying behavior of consumers",
      "authors": [
        "Isabel Rosa",
        "Paula Rodrigues",
        "Miguel Lopes",
        "Varela"
      ],
      "year": "2021",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "86",
      "title": "China using 'emotion recognition technology' for surveillance",
      "authors": [
        "Jackie Salo"
      ],
      "year": "2021",
      "venue": "China using 'emotion recognition technology' for surveillance"
    },
    {
      "citation_id": "87",
      "title": "Bias and Fairness on Multimodal Emotion Detection Algorithms",
      "authors": [
        "Matheus Schmitz",
        "Rehan Ahmed",
        "Jimi Cao"
      ],
      "year": "2022",
      "venue": "Bias and Fairness on Multimodal Emotion Detection Algorithms",
      "arxiv": "arXiv:2205.08383"
    },
    {
      "citation_id": "88",
      "title": "Imperio: Robust over-the-air adversarial examples for automatic speech recognition systems",
      "authors": [
        "Lea SchÃ¶nherr",
        "Thorsten Eisenhofer",
        "Steffen Zeiler",
        "Thorsten Holz",
        "Dorothea Kolossa"
      ],
      "year": "2020",
      "venue": "Annual Computer Security Applications Conference"
    },
    {
      "citation_id": "89",
      "title": "Microsoft patents AI Emotion Detection System for xbox",
      "authors": [
        "Hal Eric",
        "Schwartz"
      ],
      "year": "2021",
      "venue": "Microsoft patents AI Emotion Detection System for xbox"
    },
    {
      "citation_id": "90",
      "title": "Circularity in judgments of relative pitch",
      "authors": [
        "N Roger",
        "Shepard"
      ],
      "year": "1964",
      "venue": "The journal of the acoustical society of America"
    },
    {
      "citation_id": "91",
      "title": "Towards learning a universal non-semantic representation of speech",
      "authors": [
        "Joel Shor",
        "Aren Jansen",
        "Ronnie Maor",
        "Oran Lang",
        "Omry Tuval",
        "Felix De",
        "Chaumont Quitry",
        "Marco Tagliasacchi",
        "Ira Shavitt",
        "Dotan Emanuel",
        "Yinnon Haviv"
      ],
      "year": "2020",
      "venue": "Towards learning a universal non-semantic representation of speech",
      "arxiv": "arXiv:2002.12764"
    },
    {
      "citation_id": "92",
      "title": "Smart speakers offer new legal challenges as privacy goes public",
      "year": "2020",
      "venue": "Smart speakers offer new legal challenges as privacy goes public"
    },
    {
      "citation_id": "93",
      "title": "Vocal indicators of emotional stress",
      "authors": [
        "Savita Sondhi",
        "Munna Khan",
        "Ritu Vijay",
        "Ashok K Salhan"
      ],
      "year": "2015",
      "venue": "International Journal of Computer Applications"
    },
    {
      "citation_id": "94",
      "title": "RTCA DO-297/EUROCAE ED-124 Integrated Modular Avionics (IMA) Design Guidance and Certification Considerations",
      "authors": [
        "R Cary",
        "Leanna Spitzer",
        "Rierson"
      ],
      "year": "2017",
      "venue": "Digital Avionics Handbook"
    },
    {
      "citation_id": "95",
      "title": "The best voice assistant | ZDNET",
      "year": "2021",
      "venue": "Reviews.com Staff"
    },
    {
      "citation_id": "96",
      "title": "Comparison of different impulse response measurement techniques",
      "authors": [
        "Guy-Bart Stan",
        "Jean-Jacques Embrechts",
        "Dominique Archambeau"
      ],
      "year": "2002",
      "venue": "Journal of the Audio engineering society"
    },
    {
      "citation_id": "97",
      "title": "A scale for the measurement of the psychological magnitude pitch",
      "authors": [
        "Stanley Stevens",
        "John Volkmann",
        "Edwin Broomell Newman"
      ],
      "year": "1937",
      "venue": "The journal of the acoustical society of america"
    },
    {
      "citation_id": "98",
      "title": "Alexa, stop spying on me!\" speech privacy protection against voice assistants",
      "authors": [
        "Ke Sun",
        "Chen Chen",
        "Xinyu Zhang"
      ],
      "year": "2020",
      "venue": "Proceedings of the 18th conference on embedded networked sensor systems"
    },
    {
      "citation_id": "99",
      "title": "Improving Speech Emotion Recognition via Fine-tuning ASR with Speaker Information",
      "authors": [
        "Bao Thang",
        "Tung Lam Nguyen",
        "Son Dang",
        "Nhat Minh Le"
      ],
      "year": "2022",
      "venue": "2022 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "100",
      "title": "A taxonomy and terminology of adversarial machine learning",
      "authors": [
        "Elham Tabassi",
        "Kevin Burns",
        "Michael Hadjimichael",
        "Andres Molina-Markham",
        "Julian Sexton"
      ],
      "year": "2019",
      "venue": "NIST IR"
    },
    {
      "citation_id": "101",
      "title": "The NAIST text-to-speech system for the Blizzard Challenge",
      "authors": [
        "Shinnosuke Takamichi",
        "Kazuhiro Kobayashi",
        "Kou Tanaka",
        "Tomoki Toda",
        "Satoshi Nakamura"
      ],
      "year": "2015",
      "venue": "Proc. Blizzard Challenge workshop"
    },
    {
      "citation_id": "102",
      "title": "Targeted adversarial examples for black box audio systems",
      "authors": [
        "Rohan Taori",
        "Amog Kamsetty",
        "Brenton Chu",
        "Nikita Vemuri"
      ],
      "year": "2019",
      "venue": "2019 IEEE security and privacy workshops (SPW)"
    },
    {
      "citation_id": "103",
      "title": "Voice assistants and smart speakers in everyday life and in education",
      "authors": [
        "George Terzopoulos",
        "Maya Satratzemi"
      ],
      "year": "2020",
      "venue": "Informatics in Education"
    },
    {
      "citation_id": "104",
      "title": "The role of F3 in the vocal expression of emotions",
      "authors": [
        "Teija Waaramaa",
        "Paavo Alku",
        "Anne-Maria Laukkanen"
      ],
      "year": "2006",
      "venue": "Logopedics Phoniatrics Vocology"
    },
    {
      "citation_id": "105",
      "title": "Monopitched expression of emotions in different vowels",
      "authors": [
        "Teija Waaramaa",
        "Anne-Maria Laukkanen",
        "Paavo Alku",
        "Eero VÃ¤yrynen"
      ],
      "year": "2008",
      "venue": "Folia Phoniatrica et Logopaedica"
    },
    {
      "citation_id": "106",
      "title": "Spotify patented emotional recognition technology to recommend songs based on user's emotions",
      "authors": [
        "Najah Walker"
      ],
      "year": "2022",
      "venue": "Spotify patented emotional recognition technology to recommend songs based on user's emotions"
    },
    {
      "citation_id": "107",
      "title": "The Tech behind Amazon alexa",
      "authors": [
        "Julian Wallis"
      ],
      "year": "2022",
      "venue": "The Tech behind Amazon alexa"
    },
    {
      "citation_id": "108",
      "title": "Communications standard dictionary",
      "authors": [
        "Martin Weik"
      ],
      "year": "2012",
      "venue": "Communications standard dictionary"
    },
    {
      "citation_id": "109",
      "title": "Automatically evading classifiers: A case study on PDF malware classifiers",
      "authors": [
        "Qi Xu Weilin",
        "Evans Yanjun",
        "David"
      ],
      "year": "2016",
      "venue": "Proceedings of the 23rd Annual Network and Distributed System Security Symposium"
    },
    {
      "citation_id": "110",
      "title": "Compulsive buying-features and characteristics of addiction",
      "authors": [
        "Aviv Weinstein",
        "Aniko Maraz",
        "Mark Griffiths",
        "Michel Lejoyeux",
        "Zsolt Demetrovics"
      ],
      "year": "2016",
      "venue": "Neuropathology of drug addictions and substance misuse"
    },
    {
      "citation_id": "111",
      "title": "On the acoustics of emotion in audio: what speech, music, and sound have in common",
      "authors": [
        "Felix Weninger",
        "Florian Eyben",
        "W BjÃ¶rn",
        "Marcello Schuller",
        "Klaus Mortillaro",
        "Scherer"
      ],
      "year": "2013",
      "venue": "Frontiers in psychology"
    },
    {
      "citation_id": "112",
      "title": "Meet the Star Witness: Your Smart Speaker",
      "authors": [
        "Wired"
      ],
      "year": "2020",
      "venue": "Meet the Star Witness: Your Smart Speaker"
    },
    {
      "citation_id": "113",
      "title": "Multiple change-point audio segmentation and classification using an MDL-based Gaussian model",
      "authors": [
        "Chung-Hsien Wu",
        "Chia-Hsin Hsieh"
      ],
      "year": "2006",
      "venue": "IEEE Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "114",
      "title": "Semi-black-box attacks against speech recognition systems using adversarial samples",
      "authors": [
        "Yi Wu",
        "Jian Liu",
        "Yingying Chen",
        "Jerry Cheng"
      ],
      "year": "2019",
      "venue": "IEEE"
    },
    {
      "citation_id": "115",
      "title": "Speech emotion recognition with multiscale area attention and data augmentation",
      "authors": [
        "Mingke Xu",
        "Fan Zhang",
        "Xiaodong Cui",
        "Wei Zhang"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "116",
      "title": "Investigating bias and fairness in facial expression recognition",
      "authors": [
        "Tian Xu",
        "Jennifer White",
        "Sinan Kalkan",
        "Hatice Gunes"
      ],
      "year": "2020",
      "venue": "Computer Vision-ECCV 2020 Workshops: Glasgow, UK"
    },
    {
      "citation_id": "117",
      "title": "Alexa is always listening -and so are Amazon workers -ABC News",
      "authors": [
        "Soo Youn"
      ],
      "year": "2019",
      "venue": "Alexa is always listening -and so are Amazon workers -ABC News"
    },
    {
      "citation_id": "118",
      "title": "{SMACK}: Semantically Meaningful Adversarial Audio Attack",
      "authors": [
        "Zhiyuan Yu",
        "Yuanhaur Chang",
        "Ning Zhang",
        "Chaowei Xiao"
      ],
      "year": "2023",
      "venue": "32nd USENIX Security Symposium"
    },
    {
      "citation_id": "119",
      "title": "Dolphinattack: Inaudible voice commands",
      "authors": [
        "Guoming Zhang",
        "Chen Yan",
        "Xiaoyu Ji",
        "Tianchen Zhang",
        "Taimin Zhang",
        "Wenyuan Xu"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 ACM SIGSAC conference on computer and communications security"
    },
    {
      "citation_id": "120",
      "title": "Perceived stress and online compulsive buying among women: A moderated mediation model",
      "authors": [
        "Yueli Zheng",
        "Xiujuan Yang",
        "Qingqi Liu",
        "Xiaowei Chu",
        "Qitong Huang",
        "Zongkui Zhou"
      ],
      "year": "2020",
      "venue": "Computers in Human Behavior"
    },
    {
      "citation_id": "121",
      "title": "Big other: surveillance capitalism and the prospects of an information civilization",
      "authors": [
        "Shoshana Zuboff"
      ],
      "year": "2015",
      "venue": "Journal of information technology"
    }
  ]
}