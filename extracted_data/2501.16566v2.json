{
  "paper_id": "2501.16566v2",
  "title": "Affectgpt: A New Dataset, Model, And Benchmark For Emotion Understanding With Multimodal Large Language Models",
  "published": "2025-01-27T23:18:39Z",
  "authors": [
    "Zheng Lian",
    "Haoyu Chen",
    "Lan Chen",
    "Haiyang Sun",
    "Licai Sun",
    "Yong Ren",
    "Zebang Cheng",
    "Bin Liu",
    "Rui Liu",
    "Xiaojiang Peng",
    "Jiangyan Yi",
    "Jianhua Tao"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The emergence of multimodal large language models (MLLMs) advances multimodal emotion recognition (MER) to the next level-from naive discriminative tasks to complex emotion understanding with advanced video understanding abilities and natural language description. However, the current community suffers from a lack of large-scale datasets with intensive, descriptive emotion annotations, as well as a multimodalcentric framework to maximize the potential of MLLMs for emotion understanding. To address this, we establish a new benchmark for MLLMbased emotion understanding with a novel dataset (MER-Caption) and a new model (AffectGPT). Utilizing our model-based crowd-sourcing data collection strategy, we construct the largest descriptive emotion dataset to date (by far), featuring over 2K fine-grained emotion categories across 115K samples. We also introduce the Af-fectGPT model, designed with pre-fusion operations to enhance multimodal integration. Finally, we present MER-UniBench, a unified benchmark with evaluation metrics tailored for typical MER tasks and the free-form, natural language output style of MLLMs. Extensive experimental results show AffectGPT's robust performance across various MER tasks. We have released both the code and the dataset to advance research and development in emotion understanding: https://github.com/zeroQiaoba/AffectGPT.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotions encapsulate human intentions, and accurately recognizing emotional states is essential for enhancing humancomputer interaction experiences  (Minsky, 1988) . Emotions can be conveyed through various human behaviors in different forms, giving rise to the task of multimodal emotion recognition (MER), which integrates multimodal information (e.g., audio, video, and text) to evaluate human emotional states. As a critical area in artificial intelligence, MER has broad applications, ranging from education  (Schutz, 2007)  and psychological counseling  (Liu et al., 2021)  to empathic embodied robots  (Spezialetti et al., 2020) .\n\nTraditional methods primarily rely on discriminative models that map human emotions to the most likely categories from predefined emotion taxonomies. The most widely used taxonomy is Ekman's theory  (Ekman & Keltner, 1970) , which classifies all emotions into six basic categories: sadness, happiness, fear, anger, surprise, and disgust. However, such categorical frameworks exhibit some limitations in modeling human affective states. For example, our emotional expressions are diverse and nuanced due to culture-specific idioms  (Matsumoto, 2001) , context-dependent metaphors  (Kövecses, 2003) , and highly personalized behavioral patterns  (Izard et al., 1993) . Current closed-set classification paradigms fail to capture the rich diversity of emotional expressions in real-world scenarios  (Plutchik, 1980) . Meanwhile, the rigid emotion taxonomies oversimplify the continuous spectrum of emotional experiences by forcing discrete labels (e.g., anger or surprise) onto nuanced affective states that often coexist  (Cowen & Keltner, 2017) . Illustrations are provided in Figure  1 , where the diverse and coexisting issues are presented in Figs. Recent advances in multi-modal large language models (MLLMs) enable emotion understanding to move beyond traditional discriminative approaches, embracing a more generative framework  (Liang et al., 2024) . This shift allows models to describe complex, coexisting emotional states in natural language. With the vast vocabulary, MLLMs can generate diverse, descriptive emotion categories beyond basic emotions, offering new opportunities for emotional understanding. However, recent research highlights Description: In the video, his eyes are wide open and his mouth is also open, indicating a surprised facial expression. In the audio, the character speaks with a stutter, which usually expresses feelings of nervousness, anxiety, or unease. Combined with the text content, the character seems to be unhappy and angry due to the prejudice of the people around him.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "(B) Coexistence",
      "text": "Description: The sentence might be the woman's comment or reaction to someone nearby. Given the audio cues of a steady tone with humor and the woman's smile and glance to the left, we can infer that her words carry a mocking or sarcastic tone. Thus, this sentence likely expresses the woman's mockery of the other person's lack of understanding or recognition of something, conveyed in a humorous manner.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "(A) Diversity",
      "text": "Figure  1 : Emotion complexity analysis. Human emotions are often diverse and coexist simultaneously. Such complex emotional states are difficult to describe using discriminative frameworks. However, MLLMs can generate emotional descriptions, offering new possibilities for complex emotion modeling. Since the original videos contain real people, to address copyright concerns, we first use DemoAI to remove personal information and then proceed with visualization.\n\nthat MLLMs still face limitations in emotion understanding  (Lian et al., 2024a; d) . To address these challenges, this paper aims to advance emotional understanding from two key perspectives: the dataset and the model. Finally, we establish a unified benchmark tailored to the free-form, natural language output style of MLLMs.\n\nDataset. The current community still suffers from a lack of large-scale datasets with intensive, descriptive emotion annotations to realize the potential of MLLMs. The annotation strategies for constructing descriptive emotion datasets can be classified into three types: model-based, humanbased, and human-model collaborative strategies. The human-based strategy is the most common way to construct emotion datasets with rich descriptive annotations. However, it's costly to conduct crowd-sourcing to scale up the dataset size with this purely manual annotation manner. Besides, humans tend to focus on main cues, resulting in brief and incomplete descriptions  (Liu et al., 2022a) . Thus, researchers propose model-based automatic annotation approaches. However, due to the lack of human proofreading, this approach may result in insufficient label quality  (Cheng et al., 2024) . Recently,  Lian et al. (2024a)  propose a humanmodel collaborative strategy, in which models provide prelabeled cues and humans conduct multiple rounds of checks, which can be seen as a human-led, model-assisted strategy. Although this approach offers more comprehensive descriptions, it is costly and difficult to scale the dataset. To balance label quality and dataset size, we introduce a novel annotation strategy that conducts model-based crowd-sourcing labeling with human priors, named model-led human-assisted, to construct a large-scale emotion descriptive dataset with diverse emotional categories.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Models.",
      "text": "Existing MLLMs typically consist of three key components: a modality encoder that converts audio and video into low-level hidden features, a connector that transforms these features into a format more suitable for LLMs, and an LLM-based generator that produces responses based on the given instructions. While the results of MLLMs are promising, existing models generally leave everything of multimodal fusion to LLMs, which is insufficient for MER that emphasizes multimodal characteristics. This paper introduces the AffectGPT model, designed with a pre-fusion operation to emphasize multimodal integration.\n\nBenchmark. Although it's desirable to generate emotional descriptions in a free-form, natural language style (see Appendix D), this poses challenges for quantitative comparison. To address this, we propose metrics specifically designed for this output style. Additionally, to ensure fair and comprehensive evaluation, we introduce MER-UniBench, a benchmark that incorporates three typical tasks: fine-grained emotion recognition, basic emotion recognition, and sentiment analysis. We believe this work can enhance the emotion understanding capabilities of MLLMs and open possibilities for complex emotion modeling. The main contributions of this paper are summarized as follows:\n\n• We construct a large-scale emotional description dataset MER-Caption, which adopts a model-led, human-assisted annotation strategy to strike a balance between label quality and dataset size.\n\n• We develop AffectGPT, which uses additional prefusion operations to enhance multimodal integration, thereby improving emotion understanding.\n\nTable  1 : Dataset comparison. \"I\", \"A\", \"V\", and \"T\" stand for image, audio, video, and text, respectively. We observe that descriptive datasets contain more diverse labels, providing the potential for modeling complex emotions.   (Mollahosseini et al., 2017)  I 450,000 8 Human EmoDB  (Burkhardt et al., 2005)  A 535 7 Human MSP-Podcast  (Lotfian & Busso, 2017)  A 73,042 8 Human DFEW  (Jiang et al., 2020)  V 11,697 7 Human FERV39k  (Wang et al., 2022)  V 38,935 7 Human MER2023  (Lian et al., 2023)  A,V,T 5,030 6 Human MELD  (Poria et al., 2019)  A,V,T 13,708 7 Human",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Descriptive Dataset",
      "text": "EmoVIT ( • We build MER-UniBench, which encompasses typical MER tasks with tailored metrics. This benchmark can offer comprehensive evaluation results for MLLMbased emotion understanding.\n\n• Extensive experiments demonstrate the effectiveness of AffectGPT, which achieves over a 9% performance improvement compared to existing MLLMs. The former directly provides emotion labels (e.g., happy), while the latter offers textual descriptions related to emotions. We first conduct preliminary experiments to extract emotion labels from descriptive datasets (see Appendix E).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Mer-Caption: Dataset Construction",
      "text": "As shown in Table  1 , descriptive datasets contain more diverse labels, offering the potential to capture complex emotions. Thus, this paper focuses on descriptive datasets.\n\nBased on the annotation manner, descriptive datasets can be categorized into model-based, human-based, and humanmodel collaborative strategies (see Table  1 ). Although the model-based approach makes it easy to expand the dataset size, it mainly relies on experience to select models and lacks human intervention, resulting in insufficient label quality  (Cheng et al., 2024) . To enhance label quality,  Liu et al. (2022a)  relied on human annotators to generate emotion descriptions. However, humans tend to focus on primary clues, easily leading to incomplete descriptions. To this end,  Lian et al. (2024a)  proposed a human-led, model-assisted strategy. Specifically, the model first provides pre-labeled descriptions, and then multiple annotators perform multiround checks. Although this strategy produces more comprehensive descriptions, it comes with high annotation costs and faces challenges in scaling the dataset. In this paper, we review these annotation methods and introduce a modelled, human-assisted strategy. As shown in Figure  2 , we leverage human priors to guide description generation and sample filtering, ultimately achieving automatic annotation for unlabeled data. Using this strategy, we construct the MER-Caption dataset, which includes 115K coarse-labeled samples and 31K fine-labeled samples, making a significant contribution to current descriptive datasets. The raw data in MER-Caption is sourced from the unlabeled portions of MER2024  (Lian et al., 2024b) , with explicit permission from the dataset owners. Therefore, this paper does not involve the collection of new data but provides additional annotations for existing datasets. Appendix H provides more comparisons with existing datasets.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Description Generation",
      "text": "The choice of base models is critical for generating accurate descriptions. Unlike previous work that relied solely on experience  (Cheng et al., 2024) , we guide model selection using human priors. Specifically, we first select a small subset of samples for preliminary experiments. In this phase, we annotate fine-grained labels for each sample, allowing annotators to assign any emotions they deem appropriate, thus providing more diverse and precise labels. Based on the results in preliminary experiments (see Appendix F), we employ SALMONN  (Tang et al., 2023)  as the audio LLM (ALLM) to generate audio cues, Chat-UniVi  (Jin et al., 2024)  as the video LLM (VLLM) to extract visual cues, and GPT-3.5 (OpenAI, 2022) (\"gpt-3.5-turbo-16k-0613\") to merge the audio and video cues with text content. Then, to further reduce annotation costs, we experimented with replacing GPT-3.5 with other open-source LLMs but observed a drop in performance. The primary reason is that multimodal fusion in MER is inherently complex, often en-",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Mer-Caption 115K",
      "text": "Model-led Human-Assisted Description Generation",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Model-Led Human-Assisted Sample Filtering",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Mer-Caption+ 31K",
      "text": "GPT Merge countering issues such as modality conflict, where inconsistencies or contradictions arise between different modalities (see Appendix G). This places high demands on the LLM's reasoning capabilities. Then, we adopt the above strategy for automatic annotation and create the MER-Caption dataset.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Sample Filtering",
      "text": "Since the descriptions generated by the above process have not been manually verified, MER-Caption inevitably contains some errors. To this end, we implement a two-level filtering process to enhance the label quality.\n\nLow-level Filtering. First, we observe that some samples contain mismatched audio and video. As shown in Figure  2 , the visible person is not speaking, while the audio comes from an invisible person. This setup differs from our task, where we aim to analyze a person's emotions based on their audio, video, and text content. Mismatched data complicates this task, shifting the focus to understanding how the interlocutor's actions may influence the target person's emotions. Therefore, we remove such data and plan to address this issue in future work. To automatically determine whether the visible person is speaking, we use TalkNet  (Tao et al., 2021) . Preliminary experiments indicate that this tool achieves over 90% accuracy in identifying the speaking individual. Then, we remove samples with mismatched audio and video. Second, the length distribution of the generated descriptions roughly follows a Gaussian distribution (see Figure  2 ). Preliminary experiments reveal that descriptions at both ends of the distribution are more likely to contain errors. For instance, when ALLM and VLLM (in Section 2.1) fail to generate responses, the resulting descriptions tend to be short. As a result, we further remove descriptions located at both ends of the distribution.\n\nHigh-level Filtering. In addition to low-level filtering, we propose a model-based crowdsourcing technique for highlevel filtering. Specifically, we train multiple multimodal emotion and sentiment classifiers using human-annotated categorical datasets. Guided by MERBench  (Lian et al., 2024c) , we use CLIP ViT-L  (Radford et al., 2021)  as the visual encoder and HUBERT-L  (Hsu et al., 2021)  as the acoustic encoder, followed by an attention-based fusion strategy to make final emotion and sentiment predictions. These pre-trained models are then used to predict labels for unlabeled data, generating multiple predictions for each sample. To mitigate potential prediction errors, we apply majority voting to determine the final label, ensuring more reliable results. We refer to this process as model-based crowdsourcing. Alternatively, emotions and sentiments can also be predicted based on the descriptions using the strategy outlined in Appendix E. If the labels extracted from the descriptions differ from those obtained through modelbased crowdsourcing, we consider these descriptions to be of low quality and remove them. Through this process, we can extract knowledge from multiple human-based datasets to guide sample selection. After applying multi-level filtering, we obtain the MER-Caption+ dataset. Table  1  presents detailed comparisons between our dataset and existing ones, highlighting that our dataset is the largest multimodal emotion description dataset with diverse emotion categories. AV-LLM mainly facilitates cross-modal interaction within the language model. In AffectGPT, we move the cross-modal interaction outside the language model and use a pre-fusion operation to enhance multimodal integration. In these figures, P can be determined based on the requirement of whether to include X t .",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Affectgpt: Model Design",
      "text": "Our primary goal is to map audio-video-text inputs to emotion-related descriptions. In this section, we first review the current mainstream architectures. We then introduce AffectGPT, a model specifically designed to highlight multimodal characteristics in emotion understanding.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Mainstream Architecture",
      "text": "MLLM aims to understand multimodal input and generate appropriate responses based on the input and user instructions. Unlike pure-text LLMs, the primary challenge for MLLMs lies in enabling the model to perceive multimodal input, i.e., providing the model with \"eyes\" and \"ears\". In existing models, the most common approach is to first extract modality-specific embeddings and then align them with the LLM through projection layers. For audio-video joint tasks, Audio-Video LLMs (AV-LLMs) typically facilitate cross-modal interaction within the language model. Figure  3  illustrates the current mainstream architecture.\n\nFormally, for each sample X, we represent its video, audio, and text content as X v , X a , and X t , respectively. Given an instruction Q, the goal is to output the correct response R.\n\nFor the visual input X v , we use a video expert to encode it into a latent space Z v , then apply a projector\n\nSimilarly, for the acoustics input X a , we use an audio expert and a projector to generate the acoustic embeddings Z a and tokens H a . For the instruction Q and text content X t , we use a template to merge them into a prompt P, and then map them to the corresponding tokens through the tokenizer and embedding layer in the language model. After obtaining these tokens, we concatenate them and feed them into the LLM decoder. The primary objective is to maximize the likelihood of the target response R, conditioned on multimodal content (X v , X a , X t ) and user instruction Q:\n\nThe above formula is optimized in an autoregressive manner, consistent with the objective function of LLMs. We represent the response as R = {r i } Lr i=1 , where L r is the number of tokens. Then, Eq. 1 is transformed into:\n\n(2)\n\nIn this equation, r l is the current token to be predicted, and R <l = {r i } l-1 i=1 is the previously generated tokens, which serve as additional conditioning during training.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Pre-Fusion Operation",
      "text": "Mainstream AV-LLMs leave everything of cross-modal interaction to the LLMs (in Figure  3 ), which is insufficient for handling MER with multimodal characteristics. To address this, we propose a pre-fusion operation that moves the cross-modal interaction outside the LLMs, further enhancing multimodal integration. We refer to this model as AffectGPT. This paper introduces two types of pre-fusion operations: Q-Former-based and attention-based pre-fusion. By default, we apply this operation to Z v ∈ R tv×d and Z a ∈ R ta×d . We also experimented with H v and H a , but this choice led to a decrease in performance.\n\nQ-Former. In this module, we preserve the temporal information in the vision features Z v and audio features Z a , and utilize Q-Former  (Li et al., 2023b)  for multimodal fusion. Specifically, to compress the multimodal content, we first create K learnable query tokens Z q ∈ R K×d . Then, we interact Z q with the concatenated Z v and Z a through cross-attention, thereby distilling the knowledge from the multimodal content into the query tokens. Formally, this process can be represented as:\n\nwhere Z av ∈ R (ta+tv)×d , with the concatenation operation applied along the temporal dimension. Here, Z f ∈ R K×d , and PE(•) represents the positional encoding.\n\nAttention. Unlike Q-Former which preserves temporal information, we propose a simpler architecture that directly compresses temporal information and applies attention mechanisms for multimodal fusion. This simplified module is inspired by MERBench  (Lian et al., 2024c) , which proves that in MER tasks, features with temporal information do not always lead to better performance than compressed features. Formally, we first apply average pooling to compress unimodal features. Then, we calculate the attention weights to emphasize important modalities:\n\nwhere\n\nFinally, we obtain the fused features\n\nRegarding computational efficiency, the pre-fusion operation relies on Q-Former or attention mechanisms, which are significantly less computationally intensive than LLMs.\n\nTheoretically, the Q-Former enables cross-modal interaction by distilling multimodal content into query tokens, whereas the attention mechanism achieves this by dynamically computing attention weights based on multimodal inputs.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Mer-Unibench: Evaluation Benchmark",
      "text": "We introduce MER-UniBench, a comprehensive evaluation benchmark designed to cover typical MER tasks. Given the free-form, natural language output style of MLLMs (see Appendix D), we also design specialized evaluation metrics. More details can be found in Appendix J.\n\nFine-grained Emotion Recognition. This task enables the prediction of fine-grained emotions, extending beyond basic categories. OV-MERD  (Lian et al., 2024a ) is a typical dataset for this task. To improve the reliability of the evaluation results, we expand its dataset size, referring to it as OV-MERD+. For the evaluation metrics, we draw inspiration from previous work  (Lian et al., 2024a)  and calculate results in two steps: eliminating the impact of synonyms and using set-level metrics. First, we apply a three-level grouping strategy to mitigate the impact of synonyms:\n\n• Level 1. We map different forms of emotion words to their base form. For example, we map happier and happiness to happy. This function is denoted as F l1 (•).\n\n• Level 2. We map synonyms to a unified label. For example, we map happy and joyful to happy. This mapping function is represented as F l2 (•).\n\n• Level 3. Emotion wheel provides natural grouping information, with core emotions displayed in the inner part and more nuanced labels in the outer part  (Plutchik, 1980) . Since there is no consensus on the emotion wheel, we use K emotion wheels (see Appendix K).\n\nFor each sector of the emotion wheel\n\nwe map all outer labels to the corresponding inner labels. This mapping function is denoted as\n\nThe above grouping functions can be summarized as:\n\nFor each sample, the number of labels is variable. Therefore, we define a set-based evaluation metric. Specifically, suppose the dataset contains N samples. For sample x i , the true labels are Y i = {y j i } ni j=1 , and the predicted labels are Ŷi = {ŷ j i } ni j=1 . The evaluation metric is defined as follows:\n\nFinally, we compute the average results across different emotion wheels for ranking. Take F s as an example:\n\nHere, Precision s indicates the number of correctly predicted labels, and Recall s indicates whether the prediction covers all ground truth. F s is a harmonic mean of two metrics. Since F s considers both accuracy and completeness, we use it as the primary metric, with Precision s and Recall s serving as secondary metrics. To extract the predicted emotions Ŷi , we employ the strategy mentioned in Appendix E.\n\nBasic Emotion Recognition. This task is a key branch of MER, whose main goal is to select the most likely label from a fixed set of basic emotions. For this task, we select four widely used benchmark datasets: MER2023  (Lian et al., 2023) , MER2024  (Lian et al., 2024b) , IEMOCAP  (Busso et al., 2008) , and MELD  (Poria et al., 2019) . However, the output of MLLMs, Ŷi = {ŷ j i } ni j=1 , contains a variable number of labels, while the dataset only provides one true label y i . In this case, traditional metrics (such as accuracy) are not suitable for performance evaluation. To address this, we propose a new metric, hit rate, which is set to 1 Table 2: Main results. This table presents the results for the primary metrics, with Section 4 outlining the primary metrics for each task. The values for other metrics can be found in Appendix L. In this table, \"MOSI\", \"MOSEI\", \"SIMS\", and \"SIMS v2\" refer to CMU-MOSI, CMU-MOSEI, CH-SIMS, and CH-SIMS v2, respectively. The last column shows the dataset-wise mean score, i.e., the average score across all datasets. when y i ∈ Ŷi and 0 otherwise. Considering that Ŷi is in free-form and y i belongs to basic emotions Y, we may encounter cases where ŷi / ∈ Y. To this end, we use the mapping function G w k (•) and define the metric as follows:\n\nwhere I[•] is an indicator function. The motivation for this metric stems from the fact that basic emotion recognition tasks typically provide majority-voted labels y i , which are generally reliable. However, emotion descriptions produce free-form outputs Ŷi that may contain multiple labels, including fine-grained ones beyond basic emotions. Therefore, we use the hit rate as the metric, ensuring that the basic label y i should be at least in Ŷi .\n\nDuring the design of this metric, we also explored the possibility of evaluating potentially incorrect labels in Ŷi . However, the labels in Ŷi that differ from the basic label y i are not necessarily incorrect -they may represent some finegrained emotions not covered by basic categories. Since basic emotion recognition tasks lack fine-grained reference labels, we have not yet established appropriate evaluation metrics for this purpose. This remains an important research direction for our future work.\n\nSentiment Analysis. This task is more fundamental than the two tasks mentioned above, aiming to predict the sentiment polarity. For this task, we select four benchmark datasets: CMU-MOSI (Zadeh et al., 2017), CMU-MOSEI  (Zadeh et al., 2018) , CH-SIMS  (Yu et al., 2020) , and CH-SIMS v2  (Liu et al., 2022b) . For these benchmark datasets, the original labels are floating-point values, ranging from [-1, 1] or  [-3, 3] . We map scores of < 0 to negative sentiment and scores of > 0 to positive sentiment. To extract sentiment labels from the MLLM's output, we employ the strategy outlined in Appendix E. Following previous work  (Zadeh et al., 2017; 2018) , we evaluate performance using accuracy (ACC) and weighted average F-score (WAF). Due to the inherent label imbalance, we choose WAF as the primary metric and ACC as the secondary metric.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Results And Discussion",
      "text": "In this section, we present the experimental results and provide an in-depth analysis. Detailed implementation information can be found in Appendix B.\n\nMain Results. We compare the performance of Affect-GPT with other MLLMs on MER-UniBench. Since our inputs include audio, video, and text content, we only select MLLMs that support at least audio or video. For models that support both audio and video, we test different modality combinations. Model cards are provided in Appendix C. To ensure a fair comparison, we use their official weights and input corresponding multimodal content, asking them to infer the emotional state. In Table  2 , AffectGPT significantly outperforms existing MLLMs. This can be attributed to the fact that current instruction datasets pay little attention to MER tasks. Additionally, existing models place the entire multimodal fusion within the LLM, which is insufficient for MER tasks that require effective multimodal integration. By leveraging our newly proposed dataset and model, we provide a promising approach to enhancing emotion understanding capability in MLLMs. Meanwhile, for different datasets, increasing the input modality does not always improve performance, as it may also introduce irrelevant information that interferes with emotional understanding.\n\nEffectiveness of MER-Caption. Table  3  compares the performance of MER-Caption with existing datasets. For a fair comparison, we use the same model architecture and experimental settings and only change the training data. For general instruction datasets, we further conduct filtering experiments to remove samples without emotion-related content, emphasizing emotion-related subsets. Specifically, we use the prompt in Appendix E and extract emotion labels from each instruction-answer pair. Samples yielding empty emotion outputs are removed.\n\nIn Table  3 , the excellent performance of MER-Caption proves the limitations of current datasets in addressing MER. On the one hand, general instruction datasets pay insufficient attention to emotion-related tasks. On the other hand, emotional description datasets often suffer from inadequate dataset scales or insufficient annotation quality. Therefore, our dataset can serve as an important complement to existing datasets. Meanwhile, for the general instruction datasets, the filtering approach is less effective on the LLaVA and VideoChat datasets. We hypothesize that the detailed descriptions in non-emotion subsets may also provide valuable cues for inferring emotional states in some scenarios.\n\nFurthermore, we would like to acknowledge that MER-Caption+ may contain inaccurate descriptions due to the use  of an automatic annotation strategy without manual checks. However, the experimental results in Table  3  show that MER-Caption+ achieves significantly better performance than the manually annotated MAFW dataset. The main reason is that humans tend to focus on major clues, which can easily lead to incomplete descriptions. These results confirm that, despite the lack of manual checks in MER-Caption+, we can still ensure the quality of the labels. In the future, we will investigate other post-filtering techniques to further improve MER-Caption+'s annotation quality.\n\nAblation Study on MER-Caption. As shown in Table  4 , compared to the results without filtering or with only lowlevel filtering, our two-level filtering leads to a performance improvement, further verifying the effectiveness of our filtering technique. These findings underscore that dataset quality is as critical as quantity, and fewer training samples do not necessarily lead to worse performance. Please see Appendix M for more details.\n\nAblation Study on Model. Table  5  compares different architectures and examines the impact of pre-fusion operations. Our results show that pre-fusion operations generally improve performance. This highlights the importance of treating cross-modal interactions as separate modules to more effectively capture multimodal characteristics.\n\nAnalysis of Input Impact. Table  6  reveals the impact of different inputs. The distinction between \"face\" and \"frame\" lies in whether an additional face extractor is used to extract faces from frames. We observe a general trend: multimodal results outperform unimodal results. These findings suggest that humans express emotions through multiple modalities, and integrating them leads to improved performance. Additionally, face inputs slightly outperform frame inputs, and their combination does not result in further improvement. This suggests that current MER datasets mainly focus on people, with limited emotional information conveyed through the environment. As a result, in this paper, we default to using audio, face, and text as the inputs.\n\nUser Study. We conduct a user study to evaluate the quality of our proposed dataset. Since MERR-Fine and MERR-Coarse  (Cheng et al., 2024)  share some samples with our dataset, we randomly select 20 overlapping samples. We then hire four expert annotators and present them with two descriptions for each sample: one from our dataset and one from the other datasets. The annotators are asked to watch the video and select the more accurate description.\n\nAs shown in Table  7 , our dataset provides more accurate descriptions than both the model-based MERR-Coarse and the human-filtered MERR-Fine, thereby validating the effectiveness of our proposed annotation strategy.\n\nChoice of LLMs. This paper adopts Qwen2.5 as the default LLM. In Figure  4 (a), we further explore the impact of different LLMs. Experimental results show that the performance difference brought by LLM is limited. These results verify that the superior performance of AffectGPT over the existing MLLMs does not come from LLM but from our proposed emotion description dataset and model.\n\nChoice of Audio and Video Encoders. In Figures  4(b ) and 4(c), the choice of audio and video encoder has a minimal impact on performance. This underscores that Af-fectGPT's exceptional performance is primarily driven by our proposed high-quality, large-scale dataset and effective framework, rather than the specific acoustic or visual encoders used. For audio encoders (Figure  4 (b)), ImageBind exhibits slightly inferior performance compared to other audio encoders. This may be attributed to the fact that other audio encoders are predominantly utilized in audio content understanding tasks (e.g., ASR), where audio content plays a critical role in emotion recognition. Similarly, for video encoders (Figure  4 (c)), CLIP VIT marginally outperforms EVA CLIP and DINOv2, aligning with findings from MERBench  (Lian et al., 2024c) , a unified benchmark for traditional categorical frameworks. These results suggest that insights derived from traditional categorical frameworks, such as encoder selection, may also be applicable to MLLM-based descriptive frameworks.\n\nRole of LoRA in LLMs. In Table  8 , we count the increase in trainable parameters when using LoRA for the LLM branch. The first row represents the model without the LoRA module. Experimental results show that fine-tuning the LLM with LoRA improves performance compared to models without LoRA. However, increasing the rank for LoRA-based models does not yield significant performance gains and instead increases computational costs.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Conclusion",
      "text": "This paper aims to enhance the emotional understanding of MLLMs from three aspects: (1) the dataset MER-Caption, which uses a model-led human-assisted strategy to create a large-scale dataset with guaranteed quality;\n\n(2) the model AffectGPT, which enhances multimodal fusion by moving cross-modal interactions outside of the LLM; and (3) the benchmark, which provides comprehensive evaluation metrics tailored to the free-form, natural language output style of MLLMs. Extensive experiments validate the effectiveness of our model and dataset. This work lays the foundation for building MLLMs with emotional understanding, contributing to the advancement of emotion AI.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Impact Statements",
      "text": "Social Impact. Emotion plays an important role in human communication, conveying human intentions and deep thoughts. As Minsky  (Minsky, 1988)  stated: The question is not whether intelligent machines can have any emotions, but whether machines can be intelligent without any emotions. The development of MER technology can enhance the human-computer interaction experience.\n\nEthics Statement. This paper does not involve the collection of new data. The original data comes from the unlabeled part of MER2024  (Lian et al., 2024b) , with permission from the dataset owners. The annotation process does not involve hiring external annotators, and no ethical issues are associated with this process. Additionally, we restrict the use of this dataset under the license of CC BY-NC 4.0, requiring researchers to use our dataset responsibly. Therefore, no ethical concerns are raised in this paper.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "A. Related Works",
      "text": "This paper focuses on constructing datasets and designing models to enhance the emotional understanding capability of MLLMs. In this section, we mainly review related work in these two aspects.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "A.1. Emotion Dataset",
      "text": "Emotion datasets are the foundation for building MER systems  (Wang et al., 2022; Chen et al., 2023) . Most research has focused on building categorical datasets, where basic emotions are first defined, and annotators are asked to select the most likely one  (Goodfellow et al., 2013)  or multiple  (Li et al., 2017)  labels from basic emotions. However, emotions are often diverse  (Demszky et al., 2020)  and can coexist  (Du et al., 2014) , making it challenging for categorical datasets to fully capture these complex emotions.\n\nTo address this, recent studies have shifted from categorical datasets to descriptive datasets, as emotion descriptions provide greater flexibility and enable the description of complex emotions in natural language. To construct such datasets,  Liu et al. (2022a)  used a human-based annotation strategy to capture the environment, body movements, facial expressions, and other emotion-related cues. However, the high annotation cost limits the scalability of these datasets. With the development of MLLMs,  Cheng et al. (2024)  used a more cost-effective automatic annotation method, where MLLMs are used to extract emotion-related descriptions from audio, facial expressions, and visual objects. However, they lacked pre-experimentation on MLLM selection, relying on empirical model choices, leading to insufficient label quality. In this paper, we propose a solution to balance label quality and dataset size. By leveraging high-quality human-based datasets to guide description generation and sample filtering, we achieve a quality-assured automatic annotation process and ultimately construct MER-Caption.",
      "page_start": 14,
      "page_end": 15
    },
    {
      "section_name": "A.2. Emotion Models",
      "text": "Emotion models are closely related to the training corpus. For categorical datasets, researchers often build classifiers to map multimodal human information to corresponding emotion labels. Apart from choosing the architecture (such as CNN, RNN, or Transformers), most research focuses on how to align and fuse multimodal information. For example,  Hazarika et al. (2020)  introduced a decomposition module to split features into modality-specific and modality-invariant representations.  Gu et al. (2018)  aligned different modalities at the word level and then learned time-dependent cross-modal interactions.  Tsai et al. (2019)  proposed using cross-attention to align features in the latent space. More recently,  Lian et al. (2024c)  conducted a fair comparison of various fusion and alignment strategies, showing that temporal-preserving features do not always outperform time-compressed features, suggesting that MER may be more suitable to solve from a global perspective.\n\nFor descriptive datasets, due to their natural language style output, the framework needs to shift from traditional discriminative methods to generative methods. With the development of LLMs and MLLMs, researchers have started to build models based on them. For example,  Huang et al. (2024)  used Vicuna as the language model, jointly training emotion labels and descriptions.  Xie et al. (2024)  used the instruction-aware Q-Former module to learn the mapping between input images and emotional descriptions.  Cheng et al. (2024)  integrated different encoders to understand multimodal inputs and used LLaMA-2 as an LLM decoder. However, current models either only focus on unimodal information  (Huang et al., 2024; Xie et al., 2024)  or leave all cross-modal interactions to the LLM  (Cheng et al., 2024) , which is insufficient for solving MER tasks with multimodal characteristics. To this end, we introduce the AffectGPT model in this paper.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "B. Implementation Details",
      "text": "Our choice of unimodal encoders is guided by previous research  (Lian et al., 2024c) , using CLIP ViT-L  (Radford et al., 2021)  as the visual encoder and HUBERT-L  (Hsu et al., 2021)  as the acoustic encoder. Given the remarkable performance of Qwen-2.5  (Yang et al., 2024) , we choose it as the LLM. To ensure training efficiency, we only fine-tune an extra LoRA module (in the LLM), projector, and pre-fusion branch, while freezing the weights of the LLM and unimodal encoders (see Figure  3 ). We default to setting the rank in the LoRA module to 16. This approach reduces GPU memory usage and speeds up training. Additionally, through preliminary experiments, we found that pre-training on other instruction datasets followed by a second-stage training on MER-Caption did not lead to performance improvements. The primary reason is the large scale of our dataset and the limited focus on MER in current instruction datasets.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "G. Prompt For Clue Merge",
      "text": "To merge multimodal clues, we use GPT-3.5 and apply the following prompt:\n\nPlease act as an expert in the field of emotions. We provide acoustic and visual clues that may be related to the character's emotional state, along with the original subtitle of the video. Please analyze which parts can infer the emotional state and explain the reasons. During the analysis, please integrate the textual, audio, and visual clues.\n\nEven when modality conflicts exist (i.e., the emotions conveyed by audio, video, and text are not the same, as shown in Figure  6 ), GPT-3.5 can provide reasonable responses, primarily due to its powerful reasoning ability.\n\nAcoustic Clue: In the audio, the character speaks with a stutter, which usually expresses feelings of nervousness, anxiety, or unease.\n\nText: Why are you all looking at me like that? So, as long as it's a woman, does she have to have a relationship with me?\n\nVisual Clue: In the video, the screen shows a male character in an indoor setting. At the beginning of the video, his eyes are wide open and his mouth is also open, indicating a surprised facial expression. In the following scenes, he looks around, seemingly explaining or narrating something to the people around him. Overall, his emotions are not positive or optimistic.\n\nMerged Clue: The subtitle expresses the male character's dissatisfaction and anger towards the people around him. Based on the surprised and negative facial expression of the male character in the video clues, as well as the stuttering speech in the audio clues, we can infer that the male character is expressing a feeling of dissatisfaction and anger in this sentence. He may feel troubled by the prejudice of the people around him and is unhappy with this unfair treatment.",
      "page_start": 18,
      "page_end": 19
    },
    {
      "section_name": "H. Dataset Comparison",
      "text": "",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "I. Video Duration Distribution",
      "text": "Figure  8  presents the video duration distribution of the MER-Caption dataset. We observe that the majority of samples have durations ranging from 2 to 5 seconds.",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "J. Mer-Unibench Details",
      "text": "MER-UniBench is a comprehensive evaluation benchmark covering three typical tasks in MER, including fine-grained emotion recognition, basic emotion recognition, and sentiment analysis. Different tasks involve different datasets, and we provide their statistical information in Table  11 . In this paper, we intentionally focus on single-person videos, as this allows us to eliminate interference from other speakers and reduce task difficulty. Multi-person MER belongs to another research topic and will be addressed in our future work. OV-MERD+ is our newly collected dataset, an extended version of the previous OV-MERD  (Lian et al., 2024a) . Unlike traditional datasets, which select a single label from basic emotions, OV-MERD is a fine-grained emotion dataset that allows each sample to have a variable number of emotions, using any emotion not restricted to predefined taxonomies. OV-MERD initially contains 332 samples, and we further expand its dataset size, obtaining OV-MERD+.\n\nMER2023  (Lian et al., 2023)  and MER2024  (Lian et al., 2024b)  are widely used in Chinese MER research, with MER2024 being an extended version of MER2023. The original data in both datasets comes from movies and TV shows. They use various techniques to segment video clips, ensuring that each clip has only one person, with their speech content being relatively complete. To ensure annotation quality, they hire multiple annotators, each selecting the most likely label from six candidate emotions: worry, happy, neutral, angry, surprised, and sad. The final label is determined through majority voting.\n\nIEMOCAP  (Busso et al., 2008)  is one of the most widely used emotion datasets. It contains five sessions, each with a male and a female actor in a laboratory environment. The dataset includes the following emotion labels: anger, happiness, sadness, neutral, excitement, frustration, fear, surprise, and others. Following previous research  (Poria et al., 2017) , we choose the last session for testing, and use the first four emotions, and merge surprise and happiness into happiness.\n\nMELD  (Poria et al., 2019)  is an extension of the text-centered EmotionLines dataset  (Hsu et al., 2018) , adding audio and video content. The raw data is derived from the Friends TV series. The dataset has seven emotion labels, and each sample is assigned to one of the most likely labels: anger, joy, sadness, neutral, disgust, fear, and surprise.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "K. Emotion Wheel",
      "text": "Since there is no universal definition of the emotion wheel, we follow previous work  (Lian et al., 2024a)  and use five emotion wheels in this paper.",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "L. Main Results",
      "text": "Table  12  reports the complete results, with several metrics for each dataset, and the primary metrics are highlighted in gray. In the last column, we report the average value of the primary metrics. These results verify the effectiveness of our AffectGPT in multimodal emotion understanding.",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "M. Ablation Study On Mer-Caption",
      "text": "",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "N. Impact Of Sampling Frames In Video Branch",
      "text": "This paper defaults to sampling 8 frames per video. But if we change the number of sampled frames, will it significantly impact performance? To answer this, we conducted additional experiments in this section. Specifically, we compare two types of inputs: (1) face-only and (2) face-text combinations, and evaluate model performance across different sampling frame counts, ranging from 2 to 64. In Figure  10 , we observe that using too few frames (e.g., fewer than 2) results in a noticeable decline in performance, indicating that insufficient frames lead to information loss. However, further increasing the number of sampling frames (e.g., more than 16) does not yield significant performance improvements. This can be attributed to the fact that MER tasks typically use short-duration videos with relatively stable facial expressions.",
      "page_start": 22,
      "page_end": 22
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , where the diverse and coexisting",
      "page": 1
    },
    {
      "caption": "Figure 1: Emotion complexity analysis. Human emotions are often diverse and coexist simultaneously. Such complex",
      "page": 2
    },
    {
      "caption": "Figure 2: Dataset construction pipeline. To create a large-scale dataset with guaranteed label quality, we propose a",
      "page": 4
    },
    {
      "caption": "Figure 2: ). Preliminary experiments reveal that descriptions",
      "page": 4
    },
    {
      "caption": "Figure 3: Model comparison. ALLM and VLLM primarily use modality-specific encoders and align them with the LLM",
      "page": 5
    },
    {
      "caption": "Figure 3: illustrates the current mainstream architecture.",
      "page": 5
    },
    {
      "caption": "Figure 3: ), which is insufficient",
      "page": 5
    },
    {
      "caption": "Figure 4: (a), we further explore the impact of",
      "page": 9
    },
    {
      "caption": "Figure 4: (b)), ImageBind",
      "page": 9
    },
    {
      "caption": "Figure 4: Ablation studies on LLMs, audio encoders, and",
      "page": 9
    },
    {
      "caption": "Figure 4: (c)), CLIP VIT marginally out-",
      "page": 9
    },
    {
      "caption": "Figure 3: ). We default to setting the rank in the LoRA module to 16. This approach reduces GPU memory usage and speeds",
      "page": 14
    },
    {
      "caption": "Figure 5: provides an example to visualize the outputs of different MLLMs. These outputs contain varying numbers of",
      "page": 16
    },
    {
      "caption": "Figure 5: Visualization of MLLM outputs.",
      "page": 16
    },
    {
      "caption": "Figure 6: ), GPT-3.5 can provide reasonable responses, primarily due to its powerful reasoning ability.",
      "page": 18
    },
    {
      "caption": "Figure 6: Example of modality conflict.",
      "page": 18
    },
    {
      "caption": "Figure 7: compares the distribution of description lengths and the number of emotions per sample. We observe that our",
      "page": 18
    },
    {
      "caption": "Figure 7: Dataset comparison. The first row compares the lengths of the descriptions, while the second row compares the",
      "page": 18
    },
    {
      "caption": "Figure 8: presents the video duration distribution of the MER-Caption dataset. We observe that the majority of samples have",
      "page": 18
    },
    {
      "caption": "Figure 8: Video duration distribution.",
      "page": 19
    },
    {
      "caption": "Figure 9: Emotion wheel. We use five emotion wheels, all of which are derived from previous research (Lian et al., 2024a).",
      "page": 20
    },
    {
      "caption": "Figure 10: , we observe that using too few frames (e.g., fewer than 2) results in a",
      "page": 22
    },
    {
      "caption": "Figure 10: Impact of sampling frames.",
      "page": 22
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Modality\nA\nV\nT\n√\n√": "×\nOneLLM\n√\n√\n×\nSECap\n√\n√\n×\nPandaGPT\n√\n√\n×\nQwen-Audio\n√\n√\n×\nSALMONN\n√\n√\n×\nAffectGPT\n√\n√",
          "Basic\nMER2023\nMER2024\nMELD\nIEMOCAP": "25.52\n17.21\n28.32\n33.44\n40.95\n52.46\n25.56\n36.92\n33.57\n39.04\n31.91\n36.55\n41.85\n31.61\n49.09\n35.47\n55.53\n45.38\n45.62\n46.84\n72.94\n73.41\n56.63\n55.68",
          "Sentiment\nMOSI\nMOSEI\nSIMS\nSIMS v2": "64.01\n54.09\n63.39\n61.98\n55.76\n54.18\n59.51\n57.41\n66.06\n61.33\n62.93\n58.88\n70.09\n46.90\n70.73\n65.26\n81.00\n67.03\n68.69\n65.93\n83.46\n80.74\n82.99\n83.75",
          "Fine-grained\nOV-MERD+": "22.25\n36.97\n31.33\n32.36\n45.00\n59.98",
          "Mean": "41.14\n46.64\n46.84\n49.26\n57.89\n72.18"
        },
        {
          "Modality\nA\nV\nT\n√\n√": "×\nOtter\n√\n√\n×\nVideo-LLaVA\n√\n√\n×\nPandaGPT\n√\n√\n×\nVideo-ChatGPT\n√\n√\n×\nVideoChat2\n√\n√\n×\nLLaMA-VID\n√\n√\n×\nVideoChat\n√\n√\n×\nChat-UniVi\n√\n√\n×\nmPLUG-Owl\n√\n√\n×\nAffectGPT\n√\n√\n√",
          "Basic\nMER2023\nMER2024\nMELD\nIEMOCAP": "16.41\n14.65\n22.57\n29.08\n36.93\n30.25\n30.73\n38.95\n39.13\n47.16\n38.33\n47.21\n44.86\n46.80\n37.33\n56.83\n33.67\n54.50\n36.64\n48.70\n50.72\n57.60\n42.75\n46.02\n48.73\n57.30\n41.11\n48.38\n57.62\n65.67\n45.61\n52.37\n56.86\n59.89\n49.11\n55.54\n74.58\n75.29\n57.63\n62.19",
          "Sentiment\nMOSI\nMOSEI\nSIMS\nSIMS v2": "52.89\n50.44\n57.56\n53.12\n56.37\n61.64\n53.28\n57.45\n58.50\n64.25\n62.07\n65.25\n54.42\n63.12\n64.82\n65.80\n66.84\n54.32\n69.49\n70.66\n61.78\n63.89\n69.35\n67.48\n65.13\n63.61\n69.52\n72.14\n54.53\n63.18\n68.15\n66.36\n72.40\n72.91\n72.13\n75.00\n82.39\n81.57\n87.20\n86.29",
          "Fine-grained\nOV-MERD+": "16.63\n34.00\n35.07\n39.80\n39.21\n45.01\n44.52\n48.00\n48.18\n61.65",
          "Mean": "34.82\n44.40\n50.77\n52.64\n52.67\n56.07\n56.71\n57.94\n62.45\n74.31"
        },
        {
          "Modality\nA\nV\nT\n√\n√": "PandaGPT\n√\n√\n√\nEmotion-LLaMA\n√\n√\n√\nAffectGPT",
          "Basic\nMER2023\nMER2024\nMELD\nIEMOCAP": "40.21\n51.89\n37.88\n44.04\n59.38\n73.62\n46.76\n55.47\n78.54\n78.80\n55.65\n60.54",
          "Sentiment\nMOSI\nMOSEI\nSIMS\nSIMS v2": "61.92\n67.61\n68.38\n67.23\n66.13\n67.66\n78.32\n77.23\n81.30\n80.90\n88.49\n86.18",
          "Fine-grained\nOV-MERD+": "37.12\n52.97\n62.52",
          "Mean": "52.92\n64.17\n74.77"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 3: show that",
      "data": [
        {
          "Low\nHigh": "×\n×\n√\n×\n√\nE\n√\nS\n√\nE+S",
          "Fine-grained\nBasic\nSentiment": "58.42\n64.36\n76.09\n58.61\n62.78\n79.04\n61.72\n67.83\n82.74\n85.04\n61.00\n66.33\n62.52\n68.38\n84.22",
          "MER-UniBench": "68.91\n69.54\n73.78\n74.05\n74.77"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 3: show that",
      "data": [
        {
          "Dataset\nFiltering": "×\n√\nMiniGPT4",
          "MER-UniBench": "31.74\n35.53"
        },
        {
          "Dataset\nFiltering": "×\n√\nVideoChat",
          "MER-UniBench": "37.16\n37.63"
        },
        {
          "Dataset\nFiltering": "×\n√\nLLaVA",
          "MER-UniBench": "46.69\n46.27"
        },
        {
          "Dataset\nFiltering": "×\n√\nWavCaps",
          "MER-UniBench": "21.65\n37.91"
        },
        {
          "Dataset\nFiltering": "EmoVIT\n–\nMAFW\n–\nMERR-Coarse\n–\nMERR-Fine\n–\nMER-Caption\n–\nMER-Caption+\n–",
          "MER-UniBench": "51.05\n58.16\n49.85\n64.55\n68.91\n74.77"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 7: , our dataset provides more accurate",
      "data": [
        {
          "Input\naudio\nface\nframe\ntext\n√": "×\n×\n×\n√\n×\n×\n×\n√\n×\n×\n×\n√\n×\n×\n×\n√\n√\n√",
          "MER-UniBench": "60.08\n60.47\n59.47\n67.44"
        },
        {
          "Input\naudio\nface\nframe\ntext\n√": "×\n√\n√\n√\n×\n√\n√\n√\n√",
          "MER-UniBench": "74.77\n73.39\n74.60"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Supported Modality": "Video, Text\nVideo, Text\nVideo, Text\nVideo, Text\nVideo, Text\nVideo, Text\nVideo, Text\nVideo, Text\nVideo, Text\nAudio, Text\nAudio, Text\nAudio, Text\nAudio, Video, Text\nAudio, Video, Text\nAudio, Video, Text",
          "Link": "https://github.com/Luodian/Otter\nhttps://github.com/OpenGVLab/Ask-Anything/tree/main/video chat\nhttps://github.com/OpenGVLab/Ask-Anything/tree/main/video chat2\nhttps://github.com/PKU-YuanGroup/Video-LLaVA\nhttps://github.com/DAMO-NLP-SG/Video-LLaMA\nhttps://github.com/mbzuai-oryx/Video-ChatGPT\nhttps://github.com/dvlab-research/LLaMA-VID\nhttps://github.com/X-PLUG/mPLUG-Owl\nhttps://github.com/PKU-YuanGroup/Chat-UniVi\nhttps://github.com/bytedance/SALMONN\nhttps://github.com/QwenLM/Qwen-Audio\nhttps://github.com/thuhcsi/SECap\nhttps://github.com/csuhan/OneLLM\nhttps://github.com/yxuansu/PandaGPT\nhttps://github.com/ZebangCheng/Emotion-LLaMA"
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset\nChosen Set\n# Samples\nLabel Description\nData Source": "unfixed categories and\nOV-MERD+\nAll\n532\nmovies, TV series\ndiverse number of labels per sample"
        },
        {
          "Dataset\nChosen Set\n# Samples\nLabel Description\nData Source": "MER2023\nMER-MULTI\n411\nmost likely label among six candidates\nmovies, TV series\nMER2024\nMER-SEMI\n1,169\nmost likely label among six candidates\nmovies, TV series\nIEMOCAP\nSession5\n1,241\nmost likely label among four candidates\nactor’s performance\nMELD\nTest\n2,610\nmost likely label among seven candidates\n”Friends” TV series"
        },
        {
          "Dataset\nChosen Set\n# Samples\nLabel Description\nData Source": "CMU-MOSI\nTest\n686\nsentiment intensity, ranging from [-3, 3]\nopinion videos in YouTube\nCMU-MOSEI\nTest\n4,659\nsentiment intensity, ranging from [-3, 3]\nopinion videos in YouTube\nCH-SIMS\nTest\n457\nsentiment intensity, ranging from [-1, 1]\nmovies, TV series, and shows\nCH-SIMS v2\nTest\n1,034\nsentiment intensity, ranging from [-1, 1]\nmovies, TV series, and shows"
        }
      ],
      "page": 19
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "A\nV\nT\n√\n√": "×\nOtter\n√\n√\n×\nOneLLM\n√\n√\n×\nVideo-LLaVA\n√\n√\n×\nSECap\n√\n√\n×\nPandaGPT\n√\n√\n×\nQwen-Audio\n√\n√\n×\nPandaGPT\n√\n√\n×\nVideo-ChatGPT\n√\n√\n×\nVideoChat2\n√\n√\n√\nPandaGPT\n√\n√\n×\nLLaMA-VID\n√\n√\n×\nVideoChat\n√\n√\n×\nSALMONN\n√\n√\n×\nChat-UniVi\n√\n√\n×\nmPLUG-Owl\n√\n√\n√\nEmotion-LLaMA\n√\n√\n√\nAffectGPT",
          "MER2023\nHIT(↑)": "16.41\n25.52\n36.93\n40.95\n33.57\n41.85\n39.13\n44.86\n33.67\n40.21\n50.72\n48.73\n55.53\n57.62\n56.86\n59.38\n78.54",
          "MER2024\nHIT(↑)": "14.65\n17.21\n30.25\n52.46\n39.04\n31.61\n47.16\n46.80\n54.50\n51.89\n57.60\n57.30\n45.38\n65.67\n59.89\n73.62\n78.80",
          "MELD\nHIT(↑)": "22.57\n28.32\n30.73\n25.56\n31.91\n49.09\n38.33\n37.33\n36.64\n37.88\n42.75\n41.11\n45.62\n45.61\n49.11\n46.76\n55.65",
          "IEMOCAP\nHIT(↑)": "29.08\n33.44\n38.95\n36.92\n36.55\n35.47\n47.21\n56.83\n48.70\n44.04\n46.02\n48.38\n46.84\n52.37\n55.54\n55.47\n60.54",
          "CMU-MOSI\nWAF(↑)\nACC(↑)": "52.89\n54.27\n64.01\n64.48\n56.37\n57.62\n55.76\n56.71\n66.06\n65.85\n70.09\n71.49\n58.50\n60.21\n54.42\n57.77\n66.84\n67.23\n61.92\n62.80\n61.78\n62.65\n65.13\n65.09\n81.00\n81.25\n54.53\n57.62\n72.40\n72.26\n66.13\n66.31\n81.30\n81.25",
          "CMU-MOSEI\nWAF(↑)\nACC(↑)": "50.44\n50.77\n54.09\n54.18\n61.64\n64.20\n54.18\n53.85\n61.33\n60.73\n46.90\n51.16\n64.25\n65.55\n63.12\n65.66\n54.32\n54.82\n68.82\n67.61\n63.89\n66.21\n63.61\n63.02\n67.03\n66.90\n63.18\n67.47\n72.91\n73.17\n67.66\n67.25\n80.90\n80.68"
        }
      ],
      "page": 21
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "A\nV\nT\n√\n√": "×\nOtter\n√\n√\n×\nOneLLM\n√\n√\n×\nVideo-LLaVA\n√\n√\n×\nSECap\n√\n√\n×\nPandaGPT\n√\n√\n×\nQwen-Audio\n√\n√\n×\nPandaGPT\n√\n√\n×\nVideo-ChatGPT\n√\n√\n×\nVideoChat2\n√\n√\n√\nPandaGPT\n√\n√\n×\nLLaMA-VID\n√\n√\n×\nVideoChat\n√\n√\n×\nSALMONN\n√\n√\n×\nChat-UniVi\n√\n√\n×\nmPLUG-Owl\n√\n√\n√\nEmotion-LLaMA\n√\n√\n√\nAffectGPT",
          "CH-SIMS\nWAF(↑)\nACC(↑)": "57.56\n60.57\n63.39\n63.92\n53.28\n54.64\n59.51\n62.89\n62.93\n62.37\n73.45\n70.73\n62.07\n61.60\n64.82\n64.43\n69.49\n69.59\n68.38\n67.78\n69.35\n68.81\n69.52\n69.33\n68.69\n69.85\n68.15\n67.78\n72.13\n71.65\n78.32\n78.61\n88.49\n88.40",
          "CH-SIMS v2\nWAF(↑)\nACC(↑)": "53.12\n56.20\n61.98\n62.46\n57.45\n59.28\n57.41\n60.92\n58.88\n58.84\n65.26\n68.17\n65.25\n65.31\n65.80\n66.85\n70.66\n71.13\n67.23\n67.40\n67.48\n67.73\n72.14\n72.12\n65.93\n67.07\n66.36\n67.18\n75.00\n74.97\n77.23\n77.39\n86.18\n86.17",
          "OV-MERD+\nFs(↑)\nPrecisions(↑)\nRecalls(↑)": "16.63\n17.67\n15.74\n22.25\n24.49\n20.41\n34.00\n36.48\n31.86\n36.97\n43.51\n32.17\n31.33\n33.08\n29.77\n32.36\n38.52\n27.91\n35.07\n37.86\n32.67\n39.80\n43.12\n36.97\n39.21\n42.85\n36.16\n37.12\n39.64\n34.91\n45.01\n46.83\n43.32\n44.52\n44.55\n44.49\n45.00\n43.57\n46.61\n48.20\n48.00\n47.81\n48.18\n48.47\n47.91\n52.97\n54.85\n51.22\n62.52\n62.21\n63.00",
          "Mean": "34.82\n41.14\n44.40\n46.64\n46.84\n49.26\n50.77\n52.64\n52.67\n52.92\n56.07\n56.71\n57.89\n57.94\n62.45\n64.17\n74.77"
        }
      ],
      "page": 21
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset": "×\n√\nMiniGPT4\n×\n√\nVideoChat\nGeneral\n×\nInstruction\n√\nLLaVA\n×\n√\nWavCaps",
          "CMU-\nCMU-\nCH-\nCH-\nOV-\nFiltering MER2023 MER2024 MELD IEMOCAP\nMOSI\nMOSEI\nSIMS\nSIMS v2\nMERD+": "11.56\n12.91\n18.89\n16.06\n53.57\n45.98\n57.66\n55.16\n13.86\n17.57\n16.65\n22.60\n30.18\n52.58\n56.50\n52.36\n51.19\n20.16\n24.87\n22.42\n21.56\n32.91\n50.13\n56.17\n50.07\n51.71\n24.56\n27.70\n24.73\n27.66\n39.46\n45.45\n56.86\n43.68\n47.05\n26.09\n42.21\n41.54\n32.97\n49.96\n54.48\n56.42\n52.04\n54.80\n35.75\n41.56\n42.30\n32.61\n46.21\n52.82\n57.72\n52.78\n53.44\n36.96\n5.75\n7.71\n4.35\n4.99\n45.59\n22.76\n53.04\n45.68\n4.95\n23.72\n26.97\n23.39\n27.30\n54.67\n49.54\n58.12\n55.93\n21.57",
          "Mean": "31.74\n35.53\n37.16\n37.63\n46.69\n46.27\n21.65\n37.91"
        },
        {
          "Dataset": "EmoVIT\n–\nMAFW\n–\nEmotion\nMERR-Coarse\n–\nDescription\nMERR-Fine\n–\nMER-Caption\n–\nMER-Caption+\n–",
          "CMU-\nCMU-\nCH-\nCH-\nOV-\nFiltering MER2023 MER2024 MELD IEMOCAP\nMOSI\nMOSEI\nSIMS\nSIMS v2\nMERD+": "39.31\n50.24\n32.36\n48.24\n53.40\n61.53\n69.72\n66.53\n38.09\n52.67\n55.99\n40.85\n57.60\n66.11\n62.27\n75.20\n70.02\n42.75\n35.34\n36.60\n29.37\n36.94\n65.10\n63.27\n75.12\n73.76\n33.18\n69.00\n72.84\n47.38\n54.49\n66.21\n60.03\n79.90\n78.54\n52.56\n72.12\n74.21\n54.69\n56.41\n75.10\n70.97\n80.21\n78.06\n58.42\n78.54\n78.80\n55.65\n60.54\n81.30\n80.90\n88.49\n86.18\n62.52",
          "Mean": "51.05\n58.16\n49.85\n64.55\n68.91\n74.77"
        }
      ],
      "page": 21
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A database of german emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "In Interspeech"
    },
    {
      "citation_id": "2",
      "title": "Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan",
        "Iemocap"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "3",
      "title": "Smg: A micro-gesture dataset towards spontaneous body gestures for emotional stress state analysis",
      "authors": [
        "H Chen",
        "H Shi",
        "X Liu",
        "X Li",
        "G Zhao"
      ],
      "year": "2023",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "4",
      "title": "Emotion-llama: Multimodal emotion recognition and reasoning with instruction tuning",
      "authors": [
        "Z Cheng",
        "Z.-Q Cheng",
        "J.-Y He",
        "K Wang",
        "Y Lin",
        "Z Lian",
        "X Peng",
        "A Hauptmann"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "5",
      "title": "Qwen-audio: Advancing universal audio understanding via unified large-scale audiolanguage models",
      "authors": [
        "Y Chu",
        "J Xu",
        "X Zhou",
        "Q Yang",
        "S Zhang",
        "Z Yan",
        "C Zhou",
        "J Zhou"
      ],
      "year": "2023",
      "venue": "Qwen-audio: Advancing universal audio understanding via unified large-scale audiolanguage models",
      "arxiv": "arXiv:2311.07919"
    },
    {
      "citation_id": "6",
      "title": "Self-report captures 27 distinct categories of emotion bridged by continuous gradients",
      "authors": [
        "A Cowen",
        "D Keltner"
      ],
      "year": "2017",
      "venue": "Proceedings of the national academy of sciences"
    },
    {
      "citation_id": "7",
      "title": "A dataset of fine-grained emotions",
      "authors": [
        "D Demszky",
        "D Movshovitz-Attias",
        "J Ko",
        "A Cowen",
        "G Nemade",
        "S Ravi",
        "Goemotions"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "8",
      "title": "Compound facial expressions of emotion",
      "authors": [
        "S Du",
        "Y Tao",
        "A Martinez"
      ],
      "year": "2014",
      "venue": "Proceedings of the national academy of sciences"
    },
    {
      "citation_id": "9",
      "title": "Universal facial expressions of emotion",
      "authors": [
        "P Ekman",
        "D Keltner"
      ],
      "year": "1970",
      "venue": "California mental health research digest"
    },
    {
      "citation_id": "10",
      "title": "Challenges in representation learning: A report on three machine learning contests",
      "authors": [
        "I Goodfellow",
        "D Erhan",
        "P Carrier",
        "A Courville",
        "M Mirza",
        "B Hamner",
        "W Cukierski",
        "Y Tang",
        "D Thaler",
        "D.-H Lee"
      ],
      "year": "2013",
      "venue": "Neural information processing: 20th international conference"
    },
    {
      "citation_id": "11",
      "title": "Multimodal affective analysis using hierarchical attention strategy with word-level alignment",
      "authors": [
        "Y Gu",
        "K Yang",
        "S Fu",
        "S Chen",
        "X Li",
        "I Marsic"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "12",
      "title": "Onellm: One framework to align all modalities with language",
      "authors": [
        "J Han",
        "K Gong",
        "Y Zhang",
        "J Wang",
        "K Zhang",
        "D Lin",
        "Y Qiao",
        "P Gao",
        "X Yue"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "13",
      "title": "Modality-invariant and-specific representations for multimodal sentiment analysis",
      "authors": [
        "D Hazarika",
        "R Zimmermann",
        "S Poria",
        "Misa"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "14",
      "title": "An emotion corpus of multiparty conversations",
      "authors": [
        "C.-C Hsu",
        "S.-Y Chen",
        "C.-C Kuo",
        "K Huang",
        "T.-H Ku",
        "L.-W Emotionlines"
      ],
      "year": "2018",
      "venue": "Proceedings of the Eleventh International Conference on Language Resources and Evaluation"
    },
    {
      "citation_id": "15",
      "title": "Selfsupervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Hubert"
      ],
      "year": "2021",
      "venue": "IEEE/ACM transactions on audio, speech, and language processing"
    },
    {
      "citation_id": "16",
      "title": "Ecr-chain: Advancing generative language models to better emotion-cause reasoners through reasoning chains",
      "authors": [
        "Z Huang",
        "J Zhao"
      ],
      "year": "2024",
      "venue": "Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI"
    },
    {
      "citation_id": "17",
      "title": "Stability of emotion experiences and their relations to traits of personality",
      "authors": [
        "C Izard",
        "D Libero",
        "P Putnam",
        "O Haynes"
      ],
      "year": "1993",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "18",
      "title": "Dfew: A large-scale database for recognizing dynamic facial expressions in the wild",
      "authors": [
        "X Jiang",
        "Y Zong",
        "W Zheng",
        "C Tang",
        "W Xia",
        "C Lu",
        "J Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM international conference on multimedia"
    },
    {
      "citation_id": "19",
      "title": "Chat-univi: Unified visual representation empowers large language models with image and video understanding",
      "authors": [
        "P Jin",
        "R Takanobu",
        "W Zhang",
        "X Cao",
        "L Yuan"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "20",
      "title": "Metaphor and emotion: Language, culture, and body in human feeling",
      "authors": [
        "Z Kövecses"
      ],
      "year": "2003",
      "venue": "Metaphor and emotion: Language, culture, and body in human feeling"
    },
    {
      "citation_id": "21",
      "title": "A multi-modal model with in-context instruction tuning",
      "authors": [
        "B Li",
        "Y Zhang",
        "L Chen",
        "J Wang",
        "J Yang",
        "Z Liu",
        "Otter"
      ],
      "year": "2023",
      "venue": "A multi-modal model with in-context instruction tuning",
      "arxiv": "arXiv:2305.03726"
    },
    {
      "citation_id": "22",
      "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
      "authors": [
        "J Li",
        "D Li",
        "S Savarese",
        "S Hoi"
      ],
      "year": "2023",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "23",
      "title": "Chat-centric video understanding",
      "authors": [
        "K Li",
        "Y He",
        "Y Wang",
        "Y Li",
        "W Wang",
        "P Luo",
        "Y Wang",
        "L Wang",
        "Y Qiao",
        "Videochat"
      ],
      "year": "2023",
      "venue": "Chat-centric video understanding",
      "arxiv": "arXiv:2305.06355"
    },
    {
      "citation_id": "24",
      "title": "Mvbench: A comprehensive multi-modal video understanding benchmark",
      "authors": [
        "K Li",
        "Y Wang",
        "Y He",
        "Y Li",
        "Y Wang",
        "Y Liu",
        "Z Wang",
        "J Xu",
        "G Chen",
        "P Luo",
        "L Wang",
        "Y Qiao"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "25",
      "title": "Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild",
      "authors": [
        "S Li",
        "W Deng",
        "J Du"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "26",
      "title": "Llama-vid: An image is worth 2 tokens in large language models",
      "authors": [
        "Y Li",
        "C Wang",
        "J Jia"
      ],
      "year": "2024",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "27",
      "title": "Multilabel learning, modality robustness, and semi-supervised learning",
      "authors": [
        "Z Lian",
        "H Sun",
        "L Sun",
        "K Chen",
        "M Xu",
        "K Wang",
        "K Xu",
        "Y He",
        "Y Li",
        "J Zhao"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "28",
      "title": "Open-vocabulary multimodal emotion recognition: Dataset, metric, and benchmark",
      "authors": [
        "Z Lian",
        "H Sun",
        "L Sun",
        "L Chen",
        "H Chen",
        "H Gu",
        "Z Wen",
        "S Chen",
        "S Zhang",
        "H Yao"
      ],
      "year": "2024",
      "venue": "Open-vocabulary multimodal emotion recognition: Dataset, metric, and benchmark",
      "arxiv": "arXiv:2410.01495"
    },
    {
      "citation_id": "29",
      "title": "Mer 2024: Semi-supervised learning, noise robustness, and openvocabulary multimodal emotion recognition",
      "authors": [
        "Z Lian",
        "H Sun",
        "L Sun",
        "Z Wen",
        "S Zhang",
        "S Chen",
        "H Gu",
        "J Zhao",
        "Z Ma",
        "X Chen"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2nd International Workshop on Multimodal and Responsible Affective Computing"
    },
    {
      "citation_id": "30",
      "title": "A unified evaluation benchmark for multimodal emotion recognition",
      "authors": [
        "Z Lian",
        "L Sun",
        "Y Ren",
        "H Gu",
        "H Sun",
        "L Chen",
        "B Liu",
        "J Tao",
        "Merbench"
      ],
      "year": "2024",
      "venue": "A unified evaluation benchmark for multimodal emotion recognition",
      "arxiv": "arXiv:2401.03429"
    },
    {
      "citation_id": "31",
      "title": "Gpt-4v with emotion: A zero-shot benchmark for generalized emotion recognition",
      "authors": [
        "Z Lian",
        "L Sun",
        "H Sun",
        "K Chen",
        "Z Wen",
        "H Gu",
        "B Liu",
        "J Tao"
      ],
      "year": "2024",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "32",
      "title": "A survey of multimodel large language models",
      "authors": [
        "Z Liang",
        "Y Xu",
        "Y Hong",
        "P Shang",
        "Q Wang",
        "Q Fu",
        "K Liu"
      ],
      "year": "2024",
      "venue": "Proceedings of the 3rd International Conference on Computer"
    },
    {
      "citation_id": "33",
      "title": "Video-llava: Learning united visual representation by alignment before projection",
      "authors": [
        "B Lin",
        "Y Ye",
        "B Zhu",
        "J Cui",
        "M Ning",
        "P Jin",
        "L Yuan"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "34",
      "title": "Towards emotional support dialog systems",
      "authors": [
        "S Liu",
        "C Zheng",
        "O Demasi",
        "S Sabour",
        "Y Li",
        "Z Yu",
        "Y Jiang",
        "M Huang"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "35",
      "title": "A large-scale, multi-modal, compound affective database for dynamic facial expression recognition in the wild",
      "authors": [
        "Y Liu",
        "W Dai",
        "C Feng",
        "W Wang",
        "G Yin",
        "J Zeng",
        "S Shan",
        "Mafw"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "36",
      "title": "Make acoustic and visual cues matter: Ch-sims v2. 0 dataset and av-mixup consistent module",
      "authors": [
        "Y Liu",
        "Z Yuan",
        "H Mao",
        "Z Liang",
        "W Yang",
        "Y Qiu",
        "T Cheng",
        "X Li",
        "H Xu",
        "K Gao"
      ],
      "year": "2022",
      "venue": "Proceedings of the International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "37",
      "title": "Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "38",
      "title": "Videochatgpt: Towards detailed video understanding via large vision and language models",
      "authors": [
        "M Maaz",
        "H Rasheed",
        "S Khan",
        "F Khan"
      ],
      "year": "2024",
      "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "39",
      "title": "Culture and emotion. The handbook of culture and psychology",
      "authors": [
        "D Matsumoto"
      ],
      "year": "2001",
      "venue": "Culture and emotion. The handbook of culture and psychology"
    },
    {
      "citation_id": "40",
      "title": "Society of mind",
      "authors": [
        "M Minsky"
      ],
      "year": "1988",
      "venue": "Society of mind"
    },
    {
      "citation_id": "41",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "42",
      "title": "",
      "authors": [
        "Openai",
        "Chatgpt"
      ],
      "year": "2022",
      "venue": ""
    },
    {
      "citation_id": "43",
      "title": "A general psychoevolutionary theory of emotion",
      "authors": [
        "R Plutchik"
      ],
      "year": "1980",
      "venue": "Emotion: Theory, research, and experience"
    },
    {
      "citation_id": "44",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "S Poria",
        "E Cambria",
        "D Hazarika",
        "N Majumder",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "45",
      "title": "A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea",
        "Meld"
      ],
      "year": "2019",
      "venue": "A multimodal multi-party dataset for emotion recognition in conversations"
    },
    {
      "citation_id": "46",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "C Hallacy",
        "A Ramesh",
        "G Goh",
        "S Agarwal",
        "G Sastry",
        "A Askell",
        "P Mishkin",
        "J Clark"
      ],
      "year": "2021",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "47",
      "title": "Emotion in education",
      "authors": [
        "P Schutz"
      ],
      "year": "2007",
      "venue": "Emotion in education"
    },
    {
      "citation_id": "48",
      "title": "Emotion recognition for human-robot interaction: Recent advances and future perspectives",
      "authors": [
        "M Spezialetti",
        "G Placidi",
        "S Rossi"
      ],
      "year": "2020",
      "venue": "Frontiers in Robotics and AI"
    },
    {
      "citation_id": "49",
      "title": "One model to instruction-follow them all",
      "authors": [
        "Y Su",
        "T Lan",
        "H Li",
        "J Xu",
        "Y Wang",
        "D Cai",
        "Pandagpt"
      ],
      "year": "2023",
      "venue": "Proceedings of the 1st Workshop on Taming Large Language Models: Controllability in the era of Interactive Assistants"
    },
    {
      "citation_id": "50",
      "title": "Towards generic hearing abilities for large language models",
      "authors": [
        "C Tang",
        "W Yu",
        "G Sun",
        "X Chen",
        "T Tan",
        "W Li",
        "L Lu",
        "Z Ma",
        "C Zhang",
        "Salmonn"
      ],
      "year": "2023",
      "venue": "The Twelfth International Conference on Learning Representations"
    },
    {
      "citation_id": "51",
      "title": "Is someone speaking? exploring long-term temporal features for audio-visual active speaker detection",
      "authors": [
        "R Tao",
        "Z Pan",
        "R Das",
        "X Qian",
        "M Shou",
        "H Li"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th ACM international conference on multimedia"
    },
    {
      "citation_id": "52",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Y.-H Tsai",
        "S Bai",
        "P Liang",
        "J Kolter",
        "L.-P Morency",
        "R Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Multimodal transformer for unaligned multimodal language sequences"
    },
    {
      "citation_id": "53",
      "title": "Ferv39k: A large-scale multiscene dataset for facial expression recognition in videos",
      "authors": [
        "Y Wang",
        "Y Sun",
        "Y Huang",
        "Z Liu",
        "S Gao",
        "W Zhang",
        "W Ge",
        "W Zhang"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "54",
      "title": "Revolutionizing emotion insights with visual instruction tuning",
      "authors": [
        "H Xie",
        "C.-J Peng",
        "Y.-W Tseng",
        "H.-J Chen",
        "C.-F Hsu",
        "H.-H Shuai",
        "W.-H Cheng",
        "Emovit"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "55",
      "title": "Secap: Speech emotion captioning with large language model",
      "authors": [
        "Y Xu",
        "H Chen",
        "J Yu",
        "Q Huang",
        "Z Wu",
        "S.-X Zhang",
        "G Li",
        "Y Luo",
        "R Gu"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "56",
      "title": "Qwen2. 5 technical report",
      "authors": [
        "A Yang",
        "B Yang",
        "B Zhang",
        "B Hui",
        "B Zheng",
        "B Yu",
        "C Li",
        "D Liu",
        "F Huang",
        "H Wei"
      ],
      "year": "2024",
      "venue": "Qwen2. 5 technical report",
      "arxiv": "arXiv:2412.15115"
    },
    {
      "citation_id": "57",
      "title": "mplug-owl: Modularization empowers large language models with multimodality",
      "authors": [
        "Q Ye",
        "H Xu",
        "G Xu",
        "J Ye",
        "M Yan",
        "Y Zhou",
        "J Wang",
        "A Hu",
        "P Shi",
        "Y Shi"
      ],
      "year": "2023",
      "venue": "mplug-owl: Modularization empowers large language models with multimodality",
      "arxiv": "arXiv:2304.14178"
    },
    {
      "citation_id": "58",
      "title": "Ch-sims: A chinese multimodal sentiment analysis dataset with fine-grained annotation of modality",
      "authors": [
        "W Yu",
        "H Xu",
        "F Meng",
        "Y Zhu",
        "Y Ma",
        "J Wu",
        "J Zou",
        "K Yang"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    }
  ]
}