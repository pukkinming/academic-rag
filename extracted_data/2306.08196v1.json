{
  "paper_id": "2306.08196v1",
  "title": "Emostim: A Database Of Emotional Film Clips With Discrete And Componential Assessment",
  "published": "2023-06-14T01:47:59Z",
  "authors": [
    "Rukshani Somarathna",
    "Patrik Vuilleumier",
    "Gelareh Mohammadi"
  ],
  "keywords": [
    "Emotion",
    "Films",
    "Discrete theory",
    "Component Process Model",
    "Film-library"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion elicitation using emotional film clips is one of the most common and ecologically valid methods in Affective Computing. However, selecting and validating appropriate materials that evoke a range of emotions is challenging. Here we present EmoStim: A Database of Emotional Film Clips as a film library with a rich and varied content. EmoStim is designed for researchers interested in studying emotions in relation to either discrete or componential models of emotion. To create the database, 139 film clips were selected from literature and then annotated by 638 participants through the CrowdFlower platform. We selected 99 film clips based on the distribution of subjective ratings that effectively distinguished between emotions defined by the discrete model. We show that the selected film clips reliably induce a range of specific emotions according to the discrete model. Further, we describe relationships between emotions, emotion organization in the componential space, and underlying dimensions representing emotional experience. The EmoStim database and participant annotations are freely available for research purposes. The database can be used to enrich our understanding of emotions further and serve as a guide to select or create additional materials.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Affective Computing (AC) is an interdisciplinary domain that focuses on understanding emotions and enhancing machines with emotional intelligence  [1] . Therefore, there has been a growing interest in developing various experimental settings to better probe and characterize emotions. Those experimental settings vary depending on the theoretical model used to define emotions, the material used, and the measures collected. An abundant literature has mainly assumed either discrete or dimensional models of emotion, and more rarely considered appraisal theories to represent emotions. Although they are not mutually exclusive, their emphasis and scope differ. For example, the discrete model explains emotions as distinct entities, such as happiness and sadness that underlie more complex emotions  [2] . The dimensional theory describes emotions according to different levels of arousal and valence elements  [3] , and the appraisal model invokes a process-based generation  [4] . The latter hypothesizes that different components are jointly engaged and interact to trigger specific emotions. The appraisal model can help explain how emotions are triggered by specific circumstances and how they influence behaviour and decision-making  [4] . The appraisal model can also help explain why different people may have different emotional reactions to the same situation, as they may have different appraisals of the situation based on their personal experiences, beliefs, and values  [5, 6] . Therefore, this model assumes that emotions are highly subjective. Further, some research considered appraisal model to explain the neural activation of the brain  [7] . Moreover, the constructivist theory of emotions proposes that emotions arise from conceptual categorization processes that rely on core affective dimensions, such as valence and arousal, which are further integrated with other sources of knowledge determined by perception, attention, and past experiences  [8, 9] . Furthermore, existing studies  [10, 11] , suggest that emotion categories are not confined to a single region or system within the brain. Instead, they are represented as configurations spanning across multiple interconnected brain networks. These findings align with the theories of appraisal and constructivism, which proposes that emotions are distinguished by a combination of perceptual, mnemonic, prospective, and motivational elements  [10] .\n\nDifferent stimuli, such as films  [12, 13] , images  [14, 15] , and sounds  [16, 17] , have been used to induce emotions and are generally pre-assigned to different categories according to one given model. Among these stimuli, films have gained wider attention and a reputation in AC as a useful medium to effectively induce diverse emotions  [12, 18, 19] . Features of films, such as cost-effectiveness  [20] , the possibility to elicit intense emotions  [21] , easy laboratory setup  [19] , and triggering of subjective and objective changes  [12, 13, 22, 23] , can be listed as some advantages.\n\nHere we present a database of emotional clips that have also been analyzed to understand emotion formation assuming the Component Process Model (CPM)  [6] , a variant of the appraisal model where emotions are proposed to result from interactive effects of different sub-processes. Full CPM focuses on five main components: Appraisal, Motivation, Physiology, Expression, and Feeling, which have rarely been studied using data-driven approaches. We further explore the relationships between the CPM and the discrete model of emotion using this database.\n\nIn this paper, we first present a film library named EmoStim, which was assessed in terms of both discrete and componential models of emotion. We then explain our main analysis of the dataset features. The EmoStim dataset and the presented analysis can be used in future research as a material or guide to study emotions using different approaches. Our annotated dataset can also be further examined to study emotions using state-of-the-art techniques and algorithms in order to extend the current result. The EmoStim film database and participant annotations are freely available for research purposes upon request.\n\nThe rest of the paper is structured as follows. The following section introduces the Component Process Model and related works. In section 3, we discuss our approach. Then we present our analysis and results in section 4. Section 5 includes our discussion and conclusion.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Background",
      "text": "Discrete theories of emotions postulate that a limited number of distinct entities represent main or \"basic\" emotions. However, many issues remain disputed; hence, there is no consensus on the exact number of discrete emotions  [20, 24] . Nevertheless, the six-emotion model proposed by Ekman, et al.  [25]  (anger, happiness, fear, surprise, disgust, sadness), as well as the six-emotion model by Frijda, et al.  [26]  (desire, happiness, interest, surprise, wonder, sorrow), or the 11 emotion model by Izard  [27]  (joy, interest, fear, sadness, disgust, anger, guilt, shame, contempt, love, attachment) are commonly used in research. Here we consider a wider range of 14 categories (fear, anxiety, anger, shame, warmhearted, joy, sadness, satisfaction, surprise, love, guilt, disgust, contempt, and calm), using a subset of emotions adapted from Differential Emotion Scale (DES)  [28, 29] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Component Process Model (Cpm)",
      "text": "The Component Process Model (CPM) is a variant of appraisal theories seeking to explain the unfolding of emotions from event perception and interpretation to subjective feeling. CPM hypothesizes emotions as a process of synchronized components including physiological changes, behaviour, and cognition over a limited time  [30] . As shown in Figure  1 , the five main components of the CPM can be defined as Appraisal, Motivation, Physiology, Expression, and Feeling, which are interconnected  [6] .\n\nAccording to a standard CPM framework, an event is first assessed by the appraisal component at four levels: 1) Relevance: \"Is the incident relevant for me?\", 2) Implications: \"What are the consequences?\", 3) Coping potential: \"How well can I overcome the consequence?\" and 4) Normative significance: \"Is this important for my self-concept and norms?\". Following such appraisals, the response will affect the other four components. This can then trigger the motivation component activating action tendencies and formulating appropriate responses modifying the previous motivational state. The physiological component incorporates bodily changes based on appraisal and motivational changes (e.g., faster heartbeat), and the expression component activates expressive motor behaviour (e.g., smiling). These components are integrated and constantly fused, merging into a coherent conscious experience defined as the feeling component. Reflection on the resulting experience can be described by categorical and verbal labels (sad, happy, and fear). The layout of these main five components sets up a recursive and parallel functioning of the CPM. Neuroscience studies show that emotions have distributed representations in multiple brain regions  [10, 31, 32] . Moreover, emotion patterning relies on differential neocortical involvements together with various patterns of cortical-subcortical interactions  [10] . Furthermore, there is no one-to-one correspondence of emotions with specific brain regions  [11, 33] . Hence, neuroscience findings are generally consistent with the brain's representation of emotion according to the componential model associated with appraisal, pleasantness, novelty, goal-relevance, action tendencies, and social norms rather than discrete or dimensional views  [10, 32, 34] .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Related Works",
      "text": "According to surveys  [20, 35] , film clips are the most prominent paradigm for emotion elicitation. In line with this, various studies have assessed the efficacy of film clips in inducing emotions represented by dimensional and discrete theories. Moreover, annotated emotional film clips based on the dimensional model  [12, 22, 36]  and the discrete model  [19, [37] [38] [39] [40]  are publicly available. However, these approaches focus on the feeling component and ignore the more complex process by which emotions are triggered. Furthermore, they do not allow studying the full emotional model with the five components described by the CPM.\n\nThe effectiveness of films has been verified by independent researchers  [19, 41]  and previous works  [42, 43]  to provide accounts based on appraisal theory. But those datasets are not publicly available. Therefore, here we present EmoStim as an open film library to study both componential and discrete models of emotion together. Our previous experimental work has already shown the effectiveness of the EmoStim library in understanding emotional features. For example, Mohammadi, et al.  [42]  uncovered six dimensions defined by the CPM principles to interpret emotional experience, beyond the traditional aspects of arousal and valence. This high-dimensional outcome shows the limitations of previous works ignoring full CPM. In another case, but using a subset of the EmoStim library, Men√©trey  [43]  collected both subjective and physiological measures from twenty participants. The results demonstrated the potential of EmoStim film clips in inducing a range of emotions as defined by discrete theory and triggering distinctive physiological variations. Similarly, EmoStim has been used to investigate emotion organization within brain networks and their link with different components  [32] .\n\nOverall, these observations emphasize the importance of the CPM perspective for better understanding emotion and the potential of EmoStim for studying the richness of human affective experiences. They also highlight the usefulness of developing annotated multimedia and dataset for future research. Therefore, we present EmoStim as a dataset of emotional film clips and subjective measures that can be shared across researchers and fields.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Approach",
      "text": "We first present the process of selecting emotional film clips, assessments, and data collection setup.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Material Selection And Assessment",
      "text": "Film clips provide naturalistic events due to their dynamic nature while being easily implemented in the laboratory  [19] . They can induce powerful emotions theorized by both discrete  [18, 19]  and dimensional  [12, 22]  models, including complex emotions such as tenderness or compassion  [19]  beyond basic categories such as fear or disgust. Their effectiveness for studying componential appraisal models  [18, 43]  is also supported by their impact on physiological bodily states such as heart function  [13, 43] , skin conductance  [13, 18] , and brain activity  [12, 13, 22, 23] . Our study embraced a wide range of discrete theory-based emotions that were induced using film clips to assess the corresponding CPM descriptors.\n\nAs the initial step, we selected 139 emotional film clips from affective literature  [19, 37, 39, 44]  based on their availability. Film clips were presented in a randomized order and rated on discrete emotional categories and CPM descriptors. The discrete scale was extracted from the Differential Emotion Scale (DES) to evaluate 14 affective states: fear, anxiety, anger, shame, warm-hearted, joy, sadness, satisfaction, surprise, love, guilt, disgust, contempt, and calm  [29, 45] . To assess CPM descriptors, we used a modified version of the GRID instrument, CoreGRID  [2] , and selected a subset of 39 features depicting the five main components of CPM (appraisal, motivation, expression, physiology and feeling). CoreGRID descriptors were chosen based on applicability for a passive emotion elicitation during movie watching, rather than active first-person engagement as in real events. This CoreGRID selection strategy appropriately reduced the noninformative data from the absence of variance. For example, descriptors such as \"it was caused by my own behavior\" or \"it was important and relevant for my goals\" were removed because they would consistently rate as \"No\". Supplementary material Table  S1  provides the full list of CoreGRID items, and supplementary material Table  S2  summarizes the 14 discrete theory-based emotions questions.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Experimental Setup",
      "text": "This work was approved by the Geneva Cantonal Research Committee and followed their guidelines in accordance with the Helsinki declaration. We used the crowdsourcing web platform: CrowdFlower to recruit participants for the study. The selected participants were limited to native English speakers from USA or UK. The estimated duration for each task was set at 10 minutes with a 110¬¢(USD) reward per task (an effective hourly wage of $6.6 (USD)). However, the actual average time for completing the task was around 6 minutes. We ensured that participants had watched the film clip using six quality control questions (e.g., did you see any animal, see anyone crying, hear any gunshot, see sports activity, see corpse). We discarded the data if a rater made two wrong answers or more out of six questions. The distribution of the correct quality control answers is given in Supplementary Figure  S1 .\n\nWe advised participants to experience emotions freely, recall feelings, and evaluate their experiences on their own real feelings, not what they believed people should feel while watching the clip. Participants started by self-reporting personality traits using the Big Five Inventory 10 (BFI-10) questionnaire  [46]  (Supplementary material Table  S3 ) and then watched the film clips in randomized order. They were instructed to mark their emotional experience on the CoreGRID and discrete emotion questionnaires along a 5-point Likert scale, from \"not at all\" to \"strongly\". We randomized the order of each questionnaire's items.\n\nFirst, in a pilot experiment, we collected five assessments per film clip (n=139). We then chose a subset of 99 clips from these based on the emotional intensity, and the emotion discreetness observed among pilot raters. Next, to achieve more statistical power for reliable inferences on emotionality, we collected ten more assessments per selected clip. The final film dataset duration is about 3.7 hours, with an average of 133.8 seconds per clip. It comprises 1792 validated assessments from 638 workers (358 males, mean age = 34, SD = 11). However, in the current paper, we analyze 1567 assessments belonging to the 99 selected clips with at least 15 assessments per video clip, gathered from 617 participants (workers with a unique ID), which ensured good statistical power for inferences. The participants' average age is 33.78 years (standard deviation = 11.76) for the used dataset with 1567 assessments. The gender distribution consists of 940 males and 627 females, while the geographic distribution comprises 1272 individuals from the USA and 295 from the UK. Supplementary material \"Dataset_FilmClipsDetails\" (https://tinyurl.com/FilmClipsDetails) lists the 139 film clips used in the full experiment, with details of the 99 clips selected for this paper, and supplementary material section 1 illustrates the average intensity of each emotion.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Analysis And Results",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Discrete Emotion Categories",
      "text": "We first analyzed the score distribution computed from the 1567 ratings across 99 clips to explore the effectiveness of selected stimuli in inducing a wide range of emotions. Figure  2  represents results for 14 discrete theory-based emotions along the 1-5 points Likert scale. Although all emotions were experienced to some level, the distribution of ashamed, warm-hearted, joy, love, guilt and contemptuous was reported less frequently. To test for any similarity or confusion across different emotions, we calculated Spearman correlation matrices per movie and then computed the average to obtain the concatenated matrix. The results are given in Figure  3  and show clear high-level differences between positive and negative emotions, with more varying correlations within these high-level families. Accordingly, warm-hearted, joy, love, and satisfaction are related with each other to different degrees. Calm is also moderately correlated with joy, satisfaction, and love, whereas surprise does not show a high correlation with any other emotion. Among negative categories, fear is correlated with anxious, and anger correlates mostly with ashamed, contempt, and disgust, followed by sad and guilt. Guilt and ashamed are better correlated and show a similar association with anger and contemptuous. Sad is correlated with anger, guilt, contemptuous, and disgust. Contemptuous is correlated with disgust, anger, sad, ashamed, and guilt. Comparing the average correlation among all positive (warm-heartedness, joy, calm, satisfaction, surprise, love) with that among all negative emotions (fear, anxious, anger, ashamed, sad, guilt, disgust, contemptuous), without the diagonal identity values, showed similar values of 0.43 and 0.48 for positive and negative categories, respectively. Hence, valence did not affect the relative similarity/dissimilarity between discrete emotions in our dataset. To further explore similarities and differences between emotions, we applied hierarchical clustering to the correlation matrix obtained above. As shown in Figure  4 , this analysis confirms a high-level differentiation between positive and negative emotions, as classically observed in the literature. At the lower level, six clusters are observed that can be interpreted as embarrassment (guilt, ashamed), threat (anxious, fear), irritation (sad, disgust, anger, contemptuous), surprise, serenity (calm), and happiness (joy, satisfaction, warm-hearted, love). Moreover, looking at the overall functional and situational relationships of these clusters suggests a distinction possibly opposing self-directed negative (threat and embarrassment) or positive (joy/satisfaction) affective responses, against otherdirected negative (anger/sad/contempt) or positive (love/warm-hearted) responses. These results accord with a hierarchical organization of emotion terms and their similarity, putatively reflecting differences in underlying components. We also investigated gender effects on emotion rating averages over all film clips using a Wilcoxon Rank Sum with Holm's correction for family wise error. As shown in Table  1 , men rated film clips significantly higher on all emotions except for anxious and disgust, where there is a significant difference. Interestingly, the largest difference concerned a lesser degree self-related positive emotion (satisfied) to a self-related negative emotion with social significance (guilt/shame), higher in males than females, whereas the least whereas the least difference was observed for more basic, sensory-driven, and non-social emotions (disgust, fear, anxious).",
      "page_start": 5,
      "page_end": 7
    },
    {
      "section_name": "Componential Emotion Features",
      "text": "Next, we examined the distribution of CoreGRID ratings across emotional clips. As shown in Figure  5 , all CPM descriptors were experienced to some level. However, a few items were reported less frequently, such as \"want to destroy something\", \"want to damage something\", \"close your eyes\", \"show tears\", \"have stomach troubles\", \"sweat\", \"produce abrupt body movements\", \"produce speech disturbances\", \"breathing is slowing down\", and \"have the jaw drop\". We then performed a cluster analysis on the CPM profile of different emotion categories as defined by the discrete model to identify similarities or differences among them in the componential space.\n\nAs shown in the following formula, the emotion profile was calculated using the weighted average of the normalized CoreGRID item scores, where the weight of each sample was proportional to the scores of each emotion term  [42, 47] .\n\n‚àë ùë§ ùëñùëó ùëõ ùëñ=1\n\nIn the above formula, wij is the score of emotion term j in sample i, xi is the CoreGRID score vector for sample i, CPj is the weighted average CPM profile for emotion term j, and n is the total number of samples. We conducted a Ward hierarchical clustering using the Euclidean distance metric  [42, 47] . We opted to utilize the Ward algorithm for clustering analysis as we aim to find clusters based on variance with a relatively large number of real-valued variables with similar scales  [48] . In addition, we did not want to fix the number of clusters and also some studies have used Ward algorithm to identify clusters, and this allows for a better comparison of cluster structures  [18, 49] . However, it is important to note that Ward's algorithm may be sensitive to outliers and noise, potentially leading to suboptimal results. Additionally, this algorithm can be computationally expensive and requires visual inspection of dendrograms to determine the optimal number of clusters. Despite these limitations, the use of Ward's hierarchical clustering method can be justified in this context, given the need to identify groups of similar emotions based on multiple variables and uncover potential subgroups of interest through dendrogram analysis.\n\nResults are depicted in Figure  6  and again show a high-level difference between positive (green cluster), and negative (orange cluster) affect. At the lower level, we observed seven clusters that resembled clustering based on discrete terms (Figure  4 ), but with some notable differences. These seven clusters can be identified as embarrassment (guilt, ashamed), surprise, irritation (contemptuous, anger, sad), distress or aversion (anxious, fear, disgust), happiness (satisfaction, joy), affection (love, warm-hearted), and serenity (calm). This similarity to the hierarchical clustering obtained from the discrete emotion ratings supports the validity of CPM in capturing the functional organization of emotional experience across a range of categories. Among emotions whose relationships changed between the two analyses, surprise shifted to the negative valence side, while calm remained on the positive side. More interestingly, disgust now clustered with fear/anxiety rather than sad/anger, whereas sadness was still grouped with the sad/anger cluster, disclosing a clearer distinction between more sensory-driven and more socially-determined sources of aversion, respectively.\n\nFigure  6  Results of the hierarchical clustering of discrete emotion terms using their CPM representation. Two general clusters of negative (orange cluster) and positive (green cluster) emotions with meaningful sub-clusters can be distinguished.\n\nSupplementary material Figure  S2  illustrates each emotion's within-subject normalized average profile on the 39 CoreGRID descriptors. This figure might be used as a guide by researchers interested in selecting stimuli that trigger certain CoreGRID items. Clear differences between positive and negative emotions are noted not only for \"feel good\", \"feel calm\", \"feel bad\", \"smile\", \"feel warm\", but also for \"feel your breathing is getting faster\", \"feel your heartbeat is getting faster\", \"have stomach troubles\", \"have a feeling of a lump in the front\", \"experience muscles tensing\", \"want the situation to continue\", \"want to undo what was happening\", \"feel the urge to stop what was happening\", \"want to damage, hit or say something that hurts\", \"want to destroy something\", \"think it violated laws/social norm\", and \"feel it was unpleasant for you\".\n\nTo complement previous analyses, we ran two new clustering analyses at the movie-level for the mean values of 14 emotions and CPM. Results from the discrete emotion ratings revealed five main emotion clusters differentially induced by the film clips, shown in supplementary material Figure  S3 . These clusters appear to reflect specificities in clip content associated with calm, happiness, anxious, distress, and irritation. Detailed graphs are available in supplementary material Figures  S5-S9 .\n\nThe second clustering analysis using the average GRID item values at the movie level is shown in supplementary material Figure  S10 . It identified six-film clip clusters that could be correlated with CPM features. Further analysis revealed that the first cluster is dominated by GRID items such as \"feel good\", \"feel calm,\" \"smile\", and \"feel strong\", which are related to valence. The second cluster was suggestive of appetitive motivations/action tendencies as it is dominated by items such as \"situation to continue\", \"smile\", \"feel good\", \"feel calm\", and \"feel warm\". The third cluster was loaded mainly with items belonging to norm violation (\"incongruent with your standards\", \"think it violated laws/social norms\"), defensive (\"feel the urge to stop\", \"want to undo\", \"want to tackle the situation\"), and autonomic features (\"muscles tensing\", \"heartbeat is getting faster\"), as well as expression or feeling (\"did you frowned?\", \"did you feel bad?\"), which can potentially be interpreted in terms of goal and norm significance. The fourth cluster is loaded with items belonging to norm violation, unexpected, defensive, and negative expression, potentially defined as expectation violations or obstructiveness. The fifth cluster is mainly dominated by items belonging to unexpected and valence features, which can be interpreted as novelty. The sixth final cluster is loaded with items belonging to valence appraisal, calm, expected, unexpected, and defensive ratings, suggesting that it can be categorized as coping ability. Detailed graphs are available in supplementary material Figures  S11-S16 .\n\nWe also compared the movies associated with each cluster to find similar relationships between the CoreGRID clustering and the discrete emotion clustering at the movie level. Correspondingly, emotion cluster 1 (calm) is related to CoreGRID 1 (valence) and 6 (coping ability), emotion cluster 2 (happiness) is associated with CoreGRID 2 (motivations/action tendencies), emotion cluster 3 (anxious) is related to CoreGRID 4 (obstructiveness) and 5 (novelty), emotion cluster 4 (distress) is associated with CoreGRID 3 (norms), and emotion cluster 5 (irritation) is related to CoreGRID 4 (obstructiveness).\n\nFinally, we conducted an exploratory factor analysis with a varimax rotation to find the latent dimensions explaining emotional experience using the z-scored normalized 39 CoreGRID items. We identified six factors explaining 51.6% total variance. Then, using the z-score normalized 39 CoreGRID items and 14 emotions, we performed a factor analysis and applied a varimax rotation  [32, 50] . We chose to use varimax orthogonal rotation because it optimizes the distribution of the squared loadings within each factor by maximizing their variance and reduces the presence of cross-factor loadings (e.g., orthogonal dimensions). This results in clearer identification and interpretation of the underlying factors in the analysis. We identified four factors based on scree plot results. The four factors explain 37.3% total variance as shown in supplementary material Table  S5 . These factors can be interpreted as norms (10.99%), valence (12.55%), novelty or suddenness (6.52%), and action tendencies (7.18%). As shown in Figure  7 , we plotted the loading values of each factor for 14 emotions. For better representation, we reversed the sign of the action tendency and norms values to show positive tendencies. As expected, valence shows the highest positive loadings for joy, satisfaction, love, warmhearted, and calm. In contrast, disgust, anxious, fear, anger, sadness, and contempt all load negatively on valence. Joy is the most positive and fear the most negative. Interestingly, surprise had an intermediate valence load. On the other hand, for norm values, joy, satisfaction, love, warm-hearted, and calm show positive loadings; while other emotions (disgust, anxious, fear, surprise, guilt, ashamed, anger, contemptuous and sad) are negatively loaded on norms, with disgust showing the highest (negative) load. For novelty and suddenness, fear and surprise show high loading, unlike calm, that is the lowest. Lastly, guilt, anger, contempt, and sad show high action-tendency loadings, whereas disgust and fear show moderate loadings, and calm and satisfaction are the lowest on action tendency.\n\nFigure  7 : Spider plot for the factor analysis loading values of each factor for 14 emotions. Zero level is indicated by the black line. Scales of all dimensions in this plot are between -0.6 and 1, where norm violation vs compliance ranges from -0.6 to 0.2, negative vs positive valence ranges from -0.3 to 0.9, no novelty vs novelty ranges from -0.2 to 0.4, and appetitive vs defensive action tendencies ranges from -0.02 to 0.6.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Potential Uses Of Emostim",
      "text": "Our EmoStim dataset, which includes emotion annotations and film clip details, has numerous potential use cases. These include:\n\n‚Ä¢ To further explore the emotion understanding from a componential perspective and using techniques such as Machine Learning to explore the feasibility of computational modelling. ‚Ä¢ Serving as a benchmark for developing and evaluating automatic emotion recognition algorithms, which could be applied to domains such as psychology, human-computer interaction, and AC. ‚Ä¢ Providing insights into audience reactions to different types of films, which could inform decisionmaking in film production, marketing, and distribution. ‚Ä¢ Facilitating teaching on emotion recognition and analysis in courses such a psychology, emotion, and AC. ‚Ä¢ Supporting the design of more emotionally engaging experiences in interactive media, such as films, video games and virtual reality, by understanding how users emotionally respond to different visual and audio stimuli. ‚Ä¢ Enabling research on mental health and emotional well-being, such as studying the effects of different films on people's mood and emotions.\n\n‚Ä¢ Selecting subset of the collected clips according to the emotional content for eliciting emotions in emotion studies.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Discussion And Conclusion",
      "text": "Although emotional film clips are widely used in affective computing, finding appropriate material for a range of emotions can be challenging. In parallel, classical methods of emotion modelling based on discrete and dimensional theories are widely used in the literature but do not appear sufficient to account for explaining the whole range of emotion mechanisms. More recently, other models, such as \"constructivist\" theory and appraisal theory were also proposed, which are better aligned with neuroscience findings that show multiple brain processes subserving emotional experience  [11, 33] .\n\nIn this study we focused on CPM because it is easier to operationalise and measure. Moreover, in this study we assumed that emotions are highly subjectively pertaining to the principles of the CPM.\n\nIn the current work, we present EmoStim as a database of emotional film clips validated with both discrete and CPM-based labels that may be further refined in future research. Specifically, we report measures based on 99 emotion clips which have at least 15 rating items per film clip. A repository of these film clips and rating data collected is accessible to researchers upon their request through this link https://tinyurl.com/EmoStimDataset.\n\nThe first goal of this study was to determine the efficacy of selected film clips in inducing a wide range of emotions. Results demonstrate that all targeted emotions were experienced to some level. However, the distribution of warm-heartedness, joy, love, guilt, and contempt is relatively low. A possible explanation for this might be the complex nature of these emotions, which imply social or self-conscious processing (guilt, contemptuous)  [51] . Similarly, Izard  [51]  interpreted love and attachment feelings as related to human evolution, normative development, and adaptation, that involve a high level of self/self-other representation and cultural cognition. Therefore, the selected film clips and duration may not be sufficient to reliably trigger such emotions for some participants. A similar explanation concerning attachment-related emotions (love, warm-hearted) underscores the inadequacy and difficulty of current lab experiments to obtain valid emotional stimuli as they result from biological tendencies  [19] . In any case, in further analysis (see below), we found evidence for a distinction between emotions with self-directed and other-directed features, which does not only support the role of self-conscious and social processing in emotion elicitation, but also indicates that EmoStim material and CPM descriptors could successfully capture these aspects despite their low incidence during movie watching.\n\nA second goal of our study was to inspect the organization of emotion space covered by EmoStim. To do so, we computed a correlation matrix across emotions at the movie level (see Figure  3 ) that revealed a clear high-level differentiation between positive and negative emotions, demonstrating the efficacy of selected clips to induce different valence levels and replicating previous research with other stimuli  [21] . Furthermore, results showed a high association between attachment-related (love, warm-heartedness) and happiness-related (joy, satisfaction) emotions, in line with their shared positive valence. Calm was also distinctively associated with positive emotions, particularly happinessrelated, which may reflect the pleasantness characteristic of calm but with a lower arousal level. In contrast, fear, anxiety, anger and disgust demonstrated strong correlations with each other, reflecting their shared negative valence and perhaps other aspects, such as uncertainty or unpredictability associated with these different emotion terms  [52] . Likewise, the correlation between guilt and ashamed and their similar general relationships with other emotions is presumably due to sharing negative valence as well as potency, unpredictability, and social aspects  [52] . This correlation accords with the literature, which considers shame, guilt and contempt as social or self-conscious emotions that require higher-order cognition and cultural cognition  [51] . On the other hand, surprise did not correlate with any other particular emotion, suggesting specific characteristics of this experience putatively based on its ambiguous valence  [53]  and the role of novelty or suddenness in its elicitation  [52] .\n\nThese complex relationships among different emotions were further dissected by our clustering analysis that sought to uncover their structural organization in the CPM space. Consistent with the literature  [18, 24] , our results (see Figure  6 ) again underscored the main high-level differences between positive and negative emotions. However, at the lower level, we observed distinct and meaningful subclusters: embarrassment, surprise, irritation, distress, happiness, affection, and serenity. Interestingly, while calm remained a distinct state of positive affect, in this analysis surprise appeared more similar to the negative than the positive subclusters, indicating that it arose in more negative contexts in the selected film clips. A similar pattern for surprise was already reported in filmbased literature  [18] , although this emotion is classically considered neutral or ambiguous in terms of valence  [53, 54] . By comparing these CPM clustering results with the analysis of emotion terms at the movie level (see Figure  4 ), we could also observe a better functional similarity among emotions in the former than in the latter case. Accordingly, while serenity, embarrassment, threat, and social irritation (anger/contempt) showed similar patterns in both analyses, disgust was closer to threat (rather than embarrassment) and sad closer to embarrassment (rather than disgust) when considering CPM features. This would be consistent with disgust involving more appraisal of harm and avoidance responses similar to threat, whereas sadness implies more other-directed social cognitions shared with embarrassment. However, at the movie-level, the happiness cluster was associated with four terms (joy, satisfaction, warm-hearted, and love), whereas in the CPM space, these terms were clustered in only two lower levels interpreted as happiness (positive self-directed) and affection (positive attachment-related). Moreover, a similar dissociation between other-regarding and selfconscious emotions appeared to emerge more systematically among negative and positive emotions in the CPM clustering analysis, suggestive of an important role of social factors for certain kinds of emotions. Overall, these results confirm the capacity of film clips to trigger varying similar and dissimilar emotions, as well as the value and reliability of film feature annotations to capture their underlying functional organization. Furthermore, this validates the validity of our experimental paradigm and the EmoStim database.\n\nFinally, we conducted an exploratory factor analysis to reveal the latent dimensions underpinning emotion formation. We found four factors: norms, valence, novelty/suddenness, and action tendencies. This study supports evidence from previous observations  [18, 32, 50, 52] , where more than two dimensions beyond valence and arousal are required to fully explain the emotional experience. Furthermore, these four factors seem to converge with a role of different component processes that are assumed to mediate differences between particular emotions, as discussed above.\n\nHere we also examined how the 14 discrete emotions mapped against the loading values of these four factors (see Figure  7 ). Besides a clear valence factor, we observed that positive emotions (joy, satisfaction, love, warm-hearted), as well as calm, loaded strongly on the first factor corresponding to behaviours aligned with social values and norms. In contrast, negative emotions loaded negatively on this factor, highlighting their link to disruptive events and norm violation. On the other hand, emotions associated with uncertainty and urgency are loaded high on novelty and suddenness factors, in contrast to calm. Specifically, high loadings of surprise  [55]  and fear  [49]  for novelty and suddenness accords with previous reports. Interestingly, the last fourth factor related to action-tendency is manifested mainly by emotions associated with embarrassment and irritation (including guilt, shame, anger, contempt, and sadness). This resonates with the previous view of Scherer  [56] , whereby guilt, shame, anger, and sadness are defined as emotions that provide adaptive functions that drive action, recovery, and motivational needs. Previous research has shown that variations of anger are related to action and action tendencies  [10, 27] .\n\nTaken together, our data demonstrate the efficacy of selected film clips in the EmoStim database to induce a variety of emotions for empirical studies in affective sciences. Nevertheless, films are passive stimuli where a participant observes emotional content  [21]  rather than being directly implicated him/herself by ongoing events. Therefore, unlike more active procedures with Virtual Reality (VR) and video games, using films also imply potential limitations due to passive participation  [57, 58]  and lack of ecological validity  [59] . These limitations can be mitigated by using passive stimuli in combination with active stimuli. For instance, recent affective research has shown the efficacy of VR paradigms in studying emotions using games  [47, 59]  or immersive films  [60, 61]  and images  [62] . The effectiveness of VR can be explained by this immersion capacity, allowing a feeling of presence, interaction, and easy installation while still permitting controlled experiments  [24] . However, while VR is a growing technology, films are backed by more robust research and more easily combined with other research tools, including neuroimaging techniques and advanced physiology sensors. Consequently, a validated film dataset such as ours can be valuably used for future research in behavioral sciences and may guide the design of future studies investigating emotional unfolding in more highly immersed and ecological valid conditions including VR. Moreover, to fully address the five components assumed in CPM, future research should also measure physiological and facial signals to better characterize the differential patterns of emotion experiences.\n\nWhile the film dataset used in this study provided valuable insights into the trends and patterns in the emotions, it is important to acknowledge the constraints of the dataset. Firstly, despite the smaller scale of EmoStim in comparison to other datasets such as BoLD  [63]  we chose to annotate film clips using discrete emotions and CoreGRID items in accordance with our approach to understanding emotions from a CPM perspective. As a result, generating and utilizing short clips that may not effectively induce emotions as required by the CPM would not be practical or feasible for our study. Moreover, we acknowledge that the dataset was collected solely from participants in the USA and UK and that, therefore, it may not be representative of other cultural or linguistic groups. However, we ensure the diversity within the population by ensuring a mix of ages, genders, and regions. Therefore, we ensure that the dataset is inclusive and culturally sensitive to USA and UK regions. Additionally, the dataset does not include the test-retest reliability  [64]  which is used in research to measure reliability. Because it may not be practical for our crowdsourcing applications in emotion research. This is due to the potential difficulty in finding willing participants to take part in the study multiple times, as well as the possibility that external factors beyond the study could influence their emotional experiences. However, we have collected more than 15 assessments per video clip from 617 participants in our dataset, providing a sufficient level of rater reliability.\n\nIn sum, emotions play an important role in human development and everyday life. Therefore, understanding emotion evolution is critical in different domains and applications, but the availability of reliably validated material is still challenging and often limited to only a restricted set of emotions.\n\nIn this paper, we present EmoStim as a database of emotional film clips built from a selection of 99 film clips combined with rich participant annotation and multistep validation. These clips cover a large emotional distribution with reliable induction. They allow a robust differentiation of affective states defined by the discrete model as well as by more elementary features derived from a CPM theorybased approach. Our results show the efficacy of our database in studying emotional experience effectively. Future research can use this database to extend emotion-related research with new measures and/or analyze the collected evaluations made available with EmoStim in order to enhance our understanding of emotion generation and experience in humans.",
      "page_start": 13,
      "page_end": 15
    },
    {
      "section_name": "Clustering At Movie Level For Average Emotion Ratings",
      "text": "Figure  S3 : Hierarchical clustering at movie-level for average emotion and CPM ratings.\n\nTo find similar movies that trigger emotions, we run a clustering analysis at the movie level by taking the average of emotions for each film. Similar to the previous hierarchical clustering (see main text), we could identify five groups including calm, happiness, anxious, distress, and irritation, as shown in figures 4-8. To find movies that trigger similar componential patterns, we ran a clustering analysis at the movie level by taking the average of CoreGRID items. We could identify six sources of similarity including valence, appetitive motivations/action tendencies, norms, obstructiveness, novelty appraisal, and coping potential, as shown in figures 10-15. The results also suggest that our film database has globally more negative than positive content. Nevertheless, our findings demonstrate that the selected emotional clips can induce a wide range of discrete emotions and CPM features at several intensities.",
      "page_start": 16,
      "page_end": 31
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , the five main components of the CPM can be",
      "page": 2
    },
    {
      "caption": "Figure 1: Component Model of emotion with five components.",
      "page": 3
    },
    {
      "caption": "Figure 2: represents results for",
      "page": 5
    },
    {
      "caption": "Figure 2: Distribution of scorings in 14 emotions. Colour scales correspond to the proportion of",
      "page": 5
    },
    {
      "caption": "Figure 3: and show clear high-level differences between positive and negative emotions,",
      "page": 6
    },
    {
      "caption": "Figure 3: Average emotion correlations at movie level. Scores represent the Spearman correlation",
      "page": 6
    },
    {
      "caption": "Figure 4: , this analysis confirms a high-level",
      "page": 6
    },
    {
      "caption": "Figure 4: Results of the correlation-based hierarchical clustering of discrete emotion ratings across",
      "page": 7
    },
    {
      "caption": "Figure 5: Distribution of scorings for 39 CoreGRID descriptors. Colour scales correspond to the",
      "page": 8
    },
    {
      "caption": "Figure 6: and again show a high-level difference between positive (green",
      "page": 9
    },
    {
      "caption": "Figure 4: ), but with some notable differences. These",
      "page": 9
    },
    {
      "caption": "Figure 6: Results of the hierarchical clustering of discrete emotion terms using their CPM",
      "page": 10
    },
    {
      "caption": "Figure 7: , we plotted the loading values of each factor for 14 emotions. For better",
      "page": 11
    },
    {
      "caption": "Figure 7: Spider plot for the factor analysis loading values of each factor for 14 emotions. Zero level is",
      "page": 12
    },
    {
      "caption": "Figure 6: ) again underscored the main high-level differences",
      "page": 14
    },
    {
      "caption": "Figure 4: ), we could also observe a better functional similarity among emotions in the",
      "page": 14
    },
    {
      "caption": "Figure 7: ). Besides a clear valence factor, we observed that positive emotions (joy,",
      "page": 14
    }
  ],
  "tables": [
    {
      "caption": "Table 1: , men rated film clips",
      "data": [
        {
          "Male": "Mean",
          "F\nemale": "Mean",
          "Test \nstatistic": "",
          "p-value for \ngender analysis": "",
          "Effect size \n(First group is \nMale)": ""
        },
        {
          "Male": "2.10",
          "F\nemale": "1.79",
          "Test \nstatistic": "5.228",
          "p-value for \ngender analysis": "p<0.001",
          "Effect size \n(First group is \nMale)": "0.257"
        },
        {
          "Male": "2.21",
          "F\nemale": "1.88",
          "Test \nstatistic": "4.803",
          "p-value for \ngender analysis": "p<0.001",
          "Effect size \n(First group is \nMale)": "0.247"
        },
        {
          "Male": "2.59",
          "F\nemale": "2.28",
          "Test \nstatistic": "4.363",
          "p-value for \ngender analysis": "p<0.001",
          "Effect size \n(First group is \nMale)": "0.237"
        },
        {
          "Male": "2.27",
          "F\nemale": "1.87",
          "Test \nstatistic": "6.094",
          "p-value for \ngender analysis": "p<0.001",
          "Effect size \n(First group is \nMale)": "0.322"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Surprise": "Love",
          "2.71": "2.12",
          "1.20": "1.28",
          "2.40": "1.78",
          "1.28": "1.13",
          "5.011": "5.028",
          "p<0.001": "p<0.001",
          "0.257": "0.276"
        },
        {
          "Surprise": "Fear",
          "2.71": "2.45",
          "1.20": "1.36",
          "2.40": "2.41",
          "1.28": "1.40",
          "5.011": "0.843",
          "p<0.001": "p<0.001",
          "0.257": "0.034"
        },
        {
          "Surprise": "Anxious",
          "2.71": "2.67",
          "1.20": "1.35",
          "2.40": "2.68",
          "1.28": "1.45",
          "5.011": "0.070",
          "p<0.001": "p<0.001",
          "0.257": "-0.007"
        },
        {
          "Surprise": "Anger",
          "2.71": "2.17",
          "1.20": "1.33",
          "2.40": "1.97",
          "1.28": "1.23",
          "5.011": "2.666",
          "p<0.001": "p<0.100",
          "0.257": "0.155"
        },
        {
          "Surprise": "Ashamed",
          "2.71": "1.90",
          "1.20": "1.15",
          "2.40": "1.56",
          "1.28": "1.03",
          "5.011": "6.204",
          "p<0.001": "p<0.100",
          "0.257": "0.312"
        },
        {
          "Surprise": "Sad",
          "2.71": "2.34",
          "1.20": "1.33",
          "2.40": "2.17",
          "1.28": "1.35",
          "5.011": "2.780",
          "p<0.001": "p<0.100",
          "0.257": "0.128"
        },
        {
          "Surprise": "Guilt",
          "2.71": "1.90",
          "1.20": "1.10",
          "2.40": "1.54",
          "1.28": "0.97",
          "5.011": "6.322",
          "p<0.001": "p>0.500",
          "0.257": "0.336"
        },
        {
          "Surprise": "Disgust",
          "2.71": "2.28",
          "1.20": "1.30",
          "2.40": "2.36",
          "1.28": "1.46",
          "5.011": "-0.340",
          "p<0.001": "p>0.500",
          "0.257": "-0.061"
        },
        {
          "Surprise": "Contemptuous",
          "2.71": "2.06",
          "1.20": "1.20",
          "2.40": "1.90",
          "1.28": "1.21",
          "5.011": "2.926",
          "p<0.001": "p>0.500",
          "0.257": "0.135"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "GRID Questionnaire \nComponent": "While watching this movie, did you..."
        },
        {
          "GRID Questionnaire \nComponent": "1. \nthink it was incongruent with your standards/ideas?"
        },
        {
          "GRID Questionnaire \nComponent": "2. \nfeel that the event was unpredictable?"
        },
        {
          "GRID Questionnaire \nComponent": "3. \nfeel the event occurred suddenly?"
        },
        {
          "GRID Questionnaire \nComponent": "4. \nthink the event was caused by chance?"
        },
        {
          "GRID Questionnaire \nComponent": "5. \nthink that the consequence was predictable?"
        },
        {
          "GRID Questionnaire \nComponent": "6. \nfeel it was unpleasant for someone else?"
        },
        {
          "GRID Questionnaire \nComponent": "7. \nthink it was important for somebody's goal or need?"
        },
        {
          "GRID Questionnaire \nComponent": "8. \nthink it violated laws/social norms?"
        },
        {
          "GRID Questionnaire \nComponent": "9. \nfeel in itself was unpleasant for you?"
        },
        {
          "GRID Questionnaire \nComponent": "10. want to destroy s.th.?"
        },
        {
          "GRID Questionnaire \nComponent": "11. feel the urge to stop what was happening?"
        },
        {
          "GRID Questionnaire \nComponent": "12. want to undo what was happening?"
        },
        {
          "GRID Questionnaire \nComponent": "13. want to damage, hit or say s.th. that hurts?"
        },
        {
          "GRID Questionnaire \nComponent": "14.  want the situation to continue?"
        },
        {
          "GRID Questionnaire \nComponent": "15. lack the motivated to pay attention to the scene?"
        },
        {
          "GRID Questionnaire \nComponent": "16. want to tackle the situation and do s.th.?"
        },
        {
          "GRID Questionnaire \nComponent": "17. have a feeling of lump in the throat?"
        },
        {
          "GRID Questionnaire \nComponent": "18. have stomach trouble?"
        },
        {
          "GRID Questionnaire \nComponent": "19. experience muscles tensing?"
        },
        {
          "GRID Questionnaire \nComponent": "20. feel warm?"
        },
        {
          "GRID Questionnaire \nComponent": "21. sweat?"
        },
        {
          "GRID Questionnaire \nComponent": "22. feel heartbeat getting faster?"
        },
        {
          "GRID Questionnaire \nComponent": "23. feel breathing getting faster?"
        },
        {
          "GRID Questionnaire \nComponent": "24. feel breathing slowing down?"
        },
        {
          "GRID Questionnaire \nComponent": "25. produce abrupt body movement?"
        },
        {
          "GRID Questionnaire \nComponent": "26. close your eyes?"
        },
        {
          "GRID Questionnaire \nComponent": "27. press lips together?"
        },
        {
          "GRID Questionnaire \nComponent": "28. have the jaw drop?"
        },
        {
          "GRID Questionnaire \nComponent": "29. show tears?"
        },
        {
          "GRID Questionnaire \nComponent": "30. have eyebrow go up?"
        },
        {
          "GRID Questionnaire \nComponent": "31. smile?"
        },
        {
          "GRID Questionnaire \nComponent": "32. frown?"
        }
      ],
      "page": 20
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "33. produce speech disturbances?": "34. feel good?",
          "Expression": "Feeling"
        },
        {
          "33. produce speech disturbances?": "35. feel bad?",
          "Expression": "Feeling"
        },
        {
          "33. produce speech disturbances?": "36. feel calm?",
          "Expression": "Feeling"
        },
        {
          "33. produce speech disturbances?": "37. feel strong?",
          "Expression": "Feeling"
        },
        {
          "33. produce speech disturbances?": "38. feel an intense emotional state?",
          "Expression": "Feeling"
        },
        {
          "33. produce speech disturbances?": "39. experience an emotional state for a long time?",
          "Expression": "Feeling"
        }
      ],
      "page": 21
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "While watching this movie, did you feel...": "1. \nfearful, scared, afraid?"
        },
        {
          "While watching this movie, did you feel...": "2. \nanxious, tense, nervous?"
        },
        {
          "While watching this movie, did you feel...": "3. \nangry, irritated, mad?"
        },
        {
          "While watching this movie, did you feel...": "4.  warm-hearted, gleeful, elated?"
        },
        {
          "While watching this movie, did you feel...": "5. \njoyful, amused, happy?"
        },
        {
          "While watching this movie, did you feel...": "6. \nsad, downhearted, blue?"
        },
        {
          "While watching this movie, did you feel...": "7. \nsatisfied, pleased?"
        },
        {
          "While watching this movie, did you feel...": "8. \nsurprised, amazed, astonished?"
        },
        {
          "While watching this movie, did you feel...": "9. \nloving, affectionate, friendly?"
        },
        {
          "While watching this movie, did you feel...": "10. guilty, remorseful?"
        },
        {
          "While watching this movie, did you feel...": "11. disgusted, turned off, repulsed?"
        },
        {
          "While watching this movie, did you feel...": "12. disdainful, scornful, contemptuous?"
        },
        {
          "While watching this movie, did you feel...": "13. calm, serene, relaxed?"
        },
        {
          "While watching this movie, did you feel...": "14. ashamed, embarrassed?"
        }
      ],
      "page": 21
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "I see myself as someone who‚Ä¶": "1. \nis reserved?"
        },
        {
          "I see myself as someone who‚Ä¶": "2. \nis generally trusting?"
        },
        {
          "I see myself as someone who‚Ä¶": "3. \ntends to be lazy?"
        },
        {
          "I see myself as someone who‚Ä¶": "4. \nis relaxed and handles stress well?"
        },
        {
          "I see myself as someone who‚Ä¶": "5. \nhas few artistic interests?"
        },
        {
          "I see myself as someone who‚Ä¶": "6. \nis outgoing sociable?"
        },
        {
          "I see myself as someone who‚Ä¶": "7. \ntends to find fault with others?"
        },
        {
          "I see myself as someone who‚Ä¶": "8. \ndoes a thorough job?"
        },
        {
          "I see myself as someone who‚Ä¶": "9. \ngets nervous easily?"
        },
        {
          "I see myself as someone who‚Ä¶": "10. has an active imagination?"
        }
      ],
      "page": 21
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Item": "feel the urge to stop what was happening",
          "Component": "Motivation",
          "F1": "0.75",
          "F2": "-0.27",
          "F3": "0.24",
          "F4": "-0.16"
        },
        {
          "Item": "want to undo what was happening",
          "Component": "Motivation",
          "F1": "0.74",
          "F2": "-0.27",
          "F3": "0.20",
          "F4": "-0.17"
        },
        {
          "Item": "feel it was unpleasant for you",
          "Component": "Appraisal",
          "F1": "0.69",
          "F2": "-0.36",
          "F3": "0.16",
          "F4": "-0.19"
        },
        {
          "Item": "think it was unpleasant for somebody else",
          "Component": "Appraisal",
          "F1": "0.66",
          "F2": "-0.34",
          "F3": "0.15",
          "F4": "0.03"
        },
        {
          "Item": "feel bad",
          "Component": "Feeling",
          "F1": "0.64",
          "F2": "-0.40",
          "F3": "0.18",
          "F4": "-0.30"
        },
        {
          "Item": "think it violated laws/social norms?",
          "Component": "Appraisal",
          "F1": "0.61",
          "F2": "-0.20",
          "F3": "0.15",
          "F4": "-0.19"
        },
        {
          "Item": "Disgust",
          "Component": "",
          "F1": "0.59",
          "F2": "-0.27",
          "F3": "0.14",
          "F4": "-0.36"
        },
        {
          "Item": "frowned",
          "Component": "Expression",
          "F1": "0.55",
          "F2": "-0.25",
          "F3": "0.22",
          "F4": "-0.27"
        },
        {
          "Item": "Anxious",
          "Component": "",
          "F1": "0.46",
          "F2": "-0.32",
          "F3": "0.46",
          "F4": "-0.30"
        },
        {
          "Item": "incongruent with your standards",
          "Component": "Appraisal",
          "F1": "0.44",
          "F2": "-0.06",
          "F3": "0.11",
          "F4": "-0.11"
        },
        {
          "Item": "feel an intense emotional state",
          "Component": "Feeling",
          "F1": "0.44",
          "F2": "0.05",
          "F3": "0.42",
          "F4": "-0.24"
        },
        {
          "Item": "want to tackle the situation",
          "Component": "Motivation",
          "F1": "0.38",
          "F2": "0.09",
          "F3": "0.26",
          "F4": "-0.25"
        },
        {
          "Item": "pressed lips together",
          "Component": "Expression",
          "F1": "0.37",
          "F2": "-0.04",
          "F3": "0.35",
          "F4": "-0.32"
        },
        {
          "Item": "feel the event was unpredictable",
          "Component": "Appraisal",
          "F1": "0.33",
          "F2": "-0.00",
          "F3": "0.32",
          "F4": "0.01"
        },
        {
          "Item": "think the consequences were predictable",
          "Component": "Appraisal",
          "F1": "0.17",
          "F2": "0.04",
          "F3": "0.06",
          "F4": "-0.15"
        },
        {
          "Item": "Joy",
          "Component": "",
          "F1": "-0.14",
          "F2": "0.86",
          "F3": "-0.02",
          "F4": "-0.03"
        },
        {
          "Item": "Satisfied",
          "Component": "",
          "F1": "-0.17",
          "F2": "0.82",
          "F3": "0.03",
          "F4": "-0.11"
        },
        {
          "Item": "feel good",
          "Component": "Feeling",
          "F1": "-0.28",
          "F2": "0.80",
          "F3": "-0.05",
          "F4": "0.02"
        },
        {
          "Item": "smile",
          "Component": "Expression",
          "F1": "-0.21",
          "F2": "0.78",
          "F3": "-0.00",
          "F4": "-0.04"
        },
        {
          "Item": "Love",
          "Component": "",
          "F1": "-0.06",
          "F2": "0.77",
          "F3": "-0.02",
          "F4": "-0.18"
        },
        {
          "Item": "Warm-hearted",
          "Component": "",
          "F1": "-0.10",
          "F2": "0.75",
          "F3": "-0.08",
          "F4": "-0.21"
        },
        {
          "Item": "Calm",
          "Component": "",
          "F1": "-0.22",
          "F2": "0.68",
          "F3": "-0.16",
          "F4": "0.02"
        },
        {
          "Item": "want the situation to continue?",
          "Component": "Motivation",
          "F1": "-0.22",
          "F2": "0.56",
          "F3": "0.19",
          "F4": "-0.19"
        },
        {
          "Item": "feel calm",
          "Component": "Feeling",
          "F1": "-0.33",
          "F2": "0.54",
          "F3": "-0.21",
          "F4": "0.08"
        },
        {
          "Item": "feel warm",
          "Component": "Physiology",
          "F1": "-0.04",
          "F2": "0.53",
          "F3": "0.17",
          "F4": "-0.33"
        },
        {
          "Item": "feel strong",
          "Component": "Feeling",
          "F1": "0.04",
          "F2": "0.50",
          "F3": "0.16",
          "F4": "-0.06"
        },
        {
          "Item": "think it was important for goals of \nsomebody",
          "Component": "Appraisal",
          "F1": "0.20",
          "F2": "0.24",
          "F3": "0.20",
          "F4": "-0.19"
        },
        {
          "Item": "feel your heartbeat is getting faster",
          "Component": "Physiology",
          "F1": "0.25",
          "F2": "-0.06",
          "F3": "0.81",
          "F4": "-0.29"
        },
        {
          "Item": "feel your breathing is getting faster",
          "Component": "Physiology",
          "F1": "0.21",
          "F2": "-0.03",
          "F3": "0.79",
          "F4": "-0.36"
        },
        {
          "Item": "Fear",
          "Component": "",
          "F1": "0.43",
          "F2": "-0.32",
          "F3": "0.45",
          "F4": "-0.32"
        },
        {
          "Item": "experience muscles tensing",
          "Component": "Physiology",
          "F1": "0.44",
          "F2": "-0.15",
          "F3": "0.45",
          "F4": "-0.36"
        },
        {
          "Item": "feel the event occurred suddenly",
          "Component": "Appraisal",
          "F1": "0.36",
          "F2": "0.04",
          "F3": "0.37",
          "F4": "-0.02"
        },
        {
          "Item": "have eyebrows go up",
          "Component": "Expression",
          "F1": "0.31",
          "F2": "0.05",
          "F3": "0.34",
          "F4": "-0.30"
        },
        {
          "Item": "Surprise",
          "Component": "",
          "F1": "0.25",
          "F2": "0.30",
          "F3": "0.33",
          "F4": "-0.13"
        },
        {
          "Item": "produce speech disturbances",
          "Component": "Expression",
          "F1": "0.10",
          "F2": "0.20",
          "F3": "0.21",
          "F4": "-0.71"
        },
        {
          "Item": "have stomach troubles",
          "Component": "Physiology",
          "F1": "0.18",
          "F2": "0.12",
          "F3": "0.23",
          "F4": "-0.69"
        },
        {
          "Item": "show tears",
          "Component": "Expression",
          "F1": "0.10",
          "F2": "0.15",
          "F3": "0.18",
          "F4": "-0.68"
        },
        {
          "Item": "Guilt",
          "Component": "",
          "F1": "0.32",
          "F2": "0.02",
          "F3": "0.15",
          "F4": "-0.63"
        },
        {
          "Item": "sweat",
          "Component": "Physiology",
          "F1": "0.06",
          "F2": "0.22",
          "F3": "0.30",
          "F4": "-0.60"
        },
        {
          "Item": "produce abrupt body movements",
          "Component": "Expression",
          "F1": "0.19",
          "F2": "0.12",
          "F3": "0.35",
          "F4": "-0.60"
        },
        {
          "Item": "feel your breathing is slowing down",
          "Component": "Physiology",
          "F1": "0.09",
          "F2": "0.24",
          "F3": "0.24",
          "F4": "-0.58"
        }
      ],
      "page": 35
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "want to damage, hit or say s.th. that hurts": "Ashamed",
          "Motivation": "",
          "0.25": "0.34",
          "0.08": "0.11",
          "0.24": "0.11",
          "-0.57": "-0.56"
        },
        {
          "want to damage, hit or say s.th. that hurts": "close your eyes",
          "Motivation": "Expression",
          "0.25": "0.20",
          "0.08": "0.07",
          "0.24": "0.34",
          "-0.57": "-0.54"
        },
        {
          "want to damage, hit or say s.th. that hurts": "Anger",
          "Motivation": "",
          "0.25": "0.49",
          "0.08": "-0.22",
          "0.24": "0.13",
          "-0.57": "-0.53"
        },
        {
          "want to damage, hit or say s.th. that hurts": "want to destroy s.th.",
          "Motivation": "Motivation",
          "0.25": "0.25",
          "0.08": "0.07",
          "0.24": "0.28",
          "-0.57": "-0.52"
        },
        {
          "want to damage, hit or say s.th. that hurts": "Contemptuous",
          "Motivation": "",
          "0.25": "0.49",
          "0.08": "-0.09",
          "0.24": "0.09",
          "-0.57": "-0.52"
        },
        {
          "want to damage, hit or say s.th. that hurts": "have a feeling of a lump in the throat",
          "Motivation": "Physiology",
          "0.25": "0.29",
          "0.08": "0.07",
          "0.24": "0.32",
          "-0.57": "-0.50"
        },
        {
          "want to damage, hit or say s.th. that hurts": "have the jaw drop",
          "Motivation": "Expression",
          "0.25": "0.26",
          "0.08": "0.17",
          "0.24": "0.34",
          "-0.57": "-0.47"
        },
        {
          "want to damage, hit or say s.th. that hurts": "Sad",
          "Motivation": "",
          "0.25": "0.47",
          "0.08": "-0.20",
          "0.24": "0.11",
          "-0.57": "-0.47"
        },
        {
          "want to damage, hit or say s.th. that hurts": "lack motivation to pay attention to the \nscene",
          "Motivation": "Motivation",
          "0.25": "0.11",
          "0.08": "0.09",
          "0.24": "0.06",
          "-0.57": "-0.42"
        },
        {
          "want to damage, hit or say s.th. that hurts": "experience an emotional state for a long \ntime",
          "Motivation": "Feeling",
          "0.25": "0.36",
          "0.08": "0.21",
          "0.24": "0.36",
          "-0.57": "-0.37"
        },
        {
          "want to damage, hit or say s.th. that hurts": "think the event was caused by chance",
          "Motivation": "Appraisal",
          "0.25": "0.13",
          "0.08": "0.25",
          "0.24": "0.19",
          "-0.57": "-0.29"
        }
      ],
      "page": 36
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Affective computing",
      "authors": [
        "R Picard"
      ],
      "year": "2000",
      "venue": "Affective computing"
    },
    {
      "citation_id": "2",
      "title": "On the Importance of Both Dimensional and Discrete Models of Emotion",
      "authors": [
        "E Harmon-Jones",
        "C Harmon-Jones",
        "E Summerell"
      ],
      "year": "2017",
      "venue": "Behav Sci (Basel)"
    },
    {
      "citation_id": "3",
      "title": "Dimensional, basic emotion, and componential approaches to meaning in psychological emotion research1",
      "authors": [
        "J Fontaine"
      ],
      "year": "2013",
      "venue": "Components of Emotional MeaningOxford"
    },
    {
      "citation_id": "4",
      "title": "Profiles of appraisal, motivation, and coping for positive emotions",
      "authors": [
        "J Yih",
        "L Kirby",
        "C Smith"
      ],
      "year": "2020",
      "venue": "Cognition and Emotion"
    },
    {
      "citation_id": "5",
      "title": "A cognitive architecture for modeling emotion dynamics: Intensity estimation from physiological signals",
      "authors": [
        "R Jenke",
        "A Peer"
      ],
      "year": "2018",
      "venue": "A cognitive architecture for modeling emotion dynamics: Intensity estimation from physiological signals"
    },
    {
      "citation_id": "6",
      "title": "The dynamic architecture of emotion: Evidence for the component process model",
      "authors": [
        "K Scherer"
      ],
      "year": "2009",
      "venue": "Cognition and Emotion"
    },
    {
      "citation_id": "7",
      "title": "Neural representations of emotion are organized around abstract event features",
      "authors": [
        "A Skerry",
        "R Saxe"
      ],
      "year": "2015",
      "venue": "Neural representations of emotion are organized around abstract event features"
    },
    {
      "citation_id": "8",
      "title": "Are emotions natural kinds?",
      "authors": [
        "L Barrett"
      ],
      "year": "2006",
      "venue": "Are emotions natural kinds?"
    },
    {
      "citation_id": "9",
      "title": "How emotions are made: The secret life of the brain",
      "authors": [
        "L Barrett"
      ],
      "year": "2017",
      "venue": "How emotions are made: The secret life of the brain"
    },
    {
      "citation_id": "10",
      "title": "A Bayesian Model of Category-Specific Emotional Brain Responses",
      "authors": [
        "T Wager",
        "J Kang",
        "T Johnson",
        "T Nichols",
        "A Satpute",
        "L Barrett"
      ],
      "year": "2015",
      "venue": "PLOS Computational Biology"
    },
    {
      "citation_id": "11",
      "title": "The brain basis of emotion: a meta-analytic review",
      "authors": [
        "K Lindquist",
        "T Wager",
        "H Kober",
        "E Bliss-Moreau",
        "L Barrett"
      ],
      "year": "2012",
      "venue": "The Behavioral and brain sciences"
    },
    {
      "citation_id": "12",
      "title": "DEAP: A Database for Emotion Analysis ;Using Physiological Signals",
      "authors": [
        "S Koelstra"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "13",
      "title": "AMIGOS: A Dataset for Affect, Personality and Mood Research on Individuals and Groups",
      "authors": [
        "J Correa",
        "M Abadi",
        "N Sebe",
        "I Patras"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "14",
      "title": "International Affective Picture System (IAPS) : Technical Manual and Affective Ratings",
      "authors": [
        "P Lang"
      ],
      "year": "1995",
      "venue": "International Affective Picture System (IAPS) : Technical Manual and Affective Ratings"
    },
    {
      "citation_id": "15",
      "title": "The Geneva affective picture database (GAPED): a new 730-picture database focusing on valence and normative significance",
      "authors": [
        "E Dan-Glauser",
        "K Scherer"
      ],
      "year": "2011",
      "venue": "Behavior Research Methods"
    },
    {
      "citation_id": "16",
      "title": "Affective auditory stimulus database: An expanded version of the International Affective Digitized Sounds (IADS-E)",
      "authors": [
        "W Yang"
      ],
      "year": "2018",
      "venue": "Behavior Research Methods"
    },
    {
      "citation_id": "17",
      "title": "Emotions elicited by indoor sound sources in wooden residential buildings",
      "authors": [
        "F Alessia",
        "J Pyoung",
        "Lee"
      ],
      "year": "2020",
      "venue": "Emotions elicited by indoor sound sources in wooden residential buildings"
    },
    {
      "citation_id": "18",
      "title": "A Multi-Componential Approach to Emotion Recognition and the Effect of Personality",
      "authors": [
        "G Mohammadi",
        "P Vuilleumier"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "19",
      "title": "Assessing the effectiveness of a large database of emotion-eliciting films: A new tool for emotion researchers",
      "authors": [
        "A Schaefer",
        "F Nils",
        "X Sanchez",
        "P Philippot"
      ],
      "year": "2010",
      "venue": "Cognition and Emotion"
    },
    {
      "citation_id": "20",
      "title": "A Review, Current Challenges, and Future Possibilities on Emotion Recognition Using Machine Learning and Physiological Signals",
      "authors": [
        "P Bota",
        "C Wang",
        "A Fred",
        "H Silva"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "21",
      "title": "Eliciting positive, negative and mixed emotional states: A film library for affective scientists",
      "authors": [
        "A Samson",
        "S Kreibig",
        "B Soderstrom",
        "A Wade",
        "J Gross"
      ],
      "year": "2016",
      "venue": "Cogn Emot"
    },
    {
      "citation_id": "22",
      "title": "ASCERTAIN: Emotion and Personality Recognition Using Commercial Sensors",
      "authors": [
        "R Subramanian",
        "J Wache",
        "M Abadi",
        "R Vieriu",
        "S Winkler",
        "N Sebe"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "23",
      "title": "DREAMER: A Database for Emotion Recognition Through EEG and ECG Signals From Wireless Low-cost Off-the-Shelf Devices",
      "authors": [
        "S Katsigiannis",
        "N Ramzan"
      ],
      "year": "2018",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "24",
      "title": "Virtual Reality for Emotion Elicitation -A Review",
      "authors": [
        "R Somarathna",
        "T Bednarz",
        "G Mohammadi"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "25",
      "title": "Constants across cultures in the face and emotion",
      "authors": [
        "P Ekman",
        "W Friesen",
        "S Psychology"
      ],
      "year": "1971",
      "venue": "Constants across cultures in the face and emotion"
    },
    {
      "citation_id": "26",
      "title": "The Emotions",
      "authors": [
        "N Frijda",
        "N Fridja",
        "A Manstead",
        "K Oatley"
      ],
      "year": "1986",
      "venue": "The Emotions"
    },
    {
      "citation_id": "27",
      "title": "Basic Emotions, Natural Kinds, Emotion Schemas, and a New Paradigm",
      "authors": [
        "C Izard"
      ],
      "year": "2007",
      "venue": "Basic Emotions, Natural Kinds, Emotion Schemas, and a New Paradigm"
    },
    {
      "citation_id": "28",
      "title": "The structure of self-reports of emotional responses to film segments",
      "authors": [
        "G Mchugo",
        "C Smith",
        "J Lanzetta"
      ],
      "year": "1982",
      "venue": "Motivation and Emotion"
    },
    {
      "citation_id": "29",
      "title": "Stability of emotion experiences and their relations to traits of personality",
      "authors": [
        "C Izard",
        "D Libero",
        "P Putnam",
        "O Haynes",
        "S Psychology"
      ],
      "year": "1993",
      "venue": "Stability of emotion experiences and their relations to traits of personality"
    },
    {
      "citation_id": "30",
      "title": "The GRID meets the Wheel: Assessing emotional feeling via self-report1",
      "authors": [
        "K Scherer",
        "V Shuman",
        "J Fontaine",
        "C Soriano"
      ],
      "year": "2013",
      "venue": "Components of Emotional MeaningOxford"
    },
    {
      "citation_id": "31",
      "title": "Multivariate pattern classification reveals autonomic and experiential representations of discrete emotions",
      "authors": [
        "P Kragel",
        "K Labar"
      ],
      "year": "2013",
      "venue": "Multivariate pattern classification reveals autonomic and experiential representations of discrete emotions"
    },
    {
      "citation_id": "32",
      "title": "Brain networks subserving functional core processes of emotions identified with componential modelling",
      "authors": [
        "G Mohammadi",
        "D Van De Ville",
        "P Vuilleumier"
      ],
      "year": "2023",
      "venue": "Cerebral Cortex"
    },
    {
      "citation_id": "33",
      "title": "Functional grouping and cortical-subcortical interactions in emotion: a meta-analysis of neuroimaging studies",
      "authors": [
        "H Kober",
        "L Barrett",
        "J Joseph",
        "E Bliss-Moreau",
        "K Lindquist",
        "T Wager"
      ],
      "year": "2008",
      "venue": "Functional grouping and cortical-subcortical interactions in emotion: a meta-analysis of neuroimaging studies"
    },
    {
      "citation_id": "34",
      "title": "Computational imaging during video game playing shows dynamic synchronization of cortical and subcortical networks of emotions",
      "authors": [
        "J Leit√£o",
        "B Meuleman",
        "D Van De Ville",
        "P Vuilleumier"
      ],
      "year": "2020",
      "venue": "Computational imaging during video game playing shows dynamic synchronization of cortical and subcortical networks of emotions"
    },
    {
      "citation_id": "35",
      "title": "Autonomic nervous system activity in emotion: A review",
      "authors": [
        "S Kreibig"
      ],
      "year": "2010",
      "venue": "Biological Psychology"
    },
    {
      "citation_id": "36",
      "title": "LIRIS-ACCEDE: A video database for affective content analysis",
      "authors": [
        "Y Baveye",
        "E Dellandrea",
        "C Chamaret",
        "L Chen"
      ],
      "year": "2015",
      "venue": "LIRIS-ACCEDE: A video database for affective content analysis"
    },
    {
      "citation_id": "37",
      "title": "Ratings for emotion film clips",
      "authors": [
        "C Gabert-Quillen",
        "E Bartolini",
        "B Abravanel",
        "C Sanislow"
      ],
      "year": "2015",
      "venue": "Ratings for emotion film clips"
    },
    {
      "citation_id": "38",
      "title": "DECAF: MEG-based multimodal database for decoding affective physiological responses",
      "authors": [
        "M Abadi",
        "R Subramanian",
        "S Kia",
        "P Avesani",
        "I Patras",
        "N Sebe"
      ],
      "year": "2015",
      "venue": "DECAF: MEG-based multimodal database for decoding affective physiological responses"
    },
    {
      "citation_id": "39",
      "title": "Emotion elicitation using films",
      "authors": [
        "J Gross",
        "R Levenson"
      ],
      "year": "1995",
      "venue": "Cognition and Emotion"
    },
    {
      "citation_id": "40",
      "title": "A Multimodal Database for Affect Recognition and Implicit Tagging",
      "authors": [
        "M Soleymani",
        "J Lichtenauer",
        "T Pun",
        "M Pantic"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "41",
      "title": "Same situation--Different emotions: How appraisals shape our emotions",
      "authors": [
        "M Siemer",
        "I Mauss",
        "J Gross"
      ],
      "year": "2007",
      "venue": "Emotion"
    },
    {
      "citation_id": "42",
      "title": "Towards Understanding Emotional Experience in a Componential Framework",
      "authors": [
        "G Mohammadi",
        "K Lin",
        "P Vuilleumier"
      ],
      "year": "2019",
      "venue": "2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "43",
      "title": "Assessing the Component Process Model of Emotion using multivariate pattern classification analyses",
      "authors": [
        "M Men√©trey"
      ],
      "year": "2019",
      "venue": "Interdisciplinary Master of Neuroscience, Faculty of Psychology and Educational Sciences"
    },
    {
      "citation_id": "44",
      "title": "Affective ranking of movie scenes using physiological signals and content analysis",
      "authors": [
        "M Soleymani",
        "G Chanel",
        "J Kierkels",
        "T Pun"
      ],
      "year": "2008",
      "venue": "Proceedings of the 2nd ACM Workshop on Multimedia Semantics"
    },
    {
      "citation_id": "45",
      "title": "The structure of self-reports of emotional responses to film segments",
      "authors": [
        "G Mchugo",
        "C Smith",
        "J Lanzetta"
      ],
      "year": "1982",
      "venue": "The structure of self-reports of emotional responses to film segments"
    },
    {
      "citation_id": "46",
      "title": "Measuring personality in one minute or less: A 10-item short version of the Big Five Inventory in English and German",
      "authors": [
        "B Rammstedt",
        "O John"
      ],
      "year": "2007",
      "venue": "Journal of Research in Personality"
    },
    {
      "citation_id": "47",
      "title": "Multi-Componential Analysis of Emotions Using Virtual Reality",
      "authors": [
        "R Somarathna",
        "T Bednarz",
        "G Mohammadi"
      ],
      "year": "2021",
      "venue": "Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology"
    },
    {
      "citation_id": "48",
      "title": "Profiles of emotion regulation: Understanding regulatory patterns and the implications for posttraumatic stress",
      "authors": [
        "S Chesney"
      ],
      "year": "2017",
      "venue": "Profiles of emotion regulation: Understanding regulatory patterns and the implications for posttraumatic stress"
    },
    {
      "citation_id": "49",
      "title": "Nonlinear Appraisal Modeling: An Application of Machine Learning to the Study of Emotion Production",
      "authors": [
        "B Meuleman",
        "K Scherer"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "50",
      "title": "An Exploratory Analysis of Interactive VR-Based Framework for Multi-Componential Analysis of Emotion",
      "authors": [
        "R Somarathna",
        "T Bednarz",
        "G Mohammadi"
      ],
      "year": "2022",
      "venue": "2022 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops)"
    },
    {
      "citation_id": "51",
      "title": "Emotion theory and research: highlights, unanswered questions, and emerging issues",
      "authors": [
        "C Izard"
      ],
      "year": "2009",
      "venue": "Annual review of psychology"
    },
    {
      "citation_id": "52",
      "title": "The world of emotions is not two-dimensional",
      "authors": [
        "J Fontaine",
        "K Scherer",
        "E Roesch",
        "P Ellsworth"
      ],
      "year": "2007",
      "venue": "Psychol Sci"
    },
    {
      "citation_id": "53",
      "title": "Human amygdala tracks a feature-based valence signal embedded within the facial expression of surprise",
      "authors": [
        "M Kim",
        "A Mattek",
        "R Bennett",
        "K Solomon",
        "J Shin",
        "P Whalen"
      ],
      "year": "2017",
      "venue": "Human amygdala tracks a feature-based valence signal embedded within the facial expression of surprise"
    },
    {
      "citation_id": "54",
      "title": "Unmasking the face: A guide to recognizing emotions from facial clues",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "2003",
      "venue": "Unmasking the face: A guide to recognizing emotions from facial clues"
    },
    {
      "citation_id": "55",
      "title": "Dimensions and Clusters of Aesthetic Emotions: A Semantic Profile Analysis",
      "authors": [
        "U Beermann"
      ],
      "year": "1949",
      "venue": "Original Research"
    },
    {
      "citation_id": "56",
      "title": "What are emotions? And how can they be measured?",
      "authors": [
        "K Scherer"
      ],
      "year": "2005",
      "venue": "Social Science Information"
    },
    {
      "citation_id": "57",
      "title": "Affective computing in virtual reality: emotion recognition from brain and heartbeat dynamics using wearable sensors",
      "authors": [
        "J Mar√≠n-Morales"
      ],
      "year": "2018",
      "venue": "Scientific Reports"
    },
    {
      "citation_id": "58",
      "title": "Affect Elicitation for Affective Computing",
      "authors": [
        "J Kory",
        "S D'mello"
      ],
      "year": "2015",
      "venue": "Affect Elicitation for Affective Computing"
    },
    {
      "citation_id": "59",
      "title": "Induction and profiling of strong multi-componential emotions in virtual reality",
      "authors": [
        "B Meuleman",
        "D Rudrauf"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "60",
      "title": "The Effect of Immersion on Emotional Responses to Film Viewing in a Virtual Environment",
      "authors": [
        "A Kim",
        "M Chang",
        "Y Choi",
        "S Jeon",
        "K Lee"
      ],
      "year": "2018",
      "venue": "The Effect of Immersion on Emotional Responses to Film Viewing in a Virtual Environment"
    },
    {
      "citation_id": "61",
      "title": "A Public Database of Immersive VR Videos with Corresponding Ratings of Arousal, Valence, and Correlations between Head Movements and Self Report Measures",
      "authors": [
        "B Li",
        "J Bailenson",
        "A Pines",
        "W Greenleaf",
        "L Williams"
      ],
      "year": "2017",
      "venue": "A Public Database of Immersive VR Videos with Corresponding Ratings of Arousal, Valence, and Correlations between Head Movements and Self Report Measures"
    },
    {
      "citation_id": "62",
      "title": "A New Terrain in HCI: Emotion Recognition Interface using Biometric Data for an Immersive VR Experience",
      "authors": [
        "J Nam",
        "H Chung",
        "Y Seong",
        "H Lee"
      ],
      "year": "2019",
      "venue": "A New Terrain in HCI: Emotion Recognition Interface using Biometric Data for an Immersive VR Experience"
    },
    {
      "citation_id": "63",
      "title": "ARBEE: Towards automated recognition of bodily expression of emotion in the wild",
      "authors": [
        "Y Luo",
        "J Ye",
        "R Adams",
        "J Li",
        "M Newman",
        "J Wang"
      ],
      "year": "2020",
      "venue": "ARBEE: Towards automated recognition of bodily expression of emotion in the wild"
    },
    {
      "citation_id": "64",
      "title": "Development and validation of Image Stimuli for Emotion Elicitation (ISEE): A novel affective pictorial system with test-retest repeatability",
      "authors": [
        "H Kim"
      ],
      "venue": "Psychiatry Research"
    }
  ]
}