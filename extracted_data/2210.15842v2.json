{
  "paper_id": "2210.15842v2",
  "title": "Leveraging Label Correlations In A Multi-Label Setting: A Case Study In Emotion",
  "published": "2022-10-28T02:27:18Z",
  "authors": [
    "Georgios Chochlakis",
    "Gireesh Mahajan",
    "Sabyasachee Baruah",
    "Keith Burghardt",
    "Kristina Lerman",
    "Shrikanth Narayanan"
  ],
  "keywords": [
    "Emotion",
    "Label Correlations",
    "SemEval"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Detecting emotions expressed in text has become critical to a range of fields. In this work, we investigate ways to exploit label correlations in multi-label emotion recognition models to improve emotion detection. First, we develop two modeling approaches to the problem in order to capture word associations of the emotion words themselves, by either including the emotions in the input, or by leveraging Masked Language Modeling (MLM). Second, we integrate pairwise constraints of emotion representations as regularization terms alongside the classification loss of the models. We split these terms into two categories, local and global. The former dynamically change based on the gold labels, while the latter remain static during training. We demonstrate state-of-the-art performance across Spanish, English, and Arabic in SemEval 2018 Task 1 E-c using monolingual BERT-based models. On top of better performance, we also demonstrate improved robustness. Code is available at https://github.com/gchochla/Demux-MEmo. 1",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotions are fundamental to human experience. They shape what people pay attention to and how they consume information, what they believe, and how they interact with others  [23, 7, 25] . Recent advances in deep learning have enabled extraction of emotion signals from language  [4, 1, 28, 2] , thereby facilitating emotion recognition from text at scale  [11] . Despite these successes, the need for more accurate, robust and fair emotion recognition models remains.\n\nWe focus on inferring categorical emotions from text, like those in Plutchik's wheel of emotions  [18] , a theory of emotion that portrays discrete emotions on a 2D space. Early textual emotion recognition models have used hand-crafted features and emotion lexicons  [21, 22, 17] . Modern deep learning models achieve better performance  [1, 27, 2]  but have to contend with the sampling bias of the data, and the subjective nature of the annotations during training.\n\nFurthermore, we concentrate on the more general setting of multi-label emotion recognition, meaning none, one, or multiple emotions can be present per example. This multi-label setting is more realistic and computationally intricate because the model's predictions correlate with each other.\n\nWe study how we can integrate these relationships in Transformerbased  [24]  models beyond the regular training. First, we consider two different modeling approaches to leverage word-level associations in the language model (LM). In the first one, we leverage 1 Funded in part by DARPA under contract HR001121C0168 Fig.  1 . We induce label-correlation awareness by pulling together or pushing apart representations of pairs of emotions. This can be achieved at the level of intermediate representations h, and at the level of predictions ŷ. Prediction pairs can be regularized using the labels y, by pulling emotions with the same gold labels together, otherwise pushing them apart. Representation pairs can be regularized in the same way, but we can also use prior relationships between them, which can conflict with current labels. Each pair's regularization term can be modulated by the strength of the relationship of the pair.\n\nglobal attention and multiple input sequences. Borrowing from  [1] , we directly use the emotions in the input by prepending them to the input text. Thereafter, their contextual embeddings are used for the final classification. In this manner, word-level associations between the emotions arise due to the attention mechanism. We refer to this modeling approach as Demux (short for \"Demultiplexer\"), because the emotions, provided in the input, can be thought of as selectors of features from the input text sequence we want to classify.\n\nThe second formulation is based on the Masked Language Modeling (MLM) pretraining task. We create a more natural prompt, a declarative statement about the emotional content of the input sequence, but with the actual emotion(s) replaced by a [MASK] token.\n\n[MASK]'s contextual embedding is then fed to the pretrained MLM prediction head or a new classifier to perform emotion recognition. Word-level associations are captured by [MASK] and the MLM head. We refer to this approach as MEmo (Masked EMOtion).\n\nThe second important question we pose is whether the classifica-tion loss is adequate to instill in the models the relationships between emotions (Figure  1 ). We do so by integrating several regularization losses into our aforementioned models. We split these into two main categories, global and local terms. The former consist of static \"contraints\" based on prior relationships between emotions. The latter are dynamic, as they take into consideration the gold labels of each example. For both, we limit our experiments to pairs of emotions. Since different pairs of emotions relate more or less strongly, we also optionally apply a weighting scheme per pair. We show roughly equivalent performance from Demux and MEmo. We also observe a trend for not pushing emotion representations apart performing favorably. We improve the state of the art for SemEval 2018 Task 1 E-c  [16] , as well as the robustness of performance across experiments and settings, further emphasizing the utility of our approach. Our contributions can be summarized as:\n\n• We demonstrate state-of-the-art performance for SemEval 2018\n\nTask 1 E-c, a widely used, realistic benchmark, using monolingual BERT-based models.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Earlier works relied on emotion lexicons and Bag-of-Words (BoW) algorithms. The General Inquirer  [21] , one of the earliest efforts at automated affective analyses on text, is a general-purpose lexicon with some affect-related quantities. More modern efforts include LIWC  [17] , which is widely used, e.g. in Computational Social Science  [15] . For more sophisticated BoW methods, DDR  [10]  extends lexicon-based methods from counting to word similarities. The most recent and successful methods deploy deep learning. Initially, single-label predictions were transformed into a multi-label output with a probability threshold  [12] . LSTMs  [13]  have been widely used for the task, e.g., the usage of DeepMoji features  [9]  by SeerNet  [8] , and NTUA-SLP  [2]  for SemEval 2018 Task 1  [16] . It is interesting that these methods also leveraged features from affective lexicons. Attention and transfer learning is also studied in  [28] . More recently, Transformers  [24]  have dominated the field. They have been coupled with Convolutional Neural Networks  [27]  or Graph Neural Networks  [26] . SpanEmo  [1]  prompts a BERT model by enumerating all emotions in its input and uses a Label-correlation aware (LCA) loss to facilitate emotion interplay.\n\nIn addition, language prompting has garnered attention because it allows language transformers to perform different tasks under the same framework, as well as zero and few-shot inference  [3, 14, 19] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Methodology",
      "text": "Label correlations can be considered at the local and the global level. For global supervision, we rely on prior information about the label relations, such as an empirical covariance matrix, or theoreticallyinspired e.g., Plutchik's wheel of emotions  [18] . We can leverage those to align intermediate representations h of a model. On the local level, however, annotations of specific examples y can indicate conflicting relationships to our priors, and predictions of the network ŷ can also be guided. In terms of modeling, we leverage the relationships between emotion words to facilitate label correlations, either by including them in the input or by using a [MASK] token.\n\nGiven n emotions, let E = {ei : i ∈ [n]} be the set of emotions and c : [n] 2 → [0, 1] a measure of correlation between them, like the correlation of the train set, ρ, or the cosine in Plutchik's wheel  [18] , θ, both projected to [0, 1] from [-1, 1]. Let L denote loss functions.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Correlation-Aware Regularization",
      "text": "In an effort to enhance the model's correlation awareness between emotions, we introduce additional terms to the classification loss. They can be categorized into two groups, global and local.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Global",
      "text": "Global supervision refers to any loss that constrains the distance of pairs of emotions based on prior relationships between them, and is, therefore, fixed during training and applied to intermediate emotion representations. Given representations per emotion h:\n\n(1)",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Local",
      "text": "Local supervision takes into account the gold labels for each example in the formulation of the loss. Therefore, the emotions are split into two groups, the present and the absent emotions in y, P and N respectively. We consider three types of local losses. First, those that dictate inter-group relationships between P and N , where we try to increase the distance of representations of pairs of emotions. The hypothesis here is that the presence of one emotion informs us what emotions are likely to be absent, and vice versa. Then, we have those that dictate intra-group relationships within P and within N , where we try to decrease the distance of each pair. The assumption here is that a present emotion informs us what other emotions are likely to be present, and similarly for absent emotions. Finally, we have their combination, where we simultaneously push apart inter-group pairs and pull together intra-group pairs. Our formulation is:\n\nwhere S quantifies \"similarity\" and D \"distance\" between some representations of the network r, and f , f are weights for each pair of emotions. S and D do not have to strictly adhere to similarity/distance properties, but decreasing S should force the emotion representations to diverge, while D should pull them closer. The denominators average the terms to keep the magnitude of the different losses similar. Borrowing from the LCA loss in  [1] , when we set f = f = 1, r as the output probabilities of the model ŷ and S(ŷi, ŷj) = e ŷi -ŷ j , D(ŷi, ŷj) = e ŷi +ŷ j ,\n\nwe retrieve LL,inter = LLCA. Note that this formulation of D requires the negative signs in LL,intra because present emotions would be pushed to 0 otherwise. Another alternative is setting r as some intermediate representations h and S(hi, hj) = cossim(hi, hj), D(hi, hj) = -cossim(hi, hj).\n\n(4) Note that for this formulation D(-ri, -rj) = D(ri, rj).\n\nWe further augment our local losses by making f and f functions of c. We pick f to be decreasing, and f to be increasing, so that each pair is weighted proportionately to the probability it would end up in that group state. For example, the weight of a frequently co-occurring pair becomes small when they appear with different gold labels in LL,inter. We fix fi,j = 1 -ci,j and f i,j = ci,j.\n\nThe final loss we use is",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Demux",
      "text": "Let x be an input text sequence. Inspired by  [1] , we use emotions E to construct x = \"e1, e2, . . . , or en?\", and use tokenizer T :\n\nwhere xi are the tokens from x, ti,j is the j-th subtoken of ei, and T typically will combine the given sequences with [SEP], and prepend [CLS] to the entire sequence. We pass x through LM L to finally get x = L(x), where x contains one output embedding corresponding to each input token. We denote the output embedding corresponding to ti,j as ti,j ∈ R d , where d is the feature dimension of L. Thereafter, we aggregate the outputs of the subtokens corresponding to each class by averaging them, and get the final probabilities p(ei|x) using a 2-layer neural network, NN : R d → R, followed by the sigmoid function σ:\n\n∀i ∈ [n], p(ei|x) = σ(NN(",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Memo",
      "text": "Let x be an input text sequence. We prepend \"emotion [MASK] in tweet \" to x to create our input, x. We then use L with its tokenizer T to get the output x = L(T (x)), where x contains one output embedding corresponding to each input token. From x, we extract the embedding that corresponds to [MASK], hM . We can use two methods to get the final predictions, the pretrained MLM head, or build a new classifier. When using the MLM head, for each emotion, we use its corresponding logit and apply the sigmoid. For the latter, to get the probability p(ei|x), we pass hM through a 2-layer neural network, NN : R d → R |E| and then apply the sigmoid\n\nWe find there is no meaningful way to implement hidden representations per emotion for this model, hence losses that depend on the cosine similarity are not integrated.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Datasets",
      "text": "We use the publicly available SemEval 2018 Task 1 E-c  [16] , which contains tweets annotated for 11 emotions in a multi-label setting, namely anger, anticipation, disgust, fear, joy, love, optimism, pessimism, sadness, surprise, and trust, in three languages, English,",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Ll",
      "text": "En SemEval 2018",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Implementation Details",
      "text": "We use Python (v3.7.4), PyTorch (v1.11.0) and the corresponding implementations of Transformer models from the Hugging Face transformers library (v4.19.2). We use up to three NVIDIA GeForce GTX 1080 Ti and one NVIDIA GeForceRTX 2070, but always one GPU per model. We use BERT for English  [6]  and Arabic  [20] , and BETO  [5]  for Spanish, as in  [1] . In terms of hyperparameters, we retain the hyperparameters used in  [1] , such as the learning rate and its schedule, batch size, and α (Eq. 5). However, for the text preprocessor, we fur- ther remove the special tags introduced by the preprocessing library, ekphrasis 2 . We found 0.1 and 0.5 to work equally well for β (Eq. 5), so we set β = 0.1 when we use the global loss. For early stopping, we reduce the patience to 5 epochs, and use JS instead as the evaluation metric. Moreover, instead of directly using the model after early stopping, we retrain it on both the train and dev sets, picking the number of epochs based on the performance distribution of the model across multiple runs. Test and dev performance are reported after 10 runs.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Mic",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Local And Global Relations",
      "text": "We run comprehensive experiments to determine the better alternative w.r.t. regularization losses and present our results in Table  1 . In general, for Demux, we see minimal changes in performance from configuration to configuration. As general trends, we observe that reinforcing intra-group relations works better compared to inter-group ones, or even their combination. This implies that creating opposing forces between groups of emotions can hurt performance, perhaps because the ability to express more nuance with multiple different emotions is hindered. The global regularization and our weighting scheme seem to have little to no effect, and the local regularization based on the cosine similarity seems to be too strict, noticeably degrading performance even compared to the baseline. For MEmo, we see better performance with the new classifier instead of the MLM head, thus we report the former's numbers. We observe reduced macro F1 but similar micro F1 and JS to Demux. We see clearer improvement by the regularization terms, suggesting this alternative does not capture word-level associations as well as Demux does, and similar trends with intra vs. inter-group terms.\n\n2 https://github.com/cbaziotis/ekphrasis",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "State-Of-The-Art",
      "text": "In this section, we compare only with the previous state of the art, SpanEmo  [1] , due to space limitations. Results can be seen in Table 2. Given that the original code has been open-sourced 3  , we also reproduce their results to derive averages and deviations.\n\nFirst, comparing  [1]  with Demux, we see a significant increase in performance no matter the latter's configuration. In fact, ranges never overlap and all our improvements are statistically significant based on t-tests with Bonferonni correction (p < 1.5 • 10 -3 ). Looking at micro-F1 and macro-F1, which the model was not explicitly tuned to perform well at, we see 3 -6% relative improvements for the former, and 6 -15% for the latter. For JS, relative improvements are around 5-14%. Note that, for consistency, we report test performance of \"intra ρ e y \" with β = 0 across all languages to facilitate reporting in future work. MEmo also outperforms the previous state of the art with statistically significant improvements (p < 4 • 10 -4 ) but for macro F1 in Arabic (p 0.112). Micro F1, macro F1 and JS improve by 3 -6%, 2 -10%, and 6 -16% over  [1]  respectively. Comparing our two models, we consistently see roughly equivalent micro F1, favorable JS from MEmo but a decrease in macro F1.\n\nCompared to the dev set, here we observe improved robustness of the model utilizing some local loss as opposed to the simple baseline, especially in English. This suggests that, while the \"potential\" (i.e., the dev set performance) of the different models is similar, generalization is aided by including local and global losses in the model's supervision. Moreover, as is evident from the standard deviations, our pipeline achieves more robust results in all configurations. Performance on individual emotions is shown in our repo.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we examine techniques to induce correlation-awareness in models of emotion recognition in a multi-label setting. We investigated a variety of configurations of global and local regularization losses constraining the behavior of pairs of emotions. We also experimented with architectures and formulations that leverage word associations between the emotion words either in the input, by explicitly including the emotions, or the output, using the MLM pretraining task of the model. We also demonstrate that better configuring the training regime can improve performance and robustness.\n\nWe achieve state-of-the-art results with monolingual BERTbased models, with improvements that are statistically significant, and indeed improve substantially over existing work. We also demonstrate that the examined regularization losses can improve the robustness of the resulting model compared to a model solely guided by a classification loss. Finally, we provide guidelines, and open-source our code, in an effort to guide other researchers when extending or repurposing our work.\n\nOur work also demonstrates that cosine-similarity-based losses cannot really improve the models. Future work should try and address these issues, as, intuitively, these losses would be expected to work favorably based on the fact that they explicitly model the relationship between the emotions, whereas the difference of the predictions can be optimized for each emotion separately, and interactions only arise because of the contextual embeddings in earlier layers. Moreover, weighting each term also did not help. Future work could investigate non-linear functions of c, general ways to deal with the change of magnitude in the regularization loss, and perhaps estimating the correlations by collating multiple sources, e.g., using a mixed-effects model.",
      "page_start": 4,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: We induce label-correlation awareness by pulling together",
      "page": 1
    },
    {
      "caption": "Figure 1: ). We do so by integrating several regularization",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ABSTRACT": "Detecting emotions expressed in text has become critical\nto a range"
        },
        {
          "ABSTRACT": "of ﬁelds.\nIn this work, we investigate ways to exploit\nlabel corre-"
        },
        {
          "ABSTRACT": "lations in multi-label emotion recognition models to improve emo-"
        },
        {
          "ABSTRACT": "tion detection.\nFirst, we develop two modeling approaches to the"
        },
        {
          "ABSTRACT": "problem in order to capture word associations of the emotion words"
        },
        {
          "ABSTRACT": "themselves, by either\nincluding the emotions\nin the input, or by"
        },
        {
          "ABSTRACT": "leveraging Masked Language Modeling (MLM). Second, we inte-"
        },
        {
          "ABSTRACT": "grate pairwise constraints of emotion representations as regulariza-"
        },
        {
          "ABSTRACT": "tion terms alongside the classiﬁcation loss of the models. We split"
        },
        {
          "ABSTRACT": "these terms into two categories,\nlocal and global. The former dy-"
        },
        {
          "ABSTRACT": "namically change based on the gold labels, while the latter remain"
        },
        {
          "ABSTRACT": "static during training. We demonstrate state-of-the-art performance"
        },
        {
          "ABSTRACT": "across Spanish, English, and Arabic in SemEval 2018 Task 1 E-c"
        },
        {
          "ABSTRACT": "using monolingual BERT-based models.\nOn top of better perfor-"
        },
        {
          "ABSTRACT": "mance, we also demonstrate improved robustness. Code is available"
        },
        {
          "ABSTRACT": "at https://github.com/gchochla/Demux-MEmo.1"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "Index Terms— Emotion, Label Correlations, SemEval"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "1.\nINTRODUCTION"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "Emotions are fundamental\nto human experience. They shape what"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "people pay attention to and how they consume information, what"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "they believe, and how they interact with others [23, 7, 25]. Recent"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "advances in deep learning have enabled extraction of emotion signals"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "from language [4, 1, 28, 2], thereby facilitating emotion recognition"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "from text at scale [11]. Despite these successes,\nthe need for more"
        },
        {
          "ABSTRACT": "accurate, robust and fair emotion recognition models remains."
        },
        {
          "ABSTRACT": "We focus on inferring categorical emotions from text, like those"
        },
        {
          "ABSTRACT": "in Plutchik’s wheel of emotions [18], a theory of emotion that por-"
        },
        {
          "ABSTRACT": "trays discrete emotions on a 2D space. Early textual emotion recog-"
        },
        {
          "ABSTRACT": "nition models have used hand-crafted features and emotion lexicons"
        },
        {
          "ABSTRACT": "[21, 22, 17]. Modern deep learning models achieve better perfor-"
        },
        {
          "ABSTRACT": "mance [1, 27, 2] but have to contend with the sampling bias of the"
        },
        {
          "ABSTRACT": "data, and the subjective nature of the annotations during training."
        },
        {
          "ABSTRACT": "Furthermore, we\nconcentrate on the more general\nsetting of"
        },
        {
          "ABSTRACT": "multi-label emotion recognition, meaning none, one, or multiple"
        },
        {
          "ABSTRACT": "emotions can be present per example.\nThis multi-label\nsetting is"
        },
        {
          "ABSTRACT": "more\nrealistic\nand computationally intricate because\nthe model’s"
        },
        {
          "ABSTRACT": "predictions correlate with each other."
        },
        {
          "ABSTRACT": "We study how we can integrate these relationships in Transformer-"
        },
        {
          "ABSTRACT": "based [24] models beyond the regular\ntraining.\nFirst, we consider"
        },
        {
          "ABSTRACT": "two different modeling approaches\nto leverage word-level associ-"
        },
        {
          "ABSTRACT": "ations\nin the language model\n(LM).\nIn the ﬁrst one, we leverage"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "1Funded in part by DARPA under contract HR001121C0168"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "tion loss is adequate to instill in the models the relationships between": "emotions (Figure 1). We do so by integrating several regularization",
          ": i ∈ [n]} be the set of emotions\nGiven n emotions, let E = {ei": "and c : [n]2 → [0, 1] a measure of correlation between them, like the"
        },
        {
          "tion loss is adequate to instill in the models the relationships between": "losses into our aforementioned models. We split these into two main",
          ": i ∈ [n]} be the set of emotions\nGiven n emotions, let E = {ei": "correlation of the train set, ρ, or the cosine in Plutchik’s wheel [18],"
        },
        {
          "tion loss is adequate to instill in the models the relationships between": "categories, global and local terms. The former consist of static “con-",
          ": i ∈ [n]} be the set of emotions\nGiven n emotions, let E = {ei": "θ, both projected to [0, 1] from [−1, 1]. Let L denote loss functions."
        },
        {
          "tion loss is adequate to instill in the models the relationships between": "traints” based on prior\nrelationships between emotions. The latter",
          ": i ∈ [n]} be the set of emotions\nGiven n emotions, let E = {ei": ""
        },
        {
          "tion loss is adequate to instill in the models the relationships between": "are dynamic, as they take into consideration the gold labels of each",
          ": i ∈ [n]} be the set of emotions\nGiven n emotions, let E = {ei": ""
        },
        {
          "tion loss is adequate to instill in the models the relationships between": "",
          ": i ∈ [n]} be the set of emotions\nGiven n emotions, let E = {ei": "3.1. Correlation-aware Regularization"
        },
        {
          "tion loss is adequate to instill in the models the relationships between": "example. For both, we limit our experiments to pairs of emotions.",
          ": i ∈ [n]} be the set of emotions\nGiven n emotions, let E = {ei": ""
        },
        {
          "tion loss is adequate to instill in the models the relationships between": "Since different pairs of emotions relate more or less strongly, we also",
          ": i ∈ [n]} be the set of emotions\nGiven n emotions, let E = {ei": "In an effort\nto enhance the model’s correlation awareness between"
        },
        {
          "tion loss is adequate to instill in the models the relationships between": "optionally apply a weighting scheme per pair.",
          ": i ∈ [n]} be the set of emotions\nGiven n emotions, let E = {ei": "emotions, we introduce additional\nterms to the classiﬁcation loss."
        },
        {
          "tion loss is adequate to instill in the models the relationships between": "We\nshow roughly equivalent performance\nfrom Demux and",
          ": i ∈ [n]} be the set of emotions\nGiven n emotions, let E = {ei": "They can be categorized into two groups, global and local."
        },
        {
          "tion loss is adequate to instill in the models the relationships between": "MEmo. We also observe a trend for not pushing emotion represen-",
          ": i ∈ [n]} be the set of emotions\nGiven n emotions, let E = {ei": ""
        },
        {
          "tion loss is adequate to instill in the models the relationships between": "",
          ": i ∈ [n]} be the set of emotions\nGiven n emotions, let E = {ei": "3.1.1. Global"
        },
        {
          "tion loss is adequate to instill in the models the relationships between": "tations apart performing favorably. We improve the state of the art",
          ": i ∈ [n]} be the set of emotions\nGiven n emotions, let E = {ei": ""
        },
        {
          "tion loss is adequate to instill in the models the relationships between": "for SemEval 2018 Task 1 E-c [16],\nas well as\nthe robustness of",
          ": i ∈ [n]} be the set of emotions\nGiven n emotions, let E = {ei": "Global supervision refers to any loss that constrains the distance of"
        },
        {
          "tion loss is adequate to instill in the models the relationships between": "performance across experiments and settings,\nfurther emphasizing",
          ": i ∈ [n]} be the set of emotions\nGiven n emotions, let E = {ei": "pairs of emotions based on prior relationships between them, and is,"
        },
        {
          "tion loss is adequate to instill in the models the relationships between": "the utility of our approach. Our contributions can be summarized as:",
          ": i ∈ [n]} be the set of emotions\nGiven n emotions, let E = {ei": "therefore, ﬁxed during training and applied to intermediate emotion"
        },
        {
          "tion loss is adequate to instill in the models the relationships between": "",
          ": i ∈ [n]} be the set of emotions\nGiven n emotions, let E = {ei": "representations. Given representations per emotion h:"
        },
        {
          "tion loss is adequate to instill in the models the relationships between": "• We demonstrate state-of-the-art performance for SemEval 2018",
          ": i ∈ [n]} be the set of emotions\nGiven n emotions, let E = {ei": ""
        },
        {
          "tion loss is adequate to instill in the models the relationships between": "Task 1 E-c, a widely used, realistic benchmark, using mono-",
          ": i ∈ [n]} be the set of emotions\nGiven n emotions, let E = {ei": "i(cid:54)=j"
        },
        {
          "tion loss is adequate to instill in the models the relationships between": "",
          ": i ∈ [n]} be the set of emotions\nGiven n emotions, let E = {ei": "1\n(cid:88)"
        },
        {
          "tion loss is adequate to instill in the models the relationships between": "lingual BERT-based models.",
          ": i ∈ [n]} be the set of emotions\nGiven n emotions, let E = {ei": ""
        },
        {
          "tion loss is adequate to instill in the models the relationships between": "",
          ": i ∈ [n]} be the set of emotions\nGiven n emotions, let E = {ei": "(1)\n(cossim(hi, hj) − ci,j)2\nLG(h; c) ="
        },
        {
          "tion loss is adequate to instill in the models the relationships between": "",
          ": i ∈ [n]} be the set of emotions\nGiven n emotions, let E = {ei": "n2 − n"
        },
        {
          "tion loss is adequate to instill in the models the relationships between": "• We\ninvestigate\ntwo different modeling approaches, MEmo,",
          ": i ∈ [n]} be the set of emotions\nGiven n emotions, let E = {ei": "(i,j)∈[n]2"
        },
        {
          "tion loss is adequate to instill in the models the relationships between": "based on MLM, and Demux, relying on label embeddings, to",
          ": i ∈ [n]} be the set of emotions\nGiven n emotions, let E = {ei": ""
        },
        {
          "tion loss is adequate to instill in the models the relationships between": "",
          ": i ∈ [n]} be the set of emotions\nGiven n emotions, let E = {ei": "3.1.2.\nLocal"
        },
        {
          "tion loss is adequate to instill in the models the relationships between": "integrate label correlations through word-level associations.",
          ": i ∈ [n]} be the set of emotions\nGiven n emotions, let E = {ei": ""
        },
        {
          "tion loss is adequate to instill in the models the relationships between": "• We study how regularization losses on pairs of emotions can",
          ": i ∈ [n]} be the set of emotions\nGiven n emotions, let E = {ei": "Local supervision takes into account the gold labels for each exam-"
        },
        {
          "tion loss is adequate to instill in the models the relationships between": "facilitate robustness and label-correlation awareness.",
          ": i ∈ [n]} be the set of emotions\nGiven n emotions, let E = {ei": "ple in the formulation of the loss. Therefore,\nthe emotions are split"
        },
        {
          "tion loss is adequate to instill in the models the relationships between": "",
          ": i ∈ [n]} be the set of emotions\nGiven n emotions, let E = {ei": "into two groups, the present and the absent emotions in y, P and N"
        },
        {
          "tion loss is adequate to instill in the models the relationships between": "",
          ": i ∈ [n]} be the set of emotions\nGiven n emotions, let E = {ei": "respectively. We consider three types of local losses. First, those that"
        },
        {
          "tion loss is adequate to instill in the models the relationships between": "2. RELATED WORK",
          ": i ∈ [n]} be the set of emotions\nGiven n emotions, let E = {ei": ""
        },
        {
          "tion loss is adequate to instill in the models the relationships between": "",
          ": i ∈ [n]} be the set of emotions\nGiven n emotions, let E = {ei": "dictate inter-group relationships between P and N , where we try to"
        },
        {
          "tion loss is adequate to instill in the models the relationships between": "",
          ": i ∈ [n]} be the set of emotions\nGiven n emotions, let E = {ei": "increase the distance of\nrepresentations of pairs of emotions.\nThe"
        },
        {
          "tion loss is adequate to instill in the models the relationships between": "Earlier works relied on emotion lexicons and Bag-of-Words (BoW)",
          ": i ∈ [n]} be the set of emotions\nGiven n emotions, let E = {ei": ""
        },
        {
          "tion loss is adequate to instill in the models the relationships between": "",
          ": i ∈ [n]} be the set of emotions\nGiven n emotions, let E = {ei": "hypothesis here is that the presence of one emotion informs us what"
        },
        {
          "tion loss is adequate to instill in the models the relationships between": "algorithms. The General Inquirer [21], one of the earliest efforts at",
          ": i ∈ [n]} be the set of emotions\nGiven n emotions, let E = {ei": ""
        },
        {
          "tion loss is adequate to instill in the models the relationships between": "",
          ": i ∈ [n]} be the set of emotions\nGiven n emotions, let E = {ei": "emotions are likely to be absent, and vice versa. Then, we have those"
        },
        {
          "tion loss is adequate to instill in the models the relationships between": "automated affective analyses on text,\nis a general-purpose lexicon",
          ": i ∈ [n]} be the set of emotions\nGiven n emotions, let E = {ei": ""
        },
        {
          "tion loss is adequate to instill in the models the relationships between": "",
          ": i ∈ [n]} be the set of emotions\nGiven n emotions, let E = {ei": "that dictate intra-group relationships within P and within N , where"
        },
        {
          "tion loss is adequate to instill in the models the relationships between": "with some affect-related quantities. More modern efforts\ninclude",
          ": i ∈ [n]} be the set of emotions\nGiven n emotions, let E = {ei": ""
        },
        {
          "tion loss is adequate to instill in the models the relationships between": "",
          ": i ∈ [n]} be the set of emotions\nGiven n emotions, let E = {ei": "we try to decrease the distance of each pair. The assumption here is"
        },
        {
          "tion loss is adequate to instill in the models the relationships between": "LIWC [17], which is widely used,\ne.g.\nin Computational Social",
          ": i ∈ [n]} be the set of emotions\nGiven n emotions, let E = {ei": ""
        },
        {
          "tion loss is adequate to instill in the models the relationships between": "",
          ": i ∈ [n]} be the set of emotions\nGiven n emotions, let E = {ei": "that a present emotion informs us what other emotions are likely to"
        },
        {
          "tion loss is adequate to instill in the models the relationships between": "Science [15]. For more sophisticated BoW methods, DDR [10] ex-",
          ": i ∈ [n]} be the set of emotions\nGiven n emotions, let E = {ei": ""
        },
        {
          "tion loss is adequate to instill in the models the relationships between": "",
          ": i ∈ [n]} be the set of emotions\nGiven n emotions, let E = {ei": "be present, and similarly for absent emotions. Finally, we have their"
        },
        {
          "tion loss is adequate to instill in the models the relationships between": "tends lexicon-based methods from counting to word similarities. The",
          ": i ∈ [n]} be the set of emotions\nGiven n emotions, let E = {ei": ""
        },
        {
          "tion loss is adequate to instill in the models the relationships between": "",
          ": i ∈ [n]} be the set of emotions\nGiven n emotions, let E = {ei": "combination, where we simultaneously push apart\ninter-group pairs"
        },
        {
          "tion loss is adequate to instill in the models the relationships between": "most recent and successful methods deploy deep learning.\nInitially,",
          ": i ∈ [n]} be the set of emotions\nGiven n emotions, let E = {ei": ""
        },
        {
          "tion loss is adequate to instill in the models the relationships between": "",
          ": i ∈ [n]} be the set of emotions\nGiven n emotions, let E = {ei": "and pull together intra-group pairs. Our formulation is:"
        },
        {
          "tion loss is adequate to instill in the models the relationships between": "single-label predictions were transformed into a multi-label output",
          ": i ∈ [n]} be the set of emotions\nGiven n emotions, let E = {ei": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "En SemEval 2018 Task 1 E-c\nLL": ""
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": "f, f (cid:48)"
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": "Group\nMic-F1\nMac-F1\nJS\nLG"
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": "72.0±0.3\n57.1±1.6\n60.9±0.3\n-\n-\n-\n-"
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": ""
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": ""
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": "72.0±0.3\n57.1±1.6\n60.9±0.3\nρ\n-\n-\n-"
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": ""
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": "72.0±0.4\n56.7±1.5\n60.9±0.4\nθ\n-\n-\n-"
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": ""
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": "72.1±0.3\n57.8±1.5\n60.9±0.3\ninter"
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": ""
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": "72.2±0.3\n58.2±1.4\n61.0±0.3\ney\nintra\n1\n-"
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": ""
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": "72.0±0.3\n57.5±1.2\n60.9±0.6\nboth"
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": ""
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": ""
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": "72.1±0.4\n57.6±1.0\n60.9±0.3\ninter"
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": ""
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": "72.3±0.4\n58.1±1.5\n61.2±0.4\nρ\ney\nintra\n-"
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": ""
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": "72.1±0.3\n56.4±1.6\n61.0±0.3\nboth"
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": "Demux\n72.0±0.4\n57.8±0.6\n60.9±0.4\ninter"
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": "72.2±0.5\n57.9±1.3\n61.1±0.4\nθ\ney\nintra\n-"
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": "72.3±0.4\n58.1±1.1\n61.3±0.4\nboth"
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": ""
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": "71.9±0.3\n56.7±1.3\n60.8±0.3\n1"
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": ""
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": "71.7±0.3\n57.1±1.5\n60.5±0.3\nρ\ncos\nboth\n-"
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": ""
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": "71.8±0.3\n57.1±1.8\n60.6±0.4\nθ"
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": "72.2±0.4\n58.3±1.3\n61.0±0.3\nintra\n1"
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": ""
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": "72.2±0.3\n57.8±1.6\n61.3±0.4\nρ\ney\nρ\nintra"
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": ""
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": "72.0±0.4\n58.0±1.4\n60.9±0.5\nθ\nboth"
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": ""
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": "71.1±0.4\n55.9±1.6\n59.9±0.5\n-\n-\n-\n-"
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": ""
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": ""
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": "71.6±0.3\n56.9±1.5\n60.3±0.4\ninter"
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": ""
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": "72.1±0.3\n57.7±1.8\n61.0±0.4\ney\nintra\n1\n-"
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": ""
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": "71.8±0.3\n57.1±1.3\n60.6±0.5\nboth"
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": ""
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": "71.5±0.4\n57.0±1.3\n60.2±0.5\ninter"
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": ""
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": "MEmo\n71.8±0.4\n57.1±1.4\n60.6±0.5\nρ\ney\nintra\n-"
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": ""
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": "71.6±0.4\n56.5±1.8\n60.3±0.4\nboth"
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": ""
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": ""
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": "71.5±0.3\n57.7±1.1\n60.2±0.3\ninter"
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": "71.6±0.5\n56.7±1.3\n60.4±0.4\nθ\ney\nintra\n-"
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": ""
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": ""
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": "71.6±0.4\n57.1±1.2\n60.3±0.5\nboth"
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": ""
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": ""
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": ""
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": "Table 1. Performance on English SemEval 2018 Task 1 E-c dev set."
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": "intra, inter and both refer to LL,inter, LL,intra and LL,both respectively"
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": ""
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": "(Eq. 2), ρ, θ, and 1 to setting c as the empirical correlation, the wheel"
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": ""
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": "angles and (for LL) keeping weights of pairs constant respectively,"
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": ""
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": "and ey and cos refer to Eq. 3 and Eq. 4 respectively."
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": ""
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": ""
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": ""
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": "Arabic, and Spanish.\nThe English subset contains 6838 training,"
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": "886 development and 3259 testing tweets,\nthe Arabic subset con-"
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": "tains 2278 training, 585 development and 1518 testing tweets, and"
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": "the Spanish subset 3561 training, 679 development and 2854 testing"
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": ""
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": "tweets. We use the Jaccard Score (JS), macro F1 and micro F1 for"
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": "all our evaluations."
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": ""
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": "4.2.\nImplementation Details"
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": ""
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": "We use Python (v3.7.4), PyTorch (v1.11.0) and the corresponding"
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": "implementations of Transformer models\nfrom the Hugging Face"
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": ""
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": "transformers library (v4.19.2). We use up to three NVIDIA GeForce"
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": "GTX 1080 Ti and one NVIDIA GeForceRTX 2070, but always one"
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": ""
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": "GPU per model."
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": "We use BERT for English [6] and Arabic [20], and BETO [5]"
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": "for Spanish, as in [1]. In terms of hyperparameters, we retain the hy-"
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": "perparameters used in [1], such as the learning rate and its schedule,"
        },
        {
          "En SemEval 2018 Task 1 E-c\nLL": "batch size, and α (Eq. 5). However, for the text preprocessor, we fur-"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": ""
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": "Demux\n67.6±0.5\n53.1±0.8\n56.0±0.5\nρ\nintra 1 ey"
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": ""
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": "Demux∗\n67.5±0.5\n53.8±0.8\n56.0±0.6\nintra ρ ey\n-"
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": ""
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": "MEmo∗\n67.9±0.4\n51.6±0.5\n56.7±0.5\nintra 1 ey\n-"
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": ""
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": ""
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": "intra\nTable 2. Performance on SemEval 2018 Task 1 E-c test set."
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": ""
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": "respectively (Eq. 2), ρ, θ, and\nand inter refer to LL,inter and LL,intra"
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": ""
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": "1 to setting c as the empirical correlation,\nthe wheel angles and (for"
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": ""
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": "LL) keeping weights of pairs constant respectively, and ey refers to"
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": ""
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": "Eq. 3. †: reproduced from authors’ code. ∗: main result."
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": ""
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": "ther remove the special tags introduced by the preprocessing library,"
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": "ekphrasis2. We found 0.1 and 0.5 to work equally well for β (Eq. 5),"
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": "so we set β = 0.1 when we use the global loss. For early stopping,"
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": "we reduce the patience to 5 epochs, and use JS instead as the eval-"
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": "uation metric. Moreover,\ninstead of directly using the model after"
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": "early stopping, we retrain it on both the train and dev sets, picking"
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": "the number of epochs based on the performance distribution of the"
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": "model across multiple runs. Test and dev performance are reported"
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": "after 10 runs."
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": ""
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": ""
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": "4.3. Local and Global relations"
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": ""
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": ""
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": "We run comprehensive experiments to determine the better alterna-"
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": ""
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": "tive w.r.t.\nregularization losses and present our results in Table 1.\nIn"
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": ""
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": "general, for Demux, we see minimal changes in performance from"
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": ""
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": "conﬁguration to conﬁguration. As general trends, we observe that re-"
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": ""
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": "inforcing intra-group relations works better compared to inter-group"
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": ""
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": "ones, or even their combination. This implies that creating opposing"
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": ""
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": "forces between groups of emotions can hurt performance, perhaps"
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": ""
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": "because the ability to express more nuance with multiple different"
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": ""
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": "emotions is hindered. The global regularization and our weighting"
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": ""
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": "scheme seem to have little to no effect, and the local regularization"
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": ""
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": "based on the cosine similarity seems to be too strict, noticeably de-"
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": ""
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": "grading performance even compared to the baseline."
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": ""
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": "For MEmo, we see better performance with the new classiﬁer"
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": ""
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": "instead of the MLM head, thus we report the former’s numbers. We"
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": ""
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": "observe reduced macro F1 but similar micro F1 and JS to Demux."
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": ""
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": "We see clearer improvement by the regularization terms, suggesting"
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": ""
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": "this alternative does not capture word-level associations as well as"
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": ""
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": "Demux does, and similar trends with intra vs.\ninter-group terms."
        },
        {
          "65.8±0.9\n50.5±1.9\n53.5±1.1\n-": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Mic-F1\nMac-F1\nJS": ""
        },
        {
          "Mic-F1\nMac-F1\nJS": "Model\nEn SemEval 2018 Task 1 E-c\nLL\nLG"
        },
        {
          "Mic-F1\nMac-F1\nJS": ""
        },
        {
          "Mic-F1\nMac-F1\nJS": "SpanEmo†\n69.9±0.8\n52.8±1.4\n57.4±1.0\ninter 1 ey\n-"
        },
        {
          "Mic-F1\nMac-F1\nJS": "Demux\n71.7±0.2\n54.9±0.4\n60.1±0.2\n-\n-"
        },
        {
          "Mic-F1\nMac-F1\nJS": "Demux\n72.2±0.2\n58.1±0.5\n60.8±0.2\nintra 1 ey\n-"
        },
        {
          "Mic-F1\nMac-F1\nJS": "Demux\n72.2±0.2\n57.1±0.3\n60.7±0.2\nintra θ ey\n-"
        },
        {
          "Mic-F1\nMac-F1\nJS": "Demux∗"
        },
        {
          "Mic-F1\nMac-F1\nJS": "72.2±0.2\n57.6±0.3\n60.8±0.2\nintra ρ ey\n-"
        },
        {
          "Mic-F1\nMac-F1\nJS": "MEmo∗"
        },
        {
          "Mic-F1\nMac-F1\nJS": "72.3±0.2\n56.1±0.5\n61.1±0.2\nintra 1 ey\n-"
        },
        {
          "Mic-F1\nMac-F1\nJS": ""
        },
        {
          "Mic-F1\nMac-F1\nJS": "Es SemEval 2018 Task 1 E-c"
        },
        {
          "Mic-F1\nMac-F1\nJS": ""
        },
        {
          "Mic-F1\nMac-F1\nJS": "SpanEmo†"
        },
        {
          "Mic-F1\nMac-F1\nJS": "59.5±1.3\n46.8±3.0\n47.7±1.7\ninter ey\n-"
        },
        {
          "Mic-F1\nMac-F1\nJS": "Demux\n62.7±0.4\n54.1±0.4\n54.2±0.4"
        },
        {
          "Mic-F1\nMac-F1\nJS": "-\n-"
        },
        {
          "Mic-F1\nMac-F1\nJS": ""
        },
        {
          "Mic-F1\nMac-F1\nJS": "Demux\n63.2±0.4\n54.2±0.5\n54.7±0.3\nintra 1 ey\n-"
        },
        {
          "Mic-F1\nMac-F1\nJS": ""
        },
        {
          "Mic-F1\nMac-F1\nJS": "Demux∗\n63.0±0.2\n54.2±0.3\n54.4±0.3\nintra ρ ey\n-"
        },
        {
          "Mic-F1\nMac-F1\nJS": ""
        },
        {
          "Mic-F1\nMac-F1\nJS": "MEmo∗\n63.3±0.4\n51.7±0.6\n55.6±0.4\nintra 1 ey\n-"
        },
        {
          "Mic-F1\nMac-F1\nJS": ""
        },
        {
          "Mic-F1\nMac-F1\nJS": "Ar SemEval 2018 Task 1 E-c"
        },
        {
          "Mic-F1\nMac-F1\nJS": ""
        },
        {
          "Mic-F1\nMac-F1\nJS": ""
        },
        {
          "Mic-F1\nMac-F1\nJS": "SpanEmo†\n65.8±0.9\n50.5±1.9\n53.5±1.1\ninter 1 ey\n-"
        },
        {
          "Mic-F1\nMac-F1\nJS": ""
        },
        {
          "Mic-F1\nMac-F1\nJS": "Demux\n67.6±0.5\n53.1±0.8\n56.0±0.5\nρ\nintra 1 ey"
        },
        {
          "Mic-F1\nMac-F1\nJS": ""
        },
        {
          "Mic-F1\nMac-F1\nJS": "Demux∗\n67.5±0.5\n53.8±0.8\n56.0±0.6\nintra ρ ey\n-"
        },
        {
          "Mic-F1\nMac-F1\nJS": ""
        },
        {
          "Mic-F1\nMac-F1\nJS": "MEmo∗\n67.9±0.4\n51.6±0.5\n56.7±0.5\nintra 1 ey\n-"
        },
        {
          "Mic-F1\nMac-F1\nJS": ""
        },
        {
          "Mic-F1\nMac-F1\nJS": ""
        },
        {
          "Mic-F1\nMac-F1\nJS": "intra\nTable 2. Performance on SemEval 2018 Task 1 E-c test set."
        },
        {
          "Mic-F1\nMac-F1\nJS": ""
        },
        {
          "Mic-F1\nMac-F1\nJS": "respectively (Eq. 2), ρ, θ, and\nand inter refer to LL,inter and LL,intra"
        },
        {
          "Mic-F1\nMac-F1\nJS": ""
        },
        {
          "Mic-F1\nMac-F1\nJS": "1 to setting c as the empirical correlation,\nthe wheel angles and (for"
        },
        {
          "Mic-F1\nMac-F1\nJS": ""
        },
        {
          "Mic-F1\nMac-F1\nJS": "LL) keeping weights of pairs constant respectively, and ey refers to"
        },
        {
          "Mic-F1\nMac-F1\nJS": ""
        },
        {
          "Mic-F1\nMac-F1\nJS": "Eq. 3. †: reproduced from authors’ code. ∗: main result."
        },
        {
          "Mic-F1\nMac-F1\nJS": ""
        },
        {
          "Mic-F1\nMac-F1\nJS": "ther remove the special tags introduced by the preprocessing library,"
        },
        {
          "Mic-F1\nMac-F1\nJS": "ekphrasis2. We found 0.1 and 0.5 to work equally well for β (Eq. 5),"
        },
        {
          "Mic-F1\nMac-F1\nJS": "so we set β = 0.1 when we use the global loss. For early stopping,"
        },
        {
          "Mic-F1\nMac-F1\nJS": "we reduce the patience to 5 epochs, and use JS instead as the eval-"
        },
        {
          "Mic-F1\nMac-F1\nJS": "uation metric. Moreover,\ninstead of directly using the model after"
        },
        {
          "Mic-F1\nMac-F1\nJS": "early stopping, we retrain it on both the train and dev sets, picking"
        },
        {
          "Mic-F1\nMac-F1\nJS": "the number of epochs based on the performance distribution of the"
        },
        {
          "Mic-F1\nMac-F1\nJS": "model across multiple runs. Test and dev performance are reported"
        },
        {
          "Mic-F1\nMac-F1\nJS": "after 10 runs."
        },
        {
          "Mic-F1\nMac-F1\nJS": ""
        },
        {
          "Mic-F1\nMac-F1\nJS": ""
        },
        {
          "Mic-F1\nMac-F1\nJS": "4.3. Local and Global relations"
        },
        {
          "Mic-F1\nMac-F1\nJS": ""
        },
        {
          "Mic-F1\nMac-F1\nJS": ""
        },
        {
          "Mic-F1\nMac-F1\nJS": "We run comprehensive experiments to determine the better alterna-"
        },
        {
          "Mic-F1\nMac-F1\nJS": ""
        },
        {
          "Mic-F1\nMac-F1\nJS": "tive w.r.t.\nregularization losses and present our results in Table 1.\nIn"
        },
        {
          "Mic-F1\nMac-F1\nJS": ""
        },
        {
          "Mic-F1\nMac-F1\nJS": "general, for Demux, we see minimal changes in performance from"
        },
        {
          "Mic-F1\nMac-F1\nJS": ""
        },
        {
          "Mic-F1\nMac-F1\nJS": "conﬁguration to conﬁguration. As general trends, we observe that re-"
        },
        {
          "Mic-F1\nMac-F1\nJS": ""
        },
        {
          "Mic-F1\nMac-F1\nJS": "inforcing intra-group relations works better compared to inter-group"
        },
        {
          "Mic-F1\nMac-F1\nJS": ""
        },
        {
          "Mic-F1\nMac-F1\nJS": "ones, or even their combination. This implies that creating opposing"
        },
        {
          "Mic-F1\nMac-F1\nJS": ""
        },
        {
          "Mic-F1\nMac-F1\nJS": "forces between groups of emotions can hurt performance, perhaps"
        },
        {
          "Mic-F1\nMac-F1\nJS": ""
        },
        {
          "Mic-F1\nMac-F1\nJS": "because the ability to express more nuance with multiple different"
        },
        {
          "Mic-F1\nMac-F1\nJS": ""
        },
        {
          "Mic-F1\nMac-F1\nJS": "emotions is hindered. The global regularization and our weighting"
        },
        {
          "Mic-F1\nMac-F1\nJS": ""
        },
        {
          "Mic-F1\nMac-F1\nJS": "scheme seem to have little to no effect, and the local regularization"
        },
        {
          "Mic-F1\nMac-F1\nJS": ""
        },
        {
          "Mic-F1\nMac-F1\nJS": "based on the cosine similarity seems to be too strict, noticeably de-"
        },
        {
          "Mic-F1\nMac-F1\nJS": ""
        },
        {
          "Mic-F1\nMac-F1\nJS": "grading performance even compared to the baseline."
        },
        {
          "Mic-F1\nMac-F1\nJS": ""
        },
        {
          "Mic-F1\nMac-F1\nJS": "For MEmo, we see better performance with the new classiﬁer"
        },
        {
          "Mic-F1\nMac-F1\nJS": ""
        },
        {
          "Mic-F1\nMac-F1\nJS": "instead of the MLM head, thus we report the former’s numbers. We"
        },
        {
          "Mic-F1\nMac-F1\nJS": ""
        },
        {
          "Mic-F1\nMac-F1\nJS": "observe reduced macro F1 but similar micro F1 and JS to Demux."
        },
        {
          "Mic-F1\nMac-F1\nJS": ""
        },
        {
          "Mic-F1\nMac-F1\nJS": "We see clearer improvement by the regularization terms, suggesting"
        },
        {
          "Mic-F1\nMac-F1\nJS": ""
        },
        {
          "Mic-F1\nMac-F1\nJS": "this alternative does not capture word-level associations as well as"
        },
        {
          "Mic-F1\nMac-F1\nJS": ""
        },
        {
          "Mic-F1\nMac-F1\nJS": "Demux does, and similar trends with intra vs.\ninter-group terms."
        },
        {
          "Mic-F1\nMac-F1\nJS": ""
        },
        {
          "Mic-F1\nMac-F1\nJS": "2https://github.com/cbaziotis/ekphrasis"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6. REFERENCES": "",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": "J. M., AND COLE, S. W. Natural\nlanguage indicators of dif-"
        },
        {
          "6. REFERENCES": "[1] ALHUZALI, H., AND ANANIADOU, S.\nSpanemo: Casting",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": "ferential gene regulation in the human immune system. Pro-"
        },
        {
          "6. REFERENCES": "arXiv\nmulti-label emotion classiﬁcation as\nspan-prediction.",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": "ceedings of the National Academy of Sciences 114, 47 (2017),"
        },
        {
          "6. REFERENCES": "preprint arXiv:2101.10038 (2021).",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": "12554–12559."
        },
        {
          "6. REFERENCES": "[2] BAZIOTIS, C., ATHANASIOU, N., CHRONOPOULOU, A.,",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": "[16] MOHAMMAD, S., BRAVO-MARQUEZ, F., SALAMEH, M.,"
        },
        {
          "6. REFERENCES": "KOLOVOU, A.,\nPARASKEVOPOULOS, G.,\nELLINAS, N.,",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": "AND KIRITCHENKO,\nS.\nSemeval-2018 task 1:\nAffect\nin"
        },
        {
          "6. REFERENCES": "NARAYANAN, S., AND POTAMIANOS, A.\nNTUA-SLP at",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": "the 12th international workshop on\ntweets.\nIn Proceedings of"
        },
        {
          "6. REFERENCES": "SemEval-2018 Task 1: Predicting affective content\nin tweets",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": "semantic evaluation (2018), pp. 1–17."
        },
        {
          "6. REFERENCES": "arXiv preprint\nwith deep attentive rnns and transfer learning.",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": ""
        },
        {
          "6. REFERENCES": "",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": "[17]\nPENNEBAKER, J. W., FRANCIS, M. E., AND BOOTH, R. J."
        },
        {
          "6. REFERENCES": "arXiv:1804.06658 (2018).",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": ""
        },
        {
          "6. REFERENCES": "",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": "Mahway:\nLinguistic\ninquiry and word count:\nLiwc 2001."
        },
        {
          "6. REFERENCES": "",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": "Lawrence Erlbaum Associates 71, 2001 (2001), 2001."
        },
        {
          "6. REFERENCES": "[3] BROWN, T., MANN, B., RYDER, N., SUBBIAH, M., KA-",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": ""
        },
        {
          "6. REFERENCES": "PLAN, J. D., DHARIWAL, P., NEELAKANTAN, A., SHYAM,",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": "[18]\nPLUTCHIK, R. A general psychoevolutionary theory of emo-"
        },
        {
          "6. REFERENCES": "P., SASTRY, G., ASKELL, A., ET AL. Language models are",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": "tion.\nIn Theories of emotion. Elsevier, 1980, pp. 3–33."
        },
        {
          "6. REFERENCES": "information processing\nfew-shot\nlearners. Advances in neural",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": ""
        },
        {
          "6. REFERENCES": "",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": "[19] REED,\nS., ZOLNA, K.,\nPARISOTTO, E., COLMENAREJO,"
        },
        {
          "6. REFERENCES": "systems 33 (2020), 1877–1901.",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": ""
        },
        {
          "6. REFERENCES": "",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": "S. G., NOVIKOV, A., BARTH-MARON, G., GIMENEZ, M.,"
        },
        {
          "6. REFERENCES": "[4] CALVO, R. A., D’MELLO, S., GRATCH,\nJ. M., AND KAP-",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": "SULSKY, Y., KAY, J., SPRINGENBERG, J. T., ET AL. A gen-"
        },
        {
          "6. REFERENCES": "PAS, A. The Oxford handbook of affective computing. Oxford",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": "eralist agent. arXiv preprint arXiv:2205.06175 (2022)."
        },
        {
          "6. REFERENCES": "Library of Psychology, 2015.",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": ""
        },
        {
          "6. REFERENCES": "",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": "[20]\nSAFAYA, A., ABDULLATIF, M., AND YURET, D. KUISAIL"
        },
        {
          "6. REFERENCES": "[5] CA ˜NETE,\nJ., CHAPERON, G.,\nFUENTES, R., HO,\nJ.-H.,",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": "at SemEval-2020 task 12: BERT-CNN for offensive speech"
        },
        {
          "6. REFERENCES": "KANG, H., AND P ´EREZ,\nJ.\nSpanish pre-trained bert model",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": "identiﬁcation in social media. In Proceedings of the Fourteenth"
        },
        {
          "6. REFERENCES": "and evaluation data.\nIn PML4DC at ICLR 2020 (2020).",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": "Workshop on Semantic Evaluation (Barcelona (online), Dec."
        },
        {
          "6. REFERENCES": "",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": "2020), International Committee for Computational Linguistics,"
        },
        {
          "6. REFERENCES": "[6] DEVLIN,\nJ., CHANG, M.-W., LEE, K., AND TOUTANOVA,",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": ""
        },
        {
          "6. REFERENCES": "",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": "pp. 2054–2059."
        },
        {
          "6. REFERENCES": "K.\nBert: Pre-training of deep bidirectional\ntransformers for",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": ""
        },
        {
          "6. REFERENCES": "arXiv\npreprint\narXiv:1810.04805\nlanguage\nunderstanding.",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": "[21]\nSTONE, P. J., DUNPHY, D. C., AND SMITH, M. S. The gen-"
        },
        {
          "6. REFERENCES": "(2018).",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": "eral inquirer: A computer approach to content analysis."
        },
        {
          "6. REFERENCES": "",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": "[22]\nSTRAPPARAVA, C., VALITUTTI, A., ET AL. Wordnet affect:"
        },
        {
          "6. REFERENCES": "[7] DUKES, D., ABRAMS, K., ADOLPHS, R., AHMED, M. E.,",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": ""
        },
        {
          "6. REFERENCES": "",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": "an affective extension of wordnet.\nIn Lrec (2004), vol. 4, Lis-"
        },
        {
          "6. REFERENCES": "BEATTY, A., BERRIDGE, K. C., BROOMHALL, S., BROSCH,",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": ""
        },
        {
          "6. REFERENCES": "",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": "bon, Portugal, p. 40."
        },
        {
          "6. REFERENCES": "T., CAMPOS, J. J., CLAY, Z., ET AL. The rise of affectivism.",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": ""
        },
        {
          "6. REFERENCES": "Nature human behaviour 5, 7 (2021), 816–820.",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": "[23] VAN KLEEF, G. A., VAN DOORN, E. A., HEERDINK, M. W.,"
        },
        {
          "6. REFERENCES": "",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": "AND KONING, L. F. Emotion is for inﬂuence. European Re-"
        },
        {
          "6. REFERENCES": "[8] DUPPADA, V., JAIN, R., AND HIRAY, S. Seernet at semeval-",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": ""
        },
        {
          "6. REFERENCES": "",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": "view of Social Psychology 22, 1 (2011), 114–163."
        },
        {
          "6. REFERENCES": "arXiv\n2018 task 1: Domain adaptation for affect\nin tweets.",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": ""
        },
        {
          "6. REFERENCES": "preprint arXiv:1804.06137 (2018).",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": "[24] VASWANI, A., SHAZEER, N., PARMAR, N., USZKOREIT, J.,"
        },
        {
          "6. REFERENCES": "",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": "JONES, L., GOMEZ, A. N., KAISER, Ł., AND POLOSUKHIN,"
        },
        {
          "6. REFERENCES": "[9]\nFELBO, B., MISLOVE, A., SØGAARD, A., RAHWAN, I., AND",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": ""
        },
        {
          "6. REFERENCES": "",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": "information\nI. Attention is all you need. Advances in neural"
        },
        {
          "6. REFERENCES": "LEHMANN, S. Using millions of emoji occurrences to learn",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": ""
        },
        {
          "6. REFERENCES": "",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": "processing systems 30 (2017)."
        },
        {
          "6. REFERENCES": "any-domain representations for detecting sentiment, emotion",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": ""
        },
        {
          "6. REFERENCES": "and sarcasm. arXiv preprint arXiv:1708.00524 (2017).",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": "[25] WAHL-JORGENSEN, K. Emotions, media and politics.\nJohn"
        },
        {
          "6. REFERENCES": "",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": "Wiley & Sons, 2019."
        },
        {
          "6. REFERENCES": "[10] GARTEN, J., HOOVER, J., JOHNSON, K. M., BOGHRATI, R.,",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": ""
        },
        {
          "6. REFERENCES": "",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": "[26] XU, P., LIU, Z., WINATA, G.\nI., LIN, Z., AND FUNG, P."
        },
        {
          "6. REFERENCES": "ISKIWITCH, C., AND DEHGHANI, M. Dictionaries and distri-",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": ""
        },
        {
          "6. REFERENCES": "",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": "Emograph: Capturing emotion correlations using graph net-"
        },
        {
          "6. REFERENCES": "butions: Combining expert knowledge and large scale textual",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": ""
        },
        {
          "6. REFERENCES": "",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": "works. arXiv preprint arXiv:2008.09378 (2020)."
        },
        {
          "6. REFERENCES": "data content analysis. Behavior research methods 50, 1 (2018),",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": ""
        },
        {
          "6. REFERENCES": "344–361.",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": "[27] YING, W., XIANG, R., AND LU, Q.\nImproving multi-label"
        },
        {
          "6. REFERENCES": "",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": "emotion classiﬁcation by integrating both general and domain-"
        },
        {
          "6. REFERENCES": "[11] GUO, S., BURGHARDT, K., RAO, A., AND LERMAN, K.",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": ""
        },
        {
          "6. REFERENCES": "",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": "the 5th Workshop on\nspeciﬁc knowledge.\nIn Proceedings of"
        },
        {
          "6. REFERENCES": "Emotion regulation and dynamics of moral concerns during",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": ""
        },
        {
          "6. REFERENCES": "",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": "Noisy User-generated Text\n(W-NUT 2019)\n(2019), pp. 316–"
        },
        {
          "6. REFERENCES": "the early covid-19 pandemic. arXiv preprint arXiv:2203.03608",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": ""
        },
        {
          "6. REFERENCES": "",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": "321."
        },
        {
          "6. REFERENCES": "(2022).",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": ""
        },
        {
          "6. REFERENCES": "",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": "[28] YU, J., MARUJO, L., JIANG, J., KARUTURI, P., AND BREN-"
        },
        {
          "6. REFERENCES": "[12] HE, H., AND XIA, R.\nJoint binary neural network for multi-",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": ""
        },
        {
          "6. REFERENCES": "",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": "DEL, W.\nImproving multi-label emotion classiﬁcation via sen-"
        },
        {
          "6. REFERENCES": "label\nlearning with applications to emotion classiﬁcation.\nIn",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": ""
        },
        {
          "6. REFERENCES": "",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": "timent classiﬁcation with dual attention transfer network.\nIn"
        },
        {
          "6. REFERENCES": "CCF International Conference on Natural Language Process-",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": ""
        },
        {
          "6. REFERENCES": "",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": "Proceedings of the 2018 Conference on Empirical Methods in"
        },
        {
          "6. REFERENCES": "ing and Chinese Computing (2018), Springer, pp. 250–259.",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": ""
        },
        {
          "6. REFERENCES": "",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": "Natural Language Processing (Brussels, Belgium, Oct.-Nov."
        },
        {
          "6. REFERENCES": "[13] HOCHREITER, S., AND SCHMIDHUBER, J. Long short-term",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": "2018), Association for Computational Linguistics, pp. 1097–"
        },
        {
          "6. REFERENCES": "memory. Neural computation 9, 8 (1997), 1735–1780.",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": "1102."
        },
        {
          "6. REFERENCES": "[14] LIU, P., YUAN, W., FU, J., JIANG, Z., HAYASHI, H., AND",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": ""
        },
        {
          "6. REFERENCES": "NEUBIG, G.\nPre-train, prompt,\nand predict: A systematic",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": ""
        },
        {
          "6. REFERENCES": "survey of prompting methods in natural\nlanguage processing.",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": ""
        },
        {
          "6. REFERENCES": "arXiv preprint arXiv:2107.13586 (2021).",
          "[15] MEHL, M. R., RAISON, C. L., PACE, T. W., AREVALO,": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Casting multi-label emotion classification as span-prediction",
      "authors": [
        "H Alhuzali",
        "S Ananiadou",
        "Spanemo"
      ],
      "year": "2021",
      "venue": "Casting multi-label emotion classification as span-prediction",
      "arxiv": "arXiv:2101.10038"
    },
    {
      "citation_id": "3",
      "title": "NTUA-SLP at SemEval-2018 Task 1: Predicting affective content in tweets with deep attentive rnns and transfer learning",
      "authors": [
        "C Baziotis",
        "N Athanasiou",
        "A Chronopoulou",
        "A Kolovou",
        "G Paraskevopoulos",
        "N Ellinas",
        "S Narayanan",
        "A Potamianos"
      ],
      "year": "2018",
      "venue": "NTUA-SLP at SemEval-2018 Task 1: Predicting affective content in tweets with deep attentive rnns and transfer learning",
      "arxiv": "arXiv:1804.06658"
    },
    {
      "citation_id": "4",
      "title": "Language models are few-shot learners",
      "authors": [
        "T Brown",
        "B Mann",
        "N Ryder",
        "M Subbiah",
        "J Ka-Plan",
        "P Dhariwal",
        "A Neelakantan",
        "P Shyam",
        "G Sastry",
        "A Askell",
        "Et Al"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "5",
      "title": "The Oxford handbook of affective computing",
      "authors": [
        "R Calvo",
        "S Mello",
        "J Gratch",
        "A Kap-Pas"
      ],
      "year": "2015",
      "venue": "The Oxford handbook of affective computing"
    },
    {
      "citation_id": "6",
      "title": "Spanish pre-trained bert model and evaluation data",
      "authors": [
        "J Ca Ñete",
        "G Chaperon",
        "R Fuentes",
        "J.-H Ho",
        "H Kang",
        "J Érez"
      ],
      "year": "2020",
      "venue": "Spanish pre-trained bert model and evaluation data"
    },
    {
      "citation_id": "7",
      "title": "Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova",
        "Bert"
      ],
      "year": "2018",
      "venue": "Pre-training of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "8",
      "title": "The rise of affectivism",
      "authors": [
        "D Dukes",
        "K Abrams",
        "R Adolphs",
        "M Ahmed",
        "A Beatty",
        "K Berridge",
        "S Broomhall",
        "T Brosch",
        "J Campos",
        "Z Clay",
        "Et Al"
      ],
      "year": "2021",
      "venue": "Nature human behaviour"
    },
    {
      "citation_id": "9",
      "title": "Seernet at semeval-2018 task 1: Domain adaptation for affect in tweets",
      "authors": [
        "V Duppada",
        "R Jain",
        "S Hiray"
      ],
      "year": "2018",
      "venue": "Seernet at semeval-2018 task 1: Domain adaptation for affect in tweets",
      "arxiv": "arXiv:1804.06137"
    },
    {
      "citation_id": "10",
      "title": "Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm",
      "authors": [
        "B Felbo",
        "A Mislove",
        "A Søgaard",
        "I Rahwan",
        "S Lehmann"
      ],
      "year": "2017",
      "venue": "Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm",
      "arxiv": "arXiv:1708.00524"
    },
    {
      "citation_id": "11",
      "title": "Dictionaries and distributions: Combining expert knowledge and large scale textual data content analysis",
      "authors": [
        "J Garten",
        "J Hoover",
        "K Johnson",
        "R Boghrati",
        "C Iskiwitch",
        "M Dehghani"
      ],
      "year": "2018",
      "venue": "Behavior research methods"
    },
    {
      "citation_id": "12",
      "title": "Emotion regulation and dynamics of moral concerns during the early covid-19 pandemic",
      "authors": [
        "S Guo",
        "K Burghardt",
        "A Rao",
        "K Lerman"
      ],
      "year": "2022",
      "venue": "Emotion regulation and dynamics of moral concerns during the early covid-19 pandemic",
      "arxiv": "arXiv:2203.03608"
    },
    {
      "citation_id": "13",
      "title": "Joint binary neural network for multilabel learning with applications to emotion classification",
      "authors": [
        "H He",
        "R Xia"
      ],
      "year": "2018",
      "venue": "CCF International Conference on Natural Language Processing and Chinese Computing"
    },
    {
      "citation_id": "14",
      "title": "Long short-term memory",
      "authors": [
        "S Hochreiter",
        "J Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural computation"
    },
    {
      "citation_id": "15",
      "title": "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing",
      "authors": [
        "P Liu",
        "W Yuan",
        "J Fu",
        "Z Jiang",
        "H Hayashi",
        "G Neubig"
      ],
      "year": "2021",
      "venue": "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing",
      "arxiv": "arXiv:2107.13586"
    },
    {
      "citation_id": "16",
      "title": "Natural language indicators of differential gene regulation in the human immune system",
      "authors": [
        "M Mehl",
        "C Raison",
        "T Pace",
        "J Arevalo",
        "S Cole"
      ],
      "year": "2017",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "17",
      "title": "Semeval-2018 task 1: Affect in tweets",
      "authors": [
        "S Mohammad",
        "F Bravo-Marquez",
        "M Salameh",
        "S Kiritchenko"
      ],
      "year": "2018",
      "venue": "Proceedings of the 12th international workshop on semantic evaluation"
    },
    {
      "citation_id": "18",
      "title": "Linguistic inquiry and word count: Liwc",
      "authors": [
        "J Pennebaker",
        "M Francis",
        "R Booth"
      ],
      "year": "2001",
      "venue": "Linguistic inquiry and word count: Liwc"
    },
    {
      "citation_id": "19",
      "title": "A general psychoevolutionary theory of emotion",
      "authors": [
        "R Plutchik"
      ],
      "year": "1980",
      "venue": "Theories of emotion"
    },
    {
      "citation_id": "20",
      "title": "A generalist agent",
      "authors": [
        "S Reed",
        "K Zolna",
        "E Parisotto",
        "S Colmenarejo",
        "A Novikov",
        "G Barth-Maron",
        "M Gimenez",
        "Y Sulsky",
        "J Kay",
        "J Springenberg",
        "Al"
      ],
      "year": "2022",
      "venue": "A generalist agent",
      "arxiv": "arXiv:2205.06175"
    },
    {
      "citation_id": "21",
      "title": "KUISAIL at SemEval-2020 task 12: BERT-CNN for offensive speech identification in social media",
      "authors": [
        "A Safaya",
        "M Abdullatif",
        "D Yuret"
      ],
      "year": "2020",
      "venue": "Proceedings of the Fourteenth Workshop on Semantic Evaluation"
    },
    {
      "citation_id": "22",
      "title": "The general inquirer: A computer approach to content analysis",
      "authors": [
        "P Stone",
        "D Dunphy",
        "M Smith"
      ],
      "venue": "The general inquirer: A computer approach to content analysis"
    },
    {
      "citation_id": "23",
      "title": "Wordnet affect: an affective extension of wordnet",
      "authors": [
        "C Strapparava",
        "A Valitutti",
        "Et Al"
      ],
      "year": "2004",
      "venue": "Lrec"
    },
    {
      "citation_id": "24",
      "title": "Emotion is for influence",
      "authors": [
        "G Van Kleef",
        "E Van Doorn",
        "M Heerdink",
        "L Koning"
      ],
      "year": "2011",
      "venue": "European Review of Social Psychology"
    },
    {
      "citation_id": "25",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "26",
      "title": "Emotions, media and politics",
      "authors": [
        "K Wahl-Jorgensen"
      ],
      "year": "2019",
      "venue": "Emotions, media and politics"
    },
    {
      "citation_id": "27",
      "title": "Emograph: Capturing emotion correlations using graph networks",
      "authors": [
        "P Xu",
        "Z Liu",
        "G Winata",
        "Z Lin",
        "P Fung"
      ],
      "year": "2020",
      "venue": "Emograph: Capturing emotion correlations using graph networks",
      "arxiv": "arXiv:2008.09378"
    },
    {
      "citation_id": "28",
      "title": "Improving multi-label emotion classification by integrating both general and domainspecific knowledge",
      "authors": [
        "W Ying",
        "R Xiang",
        "Q Lu"
      ],
      "year": "2019",
      "venue": "Proceedings of the 5th Workshop on Noisy User-generated Text"
    },
    {
      "citation_id": "29",
      "title": "Improving multi-label emotion classification via sentiment classification with dual attention transfer network",
      "authors": [
        "J Yu",
        "L Marujo",
        "J Jiang",
        "P Karuturi",
        "W Bren-Del"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing"
    }
  ]
}