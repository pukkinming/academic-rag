{
  "paper_id": "2002.07551v1",
  "title": "Hierarchical Transformer Network For Utterance-Level Emotion Recognition",
  "published": "2020-02-18T13:44:49Z",
  "authors": [
    "QingBiao Li",
    "ChunHua Wu",
    "KangFeng Zheng",
    "Zhe Wang"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "While there have been significant advances in detecting emotions in text, in the field of utterancelevel emotion recognition (ULER), there are still many problems to be solved. In this paper, we address some challenges in ULER in dialog systems. (1) The same utterance can deliver different emotions when it is in different contexts or from different speakers. (2) Long-range contextual information is hard to effectively capture. (3) Unlike the traditional text classification problem, this task is supported by a limited number of datasets, among which most contain inadequate conversations or speech. To address these problems, we propose a hierarchical transformer framework (apart from the description of other studies, the \"transformer\" in this paper usually refers to the encoder part of the transformer) with a lower-level transformer to model the word-level input and an upper-level transformer to capture the context of utterance-level embeddings. We use a pretrained language model bidirectional encoder representations from transformers (BERT) as the lower-level transformer, which is equivalent to introducing external data into the model and solve the problem of data shortage to some extent. In addition, we add speaker embeddings to the model for the first time, which enables our model to capture the interaction between speakers. Experiments on three dialog emotion datasets, Friends, Emotion-Push, and EmoryNLP, demonstrate that our proposed hierarchical transformer network models achieve 1.98%, 2.83%, and 3.94% improvement, respectively, over the state-of-the-art methods on each dataset in terms of macro-F1.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Sentiment analysis, considered one of the most important methods for analyzing real-world communication, is a kind of classification task for extracting emotion from language. It can help us progress in many fields, such as data mining and developing empathetic machines for people. In this paper, we consider one of the tasks in this research direction, utterance-level emotion recognition (ULER)  [Poria et al., 2017] . In ULER, an utterance  [Olson, 1977]  is a unit of speech bounded by breathes or pauses, and its goal is to tag each utterance in a dialog with the indicated emotion (e.g., happy, sad, or angry). Traditional sentiment analysis methods are confined to analyzing only a single sentence or document, regardless of its surrounding information. However, in the field of ULER, contextual information is indispensable in emotional discrimination. For example, in Figure  1 , the utterance \"Yes, I agree with this point.\" can deliver different emotions in different contexts. To identify a speaker's emotion precisely,  [Hazarika et al., 2018]  produced contextual representations for prediction with a recurrent neural network (RNN), where each utterance is represented by a feature vector extracted by convolutional neural networks (CNN) at an earlier stage. Similarly,  [Jiao et al., 2019]  proposed a hierarchical gated recurrent unit (HiGRU) framework with a lowerlevel GRU to model the word-level inputs and an upper-level GRU to capture the contexts of utterance-level embeddings.\n\nTheoretically, RNNs such as long short-term memory (LSTM) and gated recurrent units (GRUs) should propagate long-term contextual information. However, in practice, this is not always the case  [Bradbury et al., 2017] . In cases where the input sequence is long, RNNs may experience an exploding gradient or vanishing gradient. Unlike traditional text classification problems, in the field of ULER, there are a limited number of datasets, and most datasets contain inadequate conversations. This issue limits the possibility of obtaining larger models for this task. To solve this issue,  [Zhong et al., 2019]  proposed a knowledge-enriched transformer (KET) to",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Hierarchical Transformer Network For Utterance-Level Emotion Recognition",
      "text": "QingBiao Li 1 , ChunHua Wu 1* , KangFeng Zheng 1 and Zhe Wang 1 1 Beijing University of Posts and Telecommunications, Beijing, China {liqingbiao, wuchunhua}@bupt.edu.cn, zkf_bupt@163.com, wangxiaozhe@bupt.edu.cn  effectively incorporate contextual information and external knowledge bases, but this model structure is complex, and the running speed is not high.  [Jiao et at., 2019]  proposed pretraining a context-dependent encoder (CoDE) for ULER by learning from unlabeled conversation data to address the aforementioned challenge, but the model did not perform better in the word-level embedding phase.\n\nIn this task, we propose a hierarchical transformer framework to solve the above issues. First, we use a transformer  [Vaswani et at., 2017]  to model the word-level input and capture the contexts of utterance-level embeddings, which has been shown to be a powerful representation learning model in many NLP tasks and can exploit contextual information more efficiently than RNNs and CNNs. Second, for the data scarcity issue, we use a pretrained language model, bidirectional encoder representations from transformers (BERT)  [Devlin et al., 2018]  as the lower-level transformer, which is equivalent to introducing external data into the model and helps our model obtain better utterance embedding. Third, the same utterance can deliver different emotions in the same context. For example, in Figure  2 , the utterance \"Yes, I agree. I think so, too.\" can deliver different emotions, joy and sadness. However, previous studies have not addressed this situation because those models did not capture the interaction between the speakers, and did not consider the emotional dynamics of the speakers in a dialog. To solve the problem, we introduce speaker embedding into our model. To the best of our knowledge, this is the first model for ULER with speaker embedding. After obtaining the contextual utterance embedding vectors with a hierarchical transformer framework, we feed them into the fully connected layers for classification. We employ dropout on the fully connected layers to prevent overfitting. Finally, we obtain an utterance category with a softmax layer. We summarize our contributions as follows:\n\nâ€¢ We propose a hierarchical transformer framework to better learn both the individual utterance embeddings and the contextual information of utterances.\n\nâ€¢ We use a pretrained language model, BERT, to obtain better dialog embedding, which is equivalent to introducing external data into the model and solve the problem of data shortage to some extent.\n\nâ€¢ For the first time, we use speaker embedding in the model for the ULER task, which allows our model to capture the interaction between speakers and better understand emotional dynamics in dialog systems.\n\nâ€¢ Our model outperforms state-of-the-art models on three benchmark datasets, Friends, EmotionPush, and EmoryNLP.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Text-based emotion recognition is a long-standing research topic, and there have been many excellent studies. However, these models do not perform well in the field of ULER because they treat texts independently and thus cannot capture the interdependence of utterances in dialogs. To capture the contexts of utterance-level embeddings more effectively, we propose a hierarchical transformer framework, which is mainly explored in the following topics.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Individual Utterance Information Extraction",
      "text": "In traditional methods, a common method of expressing text is the bag-of-words method. However, the bag-of-words method loses the order of the words. The n-gram model is a very popular statistical language model and usually performs well  [Thorsten, 1998] . However, the n-gram model has a large defect in that it is affected by data sparsity  [Bengio et at., 2013] . Recently, neural network methods have become increasingly popular. There is a trend moving from traditional methods to deep learning methods to obtain better text representations. Some prominent models include recursive autoencoders (RAEs)  [Socher et al., 2011] , convolutional neural networks (CNNs)  [Kim, 2014] , and recurrent neural networks (RNNs)  [Abdul-Mageed and Ungar, 2017 ]. Although we can train a more complex model with a neural network, when the quantity of data is small, it does not perform well.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Pretrained Language Models",
      "text": "Unsupervised pretraining is a special case of semisupervised learning where the goal is to find a good initialization point. Pretrained language models, such as ELMo  [Peters et al.,2018] , OpenAI  GPT [Radford et al., 2018] , and BERT  [Devlin et al., 2018] , have achieved great success in a variety of NLP tasks, such as sentiment analysis and textual classification. They can generate deep contextualized embeddings since they are pretrained on a massive unlabeled corpus (i.e., English Wikipedia). Some proposed models  [Sun et al., 2019]  with pretrained language models have obtained outstanding results on the sentiment analysis task of individual sentences.\n\n[  Reimers et at., 2019]  proposed Siamese BERT-networks (SBERT) to obtain sentence embeddings and proved that their model outperforms other state-of-the-art sentence embedding methods.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Contextual Information Extraction",
      "text": "The RNN architecture is a standard method for capturing the sequential relationship of data.  [Poria et at., 2015]  captured the contextual information with a bidirectional long shortterm memory (BiLSTM) network and obtained great\n\nThe architecture of our proposed HiTransformer-s. By removing the \"Speaker Embedding\" layer, we attain HiTransformer.\n\nperformance. Similarly,  [Jiao et al., 2019]  applied bidirectional GRU to model contextual information. In addition, they placed a self-attention layer in the hidden states of GRU and fused the attention outputs with the individual utterance embeddings to learn the contextual utterance embeddings.\n\n[  Luo et al., 2019]  applied self-attention to model the context of textual features extracted by BiLSTM.  [Zahiri and Choi, 2018]  proposed sequence-based convolutional neural networks (SCNN) that utilize emotion sequences from previous utterances to detect the emotion of the current utterance.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Transformer",
      "text": "The transformer learns the dependencies between words based entirely on self-attention without any recurrent or convolutional layers. Due to its rich representation and fast computation, it has been applied to many NLP tasks, e.g., response matching in dialog systems  [Zhou et al., 2018]  and language modeling  [Dai et al.,2019] . The success of transformer has raised a large body of follow-up work. Therefore, some transformer variations have also been proposed, such as GPT  [Radford et al., 2018] , BERT  [Devlin et al., 2018] , universal transformer  [Dehghani et al., 2018]  and CN3  [Liu et al., 2018] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Approach",
      "text": "In this section, we present the task definition and our proposed hierarchical transformer (HiTransformer) network. In addition, we propose a variation in HiTransformer by adding speaker embedding, named HiTransformer-s. The overall architecture of our models is illustrated in Figure  3 .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Task Definition",
      "text": "Let there be a set of speakers, ğ‘† = {ğ‘  ğ‘– } ğ‘–=1 ğ‘€ , where ğ‘€ is the number of speakers, and a set of emotions, ğ¶ = {ğ‘ ğ‘– } ğ‘–=1 ğ‘ , where ğ‘ is the number of emotions, such as anger, joy, sadness, and neutral. Assume we are given a set of dialogs, ğ· = {ğ· ğ‘– } ğ‘–=1 ğ¿ , where ğ¿ is the number of dialogs. In each dialog, ğ· ğ‘– = {(ğ‘¢ ğ‘— , ğ‘  ğ‘— , ğ‘ ğ‘— )} ğ‘—=1 ğ‘ ğ‘– is a sequence of utterances, where the utterance ğ‘¢ ğ‘— is spoken by ğ‘  ğ‘— âˆˆ ğ‘† with an emotion ğ‘ ğ‘— âˆˆ ğ¶. Our goal is to train a model to find the most likely emotion from ğ¶ for each new utterance.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Hitransformer: Hierarchical Transformer",
      "text": "Our HiTransformer consists of two-level transformers: the lower-level transformer models the word-level input and obtains the individual utterance embedding. The upper-level transformer captures the contextual information and obtains utterance-level embeddings.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Multi-Head Attention",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Add",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Individual Utterance Embedding",
      "text": "For the input utterance ğ‘¢ ğ‘— = {ğ‘¤ ğ‘˜ } ğ‘˜=1 ğ‘€ ğ‘— , where ğ‘¢ ğ‘— is the ğ‘— -ğ‘¡â„ utterance in ğ· ğ‘– and ğ‘€ ğ‘— is the number of words in the utterance ğ‘¢ ğ‘— . First, the utterance ğ‘¢ ğ‘— is lower-cased and tokenized according to a byte pair encoding (BPE) algorithm. If there are tokens exceeding the preset maximum length of input tokens, those tokens are excluded from the list. Then, we embed those tokens through WordPiece embeddings  [Wu et at., 2016]  and obtained the token embeddings ğ‘’ = {ğ‘’ ğ‘˜ } ğ‘˜=1 ğ‘€ ğ‘— . Finally, the input embeddings ğ¸ = {ğ¸ ğ‘˜ } ğ‘˜=1 ğ‘€ ğ‘— are the summation of the token embeddings ğ‘’ and the positional embeddings ğ‘ = {ğ‘ ğ‘˜ } ğ‘˜=1 ğ‘€ ğ‘— :\n\nwhere âŠ™ denotes element -wise addition.\n\nWe feed the input embeddings ğ¸ into the lower-level transformer to learn the individual utterance embedding. We adopt the transformer-based pretrained language model BERT (illustrated in Figure  4 ) as the lower-level transformer, which is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning both the left and right contexts in all layers. The detailed structure is shown in Figure  4 . The language model converts input embeddings ğ¸ into contextual word embedding ğ‘‡ = {ğ‘‡ ğ‘˜ } ğ‘˜=1 ğ‘€ ğ‘— .\n\nThe individual utterance embedding is then obtained by max-pooling on the contextual word embeddings within an utterance, which can assist in retaining important information in each dimension:",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "ğ‘“(ğ‘¢ ğ‘— ) = ğ‘šğ‘ğ‘¥ğ‘ğ‘œğ‘œğ‘™(ğ‘‡)",
      "text": "(3)",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Contextual Utterance Embedding",
      "text": "For the ğ‘– -ğ‘¡â„ dialog in ğ·, ğ· ğ‘– = {(ğ‘¢ ğ‘— , ğ‘  ğ‘— , ğ‘ ğ‘— )} ğ‘—=1 ğ‘ ğ‘– , the individual utterance embedding is {ğ‘“(ğ‘¢ ğ‘— )} ğ‘—=1 ğ‘ ğ‘– . We concatenate the individual embeddings with the position embeddings to obtain ğ‘ˆ = {ğ‘“(ğ‘¢ ğ‘— ) âŠ™ ğ‘ ğ‘— } ğ‘—=1 ğ‘ ğ‘– , where ğ‘ ğ‘— is the embedding of position ğ‘—. Then, we feed ğ‘ˆ into the upper-level transformer to capture the sequential and contextual relationship of utterances in a dialog and obtain the contextual utterance embedding ğ‘¡ = {ğ‘¡ ğ‘— } ğ‘—=1 ğ‘ ğ‘– .\n\nThen, we feed the contextual utterance embedding vector into the classifier, which consists of two linear layers, one activation function and dropout. Finally, we obtain the predicted vector over all emotions with a softmax function.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "ğ‘ ğ‘’ğ‘™ğ‘¢(ğ‘¥) = ğœ† {",
      "text": "ğ‘¥ ğ‘–ğ‘“ ğ‘¥ > 0 ğ›¼ğ‘’ ğ‘¥ -ğ›¼ ğ‘–ğ‘“ ğ‘¥ â‰¤ 0\n\n(5)\n\nğ‘¦ Ì‚= ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(ğ‘Š 2 ğ‘ ğ‘’ğ‘™ğ‘¢(ğ‘Š 1 ğ‘¡ + ğ‘ 1 ) + ğ‘ 2 ) (6)",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Hitransformer-S: Hierarchical Transformer With Speaker Embeddings",
      "text": "The HiTransformer contains a main issue that it cannot capture the interaction of speakers in a dialog. For example, in Figure  2 , the utterance \"Yes, I agree. I think so, too.\" delivers different emotions, sadness and joy. However, the Hi-Transformer cannot tag it exactly. To solve this problem, we propose hierarchical transformer with speaker embeddings (HiTransformer-s), which can model the interaction of speakers in a dialog.\n\nFor the ğ‘– -ğ‘¡â„ dialog in ğ·, ğ· ğ‘– = {(ğ‘¢ ğ‘— , ğ‘  ğ‘— , ğ‘ ğ‘— )} ğ‘—=1 ğ‘ ğ‘– , the indi- Finally, we concatenate the summation of the individual utterance embeddings and the embeddings of position with the speaker embeddings of every utterance as the input of the upper-level transformer.\n\nWhere âŠ™ denotes element -wise addition, and âŠ• is the concatenation operator.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Model Training",
      "text": "To solve the issue of class imbalance, following the above research  [Khosla, 2018] , we use weighted cross entropy as the training loss to weight the samples of minority classes as below.\n\nwhere ğ‘ ğ‘– denotes the number of utterances with emotion ğ‘– in the training set.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experimental Settings",
      "text": "In this section, we present the datasets, evaluation metrics, baselines and experimental results of our model.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Dataset",
      "text": "Friends  [Hsu and Ku, 2018] : The dataset is annotated from the Friends TV Scripts, and each dialog in the dataset consists of a scene of multiple speakers. In total, there are 1,000 dialogs, which are split into three parts: 720 for training, 80 for validation, and 200 dialogs for testing. Each utterance is tagged with an emotion in a set of emotions, {anger, joy, sadness, neutral, surprise, disgust, fear, and nonneutral}. EmotionPush  [Hsu and Ku, 2018] : The dataset consists of private conversations between friends on Facebook include 1,000 dialogs, which are split into 720, 80, and 200 dialogs for training, validation and testing, respectively. Each utterance is tagged with an emotion in a set of emotions as in the Friends dataset.\n\nEmoryNLP  [Zahiri and Choi, 2018] : The dataset is annotated from the Friends TV Scripts as well. It includes 713 dialogs for training, 99 dialogs for validation and 85 dialogs for testing. The emotion labels include neutral, sad, mad, scared, powerful, peaceful, and joyful.\n\nFor the first two datasets, we follow previous works  [Jiao et at., 2019]  to consider only four emotion classes, i.e., anger, joy, sadness, and neutral, and consider all the emotion classes for EmoryNLP.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "Following  [Jiao et at., 2019] , which achieved the best performance on several ULER datasets, we choose macro-averaged F1-score as the primary metric for evaluating the performance of our models.\n\nwhere ğ¹1 ğ‘ is the F1-score of emotion ğ‘. We also report the weighted accuracy (WA) and unweighted accuracy (UWA), which were adopted in a previous work  [Hsu and Ku, 2018] .\n\nwhere ğ‘¤ ğ‘ is the percentage of class ğ‘ in the testing set, and ğ‘ ğ‘ is the corresponding accuracy. As shown in Table  1  and Table  2 , most of the datasets in this paper have an imbalanced emotion distribution, so the F1-score is better for measuring the model performance.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Compared Methods",
      "text": "We compare our model HiTransformer and HiTransformer-s with the following state-of-the-art baselines:\n\nSA-BiLSTM  [Luo et at., 2018] : A self-attentive bidirectional LSTM model, an efficient model that achieved second place in the EmotionX Challenge  [Hsu and Ku, 2018] ;\n\nCNN-DCNN  [Khosla, 2018] : A convolutional-deconvolutional autoencoder with more handmade features, and the winner of the EmotionX Challenge  [Hsu and Ku, 2018] ; ğ›ğœğ‹ğ’ğ“ğŒ +  [Jiao et at., 2019] : A model with a 1-D CNN to extract the utterance embeddings, and a bidirectional LSTM to model the relationship of utterances; bcGRU  [Jiao et at., 2019] : A variant of ğ›ğœğ‹ğ’ğ“ğŒ + with a BiGRU to capture the utterance-level context; ğ‚ğ¨ğƒğ„ ğ’ğ’Šğ’…  [Jiao et at., 2019] : ğ‚ğ¨ğƒğ„ ğ’ğ’Šğ’… is a context-dependent encoder (CoDE) model with a bidirectional GRU that extracts the utterance embeddings and a bidirectional GRU that models the relationship of utterances; ğğ“ -ğ‚ğ¨ğƒğ„ ğ’ğ’Šğ’…  [Jiao et at., 2019] : A variant of ğ‚ğ¨ğƒğ„ ğ’ğ’Šğ’… that pretrains a context-dependent encoder (CoDE) for ULER by learning from unlabeled conversation data; HiGRU  [Jiao et al., 2019] : A hierarchical gated recurrent unit (HiGRU) framework with a lower-level GRU to model the word-level inputs and an upper-level GRU to capture the contexts of utterance-level embeddings; HiGRU-f  [Jiao et al., 2019] : A variant of HiGRU with individual feature fusion; HiGRU-sf  [Jiao et al., 2019] : A variant of HiGRU with selfattention and feature fusion; SCNN  [Zahiri and Choi, 2018] : A sequence-based convolutional neural networks that utilizes the emotion sequence from the previous utterances for detecting the emotion of the current utterance.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Parameters",
      "text": "We adopt the pretrained uncased BERT-Base 1 model as the lower-level transferable language model, where the maximum input length is 512. The number of combination layers of a multi-head attention and a feedforward neural network is 12. For the upper-level transformer layers, the number of transformer layers is 4 and the number of heads in the multihead attention is 8. For the classification layer, the internal hidden size of the classification layer is set to 300, and the dropout rate is 0.5 to prevent overfitting. We adopt Adam  [Kingma and Ba, 2015]  as the optimizer with a batch size of 1 and a learning rate of 1 Ã— 10 -5 .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Result Analysis",
      "text": "We report the empirical results in Table  3 , which present the overall performance of our models on all datasets. From these results, we make the following observations.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Comparison With Baselines",
      "text": "Our proposed HiTransformer-s outperforms the state-of-theart methods with significant margins on all the datasets in terms of macro-F1 score. Specifically, HiTransformer-s obtains 1.98%, 2.83%, and 3.94% absolute improvement on Friends, EmotionPush, and EmoryNLP, respectively. In addition, for Friends, HiTransformer-s obtains 0.88% improvement compared with the best performance in the past in terms of WA, and 0.12% less than the best performance from HiGRU-sf in terms of UWA. However, HiTransformer-s obtains an 8.18% improvement compared with HiGRU-sf in terms of WA. For EmotionPush, although HiTransformer-s is 0.78% lower than SA-BiLSTM in terms of WA, HiTransformer-s is 8.03% above SA-BiLSTM in terms of WA. Similarly, HiTransformer-s is 5.07% lower than HiGRU-sf in terms of UWA and 13.92% above HiGRU-sf in terms of WA. For EmoryNLP, HiTransformer-s obtains 1.88% and 2.37% absolute improvement in terms of WA and UWA, respectively. The HiTransformer outperforms the state-of-the-art methods on all the datasets in terms of the macro-F1 score as well. The above results demonstrate the superior power of HiTransformer-s and HiTransformer.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Hitransformer Vs. Hitransformer-S",
      "text": "By analyzing ULER, we find that speaker information plays an important role in utterance classification. Therefore, we proposed HiTransformer-s on the basis of HiTransformer.\n\nFrom Table  3 , we observe that HiTransformer-s outperforms HiTransformer on all three datasets in terms of macro-F1, WA, and UWA. Specifically, on Friends, HiTransformer-s attains 1.22%, 0.07% and 5.07% improvement over HiTransformer in terms of macro-F1, WA, and UWA, respectively.\n\nOn EmotionPush, HiTransformer-s attains 1.53%, 0.05% and 1.52% improvement over HiTransformer in terms of macro-F1, WA, and UWA. On EmoryNLP, HiTransformer-s attains 1.68%, 0.73% and 3.43% improvement over HiTransformer in terms of macro-F1, WA, and UWA. The results demonstrate that HiTransformer-s including speaker information is indeed capable of boosting the performance of the HiTransformer.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, to address utterance-level emotion recognition in dialog systems, we propose a hierarchical transformer (Hi-Transformer) framework with a lower-level transformer to model word-level input and an upper-level transformer to capture the contexts of utterance-level embeddings. To obtain better individual utterance embeddings, we adopt BERT, which is pretrained on a massive unlabeled corpus as the lower-level transformer. To enable HiTransformer to obtain speaker information, we propose HiTransformer-s. Our proposed hierarchical transformer models outperform the stateof-the-art methods on all three datasets, which demonstrates that hierarchical transformer models can sufficiently capture the available utterance information in a dialog. In the future, we plan to pretrain a transformer model to capture the relationship of utterances, similar to BERT, and adopt it as the upper-level transformer to capture the textual information more sufficiently, which can also address the problem of data scarcity in ULER.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The utterance â€œYes, I agree with this point.â€ can deliver",
      "page": 1
    },
    {
      "caption": "Figure 1: , the utter-",
      "page": 1
    },
    {
      "caption": "Figure 2: The utterance â€œYes, I agree. I think so, too.â€ delivers dif-",
      "page": 2
    },
    {
      "caption": "Figure 2: , the utterance â€œYes, I agree.",
      "page": 2
    },
    {
      "caption": "Figure 3: The architecture of our proposed HiTransformer-s. By removing the â€œSpeaker Embeddingâ€ layer, we attain HiTransformer.",
      "page": 3
    },
    {
      "caption": "Figure 3: 3.1 Task Definition",
      "page": 3
    },
    {
      "caption": "Figure 4: Structure of BERT",
      "page": 3
    },
    {
      "caption": "Figure 4: ) as the lower-level transformer,",
      "page": 4
    },
    {
      "caption": "Figure 4: The language model converts input em-",
      "page": 4
    },
    {
      "caption": "Figure 2: , the utterance â€œYes, I agree. I think so, too.â€ de-",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Detailed descriptions of Friends and EmotionPush",
      "data": [
        {
          "Dataset": "",
          "#Dialog (#Utterance)": "Train \nVal \nTest",
          "Emotion": "Ang \nHap/Joy \nSad \nNeu \nOthers"
        },
        {
          "Dataset": "Friends",
          "#Dialog (#Utterance)": "720(10651) \n80(721) \n200(1208)",
          "Emotion": "756 \n1710 \n498 \n6530 \n5006"
        },
        {
          "Dataset": "EmoryPush",
          "#Dialog (#Utterance)": "720(10733) \n80(1202) \n200(2807)",
          "Emotion": "140 \n2100 \n514 \n9855 \n2133"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 3: Testing results on Friend, EmotionPush, and EmoryNLP",
      "data": [
        {
          "Model": "",
          "Friends": "Macro-F1 \nWA \nUWA",
          "EmotionPush": "Macro-F1 \nWA \nUWA",
          "EmoryNLP": "Macro-F1 \nWA \nUWA"
        },
        {
          "Model": "SA-BiLSTM \nCNN-DCNN \nğ›ğœğ‹ğ’ğ“ğŒ+  \nbcGRU \nğ‚ğ¨ğƒğ„ğ’ğ’Šğ’… \nğğ“ âˆ’ ğ‚ğ¨ğƒğ„ğ’ğ’Šğ’… \nHiGRU \nHiGRU-f \nHiGRU-sf \nSCNN",
          "Friends": "- \n79.8 \n59.6 \n- \n67.0 \n62.5 \n63.1 \n79.9 \n63.3 \n62.4 \n77.6 \n66.1 \n62.4 \n78.0 \n65.3 \n65.9 \n81.3 \n66.8 \n- \n74.4 \n67.2 \n- \n71.3 \n68.4 \n- \n74.0 \n68.9 \n- \n- \n-",
          "EmotionPush": "- \n87.7 \n55.0 \n- \n75.7 \n62.5 \n60.3 \n84.8 \n57.9 \n60.5 \n84.6 \n56.9 \n60.3 \n84.2 \n58.5 \n62.6 \n84.7 \n60.4 \n- \n73.8 \n66.3 \n- \n73.0 \n66.9 \n- \n73.0 \n68.1 \n- \n- \n-",
          "EmoryNLP": "- \n- \n- \n- \n- \n- \n25.5 \n33.5 \n27.6 \n26.1 \n33.1 \n27.4 \n26.7 \n34.7 \n28.8 \n29.1 \n36.1 \n30.3 \n- \n- \n- \n- \n- \n- \n- \n- \n- \n26.9 \n37.9 \n-"
        },
        {
          "Model": "HiTransformer \nHiTransformer-s",
          "Friends": "66.66 \n82.11 \n63.71 \n67.88 \n82.18 \n68.78",
          "EmotionPush": "63.90 \n86.87 \n61.55 \n65.43 \n86.92 \n63.03",
          "EmoryNLP": "31.36 \n37.25 \n29.24 \n33.04 \n37.98 \n32.67"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "Poria"
      ],
      "year": "2017",
      "venue": "ACL"
    },
    {
      "citation_id": "2",
      "title": "From utterance to text: The bias of language in speech and writing",
      "authors": [
        "David Olson",
        "Olson"
      ],
      "year": "1977",
      "venue": "Harvard educational review"
    },
    {
      "citation_id": "3",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "Hazarika"
      ],
      "year": "2018",
      "venue": "NAACL-HLT"
    },
    {
      "citation_id": "4",
      "title": "HiGRU: Hierarchical Gated Recurrent Units for Utterance-level Emotion Recognition",
      "authors": [
        "Jiao"
      ],
      "year": "2019",
      "venue": "NAACL"
    },
    {
      "citation_id": "5",
      "title": "Quasi-Recurrent Neural Networks",
      "authors": [
        "Bradbury"
      ],
      "year": "2017",
      "venue": "ICLR"
    },
    {
      "citation_id": "6",
      "title": "Knowledge-Enriched Transformer for Emotion Detection in Textual Conversations",
      "authors": [
        "Zhong"
      ],
      "year": "2019",
      "venue": "EMNLP-IJCNLP"
    },
    {
      "citation_id": "7",
      "title": "PT-CoDE: Pre-trained Context-Dependent Encoder for Utterance-level Emotion Recognition",
      "authors": [
        "Jiao",
        "Jiao",
        "Michael Wenxiang",
        "Irwin Lyu",
        "King"
      ],
      "year": "2019",
      "venue": "PT-CoDE: Pre-trained Context-Dependent Encoder for Utterance-level Emotion Recognition",
      "arxiv": "arXiv:1910.08916"
    },
    {
      "citation_id": "8",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Åukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "NIPS"
    },
    {
      "citation_id": "9",
      "title": "Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Devlin"
      ],
      "year": "2018",
      "venue": "Pre-training of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "10",
      "title": "Text categorization with support vector machines: Learning with many relevant features",
      "authors": [
        "Thorsten Joachims",
        "Thorsten ; Yoshua Bengio",
        "RÃ©jean Ducharme",
        "Pascal Vincent",
        "Christian Jauvin"
      ],
      "year": "1998",
      "venue": "European conference on machine learning"
    },
    {
      "citation_id": "11",
      "title": "Semi-supervised recursive autoencoders for predicting sentiment distributions",
      "authors": [
        "Socher"
      ],
      "year": "2011",
      "venue": "EMNLP"
    },
    {
      "citation_id": "12",
      "title": "Convolutional neural networks for sentence classification",
      "authors": [
        "Yoon Kim"
      ],
      "year": "2014",
      "venue": "EMNLP"
    },
    {
      "citation_id": "13",
      "title": "Emonet: Fine-grained emotion detection with gated recurrent neural networks",
      "authors": [
        "Muhammad Abdul-Mageed",
        "Lyle Ungar"
      ],
      "year": "2017",
      "venue": "ACL"
    },
    {
      "citation_id": "14",
      "title": "Improving language understanding by generative pre-training",
      "authors": [
        "Peters"
      ],
      "year": "2018",
      "venue": "NAACL"
    },
    {
      "citation_id": "15",
      "title": "Poria et at.] Soujanya Poria, Erik Cambria, and Alexander F. Gelbukh. Deep convolutional neural network textual features and multiple kernel learning for utterance-level multimodal sentiment analysis",
      "authors": [
        "Sun"
      ],
      "year": "2015",
      "venue": "EMNLP-IJCNLP"
    },
    {
      "citation_id": "16",
      "title": "Emotion detection on TV show transcripts with sequencebased convolutional neural networks",
      "authors": [
        "Zahiri",
        "M Choi ; Sayyed",
        "Jinho Zahiri",
        "Choi"
      ],
      "year": "2018",
      "venue": "AAAI"
    },
    {
      "citation_id": "17",
      "title": "Multi-turn response selection for chatbots with deep attention matching network",
      "authors": [
        "Zhou"
      ],
      "year": "2018",
      "venue": "ACL"
    },
    {
      "citation_id": "18",
      "title": "Transformer-xl: Attentive language models beyond a fixed-length context",
      "authors": [
        "Dai"
      ],
      "year": "2019",
      "venue": "ACL"
    },
    {
      "citation_id": "19",
      "title": "Universal transformers",
      "authors": [
        "Dehghani"
      ],
      "year": "2018",
      "venue": "ICLR"
    },
    {
      "citation_id": "20",
      "title": "Contextualized non-local neural networks for squence learning",
      "authors": [
        "Liu"
      ],
      "year": "2018",
      "venue": "AAAI"
    },
    {
      "citation_id": "21",
      "title": "Google's neural machine translation system: Bridging the gap between human and machine translation",
      "authors": [
        "Wu",
        "Mike Wu",
        "Zhifeng Schuster",
        "Chen",
        "V Quoc",
        "Mohammad Le",
        "Wolfgang Norouzi",
        "Maxim Macherey",
        "Yuan Krikun",
        "Qin Cao",
        "Klaus Gao",
        "Macherey"
      ],
      "year": "2016",
      "venue": "Google's neural machine translation system: Bridging the gap between human and machine translation",
      "arxiv": "arXiv:1609.08144"
    },
    {
      "citation_id": "22",
      "title": "Emotionx-ar: CNN-DCNN autoencoder based emotion classifier",
      "authors": [
        "Sopan Khosla",
        "Khosla"
      ],
      "year": "2018",
      "venue": "ACL"
    },
    {
      "citation_id": "23",
      "title": "Socialnlp 2018 emotionx challenge overview: Recognizing emotions in dialogues",
      "authors": [
        "Ku ; Chao-Chun Hsu",
        "Lun-Wei Hsu",
        "P Ku ; Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2014",
      "venue": "ICLR"
    }
  ]
}