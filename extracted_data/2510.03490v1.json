{
  "paper_id": "2510.03490v1",
  "title": "Seer: The Span-Based Emotion Evidence Retrieval Benchmark",
  "published": "2025-10-03T20:15:24Z",
  "authors": [
    "Aneesha Sampath",
    "Oya Aran",
    "Emily Mower Provost"
  ],
  "keywords": [
    "and false positives in neutral text."
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "We introduce the SEER (Span-based Emotion Evidence Retrieval) Benchmark to test Large Language Models' (LLMs) ability to identify the specific spans of text that express emotion. Unlike traditional emotion recognition tasks that assign a single label to an entire sentence, SEER targets the underexplored task of emotion evidence detection: pinpointing which exact phrases convey emotion. This span-level approach is crucial for applications like empathetic dialogue and clinical support, which need to know how emotion is expressed, not just what the emotion is. SEER includes two tasks: identifying emotion evidence within a single sentence, and identifying evidence across a short passage of five consecutive sentences. It contains new annotations for both emotion and emotion evidence on 1200 real-world sentences. We evaluate 14 open-source LLMs and find that, while some models approach average human performance on single-sentence inputs, their accuracy degrades in longer passages. Our error analysis reveals key failure modes, including overreliance on emotion keywords and false positives in neutral text.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "We introduce the Span-based Emotion Evidence Retrieval (SEER) Benchmark, which evaluates Large Language Models (LLMs) on their ability to identify which spans of text express emotion in real-world discourse. SEER consists of two tasks: identifying emotion evidence within (1) a single sentence or (2) a short passage of five sentences. These span-level tasks differ from traditional emotion recognition benchmarks, which assign a single label to an entire utterance and do not isolate the exact phrases where emotion is expressed. SEER contains new annotations on 1200 real-world sentences, making it comparable in size to related emotion benchmarks  (Sabour et al., 2024) . For Task 1 (single sentence), we use GPT-4.1  (Achiam et al., 2023)  with human verification to label sentencelevel emotion, with emotion evidence spans labeled by humans only. For Task 2 (five-sentence context), both the emotion labels and evidence spans are labeled by humans only. Figure  1  illustrates the SEER benchmark.\n\nEmotion evidence refers to the spans of text that reveal a speaker's emotional state  (Poria et al., 2021) . Identifying such spans is critical for applications like empathetic dialogue and clinical telehealth sessions, where responses depend not just on knowing what emotion is present, but how it is expressed linguistically. For example, knowing that a person is 'sad' is less actionable than knowing that they said, 'nobody cares anymore.  '  Most prior work frames emotion recognition either as a sentence-level classification task  (Bharti et al., 2022; Alvarez-Gonzalez et al., 2021; Wagner et al., 2023)  or as word-level tagging  (Ito et al., 2020; Li et al., 2021) . Sentence-level labels obscure which phrases convey emotion, while wordlevel tags often over-fragment emotionally coherent expressions  (Hosseini and Staab, 2024) . Spanlevel annotation offers a middle ground: localized enough for interpretability, but flexible enough to capture multi-word emotion cues.\n\nRelated work in speech emotion recognition focuses on identifying when an emotion occurs in audio (e.g., at a certain timestamp), without linking those signals to the words used  (Parthasarathy and Busso, 2016; Aldeneh and Provost, 2017) . As a result, these approaches cannot answer which parts of the linguistic content express emotion.\n\nProgress on span-based emotion evidence detection has been limited by two main challenges: (1) a lack of datasets with span-level annotations (most provide only utterance-level labels  (Busso et al., 2008; Lotfian and Busso, 2017) ), and (2) a lack of datasets grounded in real-world discourse  (Poria et al., 2019; Busso et al., 2008) .\n\nWe evaluate 14 open-source LLMs on the SEER benchmark. Our results show that while several models approach average human performance in single-sentence settings, their accuracy declines in multi-sentence contexts. Key failure modes include fixation on explicit emotion keywords (e.g., 'grateful') and false identification of emotion spans in neutral text. Future work could leverage SEER's span-level annotations to build models with better multi-word emotion identification and explore techniques to incorporate broader context to discourage keyword-matching. These directions can lead to LLMs that can more reliably pinpoint emotion expression in real-world discourse.\n\nWe publicly release all new annotations. 1 Users must obtain access to the original datasets separately before working with the full SEER benchmark to comply with licensing requirements.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Preliminaries",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Definition Of Emotion",
      "text": "Emotion refers to a complex reaction involving experiential, behavioral, and physiological components, typically triggered by a personally meaningful event or situation (American Psychological Association). Theories of emotion organize these within systematic frameworks.\n\nThe categorical emotion theory posits that basic emotions developed in response to evolutionary needs. These emotions can include happiness, surprise, fear, sadness, anger, and disgust  (Ekman, 1992) . The dimensional emotion theory maps emotion along valence (negative to positive) and activation (calm to excited)  (Harmon-Jones et al., 2017; Russell, 1979) . Since text provides a stronger signal for valence compared to activation (Wagner   1 https://github.com/chailab-umich/SEER et al., 2023), we focus on valence only for dimensional emotion. We conduct error analysis on the SEER tasks for categorical emotions and valence.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Definition Of Emotion Evidence",
      "text": "Emotion evidence is defined as \"a part of the text that indicates the presence of an emotion in the speaker's emotional state. It acts in the real world between the text and the reader\"  (Poria et al., 2021) . This should be distinguished from emotion cause, which is the \"part of the text expressing the reason for the speaker to feel the emotion given by the emotion evidence\"  (Poria et al., 2021) .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Emotion Hotspot Detection",
      "text": "Most emotion recognition work has targeted utterance-level classification, whereas emotion hotspots identify specific points at which emotion shifts and intensifies  (Huang and Epps, 2016; Huang et al., 2015; Parthasarathy and Busso, 2016) . Existing work on emotion hotspot detection has predominantly leveraged audio data. Some methods identify deviations from a baseline emotion state in valence-activation time-series traces  (Parthasarathy and Busso, 2016, 2018) . Other approaches partition an audio stream to answer 'which emotion appears when?'  (Stemmer et al., 2023; Wang et al., 2023) . The output of these audio-based methods",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Benchmark",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Focus Data Type",
      "text": "EmoBench  (Sabour et al., 2024)  scenario understanding H EmoLLMs  (Liu et al., 2024)  emotion recognition R EmotionQueen  (Liu et al., 2024)  empathy generation S SEER (ours) emotion evidence R is a set of timestamps and corresponding emotion. They do not identify the specific spans used to express it. Our work addresses this complementary task: identifying the discrete, linguistic spans of emotion evidence directly from text, a capability that remains underexplored.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Emotion Benchmarking In Llms",
      "text": "Benchmarks such as EmoBench, EmotionQueen, and EmoLLMs evaluate LLMs on emotion-related tasks but differ from SEER (see Table  1 ). SEER evaluates the capability of LLMs to identify the precise spans where emotion evidence occurs. The other benchmarks have different goals, which we outline here. EmoBench  (Sabour et al., 2024)  evaluates emotional reasoning through hand-crafted scenarios with multiple choice answers. Given an input, \"I have a teacher who gives the F grade as the highest mark... I saw he gave me an F,\" the LLM must identify the emotion of the speaker, and also the cause. This tests emotion recognition and emotion cause recognition, but uses hand-crafted scenarios. EmotionQueen  (Chen et al., 2024)  evaluates LLMs' ability to generate empathetic responses. Given a statement such as \"I've been busy with work all day,\" an empathetic model response might be \"Do you feel overwhelmed? Have you tried some ways to relax?\", which provides emotional support, whereas a reply like \"Hard work!\" is considered non-empathetic. EmoLLMs  (Liu et al., 2024)  evaluates sentence-level emotion classification, where models assign emotion labels (e.g., happy, angry) to individual sentences. However, in all cases, these benchmarks do not localize the precise text that expresses emotion.\n\nSEER tasks models with pinpointing the exact phrases that convey emotion, in both singlesentence and five-sentence passage settings. This focus on emotionally salient text spans grounded in real-world language fills a critical gap in existing emotion benchmarks.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "The Seer Benchmark",
      "text": "The goal of SEER is to assess emotion evidence identification capabilities in LLMs. SEER comprises two primary tasks: single-and multisentence emotion evidence identification (Figure  1 ). All data are drawn from non-acted transcriptions of real-world speech (see Section 5) and annotated with both emotion labels and emotion evidence spans (see Sections 5.2 and 5.3 for annotation protocol).",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Task Versions: Retrieve And Highlight",
      "text": "Each task has two versions: retrieve and highlight. In retrieve, the LLM must output a series of spans. The output can be empty if the LLM identifies no spans of emotion evidence. In highlight, the LLM must output the entire input passage, with the spans surrounded by '**' markers to indicate the start and end of an emotion evidence span.\n\nThese two prompt formats reflect real-world needs: Retrieve supports applications like evidence grounding or snippet retrieval, while Highlight supports scenarios requiring interpretable, in-context marking. To validate both formats, we conduct controlled prompting experiments with simple handcrafted inputs (see Appendix B). Success in these setups suggests that failures on SEER tasks stem from challenges in processing real-world emotion, not formatting or retrieval deficiencies.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Task 1: Single-Sentence Emotion Evidence",
      "text": "LLMs must identify all emotion evidence that occurs within a single, non-neutral sentence. The goal is to isolate short-form emotion expression.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Task 2: Multi-Sentence Emotion Evidence",
      "text": "LLMs must identify all emotion evidence that occurs within a series of five consecutive sentences. We select five sentences as a starting point. This length is manageable for annotation and analysis, yet long enough to see whether models can track emotion expression across coherent discourse. The goal is to test emotion evidence tracking in longer, more variable contexts, where the overall emotion may shift over the course of the passage.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Datasets",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Pre-Existing Datasets",
      "text": "We use samples from MSP-Podcast  (Lotfian and Busso, 2017)  and MuSE  (Jaiswal et al., 2019)  for SEER. They contain non-acted speech (rather than scripted performances). We generate transcripts using Whisper 2  (Radford et al., 2023) . MSP-Podcast contains non-acted English conversational speech from podcasts and includes both categorical and dimensional emotion annotations (version 1.11)  (Lotfian and Busso, 2017) . We use the subset of the data that overlaps with the MSP-Conversation corpus version 1.1  (Martinez-Lucas et al., 2020) , which contains continuous timeseries trace annotations of dimensional emotion on speech. This is to allow for future research combining the strengths of both continuous and sentencelevel labels. We use samples in the \"Test1\" split, totaling 2249 utterances.\n\nThe Multimodal Stressed Emotion (MuSE) dataset contains non-acted audiovisual English monologues  (Jaiswal et al., 2019) . It includes crowdsourced annotations for dimensional emotion. It totals 2648 utterances.\n\nWe collect new text-based annotations for categorical emotion and valence to align with the LLMs' input modality. The original MSP-Podcast and MuSE labels are audio-or video-based and the labels may not reflect textual cues. Using the original labels risks penalizing models for modality mismatch rather than genuine errors. In addition, MuSE lacks categorical emotion labels. Further, LLMs in Task 2 receive five-sentence context, which was not available to original annotators. Annotation details are in Sections 5.2 and 5.3.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Task 1 Data Annotation And Selection",
      "text": "Single Sentence Filtering. We filter the datasets to retain samples where utterances contain a single sentence only. We use NLTK  (Bird et al., 2009)  to tokenize by sentence. This leaves 1494 samples from MSP-Podcast and 1687 samples from MuSE.\n\nGPT Labeling and Filtering. We use GPT-4.1  (Achiam et al., 2023)  to ease the labeling burden. Prior work has shown GPT's capabilities for emotion labeling  (Niu et al., 2024; Tarkka et al., 2024) . We complement it with human verification.\n\nWe use the \"gpt-4.1\" model via the Azure Ope-nAI API to annotate the 1494 sentences from MSP-Podcast and 1687 sentences from MuSE for both valence (positive, negative, neutral) and categorical emotion  (happy, sad, disgust, contempt, fear, angry, surprise, neutral) . We select the eight categorical emotions that match the original label space from 2 openai/whisper-large-v2 MSP-Podcast to encourage future research in the audio modality. The prompt is shown in Appendix B.5. We drop all samples that GPT-4.1 labeled as neutral. This leaves 488 sentences from MSP-Podcast and 854 sentences from MuSE. Human Verification and Filtering. Two trained student workers then independently indicated agreement or disagreement with the GPT-4.1 labels. This study is IRB-approved (HUM00273067). The annotator and GPT-4.1 agreement is in Table  2 .\n\nWe then filtered to only retain sentences where both annotators marked agree for both the categorical and valence GPT-4.1 labels of a single sentence. This leaves 215 samples from MSP-Podcast and 703 samples from MuSE. Emotion Class Balancing. As a final filtering step, we balance the samples across the emotion classes by downsampling from over-represented classes. This leaves 30 samples per emotion class, except for surprise, which is slightly underrepresented with 20 samples, totaling 200 samples (103 from MSP-Podcast, 97 from MuSE). Since we balance by categorical emotion, valence is unbalanced since most of the emotion classes are negative. There are 155 negative and 45 positive samples. This is the final set of samples for Task 1.\n\nEmotion Evidence Annotation. The student workers received a short training to define emotion evidence and were instructed to openly discuss examples. In the final annotation step, they jointly identified and labeled the gold spans of emotion evidence by discussing and highlighting the emotion evidence in the input text. This study is IRB-approved (omitted).",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Task 2 Data Annotation And Selection",
      "text": "Task 2 requires five consecutive sentences in order to maintain semantic cohesion. We first split the original 2249 and 2648 utterances from MSP-Podcast and MuSE, respectively, into single sen-tences using NLTK. We retain instances with a series of five consecutive sentences. We then remove overlapping instances (i.e., only including sentences  4-8 and 9-13, instead of 4-8 and 5-9) . This totals 200 sets of five consecutive sentences.\n\nThe trained student workers were given each passage of five consecutive sentences, then discussed and jointly annotated the emotion (categorical and valence) of each sentence. They had access to all five sentences when annotating each sentence. This annotation step is necessary since the context of prior sentences can impact the emotion perception of a target sentence  (Jaiswal et al., 2019) . This results in emotion labels for each sentence within the series of five sentences. We did not use GPT for multi-sentence emotion annotation, since it is not validated in prior work  (Niu et al., 2024) .\n\nFor the final annotation step, the trained student workers jointly identified and labeled the gold spans of emotion evidence by discussing and highlighting their answers, as in Task 1.\n\nEmotion Class Distribution Of the 1000 sentences (200 samples of 5 sentences each), they are 42.5% neutral, 27.8% happy, 11.6% sad, 5.1% surprise, 5.1% fear, 4% angry, 3.2% contempt, and 0.7% disgust. For valence, they are 42.7% neutral, 30.7% positive, and 26.6% negative. We do not perform balancing due to the nature of the emotion shifts within passages of longer discourse. The most common emotion transition between adjacent sentences are maintaining the current emotion or transitioning to and from neutral. The exact distribution of emotion transitions is in Appendix C.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "We use two primary evaluation metrics: token-level F1-score (F1) and cosine similarity (Sim). F1 is a common metric for span-extraction tasks  (Rajpurkar et al., 2016) . It serves as a \"fuzzy-match\" metric. In addition to F1, embedding similarity metrics have also emerged as a way to assess semantic similarity as opposed to exact matches  (Zhang* et al., 2020; Arabzadeh et al., 2024) , which can serve to reduce penalties for differences in span boundaries in SEER tasks. We use sentence-BERT  (Reimers and Gurevych, 2019)  to embed spans, and then compute the cosine similarity between the embeddings of the gold and predicted spans. We do not report exact-match accuracy due to the subjective nature of span boundaries.\n\nWe use the Kuhn-Munkres Algorithm to align gold and predicted spans, which finds the optimal one-to-one matching between two sets  (Luo, 2005) . For example, consider two gold spans {g 1 , g 2 } and three predicted spans {p 1 , p 2 , p 3 }. The algorithm considers all possible matchings: (g 1 , p 1 ), (g 2 , p 2 ), (g 1 , p 2 ), (g 2 , p 1 ), etc. Each pairing is scored by ranking the similarity of the aligned span pairs, where similarity is defined as ϕ(g, p) = F 1 (g, p), following  Luo (2005) . Since g and p are of unequal size, one span in p remains unmatched.\n\nWe compute a modified score for both F1 and Sim that penalizes a model for predicting an incorrect number of spans. This approach ensures that a high score is achieved only when a model identifies the correct spans and the correct number of them. The score is calculated as the sum of the metrics from the aligned spans, normalized by the greater of the number of gold or predicted spans. This penalizes both irrelevant predicted spans (false positives) and missed gold spans (false negatives). The formula for metric M (Sim or F1) is:\n\n7 Implementation Details",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Prompts",
      "text": "We evaluate LLMs in two zero-shot prompt settings: Base prompting (Base) and chain-of-thought prompting (CoT), as in prior emotion benchmarking  (Sabour et al., 2024) . See Appendix B.5, Tables  11  and 10  for the exact prompts.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Llms Evaluated",
      "text": "We evaluate the performance of 14 LLMs on the SEER benchmark. We select LLMs from the LLaMA  (Dubey et al., 2024) , Qwen  (Yang et al., 2025 , 2024 ), Phi4 (Abdin et al., 2024 , 2025) , and Gemma3  (Team et al., 2025)  families.\n\nWe select models that achieve F1 ≥ 0.5 on prompting experiment three for further evaluation on the main SEER tasks. For the retrieve prompt, we retain all models with at least 1.7B parameters. For the highlight prompt, we retain models with at least 14B parameters, except for Qwen2.5-14B.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Experimental Setup",
      "text": "We use the huggingface transformers library 3  and load models in BF16. The full list of model checkpoint names is shown in Appendix Table  13 . We use the default hyperparameters and allow a maximum of three retries. For each model, we report the average and standard deviation across five runs.\n\nWe run experiments on an HPC with NVIDIA A40 GPUs. We use one GPU for models in the 0.5-14B range, two for 32B, and four for 70-72B.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Task 1: Single-Sentence Emotion Evidence",
      "text": "The results are shown in Performance drops for the Highlight prompt compared to Retrieve. This pattern is consistent with our prompting experiments, where we observe a typical performance drop of 0.3-0.4 F1 when comparing the same samples under Retrieve and Highlight settings (see Appendix B). This drop is expected, as Highlight requires models to reproduce the input text verbatim with added markup. Any hallucination results in an automatic score of 0. However, models in the 14-32B range drop only about 0.15 F1, while the 70-72B models drop about 0.2 F1. All models tested with the Highlight prompt perform better or similarly under the CoT prompt than the Base prompt.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Task 2: Multi-Sentence Emotion Evidence",
      "text": "The results are in Table  4 . Notably, no model exceeds 0.41 F1 in any prompt version on Task 2, underscoring the need for models capable of emotion evidence identification in extended passages.\n\nThe Qwen-family models again perform best on Retrieve-Base, with Qwen3-14B and Qwen3-32B leading with 0.406 and 0.405 F1, respectively. LLaMA3.2-3B remains the weakest model (0.205 F1), and the larger LLaMA3.1/3.3-70B variants are again outperformed by most smaller models. These results further reinforce that model size does not directly predict performance.\n\nUnlike Task 1 Retrieve, many models improve with CoT prompting in Task 2 Retrieve, including LLaMA3.2-3B, Phi4-Mini-3.8B, Phi4-14B, Qwen3-32B, LLaMA3.1-70B, and LLaMA3.3-70B. This suggest that CoT prompting is more effective for longer contexts, where reasoning steps may help localize relevant spans. For Highlight, this pattern persists: all models benefit from CoT prompts compared to Base. Highlight performance still falls short of Retrieve, as in Task 1.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Human Performance Comparison",
      "text": "We collected a crowdsourced human performance baseline for comparison with LLMs (IRBapproved). Details are in Appendix D. Each example received three independent annotations. The results are in Tables  3  and 4  in the 'Human Annotator' rows. The 'Average' row reflects the mean performance across all annotators, while the 'Best' row reports the per-sample maximum: i.e., the annotation associated with the best-performing crowdsourced annotator, compared to the gold annotations, over each sample.\n\nMany LLMs outperform Average, but only Qwen3-32B in Task 1 slightly exceeds the Best-Human. This suggests that at least one annotator often identifies the emotion evidence in the gold labels, but the annotations of crowdsourced annotators are of variable quality. It is expected that untrained workers underperform relative to expert annotators given the nuance of emotion evidence identification. The LLM-Best-Human gap is larger in Task 2 (about .1 F1) than in Task 1 (about .01 F1), indicating that a crowdsourced annotator outperforms LLMs in longer contexts.\n\n10 Error Analysis 10.1 Base and Chain-of-Thought Prompt CoT prompting does not consistently improve performance across models in the Retrieve prompt setting (see Tables  3  and 4 ). This aligns with findings from  Sabour et al. (2024) , who reported that CoT prompting reduced or marginally changed performance on Emotion Intelligence tasks.\n\nIn Task 1 Retrieve, CoT prompting yields improvements for LLaMA3.1-70B and LLaMA3.3-70B, but degrades performance for all smaller models except LLaMA3.2-3B. In contrast, Task 2 Retrieve shows a less consistent trend: LLaMA3.2-3B, Phi4-Mini-3.8B, Qwen3-8B, Phi4-14B, Qwen3-32B, LLaMA3.1-70B, and LLaMA3.3-70B benefit from CoT prompting. This may reflect the nature of the longer input text in Task 2, where reasoning could assist span identification in longer passages.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Hallucination Rates",
      "text": "We define hallucination rate as the fraction of predicted spans that do not appear in the original text.\n\nWe normalize each predicted span by removing punctuation and converting to lowercase, and then compare it to the similarly-normalized transcription. We mark a span as 'hallucinated' if it does not appear exactly as it is in the normalized text. Models with lower hallucination scores reliably achieved higher F1 and similarity metrics. The Qwen family consistently showed the lowest hallucination rates across tasks and prompts.\n\nSmaller models Phi4-mini-3.8B, LLaMA3.1-8B, and LLaMA3.2-3B exhibited the worst hallucination rates (16.3%, 15.2%, and 12.3% for Task 1 Retrieve-Base). CoT had minimal impact on hallucination. These three models along with Qwen3-1.7B also exhibited high hallucination rates on Task 2 Retrieve-Base, contributing to their poor performance. The models evaluated in the Highlight prompts for both Task 1 and 2 consistently exhibit low hallucination rates (≤ 5%).",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Errors By Emotion Category",
      "text": "We discuss performance by emotion category on the Base prompts only. See Figures  3, 4 , 5 and 6 in Appendix E for visualizations.\n\nFor Task 1, performance by emotion category is variable. In both Retrieve and Highlight, emotion evidence identification on sentences expressing disgust and anger perform best (Figures  3a  and 4a ). The best performing model, Qwen3-32B, outperforms all other models on disgust and happy sentences in retrieve (Figure  3a ), and outperforms other models on disgust, contempt, angry, and happy for highlight (Figure  4a ). The Qwen model family consistently performs better on negative sentences compared to positive sentences  (Figures 3b and 4b ). This pattern is not consistent for the LLaMA, Gemma, and Phi families.\n\nFor Task 2, performance by emotion category resembles that of Task 1. However, unlike Task 1, Task 2 includes neutral sentences. The primary source of performance drop is the incorrect marking of emotion evidence in these neutral sentences. As shown in Figures  5c  and 6c  in Appendix E, models falsely identify emotion evidence in up to 50% of neutral sentences. This high rate of neutral false positives degrades performance across models.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Errors From Emotion Keyword Fixation",
      "text": "We probe over-reliance on salient emotion words by checking if models extract emotion keywords in isolation rather than the full span. We use the Empath lexicon  (Fast et al., 2016)  to identify instances where transcripts contain terms from the positiveemotion and negative-emotion categories. In 61 of the 200 sentences in Task 1 and 84 of the 1000 sen- tences in Task 2, a gold span contains an emotion keyword. We define a fixation as any prediction in which the model identifies only the keyword itself (e.g., predicting 'disgust' when the gold span is 'I would like to state my utter disgust.').\n\nFor both tasks, this error pattern appears most prominently in Highlight. For Highlight-Base, LLaMA3.1-70B and LLaMA3.3-70B exhibit high fixation rates of 27.9% and 26.6% for Task 1, and 36.2% and 53.6% for Task 2, respectively, where samples with an emotion keyword default to isolated words despite the emotion expressions themselves consisting of longer spans. In contrast, Qwen3-32B, the best-performing model, has only 1.6% of samples with this behavior in Task 1 and 5.7% in Task 2 (lowest fixation rate of all models). For Retrieve, most models have 0% fixation rates in Task 1, with the exception of LLaMA3.2-3B (3.6%), Qwen3-4B (1%), Phi4 (0.3%), LLaMA3.1-70B (1.6%), and LLaMA3.3-70B (6.9%). Similarly, in Task 2, the highest fixation rate is 7.1% (LLaMA3.2-3B), with most models falling within 0-3% fixation (except for Qwen3-4B with 5.7%).\n\nCoT prompting partially mitigates this behavior for larger models. Fixation rates for LLaMA3.1-70B and LLaMA3.3-70B drop to 15.4% and 16.1%, for Task 1 Highlight-CoT and to 14.3% and 36.2% for Task 2, respectively, suggesting that reasoning steps can encourage more holistic span identification. However, this trend does not generalize across scales. In Task 1 Retrieve-CoT, smaller models Qwen3-1.7B and Qwen3-4B show increased fixation under CoT (11.8% and 9.8%, respectively), despite minimal errors with the base prompt.\n\nThese results underscore that keyword fixation is a nuanced failure mode. While CoT can guide larger models toward more nuanced span identification, it may also backfire in smaller models by drawing attention to more obvious word-level cues. Crowdsourced annotators also exhibit about 3% fixation rate in both tasks, suggesting that even human annotators are prone to this error mode.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we propose the SEER Benchmark for evaluating LLM capability in emotion evidence identification. SEER comprises two tasks: single-sentence and multi-sentence emotion evidence identification in real-world discourse. We collect new annotations for 1200 sentences for emotion category, valence, and evidence. We evaluate SEER on 14 open-source LLMs and conduct a comprehensive error analysis. We find that models can somewhat reliably identify emotion evidence in single sentences, however, these models falsely identify emotion evidence in neutral sentences in multi-sentence contexts. Key error modes also include fixation on emotion keywords and modification of the input text (hallucination). Of the models we evaluate, Qwen3-32B performs the best in both SEER tasks.\n\nFuture work may explore whether performance on SEER aligns with standard emotion classification by evaluating the same LLMs on related tasks. SEER can also be evaluated on closed-source and reasoning models. Finally, SEER could be adapted to the audio modality by using the original datasets' annotations for evaluation on audio-LLMs.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Limitations",
      "text": "Sample Size. SEER is limited to 200 samples in Task 1 and 200 samples in Task 2. While this contains high-quality annotations for emotion evidence, emotion valence, and emotion category, and also is similar to the size of other emotion benchmarks  (Sabour et al., 2024) , we acknowledge that our dataset scale is limited, and could benefit from additional samples. Prompt Tuning. We acknowledge that LLM outputs are highly sensitive to input prompts and that additional techniques could influence performance. We conducted extensive prompting experiments to mitigate this effect. Prompt design adjustments may impact the exact numerical scores, however we argue that they are unlikely to alter the overall trends observed across the tasks, as observed when comparing base and chain-of-thought prompting results.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "A Hand-Crafted Sentences",
      "text": "We design a set of hand-crafted sentences for the prompting experiments (detailed in Appendix B). The sentences include both neutral and emotion expressions. This allows us to evaluate whether models can reliably execute instruction-following behavior and return outputs in the expected format without the challenges of subtle and potentially ambiguous real-world language. Demonstrating robust performance under these conditions ensures that any failures observed in the main SEER tasks, which use real-world discourse, are not due to fundamental retrieval limitations or format misalignment, but instead reflect genuine challenges in understanding real-world emotion expression.\n\nThe sentences contain no repeating bi-grams across both the neutral and emotion sentences. The reasoning for this is described in Appendix B.3. We construct ten neutral sentences, two sentences for each categorical emotion in MSP-Podcast  (Lotfian and Busso, 2017) (happy, sad, disgust, contempt, fear, anger, surprise) , and five sentences for each valence (positive, negative). The full list of sentences is shown in Table  5 .",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "B Prompting Experiments",
      "text": "In this section, we detail our experiments for the retrieval and highlight prompting styles. These allow us to identify which LLMs are capable of producing responses in our desired format.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "B.1 Motivation",
      "text": "In some applications, it may be sufficient to have models retrieve only exact quotes of emotion evidence. In others, it may be necessary to place the emotion evidence back within the original context in which it was communicated. We observe that smaller models are generally capable of reproducing given text without hallucination, however they fail to reliably \"highlight\" a sentence within the given text. In order properly to evaluate emotion evidence capabilities in both smaller and larger models, we develop two sets of prompts: retrieval and highlight. The retrieval prompts evaluate LLM capability in retrieving exact quotes of emotion evidence only. The highlight prompts evaluate LLM capability in highlighting the exact regions of emotion evidence while also retrieving the original text without hallucination. These experiments are summarized in Table  6 .\n\nThe goal of these experiments is to assess retrieval capacity, not to assess ability to identify subtle emotion evidence. Thus, for the prompting experiments, we use hand-crafted data only. These sentences are designed to unambiguously express either neutrality or a categorical emotion. See Appendix A for the full list of sentences.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "B.2 Experiments",
      "text": "We include four experiments. Experiment 0 is a baseline experiment to assess LLM capability in reproducing passages without any modification. The LLM must reproduce the exact input text, unmodified. Experiment 1 requires the LLM to identify one span of text that occurs within the original text. The target span is provided in the prompt. This experiment uses neutral sentences only. Experiment 2 requires the LLM to identify all spans of emotion evidence in the text. The LLM is given three span options in the prompt itself, in which either one or two of the three spans are correct. Experiment 3 requires the LLM to identify all spans of emotion evidence in the text, without any options provided in the prompt.\n\nEach experiment has a retrieval and highlight version, except for Experiment 0, since there is no span identification involved. In the retrieval versions, the LLM must retrieve the specific spans of text only. In the highlight versions, the LLM must return the entire input text, and mark the specific spans by surrounding the spans with '**' markers.\n\nWe set the number of sentences (n_sentences) that the LLM must retrieve in  [1, 10] , to identify if errors stem from the length of the input/output or from the nature of the experiment. For Experiment 0, we create ten variants for each n ∈ n_sentences, where n neutral sentences are randomly samples and randomly shuffled. This totals 100 samples. For Experiment 1, we use the same logic as Experiment 0, except with n_sentences ∈ [2, 10], since we need at least two sentences in order to be able to identify the target sentence. This totals 90 samples. For Experiment 2 and 3, which contain the same samples, we also set n_sentences ∈ [2, 10]. We create two variants for each emotion sentence, where three sets of neutral sentences are randomly selected and shuffled. The emotion sentence is randomly placed within the neutral sentences. This totals 432 samples.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "B.3 Metrics And Constraints",
      "text": "We use metrics Exact-match accuracy (EM) and token-level F1-Score (F1) to evaluate LLM performance on the prompting experiments. These Table  6 : Overview of prompting experiments. Each experiment tests a different task objective. Retrieval variants require the LLM to output the relevant span. Highlight variants require the LLM to reproduce the input with the relevant span marked using a delimiter (**...**).",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Experiment Task Description",
      "text": "Exp 0\n\nReproduce the input text exactly, with no modifications. Exp 1\n\nIdentify one span given the exact span in the instructions. Exp 2 Identify all spans of emotion evidence given three span options in the instructions. Exp 3 Identify all spans of emotion evidence in the original text.\n\nTable  7 : Exact-match accuracy (EM) and F1 scores for prompting Experiment 0 (baseline reproduction). We report the average and standard deviation over five runs.\n\nModel EM F1\n\n0.5-2B Qwen 3 0.6B 1.000 ± .000 1.000 ± .000 Gemma 3 1B 1.000 ± .000 1.000 ± .000 LLaMA 3.2 1B 0.730 ± .012 0.962 ± .011 Qwen 3 1.7B 0.832 ± .013 0.943 ± .005 3-4B LLaMA 3.2 3B 0.990 ± .000 1.000 ± .000 Phi 4 Mini 3.8B 0.710 ± .019 0.956 ± .003 Gemma 3 4B 1.000 ± .000 1.000 ± .000 Qwen 3 4B 1.000 ± .000 1.000 ± .000",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "8B",
      "text": "LLaMA 3.1 8B 1.000 ± .000 1.000 ± .000 Qwen 3 8B 1.000 ± .000 1.000 ± .000 14B Phi 4 14B 0.986 ± .015 0.998 ± .002 Qwen 2.5 14B 1.000 ± .000 1.000 ± .000 Qwen 3 14B 1.000 ± .000 1.000 ± .000",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "32B",
      "text": "Qwen 3 32B 1.000 ± .000 1.000 ± .000",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "70-72B",
      "text": "LLaMA 3.1 70B 1.000 ± .000 1.000 ± .000 LLaMA 3.3 70B 1.000 ± .000 1.000 ± .000 Qwen 2.5 72B 1.000 ± .000 1.000 ± .000 metrics are used in the SQuAD benchmark for question-answering  (Rajpurkar et al., 2016) . We use these to compare the LLM output to the expected output.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "B.4 Results",
      "text": "The results are shown in Tables  7, 8 , and 9.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "B.5 Prompts Provided To Llms",
      "text": "The system and user prompts used for the main SEER tasks are shown in Table  11  and 10 . The system prompts and user prompts used for the prompting experiments are shown in Tables  11  and 12 .\n\nFor GPT annotation, we follow the approach of  Niu et al. (2024)  and provide the instructions and labeling schema in the system prompt, and provide the transcript itself in the user prompt. The GPT annotation system prompt is: \"You are an emotionally-intelligent and empathetic agent. You will be given a piece of text, and your task is to identify the emotions expressed by the speaker. You are only allowed to make one selection from the following emotions: {set of emotions}. Do not return anything else.\"",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "C Task 2 Emotion Transition Distribution",
      "text": "Figure  2  describes the emotion transitions between adjacent sentences for Task 2. The most common transition is neutral to neutral. Figure  2a  shows the valence transitions, and Figure  2b  shows the categorical emotion transitions.",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "D Annotator Recruitment",
      "text": "We recruited annotators using the Prolific platform  4  . We separated the 200 samples per task into surveys with 20 samples each to reduce annotation fatigue. We recruited three annotators per survey and paid them at a rate of $10 an hour. We recruited annotators that were (1) native English speakers, (2) residents of the USA.\n\nThe instructions were as follows: \"In the following task, you will be asked to identify emotionallyrelevant text. You will be presented with short passages and asked to identify the emotional text within the passage. Please use your mouse to highlight the regions of text that are emotionally salient. Ensure that you highlight only the specific text that expresses emotion.\" The participants were then asked to consent to the following: \"(1) I have read and understood the information above, (2) I understand I might see potentially offensive or sexual content, and (3) I want to participate in this research and continue with the study,\" before proceeding to the main task.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "E Emotion Category Errors",
      "text": "See Figures  3  and 4  for Task 1, 5 and 6 for Task 2.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "F Model Checkpoints",
      "text": "The exact model checkpoints from huggingface transformers library are in Table  13 .        -Never delete, normalize, split, merge, or alter any original character (letters, apostrophes, punctuation, whitespace).",
      "page_start": 15,
      "page_end": 17
    },
    {
      "section_name": "Subjective Emotion Only",
      "text": "-Only highlight spans that reveal the speaker's internal emotional state or attitude.\n\n-This includes:\n\n-Explicit emotion words -Implicit cues/phrases of feeling or reaction -Do not mark:\n\n-Purely factual or descriptive statements -Neutral descriptions of events without any sentiment 3. Self-Check -After inserting your markers, remove all ** and verify that the remaining text is identical to the input. Retry until it passes.\n\n-If the input is completely neutral, return it unchanged, with no markers.\n\n-Any pair of ** must surround the entire span.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Output",
      "text": "-Return only the marked text. No headers, no metadata, no removed, added, or modified words.",
      "page_start": 19,
      "page_end": 19
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: illustrates the",
      "page": 1
    },
    {
      "caption": "Figure 1: SEER includes two tasks: single- and multi-sentence emotion evidence identification. Each has two",
      "page": 2
    },
    {
      "caption": "Figure 3: a), and out-",
      "page": 7
    },
    {
      "caption": "Figure 4: a). The Qwen",
      "page": 7
    },
    {
      "caption": "Figure 2: describes the emotion transitions between",
      "page": 13
    },
    {
      "caption": "Figure 2: b shows the",
      "page": 13
    },
    {
      "caption": "Figure 2: Emotion transitions between adjacent sentences for Task 2.",
      "page": 15
    },
    {
      "caption": "Figure 3: Task 1 (Retrieve-Base). (a) Per-emotion F1 scores. (b) Per-valence F1 scores.",
      "page": 15
    },
    {
      "caption": "Figure 4: Task 1 (Highlight-Base). (a) Per-emotion F1 scores. (b) Per-valence F1 scores.",
      "page": 16
    },
    {
      "caption": "Figure 5: Emotion category errors in Task 2 Retrieve-Base.",
      "page": 19
    },
    {
      "caption": "Figure 6: Emotion category errors in Task 2 Highlight-Base.",
      "page": 20
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "EXAMPLE:EMOTIONEVIDENCEVSCAUSE(PORIAETAL.,2021)": "P_A:Ihavebeenacceptedintograduateschool!\nP_B:Whatanamazingaccomplishment!\nP_BLABEL: happy/positive\nCAUSE: acceptedintograduateschool\nEVIDENCE: amazingaccomplishment"
        }
      ],
      "page": 2
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Neel Joshi, and 1 others. 2025. Phi-4-reasoning technical report",
      "authors": [
        "Sahaj Marah Abdin",
        "Ahmed Agarwal",
        "Vidhisha Awadallah",
        "Harkirat Balachandran",
        "Lingjiao Behl",
        "Gustavo Chen",
        "Suriya De Rosa",
        "Mojan Gunasekar",
        "Javaheripi"
      ],
      "venue": "Neel Joshi, and 1 others. 2025. Phi-4-reasoning technical report",
      "arxiv": "arXiv:2504.21318"
    },
    {
      "citation_id": "2",
      "title": "Piero Kauffmann, and 1 others. 2024. Phi-4 technical report",
      "authors": [
        "Jyoti Marah Abdin",
        "Harkirat Aneja",
        "Sébastien Behl",
        "Ronen Bubeck",
        "Suriya Eldan",
        "Michael Gunasekar",
        "Russell Harrison",
        "Mojan Hewett",
        "Javaheripi"
      ],
      "venue": "Piero Kauffmann, and 1 others. 2024. Phi-4 technical report",
      "arxiv": "arXiv:2412.08905"
    },
    {
      "citation_id": "3",
      "title": "Shyamal Anadkat, and 1 others. 2023. Gpt-4 technical report",
      "authors": [
        "Josh Achiam",
        "Steven Adler",
        "Sandhini Agarwal",
        "Lama Ahmad",
        "Ilge Akkaya",
        "Florencia Leoni Aleman",
        "Diogo Almeida",
        "Janko Altenschmidt",
        "Sam Altman"
      ],
      "venue": "Shyamal Anadkat, and 1 others. 2023. Gpt-4 technical report",
      "arxiv": "arXiv:2303.08774"
    },
    {
      "citation_id": "4",
      "title": "Using regional saliency for speech emotion recognition",
      "authors": [
        "Zakaria Aldeneh",
        "Emily Provost"
      ],
      "year": "2017",
      "venue": "2017 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "5",
      "title": "Uncovering the limits of text-based emotion detection",
      "authors": [
        "Nurudin Alvarez-Gonzalez",
        "Andreas Kaltenbrunner",
        "Vicenç Gómez"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021"
    },
    {
      "citation_id": "6",
      "title": "Adapting standard retrieval benchmarks to evaluate generated answers",
      "authors": [
        "Negar Arabzadeh",
        "Amin Bigdeli",
        "Charles La Clarke"
      ],
      "year": "2024",
      "venue": "European Conference on Information Retrieval"
    },
    {
      "citation_id": "7",
      "title": "Text-based emotion recognition using deep learning approach",
      "authors": [
        "Santosh Kumar Bharti",
        "S Varadhaganapathy",
        "Rajeev Ku"
      ],
      "year": "2022",
      "venue": "Computational Intelligence and Neuroscience"
    },
    {
      "citation_id": "8",
      "title": "Natural language processing with Python: analyzing text with the natural language toolkit",
      "authors": [
        "Steven Bird",
        "Ewan Klein",
        "Edward Loper"
      ],
      "year": "2009",
      "venue": "Natural language processing with Python: analyzing text with the natural language toolkit"
    },
    {
      "citation_id": "9",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "10",
      "title": "Emotionqueen: A benchmark for evaluating empathy of large language models",
      "authors": [
        "Yuyan Chen",
        "Songzhou Yan",
        "Sijia Liu",
        "Yueze Li",
        "Yanghua Xiao"
      ],
      "year": "2024",
      "venue": "Findings of the Association for Computational Linguistics ACL 2024"
    },
    {
      "citation_id": "11",
      "title": "Angela Fan, and 1 others. 2024. The llama 3 herd of models",
      "authors": [
        "Abhimanyu Dubey",
        "Abhinav Jauhri",
        "Abhinav Pandey",
        "Abhishek Kadian",
        "Ahmad Al-Dahle",
        "Aiesha Letman",
        "Akhil Mathur",
        "Alan Schelten",
        "Amy Yang"
      ],
      "venue": "Angela Fan, and 1 others. 2024. The llama 3 herd of models",
      "arxiv": "arXiv:2407.21783"
    },
    {
      "citation_id": "12",
      "title": "An argument for basic emotions",
      "authors": [
        "Paul Ekman"
      ],
      "year": "1992",
      "venue": "Cognition & emotion"
    },
    {
      "citation_id": "13",
      "title": "Empath: Understanding topic signals in large-scale text",
      "authors": [
        "Ethan Fast",
        "Binbin Chen",
        "Michael Bernstein"
      ],
      "year": "2016",
      "venue": "Proceedings of the 2016 CHI conference on human factors in computing systems"
    },
    {
      "citation_id": "14",
      "title": "On the importance of both dimensional and discrete models of emotion",
      "authors": [
        "Eddie Harmon-Jones",
        "Cindy Harmon-Jones",
        "Elizabeth Summerell"
      ],
      "year": "2017",
      "venue": "Behavioral sciences"
    },
    {
      "citation_id": "15",
      "title": "Disambiguating emotional connotations of words using contextualized word representations",
      "authors": [
        "Sadat Akram",
        "Steffen Hosseini",
        "Staab"
      ],
      "year": "2024",
      "venue": "Proceedings of the 13th Joint Conference on Lexical and Computational Semantics (* SEM 2024)"
    },
    {
      "citation_id": "16",
      "title": "Detecting the instant of emotion change from speech using a martingale framework",
      "authors": [
        "Zhaocheng Huang",
        "Julien Epps"
      ],
      "year": "2016",
      "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "17",
      "title": "An investigation of emotion change detection from speech",
      "authors": [
        "Zhaocheng Huang",
        "Julien Epps",
        "Eliathamby Ambikairajah"
      ],
      "year": "2015",
      "venue": "INTERSPEECH"
    },
    {
      "citation_id": "18",
      "title": "Word-level contextual sentiment analysis with interpretability",
      "authors": [
        "Tomoki Ito",
        "Kota Tsubouchi",
        "Hiroki Sakaji"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "19",
      "title": "Muse-ing on the impact of utterance ordering on crowdsourced emotion annotations",
      "authors": [
        "Mimansa Jaiswal",
        "Zakaria Aldeneh",
        "Cristian-Paul Bara",
        "Yuanhang Luo",
        "Mihai Burzo",
        "Rada Mihalcea",
        "Emily Provost"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "20",
      "title": "Word-level emotion distribution with two schemas for short text emotion classification",
      "authors": [
        "Zongxi Li",
        "Haoran Xie",
        "Gary Cheng",
        "Qing Li"
      ],
      "year": "2021",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "21",
      "title": "Emollms: A series of emotional large language models and annotation tools for comprehensive affective analysis",
      "authors": [
        "Zhiwei Liu",
        "Kailai Yang",
        "Qianqian Xie",
        "Tianlin Zhang",
        "Sophia Ananiadou"
      ],
      "year": "2024",
      "venue": "Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining"
    },
    {
      "citation_id": "22",
      "title": "Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings",
      "authors": [
        "Reza Lotfian",
        "Carlos Busso"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "23",
      "title": "On coreference resolution performance metrics",
      "authors": [
        "Xiaoqiang Luo"
      ],
      "year": "2005",
      "venue": "Proceedings of human language technology conference and conference on empirical methods in natural language processing"
    },
    {
      "citation_id": "24",
      "title": "From text to emotion: Unveiling the emotion annotation capabilities of llms",
      "authors": [
        "Minxue Niu",
        "Mimansa Jaiswal",
        "Emily Provost"
      ],
      "year": "2024",
      "venue": "Proc. Interspeech 2024"
    },
    {
      "citation_id": "25",
      "title": "Defining emotionally salient regions using qualitative agreement method",
      "authors": [
        "Srinivas Parthasarathy",
        "Carlos Busso"
      ],
      "year": "2016",
      "venue": "Interspeech"
    },
    {
      "citation_id": "26",
      "title": "Predicting emotionally salient regions using qualitative agreement of deep neural network regressors",
      "authors": [
        "Srinivas Parthasarathy",
        "Carlos Busso"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "27",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Gautam Naik",
        "Erik Cambria",
        "Rada Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "28",
      "title": "Abhinaba Roy, Niyati Chhaya, and 1 others. 2021. Recognizing emotion cause in conversations",
      "authors": [
        "Soujanya Poria",
        "Navonil Majumder",
        "Devamanyu Hazarika",
        "Deepanway Ghosal",
        "Rishabh Bhardwaj",
        "Samson Yu Bai Jian",
        "Pengfei Hong",
        "Romila Ghosh"
      ],
      "venue": "Cognitive Computation"
    },
    {
      "citation_id": "29",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Tao Xu",
        "Greg Brockman",
        "Christine Mcleavey",
        "Ilya Sutskever"
      ],
      "year": "2023",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "30",
      "title": "Squad: 100,000+ questions for machine comprehension of text",
      "authors": [
        "Pranav Rajpurkar",
        "Jian Zhang",
        "Konstantin Lopyrev",
        "Percy Liang"
      ],
      "year": "2016",
      "venue": "Squad: 100,000+ questions for machine comprehension of text",
      "arxiv": "arXiv:1606.05250"
    },
    {
      "citation_id": "31",
      "title": "Sentence-BERT: Sentence embeddings using Siamese BERTnetworks",
      "authors": [
        "Nils Reimers",
        "Iryna Gurevych"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
      "doi": "10.18653/v1/D19-1410"
    },
    {
      "citation_id": "32",
      "title": "Journal of personality and social psychology",
      "authors": [
        "Russell James"
      ],
      "year": "1979",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "33",
      "title": "Emobench: Evaluating the emotional intelligence of large language models",
      "authors": [
        "Sahand Sabour",
        "Siyang Liu",
        "Zheyuan Zhang",
        "June Liu",
        "Jinfeng Zhou",
        "Alvionna Sunaryo",
        "Tatia Lee",
        "Rada Mihalcea",
        "Minlie Huang"
      ],
      "year": "2024",
      "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "34",
      "title": "Detection of emotional hotspots in meetings using a cross-corpus approach",
      "authors": [
        "Georg Stemmer",
        "Paulo Meyer",
        "Juan Del Hoyo",
        "Jose Ontiveros",
        "Hector Lopez",
        "Cordourier",
        "Tobias Maruri",
        "Bocklet"
      ],
      "year": "2023",
      "venue": "Proc. Interspeech 2023"
    },
    {
      "citation_id": "35",
      "title": "Automated emotion annotation of finnish parliamentary speeches using gpt",
      "authors": [
        "Otto Tarkka",
        "Jaakko Koljonen",
        "Markus Korhonen",
        "Juuso Laine",
        "Kristian Martiskainen",
        "Kimmo Elo",
        "Veronika Laippala"
      ],
      "year": "2024",
      "venue": "Automated emotion annotation of finnish parliamentary speeches using gpt"
    },
    {
      "citation_id": "36",
      "title": "Proceedings of the IV Workshop on Creating, Analysing, and Increasing Accessibility of Parliamentary Corpora (ParlaCLARIN)@ LREC-COLING 2024",
      "venue": "Proceedings of the IV Workshop on Creating, Analysing, and Increasing Accessibility of Parliamentary Corpora (ParlaCLARIN)@ LREC-COLING 2024"
    },
    {
      "citation_id": "37",
      "title": "Alexandre Ramé, Morgane Rivière, and 1 others",
      "authors": [
        "Gemma Team",
        "Aishwarya Kamath",
        "Johan Ferret",
        "Shreya Pathak",
        "Nino Vieillard",
        "Ramona Merhej",
        "Sarah Perrin",
        "Tatiana Matejovicova"
      ],
      "year": "2025",
      "venue": "Gemma 3 technical report",
      "arxiv": "arXiv:2503.19786"
    },
    {
      "citation_id": "38",
      "title": "Dawn of the transformer era in speech emotion recognition: closing the valence gap",
      "authors": [
        "Johannes Wagner",
        "Andreas Triantafyllopoulos",
        "Hagen Wierstorf",
        "Maximilian Schmitt",
        "Felix Burkhardt",
        "Florian Eyben",
        "Björn Schuller"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "39",
      "title": "Speech emotion diarization: Which emotion appears when?",
      "authors": [
        "Yingzhi Wang",
        "Mirco Ravanelli",
        "Alya Yacoubi"
      ],
      "year": "2023",
      "venue": "2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)"
    },
    {
      "citation_id": "40",
      "title": "Chenxu Lv, and 1 others. 2025. Qwen3 technical report",
      "authors": [
        "An Yang",
        "Anfeng Li",
        "Baosong Yang",
        "Beichen Zhang",
        "Binyuan Hui",
        "Bo Zheng",
        "Bowen Yu",
        "Chang Gao",
        "Chengen Huang"
      ],
      "venue": "Chenxu Lv, and 1 others. 2025. Qwen3 technical report",
      "arxiv": "arXiv:2505.09388"
    },
    {
      "citation_id": "41",
      "title": "Haoran Wei, and 1 others. 2024. Qwen2. 5 technical report",
      "authors": [
        "An Yang",
        "Baosong Yang",
        "Beichen Zhang",
        "Binyuan Hui",
        "Bo Zheng",
        "Bowen Yu",
        "Chengyuan Li",
        "Dayiheng Liu",
        "Fei Huang"
      ],
      "venue": "Haoran Wei, and 1 others. 2024. Qwen2. 5 technical report",
      "arxiv": "arXiv:2412.15115"
    },
    {
      "citation_id": "42",
      "title": "Bertscore: Evaluating text generation with bert",
      "authors": [
        "Tianyi Zhang",
        "Varsha Kishore",
        "Felix Wu",
        "Kilian Weinberger",
        "Yoav Artzi"
      ],
      "year": "2020",
      "venue": "International Conference on Learning Representations"
    }
  ]
}