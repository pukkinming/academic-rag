{
  "paper_id": "2203.03812v2",
  "title": "Speechformer: A Hierarchical Efficient Framework Incorporating The Characteristics Of Speech",
  "published": "2022-03-08T02:22:28Z",
  "authors": [
    "Weidong Chen",
    "Xiaofen Xing",
    "Xiangmin Xu",
    "Jianxin Pang",
    "Lan Du"
  ],
  "keywords": [
    "hierarchical framework",
    "speech signal processing",
    "speech emotion recognition",
    "cognitive disorder detection"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Transformer has obtained promising results on cognitive speech signal processing field, which is of interest in various applications ranging from emotion to neurocognitive disorder analysis. However, most works treat speech signal as a whole, leading to the neglect of the pronunciation structure that is unique to speech and reflects the cognitive process. Meanwhile, Transformer has heavy computational burden due to its full attention operation. In this paper, a hierarchical efficient framework, called SpeechFormer, which considers the structural characteristics of speech, is proposed and can be served as a generalpurpose backbone for cognitive speech signal processing. The proposed SpeechFormer consists of frame, phoneme, word and utterance stages in succession, each performing a neighboring attention according to the structural pattern of speech with high computational efficiency. SpeechFormer is evaluated on speech emotion recognition (IEMOCAP & MELD) and neurocognitive disorder detection (Pitt & DAIC-WOZ) tasks, and the results show that SpeechFormer outperforms the standard Transformer-based framework while greatly reducing the computational cost. Furthermore, our SpeechFormer achieves comparable results to the state-of-the-art approaches.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech signal is able to express the most information in the simplest way  [1] . Speech based emotion and neurocognitive disorder analysis, which is collectively referred to as cognitive speech signal processing (CoSSP), has covered a wide area of applications, including speech emotion recognition (SER), depression classification, Alzheimer's disease (AD) detection and so on. Because of its broad application value, CoSSP is becoming an increasing interest in speech signal processing field.\n\nIn the last century, Hidden Markov model, which is a statistical Markov model and assumes the system being modeled to be a Markov process, is proposed to model speech signal  [2, 3] . After that, more and more machine learning methods, such as decision tree  [4, 5]  and restricted Boltzmann machine  [6, 7]  and so on, are applied to CoSSP. Recently, with the development of deep learning, Convolutional Neural Network  [8, 9, 10, 11] , Recurrent Neural Network and its variants  [12, 13, 14, 15]  are proposed and achieve promising results. After that, deep learning methods deliver superior performances in CoSSP filed.\n\nInspired by the global attention mechanism, Transformer  [16] , which is outstanding in modeling long-range dependencies in the sequence, has achieved great success in natural language processing (NLP). Although the original Transformer is The natural structure of a speech signal, in which several frames constitute a phoneme, several phonemes constitute a word, and multiple words form an utterance in speech signal.\n\ndesigned for machine translation task in NLP, researchers are active in investigating its adaptation to many other fields, specifically computer vision  [17, 18] . Certainly, several attempts also have been made in CoSSP field  [19, 20, 21, 22] . However, most studies omit the natural characteristics of speech while using the attention with Transformer, leading to the neglect of the pronunciation structure that is unique to speech and conveys lots of messages. For example, even if I don't understand Greek, I can still determine the emotion in a Greek recording by utilizing the characteristics in speech, such as articulation, prolongation and the dynamic change of speech sound. Meanwhile, Transformer has heavy computational burden, as the computational complexity of its full attention is quadratic to input length. In other words, the standard Transformer needs to incorporate the characteristics of speech before all its performance can be exploited in CoSSP field.\n\nTo solve the above problems, we should rethink the structure of speech signal first. As shown in Figure  1 , we can observe that an utterance consists of several words, a word consists of several phonemes (e.g. word 'WANT' includes phonemes 'W', 'AA1', 'N' and 'T') and a phoneme consists of several frames (e.g. phoneme 'AA1' includes four frames). This progressive structure reveals the importance of the interaction between adjacent elements and indicates that we can model the speech signal hierarchically based on the nature of pronunciation. Therefore, a hierarchical framework, called SpeechFormer, which consists of frame, phoneme, word and utterance stages, is proposed to model the signal step-by-step. Firstly, we capture the frame, phoneme and word-level features through the first three stages and merge the features between two successive stages, both of which are performed under the instruction of the characteristics of speech with high computational efficiency. At last, an utterance stage is applied to gather all the word-level features and generate an utterance-level representation for classification. The contributions of this paper can be summarized as follows:  ‚Ä¢ We propose a hierarchical efficient framework, called SpeechFormer, to serve as a general-purpose backbone for cognitive speech signal processing. SpeechFormer improves the modeling process by incorporating the characteristics of speech, which follows the natural pronunciation structure of the input speech signal. ‚Ä¢ We evaluate SpeechFormer on IEMOCAP  [23] , MELD  [24] , Pitt  [25]  and DAIC-WOZ  [26] , and demonstrate that SpeechFormer substantially outperforms the vanilla Transformer-based framework in terms of performance and computational efficiency. Moreovre, SpeechFormer achieves comparable results to the state-of-the-art approaches. Our codes are publicly available at https: //github.com/HappyColor/SpeechFormer.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methodology",
      "text": "The proposed SpeechFormer, as shown in Figure  2 , mainly consists of four stages and three merging blocks (M-Blocks).\n\nIn which, frame stage (F-Stage), phoneme stage (P-Stage) and word stage (W-Stage) are used to learn features of different levels, and utterance stage (U-Stage) aims to generate a global representation for classification. Three M-Blocks refine the redundant features between two consecutive stages by reducing the number of tokens. Moreover, an additional branch is employed to provide the statistical natures of speech signal. More details will be introduced in the following subsections.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Vanilla Transformer",
      "text": "Transformer (refers only to its encoder part in this paper), as shown in the left part of Figure  3 , consists of two sub-layers of Multi-Head Self-Attention (MSA) and Feed-Forward Network (FFN). MSA is at the core of Transformer and we will give a brief introduction to it. More details can be found in  [16] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Msa In Vanilla Transformer",
      "text": "For a sequential input x ‚àà R T √ódm , where T and dm are the length and dimension of input, respectively. Transformer first obtains query Q, key K and value V by applying three projections to x . QKV are further divided into h parts, producing d h dimensional features, where d h = dm/h and h is the number of heads. Each head performs Single-Head Self-Attention (SSA) and the output value of each head is concatenated to form the final output of MSA. SSA is depicted as fellows:",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Speechformer Framework",
      "text": "The vanilla Transformer-based framework, illustrated in the right part of Figure  3 , neglects the implicit relations in speech. Conversely, our SpeechFormer, shown in Figure  2 , makes use of these relations to model speech signal in a hierarchical manner. To be specific, SpeechFormer block employs Speech-based Multi-Head Self-Attention (Speech-MSA) to capture the relations between adjacent elements. Merging block is applied to refine the redundant feature. Both of them are performed under the instruction of the statistical characteristics of speech.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Statistical Characteristics Of Speech",
      "text": "Elements of speech signal. Phoneme is the minimal sound unit in language, recycled to form all our spoken words. Multiple words are arranged together to form an utterance that is recorded in a wave file. In data format terms, the digital speech signal is divided into numerous frames. Each frame contains information at that particular point in time. Therefore, frame is the basic processing unit in digital system, which then gradually forms a phoneme, a word, and finally an utterance. Time duration. The frame length is literally the size of the window during, which can be set manually. Phonemes are of different lengths, varying from 50 ms to 200 ms. To analyse the duration of word, we use P2FA  [28]  to extract the phonemes from corpora used and find that more than 90% of words contain less than 5 phonemes. Thus, the duration of word is considered as 250 ms to 1000 ms (5√ó the duration of phoneme). Time duration divided by hop is the approximate number of tokens in the feature, where hop indicates the feature's hop length.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Speech-Msa In Speechformer Block",
      "text": "The only difference between SpeechFormer block and standard Transformer is the replacement of MSA with Speech-MSA. As shown in Figure  4 , Speech-MSA applies a window Tw to limit the full attention computation to a small scope of adjacent tokens, which can greatly relieve the computational burden. Furthermore, as displayed in Table  1 , the value of Tw ensures that the Speech-MSA in the first three stages can learn the interactions between neighboring frames, phonemes and words, respectively. And in U-Stage, Tw is set to the length of its input such that a global representation is learnt. The complete output of Speech-MSA is calculated by following steps:\n\n(i) Apply for a window Tw according to the current stagelevel. The value and explanation are shown in Table  1 .\n\n(ii) Select the (t -Tw 2 )-th to (t + Tw 2 )-th tokens in K, each performing a scaled dot-product with the t-th token in Q to produce a score. All the scores are concatenated and scaled by Softmax to generate the attention weights.\n\n(iii) Fetch the (t -Tw 2 )-th to (t + Tw 2 )-th tokens in V and calculate the sum of each token multiplied by its weight. The result is the output of the t-th token in Speech-MSA.\n\n(iv) Return to (ii) until the value of t varies from 1 to T .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Merging Block In Speechformer Framework",
      "text": "The merging blocks between each two successive stages refine features under the instruction of the characteristics of speech, making a hierarchical framework which follows the natural structure of speech signal. Each merging block consists of an average pooling layer and a linear layer in succession. The first layer merges M consecutive tokens in the input, where M is the merging scale, and the second layer expands the embedding dimension of the input by a factor of r. Each merging block aims to prepare the appropriate feature for its following stage as input. Specifically, each token should represent a subphoneme in P-Stage such that P-Stage can model the interaction between neighboring phonemes. Therefore, the M1 before P-Stage should be no less than the minimum length of phoneme. Similarly, the M2 before W-Stage should be no less than the minimum length of word. More details are displayed in Table  1 . Pitt corpus is used in AD detection field. The AD patients and healthy controls are asked to take the \"Cookie Theft\" picture description task  [29]  to produce recordings. Experiments are conducted in speaker-independent 10-fold cross-validation strategy. DAIC-WOZ corpus , used in AVEC 2016  [30] , contains recordings labeled depressed / not depressed. Since the labels of test data are not provided, we report the results on validation set.   2 . SGD  [32]  is employed to optimize the model. The number of Transformers N used in baseline framework is 12. The number of heads used in multihead attention is 8. As for the SpeechFormer framework, we introduce two versions with different model sizes and computational complexity. Their hyper-parameters are:\n\n‚Ä¢ SpeechFormer-S: The comparison results of the proposed SpeechFormer and the baseline framework on IEMOCAP, MELD, Pitt and DAIC-WOZ are summarized in Table  3 . Speech emotion recognition, Alzheimer's disease detection and depression classification tasks are involved. From Table  3 , we can observe that the model sizes of SpeechFormer-S and baseline are very close, where the former is slightly larger than the later. However, the theoretical computational complexity 1 (FLOPs) of SpeechFormer-S on four corpora are about 10.2%, 15.1%, 13.2% and 7.8% of the baseline, respectively. SpeechFormer-B expands the model size of SpeechFormer-S by approximately two times, while keeping the FLOPs comparable. While conducting speech emotion recognition, the baseline results are lower than those of SpeechFormer, regardless of the feature used. When performing Alzheimer's disease detection, the baseline achieves better WA using Logmel as input. In other cases, SpeechFormer obtains higher performances. As for depression classification, SpeechFormer framework substantially outperforms the baseline in all cases. In summary, our SpeechFormer framework generally outperforms the vanilla Transformer-based framework in terms of performance and computational efficiency.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Comparison To Previous State-Of-The-Art",
      "text": "Table  4  gives the comparison among SpeechFormer with some known systems on four corpora, in which, all systems utilize only speech feature as input to allow a fair comparison. On IEMOCAP, SpeechFormer-S achieves comparable performances to  [33] : +0.6% WA. When evaluated on MELD, Pitt and DAIC-WOZ, SpeechFormer-B outperforms other comparisons with promising gains: +1.7% WF1 over  [20] , +1.8% (+11.1%) WA (UA) over  [34]  and +3.4% MF1 over  [15] , respectively. 1 We omit Softmax computation in determining complexity.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "A hierarchical efficient framework, named SpeechFormer, has been proposed for cognitive speech signal processing. Speech-Former applies four stages to learn the representation, following the natural structure of speech. The Speech-MSA and merging block in SpeechFormer model speech signal efficiently by incorporating the statistical characteristics of speech. Experimental results on four corpora, including three speech-related cognitive tasks, demonstrate that SpeechFormer outperforms the vanilla Transformer-based framework in terms of performance and computational efficiency. The comparison to state-of-theart also verifies the effectiveness of SpeechFormer. In the future, we plan to extend SpeechFormer to other speech tasks, such as automatic speech recognition and speaker identification.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The natural structure of a speech signal, in which sev-",
      "page": 1
    },
    {
      "caption": "Figure 1: , we can observe",
      "page": 1
    },
    {
      "caption": "Figure 2: Overview of the proposed SpeechFormer framework.",
      "page": 2
    },
    {
      "caption": "Figure 3: , consists of two sub-layers of",
      "page": 2
    },
    {
      "caption": "Figure 3: The structure of the vanilla Transformer (left) and the",
      "page": 2
    },
    {
      "caption": "Figure 3: , neglects the implicit relations in speech.",
      "page": 2
    },
    {
      "caption": "Figure 2: , makes use",
      "page": 2
    },
    {
      "caption": "Figure 4: Speech-based Multi-Head Self-Attention (Speech-",
      "page": 3
    },
    {
      "caption": "Figure 4: , Speech-MSA applies a window Tw to limit",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table 2: The training settings on four corpora. The learning",
      "data": [
        {
          "Module": "F-Stage",
          "Tw or M": "50 ms / hop1",
          "Description": "Min.\nlength of phoneme"
        },
        {
          "Module": "M-Block",
          "Tw or M": "Characteristics\n50 ms / hop1",
          "Description": "Min.\nlength of phoneme"
        },
        {
          "Module": "P-Stage",
          "Tw or M": "of Speech: ùëáùë§\n400 ms / hop2",
          "Description": "2√óMax.\nlength of phoneme"
        },
        {
          "Module": "M-Block",
          "Tw or M": "250 ms / hop2",
          "Description": "Min.\nlength of word"
        },
        {
          "Module": "W-Stage",
          "Tw or M": "2000 ms / hop3",
          "Description": "2√óMax.\nlength of word"
        },
        {
          "Module": "M-Block",
          "Tw or M": "1000 ms / hop3",
          "Description": "Max.\nlength of word"
        },
        {
          "Module": "U-Stage",
          "Tw or M": "T",
          "Description": "Input length"
        },
        {
          "Module": "Notes: hop2 = M1hop1, hop3 = M2hop2",
          "Tw or M": "",
          "Description": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 3: The performances of the baseline framework and the proposed SpeechFormer framework on IEMOCAP, MELD, Pitt and",
      "data": [
        {
          "Feature": "",
          "Architecture": "",
          "Speech emotion recognition: IEMOCAP": "Input size",
          "Speech emotion recognition: MELD": "Input size"
        },
        {
          "Feature": "Spec",
          "Architecture": "Baseline\nSpeechFormer-S\nSpeechFormer-B",
          "Speech emotion recognition: IEMOCAP": "653√ó161",
          "Speech emotion recognition: MELD": "449√ó442"
        },
        {
          "Feature": "Logmel",
          "Architecture": "Baseline\nSpeechFormer-S\nSpeechFormer-B",
          "Speech emotion recognition: IEMOCAP": "651√ó128",
          "Speech emotion recognition: MELD": "446√ó256"
        },
        {
          "Feature": "Wav2vec",
          "Architecture": "Baseline\nSpeechFormer-S\nSpeechFormer-B",
          "Speech emotion recognition: IEMOCAP": "651√ó512",
          "Speech emotion recognition: MELD": "446√ó512"
        },
        {
          "Feature": "Feature",
          "Architecture": "Architecture",
          "Speech emotion recognition: IEMOCAP": "Alzheimer‚Äôs disease detection: Pitt",
          "Speech emotion recognition: MELD": "Depression classiÔ¨Åcation: DAIC-WOZ"
        },
        {
          "Feature": "",
          "Architecture": "",
          "Speech emotion recognition: IEMOCAP": "Input size",
          "Speech emotion recognition: MELD": "Input size"
        },
        {
          "Feature": "Spec",
          "Architecture": "Baseline\nSpeechFormer-S\nSpeechFormer-B",
          "Speech emotion recognition: IEMOCAP": "656√ó442",
          "Speech emotion recognition: MELD": "1076√ó161"
        },
        {
          "Feature": "Logmel",
          "Architecture": "Baseline\nSpeechFormer-S\nSpeechFormer-B",
          "Speech emotion recognition: IEMOCAP": "658√ó256",
          "Speech emotion recognition: MELD": "1074√ó128"
        },
        {
          "Feature": "Wav2vec",
          "Architecture": "Baseline\nSpeechFormer-S\nSpeechFormer-B",
          "Speech emotion recognition: IEMOCAP": "658√ó512",
          "Speech emotion recognition: MELD": "1074√ó512"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 3: The performances of the baseline framework and the proposed SpeechFormer framework on IEMOCAP, MELD, Pitt and",
      "data": [
        {
          "Dataset": "IEMOCAP",
          "Method": "[Guo et al.,2021][8]\n[Yin et al.,2021][33]",
          "WA\nUA": "0.613\n0.604\n0.623\n-\n0.629\n0.645\n0.623\n0.636"
        },
        {
          "Dataset": "",
          "Method": "SpeechFormer-S\nSpeechFormer-B",
          "WA\nUA": ""
        },
        {
          "Dataset": "MELD",
          "Method": "[Liang et al.,2020][20]\n[Lian et al.,2021][21]",
          "WA\nUA": "0.402 (WF1)\n0.382 (WF1)\n0.418 (WF1)\n0.419 (WF1)"
        },
        {
          "Dataset": "",
          "Method": "SpeechFormer-S\nSpeechFormer-B",
          "WA\nUA": ""
        },
        {
          "Dataset": "Pitt",
          "Method": "[Makiuchi et al.,2021][10]\n[Bertini et al.,2022][34]",
          "WA\nUA": "0.731\n-\n0.739\n0.641\n0.752\n0.751\n0.757\n0.752"
        },
        {
          "Dataset": "",
          "Method": "SpeechFormer-S\nSpeechFormer-B",
          "WA\nUA": ""
        },
        {
          "Dataset": "DAIC-WOZ",
          "Method": "[Solieman et al.,2021][11]\n[Dumpala et al.,2021][15]",
          "WA\nUA": "0.610 (MF1)\n0.660 (MF1)\n0.676 (MF1)\n0.694 (MF1)"
        },
        {
          "Dataset": "",
          "Method": "SpeechFormer-S\nSpeechFormer-B",
          "WA\nUA": ""
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Introduction. the perception of speech: from sound to meaning",
      "authors": [
        "B Moore",
        "L Tyler",
        "W Marslen-Wilson"
      ],
      "year": "2008",
      "venue": "Philosophical transactions of the Royal Society of London. Series B, Biological sciences"
    },
    {
      "citation_id": "3",
      "title": "Speech parameter generation from HMM using dynamic features",
      "authors": [
        "K Tokuda",
        "T Kobayashi",
        "S Imai"
      ],
      "year": "1995",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "4",
      "title": "Hidden Markov modelbased speech emotion recognition",
      "authors": [
        "B Schuller",
        "G Rigoll",
        "M Lang"
      ],
      "year": "2003",
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "5",
      "title": "Robust decision tree state tying for continuous speech recognition",
      "authors": [
        "W Reichl",
        "W Chou"
      ],
      "year": "2000",
      "venue": "IEEE Transactions on Speech and Audio Processing"
    },
    {
      "citation_id": "6",
      "title": "Decision tree based depression classification from audio video and language information",
      "authors": [
        "L Yang",
        "D Jiang",
        "L He",
        "E Pei",
        "M Oveneke",
        "H Sahli"
      ],
      "year": "2016",
      "venue": "Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge"
    },
    {
      "citation_id": "7",
      "title": "Deep neural networks for acoustic emotion recognition: Raising the benchmarks",
      "authors": [
        "A Stuhlsatz",
        "C Meyer",
        "F Eyben",
        "T Zielke",
        "G Meier",
        "B Schuller"
      ],
      "year": "2011",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "8",
      "title": "Acoustic emotion recognition based on fusion of multiple feature-dependent deep boltzmann machines",
      "authors": [
        "K Poon-Feng",
        "D.-Y Huang",
        "M Dong",
        "H Li"
      ],
      "year": "2014",
      "venue": "The 9th International Symposium on Chinese Spoken Language Processing"
    },
    {
      "citation_id": "9",
      "title": "Representation learning with spectro-temporal-channel attention for speech emotion recognition",
      "authors": [
        "L Guo",
        "L Wang",
        "C Xu",
        "J Dang",
        "E Chng",
        "H Li"
      ],
      "year": "2021",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "10",
      "title": "Generalized dilated CNN models for depression detection using inverted vocal tract variables",
      "authors": [
        "N Seneviratne",
        "C Espy-Wilson"
      ],
      "venue": "Proc. Interspeech, 2021"
    },
    {
      "citation_id": "11",
      "title": "Speech paralinguistic approach for detecting dementia using gated convolutional neural network",
      "authors": [
        "M Makiuchi",
        "T Warnita",
        "N Inoue",
        "K Shinoda",
        "M Yoshimura",
        "M Kitazawa",
        "K Funaki",
        "Y Eguchi",
        "T Kishimoto"
      ],
      "year": "2021",
      "venue": "IEICE transactions on Information and Systems"
    },
    {
      "citation_id": "12",
      "title": "The detection of depression using multimodal models based on text and voice quality features",
      "authors": [
        "H Solieman",
        "E Pustozerov"
      ],
      "year": "2021",
      "venue": "IEEE Conference of Russian Young Researchers in Electrical and Electronic Engineering"
    },
    {
      "citation_id": "13",
      "title": "High-level feature representation using recurrent neural network for speech emotion recognition",
      "authors": [
        "J Lee",
        "I Tashev"
      ],
      "year": "2015",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "14",
      "title": "Automatic speech emotion recognition using recurrent neural networks with local attention",
      "authors": [
        "S Mirsamadi",
        "E Barsoum",
        "C Zhang"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "15",
      "title": "A novel attention-based gated recurrent unit and its efficacy in speech emotion recognition",
      "authors": [
        "S Rajamani",
        "K Rajamani",
        "A Mallol-Ragolta",
        "S Liu",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "16",
      "title": "Significance of speaker embeddings and temporal context for depression detection",
      "authors": [
        "S Dumpala",
        "S Rodriguez",
        "S Rempel",
        "R Uher",
        "S Oore"
      ],
      "year": "2021",
      "venue": "Significance of speaker embeddings and temporal context for depression detection",
      "arxiv": "arXiv:2107.13969"
    },
    {
      "citation_id": "17",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Proceedings of the 31st International Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "18",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly",
        "J Uszkoreit",
        "N Houlsby"
      ],
      "year": "2021",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "19",
      "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
      "authors": [
        "Z Liu",
        "Y Lin",
        "Y Cao",
        "H Hu",
        "Y Wei",
        "Z Zhang",
        "S Lin",
        "B Guo"
      ],
      "venue": "International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "20",
      "title": "A novel end-to-end speech emotion recognition network with stacked transformer layers",
      "authors": [
        "X Wang",
        "M Wang",
        "W Qi",
        "W Su",
        "X Wang",
        "H Zhou"
      ],
      "year": "2021",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "21",
      "title": "Semi-supervised multi-modal emotion recognition with cross-modal distribution matching",
      "authors": [
        "J Liang",
        "R Li",
        "Q Jin"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "22",
      "title": "Ctnet: Conversational transformer network for emotion recognition",
      "authors": [
        "Z Lian",
        "B Liu",
        "J Tao"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "23",
      "title": "Key-sparse transformer with cascaded cross-attention block for multimodal speech emotion recognition",
      "authors": [
        "W Chen",
        "X Xing",
        "X Xu",
        "J Yang"
      ],
      "year": "2021",
      "venue": "Key-sparse transformer with cascaded cross-attention block for multimodal speech emotion recognition",
      "arxiv": "arXiv:2106.11532"
    },
    {
      "citation_id": "24",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "25",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "26",
      "title": "The natural history of alzheimer's disease: Description of study cohort and accuracy of diagnosis",
      "authors": [
        "J Becker",
        "F Boiler",
        "O Lopez",
        "J Saxton",
        "K Mcgonigle"
      ],
      "year": "1994",
      "venue": "Archives of Neurology"
    },
    {
      "citation_id": "27",
      "title": "The distress analysis interview corpus of human and computer interviews",
      "authors": [
        "J Gratch",
        "R Artstein",
        "G Lucas",
        "G Stratou",
        "S Scherer",
        "A Nazarian",
        "R Wood",
        "J Boberg",
        "D Devault",
        "S Marsella",
        "D Traum",
        "S Rizzo",
        "L.-P Morency"
      ],
      "year": "2014",
      "venue": "Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC'14)"
    },
    {
      "citation_id": "28",
      "title": "Layer normalization",
      "authors": [
        "J Lei Ba",
        "J Kiros",
        "G Hinton"
      ],
      "year": "2016",
      "venue": "Layer normalization",
      "arxiv": "arXiv:1607.06450"
    },
    {
      "citation_id": "29",
      "title": "Speaker identification on the scotus corpus",
      "authors": [
        "J Yuan",
        "M Liberman"
      ],
      "year": "2008",
      "venue": "Journal of the Acoustical Society of America"
    },
    {
      "citation_id": "30",
      "title": "The boston diagnostic aphasia examination",
      "authors": [
        "K Goodglass"
      ],
      "year": "1983",
      "venue": "Lea & Febinger"
    },
    {
      "citation_id": "31",
      "title": "Avec 2016: Depression, mood, and emotion recognition workshop and challenge",
      "authors": [
        "M Valstar",
        "J Gratch",
        "B Schuller",
        "F Ringeval",
        "D Lalanne",
        "M Torres",
        "S Scherer",
        "G Stratou",
        "R Cowie",
        "M Pantic"
      ],
      "year": "2016",
      "venue": "Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge"
    },
    {
      "citation_id": "32",
      "title": "wav2vec: Unsupervised pre-training for speech recognition",
      "authors": [
        "S Schneider",
        "A Baevski",
        "R Collobert",
        "M Auli"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "33",
      "title": "A stochastic approximation method",
      "authors": [
        "H Robbins",
        "S Monro"
      ],
      "year": "1951",
      "venue": "The annals of mathematical statistics"
    },
    {
      "citation_id": "34",
      "title": "Progressive co-teaching for ambiguous speech emotion recognition",
      "authors": [
        "Y Yin",
        "Y Gu",
        "L Yao",
        "Y Zhou",
        "X Liang",
        "H Zhang"
      ],
      "year": "2021",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "35",
      "title": "An automatic alzheimer's disease classifier based on spontaneous spoken english",
      "authors": [
        "F Bertini",
        "D Allevi",
        "G Lutero",
        "L Calz√†",
        "D Montesi"
      ],
      "year": "2022",
      "venue": "Computer Speech & Language"
    }
  ]
}