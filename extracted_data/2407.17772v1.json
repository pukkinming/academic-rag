{
  "paper_id": "2407.17772v1",
  "title": "Erit Lightweight Multimodal Dataset For Elderly Emotion Recognition And Multimodal Fusion Evaluation",
  "published": "2024-07-25T05:02:27Z",
  "authors": [
    "Rita Frieske",
    "Bertram E. Shi"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "ERIT is a novel multimodal dataset designed to facilitate research in a lightweight multimodal fusion. It contains text and image data collected from videos of elderly individuals reacting to various situations, as well as seven emotion labels for each data sample. Because of the use of labeled images of elderly users reacting emotionally, it is also facilitating research on emotion recognition in an underrepresented age group in machine learning visual emotion recognition. The dataset is validated through comprehensive experiments indicating its importance in neural multimodal fusion research. Preprint. Under review.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition plays a crucial role in understanding human behavior and improving humancomputer interaction. With the growing elderly population, there is an increasing need for effective emotion recognition systems tailored to the specific characteristics of this demographic. ERIT is a multimodal dataset that aims to address this need by providing a rich source of data for training and evaluating emotion recognition models for elderly individuals. This paper describes the motivation behind creating the ERIT dataset, the methodology used to obtain and validate the emotion labels, and the potential applications of the dataset in the field of elderly emotion recognition.\n\nThe primary motivation for creating ERIT dataset is the lightweight evaluation of multimodal fusion. The task chosen for research on multimodal fusion needs to be able to perform across different modalities, possibly with a limited amount of classes for an easy evaluation. Emotion recognition meets the demands of such a task, with only seven classes and the possibility of performing emotion recognition among different modalities, such as facial expresion  Leong et al. [2023] ,  Huang et al. [2023] , pose  Yang et al. [2020 ], text Acheampong et al. [2020] ,  Nandwani and Verma [2021]  or audio  Zhao and Shu [2023] ,  George and Muhamed Ilyas [2024]  or non-contact physiological signals  Li and Peng [2024] .\n\nThe secondary motivation behind the creation of the ERIT dataset is to facilitate research in elderly emotion recognition and contribute to the development of more accurate and robust emotion recognition systems for this demographic. The dataset can be used for various applications, including healthcare, elderly care, and assistive technologies  Sharma et al. [2021] .",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Methodology",
      "text": "The ERIT dataset contains lightweight text and image data collected from videos of elderly people reacting to various stimuli. The dataset includes transcriptions of the speech and emotion labels extracted from the video frames. The emotion labels are provided for seven basic emotions: anger, disgust, fear, happiness, sadness, surprise, and neutral. The dataset was built from frames extracted from the ElderReact video dataset by  Ma et al. [2019] . The reasons behind creating an image and text-based dataset were: lightweight processing of text and image, accuracy of evaluating fusion compared to frames randomly extracted from the video and filling the gap in emotion recognition among different age groups.\n\nText and image are better for lightweight fusion evaluation than computationally expensive audio or video processing and can be processed by most of the large multimodal models (LMMs) such as GPT4v OpenAI [2023b] and GPT4o OpenAI [2023a], multimodal versions of LLaMA  Zhang et al. [2023] , or Flamingo  Alayrac et al. [2022]  etc.\n\nDue to the labeling of specific frames in the video, the prediction is more accurate than in cases of videos labeled and evaluated by sampling a few frames, the method used on evaluated MER-MULTI dataset by  Lian et al. [2024] . The authors admit that this method could potentially ignore key samples and decrease the performance.\n\nFinally, the emotion recognition images try to fill the age gap in the emotion recognition community by creating of an openly public dataset for facial emotion recognition. While some of the elderly facial emotion recognition image datasets  Ebner et al. [2010]  (1068 images of older faces) and  Minear and Park [2004]  provide acted emotions, emotions of the elderly users in ERIT were their natural reaction to the presented material.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Data Collection And Verification",
      "text": "The data for the ERIT dataset was collected from videos of older people reacting to things associated with younger generations such as various video games, popular music and entertainment, slang, The audio from the videos was transcribed using automatic speech recognition (ASR), specifically Whisper  Radford et al. [2023] , which performs with WER similar to supervised models on Lib-riSpeech and outperforms the wav2vec2 model. We also tested Google Speech Recognition but did not annotate multiple samples, whereas Whisper returned all the videos annotated.\n\nFor the frame selection, we used labels from the ElderReact dataset as a ground truth. Since each video was labeled with one or more labels, we extracted as many frames as there were labels assigned to the video. Subsequently, we performed a search for each label in the video and selected the one with the highest emotion score by using the DeepFace framework, which is a state-of-the-art facial emotion recognition system. As a result of each video, we obtained appx. 2 different frames, with different labels but with the same transcription. That roughly doubled the amount of labels in the video dataset 1.  , derived labels. This validation process ensures that the emotion labels in the dataset are accurate and reliable.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Experiments And Results",
      "text": "This analysis focuses on how different models perform across various data types, particularly using the ERIT dataset as a benchmark for evaluating multimodal fusion compared to single-mode text and image analyses. By assessing models' abilities to integrate both textual and visual inputs effectively, the study underscores the importance of leveraging combined data sources for enhanced sentiment analysis and recognition tasks. Models that excel in multimodal fusion demonstrate a clear advantage in leveraging the complementary strengths of text and image inputs, showcasing their capability to deliver nuanced and accurate analyses across diverse datasets like ERIT.\n\nPrompts used for evaluation of LMMs were similar for each case, and varied only in the type of modal information passed. The correct prompts were generated with the help of the GPT 3.5 prompt generator which largely improved the number of predictions compared to hand-engineered prompts.\n\nFinally, we analyze the performance of LLMs on emotion recognition tasks using only textual data across various emotion datasets, including ERIT. Models such as GPT-4 and LLaMA-7B demonstrate competitive performance on datasets like EmoWOZ and DAIC-WOZ, indicating strong proficiency in text-based emotion recognition tasks. However, their effectiveness varies when applied to the ERIT dataset, which poses challenges due to its complex emotional expressions and different age groups that might necessitate the integration of multimodal inputs for more accurate analysis.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "The ERIT dataset is a valuable resource for researchers and practitioners working in the field of both multimodal fusion as well as elderly emotion recognition. The dataset provides a rich source of multimodal data, that is images paired with text and labeled with emotions, which can be used to research multimodal fusion performance. It can also be used for emotion recognition models specifically tailored to the elderly population. The comprehensive experiments on LMMs ensure the accuracy and reliability of the emotion labels and make the dataset applicable to a wide range of lightweight multimodal experiments.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Examples of frames labeled with different emotions from ERIT.",
      "page": 2
    },
    {
      "caption": "Figure 2: Percentage of different emotion labels among ERIT test, dev, and train splits.",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Abstract": ""
        },
        {
          "Abstract": "It contains text and image data collected from videos of"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "Because of\nthe use of\nlabeled images of elderly users"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1\nIntroduction": "Emotion recognition plays a crucial role in understanding human behavior and improving human-"
        },
        {
          "1\nIntroduction": "computer interaction. With the growing elderly population, there is an increasing need for effective"
        },
        {
          "1\nIntroduction": "emotion recognition systems tailored to the specific characteristics of this demographic. ERIT is a"
        },
        {
          "1\nIntroduction": "multimodal dataset that aims to address this need by providing a rich source of data for training and"
        },
        {
          "1\nIntroduction": "evaluating emotion recognition models for elderly individuals. This paper describes the motivation"
        },
        {
          "1\nIntroduction": "behind creating the ERIT dataset, the methodology used to obtain and validate the emotion labels,"
        },
        {
          "1\nIntroduction": "and the potential applications of the dataset in the field of elderly emotion recognition."
        },
        {
          "1\nIntroduction": "The primary motivation for creating ERIT dataset is the lightweight evaluation of multimodal fusion."
        },
        {
          "1\nIntroduction": "The task chosen for research on multimodal fusion needs to be able to perform across different"
        },
        {
          "1\nIntroduction": "modalities, possibly with a limited amount of classes for an easy evaluation. Emotion recognition"
        },
        {
          "1\nIntroduction": "meets the demands of such a task, with only seven classes and the possibility of performing emotion"
        },
        {
          "1\nIntroduction": ""
        },
        {
          "1\nIntroduction": "[2023], pose Yang et al.\n[2020],\ntext Acheampong et al."
        },
        {
          "1\nIntroduction": "or audio Zhao and Shu [2023], George and Muhamed Ilyas [2024] or non-contact physiological"
        },
        {
          "1\nIntroduction": "signals Li and Peng [2024]."
        },
        {
          "1\nIntroduction": "The secondary motivation behind the creation of the ERIT dataset is to facilitate research in elderly"
        },
        {
          "1\nIntroduction": "emotion recognition and contribute to the development of more accurate and robust emotion recog-"
        },
        {
          "1\nIntroduction": "nition systems for this demographic. The dataset can be used for various applications,"
        },
        {
          "1\nIntroduction": "healthcare, elderly care, and assistive technologies Sharma et al. [2021]."
        },
        {
          "1\nIntroduction": "Preprint. Under review."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2\nMethodology": "The ERIT dataset contains lightweight text and image data collected from videos of elderly people"
        },
        {
          "2\nMethodology": "reacting to various stimuli. The dataset\nincludes transcriptions of the speech and emotion labels"
        },
        {
          "2\nMethodology": "extracted from the video frames. The emotion labels are provided for seven basic emotions: anger,"
        },
        {
          "2\nMethodology": "disgust, fear, happiness, sadness, surprise, and neutral. The dataset was built from frames extracted"
        },
        {
          "2\nMethodology": "from the ElderReact video dataset by Ma et al. [2019]. The reasons behind creating an image and"
        },
        {
          "2\nMethodology": "text-based dataset were:\nlightweight processing of text and image, accuracy of evaluating fusion"
        },
        {
          "2\nMethodology": "compared to frames randomly extracted from the video and filling the gap in emotion recognition"
        },
        {
          "2\nMethodology": "among different age groups."
        },
        {
          "2\nMethodology": "Text and image are better for lightweight fusion evaluation than computationally expensive audio"
        },
        {
          "2\nMethodology": "or video processing and can be processed by most of the large multimodal models (LMMs) such as"
        },
        {
          "2\nMethodology": "GPT4v OpenAI [2023b] and GPT4o OpenAI [2023a], multimodal versions of LLaMA Zhang et al."
        },
        {
          "2\nMethodology": "[2023], or Flamingo Alayrac et al. [2022] etc."
        },
        {
          "2\nMethodology": "Due to the labeling of specific frames in the video, the prediction is more accurate than in cases of"
        },
        {
          "2\nMethodology": "videos labeled and evaluated by sampling a few frames, the method used on evaluated MER-MULTI"
        },
        {
          "2\nMethodology": "dataset by Lian et al. [2024]. The authors admit that this method could potentially ignore key samples"
        },
        {
          "2\nMethodology": "and decrease the performance."
        },
        {
          "2\nMethodology": "Finally, the emotion recognition images try to fill the age gap in the emotion recognition community"
        },
        {
          "2\nMethodology": "by creating of an openly public dataset for facial emotion recognition. While some of the elderly"
        },
        {
          "2\nMethodology": "facial emotion recognition image datasets Ebner et al. [2010] (1068 images of older faces) and Minear"
        },
        {
          "2\nMethodology": "and Park [2004] provide acted emotions, emotions of the elderly users in ERIT were their natural"
        },
        {
          "2\nMethodology": "reaction to the presented material."
        },
        {
          "2\nMethodology": "2.1\nData Collection and Verification"
        },
        {
          "2\nMethodology": "The data for the ERIT dataset was collected from videos of older people reacting to things associated"
        },
        {
          "2\nMethodology": "with younger generations such as various video games, popular music and entertainment, slang,"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 2: Percentage of different emotion labels among ERIT test, dev, and train splits.": "modern technology, etc. The videos were obtained from the YouTube video series called Elders React"
        },
        {
          "Figure 2: Percentage of different emotion labels among ERIT test, dev, and train splits.": "which has been on YouTube for over 10 years with 20M subscribers. We built upon the ElderReact"
        },
        {
          "Figure 2: Percentage of different emotion labels among ERIT test, dev, and train splits.": "dataset that was labeled for emotions by AmazonTurk respondents Ma et al. [2019]. The dataset is"
        },
        {
          "Figure 2: Percentage of different emotion labels among ERIT test, dev, and train splits.": "split into separate training, validation, and testing sets to facilitate model development and evaluation."
        },
        {
          "Figure 2: Percentage of different emotion labels among ERIT test, dev, and train splits.": "Each set contains a diverse range of emotional expressions and individual differences, ensuring a"
        },
        {
          "Figure 2: Percentage of different emotion labels among ERIT test, dev, and train splits.": "robust dataset for training and evaluating emotion recognition models (see: Fig 2)."
        },
        {
          "Figure 2: Percentage of different emotion labels among ERIT test, dev, and train splits.": "The audio from the videos was transcribed using automatic speech recognition (ASR), specifically"
        },
        {
          "Figure 2: Percentage of different emotion labels among ERIT test, dev, and train splits.": "Whisper Radford et al. [2023], which performs with WER similar to supervised models on Lib-"
        },
        {
          "Figure 2: Percentage of different emotion labels among ERIT test, dev, and train splits.": "riSpeech and outperforms the wav2vec2 model. We also tested Google Speech Recognition but did"
        },
        {
          "Figure 2: Percentage of different emotion labels among ERIT test, dev, and train splits.": "not annotate multiple samples, whereas Whisper returned all the videos annotated."
        },
        {
          "Figure 2: Percentage of different emotion labels among ERIT test, dev, and train splits.": "For the frame selection, we used labels from the ElderReact dataset as a ground truth. Since each"
        },
        {
          "Figure 2: Percentage of different emotion labels among ERIT test, dev, and train splits.": "video was labeled with one or more labels, we extracted as many frames as there were labels assigned"
        },
        {
          "Figure 2: Percentage of different emotion labels among ERIT test, dev, and train splits.": "to the video. Subsequently, we performed a search for each label in the video and selected the one"
        },
        {
          "Figure 2: Percentage of different emotion labels among ERIT test, dev, and train splits.": "with the highest emotion score by using the DeepFace framework, which is a state-of-the-art facial"
        },
        {
          "Figure 2: Percentage of different emotion labels among ERIT test, dev, and train splits.": "emotion recognition system. As a result of each video, we obtained appx. 2 different frames, with"
        },
        {
          "Figure 2: Percentage of different emotion labels among ERIT test, dev, and train splits.": "different labels but with the same transcription. That roughly doubled the amount of labels in the"
        },
        {
          "Figure 2: Percentage of different emotion labels among ERIT test, dev, and train splits.": "video dataset 1."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "with the highest emotion score by using the DeepFace framework, which is a state-of-the-art facial": ""
        },
        {
          "with the highest emotion score by using the DeepFace framework, which is a state-of-the-art facial": ""
        },
        {
          "with the highest emotion score by using the DeepFace framework, which is a state-of-the-art facial": "video dataset 1."
        },
        {
          "with the highest emotion score by using the DeepFace framework, which is a state-of-the-art facial": ""
        },
        {
          "with the highest emotion score by using the DeepFace framework, which is a state-of-the-art facial": ""
        },
        {
          "with the highest emotion score by using the DeepFace framework, which is a state-of-the-art facial": ""
        },
        {
          "with the highest emotion score by using the DeepFace framework, which is a state-of-the-art facial": ""
        },
        {
          "with the highest emotion score by using the DeepFace framework, which is a state-of-the-art facial": ""
        },
        {
          "with the highest emotion score by using the DeepFace framework, which is a state-of-the-art facial": ""
        },
        {
          "with the highest emotion score by using the DeepFace framework, which is a state-of-the-art facial": ""
        },
        {
          "with the highest emotion score by using the DeepFace framework, which is a state-of-the-art facial": "ElderReact"
        },
        {
          "with the highest emotion score by using the DeepFace framework, which is a state-of-the-art facial": "ERIT:"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "GPT4v": "Dev (↑)",
          "GPT4o": "Dev (↑)",
          "LLaMA w Adapter": "Test (↑)"
        },
        {
          "GPT4v": "38.97",
          "GPT4o": "37.38",
          "LLaMA w Adapter": "33.16"
        },
        {
          "GPT4v": "29.39",
          "GPT4o": "24.11",
          "LLaMA w Adapter": "33.68"
        },
        {
          "GPT4v": "39.47",
          "GPT4o": "39.5",
          "LLaMA w Adapter": "33.51"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fusion": "",
          "39.47\n42.3\n39.5": "",
          "41.18": "(a)",
          "35.61": "",
          "33.51": ""
        },
        {
          "Fusion": "",
          "39.47\n42.3\n39.5": "",
          "41.18": "GPT_4_V",
          "35.61": "",
          "33.51": ""
        },
        {
          "Fusion": "ERIT",
          "39.47\n42.3\n39.5": "ERIT\nMVSA-\nMVSA-",
          "41.18": "CH-",
          "35.61": "CMU-",
          "33.51": "MER-"
        },
        {
          "Fusion": "Dev",
          "39.47\n42.3\n39.5": "Test\nSingleLian et al. [2024]\nMultipleLian et al. [2024]",
          "41.18": "SIMSLian et al. [2024]",
          "35.61": "MOSILian et al. [2024]",
          "33.51": "MULTILian et al. [2024]"
        },
        {
          "Fusion": "(↑)",
          "39.47\n42.3\n39.5": "(↑)\n(↑)\n(↑)",
          "41.18": "(↑)",
          "35.61": "(↑)",
          "33.51": "(↑)"
        },
        {
          "Fusion": "Image\n38.97",
          "39.47\n42.3\n39.5": "40.14\n58.68\n63.35",
          "41.18": "76.13",
          "35.61": "51.17",
          "33.51": "46.23"
        },
        {
          "Fusion": "Text\n29.39",
          "39.47\n42.3\n39.5": "34.48\n57.65\n62.71",
          "41.18": "70.07",
          "35.61": "82.32",
          "33.51": "34.57"
        },
        {
          "Fusion": "Fusion\n39.47",
          "39.47\n42.3\n39.5": "42.3\n61.25\n66.82",
          "41.18": "81.24",
          "35.61": "80.43",
          "33.51": "65.39"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Image": "Text",
          "38.97": "29.39",
          "40.14": "34.48",
          "58.68": "57.65",
          "63.35": "62.71",
          "76.13": "70.07",
          "51.17": "82.32",
          "46.23": "34.57"
        },
        {
          "Image": "Fusion",
          "38.97": "39.47",
          "40.14": "42.3",
          "58.68": "61.25",
          "63.35": "66.82",
          "76.13": "81.24",
          "51.17": "80.43",
          "46.23": "65.39"
        },
        {
          "Image": "",
          "38.97": "",
          "40.14": "",
          "58.68": "",
          "63.35": "",
          "76.13": "(b)",
          "51.17": "",
          "46.23": ""
        },
        {
          "Image": "Table 2: (a) Accuracy of emotion recognition of images, texts and both fused by GPT4v, GPT40, and",
          "38.97": "",
          "40.14": "",
          "58.68": "",
          "63.35": "",
          "76.13": "",
          "51.17": "",
          "46.23": ""
        },
        {
          "Image": "",
          "38.97": "LLaMA w Adapter on ERIT and (b) evaluation of different datasets on GPT4v. Among the datasets",
          "40.14": "",
          "58.68": "",
          "63.35": "",
          "76.13": "",
          "51.17": "",
          "46.23": ""
        },
        {
          "Image": "",
          "38.97": "ERIT and MVSA are text image-based, and CH-SIMS, CMU-MOSI, MER-MULTI evaluate text and",
          "40.14": "",
          "58.68": "",
          "63.35": "",
          "76.13": "",
          "51.17": "",
          "46.23": ""
        },
        {
          "Image": "",
          "38.97": "3 frames per video. ERIT and MER-MULTI are multi-label datasets, while MVSA, CH-SIMS, and",
          "40.14": "",
          "58.68": "",
          "63.35": "",
          "76.13": "",
          "51.17": "",
          "46.23": ""
        },
        {
          "Image": "",
          "38.97": "CMU-MOSI provide sentiment analysis.",
          "40.14": "",
          "58.68": "",
          "63.35": "",
          "76.13": "",
          "51.17": "",
          "46.23": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "DAIC-": ""
        },
        {
          "DAIC-": "WOZ"
        },
        {
          "DAIC-": ""
        },
        {
          "DAIC-": "(test)"
        },
        {
          "DAIC-": ""
        },
        {
          "DAIC-": "F1 (↑)"
        },
        {
          "DAIC-": "-"
        },
        {
          "DAIC-": "-"
        },
        {
          "DAIC-": "-"
        },
        {
          "DAIC-": "64.3"
        },
        {
          "DAIC-": "59.3"
        },
        {
          "DAIC-": ""
        },
        {
          "DAIC-": "70"
        },
        {
          "DAIC-": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "References": "Francisca Adoma Acheampong, Chen Wenyu, and Henry Nunoo-Mensah.\nText-based emotion"
        },
        {
          "References": "detection: Advances, challenges, and opportunities. Engineering Reports, 2(7), May 2020.\nISSN"
        },
        {
          "References": "2577-8196. doi: 10.1002/eng2.12189. URL http://dx.doi.org/10.1002/eng2.12189."
        },
        {
          "References": "Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel"
        },
        {
          "References": "Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford,"
        },
        {
          "References": "Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick,"
        },
        {
          "References": "Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski,"
        },
        {
          "References": "Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: a visual"
        },
        {
          "References": "language model for few-shot learning.\nIn Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and"
        },
        {
          "References": "Kyunghyun Cho, editors, Advances in Neural\nInformation Processing Systems, 2022.\nURL"
        },
        {
          "References": "https://openreview.net/forum?id=EbMuimAbPbs."
        },
        {
          "References": "Natalie C. Ebner, Michaela Riediger, and Ulman Lindenberger. Faces—a database of facial expres-"
        },
        {
          "References": "sions in young, middle-aged, and older women and men: Development and validation. Behavior"
        },
        {
          "References": "Research Methods, 42(1):351–362, feb 2010.\nISSN 1554-3528. doi: 10.3758/brm.42.1.351. URL"
        },
        {
          "References": "http://dx.doi.org/10.3758/BRM.42.1.351."
        },
        {
          "References": "Swapna Mol George and P. Muhamed Ilyas. A review on speech emotion recognition: A survey,"
        },
        {
          "References": "recent advances, challenges, and the influence of noise. Neurocomputing, 568:127015, February"
        },
        {
          "References": "2024.\nISSN 0925-2312. doi: 10.1016/j.neucom.2023.127015. URL http://dx.doi.org/10."
        },
        {
          "References": "1016/j.neucom.2023.127015."
        },
        {
          "References": "Zi-Yu Huang, Chia-Chin Chiang, Jian-Hao Chen, Yi-Chian Chen, Hsin-Lung Chung, Yu-Ping Cai,"
        },
        {
          "References": "Scientific\nand Hsiu-Chuan Hsu. A study on computer vision for\nfacial emotion recognition."
        },
        {
          "References": "Reports, 13(1), May 2023.\nISSN 2045-2322. doi: 10.1038/s41598-023-35446-4. URL http:"
        },
        {
          "References": "//dx.doi.org/10.1038/s41598-023-35446-4."
        },
        {
          "References": "Sze Chit Leong, Yuk Ming Tang, Chung Hin Lai, and C.K.M. Lee. Facial expression and body"
        },
        {
          "References": "gesture emotion recognition: A systematic review on the use of visual data in affective computing."
        },
        {
          "References": "Computer Science Review, 48:100545, May 2023.\nISSN 1574-0137. doi: 10.1016/j.cosrev.2023."
        },
        {
          "References": "100545. URL http://dx.doi.org/10.1016/j.cosrev.2023.100545."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Jixiang Li and Jianxin Peng. End-to-end multimodal emotion recognition based on facial expressions": "and remote photoplethysmography signals.\nIEEE Journal of Biomedical and Health Informatics,"
        },
        {
          "Jixiang Li and Jianxin Peng. End-to-end multimodal emotion recognition based on facial expressions": "pages 1–10, 2024. doi: 10.1109/JBHI.2024.3430310."
        },
        {
          "Jixiang Li and Jianxin Peng. End-to-end multimodal emotion recognition based on facial expressions": "Zheng Lian, Licai Sun, Haiyang Sun, Kang Chen, Zhuofan Wen, Hao Gu, Bin Liu, and Jianhua Tao."
        },
        {
          "Jixiang Li and Jianxin Peng. End-to-end multimodal emotion recognition based on facial expressions": "Information\nGpt-4v with emotion: A zero-shot benchmark for generalized emotion recognition."
        },
        {
          "Jixiang Li and Jianxin Peng. End-to-end multimodal emotion recognition based on facial expressions": "Fusion, 108:102367, aug 2024.\nISSN 1566-2535.\ndoi:\n10.1016/j.inffus.2024.102367. URL"
        },
        {
          "Jixiang Li and Jianxin Peng. End-to-end multimodal emotion recognition based on facial expressions": "http://dx.doi.org/10.1016/j.inffus.2024.102367."
        },
        {
          "Jixiang Li and Jianxin Peng. End-to-end multimodal emotion recognition based on facial expressions": "Kaixin Ma, Xinyu Wang, Xinru Yang, Mingtong Zhang, Jeffrey M Girard, and Louis-Philippe"
        },
        {
          "Jixiang Li and Jianxin Peng. End-to-end multimodal emotion recognition based on facial expressions": "Morency. Elderreact: A multimodal dataset for recognizing emotional response in aging adults."
        },
        {
          "Jixiang Li and Jianxin Peng. End-to-end multimodal emotion recognition based on facial expressions": "In 2019 International Conference on Multimodal Interaction,\nICMI ’19, page 349–357, New"
        },
        {
          "Jixiang Li and Jianxin Peng. End-to-end multimodal emotion recognition based on facial expressions": "York, NY, USA, 2019. Association for Computing Machinery.\nISBN 9781450368605.\ndoi:"
        },
        {
          "Jixiang Li and Jianxin Peng. End-to-end multimodal emotion recognition based on facial expressions": "10.1145/3340555.3353747. URL https://doi.org/10.1145/3340555.3353747."
        },
        {
          "Jixiang Li and Jianxin Peng. End-to-end multimodal emotion recognition based on facial expressions": "Meredith Minear and Denise C. Park. A lifespan database of adult facial stimuli. Behavior Research"
        },
        {
          "Jixiang Li and Jianxin Peng. End-to-end multimodal emotion recognition based on facial expressions": "Methods, Instruments, & Computers, 36(4):630–633, nov 2004.\nISSN 1532-5970. doi: 10.3758/"
        },
        {
          "Jixiang Li and Jianxin Peng. End-to-end multimodal emotion recognition based on facial expressions": "bf03206543. URL http://dx.doi.org/10.3758/BF03206543."
        },
        {
          "Jixiang Li and Jianxin Peng. End-to-end multimodal emotion recognition based on facial expressions": "Pansy Nandwani and Rupali Verma. A review on sentiment analysis and emotion detection from"
        },
        {
          "Jixiang Li and Jianxin Peng. End-to-end multimodal emotion recognition based on facial expressions": "text. Social Network Analysis and Mining, 11(1), August 2021.\nISSN 1869-5469. doi: 10.1007/"
        },
        {
          "Jixiang Li and Jianxin Peng. End-to-end multimodal emotion recognition based on facial expressions": "s13278-021-00776-6. URL http://dx.doi.org/10.1007/s13278-021-00776-6."
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Text-based emotion detection: Advances, challenges, and opportunities",
      "authors": [
        "Francisca Adoma",
        "Chen Wenyu",
        "Henry Nunoo-Mensah"
      ],
      "year": "2020",
      "venue": "Engineering Reports",
      "doi": "10.1002/eng2.12189"
    },
    {
      "citation_id": "2",
      "title": "Flamingo: a visual language model for few-shot learning",
      "authors": [
        "Jean-Baptiste Alayrac",
        "Jeff Donahue",
        "Pauline Luc",
        "Antoine Miech",
        "Iain Barr",
        "Yana Hasson",
        "Karel Lenc",
        "Arthur Mensch",
        "Katherine Millican",
        "Malcolm Reynolds",
        "Roman Ring",
        "Eliza Rutherford",
        "Serkan Cabi",
        "Tengda Han",
        "Zhitao Gong",
        "Sina Samangooei",
        "Marianne Monteiro",
        "Jacob Menick",
        "Sebastian Borgeaud",
        "Andrew Brock",
        "Aida Nematzadeh",
        "Sahand Sharifzadeh",
        "Mikolaj Binkowski",
        "Ricardo Barreira",
        "Oriol Vinyals",
        "Andrew Zisserman",
        "Karen Simonyan"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "3",
      "title": "Faces-a database of facial expressions in young, middle-aged, and older women and men: Development and validation",
      "authors": [
        "Natalie Ebner",
        "Michaela Riediger",
        "Ulman Lindenberger"
      ],
      "year": "2010",
      "venue": "Behavior Research Methods",
      "doi": "10.3758/BRM.42.1.351"
    },
    {
      "citation_id": "4",
      "title": "A review on speech emotion recognition: A survey, recent advances, challenges, and the influence of noise",
      "authors": [
        "Swapna Mol",
        "P Ilyas"
      ],
      "year": "2024",
      "venue": "Neurocomputing",
      "doi": "10.1016/j.neucom.2023.127015"
    },
    {
      "citation_id": "5",
      "title": "A study on computer vision for facial emotion recognition",
      "authors": [
        "Zi-Yu Huang",
        "Chia-Chin Chiang",
        "Jian-Hao Chen",
        "Yi-Chian Chen",
        "Hsin-Lung Chung",
        "Yu-Ping Cai",
        "Hsiu-Chuan Hsu"
      ],
      "year": "2023",
      "venue": "Scientific Reports",
      "doi": "10.1038/s41598-023-35446-4"
    },
    {
      "citation_id": "6",
      "title": "Facial expression and body gesture emotion recognition: A systematic review on the use of visual data in affective computing",
      "authors": [
        "Chit Sze",
        "Yuk Leong",
        "Ming Tang",
        "Chung Lai",
        "C Lee"
      ],
      "year": "2023",
      "venue": "Computer Science Review",
      "doi": "10.1016/j.cosrev.2023.100545"
    },
    {
      "citation_id": "7",
      "title": "End-to-end multimodal emotion recognition based on facial expressions and remote photoplethysmography signals",
      "authors": [
        "Jixiang Li",
        "Jianxin Peng"
      ],
      "year": "2024",
      "venue": "IEEE Journal of Biomedical and Health Informatics",
      "doi": "10.1109/JBHI.2024.3430310"
    },
    {
      "citation_id": "8",
      "title": "Gpt-4v with emotion: A zero-shot benchmark for generalized emotion recognition",
      "authors": [
        "Zheng Lian",
        "Licai Sun",
        "Haiyang Sun",
        "Kang Chen",
        "Zhuofan Wen",
        "Hao Gu",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2024",
      "venue": "Information Fusion",
      "doi": "10.1016/j.inffus.2024.102367"
    },
    {
      "citation_id": "9",
      "title": "Elderreact: A multimodal dataset for recognizing emotional response in aging adults",
      "authors": [
        "Kaixin Ma",
        "Xinyu Wang",
        "Xinru Yang",
        "Mingtong Zhang",
        "Jeffrey Girard",
        "Louis-Philippe Morency"
      ],
      "year": "2019",
      "venue": "2019 International Conference on Multimodal Interaction, ICMI '19",
      "doi": "10.1145/3340555.3353747"
    },
    {
      "citation_id": "10",
      "title": "A lifespan database of adult facial stimuli",
      "authors": [
        "Meredith Minear",
        "Denise Park"
      ],
      "year": "2004",
      "venue": "Behavior Research Methods, Instruments, & Computers",
      "doi": "10.3758/BF03206543"
    },
    {
      "citation_id": "11",
      "title": "A review on sentiment analysis and emotion detection from text",
      "authors": [
        "Pansy Nandwani",
        "Rupali Verma"
      ],
      "year": "2021",
      "venue": "Social Network Analysis and Mining",
      "doi": "10.1007/s13278-021-00776-6"
    },
    {
      "citation_id": "12",
      "title": "OpenAI. Gpt-4o technical report",
      "year": "2023",
      "venue": "OpenAI. Gpt-4o technical report",
      "arxiv": "arXiv:2303.08774"
    },
    {
      "citation_id": "13",
      "title": "OpenAI. Gpt-4v technical report",
      "year": "2023",
      "venue": "OpenAI. Gpt-4v technical report",
      "arxiv": "arXiv:2303.08774"
    },
    {
      "citation_id": "14",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Tao Xu",
        "Greg Brockman",
        "Christine Mcleavey",
        "Ilya Sutskever"
      ],
      "year": "2023",
      "venue": "Proceedings of the 40th International Conference on Machine Learning, ICML'23"
    },
    {
      "citation_id": "15",
      "title": "A systematic review of assistance robots for elderly care",
      "authors": [
        "Aparna Sharma",
        "Yash Rathi",
        "Vibhor Patni",
        "Deepak Sinha"
      ],
      "year": "2021",
      "venue": "2021 International Conference on Communication information and Computing Technology (ICCICT)",
      "doi": "10.1109/ICCICT50803.2021.9510142"
    },
    {
      "citation_id": "16",
      "title": "Pose-based body language recognition for emotion and psychiatric symptom interpretation",
      "authors": [
        "Zhengyuan Yang",
        "Amanda Kay",
        "Yuncheng Li",
        "Wendi Cross",
        "Jiebo Luo"
      ],
      "year": "2020",
      "venue": "25th International Conference on Pattern Recognition (ICPR)"
    },
    {
      "citation_id": "17",
      "title": "Llama-adapter: Efficient fine-tuning of language models with zero-init attention",
      "authors": [
        "Renrui Zhang",
        "Jiaming Han",
        "Chris Liu",
        "Peng Gao",
        "Aojun Zhou",
        "Xiangfei Hu",
        "Shilin Yan",
        "Pan Lu",
        "Hongsheng Li",
        "Yu Qiao"
      ],
      "year": "2023",
      "venue": "Llama-adapter: Efficient fine-tuning of language models with zero-init attention",
      "arxiv": "arXiv:2303.16199"
    },
    {
      "citation_id": "18",
      "title": "Speech emotion analysis using convolutional neural network (cnn) and gamma classifier-based error correcting output codes (ecoc)",
      "authors": [
        "Yunhao Zhao",
        "Xiaoqing Shu"
      ],
      "year": "2023",
      "venue": "Scientific Reports",
      "doi": "10.1038/s41598-023-47118-4"
    }
  ]
}