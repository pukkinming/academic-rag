{
  "paper_id": "2302.13277v2",
  "title": "Mingling Or Misalignment? Temporal Shift For Speech Emotion Recognition With Pre-Trained Representations",
  "published": "2023-02-26T09:35:24Z",
  "authors": [
    "Siyuan Shen",
    "Feng Liu",
    "Aimin Zhou"
  ],
  "keywords": [
    "Speech emotion recognition",
    "wav2vec 2.0",
    "HuBERT"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Fueled by recent advances of self-supervised models, pretrained speech representations proved effective for the downstream speech emotion recognition (SER) task. Most prior works mainly focus on exploiting pre-trained representations and just adopt a linear head on top of the pre-trained model, neglecting the design of the downstream network. In this paper, we propose a temporal shift module to mingle channel-wise information without introducing any parameter or FLOP. With the temporal shift module, three designed baseline building blocks evolve into corresponding shift variants, i.e. ShiftCNN, ShiftLSTM, and Shiftformer. Moreover, to balance the trade-off between mingling and misalignment, we propose two technical strategies, placement of shift and proportion of shift. The family of temporal shift models all outperforms the state-of-the-art methods on the benchmark IEMOCAP dataset under both finetuning and feature extraction settings. Our code is available at https://github.com/ECNU-Cross-Innovation-Lab/ShiftSER.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech emotion recognition (SER) refers to recognizing human emotion and affective states from audio. Benefiting from different emotional datasets annotated with categorical or dimensional labels, deep neural networks (DNNs) have shown great capability of recognizing emotion from speech efficiently  [1] . The standard practice is training from scratch with conventional features  [2] . However, the relatively small SER datasets  [3] , containing only a few speakers and utterances, and conventional hand-crafted acoustic features restrict the performance  [4] .\n\nInspired by the success of pre-trained features like wav2vec 2.0  [5]  and HuBERT  [6]  in other speech tasks, some re-  searchers begin to validate its superiority over hand-engineered features in SER  [7] . Some works present fusion methods of pre-trained and traditional features  [8]  while others explore task adaptive pre-training strategies for SER  [9] . However, most of previous works mainly focus on exploiting pretrained representations and just adopt linear head on top of the pre-trained model, neglecting the design of downstream network. The building of specialist models for speech downstream tasks is also necessary for pre-trained representations. Therefore, we take initiatives to explore the architecture of the networks for SER on pre-trained representations and propose a parameter-free, FLOP-free temporal shift module to promote channel mingling. We first draw inspirations from the advancement of Transformers  [10]  as well as modern CNNs  [11]  and investigate the common network configurations to set strong baselines. Then, as shown in Figure  1 , we introduce an efficient channel-wise network module, where partial channels are shifted along the temporal dimension by one frame to mingle the past feature to the present one. The core idea of our temporal shift is to introduce channelwise information mingling. It is noteworthy that the features are shifted partially and thus endows the whole model with partial receptive field enlargement, different from the sufficient frame-level receptive field enlargement by stacking pretrained CNN or Transformer blocks. Moreover, our temporal shift serves as the network module similar to the shift operations in computer vision  [12]  rather than the common augmentation applications in audio processing  [13] . However, such channel-wise partial shift seems to go against the characteristic of alignment in many speech tasks  [14] . From the perspective of alignment, the information contained in the shifted channels becomes inaccessible for the original frame, indicating misalignment along with mingling. To balance the trade-off between mingling and misalignment, we propose two strategies for applying temporal shift, including proportion and placement. Proportion of shift is defined to adjust the ratio of shifted channels and the placement of shift can be unified into two manners (Figure  2 ). To the best of our knowledge, we are the first to propose channel-wise shift operation in audio processing and apply temporal shift to three mainstream building blocks. The family of shift models, including ShiftCNN, Shiftformer and ShiftLSTM, all outperform the state-of-the-art methods on the benchmark IEMOCAP dataset under both finetuning and feature extraction settings.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Methodology",
      "text": "In this section, we first illustrate the property of temporal shift and its application strategy. Then we specify the design of basic building blocks and the shift variants.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Temporal Shift",
      "text": "As motivated, our temporal shift is to introduce channel-wise information mingling to the standard building block. Part of the channels will be shifted to its neighbouring time stamp and the rest channels remain as shown in Figure  1 . The unidirectional shift mingles the past feature to the current one while the bidirectional shift combines the future feature with the current one additionally.\n\nNonetheless, the partial shifted data leads to frame misalignment and the original information contained in the shifted channels is inaccessible for that time stamp. Specifically, for each time stamp, the un-shifted features are forced to adapt to the neighboring shifted channels while the shifted channels are moved aside from the position it used to be. To balance the trade-off between mingling and misalignment, we provide two strategies for temporal shift.\n\nProportion of shift. The proportion of shifted channels can be defined as the hyperparameter Î±, adjusting the bargain between mingling and misalignment. Intuitively, if Î± is kept small enough, the negative effect can be negligible. If Î± is set large, the information mingling is promoted but the underlying misalignment may cause performance degradation. We will measure the variants with shift proportion in {1/2, 1/4, 1/8, 1/16}.  shifted representations from the residual connection can serve as a complementary to the original representations, behaving like an ensemble  [15] .\n\nIn addition to technical strategies, the underlying misalignment can be remitted by the SER task from pre-trained representations itself. Specifically, the characteristic of SER task indicates its focus on invariant and discriminative representations rather than alignments between the input and target sequence in other speech tasks  [14] .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Specification Of Temporal Shift Models",
      "text": "We insert temporal shift module into three types of popular building blocks in SER, including convolution neural network, recurrent neural network, and Transformer. The number of channels C matches the representations of pre-trained wav2vec 2.0 Base and HuBERT Base. The number of blocks B is set to ensure the number of parameters roughly the same (9.5M). It is worth noting that our temporal shift can be inserted flexibly and the proportion of shift can be adjusted as the hyperparameter. For uniformity, we adopt the same architecture with fixed proportion in our experiments to validate its effectiveness. The isotropic architectures are summarized as follows.\n\nâ€¢ ShiftCNN: C = (768, 3072, 768), B = 2, Î± = 1/16\n\nâ€¢ Shiftformer: C = (768, 3072, 768), B = 2, Î± = 1/4\n\nCNN. We draw inspirations from the advancement of ConvNext blocks  [11]  and time-channel separable convolution. This pure convolution neural network architecture described in Figure  3 (a) can serve as a strong baseline. For its shift variant ShiftCNN with kernel size 7, we have the last block with residual shift for 1/16 proportion, which enables channel-aligned modeling capability and subtle channel-wise information fusion.\n\nTransformer. We employ the standard Transformer with relative positional encoding (RPE) as shown in Figure  3(b) . However, the standard Transformer with residual shift brings no more improvement (Table  3  residualâ†’shift of Transformer). Inspired by the recent literature  [16] , we further sidestep self-attention and view Transformer as a more general architecture, consisting of token mixer and MLP. The  RNN. Bidirectional long short-term memory (LSTM)  [17]  is set to capture both past and future contexts. ShiftL-STM is constructed by straightforward in-place shift with moderate 1/4 shift proportion.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "In this section, we first observe the main properties of temporal shift module. Next we demonstrate the superiority of the family of shift models on the benchmark dataset IEMOCAP under both finetuning and feature extraction settings. Finally, we ablate key components of our proposed models.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experimental Setup",
      "text": "Dataset. Interactive emotional dyadic motion capture database (IEMOCAP)  [3]  is a widely-used benchmark SER dataset with total length of about 12h, containing five sessions with one male and one female each. We adopt leave-one-sessionout 5-fold cross-validation and four emotion categories, including 1636 happy utterances, 1084 sad utterances, 1103 angry utterances, and 1708 neutral utterances. Each sample is clipped to 7.5 seconds and the sampling rate is 16kHz.  Evaluation. We do comparisons under two configurations, feature extraction and finetuning. Feature extraction  [8]  is to evaluate the sole downstream model, where the frozen pre-trained model is taken as the feature extractor and the downstream models are trained in the supervised manner. And under the finetuning configuration, the parameters of both upstream pre-trained models (only the Transformer layers) and downstream models are updated during training.\n\nThe unweighted accuracy (UA) and weighted accuracy (WA) are used as evaluation metrics. UA is computed as the average over individual class accuracies while WA is the correct prediction over the total samples. The reported accuracy is the average of 5 folds results.\n\nImplementation Details. We conduct experiments with two representative self-supervised pre-trained models, wav2vec 2.0 Base  [5]  and HuBERT Base  [6] . We finetune the models using Adam optimizer with learning rate of 5 Ã— 10 -4 . We use AdamW optimizer with cosine scheduler and decay rate of 0.1 for feature extraction. We train at batch size of 32 for 100 epochs with 5 epochs of linear warm-up.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Properties Of Shift Variants",
      "text": "We conduct experiments on HuBERT feature extraction and observe the behavior of temporal shift. Since our temporal shift module provides channel-wise enlargement of receptive field, the effect is closely related to that of the baseline model. Figure  4 (a) varies CNN kernel sizes to probe the potential of temporal shift, where residual shift brings more gain with small kernels (from 71.3% to 72.1% for size 7) than large ones. Residual shift also achieves better performance than inplace shift for all relatively small kernels. From similar perspective as  [15] , the residual shift makes the model an implicit ensemble composed of shift and origin modules, thus benefiting from both channel-aligned and channel-mingled information at no extra computation. However, the projection shortcut added to bidirectional LSTM in Figure  4 (b) introduces extra parameters and puts residual shift at a disadvantage, leading to nearly 1% degradation. The accuracy of Shiftformer increases steadily with proportion of bidirectional shift in Fig- The results of pre-trained models are cited from  [4]  and others are from original paper. To be consistent with prior works, we adopt wav2vec 2.0 Base features. ure 4(b), surpassing the standard Transformer by about 1% with 27% fewer parameters. This suggests high proportion with bidirectional manner improves temporal modeling for MLP-like architecture and meanwhile avoids high complexity of self-attention operation.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Comparison With State-Of-The-Arts",
      "text": "Baselines. We compare our proposed models with existing state-of-the-art methods under both finetuning and feature extraction settings. Finetuning methods include vanilla finetuning, P-TAPT, TAPT  [9] , CNN-BiLSTM  [7]  and SNP, TAP  [18] . Feature extraction methods include conventional feature (AR-GRU  [2] , SeqCap with NAS  [1] ), feature fusion (eGeMAPS+wav2vec 2.0  [8] , MFCC+Spectrogram+W2E  [19] ) and pre-trained features (wav2vec 2.0  [5] , HuBERT  [6]  and WavLM  [4] ). Moreover, we adopt temporal shift as data augmentation on hidden states for comparison, namely employed randomly just in the training stage.\n\nComparison on finetuning. As shown in Table  1 , the family of temporal shift models outperforms the state-of-the- Comparison on feature extraction. Table  2  is split into multiple parts to include methods adopting different types of features. We follow the evaluation protocol as  [8] , namely adopting the trainable weighted sum of embeddings. Taking the wav2vec 2.0 Base as the feature extractor, our ShiftCNN and Shiftformer outperform all the other methods, even attaining better performance than one of the latest advanced pre-trained model WavLM Large by 1.5%.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Ablation Studies",
      "text": "We conduct ablation studies on CNN, Transformer, and LSTM respectively in Table  3  under both wav2vec 2.0 finetuning and HuBERT feature extraction (dubbed FeatEx.). UA is reported in the table. The advancement of the overall architecture is verified by the ablation of key components, covering types of normalization, convolution and position encoding. Interestingly, Transformer with residual shift fails to catch up with the standard one while our shiftformer, replacing multi-head self-attention (MHSA) with shift operation, outperforming the others with 3.6M fewer parameters. This indicates the benefit of reducing complexity and introducing channel-wise information mingling.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we propose temporal shift module to introduce channel-wise mingling for downstream models. We hope our shift variants inspire future research on downstream model design and look forward further application of temporal shift.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The original speech representation and temporal shift.",
      "page": 1
    },
    {
      "caption": "Figure 2: ). To the best of our knowl-",
      "page": 2
    },
    {
      "caption": "Figure 1: . The uni-",
      "page": 2
    },
    {
      "caption": "Figure 2: (a)) forces the model to ï¬t the",
      "page": 2
    },
    {
      "caption": "Figure 2: (b)) is placed on the branch of the network,",
      "page": 2
    },
    {
      "caption": "Figure 2: Two types of the building blocks of our temporal-shift",
      "page": 2
    },
    {
      "caption": "Figure 3: (a) can serve as a strong baseline. For",
      "page": 2
    },
    {
      "caption": "Figure 3: Block design of CNN, Transformer, and Shiftformer.",
      "page": 3
    },
    {
      "caption": "Figure 3: (c)), closer to MLP-like",
      "page": 3
    },
    {
      "caption": "Figure 4: (a) The impact of temporal shift on ShiftCNN as ker-",
      "page": 3
    },
    {
      "caption": "Figure 4: (a) varies CNN kernel sizes to probe the potential",
      "page": 3
    },
    {
      "caption": "Figure 4: (b) introduces extra",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table 3: residualâ†’shift of Trans-",
      "data": [
        {
          "Column_1": "mationcontainedinthe\nShift\nefortheoriginalframe,\ningling. Tobalancethe Shift Model Model\nalignment, we propose\nshift, including propor- (a) In-placeShift. (b) ResidualShift.\nhift is defined to adjust\nlacementofshiftcanbe Fig.2. Twotypesofthebuildingblocksofourtemporal-shift\nothebestofourknowl- networks,namelyin-placeshiftandresidualshift.\nnel-wiseshiftoperation\nral shift to three main-\nshiftedrepresentationsfromtheresidualconnectioncanserve\nshiftmodels,including\nasacomplementarytotheoriginalrepresentations,behaving",
          "Shift\nShift Model Model\n(a) In-placeShift. (b) ResidualShift.\n2. Twotypesofthebuildingblocksofourtemporal-shift\nworks,namelyin-placeshiftandresidualshift.\ntedrepresentationsfromtheresidualconnectioncanserve": "Shift\nShift Model Model\n(a) In-placeShift. (b) ResidualShift.\n2. Twotypesofthebuildingblocksofourtemporal-shift\nworks,namelyin-placeshiftandresidualshift.\ntedrepresentationsfromtheresidualconnectioncanserve"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "",
          "\u0000\u001a\u0000\u0015\u0000\u0011\u0000\u0013\n\u0000\u001a\u0000\u0014\u0000\u0011\u0000\u0018\nMulti-head Shift\nSelf-attention \u0000\u001a\u0000\u0014\u0000\u0011\u0000\u0018 \u0000\f\u0000\b\u0000\u000b\u0000\u0003\u0000$\u00008 \u0000\u001a\u0000\u0014\u0000\u0011\u0000\u0013 \u0000\f\u0000\b\u0000\u000b\u0000\u0003\u0000$\u00008\n\u0000\u001a\u0000\u0013\u0000\u0011\u0000\u0018\n1D depthwise [ğ‘‡,ğ¶] [ğ‘‡,ğ¶] \u0000\u001a\u0000\u0014\u0000\u0011\u0000\u0013\n\u0000L\u0000Q\u0000\u0010\u0000S\u0000O\u0000D\u0000F\u0000H\u0000\u0003\u0000V\u0000K\u0000L\u0000I\u0000W \u0000\u001a\u0000\u0013\u0000\u0011\u0000\u0013\nConv LayerNorm Shift [ğ‘‡,ğ¶] \u0000\u001a\u0000\u0013\u0000\u0011\u0000\u0018 \u0000U\u0000H\u0000V\u0000L\u0000G\u0000X\u0000D\u0000O\u0000\u0003\u0000V\u0000K\u0000L\u0000I\u0000W \u0000%\u0000D\u0000V\u0000H\u0000O\u0000L\u0000Q\u0000H \u0000\u0019\u0000\u001c\u0000\u0011\u0000\u0018\n\u0000\u0016 \u0000\u001a \u0000\u0014\u0000\u0016 \u0000\u0016\u0000\u0014 \u0000\u0014\u0000\u0012\u0000\u0014\nLayerNorm \u0000.\u0000H\u0000U\u0000Q\u0000H\u0000O\u0000\u0003\u00006\u0000L\u0000]\u0000H\nLinear Layer Linear Layer (a)\n1D pointwise [ğ‘‡,ğ¶Ã—4] [ğ‘‡,ğ¶Ã—4]\nConv GELU GELU Fig.4. (a)Theimpactoftemporalsh\n[ğ‘‡,ğ¶Ã—4] nelsizevaries. (b)Theimpactofshif\nGELU STMandShiftformer.\nLinear Layer Linear Layer\n1D pointwise [ğ‘‡,ğ¶] [ğ‘‡,ğ¶]\nConv Evaluation. We do comparison\n[ğ‘‡,ğ¶] LayerNorm LayerNorm tions, feature extraction and finetuni\n[8] is to evaluate the sole downstre": "\u0000\u001a\u0000\u0015\u0000\u0011\u0000\u0013\nMulti-head Shift\nSelf-attention \u0000\u001a\u0000\u0014\u0000\u0011\u0000\u0018 \u0000\f\u0000\b\u0000\u000b\u0000\u0003\u0000$\u00008\n1D depthwise [ğ‘‡,ğ¶] [ğ‘‡,ğ¶] \u0000\u001a\u0000\u0014\u0000\u0011\u0000\u0013\n\u0000L\u0000Q\u0000\u0010\u0000S\u0000O\u0000D\u0000F\u0000H\u0000\u0003\u0000V\u0000K\u0000L\u0000I\u0000W\nConv LayerNorm Shift [ğ‘‡,ğ¶] \u0000\u001a\u0000\u0013\u0000\u0011\u0000\u0018 \u0000U\u0000H\u0000V\u0000L\u0000G\u0000X\u0000D\u0000O\u0000\u0003\u0000V\u0000K\u0000L\u0000I\u0000W \u0000%\u0000D\u0000V\u0000H\u0000O\u0000L\u0000Q\u0000H\n\u0000\u0016 \u0000\u001a \u0000\u0014\u0000\u0016 \u0000\u0016\u0000\u0014\nLayerNorm \u0000.\u0000H\u0000U\u0000Q\u0000H\u0000O\u0000\u0003\u00006\u0000L\u0000]\u0000H\nLinear Layer Linear Layer (a)\n1D pointwise [ğ‘‡,ğ¶Ã—4] [ğ‘‡,ğ¶Ã—4]\nConv GELU GELU Fig.4. (a)Theimpactoftem\n[ğ‘‡,ğ¶Ã—4] nelsizevaries. (b)Theimpa\nGELU STMandShiftformer.\nLinear Layer Linear Layer\n1D pointwise [ğ‘‡,ğ¶] [ğ‘‡,ğ¶]\nConv Evaluation. We do co\n[ğ‘‡,ğ¶] LayerNorm LayerNorm tions, feature extraction and\n[8] is to evaluate the sole"
        },
        {
          "Column_1": "Multi-head Shift\nSelf-attention\n1D depthwise [ğ‘‡,ğ¶] [ğ‘‡,ğ¶]\nConv LayerNorm Shift\n[ğ‘‡,ğ¶]\nLayerNorm\nLinear Layer Linear Layer\n1D pointwise [ğ‘‡,ğ¶Ã—4] [ğ‘‡,ğ¶Ã—4]\nConv GELU GELU\n[ğ‘‡,ğ¶Ã—4]\nGELU\nLinear Layer Linear Layer\n1D pointwise [ğ‘‡,ğ¶] [ğ‘‡,ğ¶]\nConv\n[ğ‘‡,ğ¶] LayerNorm LayerNorm\n(a) CNN (b) Transformer (c) Shiftformer",
          "Column_2": "",
          "\u0000\u001a\u0000\u0015\u0000\u0011\u0000\u0013\n\u0000\u001a\u0000\u0014\u0000\u0011\u0000\u0018\nMulti-head Shift\nSelf-attention \u0000\u001a\u0000\u0014\u0000\u0011\u0000\u0018 \u0000\f\u0000\b\u0000\u000b\u0000\u0003\u0000$\u00008 \u0000\u001a\u0000\u0014\u0000\u0011\u0000\u0013 \u0000\f\u0000\b\u0000\u000b\u0000\u0003\u0000$\u00008\n\u0000\u001a\u0000\u0013\u0000\u0011\u0000\u0018\n1D depthwise [ğ‘‡,ğ¶] [ğ‘‡,ğ¶] \u0000\u001a\u0000\u0014\u0000\u0011\u0000\u0013\n\u0000L\u0000Q\u0000\u0010\u0000S\u0000O\u0000D\u0000F\u0000H\u0000\u0003\u0000V\u0000K\u0000L\u0000I\u0000W \u0000\u001a\u0000\u0013\u0000\u0011\u0000\u0013\nConv LayerNorm Shift [ğ‘‡,ğ¶] \u0000\u001a\u0000\u0013\u0000\u0011\u0000\u0018 \u0000U\u0000H\u0000V\u0000L\u0000G\u0000X\u0000D\u0000O\u0000\u0003\u0000V\u0000K\u0000L\u0000I\u0000W \u0000%\u0000D\u0000V\u0000H\u0000O\u0000L\u0000Q\u0000H \u0000\u0019\u0000\u001c\u0000\u0011\u0000\u0018\n\u0000\u0016 \u0000\u001a \u0000\u0014\u0000\u0016 \u0000\u0016\u0000\u0014 \u0000\u0014\u0000\u0012\u0000\u0014\nLayerNorm \u0000.\u0000H\u0000U\u0000Q\u0000H\u0000O\u0000\u0003\u00006\u0000L\u0000]\u0000H\nLinear Layer Linear Layer (a)\n1D pointwise [ğ‘‡,ğ¶Ã—4] [ğ‘‡,ğ¶Ã—4]\nConv GELU GELU Fig.4. (a)Theimpactoftemporalsh\n[ğ‘‡,ğ¶Ã—4] nelsizevaries. (b)Theimpactofshif\nGELU STMandShiftformer.\nLinear Layer Linear Layer\n1D pointwise [ğ‘‡,ğ¶] [ğ‘‡,ğ¶]\nConv Evaluation. We do comparison\n[ğ‘‡,ğ¶] LayerNorm LayerNorm tions, feature extraction and finetuni\n[8] is to evaluate the sole downstre": "Multi-head Shift\nSelf-attention\n1D depthwise [ğ‘‡,ğ¶] [ğ‘‡,ğ¶]\nConv LayerNorm Shift\n[ğ‘‡,ğ¶]\nLayerNorm\nLinear Layer Linear Layer\n1D pointwise [ğ‘‡,ğ¶Ã—4] [ğ‘‡,ğ¶Ã—4]\nConv GELU GELU\n[ğ‘‡,ğ¶Ã—4]\nGELU\nLinear Layer Linear Layer\n1D pointwise [ğ‘‡,ğ¶] [ğ‘‡,ğ¶]\nConv\n[ğ‘‡,ğ¶] LayerNorm LayerNorm"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "\u0000\u001a\u0000\u0014\u0000\u0011\u0000\u0018\n\u0000\u001a\u0000\u0014\u0000\u0011\u0000\u0013 \u0000\f\u0000\b\u0000\u000b\u0000\u0003\u0000$\u00008\n\u0000\u001a\u0000\u0013\u0000\u0011\u0000\u0018\n\u0000\u001a\u0000\u0013\u0000\u0011\u0000\u0013\n\u0000\u0019\u0000\u001c\u0000\u0011\u0000\u0018": "",
          "Column_2": "",
          "Column_3": "\u0000L\u0000Q\u0000\u0010\u0000S\u0000O\u0000D\u0000F\u0000H\u0000\u0003\u0000/\u00006\u00007\u00000\n\u0000U\u0000H\u0000V\u0000L\u0000G\u0000X\u0000D\u0000O\u0000\u0003\u0000/\u00006\u00007\u00000\n\u0000X\u0000Q\u0000L\u0000\u0010\u0000V\u0000K\u0000L\u0000I\u0000W\u0000I\u0000R\u0000U\u0000P\u0000H\u0000U\n\u0000E\u0000L\u0000\u0010\u0000V\u0000K\u0000L\u0000I\u0000W\u0000I\u0000R\u0000U\u0000P\u0000H\u0000U\n\u0000/\u00006\u00007\u00000\n\u00007\u0000U\u0000D\u0000Q\u0000V\u0000I\u0000R\u0000U\u0000P\u0000H\u0000U"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "1D depthwise\nConv\n[ğ‘‡,ğ¶]",
          "Column_4": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "LayerNorm",
          "Column_4": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "[ğ‘‡,ğ¶Ã—4]"
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "GELU",
          "Column_4": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "1D pointwise\nConv",
          "Column_4": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "[ğ‘‡,ğ¶]"
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Neural architecture search for speech emotion recognition",
      "authors": [
        "Xixin Wu",
        "Shoukang Hu",
        "Zhiyong Wu",
        "Xunying Liu",
        "Helen Meng"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "3",
      "title": "A novel attention-based gated recurrent unit and its efficacy in speech emotion recognition",
      "authors": [
        "Srividya Tirunellai Rajamani",
        "Adria Kumar T Rajamani",
        "Shuo Mallol-Ragolta",
        "BjÃ¶rn Liu",
        "Schuller"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "4",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "5",
      "title": "Wavlm: Large-scale self-supervised pre-training for full stack speech processing",
      "authors": [
        "Sanyuan Chen",
        "Chengyi Wang",
        "Zhengyang Chen",
        "Yu Wu",
        "Shujie Liu",
        "Zhuo Chen",
        "Jinyu Li",
        "Naoyuki Kanda",
        "Takuya Yoshioka",
        "Xiong Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "6",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "7",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "Wei-Ning Hsu",
        "Benjamin Bolte",
        "Hubert Yao-Hung",
        "Kushal Tsai",
        "Ruslan Lakhotia",
        "Abdelrahman Salakhutdinov",
        "Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "8",
      "title": "Temporal context in speech emotion recognition",
      "authors": [
        "Yangyang Xia",
        "Li-Wei Chen",
        "Alexander Rudnicky",
        "Richard Stern"
      ],
      "venue": "Proc. Interspeech, 2021"
    },
    {
      "citation_id": "9",
      "title": "Emotion Recognition from Speech Using",
      "authors": [
        "Leonardo Pepino",
        "Pablo Riera",
        "Luciana Ferrer"
      ],
      "venue": "Emotion Recognition from Speech Using"
    },
    {
      "citation_id": "10",
      "title": "Proc. Interspeech",
      "authors": [
        "Embeddings"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "11",
      "title": "Exploring wav2vec 2.0 fine-tuning for improved speech emotion recognition",
      "authors": [
        "Li-Wei Chen",
        "Alexander Rudnicky"
      ],
      "year": "2021",
      "venue": "Exploring wav2vec 2.0 fine-tuning for improved speech emotion recognition",
      "arxiv": "arXiv:2110.06309"
    },
    {
      "citation_id": "12",
      "title": "Advances in neural information processing systems",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Lukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "13",
      "title": "A convnet for the 2020s",
      "authors": [
        "Zhuang Liu",
        "Hanzi Mao",
        "Chao-Yuan Wu",
        "Christoph Feichtenhofer",
        "Trevor Darrell",
        "Saining Xie"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "14",
      "title": "Tsm: Temporal shift module for efficient video understanding",
      "authors": [
        "Ji Lin",
        "Chuang Gan",
        "Song Han"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "15",
      "title": "CyclicAugment: Speech Data Random Augmentation with Cosine Annealing Scheduler for Auotmatic Speech Recognition",
      "authors": [
        "Zhihan Wang",
        "Feng Hou",
        "Yuanhang Qiu",
        "Zhizhong Ma",
        "Satwinder Singh",
        "Ruili Wang"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "16",
      "title": "Sequence transduction with recurrent neural networks",
      "authors": [
        "Alex Graves"
      ],
      "year": "2012",
      "venue": "Sequence transduction with recurrent neural networks",
      "arxiv": "arXiv:1211.3711"
    },
    {
      "citation_id": "17",
      "title": "Residual networks behave like ensembles of relatively shallow networks",
      "authors": [
        "Andreas Veit",
        "Michael Wilber",
        "Serge Belongie"
      ],
      "year": "2016",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "18",
      "title": "Metaformer is actually what you need for vision",
      "authors": [
        "Weihao Yu",
        "Mi Luo",
        "Pan Zhou",
        "Chenyang Si",
        "Yichen Zhou",
        "Xinchao Wang",
        "Jiashi Feng",
        "Shuicheng Yan"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "19",
      "title": "Long shortterm memory",
      "authors": [
        "Sepp Hochreiter",
        "JÃ¼rgen Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural computation"
    },
    {
      "citation_id": "20",
      "title": "Speaker normalization for self-supervised speech emotion recognition",
      "authors": [
        "Itai Gat",
        "Hagai Aronowitz",
        "Weizhong Zhu",
        "Edmilson Morais",
        "Ron Hoory"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "21",
      "title": "Speech emotion recognition with co-attention based multi-level acoustic information",
      "authors": [
        "Heqing Zou",
        "Yuke Si",
        "Chen Chen",
        "Deepu Rajan",
        "Eng Siong"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    }
  ]
}