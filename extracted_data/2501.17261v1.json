{
  "paper_id": "2501.17261v1",
  "title": "Nus-Emo At Semeval-2024 Task 3: Instruction-Tuning Llm For Multimodal Emotion-Cause Analysis In Conversations",
  "published": "2024-08-22T08:34:39Z",
  "authors": [
    "Meng Luo",
    "Han Zhang",
    "Shengqiong Wu",
    "Bobo Li",
    "Hong Han",
    "Hao Fei"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This paper describes the architecture of our system developed for Task 3 of SemEval-2024: Multimodal Emotion-Cause Analysis in Conversations. Our project targets the challenges of subtask 2, dedicated to Multimodal Emotion-Cause Pair Extraction with Emotion Category (MECPE-Cat), and constructs a dualcomponent system tailored to the unique challenges of this task. We divide the task into two subtasks: emotion recognition in conversation (ERC) and emotion-cause pair extraction (ECPE). To address these subtasks, we capitalize on the abilities of Large Language Models (LLMs), which have consistently demonstrated state-of-the-art performance across various natural language processing tasks and domains. Most importantly, we design an approach of emotion-cause-aware instructiontuning for LLMs, to enhance the perception of the emotions with their corresponding causal rationales. Our method enables us to adeptly navigate the complexities of MECPE-Cat, achieving a weighted average 34.71% F1 score of the task, and securing the 2 nd rank on the leaderboard. 1 The code and metadata to reproduce our experiments are all made publicly available. 2 * Equal contributions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion cause analysis is a critical component of human communication and decision-making, offering substantial applications across diverse fields. It enables a deeper and more detailed understanding of sentiments. The introduction of emotioncause analysis in textual conversations by  Poria et al. (2021)  has paved the way for advancements in understanding emotional dynamics within dialogues. However, textual analysis alone does not fully capture the complexity of human emotional expression, as emotions and their causes are often conveyed through a blend of modalities  (Hazarika et al., 2018; Wu et al., 2023a; Fei et al., 2023b) . Subtask 2 of SemEval-2024 Task 3, referred to as MECPE-Cat, seeks to expand this analysis into the multimodal domain, focusing on English-language conversations. The task draws inspiration from the seminal work of  Wang et al. (2021) , which sets out to jointly extract emotions and their corresponding causes from conversations across multiple modalities, including text, audio, and video, and it also encompasses the identification of the corresponding emotion category for each emotion-cause pair.\n\nIn our system, we leverage LLMs such as GPT-3  (Brown et al., 2020) ,  Flan-T5 (Chung et al., 2022) , and GLM  (Du et al., 2021)  known for their exceptional performance in various natural language processing tasks. We employ parameter-efficient fine-tuning, specifically LoRA  (Hu et al., 2021) , to efficiently fine-tune LLMs, enhancing their performance with minimal computational overhead. Additionally, we harness emotion-cause-aware prompt-based learning and instruction-tuning to enhance model performance such that the LLMs can more accurately perceive the emotions with their corresponding causal rationales. Prompt-based learning guides LLMs to generate contextually relevant outputs, while instruction-fine-tuning models for our specific tasks by improving their response to explicit instructions.\n\nIn this paper, we investigate the optimal LLM for the MECPE-Cat task, selecting ChatGLM based on its superior zero-shot performance. We further refine ChatGLM through instruction-tuning, using carefully crafted prompts to enhance its taskspecific accuracy. Our fine-tuned model achieves the second-highest score on the official test set for subtask 2, with a weighted average of 34.71% F1, underscoring the effectiveness of our approach. We also discuss the current limitations of our model and methodology, alongside directions for future research and improvement. We will release our codes and resources mentioned in this paper to facilitate relevant research.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Background",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Task And Dataset Description",
      "text": "The SemEval-2024 Task 3 is based on the multimodal conversational emotion-cause dataset, Emotion-Cause-in-Friends (ECF;  Wang et al., 2021) , by choosing a multimodal dataset MELD  (Poria et al., 2018)  as the data source and further annotating the corresponding causes for the given emotion annotations. The ECF dataset contains 9,794 emotion-cause pairs, covering three modalities. The subtask 2 is to extract all emotion-cause pairs in a given conversation under three modalities, where each pair contains an emotion utterance along with its emotion category and a cause utterance, e.g., (U3_Joy, U2), which means that the speaker's joy emotion in utterance 3 is triggered by the cause from utterance 2. Figure  1  displays a real example of this task and annotated dataset. In this conversation, it is expected to extract a set of six utterance-level emotion-cause pairs in total, e.g., Chandler's Joy emotion in Utterance 4 (U4 for short) is triggered by the objective cause that he and Monica had made up and Monica's subjective opinion in U3, forming the pairs (U4_joy, U2) and (U4_joy, U3); The cause for Phoebe's Disgust in U5 is the objective event that Monica and Chandler were kissing in front of her (mainly reflected in the visual modality of U5), forming the pair (U5_disgust, U5).",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "The exploration of ECPE within textual and conversational contexts has been approached through various methodologies, each tailored to specific task settings  (Chen et al., 2022) .  Cheng et al. (2023)  reframe the ECPE task as a process akin to engaging in a two-stage machine reading comprehension (MRC) challenge.  Zheng et al. (2023)  expand the ECPE task to Emotion-Cause Quadruple Extraction in Dialogs (ECQED), focusing on detecting pairs of emotion-cause utterances and their types. They present a model utilizing a heterogeneous graph and a parallel grid tagging scheme for this purpose. In addressing the specific challenge of the MECPE-Cat task,  Wang et al. (2021)  set a benchmark for this task by introducing two preliminary baseline systems. They utilize a heuristic approach to leverage inherent patterns in the localization of causes and emotions, alongside a deep learning strategy, MECPE-2steps, which adapts a prominent ECPE methodology for news articles to include multimodal data.\n\nDrawing from the varied methodologies of previous work, it becomes clear that effectively solving the MECPE-Cat task demands a deep understanding of dialogue content, precise identification of conversational emotions, extraction of emotioncause pairs, and the integration of multimodal information. Motivated by the strong performance of LLMs on various metrics, we opt to utilize these models to address this intricate challenge. Through exhaustive model evaluations and extensive prompt testing, we have showcased the practicality, superiority, and adaptability of our chosen approach.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methodology",
      "text": "In this section, we first conduct preliminary experiments to determine which LLM to select as a backbone reasoner. We then elaborate on how we design the system and emotion-cause-aware instructions for tuning our chosen LLM.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Pilot Study For Llm Selection",
      "text": "Currently, there exists a variety of LLMs, such as OPT-IML  (Iyer et al., 2022) , GPT-3, Flan-T5, and GLM. However, it is essential to select a model that not only performs optimally but is also the most suitable for our specific task. To this end, we carry out a pilot study to determine the most appropriate model selection. For our zero-shot testing experiment, we rigorously evaluate several models, including OPT-IML 3  , Instruct-GPT 4    (Ouyang et al., 2022) , Flan-T5 5  , alongside the ChatGLM models, to identify the most effective tool for this specific task. We customize instructions for each model's specific tuning style, recognizing that a single set of instructions does not suit all models effectively. We also embed expected output labels within these instructions to secure precise responses from each model. Figure  2  depicts the zero-shot performance of these models. The ChatGLM 6  LLM is ultimately selected based on its superior performance in these tests. This selection is informed not merely by the innovative features or the advanced training methodologies of ChatGLM but by empirical evidence of its exceptional zero-shot performance among the models considered.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Multimodal Feature Encoding",
      "text": "Given that the inputs for our task incorporate multimodal signals, including visual information to assist in more accurate emotion recognition, it is imperative to fully leverage the non-textual modal information. However, our LLM backbone does not natively support the direct inclusion of nontextual modal signals. To address this, we consider employing ImageBind  (Girdhar et al., 2023)  for encoding the multimodal portion of input information, owing to its robust multimodal alignment capabilities and visual perception proficiency. Subsequently, we concatenate the multimodal representations with other textual embeddings before feeding them into the LLM.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Constructing Emotion-Cause-Aware",
      "text": "Instructions for LLM Tuning\n\nFigure  3  first illustrates the workflow of our proposed framework. Initially, we fine-tune the model on the ERC task. Following this, we incorporate the predicted emotion labels into each utterance, setting the stage for the ECPE task execution. Subsequently, we employ the model, now fine-tuned with data labeled with emotion tags, to perform in-ference on the MECPE-Cat task, yielding an initial set of emotion-cause pairs. These preliminary results are then reintegrated into the original training dataset for a second round of fine-tuning, culminating in the refinement of our model to produce the final set of emotion-cause pairs.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Question:",
      "text": "The emotion-cause indices of the target utterance are:\n\nTo enhance the perception of identifying emotion-cause pairs and mitigate the task's inherent complexity and potential confusion, we design the template for producing emotion-cause-aware instructions to guide the model. Figure  4  illustrates the construction of the instruction template, which encompasses the task definition, a demonstration example, and the dataset for which the model is expected to predict outcomes. This structured approach not only simplifies the task's complexity for the model but also aligns the model's processing capabilities with the requirements of accurately identifying emotion-cause pairs in conversations. In the above box we showcase a real example.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiments",
      "text": "This section will quantify the effectiveness of our systems via experiments and also show more analyses to gain more observations.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Implementation",
      "text": "The hyperparameter of our system used to achieve the highest weighted average F1 score on the subtask 2 is listed in 1. The ChatGLM model was fine-tuned using a learning rate of 1e-4 with LoRAspecific configurations including a rank of 8, alpha value of 32, and a dropout rate of 0.1. The training was conducted with a maximum instruction length of 2048 tokens and an output length limited to 128 tokens, using a batch size of 1. We used a single gradient accumulation step across 2 training epochs. These parameters were meticulously selected to optimize our model's performance. We here perform an ablation study on the contributions of each part of the instructions we designed for the task. We derive three variants:\n\n• Only Task Definition: Compared to the zeroshot paradigm, this condition offers a more detailed and precise description of the task. • Task + Example: We provide a demonstrative example to clearly show the expected outcome in a real-world dialogue, offering the model a practical reference for task execution • Task + Example + Candidate utterances:\n\nThis design simplifies the task by introducing 'candidate utterances,' enabling the model to analyze emotion-cause pairs sentence by sentence, rather than across entire dialogues, and pinpoint the specific causes of emotions from the preceding content. Table  2  demonstrates the comparative performance of these diverse templates. We see that different components of the instruction templates show clear influences, such as task definition, example demonstration, and candidate utterances. Thus, we apply all these components into our instruction templates.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Instruction-Tuning Llm",
      "text": "For our experiments, we adopt a meticulous finetuning process for the ChatGLM. We set a learning rate of 1e-4, aiming for a balance between rapid convergence and maintaining the model's ability to adapt without overfitting. We leverage the LoRA technique with a rank of 8 and alpha of 32 to introduce task-adaptive parameters without bloating the model size, alongside a dropout rate of 0.1 to prevent overfitting. The model processed inputs with a max sequence length of 2048 tokens, accommodating the depth of context required for our task, while the outputs are capped at 128 tokens to focus on generating concise and relevant responses. Both batch size and gradient accumulation steps are set to 1, tailored to our computational resources while ensuring effective backpropagation. This configuration, selected after careful evaluation of various setups, is instrumental in fine-tuning the ChatGLM model to achieve the best performance on our task.\n\nOur experiments capitalize on the robust computational capabilities provided by NVIDIA A800-SXM GPUs, each boasting 80 GB of VRAM, to ensure sufficient resources are available to train large language models. This fine-tuning process is facilitated using a customized script derived from the Hugging Face Transformers framework, chosen for its extensive support of transformer models and seamless integration with our setup, thereby enabling us to leverage advanced hardware capabilities while utilizing a leading-edge software environment for our model's optimization.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Task Decomposition",
      "text": "We decompose the MECPE-Cat task into ERC and ECPE phases to strategically alleviate its complexity. This division offered a two-fold advantage: firstly, it distills the task into clearer, more focused components, facilitating a more straightforward understanding and execution of the model. Secondly, by leveraging emotion labels obtained from the ERC phase during the ECPE phase, we enhance the model's capability to pinpoint emotion-cause pairs with greater accuracy. Tabel 3 showcases incremental improvements in weighted average F1 scores across three distinct setups. This progression underscores the dual benefits of our approach: simplifying the task's complexity for the model and enriching the ECPE phase with contextual emotion labels, thereby optimizing the extraction of emotion-cause pairs.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Methods",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Multimodal Integration",
      "text": "To assess the impact of multimodal information on our model's performance, we adopt a methodological approach that harnessed GPT-4V  Achiam et al. (2023)  for extracting insights from modalities beyond text. Specifically, we enrich the instruction template with \"video description of target utterance\" derived from GPT-4V, presenting it as supplementary information to guide the model. This strategic integration of multimodal data leads to an improvement in the model's F1 score, as shown in Table  5 , which validates the utility of multimodal information in providing richer contextual understanding.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we explore the LLMs for solving the Multimodal Emotion-Cause Pair Extraction with Emotion Category (MECPE-Cat) task. Through a pilot study, we first select an LLM, ChatGLM, that assists in achieving optimal task performance. The backbone ChatGLM receives textual dialogue, and also perceives the multimodal information via the ImageBind vision encoder. Lastly, we devise an emotion-cause-aware instruction-tuning mechanism for updating LLMs, which enhances the perception of the emotions with their corresponding causal rationales. Our system achieves a weighted average F1 score of 34.71%, securing second place on the MECPE-Cat leaderboard.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: An example of an official task and annotated dataset. Each arc points from the cause utterance to the",
      "page": 2
    },
    {
      "caption": "Figure 2: Zero-shot test set performance of various",
      "page": 3
    },
    {
      "caption": "Figure 2: depicts the zero-shot performance",
      "page": 3
    },
    {
      "caption": "Figure 3: Proposed method workflow for the MECPE-",
      "page": 3
    },
    {
      "caption": "Figure 4: The construction of the instruction template",
      "page": 3
    },
    {
      "caption": "Figure 3: first illustrates the workflow of our pro-",
      "page": 3
    },
    {
      "caption": "Figure 4: illustrates",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "our experiments are all made publicly avail-": ""
        },
        {
          "our experiments are all made publicly avail-": "able.2"
        },
        {
          "our experiments are all made publicly avail-": ""
        },
        {
          "our experiments are all made publicly avail-": "Introduction"
        },
        {
          "our experiments are all made publicly avail-": ""
        },
        {
          "our experiments are all made publicly avail-": "Emotion cause analysis is a critical component of"
        },
        {
          "our experiments are all made publicly avail-": ""
        },
        {
          "our experiments are all made publicly avail-": "human communication and decision-making, of-"
        },
        {
          "our experiments are all made publicly avail-": ""
        },
        {
          "our experiments are all made publicly avail-": "fering substantial applications across diverse fields."
        },
        {
          "our experiments are all made publicly avail-": ""
        },
        {
          "our experiments are all made publicly avail-": "It enables a deeper and more detailed understand-"
        },
        {
          "our experiments are all made publicly avail-": ""
        },
        {
          "our experiments are all made publicly avail-": "ing of sentiments. The introduction of emotion-"
        },
        {
          "our experiments are all made publicly avail-": ""
        },
        {
          "our experiments are all made publicly avail-": "cause analysis in textual conversations by Poria"
        },
        {
          "our experiments are all made publicly avail-": ""
        },
        {
          "our experiments are all made publicly avail-": "et al. (2021) has paved the way for advancements"
        },
        {
          "our experiments are all made publicly avail-": ""
        },
        {
          "our experiments are all made publicly avail-": "in understanding emotional dynamics within dia-"
        },
        {
          "our experiments are all made publicly avail-": ""
        },
        {
          "our experiments are all made publicly avail-": "logues. However, textual analysis alone does not"
        },
        {
          "our experiments are all made publicly avail-": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "Abstract",
          "haofei37@nus.edu.sg": "fully capture the complexity of human emotional"
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "",
          "haofei37@nus.edu.sg": "expression, as emotions and their causes are often"
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "This paper describes the architecture of our",
          "haofei37@nus.edu.sg": ""
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "",
          "haofei37@nus.edu.sg": "conveyed through a blend of modalities (Hazarika"
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "system developed\nfor Task\n3\nof SemEval-",
          "haofei37@nus.edu.sg": ""
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "",
          "haofei37@nus.edu.sg": "et al., 2018; Wu et al., 2023a; Fei et al., 2023b)."
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "2024: Multimodal Emotion-Cause Analysis in",
          "haofei37@nus.edu.sg": ""
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "Conversations. Our project\ntargets the chal-",
          "haofei37@nus.edu.sg": "Subtask 2 of SemEval-2024 Task 3, referred to as"
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "lenges of subtask 2, dedicated to Multimodal",
          "haofei37@nus.edu.sg": "MECPE-Cat, seeks to expand this analysis into the"
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "Emotion-Cause Pair Extraction with Emotion",
          "haofei37@nus.edu.sg": "multimodal domain, focusing on English-language"
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "Category (MECPE-Cat), and constructs a dual-",
          "haofei37@nus.edu.sg": ""
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "",
          "haofei37@nus.edu.sg": "conversations. The task draws inspiration from the"
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "component system tailored to the unique chal-",
          "haofei37@nus.edu.sg": ""
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "",
          "haofei37@nus.edu.sg": "seminal work of Wang et al. (2021), which sets out"
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "lenges of\nthis task. We divide the task into",
          "haofei37@nus.edu.sg": ""
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "",
          "haofei37@nus.edu.sg": "to jointly extract emotions and their corresponding"
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "two subtasks: emotion recognition in conversa-",
          "haofei37@nus.edu.sg": ""
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "",
          "haofei37@nus.edu.sg": "causes from conversations across multiple modal-"
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "tion (ERC) and emotion-cause pair extraction",
          "haofei37@nus.edu.sg": ""
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "",
          "haofei37@nus.edu.sg": "ities, including text, audio, and video, and it also"
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "(ECPE). To address these subtasks, we capi-",
          "haofei37@nus.edu.sg": ""
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "talize on the abilities of Large Language Mod-",
          "haofei37@nus.edu.sg": "encompasses the identification of the correspond-"
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "els (LLMs), which have consistently demon-",
          "haofei37@nus.edu.sg": "ing emotion category for each emotion-cause pair."
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "strated state-of-the-art performance across var-",
          "haofei37@nus.edu.sg": ""
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "",
          "haofei37@nus.edu.sg": "In our system, we leverage LLMs such as GPT-3"
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "ious natural\nlanguage processing tasks\nand",
          "haofei37@nus.edu.sg": ""
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "",
          "haofei37@nus.edu.sg": "(Brown et al., 2020), Flan-T5 (Chung et al., 2022),"
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "domains.\nMost\nimportantly, we design an",
          "haofei37@nus.edu.sg": ""
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "",
          "haofei37@nus.edu.sg": "and GLM (Du et al., 2021) known for\ntheir ex-"
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "approach of emotion-cause-aware instruction-",
          "haofei37@nus.edu.sg": ""
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "",
          "haofei37@nus.edu.sg": "ceptional performance in various natural language"
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "tuning for LLMs, to enhance the perception of",
          "haofei37@nus.edu.sg": ""
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "",
          "haofei37@nus.edu.sg": "processing tasks. We employ parameter-efficient"
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "the emotions with their corresponding causal ra-",
          "haofei37@nus.edu.sg": ""
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "tionales. Our method enables us to adeptly nav-",
          "haofei37@nus.edu.sg": "fine-tuning, specifically LoRA (Hu et al., 2021),"
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "igate the complexities of MECPE-Cat, achiev-",
          "haofei37@nus.edu.sg": "to efficiently fine-tune LLMs, enhancing their per-"
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "ing a weighted average 34.71% F1 score of the",
          "haofei37@nus.edu.sg": "formance with minimal computational overhead."
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "task, and securing the 2nd rank on the leader-",
          "haofei37@nus.edu.sg": ""
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "",
          "haofei37@nus.edu.sg": "Additionally, we\nharness\nemotion-cause-aware"
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "board.1 The code and metadata to reproduce",
          "haofei37@nus.edu.sg": ""
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "",
          "haofei37@nus.edu.sg": "prompt-based learning and instruction-tuning to en-"
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "our experiments are all made publicly avail-",
          "haofei37@nus.edu.sg": ""
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "",
          "haofei37@nus.edu.sg": "hance model performance such that the LLMs can"
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "able.2",
          "haofei37@nus.edu.sg": ""
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "",
          "haofei37@nus.edu.sg": "more accurately perceive the emotions with their"
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "Introduction",
          "haofei37@nus.edu.sg": "corresponding causal\nrationales.\nPrompt-based"
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "",
          "haofei37@nus.edu.sg": "learning guides LLMs to generate contextually rel-"
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "",
          "haofei37@nus.edu.sg": ""
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "",
          "haofei37@nus.edu.sg": "evant outputs, while instruction-fine-tuning models"
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "human communication and decision-making, of-",
          "haofei37@nus.edu.sg": ""
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "",
          "haofei37@nus.edu.sg": "for our specific tasks by improving their response"
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "fering substantial applications across diverse fields.",
          "haofei37@nus.edu.sg": ""
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "",
          "haofei37@nus.edu.sg": "to explicit instructions."
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "It enables a deeper and more detailed understand-",
          "haofei37@nus.edu.sg": ""
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "",
          "haofei37@nus.edu.sg": "In this paper, we investigate the optimal LLM for"
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "ing of sentiments. The introduction of emotion-",
          "haofei37@nus.edu.sg": ""
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "",
          "haofei37@nus.edu.sg": "the MECPE-Cat\ntask, selecting ChatGLM based"
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "",
          "haofei37@nus.edu.sg": ""
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "",
          "haofei37@nus.edu.sg": "on its\nsuperior zero-shot performance. We fur-"
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "",
          "haofei37@nus.edu.sg": ""
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "",
          "haofei37@nus.edu.sg": "ther refine ChatGLM through instruction-tuning,"
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "in understanding emotional dynamics within dia-",
          "haofei37@nus.edu.sg": ""
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "",
          "haofei37@nus.edu.sg": "using carefully crafted prompts to enhance its task-"
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "",
          "haofei37@nus.edu.sg": ""
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "",
          "haofei37@nus.edu.sg": "specific accuracy. Our fine-tuned model achieves"
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "*Equal contributions.",
          "haofei37@nus.edu.sg": ""
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "",
          "haofei37@nus.edu.sg": "the second-highest score on the official test set for"
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "†Corresponding author.",
          "haofei37@nus.edu.sg": ""
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "",
          "haofei37@nus.edu.sg": "subtask 2, with a weighted average of 34.71% F1,"
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "1https://nustm.github.io/SemEval-2024_ECAC/",
          "haofei37@nus.edu.sg": ""
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "",
          "haofei37@nus.edu.sg": "underscoring the effectiveness of our approach. We"
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "2https://github.com/zhanghanXD/",
          "haofei37@nus.edu.sg": ""
        },
        {
          "boboli@whu.edu.cn\nhanh@mail.xidian.edu.cn": "",
          "haofei37@nus.edu.sg": "also discuss the current\nlimitations of our model"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 1: An example of an official task and annotated dataset. Each arc points from the cause utterance to the": "emotional triggers. The cause spans have been highlighted in yellow. Background: Chandler and his girlfriend"
        },
        {
          "Figure 1: An example of an official task and annotated dataset. Each arc points from the cause utterance to the": "Monica walked into the casino (they had a quarrel earlier but made up soon), and then started a conversation with"
        },
        {
          "Figure 1: An example of an official task and annotated dataset. Each arc points from the cause utterance to the": "Phoebe."
        },
        {
          "Figure 1: An example of an official task and annotated dataset. Each arc points from the cause utterance to the": "and methodology, alongside directions for future re-"
        },
        {
          "Figure 1: An example of an official task and annotated dataset. Each arc points from the cause utterance to the": "search and improvement. We will release our codes"
        },
        {
          "Figure 1: An example of an official task and annotated dataset. Each arc points from the cause utterance to the": "and resources mentioned in this paper to facilitate"
        },
        {
          "Figure 1: An example of an official task and annotated dataset. Each arc points from the cause utterance to the": "relevant research."
        },
        {
          "Figure 1: An example of an official task and annotated dataset. Each arc points from the cause utterance to the": ""
        },
        {
          "Figure 1: An example of an official task and annotated dataset. Each arc points from the cause utterance to the": "2\nBackground"
        },
        {
          "Figure 1: An example of an official task and annotated dataset. Each arc points from the cause utterance to the": ""
        },
        {
          "Figure 1: An example of an official task and annotated dataset. Each arc points from the cause utterance to the": "2.1\nTask and Dataset Description"
        },
        {
          "Figure 1: An example of an official task and annotated dataset. Each arc points from the cause utterance to the": ""
        },
        {
          "Figure 1: An example of an official task and annotated dataset. Each arc points from the cause utterance to the": "The\nSemEval-2024\nTask\n3\nis\nbased\non\nthe"
        },
        {
          "Figure 1: An example of an official task and annotated dataset. Each arc points from the cause utterance to the": ""
        },
        {
          "Figure 1: An example of an official task and annotated dataset. Each arc points from the cause utterance to the": "multimodal conversational emotion-cause dataset,"
        },
        {
          "Figure 1: An example of an official task and annotated dataset. Each arc points from the cause utterance to the": ""
        },
        {
          "Figure 1: An example of an official task and annotated dataset. Each arc points from the cause utterance to the": "Emotion-Cause-in-Friends\n(ECF;\nWang et\nal.,"
        },
        {
          "Figure 1: An example of an official task and annotated dataset. Each arc points from the cause utterance to the": ""
        },
        {
          "Figure 1: An example of an official task and annotated dataset. Each arc points from the cause utterance to the": "2021), by choosing a multimodal dataset MELD"
        },
        {
          "Figure 1: An example of an official task and annotated dataset. Each arc points from the cause utterance to the": ""
        },
        {
          "Figure 1: An example of an official task and annotated dataset. Each arc points from the cause utterance to the": "(Poria et al., 2018) as the data source and further"
        },
        {
          "Figure 1: An example of an official task and annotated dataset. Each arc points from the cause utterance to the": ""
        },
        {
          "Figure 1: An example of an official task and annotated dataset. Each arc points from the cause utterance to the": "annotating the corresponding causes for the given"
        },
        {
          "Figure 1: An example of an official task and annotated dataset. Each arc points from the cause utterance to the": ""
        },
        {
          "Figure 1: An example of an official task and annotated dataset. Each arc points from the cause utterance to the": "emotion annotations. The ECF dataset contains"
        },
        {
          "Figure 1: An example of an official task and annotated dataset. Each arc points from the cause utterance to the": ""
        },
        {
          "Figure 1: An example of an official task and annotated dataset. Each arc points from the cause utterance to the": "9,794 emotion-cause pairs, covering three modali-"
        },
        {
          "Figure 1: An example of an official task and annotated dataset. Each arc points from the cause utterance to the": ""
        },
        {
          "Figure 1: An example of an official task and annotated dataset. Each arc points from the cause utterance to the": "ties. The subtask 2 is to extract all emotion-cause"
        },
        {
          "Figure 1: An example of an official task and annotated dataset. Each arc points from the cause utterance to the": ""
        },
        {
          "Figure 1: An example of an official task and annotated dataset. Each arc points from the cause utterance to the": "pairs in a given conversation under three modali-"
        },
        {
          "Figure 1: An example of an official task and annotated dataset. Each arc points from the cause utterance to the": ""
        },
        {
          "Figure 1: An example of an official task and annotated dataset. Each arc points from the cause utterance to the": "ties, where each pair contains an emotion utterance"
        },
        {
          "Figure 1: An example of an official task and annotated dataset. Each arc points from the cause utterance to the": ""
        },
        {
          "Figure 1: An example of an official task and annotated dataset. Each arc points from the cause utterance to the": "along with its emotion category and a cause ut-"
        },
        {
          "Figure 1: An example of an official task and annotated dataset. Each arc points from the cause utterance to the": ""
        },
        {
          "Figure 1: An example of an official task and annotated dataset. Each arc points from the cause utterance to the": "terance, e.g., (U3_Joy, U2), which means that the"
        },
        {
          "Figure 1: An example of an official task and annotated dataset. Each arc points from the cause utterance to the": ""
        },
        {
          "Figure 1: An example of an official task and annotated dataset. Each arc points from the cause utterance to the": "speaker’s joy emotion in utterance 3 is triggered"
        },
        {
          "Figure 1: An example of an official task and annotated dataset. Each arc points from the cause utterance to the": ""
        },
        {
          "Figure 1: An example of an official task and annotated dataset. Each arc points from the cause utterance to the": "by the cause from utterance 2. Figure 1 displays"
        },
        {
          "Figure 1: An example of an official task and annotated dataset. Each arc points from the cause utterance to the": "a real example of this task and annotated dataset."
        },
        {
          "Figure 1: An example of an official task and annotated dataset. Each arc points from the cause utterance to the": "In this conversation, it is expected to extract a set"
        },
        {
          "Figure 1: An example of an official task and annotated dataset. Each arc points from the cause utterance to the": "of six utterance-level emotion-cause pairs in total,"
        },
        {
          "Figure 1: An example of an official task and annotated dataset. Each arc points from the cause utterance to the": "e.g., Chandler’s Joy emotion in Utterance 4 (U4"
        },
        {
          "Figure 1: An example of an official task and annotated dataset. Each arc points from the cause utterance to the": "for short) is triggered by the objective cause that he"
        },
        {
          "Figure 1: An example of an official task and annotated dataset. Each arc points from the cause utterance to the": "and Monica had made up and Monica’s subjective"
        },
        {
          "Figure 1: An example of an official task and annotated dataset. Each arc points from the cause utterance to the": "opinion in U3, forming the pairs (U4_joy, U2) and"
        },
        {
          "Figure 1: An example of an official task and annotated dataset. Each arc points from the cause utterance to the": "(U4_joy, U3); The cause for Phoebe’s Disgust in"
        },
        {
          "Figure 1: An example of an official task and annotated dataset. Each arc points from the cause utterance to the": "U5 is the objective event\nthat Monica and Chan-"
        },
        {
          "Figure 1: An example of an official task and annotated dataset. Each arc points from the cause utterance to the": "dler were kissing in front of her (mainly reflected"
        },
        {
          "Figure 1: An example of an official task and annotated dataset. Each arc points from the cause utterance to the": "in the visual modality of U5),\nforming the pair"
        },
        {
          "Figure 1: An example of an official task and annotated dataset. Each arc points from the cause utterance to the": "(U5_disgust, U5)."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 2: Zero-shot\ntest set performance of various": ""
        },
        {
          "Figure 2: Zero-shot\ntest set performance of various": "instruction-tuned LLMs."
        },
        {
          "Figure 2: Zero-shot\ntest set performance of various": ""
        },
        {
          "Figure 2: Zero-shot\ntest set performance of various": "3\nMethodology"
        },
        {
          "Figure 2: Zero-shot\ntest set performance of various": "In this section, we first conduct preliminary ex-"
        },
        {
          "Figure 2: Zero-shot\ntest set performance of various": "periments to determine which LLM to select as"
        },
        {
          "Figure 2: Zero-shot\ntest set performance of various": "a backbone reasoner. We then elaborate on how"
        },
        {
          "Figure 2: Zero-shot\ntest set performance of various": "we design the system and emotion-cause-aware in-"
        },
        {
          "Figure 2: Zero-shot\ntest set performance of various": "structions for tuning our chosen LLM."
        },
        {
          "Figure 2: Zero-shot\ntest set performance of various": "3.1\nPilot Study for LLM Selection"
        },
        {
          "Figure 2: Zero-shot\ntest set performance of various": "Currently, there exists a variety of LLMs, such as"
        },
        {
          "Figure 2: Zero-shot\ntest set performance of various": "OPT-IML (Iyer et al., 2022), GPT-3, Flan-T5, and"
        },
        {
          "Figure 2: Zero-shot\ntest set performance of various": ""
        },
        {
          "Figure 2: Zero-shot\ntest set performance of various": "GLM. However,\nit\nis essential\nto select a model"
        },
        {
          "Figure 2: Zero-shot\ntest set performance of various": ""
        },
        {
          "Figure 2: Zero-shot\ntest set performance of various": "that not only performs optimally but\nis also the"
        },
        {
          "Figure 2: Zero-shot\ntest set performance of various": "most suitable for our specific task. To this end, we"
        },
        {
          "Figure 2: Zero-shot\ntest set performance of various": ""
        },
        {
          "Figure 2: Zero-shot\ntest set performance of various": "carry out a pilot study to determine the most appro-"
        },
        {
          "Figure 2: Zero-shot\ntest set performance of various": "priate model selection. For our zero-shot\ntesting"
        },
        {
          "Figure 2: Zero-shot\ntest set performance of various": "experiment, we rigorously evaluate several models,"
        },
        {
          "Figure 2: Zero-shot\ntest set performance of various": "including OPT-IML3, Instruct-GPT4 (Ouyang et al.,"
        },
        {
          "Figure 2: Zero-shot\ntest set performance of various": "2022), Flan-T55, alongside the ChatGLM models,"
        },
        {
          "Figure 2: Zero-shot\ntest set performance of various": "to identify the most effective tool for this specific"
        },
        {
          "Figure 2: Zero-shot\ntest set performance of various": "task. We customize instructions for each model’s"
        },
        {
          "Figure 2: Zero-shot\ntest set performance of various": "specific tuning style, recognizing that a single set"
        },
        {
          "Figure 2: Zero-shot\ntest set performance of various": "of instructions does not suit all models effectively."
        },
        {
          "Figure 2: Zero-shot\ntest set performance of various": "We also embed expected output labels within these"
        },
        {
          "Figure 2: Zero-shot\ntest set performance of various": "instructions to secure precise responses from each"
        },
        {
          "Figure 2: Zero-shot\ntest set performance of various": "model. Figure 2 depicts the zero-shot performance"
        },
        {
          "Figure 2: Zero-shot\ntest set performance of various": "of these models. The ChatGLM6 LLM is ultimately"
        },
        {
          "Figure 2: Zero-shot\ntest set performance of various": "selected based on its superior performance in these"
        },
        {
          "Figure 2: Zero-shot\ntest set performance of various": "tests.\nThis selection is informed not merely by"
        },
        {
          "Figure 2: Zero-shot\ntest set performance of various": "the innovative features or\nthe advanced training"
        },
        {
          "Figure 2: Zero-shot\ntest set performance of various": ""
        },
        {
          "Figure 2: Zero-shot\ntest set performance of various": "methodologies of ChatGLM but by empirical ev-"
        },
        {
          "Figure 2: Zero-shot\ntest set performance of various": ""
        },
        {
          "Figure 2: Zero-shot\ntest set performance of various": "idence of\nits exceptional zero-shot performance"
        },
        {
          "Figure 2: Zero-shot\ntest set performance of various": "among the models considered."
        },
        {
          "Figure 2: Zero-shot\ntest set performance of various": ""
        },
        {
          "Figure 2: Zero-shot\ntest set performance of various": "https://\n3OPT-IML-30B, max\nversion\nwith\n30B,"
        },
        {
          "Figure 2: Zero-shot\ntest set performance of various": "huggingface.co/facebook/opt-iml-30b"
        },
        {
          "Figure 2: Zero-shot\ntest set performance of various": "4Instruct-GPT-175B, an advanced version of the GPT-3.5."
        },
        {
          "Figure 2: Zero-shot\ntest set performance of various": "https://huggingface.co/\n5Flan-T5-xxl,\nwith\n11B,"
        },
        {
          "Figure 2: Zero-shot\ntest set performance of various": ""
        },
        {
          "Figure 2: Zero-shot\ntest set performance of various": "google/flan-t5-xxl"
        },
        {
          "Figure 2: Zero-shot\ntest set performance of various": ""
        },
        {
          "Figure 2: Zero-shot\ntest set performance of various": "6ChatGLM, 3rd version with 6B, https://github.com/"
        },
        {
          "Figure 2: Zero-shot\ntest set performance of various": "THUDM/ChatGLM3."
        },
        {
          "Figure 2: Zero-shot\ntest set performance of various": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ference on the MECPE-Cat task, yielding an initial": "set of emotion-cause pairs. These preliminary re-",
          "for the model but also aligns the model’s process-": "ing capabilities with the requirements of accurately"
        },
        {
          "ference on the MECPE-Cat task, yielding an initial": "sults are then reintegrated into the original training",
          "for the model but also aligns the model’s process-": "identifying emotion-cause pairs in conversations."
        },
        {
          "ference on the MECPE-Cat task, yielding an initial": "dataset for a second round of fine-tuning, culminat-",
          "for the model but also aligns the model’s process-": "In the above box we showcase a real example."
        },
        {
          "ference on the MECPE-Cat task, yielding an initial": "ing in the refinement of our model to produce the",
          "for the model but also aligns the model’s process-": ""
        },
        {
          "ference on the MECPE-Cat task, yielding an initial": "final set of emotion-cause pairs.",
          "for the model but also aligns the model’s process-": "4\nExperiments"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "dataset for a second round of fine-tuning, culminat-": "ing in the refinement of our model to produce the",
          "In the above box we showcase a real example.": ""
        },
        {
          "dataset for a second round of fine-tuning, culminat-": "final set of emotion-cause pairs.",
          "In the above box we showcase a real example.": "4\nExperiments"
        },
        {
          "dataset for a second round of fine-tuning, culminat-": "",
          "In the above box we showcase a real example.": "This section will quantify the effectiveness of our"
        },
        {
          "dataset for a second round of fine-tuning, culminat-": "Task Definition:",
          "In the above box we showcase a real example.": ""
        },
        {
          "dataset for a second round of fine-tuning, culminat-": "",
          "In the above box we showcase a real example.": "systems via experiments and also show more anal-"
        },
        {
          "dataset for a second round of fine-tuning, culminat-": "“You’re\nan\nexpert\nin\nsentiment\nanalysis",
          "In the above box we showcase a real example.": ""
        },
        {
          "dataset for a second round of fine-tuning, culminat-": "",
          "In the above box we showcase a real example.": "yses to gain more observations."
        },
        {
          "dataset for a second round of fine-tuning, culminat-": "and emotion cause identification.\nBelow",
          "In the above box we showcase a real example.": ""
        },
        {
          "dataset for a second round of fine-tuning, culminat-": "is\na\nconversation\ncontaining\nmultiple",
          "In the above box we showcase a real example.": ""
        },
        {
          "dataset for a second round of fine-tuning, culminat-": "",
          "In the above box we showcase a real example.": "4.1\nImplementation"
        },
        {
          "dataset for a second round of fine-tuning, culminat-": "utterances from different speakers, along",
          "In the above box we showcase a real example.": ""
        },
        {
          "dataset for a second round of fine-tuning, culminat-": "",
          "In the above box we showcase a real example.": "The hyperparameter of our system used to achieve"
        },
        {
          "dataset for a second round of fine-tuning, culminat-": "with the corresponding emotion label\nfor",
          "In the above box we showcase a real example.": ""
        },
        {
          "dataset for a second round of fine-tuning, culminat-": "",
          "In the above box we showcase a real example.": "the highest weighted average F1 score on the sub-"
        },
        {
          "dataset for a second round of fine-tuning, culminat-": "each utterance. Your task is to identify the",
          "In the above box we showcase a real example.": ""
        },
        {
          "dataset for a second round of fine-tuning, culminat-": "",
          "In the above box we showcase a real example.": "task 2 is listed in 1.\nThe ChatGLM model was"
        },
        {
          "dataset for a second round of fine-tuning, culminat-": "indices of\nthe\ncandidate utterances\nthat",
          "In the above box we showcase a real example.": ""
        },
        {
          "dataset for a second round of fine-tuning, culminat-": "",
          "In the above box we showcase a real example.": "fine-tuned using a learning rate of 1e-4 with LoRA-"
        },
        {
          "dataset for a second round of fine-tuning, culminat-": "elicited the emotion in the target utterance.”",
          "In the above box we showcase a real example.": ""
        },
        {
          "dataset for a second round of fine-tuning, culminat-": "",
          "In the above box we showcase a real example.": "specific configurations including a rank of 8, alpha"
        },
        {
          "dataset for a second round of fine-tuning, culminat-": "",
          "In the above box we showcase a real example.": "value of 32, and a dropout rate of 0.1. The train-"
        },
        {
          "dataset for a second round of fine-tuning, culminat-": "Input conversation:",
          "In the above box we showcase a real example.": ""
        },
        {
          "dataset for a second round of fine-tuning, culminat-": "",
          "In the above box we showcase a real example.": "ing was conducted with a maximum instruction"
        },
        {
          "dataset for a second round of fine-tuning, culminat-": "1_joy. Chandler: Hey Pheebs!",
          "In the above box we showcase a real example.": ""
        },
        {
          "dataset for a second round of fine-tuning, culminat-": "",
          "In the above box we showcase a real example.": "length of 2048 tokens and an output length limited"
        },
        {
          "dataset for a second round of fine-tuning, culminat-": "2_surprise. Phoebe: Ohh! You made up!",
          "In the above box we showcase a real example.": ""
        },
        {
          "dataset for a second round of fine-tuning, culminat-": "",
          "In the above box we showcase a real example.": "to 128 tokens, using a batch size of 1. We used"
        },
        {
          "dataset for a second round of fine-tuning, culminat-": "3_joy. Monica: Yeah, I couldn’t be mad at",
          "In the above box we showcase a real example.": ""
        },
        {
          "dataset for a second round of fine-tuning, culminat-": "",
          "In the above box we showcase a real example.": "a single gradient accumulation step across 2 train-"
        },
        {
          "dataset for a second round of fine-tuning, culminat-": "him for too long.",
          "In the above box we showcase a real example.": ""
        },
        {
          "dataset for a second round of fine-tuning, culminat-": "",
          "In the above box we showcase a real example.": "ing epochs. These parameters were meticulously"
        },
        {
          "dataset for a second round of fine-tuning, culminat-": "4_joy. Chandler: Yeah, she couldn’t\nlive",
          "In the above box we showcase a real example.": ""
        },
        {
          "dataset for a second round of fine-tuning, culminat-": "",
          "In the above box we showcase a real example.": "selected to optimize our model’s performance."
        },
        {
          "dataset for a second round of fine-tuning, culminat-": "without the Chan Love.",
          "In the above box we showcase a real example.": ""
        },
        {
          "dataset for a second round of fine-tuning, culminat-": "5_disgust. Phoebe: Ohh, get a room.",
          "In the above box we showcase a real example.": ""
        },
        {
          "dataset for a second round of fine-tuning, culminat-": "",
          "In the above box we showcase a real example.": "Hyperparameter\nValue"
        },
        {
          "dataset for a second round of fine-tuning, culminat-": "Candidate utterances:",
          "In the above box we showcase a real example.": ""
        },
        {
          "dataset for a second round of fine-tuning, culminat-": "",
          "In the above box we showcase a real example.": "Learning rate\n1e-4"
        },
        {
          "dataset for a second round of fine-tuning, culminat-": "1_joy. Chandler: Hey Pheebs!",
          "In the above box we showcase a real example.": ""
        },
        {
          "dataset for a second round of fine-tuning, culminat-": "",
          "In the above box we showcase a real example.": "LoRA rank\n8"
        },
        {
          "dataset for a second round of fine-tuning, culminat-": "2_surprise. Phoebe: Ohh! You made up!",
          "In the above box we showcase a real example.": ""
        },
        {
          "dataset for a second round of fine-tuning, culminat-": "",
          "In the above box we showcase a real example.": "LoRA alpha\n32"
        },
        {
          "dataset for a second round of fine-tuning, culminat-": "3_joy. Monica: Yeah, I couldn’t be mad at",
          "In the above box we showcase a real example.": ""
        },
        {
          "dataset for a second round of fine-tuning, culminat-": "",
          "In the above box we showcase a real example.": "LoRA dropout\n0.1"
        },
        {
          "dataset for a second round of fine-tuning, culminat-": "him for too long.",
          "In the above box we showcase a real example.": ""
        },
        {
          "dataset for a second round of fine-tuning, culminat-": "",
          "In the above box we showcase a real example.": "Max instruction length\n2048"
        },
        {
          "dataset for a second round of fine-tuning, culminat-": "",
          "In the above box we showcase a real example.": "Max output length\n128"
        },
        {
          "dataset for a second round of fine-tuning, culminat-": "Target utterance:",
          "In the above box we showcase a real example.": ""
        },
        {
          "dataset for a second round of fine-tuning, culminat-": "",
          "In the above box we showcase a real example.": "Batch size\n1"
        },
        {
          "dataset for a second round of fine-tuning, culminat-": "4_joy. Chandler: Yeah, she couldn’t\nlive",
          "In the above box we showcase a real example.": ""
        },
        {
          "dataset for a second round of fine-tuning, culminat-": "",
          "In the above box we showcase a real example.": "Gradient accumulation steps\n1"
        },
        {
          "dataset for a second round of fine-tuning, culminat-": "without the Chan Love.",
          "In the above box we showcase a real example.": ""
        },
        {
          "dataset for a second round of fine-tuning, culminat-": "",
          "In the above box we showcase a real example.": "Epochs\n2"
        },
        {
          "dataset for a second round of fine-tuning, culminat-": "Question:",
          "In the above box we showcase a real example.": ""
        },
        {
          "dataset for a second round of fine-tuning, culminat-": "",
          "In the above box we showcase a real example.": "Table 1: Hyperparameter used for the best performing"
        },
        {
          "dataset for a second round of fine-tuning, culminat-": "The emotion-cause indices of the target ut-",
          "In the above box we showcase a real example.": "model."
        },
        {
          "dataset for a second round of fine-tuning, culminat-": "terance are:",
          "In the above box we showcase a real example.": ""
        },
        {
          "dataset for a second round of fine-tuning, culminat-": "[LLM output]",
          "In the above box we showcase a real example.": ""
        },
        {
          "dataset for a second round of fine-tuning, culminat-": "",
          "In the above box we showcase a real example.": "4.2\nEvaluating Template Designing"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 3: Comparison of weighted average F1 Scores",
      "data": [
        {
          "model to achieve the best performance on our task.": ""
        },
        {
          "model to achieve the best performance on our task.": "Our experiments capitalize on the robust com-"
        },
        {
          "model to achieve the best performance on our task.": ""
        },
        {
          "model to achieve the best performance on our task.": "putational capabilities provided by NVIDIA A800-"
        },
        {
          "model to achieve the best performance on our task.": ""
        },
        {
          "model to achieve the best performance on our task.": "SXM GPUs, each boasting 80 GB of VRAM,\nto"
        },
        {
          "model to achieve the best performance on our task.": ""
        },
        {
          "model to achieve the best performance on our task.": "ensure sufficient\nresources are available to train"
        },
        {
          "model to achieve the best performance on our task.": "large language models. This fine-tuning process is"
        },
        {
          "model to achieve the best performance on our task.": "facilitated using a customized script derived from"
        },
        {
          "model to achieve the best performance on our task.": "the Hugging Face Transformers framework, cho-"
        },
        {
          "model to achieve the best performance on our task.": "sen for its extensive support of transformer models"
        },
        {
          "model to achieve the best performance on our task.": ""
        },
        {
          "model to achieve the best performance on our task.": "and seamless integration with our setup,\nthereby"
        },
        {
          "model to achieve the best performance on our task.": ""
        },
        {
          "model to achieve the best performance on our task.": "enabling us to leverage advanced hardware capabil-"
        },
        {
          "model to achieve the best performance on our task.": ""
        },
        {
          "model to achieve the best performance on our task.": "ities while utilizing a leading-edge software envi-"
        },
        {
          "model to achieve the best performance on our task.": ""
        },
        {
          "model to achieve the best performance on our task.": "ronment for our model’s optimization."
        },
        {
          "model to achieve the best performance on our task.": ""
        },
        {
          "model to achieve the best performance on our task.": "4.4\nTask Decomposition"
        },
        {
          "model to achieve the best performance on our task.": ""
        },
        {
          "model to achieve the best performance on our task.": "We decompose the MECPE-Cat task into ERC and"
        },
        {
          "model to achieve the best performance on our task.": ""
        },
        {
          "model to achieve the best performance on our task.": "ECPE phases to strategically alleviate its complex-"
        },
        {
          "model to achieve the best performance on our task.": ""
        },
        {
          "model to achieve the best performance on our task.": "ity.\nThis division offered a two-fold advantage:"
        },
        {
          "model to achieve the best performance on our task.": ""
        },
        {
          "model to achieve the best performance on our task.": "firstly, it distills the task into clearer, more focused"
        },
        {
          "model to achieve the best performance on our task.": ""
        },
        {
          "model to achieve the best performance on our task.": "components, facilitating a more straightforward un-"
        },
        {
          "model to achieve the best performance on our task.": ""
        },
        {
          "model to achieve the best performance on our task.": "derstanding and execution of the model. Secondly,"
        },
        {
          "model to achieve the best performance on our task.": ""
        },
        {
          "model to achieve the best performance on our task.": "by leveraging emotion labels obtained from the"
        },
        {
          "model to achieve the best performance on our task.": ""
        },
        {
          "model to achieve the best performance on our task.": "ERC phase during the ECPE phase, we enhance"
        },
        {
          "model to achieve the best performance on our task.": ""
        },
        {
          "model to achieve the best performance on our task.": "the model’s capability to pinpoint emotion-cause"
        },
        {
          "model to achieve the best performance on our task.": ""
        },
        {
          "model to achieve the best performance on our task.": "pairs with greater accuracy. Tabel 3 showcases in-"
        },
        {
          "model to achieve the best performance on our task.": ""
        },
        {
          "model to achieve the best performance on our task.": "cremental\nimprovements in weighted average F1"
        },
        {
          "model to achieve the best performance on our task.": ""
        },
        {
          "model to achieve the best performance on our task.": "scores across three distinct setups. This progression"
        },
        {
          "model to achieve the best performance on our task.": ""
        },
        {
          "model to achieve the best performance on our task.": "underscores the dual benefits of our approach: sim-"
        },
        {
          "model to achieve the best performance on our task.": ""
        },
        {
          "model to achieve the best performance on our task.": "plifying the task’s complexity for the model and"
        },
        {
          "model to achieve the best performance on our task.": ""
        },
        {
          "model to achieve the best performance on our task.": "enriching the ECPE phase with contextual emo-"
        },
        {
          "model to achieve the best performance on our task.": ""
        },
        {
          "model to achieve the best performance on our task.": "tion labels,\nthereby optimizing the extraction of"
        },
        {
          "model to achieve the best performance on our task.": ""
        },
        {
          "model to achieve the best performance on our task.": "emotion-cause pairs."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 4: Comparison of weighted average F1 Scores",
      "data": [
        {
          "model’s own outputs to refine its accuracy.": "Data\nEpoch 1\nEpoch 2\nEpoch 3"
        },
        {
          "model’s own outputs to refine its accuracy.": "Train\n0.3390\n0.3396\n0.3393"
        },
        {
          "model’s own outputs to refine its accuracy.": "Train + Trial\n0.3404\n0.3410\n0.3406"
        },
        {
          "model’s own outputs to refine its accuracy.": ""
        },
        {
          "model’s own outputs to refine its accuracy.": "0.3416\nIterative Train\n0.3408\n0.3411"
        },
        {
          "model’s own outputs to refine its accuracy.": ""
        },
        {
          "model’s own outputs to refine its accuracy.": ""
        },
        {
          "model’s own outputs to refine its accuracy.": "Table 4: Comparison of weighted average F1 Scores"
        },
        {
          "model’s own outputs to refine its accuracy.": "across different training data and epochs."
        },
        {
          "model’s own outputs to refine its accuracy.": ""
        },
        {
          "model’s own outputs to refine its accuracy.": ""
        },
        {
          "model’s own outputs to refine its accuracy.": ""
        },
        {
          "model’s own outputs to refine its accuracy.": "4.6\nMultimodal Integration"
        },
        {
          "model’s own outputs to refine its accuracy.": ""
        },
        {
          "model’s own outputs to refine its accuracy.": "To assess the impact of multimodal\ninformation"
        },
        {
          "model’s own outputs to refine its accuracy.": ""
        },
        {
          "model’s own outputs to refine its accuracy.": "on our model’s performance, we adopt a method-"
        },
        {
          "model’s own outputs to refine its accuracy.": ""
        },
        {
          "model’s own outputs to refine its accuracy.": "ological approach that harnessed GPT-4V Achiam"
        },
        {
          "model’s own outputs to refine its accuracy.": ""
        },
        {
          "model’s own outputs to refine its accuracy.": "et al. (2023) for extracting insights from modalities"
        },
        {
          "model’s own outputs to refine its accuracy.": ""
        },
        {
          "model’s own outputs to refine its accuracy.": "beyond text. Specifically, we enrich the instruction"
        },
        {
          "model’s own outputs to refine its accuracy.": ""
        },
        {
          "model’s own outputs to refine its accuracy.": "template with “video description of target utterance”"
        },
        {
          "model’s own outputs to refine its accuracy.": ""
        },
        {
          "model’s own outputs to refine its accuracy.": "derived from GPT-4V, presenting it as supplemen-"
        },
        {
          "model’s own outputs to refine its accuracy.": "tary information to guide the model. This strategic"
        },
        {
          "model’s own outputs to refine its accuracy.": ""
        },
        {
          "model’s own outputs to refine its accuracy.": "integration of multimodal data leads to an improve-"
        },
        {
          "model’s own outputs to refine its accuracy.": ""
        },
        {
          "model’s own outputs to refine its accuracy.": "ment in the model’s F1 score, as shown in Table 5,"
        },
        {
          "model’s own outputs to refine its accuracy.": ""
        },
        {
          "model’s own outputs to refine its accuracy.": "which validates the utility of multimodal informa-"
        },
        {
          "model’s own outputs to refine its accuracy.": ""
        },
        {
          "model’s own outputs to refine its accuracy.": "tion in providing richer contextual understanding."
        },
        {
          "model’s own outputs to refine its accuracy.": ""
        },
        {
          "model’s own outputs to refine its accuracy.": "Information\nF1 Score"
        },
        {
          "model’s own outputs to refine its accuracy.": ""
        },
        {
          "model’s own outputs to refine its accuracy.": "Text\n0.3416"
        },
        {
          "model’s own outputs to refine its accuracy.": "0.3471\nText + Video"
        },
        {
          "model’s own outputs to refine its accuracy.": ""
        },
        {
          "model’s own outputs to refine its accuracy.": "Table 5: Comparison of weighted average F1 Scores"
        },
        {
          "model’s own outputs to refine its accuracy.": ""
        },
        {
          "model’s own outputs to refine its accuracy.": "between pure text and multimodal information."
        },
        {
          "model’s own outputs to refine its accuracy.": ""
        },
        {
          "model’s own outputs to refine its accuracy.": ""
        },
        {
          "model’s own outputs to refine its accuracy.": ""
        },
        {
          "model’s own outputs to refine its accuracy.": ""
        },
        {
          "model’s own outputs to refine its accuracy.": "5\nConclusion"
        },
        {
          "model’s own outputs to refine its accuracy.": ""
        },
        {
          "model’s own outputs to refine its accuracy.": ""
        },
        {
          "model’s own outputs to refine its accuracy.": "In this work, we explore the LLMs for solving the"
        },
        {
          "model’s own outputs to refine its accuracy.": ""
        },
        {
          "model’s own outputs to refine its accuracy.": "Multimodal Emotion-Cause Pair Extraction with"
        },
        {
          "model’s own outputs to refine its accuracy.": ""
        },
        {
          "model’s own outputs to refine its accuracy.": "Emotion Category (MECPE-Cat) task. Through"
        },
        {
          "model’s own outputs to refine its accuracy.": ""
        },
        {
          "model’s own outputs to refine its accuracy.": "a pilot study, we first select an LLM, ChatGLM,"
        },
        {
          "model’s own outputs to refine its accuracy.": ""
        },
        {
          "model’s own outputs to refine its accuracy.": "that assists in achieving optimal task performance."
        },
        {
          "model’s own outputs to refine its accuracy.": ""
        },
        {
          "model’s own outputs to refine its accuracy.": "The backbone ChatGLM receives textual dialogue,"
        },
        {
          "model’s own outputs to refine its accuracy.": ""
        },
        {
          "model’s own outputs to refine its accuracy.": "and also perceives the multimodal information via"
        },
        {
          "model’s own outputs to refine its accuracy.": "the ImageBind vision encoder. Lastly, we devise"
        },
        {
          "model’s own outputs to refine its accuracy.": ""
        },
        {
          "model’s own outputs to refine its accuracy.": "an emotion-cause-aware instruction-tuning mecha-"
        },
        {
          "model’s own outputs to refine its accuracy.": "nism for updating LLMs, which enhances the per-"
        },
        {
          "model’s own outputs to refine its accuracy.": ""
        },
        {
          "model’s own outputs to refine its accuracy.": "ception of the emotions with their corresponding"
        },
        {
          "model’s own outputs to refine its accuracy.": ""
        },
        {
          "model’s own outputs to refine its accuracy.": "causal rationales. Our system achieves a weighted"
        },
        {
          "model’s own outputs to refine its accuracy.": ""
        },
        {
          "model’s own outputs to refine its accuracy.": "average F1 score of 34.71%, securing second place"
        },
        {
          "model’s own outputs to refine its accuracy.": ""
        },
        {
          "model’s own outputs to refine its accuracy.": "on the MECPE-Cat leaderboard."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "2023c. Nonautoregressive encoder-decoder neural",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": "jumder, Gautam Naik, Erik Cambria, and Rada Mi-"
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "framework for end-to-end aspect-based sentiment",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": "halcea. 2018.\nMeld:\nA multimodal multi-party"
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "IEEE Trans. Neural Networks\ntriplet extraction.",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": "dataset\nfor\nemotion recognition in conversations."
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "Learn. Syst., 34(9):5544–5556.",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": "arXiv preprint arXiv:1810.02508."
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "Hao Fei, Yue Zhang, Yafeng Ren, and Donghong Ji.",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": "Soujanya Poria, Navonil Majumder, Devamanyu Haz-"
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "2020. Latent emotion memory for multi-label emo-",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": "arika, Deepanway Ghosal, Rishabh Bhardwaj, Sam-"
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "tion classification.\nIn Proceedings of the AAAI con-",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": "son Yu Bai Jian, Pengfei Hong, Romila Ghosh, Ab-"
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "ference on artificial intelligence, volume 34, pages",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": "hinaba Roy, Niyati Chhaya, et al. 2021. Recognizing"
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "7692–7699.",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": "emotion cause in conversations. Cognitive Computa-"
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": "tion, 13:1317–1332."
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Man-",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": ""
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "nat Singh, Kalyan Vasudev Alwala, Armand Joulin,",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": ""
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": "Leigang Qu, Shengqiong Wu, Hao Fei, Liqiang Nie,"
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "and Ishan Misra. 2023.\nImagebind: One embed-",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": ""
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": "and Tat-Seng Chua. 2023. Layoutllm-t2i: Eliciting"
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "the\nding space to bind them all.\nIn Proceedings of",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": ""
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": "layout guidance from llm for text-to-image genera-"
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "IEEE/CVF Conference on Computer Vision and Pat-",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": ""
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": "tion.\nIn Proceedings of the 31st ACM International"
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "tern Recognition, pages 15180–15190.",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": ""
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": "Conference on Multimedia, pages 643–654."
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "Devamanyu Hazarika, Soujanya Poria, Rada Mihalcea,",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": ""
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": "Fanfan Wang, Zixiang Ding, Rui Xia, Zhaoyu Li,"
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "Erik Cambria, and Roger Zimmermann. 2018.\nIcon:",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": ""
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": "and Jianfei Yu. 2021. Multimodal emotion-cause"
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "Interactive conversational memory network for multi-",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": ""
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": "arXiv preprint\npair\nextraction in conversations."
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "modal emotion detection.\nIn Proceedings of the 2018",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": ""
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": "arXiv:2110.08020."
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "conference on empirical methods in natural language",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": ""
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "processing, pages 2594–2604.",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": "Thomas Wolf, Lysandre Debut, Victor Sanh,\nJulien"
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": "Chaumond, Clement Delangue, Anthony Moi, Pier-"
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": ""
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": "ric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,"
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": ""
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": "et al. 2020. Transformers: State-of-the-art natural"
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "and Weizhu Chen. 2021.\nLora:\nLow-rank adap-",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": ""
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": "language processing.\nIn Proceedings of the 2020 con-"
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "arXiv preprint\ntation of\nlarge language models.",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": ""
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": "ference on empirical methods in natural\nlanguage"
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "arXiv:2106.09685.",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": ""
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": "processing: system demonstrations, pages 38–45."
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru,",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": ""
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": "Shengqiong Wu, Hao Fei, Wei Ji, and Tat-Seng Chua."
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "Todor Mihaylov, Dániel Simig, Ping Yu, Kurt Shuster,",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": ""
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": "2023a.\nCross2stra: Unpaired cross-lingual\nimage"
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "Tianlu Wang, Qing Liu, Punit Singh Koura, et al.",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": ""
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": "captioning with cross-lingual cross-modal structure-"
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "2022. Opt-iml: Scaling language model instruction",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": ""
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": "pivoted alignment.\nIn Proceedings of the 61st Annual"
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "meta learning through the lens of generalization.",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": ""
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": "Meeting of\nthe Association for Computational Lin-"
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": "guistics (Volume 1: Long Papers), ACL 2023, Toronto,"
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": ""
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": "Canada, July 9-14, 2023, pages 2593–2608. Associa-"
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "The power of scale for parameter-efficient prompt",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": ""
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": "tion for Computational Linguistics."
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "tuning. arXiv preprint arXiv:2104.08691.",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": ""
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "Bobo Li, Hao Fei, Fei Li, Yuhan Wu, Jinsong Zhang,",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": "Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and"
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "Shengqiong Wu, Jingye Li, Yijiang Liu, Lizi Liao,",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": "Tat-Seng Chua. 2023b. Next-gpt: Any-to-any multi-"
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "Tat-Seng Chua, et al. 2022. Diaasq: A benchmark",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": "modal llm. arXiv preprint arXiv:2309.05519."
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "of conversational aspect-based sentiment quadruple",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": ""
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": "Shengqiong Wu, Hao Fei, Hanwang Zhang, and Tat-"
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "analysis. arXiv preprint arXiv:2211.05705.",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": ""
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": "Seng Chua. 2024.\nImagine that! abstract-to-intricate"
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "Bobo Li, Hao Fei, Lizi Liao, Yu Zhao, Chong Teng,",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": "text-to-image synthesis with scene graph hallucina-"
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "Tat-Seng Chua, Donghong Ji, and Fei Li. 2023. Re-",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": "tion diffusion. Advances in Neural Information Pro-"
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "visiting disentanglement and fusion on modality and",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": "cessing Systems, 36."
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "context in conversational multimodal emotion recog-",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": ""
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": "Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,"
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "nition.\nIn Proceedings of the 31st ACM International",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": ""
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": "Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,"
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "Conference on Multimedia, pages 5923–5934.",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": ""
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": "Wendi Zheng, Xiao Xia, et al. 2022.\nGlm-130b:"
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": "An open bilingual pre-trained model. arXiv preprint"
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "Hiroaki Hayashi, and Graham Neubig. 2023.\nPre-",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": "arXiv:2210.02414."
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "train, prompt, and predict: A systematic survey of",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": ""
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "prompting methods in natural language processing.",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": "Ao Zhang, Hao Fei, Yuan Yao, Wei Ji, Li Li, Zhiyuan"
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "ACM Computing Surveys, 55(9):1–35.",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": "Liu, and Tat-Seng Chua. 2024a. Vpgtrans: Transfer"
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": "Advances in\nvisual prompt generator across llms."
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": ""
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": "Neural Information Processing Systems, 36."
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "Carroll Wainwright, Pamela Mishkin, Chong Zhang,",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": ""
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "Sandhini Agarwal, Katarina Slama, Alex Ray, et al.",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": "Meishan\nZhang,\nBin Wang,\nHao\nFei,\nand Min"
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "2022. Training language models to follow instruc-",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": "Zhang. 2024b.\nIn-context\nlearning for\nfew-shot"
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "Advances in Neural\ntions with human feedback.",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": "arXiv preprint\nnested named entity recognition."
        },
        {
          "Hao Fei, Yafeng Ren, Yue Zhang, and Donghong Ji.": "Information Processing Systems, 35:27730–27744.",
          "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-": "arXiv:2402.01182."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang,": "Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tian-"
        },
        {
          "Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang,": "wei Zhang, Fei Wu, et al. 2023.\nInstruction tuning"
        },
        {
          "Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang,": "for large language models: A survey. arXiv preprint"
        },
        {
          "Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang,": "arXiv:2308.10792."
        },
        {
          "Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang,": "Yu Zhao, Hao Fei, Yixin Cao, Bobo Li, Meishan Zhang,"
        },
        {
          "Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang,": "Jianguo Wei, Min Zhang, and Tat-Seng Chua. 2023."
        },
        {
          "Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang,": "Constructing holistic spatio-temporal scene graph for"
        },
        {
          "Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang,": "video semantic role labeling.\nIn Proceedings of the"
        },
        {
          "Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang,": "31st ACM International Conference on Multimedia,"
        },
        {
          "Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang,": "pages 5281–5291."
        },
        {
          "Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang,": "Li Zheng, Donghong Ji, Fei Li, Hao Fei, Shengqiong"
        },
        {
          "Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang,": "Wu, Jingye Li, Bobo Li, and Chong Teng. 2023. Ec-"
        },
        {
          "Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang,": "qed: Emotion-cause quadruple extraction in dialogs."
        },
        {
          "Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang,": "arXiv preprint arXiv:2306.03969."
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Shyamal Anadkat, et al. 2023. Gpt-4 technical report",
      "authors": [
        "Josh Achiam",
        "Steven Adler",
        "Sandhini Agarwal",
        "Lama Ahmad",
        "Ilge Akkaya",
        "Florencia Leoni Aleman",
        "Diogo Almeida",
        "Janko Altenschmidt",
        "Sam Altman"
      ],
      "venue": "Shyamal Anadkat, et al. 2023. Gpt-4 technical report",
      "arxiv": "arXiv:2303.08774"
    },
    {
      "citation_id": "2",
      "title": "Language models are few-shot learners",
      "authors": [
        "Tom Brown",
        "Benjamin Mann",
        "Nick Ryder",
        "Melanie Subbiah",
        "Jared Kaplan",
        "Prafulla Dhariwal",
        "Arvind Neelakantan",
        "Pranav Shyam",
        "Girish Sastry",
        "Amanda Askell"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "3",
      "title": "Prompt-based generative multi-label emotion prediction with label contrastive learning",
      "authors": [
        "Yuyang Chai",
        "Chong Teng",
        "Hao Fei",
        "Shengqiong Wu",
        "Jingye Li",
        "Ming Cheng",
        "Donghong Ji",
        "Fei Li"
      ],
      "year": "2022",
      "venue": "CCF International Conference on Natural Language Processing and Chinese Computing"
    },
    {
      "citation_id": "4",
      "title": "Joint alignment of multi-task feature and label spaces for emotion cause pair extraction",
      "authors": [
        "Shunjie Chen",
        "Xiaochuan Shi",
        "Jingye Li",
        "Shengqiong Wu",
        "Hao Fei",
        "Fei Li",
        "Donghong Ji"
      ],
      "year": "2022",
      "venue": "Proceedings of the 29th International Conference on Computational Linguistics, COLING 2022"
    },
    {
      "citation_id": "5",
      "title": "A consistent dualmrc framework for emotion-cause pair extraction",
      "authors": [
        "Zifeng Cheng",
        "Zhiwei Jiang",
        "Yafeng Yin",
        "Cong Wang",
        "Shiping Ge",
        "Qing Gu ; Le",
        "Shayne Hou",
        "Barret Longpre",
        "Yi Zoph",
        "William Tay",
        "Yunxuan Fedus",
        "Xuezhi Li",
        "Mostafa Wang",
        "Dehghani"
      ],
      "year": "2023",
      "venue": "Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models",
      "arxiv": "arXiv:2210.11416"
    },
    {
      "citation_id": "6",
      "title": "Glm: General language model pretraining with autoregressive blank infilling",
      "authors": [
        "Zhengxiao Du",
        "Yujie Qian",
        "Xiao Liu",
        "Ming Ding",
        "Jiezhong Qiu",
        "Zhilin Yang",
        "Jie Tang"
      ],
      "year": "2021",
      "venue": "Glm: General language model pretraining with autoregressive blank infilling",
      "arxiv": "arXiv:2103.10360"
    },
    {
      "citation_id": "7",
      "title": "Reasoning implicit sentiment with chain-of-thought prompting",
      "authors": [
        "Bobo Hao Fei",
        "Qian Li",
        "Lidong Liu",
        "Fei Bing",
        "Tat-Seng Li",
        "Chua"
      ],
      "year": "2023",
      "venue": "Reasoning implicit sentiment with chain-of-thought prompting",
      "arxiv": "arXiv:2305.11255"
    },
    {
      "citation_id": "8",
      "title": "Scene graph as pivoting: Inference-time image-free unsupervised multimodal machine translation with visual scene hallucination",
      "authors": [
        "Qian Hao Fei",
        "Meishan Liu",
        "Min Zhang",
        "Tat-Seng Zhang",
        "Chua"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "9",
      "title": "2023c. Nonautoregressive encoder-decoder neural framework for end-to-end aspect-based sentiment triplet extraction",
      "authors": [
        "Yafeng Hao Fei",
        "Yue Ren",
        "Donghong Zhang",
        "Ji"
      ],
      "venue": "IEEE Trans. Neural Networks Learn. Syst"
    },
    {
      "citation_id": "10",
      "title": "Latent emotion memory for multi-label emotion classification",
      "authors": [
        "Yue Hao Fei",
        "Yafeng Zhang",
        "Donghong Ren",
        "Ji"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "11",
      "title": "Imagebind: One embedding space to bind them all",
      "authors": [
        "Rohit Girdhar",
        "Alaaeldin El-Nouby",
        "Zhuang Liu",
        "Mannat Singh",
        "Kalyan Vasudev Alwala",
        "Armand Joulin",
        "Ishan Misra"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "12",
      "title": "Icon: Interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Rada Mihalcea",
        "Erik Cambria",
        "Roger Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 conference on empirical methods in natural language processing"
    },
    {
      "citation_id": "13",
      "title": "Lora: Low-rank adaptation of large language models",
      "authors": [
        "J Edward",
        "Yelong Hu",
        "Phillip Shen",
        "Zeyuan Wallis",
        "Yuanzhi Allen-Zhu",
        "Shean Li",
        "Lu Wang",
        "Weizhu Wang",
        "Chen"
      ],
      "year": "2021",
      "venue": "Lora: Low-rank adaptation of large language models",
      "arxiv": "arXiv:2106.09685"
    },
    {
      "citation_id": "14",
      "title": "Opt-iml: Scaling language model instruction meta learning through the lens of generalization",
      "authors": [
        "Srinivasan Iyer",
        "Xi Victoria Lin",
        "Ramakanth Pasunuru",
        "Todor Mihaylov",
        "Dániel Simig",
        "Ping Yu",
        "Kurt Shuster",
        "Tianlu Wang",
        "Qing Liu",
        "Punit Singh Koura"
      ],
      "year": "2022",
      "venue": "Opt-iml: Scaling language model instruction meta learning through the lens of generalization"
    },
    {
      "citation_id": "15",
      "title": "The power of scale for parameter-efficient prompt tuning",
      "authors": [
        "Brian Lester",
        "Rami Al-Rfou",
        "Noah Constant"
      ],
      "year": "2021",
      "venue": "The power of scale for parameter-efficient prompt tuning",
      "arxiv": "arXiv:2104.08691"
    },
    {
      "citation_id": "16",
      "title": "Diaasq: A benchmark of conversational aspect-based sentiment quadruple analysis",
      "authors": [
        "Bobo Li",
        "Hao Fei",
        "Fei Li",
        "Yuhan Wu",
        "Jinsong Zhang",
        "Shengqiong Wu",
        "Jingye Li",
        "Yijiang Liu",
        "Lizi Liao",
        "Tat-Seng Chua"
      ],
      "year": "2022",
      "venue": "Diaasq: A benchmark of conversational aspect-based sentiment quadruple analysis",
      "arxiv": "arXiv:2211.05705"
    },
    {
      "citation_id": "17",
      "title": "Revisiting disentanglement and fusion on modality and context in conversational multimodal emotion recognition",
      "authors": [
        "Bobo Li",
        "Hao Fei",
        "Lizi Liao",
        "Yu Zhao",
        "Chong Teng",
        "Tat-Seng Chua",
        "Donghong Ji",
        "Fei Li"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "18",
      "title": "Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing",
      "authors": [
        "Pengfei Liu",
        "Weizhe Yuan",
        "Jinlan Fu",
        "Zhengbao Jiang",
        "Hiroaki Hayashi",
        "Graham Neubig"
      ],
      "year": "2023",
      "venue": "ACM Computing Surveys"
    },
    {
      "citation_id": "19",
      "title": "Training language models to follow instructions with human feedback",
      "authors": [
        "Long Ouyang",
        "Jeffrey Wu",
        "Xu Jiang",
        "Diogo Almeida",
        "Carroll Wainwright",
        "Pamela Mishkin",
        "Chong Zhang",
        "Sandhini Agarwal",
        "Katarina Slama",
        "Alex Ray"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "20",
      "title": "Navonil Majumder, Gautam Naik, Erik Cambria, and Rada Mihalcea",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika"
      ],
      "year": "2018",
      "venue": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "21",
      "title": "Recognizing emotion cause in conversations",
      "authors": [
        "Soujanya Poria",
        "Navonil Majumder",
        "Devamanyu Hazarika",
        "Deepanway Ghosal",
        "Rishabh Bhardwaj",
        "Samson Yu Bai Jian",
        "Pengfei Hong",
        "Romila Ghosh",
        "Abhinaba Roy",
        "Niyati Chhaya"
      ],
      "year": "2021",
      "venue": "Cognitive Computation"
    },
    {
      "citation_id": "22",
      "title": "Layoutllm-t2i: Eliciting layout guidance from llm for text-to-image generation",
      "authors": [
        "Leigang Qu",
        "Shengqiong Wu",
        "Hao Fei",
        "Liqiang Nie",
        "Tat-Seng Chua"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "23",
      "title": "Multimodal emotion-cause pair extraction in conversations",
      "authors": [
        "Fanfan Wang",
        "Zixiang Ding",
        "Rui Xia",
        "Zhaoyu Li",
        "Jianfei Yu"
      ],
      "year": "2021",
      "venue": "Multimodal emotion-cause pair extraction in conversations",
      "arxiv": "arXiv:2110.08020"
    },
    {
      "citation_id": "24",
      "title": "Transformers: State-of-the-art natural language processing",
      "authors": [
        "Thomas Wolf",
        "Lysandre Debut",
        "Victor Sanh",
        "Julien Chaumond",
        "Clement Delangue",
        "Anthony Moi",
        "Pierric Cistac",
        "Tim Rault",
        "Rémi Louf",
        "Morgan Funtowicz"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations"
    },
    {
      "citation_id": "25",
      "title": "2023a. Cross2stra: Unpaired cross-lingual image captioning with cross-lingual cross-modal structurepivoted alignment",
      "authors": [
        "Shengqiong Wu",
        "Hao Fei",
        "Wei Ji",
        "Tat-Seng Chua"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "26",
      "title": "Next-gpt: Any-to-any multimodal llm",
      "authors": [
        "Shengqiong Wu",
        "Hao Fei",
        "Leigang Qu",
        "Wei Ji",
        "Tat-Seng Chua"
      ],
      "year": "2023",
      "venue": "Next-gpt: Any-to-any multimodal llm",
      "arxiv": "arXiv:2309.05519"
    },
    {
      "citation_id": "27",
      "title": "Imagine that! abstract-to-intricate text-to-image synthesis with scene graph hallucination diffusion",
      "authors": [
        "Shengqiong Wu",
        "Hao Fei",
        "Hanwang Zhang",
        "Tat-Seng Chua"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "28",
      "title": "Glm-130b: An open bilingual pre-trained model",
      "authors": [
        "Aohan Zeng",
        "Xiao Liu",
        "Zhengxiao Du",
        "Zihan Wang",
        "Hanyu Lai",
        "Ming Ding",
        "Zhuoyi Yang",
        "Yifan Xu",
        "Wendi Zheng",
        "Xiao Xia"
      ],
      "year": "2022",
      "venue": "Glm-130b: An open bilingual pre-trained model",
      "arxiv": "arXiv:2210.02414"
    },
    {
      "citation_id": "29",
      "title": "Vpgtrans: Transfer visual prompt generator across llms",
      "authors": [
        "Ao Zhang",
        "Hao Fei",
        "Yuan Yao",
        "Wei Ji",
        "Li Li",
        "Zhiyuan Liu",
        "Tat-Seng Chua"
      ],
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "30",
      "title": "2024b. In-context learning for few-shot nested named entity recognition",
      "authors": [
        "Meishan Zhang",
        "Bin Wang",
        "Hao Fei",
        "Min Zhang"
      ],
      "venue": "2024b. In-context learning for few-shot nested named entity recognition",
      "arxiv": "arXiv:2402.01182"
    },
    {
      "citation_id": "31",
      "title": "Instruction tuning for large language models: A survey",
      "authors": [
        "Shengyu Zhang",
        "Linfeng Dong",
        "Xiaoya Li",
        "Sen Zhang",
        "Xiaofei Sun",
        "Shuhe Wang",
        "Jiwei Li",
        "Runyi Hu",
        "Tianwei Zhang",
        "Fei Wu"
      ],
      "year": "2023",
      "venue": "Instruction tuning for large language models: A survey",
      "arxiv": "arXiv:2308.10792"
    },
    {
      "citation_id": "32",
      "title": "Constructing holistic spatio-temporal scene graph for video semantic role labeling",
      "authors": [
        "Yu Zhao",
        "Hao Fei",
        "Yixin Cao",
        "Bobo Li",
        "Meishan Zhang",
        "Jianguo Wei",
        "Min Zhang",
        "Tat-Seng Chua"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "33",
      "title": "Ecqed: Emotion-cause quadruple extraction in dialogs",
      "authors": [
        "Li Zheng",
        "Donghong Ji",
        "Fei Li",
        "Hao Fei",
        "Shengqiong Wu",
        "Jingye Li",
        "Bobo Li",
        "Chong Teng"
      ],
      "year": "2023",
      "venue": "Ecqed: Emotion-cause quadruple extraction in dialogs",
      "arxiv": "arXiv:2306.03969"
    }
  ]
}