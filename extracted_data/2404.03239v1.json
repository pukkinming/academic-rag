{
  "paper_id": "2404.03239v1",
  "title": "Exploring Emotions In Multi-Componential Space Using Interactive Vr Games",
  "published": "2024-04-04T06:54:44Z",
  "authors": [
    "Rukshani Somarathna",
    "Gelareh Mohammadi"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion understanding is a complex process that involves multiple components. The ability to recognise emotions not only leads to new context awareness methods but also enhances system interaction's effectiveness by perceiving and expressing emotions. Despite the attention to discrete and dimensional models, neuroscientific evidence supports those emotions as being complex and multi-faceted. One framework that resonated well with such findings is the Component Process Model (CPM), a theory that considers the complexity of emotions with five interconnected components: appraisal, expression, motivation, physiology and feeling. However, the relationship between CPM and discrete emotions has not yet been fully explored. Therefore, to better understand emotions' underlying processes, we operationalised a data-driven approach using interactive Virtual Reality (VR) games and collected multimodal measures (self-reports, physiological and facial signals) from 39 participants. We used Machine Learning (ML) methods to identify the unique contributions of each component to emotion differentiation. Our results showed the role of different components in emotion differentiation, with the model including all components demonstrating the most significant contribution. Moreover, we found that at least five dimensions are needed to represent the variation of emotions in our dataset. These findings also have implications for using VR environments in emotion research and highlight the role of physiological signals in emotion recognition within such environments.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "U NDERSTANDING emotions and their formation is a fundamental aspect of human social interactions, and it has extensive implications in many fields, such as psychology, computer science, and human-computer interaction. Emotions are complex cultural and psychobiological states that are an important aspect of human experience and play a central role in our social interactions and decision-making  [1] . Therefore, emotion understanding enables individuals to comprehend and respond appropriately to events and regulate their emotions. Additionally, research on emotion understanding has the potential to inform the development of systems that possess context awareness and the ability to perceive and express emotions  [2] . Recent developments in the field have led to numerous innovations and advances in Artificial Intelligence (AI) systems such as virtual assistance, chatbots, Virtual Reality (VR) environments, crosscultural models that can recognise and respond to emotions across different cultures, wearable devices, and smart applications that benefit from emotional awareness. Although discrete and dimensional models have received significant attention, neuroscientific evidence suggests that emotions are complex, multi-faceted in nature, and show multiple brain processes subserving emotional experience  [3] ,  [4] . Affective Computing (AC) research is mainly attributed to Discrete, Dimensional, and Appraisal  [5] ,  [6]  models in datadriven analysis. Each of these models approaches emotions from different perspectives; however, only discrete, and dimensional models have gained widespread recognition in research on emotion formation  [7] ,  [8] . Discrete models suggest that emotions are distinct, separate entities that can be easily distinguished from one another  [7] ,  [9] . Dimensional models postulate that emotions can be differentiated along continuous dimensions such as valence and arousal  [5] ,  [7] ,  [10] ,  [11] .\n\nAppraisal models theorise that emotions are the responses of an individual's cognitive evaluation of an event  [12] ,  [13] ,  [14] . Appraisal models assume the cognitive processes, the role of individual differences, and contextual awareness. Nevertheless, discrete models may not adequately capture the complexity of emotion formation, as they treat emotions as distinct categories  [15] ,  [16]  rather than considering the continuity of emotional experiences. Dimensional models address this to some extent by recognising the continuity, however mostly limited to basic dimensions  [5] ,  [7] . Further, both discrete and dimensional models tend to focus on the feeling component without explaining the interactive effects of sub-processes  [1] . While appraisal models acknowledge the role of cognitive processes in emotion formation, there is limited data-driven research  [7] ,  [13] ,  [17] . One possible reason may be the complexity of operationalising and assessing, as they rely on subjective evaluations of events. However, as the appraisal model recognises the complexity of emotions, encompassing a range of cognitive, physiological, and behavioural processes  [5] ,  [18] , it is crucial to consider a process-based model to understand the mechanisms behind emotion formation.\n\nThe appraisal model has influenced a significant amount of research. The Component Process Model (CPM)  [1] ,  [12] ,  [18] , a variant of the appraisal model, has also significantly shaped this research. CPM assumes the process-based mod-elling between five main components: appraisal, motivation, expression, physiology, and feeling  [1] . Renewed interest in the CPM has led to data-driven analyses. However, these studies have often been limited in their use of active participation  [12] ,  [19] , objective measures of the physiology and expression components  [17] , and the induction of a broader range of emotions  [12] ,  [20] . Despite a few existing studies on CPM, there is a need for more research using enhanced data-driven approaches that involve active participation and objective measures to gain a better understanding of emotions.\n\nIn this study, we operationalise a VR-based setting to explore the relationship between CPM and discrete emotions by creating a more realistic emotional induction  [21] . In this research, our emphasis was on CPM due to the availability of a standard questionnaire to measure components and operationalise them. We induced a more comprehensive range of emotions using interactive VR games and collected multimodal measures: self-reports, physiological, and facial signals. Our goal is to better understand the role of each CPM component in capturing emotions using self-reports and physiological and facial signals. We also explore the potential of VR games to induce a range of emotions with different intensities. It's worth noting the main reason for employing VR games is that they provide an immersive and more ecological medium for eliciting emotions, which goes beyond the previous studies that used still images or video clips that result in a more passive experience of emotions. Therefore, our analyses in this manuscript are confined to understanding emotions rather than the impact on VR gameplay. Additionally, we examine latent dimensions that help understand distinct emotional features using CPM self-reports. Our findings inform the role of each CPM component in capturing emotions. These insights can be used for enhanced context awareness, system interaction, and system adaptations in domains including healthcare, VR environments, education, gaming, and interfaces. Moreover, it uses a data-driven approach to investigate emotion formation positing that emotions are subjective experiences.\n\nThis manuscript builds upon previous research in this area  [22] ,  [23] ,  [24]  by presenting more comprehensive experiments and analyses with further data samples and generalised ML models using more rigorous evaluation methods (e.g. Leave-One-Subject-Out (LOSO) cross-validations). This paper addresses the following research questions: (i) How are CPM and discrete emotions related when emotions are induced through interactive VR games? (ii) What are the latent dimensions that underlie emotional experience? and (iii) What is the contribution of each CPM component in capturing discrete emotions? This study makes several significant contributions to the field of affective computing and emotion recognition, which can be summarised as (i) Exploring the componential process model in an interactive VR environment with a much broader scope than previous works, (ii) Providing new insights into the relationship between discrete emotions and CPM, and (iii) Exploring the importance of each component in differentiating discrete emotions.\n\nThe paper is organised as follows: Section 2 presents theoretical foundations, Section 3 covers our research methodology, Section 4 details results and implications, and Section 5 concludes with a summary.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Background And Related Works",
      "text": "The purpose of this section is first to provide a background on the topic of emotion understanding from CPM and then to review relevant research in the field.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Component Process Model (Cpm)",
      "text": "The CPM is a theoretical framework composed of five main components: appraisal, motivation, physiology, expression, and feeling, which are used to understand and interpret emotional experiences  [1] ,  [18] . Refer to Figure  1  for a visual representation of these components. CPM is based on the idea that emotions are complex and multi-faceted in nature, with various cognitive, physiological, and behavioural processes converging to create an emotional experience.\n\nThe CPM's appraisal component consists of cognitive processes to evaluate an event. It has four objectives that evaluate an event as 1) Relevance: \"Is this event relevant?\", 2) Implications: \"What will be the impact of this event?\", 3) Coping potential: \"What are the chances of me being able to overcome the potential consequences of this event?\" and 4) Normative significance: \"Does this event align with my values and beliefs?\". The outcome of these assessments activates mutually dependent processes on other components. The motivation component refers to the action tendencies, goals, and motivations triggered by an individual's appraisal of an event and drives their emotional reactions. Driven by appraisal and motivation outputs, the expression component initiates verbal or non-verbal behaviour to convey the emotional experience. Similarly, the physiology component begins appropriate changes in bodily functions to adjust to the situation. Finally, the feeling component represents the conscious and non-conscious subjective emotional experience.\n\nThe suggestive features and functions of the CPM in emotion formation can be evaluated by the GRID instrument  [25]  or by the shorter versions, CoreGRID and Mini-GRID  [26] , which describe the meanings of emotion words. Unfolding the emotional experiences by CPM can be done by either selecting full or a combination of components. Accordingly, selecting proper GRID items that correspond to the chosen stimuli and selecting appropriate data-analytic procedures for operationalising the selected CPM components is critical  [25] .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Related Works",
      "text": "In literature, CPM-based data-driven research aims to identify the structure of discrete emotions within the componential space using data. Further, the exploration of computational modelling in understanding emotions has been a recurring theme in diverse research endeavors  [14] ,  [27] ,  [28] . These approaches use collected data and computational models to provide bottom-up insights into discrete emotions from a CPM perspective. Its merit lies in minimising reliance on assumptions about the relationships between variables, enabling data to guide the interpretation of emotions from a CPM perspective.\n\nMost of the data-driven research has been conducted using individual or a subset of CPM components  [20] ,  [29] ,  [30] ,  [31] ,  [32] ,  [33] . However, this narrow scope can also limit our comprehensive understanding of emotions, leading to less reliable conclusions being drawn by computational models. Therefore, considering a full CPM with five components can potentially improve the consistency of computational models and expand the current understanding of emotions  [18]  and their representation within different components. For example, in a data-driven study by Mohammadi and Vuilleumier  [12] , the authors used the full CPM to investigate the relationship between 10 discrete emotions and their corresponding CoreGRID responses using films as stimuli. The researchers discovered a clear hierarchy between positive and negative emotions within the CPM space. They also found that six dimensions were necessary to capture the differences between emotions, and ML can be used to differentiate emotion features. Another data-driven study used VR to induce emotions actively and examined the full CPM  [17]  but focused more on the appraisal component. Also, due to the limited selection of only 7 VR games, a broader range of emotions may not have been effectively induced. Additionally, their results were influenced by the novelty factor associated with experiencing VR, as not all participants had prior exposure to VR. Collectively, both of these data-driven studies have solely depended on measuring CoreGRID subjective annotations for interpreting the objectively measurable components (expression and physiology) of the CPM.\n\nPhysiological signals have been used in AC research  [11] ,  [34] ,  [35]  to measure emotion responses, providing details about the non-conscious and conscious experience  [36] ,  [37] , intensity, valence, and temporal variations. Recent technological advancements have allowed it to collect physiological data non-invasively with minimal setup and configuration  [9] ,  [38] ,  [39] . Researchers have focused on using physiological signals to study different emotions, but relatively little attention has been given to using these signals in studies based on the full CPM. One such example is a data-driven study by Menétrey, et al.  [19] , which has been expanded to include heart activity, skin conductance, and respiration other than the CoreGRID annotations. Using the data collected inside a functional Magnetic Resonance Imaging (fMRI) machine, researchers confirmed ML's ability to differentiate emotions and assess synchronisation between the components during an emotional episode. The full CPM showed better when all components were considered rather than just one. However, this experiment setup may be less relaxing due to the fMRI setup and passive emotion induction due to the use of films  [21] . Moreover, some studies have used facial signals for measuring emotions.\n\nFacial electromyography (fEMG) is a non-invasive technique that uses electrodes to measure facial muscle activations  [40] ,  [41] . Studies showed that fEMG can reflect changes in the dimensional model of emotion  [40] ,  [42] ,  [43] ,  [44] , discrete model of emotion  [45] ,  [46] ,  [47] , and facial expressions  [41] ,  [48] ,  [49] . Several studies have focused on individual components or processes of the CPM  [30] ,  [31] , but to our knowledge, none of these studies has examined the full CPM using fEMG as an objective measure of expression.\n\nOverall, the CPM is progressively getting more recognition in AC to understand the multi-faceted nature of emotions and underlying processes. The CPM can also be considered as a comprehensive model that takes context into account and goes beyond using only the expression. Current research has provided valuable insights into emotion formation's cognitive, physiological, and behavioural processes. However, further research is yet needed to provide deeper insights into underlying mechanisms, the role of physiology, and other components in a more immersive and naturalistic setting.\n\nMany of the previous studies in this field have utilised passive methods, such as films, for eliciting emotions. However, these methods have been shown to be less effective in eliciting emotions than active methods. While some studies have used active methods, such as VR, they have often restricted their stimulus selection to a narrower spectrum of emotions, relied solely on self-reporting, and have not fully investigated the objectively measurable components of emotions or the contributions of each component. Furthermore, most studies tend to overlook the subjective nature of emotions and pre-label emotional situations with fixed labels. Therefore, we propose a data-driven approach, postulating emotional experience as subjective, to understanding emotions from a componential perspective. We will employ active emotion elicitation techniques and collect multimodal objective and subjective measures to assess the contribution of each component in capturing discrete emotions.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Methodology",
      "text": "This section outlines the research design, material selection, and data collection procedures employed in this study.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Material And Assessment",
      "text": "Material selection is a significant step toward the reliability of research conclusions. Therefore, we used interactive VR games to create a more realistic and immersive emotion induction paradigm as opposed to passive paradigms  [21] ,  [50] . We selected 96 VR games from Steam 1 used in previous works  [17] ,  [51] ,  [52]  and which were rated on game review websites. From that, we chose 27 VR games (refer to Supplementary Table  S1 ) based on the game mechanics' simplicity, clarity, diversity, and the possibility to generate emotions within three minutes. For instance, the chosen games feature VR scenarios like shooting and zombie encounters mainly at evoking emotions such as fear, disgust, and hate. Additionally, VR experiences like slingshotting, slicing fruits, and slashing blocks are included to evoke feelings of joy, pleasure, and interest. To ensure the selection encompasses a wide range of emotions, games were labelled with a dominant emotion only for selection purposes using the Geneva Emotion Wheel (GEW)  [53] ; however, these labels were not used in the following analysis. We used all the 20 emotions from the GEW, which are defined as positive (interest, amusement, pride, joy, pleasure, contentment, admiration, love, relief, compassion) and negative (sadness, guilt, regret, shame, disappointment, fear, disgust, contempt, hate, anger). Given that VR games are primarily developed for entertainment purposes, it was difficult to identify games that could elicit negative emotions and some of the positive emotions within a three-minute time frame. Consequently, an uneven distribution of emotions was observed in the selected games, as indicated in Supplementary Table  S1 .\n\nWe labelled seven games based on previous works  [17] ,  [51] ,  [52]  and the rest of the 20 games based on researchers' gameplay experience and game review ratings and comments. While we termed each game with primary emotion, we did not expect all participants to experience the same emotions due to subjective variations  [20] ,  [35] ,  [54] . Therefore, we used the GEW and CoreGRID as surveys to collect participants' emotional experiences following each game.\n\nIn the experimental setup, we organised a training session to train the participants to VR and to reduce the novelty effect of VR. For that, we used the SteamVR tutorial application, NVIDIA® VR Funhouse, Fantasynth One, Expedia Cenote VR, Luna, and Google Spotlight Stories: Age of Sail games.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Proposed System",
      "text": "Figure  2  shows our experimental setup, where we used an HTC VIVE Pro headset with controllers for game presentation. We used emteqPRO  [55]  wearable device to measure the facial EMG activations and Empatica E4 wristband to record Heart Rate (HR), Electrodermal Activity (EDA), Blood Volume Pulse (BVP), Inter Beat Intervals (IBI), skin temperature, and acceleration. We also used Shimmer sensors to measure respiration and Electrocardiography (ECG) and an off-the-shelf Inertial Measurement Unit (IMU)  [56]  to track body movements. However, for the purpose of the present study, our focus was limited to the analysis of facial EMG, BVP, skin temperature, EDA, and respiration.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Procedure",
      "text": "The research was approved by the University of New South Wales, Human Research Ethics Committee (HC200809). The dataset was collected from 39 participants (18 females, 21 males, mean age = 25 & SD = 5.42 years) who were first screened by the Motion Sickness Susceptibility Questionnaire (MSSQ)  [57]  for motion sickness. The inclusion/exclusion criteria to recruit participants were an age range of 18-40 years, without any type of prior psychological or prior neurological disorders, fluent in the English language, not wearing glasses or with corrected refractive error, and without prior vertigo, hearing, or vestibular problems. Each participant went through a one-hour training session and three 100-minute actual data collection sessions as part of our data collection process. Each participant received $150 as reimbursement for their participation. The training session began with an explanation of the experiment setup and instructions for the participants to report their own experiences rather than what they thought should be felt while playing the games in actual data collection sessions.\n\nWe developed an app using PsychoPy  [58]  to present the games, administer the questionnaires, synchronise each event, and save our data. To ensure a balanced distribution of emotional experiences across the three data collection sessions, we randomised the sequence of pre-labelled games. We minimised repeating any similar experiences within a single session. Additionally, we randomised the presentation of each survey item to reduce any effect of the order. We collected each participant's demography (pre-experimental survey), personality (Big Five Inventory survey), and mood (Brief Mood Introspection Scale survey) through surveys.\n\nFor the calibration of wearable devices, we first collected neutral expressions. For the calibration of emteqPRO, we further collected several maximum smiles, frowns, and eyebrow raises expressions. To instruct the participants in the calibration phase, we developed 3D scenes using Blender  [59] , which were deployed to VR environments using HAR-FANG®3D  [60] . Then each participant played a VR game for three minutes and completed GEW and CoreGRID (refer to Supplementary Tables  S2  and S3 ) surveys subsequently, where they marked emotional experience on a 5-point Likert scale (1-Not at all, 5-Strongly). This process was repeated until each participant had completed all 27 games in three data collection sessions. We collected 1053 observations (27 games × 39 participants) from all participants but only considered 1041 observations in our analysis. The rest of the observations were removed due the technical concerns such as network errors, game updates, and headset disconnections. Lastly, we post-processed our collected facial EMG data using the SuperVision  [55]  application to get the frequency and intensity of facial expressions such as smile, frown, eyebrow raises and neutral, arousal, and valence insights.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Results And Discussion",
      "text": "The following subsections describe our results and implications. The first section, 4.1, discusses the different discrete emotions experienced in VR, while section 4.2 explores the componential features of these emotions. In section 4.3, the focus is on finding the emotional dimensions, while section 4.4 explains the role of facial EMG in the emotional experience. Finally, section 4.5 discusses the use of ML in modelling emotions and investigates the role of each component. Together, these sections provide a comprehensive understanding of the various facets of emotional experience in VR.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Discrete Emotions In Vr",
      "text": "Given the novelty of VR to emotional studies, the efficacy of VR games in inducing a wider range of emotions should be validated to answer our research questions. Therefore, in Figure  3 , we illustrated the 39 participant's average intensity scores of each discrete emotion (defined by the 20 GEW terms) when playing the selected VR games. This plot may be important to researchers interested in selecting VR games that evoke specific emotions. It can provide insight into which VR games are most effective at eliciting specific emotions when designing future studies and aid in identifying which emotions are most elicited by VR games in general, which can inform the development of VR games that are more emotionally engaging.\n\nIt is apparent from this figure that all the selected emotions were experienced to a certain intensity by participants. Regardless of the negative valence of some games, positive emotions such as interest, amusement, pride, joy, pleasure, contentment, and admiration are highly rated. This is supported by the high immersion and pleasantness in VR games that increase interaction and engagement. Additionally, love, relief, compassion, disappointment, fear, and anger have been triggered by some games. However, the intensity of sadness, guilt, regret, shame, disgust, contempt, and hate is relatively low. As VR games are generally targeted for entertainment, it was not easy to find games that elicit such complex emotions. Our results align with previous surveys  [21]  and studies that found sadness, guilt, and contempt  [17]  to be rated as low-intensity emotions in general in laboratory experiments. This could be due to the intricate nature of some of these emotions, which are often referred to as social or self-conscious emotions (shame, guilt, and contempt  [6] ,  [61] ,  [62] ). Furthermore, we observed a discrepancy between our initial categorisation (Supplementary Table  S1 ) of emotions, which was based on previous works, the experience of the researchers, and ratings and comments from game review sites. The outcomes of this analysis suggest that individuals exhibit varied emotions in similar situations. Consequently, the ratings for the selected games and their content reveal the presence and complex nature of appraisal objectives to react to an event adaptively. Moreover, the figure highlights the presence of complex emotional structures and mixed feelings in each game, which makes the current emotion recognition models that usually label an event with only one emotion less effective. The study's results indicate that our VR games are effective means of eliciting a range of emotions (though it is usually mixed emotions), which can be used to address our research questions.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Componential Emotion Features In Vr",
      "text": "We performed a cluster analysis on the CPM profile of different emotion categories to unfold the hierarchical organisation of each emotion in the CPM space. Here, our analyses were based on the self-assessment of 51 CoreGRID and 20 GEW items, with the CoreGRID items serving as representatives within the CPM. The formula for the weighted average CPM profile (CP j ) for emotion term j is given as follows:\n\nis the score of emotion term j in sample i, x i is the CoreGRID score vector for sample i and n is the number of samples. We conducted a Ward method-based hierarchical clustering using the Euclidean distance metric  [23] ,  [63] . The hierarchical organisation illustrated in Figure  4  depicts the literature's classical definitions of positive and negative emotions  [53] . Further, it can be explained by the CPM's intrinsic (un)pleasantness, goal obstructiveness, and goal relevance  [20] . Lower-level branching can also be identified as eight meaningful sub-clusters: anger (hate, disgust, anger), fear, sadness (disappointment, sadness), embarrassment (shame, regret, guilt, contempt), happiness (pleasure, joy, amusement, pride), satisfaction (admiration, contentment), serenity (relief, interest) and affection (compassion, love).\n\nIn our negative cluster in the CPM space, we found anger, fear, sadness, and embarrassment as the main sublevels. These hierarchically organised clusters demonstrate the high arousal activation of anger and fear against lowarousal emotions such as sadness  [64] . Also, shame, regret, and guilt clustered in terms of embarrassment suggest the clustering of low power and negative valence emotions, as depicted in GEW  [53] . However, contempt, which is clustered in the higher-level branch of the embarrassment cluster, is defined as a negative valenced and high-power in GEW  [53] . But contempt clustered here with shame and guilt potentially due to the complex nature of these emotions as social or self-conscious emotions  [62] . In our positive cluster, we found happiness, satisfaction, serenity, and affection as the main sub-levels. The happiness cluster consists of high power and positive valence emotions agreeing to the independent research  [12] ,  [20] . Similarly, the hierarchical organisation of the satisfaction cluster agrees with GEW's low power and positive valenced emotions  [53] . However, a separate serenity cluster is observed with interest and relief, which are positive valence but opposed poled with power dimension regarding GEW  [53] . This observation contrasts with the previous findings where interest and relief were observed in two different clusters in CPM space  [17] . Our observation may be possibly due to the high immersion and pleasantness in selected VR game experiences. The hierarchical organisation of affection cluster in the CPM space agrees with the low power and positive valence dimensions, including attachment-related emotions such as compassion and love  [36] ,  [65] .\n\nThe vector values of the weighted average CPM profile (CP j ) of each discrete emotion term are presented in Supplementary Figure  S2 . Accordingly, the intensity of different CoreGRID items varied in response to different discrete emotions or combinations of them. This illustrates the impacting factors for each emotion and supports the previous clustering results.\n\nOverall, our results' validity and meaningful clustering for a wide range of emotions show that VR is a powerful tool for emotion research, offering a more realistic environment than traditional methods such as films  [12] ,  [20] .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Labeling The Emotional Dimensions",
      "text": "Emotion terms are differentiated by various dimensional models of emotion. While the basic valence and arousal dimensional model is generally recognised, there are complications in rationalising some emotions  [66] . For example, intense anger is considered high arousal, and intense sadness is considered low arousal  [67] . Given that, the basic dimensional model cannot distinguish between intense and less intense sadness due to low arousal in both cases. Therefore, the current scope of the basic dimensional model creates limitations in differentiating certain emotions.\n\nTo explore the latent dimensions and address our second research question, we conducted an exploratory factor anal-ysis on the 51 CoreGRID items, examining dimensions from a componential perspective. We performed a factor analysis followed by a Varimax rotation on the z-score normalised 51 CoreGRID items  [68]  and identified five factors based on a scree plot. We found five meaningful factors explaining 43.84% of the total variance, capturing aspects related to body changes and expressions associated with suddenness and novelty (14.20%), valence (10.89%), agency (6.94%), novelty (5.42%), and norms (6.40%). Our results align with the individual research  [12] ,  [66] ,  [68] ,  [69] , suggesting more than the commonly considered two dimensions of valence and arousal. As an example, the full CPM data-driven study by  [12]  identified action tendency, (un)pleasantness, novelty, norms, arousal, and goal relevance as dimensional representations of emotions. Some of the previous works have used a few components  [33] , emotional terms, and facial expressions  [65] ,  [70] ,  [71] , so the results were limited to a few dimensions. Nevertheless, our assumption of full CPM with CoreGRID items expands the dimensions that can differentiate among emotion words. Moreover, a multicomponential architecture of emotion aligns broadly with our five-factor analysis.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Role Of Facial Emg In The Emotional Experience",
      "text": "Facial expressions are a prominent form of non-verbal communication of emotions  [41] ,  [49] . However, when face monitoring is not feasible, facial EMG has been used to measure muscle movements using surface EMG sensors  [72] . Therefore, we investigated the power of facial EMG in differentiating self-reported emotional (20 emotion terms from GEW) experiences to gain insights into the role of CPM expression components in emotions and their correlation. We uploaded our calibration data (neutral, max smile, max frown, max eyebrow raises) and gameplay data to SuperVision to get facial EMG expressions (smile, frown, eyebrow raise) and their intensities. Figure  5  shows the Spearman correlation of three facial EMG expressions with 20 emotions.\n\nThe results showed a significant positive correlation of \"smile emg\" with interest, joy, pleasure (p<0.001), amusement (p<0.01), and pride, fear (p<0.05), indicating activation of zygomaticus muscles in pleasant scenarios except for fear  [30] ,  [42] ,  [43] ,  [49] . However, the positive correlation of EMG between fear, shame, hate, and anger may be due to lip tightener, lip stretcher, and jaw drop  [73] , which are related to mouth areas. A significant negative correlation was observed with admiration, sadness, and contempt (p<0.05), possibly due to lower arousal  [53]  and complexity  [62] ,  [74]  of these emotions, leading to lower activation in the zygomaticus muscle. The \"frown emg\" was positively correlated with most of the negative emotions disgust (p<0.001), hate (p<0.01), fear, and anger (p<0.05). Further, the positive emotions except admiration and compassion were significantly negatively correlated with \"frown emg\". The high activation of corrugator supercilli facial muscles in negative experiences can explain the observation  [43] ,  [72] ,  [75] .To further note, the significant correlation between the corrugator or \"frown emg\" and positive/negative emotions can be attributed to its positioning below the frontalis muscle, which is involved in facial expressions such as frowning and eyebrow-raising  [72] . The pattern of \"eyebrowraise emg\" with emotions was ambiguous, given that it showed a significant positive correlation with \"frown emg\" (p<0.001), admiration, and compassion (p<0.01). The positive correlations with admiration and compassion, typically associated with positive emotions, suggest that \"eyebrowraise emg\" may not be a definitive indicator of positive emotions. The significant negative correlation of \"eyebrowraise emg\" with interest and pleasure (p<0.01) may be due to the high arousal of these emotions showing lower activations of frontalis EMG  [42] . Although most were non-significant, the positive correlation between \"eyebrowraise emg\" and emotions, except for seven, suggest ambiguous valence  [76]  and the role of novelty  [66] , more like in surprise elicitation.\n\nSimilarly, we did a Spearman correlation but with the eight CoreGRID expression component items and three facial EMG activations. As given in Figure  6 , a positive correlation of \"smile emg\" with \"smile?\", \"shout or exclaim?\" (p<0.001), \"eyebrows go up?\" (p<0.01), and \"frown?\" (p<0.05) were visible. The \"smile emg\" correlation between \"smile?\", \"shout or exclaim?\" was expected due to the zygomaticus muscle movement caused by an actual smile  [42] ,  [77]  and involuntary speech or orofacial movements  [49] , respectively, where SuperVision cannot differentiate between two expressions linked to similar muscle areas. The correlation of \"smile emg\" with self-reported \"frown?\" may be due to the activations in the cheek region, such as nose wrinkling, upper lip raising, lip corner depression, and lips tightening in unpleasant outcomes  [30] . In contrast, \"smile emg\" was negatively correlated with \"cry?\" (p<0.01), which is consistent with the lower activation of the zygomaticus muscle previously observed in other studies  [78] . Given the differences in individual experiment settings, insights from SuperVision may be more general, so a customised model may improve interpretations.\n\nAs the next observation, \"frown emg\" showed a positive correlation with \"frown?\" (p<0.05) and a negative correlation with \"smile?\" (p<0.01). This may be possibly due to the high and opposite activation of corrugator supercilli muscles  [79] ,  [80]  respectively or due to the involvement of corrugator muscle in both negative and non-negative facial expressions  [72] . This finding aligns with the obstructive and conducive appraisal in the brow region  [30] .\n\nThe \"eyebrowraise emg\" showed a positive correlation with \"frown emg\" (p<0.001) and negative correlations with \"smile?\" (p<0.05). The negative correlations suggest that raised eyebrows are less likely to occur with smiling, possibly indicating a more subdued emotional state or a lack of intensity in the emotional response. However, the correlation between EMG and self-reported eyebrow activations was negative and non-significant, perhaps due to certain oversights in the SuperVision application or individuals' perception of eyebrows raising is not necessarily reliable.\n\nIn any case, the results demonstrate the possibility of facial EMG in monitoring facial expressions to detect emotions. Further, it emphasises the role of the expression component in emotion formation after event appraisal.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Modelling Using Machine Learning",
      "text": "To test for any profound relationships of emotions with full CPM, including signals, we used ML classifiers. This section focuses specifically on our third research question, which seeks to determine the extent to which each component of the CPM contributes to the identification of discrete emotions. For that, we first preprocessed the data and applied digital signal processing. EMG signals are typically sampled at a rate of 1000 Hz to capture and preserve the signal's full range of features and nuances  [41] . Therefore, we resampled all the signals to 1000 Hz to ensure that the signals are evenly sampled in time and allow for accurate comparison and analysis of the signals. The BVP signal was up-sampled from 64 Hz to 1000 Hz, and a median filter was applied to reduce the noise  [81] . Both skin temperature and EDA up sampled from 4 Hz to 1000 Hz. Then applied, a Savitzky-Golay (SG) filter  [82]  for skin temperature (window length=9 and polyorder=5) and EDA (window length=11 and polyorder=5)  [83]  for smoothing the signal. The respiration signal was up-sampled from 256 Hz to 1000 Hz and applied an SG filter (window length=11 and polyorder=5)  [83] . Seven EMG amplitude channels (right and left frontalis, right and left orbicularis, right and left zygomaticus, corrugator) were downsampled to 1000 Hz from 2000 Hz.\n\nFor feature extraction, we employed Discrete Wavelet Transformations (DWT). DWT is beneficial in this context because it allows for localisation in both the frequency and time domains and provides a multi-resolution analysis at different frequencies or scales. Through experimentation, we found that the Daubechies wavelet family and five levels of frequency decomposition yielded the best results. We used the coefficients' mean, variance, and median resulting from the five decomposition levels as features. Accordingly, we used 51 CoreGRID items, three statistical features of DWT of BVP (15 features), skin temperature (15 features), EDA (15 features), respiration (15 features), and seven EMG channels (105 features). Apart from that, we fed the average intensities of smile, frown, and eyebrow raise retrieved from the SuperVision application, resulting in 219 input features.\n\nWe trained ML classifiers treating the CoreGRID items and signal features as input representing the CPM interpretations. We used each discrete emotion's high and low values as split by the first quartile output. Due to the imbalance between high and low values, we selected the first quartile as the threshold instead of the median. We within-subject z-normalised the input features and sampled the training dataset using the Synthetic Minority Oversampling Technique (SMOTE)  [84]  to reduce the imbalance in the dataset. For better generalisability, we report the LOSO cross-validation results, taking the average after all iterations.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Interpretation Of Emotional Experience Through The Coregrid And Physiological Changes",
      "text": "To understand the emotional experience through the Core-GRID items and physiological changes, we trained several commonly used ML classifiers such as Support Vector Machine (SVM), Random Forest (RF), eXtreme Gradient Boosting (XGB) and Light Gradient Boosting Machine (LGBM). We followed the process outlined in Section 4.5 and evaluated the performance of our model using LOSO cross-validation, repeating the process. Although we trained several classification models, we found that the LGBM had the best performance, so we presented the results from that model.\n\nWe initially assessed model performance using Core-GRID items alone and in combination with physiological measures. The combined approach showed an average increase of 1.61%, ranging from 0.22% to 4.48%. Therefore, the results of training the LGBM classifier using both measures for each emotion against the chance level (majority class prediction) are shown in Figure  7 . In our analysis, we generated the baseline using both the chance level and a nonparametric chance distribution through permutations. However, since both baselines were nearly similar, we opted to use the chance level as the baseline for all of our analyses. In all cases except four (shame, regret, guilt, and contempt), the model exceeds the chance level. However, according to the one-sample t-tests, accuracy was significantly higher (p<0.001) for interest, amusement, pride, joy, pleasure, contentment, admiration, love, and fear. Accuracy of relief, compassion, and anger are moderately significant (p<0.05). The lower performance for the remaining emotions can be attributed to the skewed distribution of the emotion ratings, resulting in fewer representative examples of the underrepresented class, and making it more difficult for the model to identify distinguishing patterns. For example, guilt, disgust, and shame are heavily skewed towards the lower end, possibly due to the complexity of these emotions  [74] . As VR games are primarily designed for entertainment, it is not easy to find games that effectively elicit these emotions. Additionally, the margin of improvement for some significant models is smaller due to the same cause. Overall, the effectiveness of selected physiological measures and CoreGRID items in differentiating emotional features is depicted.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Role Of Each Cpm Component And Physiological Changes In Differentiating Discrete Emotions",
      "text": "To investigate the contribution of each CPM component and physiological measures, we trained ML models as previously described but using the relevant combination of features for each component. (For example, the appraisal component includes 17 appraisal CoreGRID items, and the physiology component includes 8 physiology CoreGRID items and BVP, skin temperature, EDA, and respiration). The results for 20 emotions and the statistical significance level for using either individual components or full CPM compared to chance level (majority class prediction) are given in Table  1 . In Table  2 , we provide the results when removing one component at a time and training with the remaining components. Using this approach, we can determine the additional information provided by each component in distinguishing discrete emotions.\n\nAccordingly, the appraisal component was significant for amusement, pride, joy, pleasure, and contentment (p<0.001) and interest, love (p<0.01). This correlation between the appraisal component and most positive emotions may be due to the compatibility with the participant's goals, their ability to cope with the experience, and the intrinsic pleasantness  [20] ,  [85]  of the VR experience. The motivation component showed significant performance for interest, amusement, pride, joy, pleasure (p<0.001), contentment, and fear (p<0.01). It appears that motivational goals such as information seeking, savouring, and wishful thinking  [85]  have impacted the experience of positive emotion. In contrast, fear correlations represented avoidance action tendencies  [61] . Next, for the CPM expression component, first, we trained classifiers only using the self-reports and both self-reports and facial signal features. We reached the conclusion that models incorporating both inputs consis- tently outperformed unimodal models, with improvements ranging from 0.02% to 12.77%. Therefore, this paper only presents the results for these multimodal models. Accordingly, the expression component was significant for interest, amusement, pride, joy, pleasure, love (p<0.001), contentment, and admiration (p<0.01). This finding suggests that facial expressions, the most natural form of nonverbal communication  [41] , are effectively captured by facial EMG signals and CoreGRID items. Like the expression component, a better performance (0.45% -8.27% improvement) was observed for the multimodal physiological component with a combination of self-reports and signal features. In parallel with previous research, the physiology component showed ambiguous patterns for emotion differentiation  [19] ,  [36] . Consequently, emotions such as pride (p<0.001), joy, love (p<0.01), pleasure, and fear (p<0.05) showed significance. Although fear showed similar behaviour, the remaining significant observations diverged from those seen in a previous study  [63] .\n\nIn that study, it was observed that only the expression and feeling components held significance for the emotions of joy, love, and satisfaction. One potential explanation could be that our study, which employed both subjective and objective measures, possessed more informative features.\n\nLastly, the feeling component is significant for interest, amusement, pride, joy, pleasure, contentment, admiration (p<0.001), love, and fear (p<0.01). However, the feeling component, traditionally the focus of previous research, demonstrated minimal performance in predicting most negative emotions. This less prediction capacity for negative emotions can be partially explained by the skewness of the emotion ratings towards the lower end because VR experiences were often perceived as pleasant even when they involved challenging or fearful content  [17] . Overall, similar to using full CPM, using single components demonstrates better distinction powers for positive emotions in our experiment.\n\nNext, we compared the results of training ML models   1 ) with results from excluding one component (Table  2 ). To better understand the role and importance of each component, in this experiment, we compared the accuracy of each model with full CPM as the baseline (Table  2 ), as our objective here is to show the relative importance of each component in the full CPM model. We expect that removing more informative components will result in higher reductions in the prediction power.\n\nTable  1  showed that emotions like pleasure, joy, pride, fear, amusement, and admiration were significant when using single components or with the full CPM. However, in Table  2 , removing each component did not lead to any significant difference when comparing these emotions (pleasure, joy, pride, fear, amusement, and admiration) with the full CPM results. This suggests that each component can complement specific emotion differentiation, and the information from removed components is already encoded in other components.\n\nAs shown in Table  1 , when comparing accuracy with the chance level for emotions like disgust, sadness, and shame, no scenario was significant. However, removing different combinations of motivation, expression, and appraisal components resulted in a significant accuracy reduction compared to the full CPM performance (Table  2 ). This implies that although these components may not directly impact the results, they provide complementary information not provided by other components or the full CPM.\n\nFor interest, the full CPM (p<0.001) and single components, except physiology, were significant. However, a significant performance reduction was observed when using all components except physiology, suggesting that even though it may not be important as a single component, it encodes relevant complementary information for interest. Similarly, for contentment, including only the feeling component (p<0.001) and removing the feeling component (p<0.01) both showed significant performance. This indi-cates that the feeling component provides significant details for contentment.\n\nA different observation was made for regret, where neither the single components nor the full CPM were significant, as shown in Table  1 . However, removing the expression component (p<0.01) in Table  2  led to an accuracy improvement. This implies that the information encoded in the expression component might be just redundant. On the other hand, for the emotions of hate, disappointment, guilt, and contempt, no model was significant in both tables. In summary, this overall comparison suggests that while some components have complementary counterparts, others contain very specific or complementary information for each emotion.\n\nAs the last analysis, we report the feature importance scores for the LGBM model while using the full CPM for emotion prediction. Supplementary material Figures  S3-S7  illustrate the scores for each component while trained using the full CPM. Accordingly, a clear differentiation can be observed among the clusters depicted in Figure  4 .\n\nOverall, the model that includes all CPM self-reports and DWT signal features performs well in distinguishing emotions. Our results reveal that each component of the CPM may have a specific role in emotion differentiation. Additionally, considering only one component of the CPM may limit our understanding of emotion formation. These findings suggest that a complete understanding of emotion formation may require considering the interplay between all five components of the CPM.\n\nThe accuracy improvements observed when using both subjective and objective measures highlight the value of incorporating objective measures in addition to subjective evaluations, particularly for components of emotion that individuals may not be aware of. This suggests that relying solely on subjective evaluations may not provide a complete understanding of emotional states.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "Our manuscript aimed to study CPM as a framework to examine the interconnected components and sub-processes that contribute to the formation of emotions. We operationalised a data collection using 27 interactive VR games and measured the subjective ratings and physiological and facial signals from 39 participants. This study makes several significant contributions to the field of affective computing and emotion recognition: (i) Using VR as an immersive environment to induce emotions as a more ecologically valid setup, (ii) It provides insights into the underlying mechanisms that link CPM and emotional experience using a realistic emotion induction paradigm without assuming any pre-labels for emotional experience. This allows for a better understanding of how these factors are related in a naturalistic setting and takes into account individual differences, (iii) Provides a comprehensive analysis of the underlying dimensions that describe the emotional experience, (iv) The study examines the contribution of different CPM components and modalities to emotional experience, providing a nuanced and detailed understanding of their respective roles, and (v) The study employs a larger sample size and generalised ML models to examine emotion formation, which allows for more accurate and reliable findings.\n\nOur study found that the CPM provides a good framework for understanding emotion formation when emotions are induced through interactive VR games, and it can help differentiate between discrete emotions. We chose VR games due to their immersive nature, providing a richer emotional experience compared to still images or video clips. Thus, our focus in this manuscript is solely on understanding emotions rather than analysing the impact on VR gameplay. Each component of the CPM framework plays a specific role in capturing these emotions, and a combination of selfreport and objective measures is effective in understanding their underlying processes. We identified five latent dimensions that underlie emotional experiences induced through interactive VR games, which may be universal to the human experience of emotions. These findings have implications for the design of systems that aim to recognise and respond to emotions, such as in healthcare, education, and gaming. Further research is needed to fully understand the complex processes underlying emotion formation and develop more effective methods for capturing and analysing emotions.\n\nIt's important to acknowledge several limitations in this study. Firstly, the imbalanced distribution of emotions is likely influenced by the overall pleasantness and immersive nature of the VR experience. Additionally, we collected various other signals that were not utilised in our current analysis. Furthermore, increasing the sample size could enhance the robustness of our interpretations. Moreover, we recognise the challenge of accurately capturing and reporting facial expressions lasting mere milliseconds within a 3-minute period, along with participants' difficulty in recalling and reporting these subtle expressions comprehensively. In future research, investigating the impact of VR games on emotions, exploring the dynamic nature of emotional responses, and understanding brain processes could offer valuable insights. Moving forward, we recognise the importance of addressing these limitations in our future research works.    Appraisal feel that the experience was unpredictable? Appraisal feel that there was no urgency in the situation? Appraisal feel in control over the outcome? Appraisal feel that the outcomes were a result of your behaviour? Appraisal feel that the experience was incongruent with your standards/ideals? Appraisal feel that you had the power over the consequences of the event? Appraisal feel powerless in the situation? Appraisal",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Simple Component Process Model (CPM) based on [18].",
      "page": 2
    },
    {
      "caption": "Figure 1: for a visual",
      "page": 2
    },
    {
      "caption": "Figure 2: A participant playing VR games while wearing wearable devices,",
      "page": 4
    },
    {
      "caption": "Figure 2: shows our experimental setup, where we used an",
      "page": 4
    },
    {
      "caption": "Figure 3: Participant’s average intensity ratings for discrete emotions by VR",
      "page": 5
    },
    {
      "caption": "Figure 3: , we illustrated the 39 participant’s average intensity",
      "page": 5
    },
    {
      "caption": "Figure 4: Results of the componential hierarchical clustering of discrete",
      "page": 6
    },
    {
      "caption": "Figure 5: shows the",
      "page": 6
    },
    {
      "caption": "Figure 5: The correlation matrix displays the average intensity of three facial EMG expressions (“smile emg”, “frown emg”, “eyebrowraise emg”) in",
      "page": 7
    },
    {
      "caption": "Figure 6: The correlation matrix presents the average intensity of",
      "page": 7
    },
    {
      "caption": "Figure 7: In our analysis, we",
      "page": 8
    },
    {
      "caption": "Figure 7: Accuracy of the LGBM binary classifiers for differentiating emotions. Accuracy is compared with the chance level (majority class portion).",
      "page": 9
    },
    {
      "caption": "Figure 4: Overall, the model that includes all CPM self-reports",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Appraisal": "77.99\n74.83\n70.09\n73.52\n66.13\n70.45\n70.97\n72.10\n77.27\n67.55\n73.64***\n73.66***\n68.40***\n69.22***\n61.83\n69.89***\n55.20\n64.99**\n58.83\n58.06**",
          "Motivation": "80.25\n77.59\n70.14\n77.30**\n59.53\n68.16\n70.62\n65.79\n76.88\n68.53\n71.83***\n71.40***\n70.43***\n62.91***\n60.50\n65.96**\n47.32\n68.24***\n60.84\n54.68",
          "Expression": "72.11\n77.98\n68.84\n73.87\n57.16\n73.91\n75.05\n61.01\n82.03\n61.86\n74.34***\n76.36***\n70.08***\n64.83***\n63.25**\n65.25**\n57.96\n67.44***\n52.46\n62.09***",
          "Physiology": "68.56\n70.62\n66.08\n75.77*\n58.61\n68.48\n70.24\n61.95\n76.09\n63.52\n60.45*\n61.66**\n56.14\n60.96***\n60.78\n62.93\n58.00\n56.80\n56.70\n58.21**",
          "Feeling": "72.73\n68.82\n69.53\n76.02**\n59.96\n69.29\n66.58\n66.48\n66.41\n61.32\n75.61***\n75.47***\n69.79***\n68.52***\n65.77***\n73.93***\n58.10\n67.63***\n59.95\n58.95**",
          "Full CPM": "82.23\n83.60\n75.52*\n81.14***\n65.22\n78.70\n78.46\n69.15\n84.64\n69.22\n76.81***\n76.96***\n72.41***\n68.21***\n66.96***\n73.57***\n60.74*\n71.90***\n64.01*\n62.67***",
          "Baseline": "79.84\n81.26\n71.12\n72.14\n62.21\n78.01\n81.16\n73.76\n85.71\n71.83\n56.23\n55.83\n54.71\n50.15\n58.36\n59.68\n55.93\n58.87\n58.46\n52.08"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 2: ). To better understand the role forcontentment.",
      "data": [
        {
          "WO Appraisal": "81.34\n83.76\n75.03\n81.16\n65.62\n76.16**\n79.05\n70.17\n84.16\n70.83\n76.06\n78.37\n71.30\n66.35\n66.38\n72.28\n61.45\n71.49\n60.14*\n59.38",
          "WO Motivation": "81.27\n80.37**\n72.87*\n79.55\n65.77\n76.47*\n77.94\n69.94\n83.12\n65.33\n77.70\n78.09\n70.43\n68.42\n66.10\n73.19\n59.65\n69.65\n62.50\n61.33",
          "WO Expression": "82.12\n81.78*\n76.15\n81.12\n64.00\n75.86*\n75.79*\n72.4**\n83.76\n70.67\n75.28\n77.98\n70.95\n68.20\n67.91\n72.48\n59.53\n69.55\n62.37\n59.88",
          "WO Physiology": "81.49\n84.32\n75.42\n80.63\n63.32\n77.80\n78.09\n69.64\n84.23\n69.06\n77.60\n77.16\n71.42\n69.12\n65.03\n72.88\n55.40*\n69.88*\n62.16\n60.34",
          "WO Feeling": "82.01\n83.92\n74.34\n80.26\n67.45\n76.23\n77.83\n68.76\n85.28\n68.96\n75.61\n77.51\n72.68\n68.10\n66.31\n69.95*\n59.98\n70.38\n61.83\n61.46",
          "Full CPM": "82.23\n83.60\n75.52*\n81.14***\n65.22\n78.70\n78.46\n69.15\n84.64\n69.22\n76.81***\n76.96***\n72.41***\n68.21***\n66.96***\n73.57***\n60.74*\n71.90***\n64.01*\n62.67***"
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotions are emergent processes: they require a dynamic computational architecture",
      "authors": [
        "K Scherer"
      ],
      "year": "2009",
      "venue": "Philosophical Transactions of the Royal Society B: Biological Sciences"
    },
    {
      "citation_id": "2",
      "title": "Affective computing",
      "authors": [
        "R Picard"
      ],
      "year": "2000",
      "venue": "Affective computing"
    },
    {
      "citation_id": "3",
      "title": "Functional grouping and cortical-subcortical interactions in emotion: a meta-analysis of neuroimaging studies",
      "authors": [
        "H Kober",
        "L Barrett",
        "J Joseph",
        "E Bliss-Moreau",
        "K Lindquist",
        "T Wager"
      ],
      "year": "2008",
      "venue": "Neuroimage"
    },
    {
      "citation_id": "4",
      "title": "The brain basis of emotion: a meta-analytic review",
      "authors": [
        "K Lindquist",
        "T Wager",
        "H Kober",
        "E Bliss-Moreau",
        "L Barrett"
      ],
      "year": "2012",
      "venue": "Behavioral and brain sciences"
    },
    {
      "citation_id": "5",
      "title": "Dimensional, basic emotion, and componential approaches to meaning in psychological emotion research1",
      "authors": [
        "J Fontaine"
      ],
      "year": "2013",
      "venue": "Dimensional, basic emotion, and componential approaches to meaning in psychological emotion research1"
    },
    {
      "citation_id": "6",
      "title": "Conscious emotional experience emerges as a function of multilevel, appraisal-driven response synchronization",
      "authors": [
        "D Grandjean",
        "D Sander",
        "K Scherer"
      ],
      "year": "2008",
      "venue": "Consciousness and cognition"
    },
    {
      "citation_id": "7",
      "title": "On the importance of both dimensional and discrete models of emotion",
      "authors": [
        "E Harmon-Jones",
        "C Harmon-Jones",
        "E Summerell"
      ],
      "year": "2017",
      "venue": "Behavioral sciences"
    },
    {
      "citation_id": "8",
      "title": "Mped: A multi-modal physiological emotion database for discrete emotion recognition",
      "authors": [
        "T Song",
        "W Zheng",
        "C Lu",
        "Y Zong",
        "X Zhang",
        "Z Cui"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "9",
      "title": "A review, current challenges, and future possibilities on emotion recognition using machine learning and physiological signals",
      "authors": [
        "P Bota",
        "C Wang",
        "A Fred",
        "H Silva"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "10",
      "title": "Amigos: A dataset for affect, personality and mood research on individuals and groups",
      "authors": [
        "J Correa",
        "M Abadi",
        "N Sebe",
        "I Patras"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "11",
      "title": "Deap: A database for emotion analysis ;using physiological signals",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani",
        "J Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "12",
      "title": "A multi-componential approach to emotion recognition and the effect of personality",
      "authors": [
        "G Mohammadi",
        "P Vuilleumier"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "13",
      "title": "Computational emotion models: A thematic review",
      "authors": [
        "S Ojha",
        "J Vitale",
        "M.-A Williams"
      ],
      "year": "2020",
      "venue": "International Journal of Social Robotics"
    },
    {
      "citation_id": "14",
      "title": "Towards a prediction and data driven computational process model of emotion",
      "authors": [
        "K Scherer"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "15",
      "title": "Basic Emotions",
      "authors": [
        "P Ekman"
      ],
      "year": "2005",
      "venue": "Basic Emotions"
    },
    {
      "citation_id": "16",
      "title": "A psychoevolutionary theory of emotions",
      "authors": [
        "R Plutchik"
      ],
      "year": "1982",
      "venue": "A psychoevolutionary theory of emotions"
    },
    {
      "citation_id": "17",
      "title": "Induction and profiling of strong multi-componential emotions in virtual reality",
      "authors": [
        "B Meuleman",
        "D Rudrauf"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "18",
      "title": "The dynamic architecture of emotion: Evidence for the component process model",
      "authors": [
        "K Scherer"
      ],
      "year": "2009",
      "venue": "Cognition and Emotion"
    },
    {
      "citation_id": "19",
      "title": "Emotion recognition in a multi-componential framework: the role of physiology",
      "authors": [
        "M Menétrey",
        "G Mohammadi",
        "J Leitão",
        "P Vuilleumier"
      ],
      "year": "2022",
      "venue": "Frontiers in computer science"
    },
    {
      "citation_id": "20",
      "title": "Nonlinear appraisal modeling: An application of machine learning to the study of emotion production",
      "authors": [
        "B Meuleman",
        "K Scherer"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "21",
      "title": "Virtual reality for emotion elicitation -a review",
      "authors": [
        "R Somarathna",
        "T Bednarz",
        "G Mohammadi"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "22",
      "title": "An exploratory analysis of interactive vr-based framework for multi-componential analysis of emotion",
      "venue": "2022 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops)"
    },
    {
      "citation_id": "23",
      "title": "Multi-componential analysis of emotions using virtual reality",
      "venue": "Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology"
    },
    {
      "citation_id": "24",
      "title": "Multicomponential emotion recognition in vr using physiological signals",
      "authors": [
        "R Somarathna",
        "A Quigley",
        "G Mohammadi"
      ],
      "year": "2022",
      "venue": "ser. AI 2022: Advances in Artificial Intelligence"
    },
    {
      "citation_id": "25",
      "title": "The why, the what, and the how of the GRID instrument1",
      "authors": [
        "J Fontaine",
        "K Scherer",
        "C Soriano"
      ],
      "year": "2013",
      "venue": "The why, the what, and the how of the GRID instrument1"
    },
    {
      "citation_id": "26",
      "title": "CoreGRID and MiniGRID: Development and validation of two short versions of the GRID instrument1",
      "authors": [
        "K Scherer",
        "J Fontaine",
        "C Soriano"
      ],
      "year": "2013",
      "venue": "CoreGRID and MiniGRID: Development and validation of two short versions of the GRID instrument1"
    },
    {
      "citation_id": "27",
      "title": "Computational models of emotion",
      "authors": [
        "S Marsella",
        "J Gratch",
        "P Petta"
      ],
      "year": "2010",
      "venue": "A Blueprint for Affective Computing-A sourcebook and manual"
    },
    {
      "citation_id": "28",
      "title": "Computational models of appraisal to understand the person-situation relation",
      "authors": [
        "N Yongsatianchot",
        "S Marsella"
      ],
      "year": "2021",
      "venue": "Measuring and Modeling Persons and Situations"
    },
    {
      "citation_id": "29",
      "title": "Emotions triggered by innovative products: A multi-componential approach of emotions for user experience tools",
      "authors": [
        "D Dupré",
        "A Tcherkassof",
        "M Dubois"
      ],
      "venue": "2015 International Conference on Affective Computing and Intelligent Interaction (ACII), Conference Proceedings"
    },
    {
      "citation_id": "30",
      "title": "Temporal unfolding of micro-valences in facial expression evoked by visual, auditory, and olfactory stimuli",
      "authors": [
        "K Gentsch",
        "U Beermann",
        "L Wu",
        "S Trznadel",
        "K Scherer"
      ],
      "year": "2020",
      "venue": "Affective Science"
    },
    {
      "citation_id": "31",
      "title": "Computational imaging during video game playing shows dynamic synchronization of cortical and subcortical networks of emotions",
      "authors": [
        "J Leitão",
        "B Meuleman",
        "D Van De Ville",
        "P Vuilleumier"
      ],
      "year": "2020",
      "venue": "PLOS Biology"
    },
    {
      "citation_id": "32",
      "title": "Investigating appraisal-driven facial expression and inference in emotion communication",
      "authors": [
        "K Scherer",
        "A Dieckmann",
        "M Unfried",
        "H Ellgring",
        "M Mortillaro"
      ],
      "year": "2019",
      "venue": "Emotion"
    },
    {
      "citation_id": "33",
      "title": "Psychophysiological responses to appraisal dimensions in a computer game",
      "authors": [
        "C Van Reekum",
        "T Johnstone",
        "R Banse",
        "A Etter",
        "T Wehrle",
        "K Scherer"
      ],
      "year": "2004",
      "venue": "Cognition and Emotion"
    },
    {
      "citation_id": "34",
      "title": "Dreamer: A database for emotion recognition through eeg and ecg signals from wireless lowcost off-the-shelf devices",
      "authors": [
        "S Katsigiannis",
        "N Ramzan"
      ],
      "year": "2018",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "35",
      "title": "Ascertain: Emotion and personality recognition using commercial sensors",
      "authors": [
        "R Subramanian",
        "J Wache",
        "M Abadi",
        "R Vieriu",
        "S Winkler",
        "N Sebe"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "36",
      "title": "Autonomic nervous system activity in emotion: A review",
      "authors": [
        "S Kreibig"
      ],
      "year": "2010",
      "venue": "Biological Psychology"
    },
    {
      "citation_id": "37",
      "title": "Wearablebased affect recognition-a review",
      "authors": [
        "P Schmidt",
        "A Reiss",
        "K Laerhoven"
      ],
      "year": "2019",
      "venue": "Sensors"
    },
    {
      "citation_id": "38",
      "title": "Emotion feature analysis and recognition based on reconstructed eeg sources",
      "authors": [
        "G Chen",
        "X Zhang",
        "Y Sun",
        "J Zhang"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "39",
      "title": "Deep neural classifiers for eeg-based emotion recognition in immersive environments",
      "authors": [
        "J Teo",
        "J Chia"
      ],
      "year": "2018",
      "venue": "2018 International Conference on Smart Computing and Electronic Enterprise (ICSCEE)"
    },
    {
      "citation_id": "40",
      "title": "Towards valence detection from emg for virtual reality applications",
      "authors": [
        "I Mavridou",
        "E Seiss",
        "M Hamedi",
        "E Balaguer-Ballester",
        "C Nduka"
      ],
      "year": "2018",
      "venue": "12th International Conference on Disability, Virtual Reality and Associated Technologies (ICDVRAT 2018)"
    },
    {
      "citation_id": "41",
      "title": "Spontaneous and posed smile recognition based on spatial and temporal patterns of facial emg",
      "authors": [
        "M Perusquía-Hernández",
        "M Hirokawa",
        "K Suzuki"
      ],
      "year": "2017",
      "venue": "2017 Seventh International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "42",
      "title": "Electromyographic activity over facial muscle regions can differentiate the valence and intensity of affective reactions",
      "authors": [
        "J Cacioppo",
        "R Petty",
        "M Losch",
        "H Kim"
      ],
      "year": "1986",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "43",
      "title": "Unconscious facial reactions to emotional facial expressions",
      "authors": [
        "U Dimberg",
        "M Thunberg",
        "K Elmehed"
      ],
      "year": "2000",
      "venue": "Unconscious facial reactions to emotional facial expressions"
    },
    {
      "citation_id": "44",
      "title": "Emotional valence sensing using a wearable facial emg device",
      "authors": [
        "W Sato",
        "K Murata",
        "Y Uraoka",
        "K Shibata",
        "S Yoshikawa",
        "M Furuta"
      ],
      "year": "2021",
      "venue": "Scientific Reports"
    },
    {
      "citation_id": "45",
      "title": "Physiohmd: A conformable, modular toolkit for collecting physiological data from head-mounted displays",
      "authors": [
        "G Bernal",
        "T Yang",
        "A Jain",
        "P Maes"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 ACM International Symposium on Wearable Computers"
    },
    {
      "citation_id": "46",
      "title": "A facial emg data analysis for emotion classification based on spectral kurtogram and cnn",
      "authors": [
        "V Kehri",
        "R Awale"
      ],
      "year": "2020",
      "venue": "International Journal of Digital Signals and Systems, Smart"
    },
    {
      "citation_id": "47",
      "title": "Emotional state detection based on emg and eog biosignals: A short survey",
      "authors": [
        "J Perdiz",
        "G Pires",
        "U Nunes"
      ],
      "venue": "2017 IEEE 5th Portuguese Meeting on Bioengineering (ENBENG)"
    },
    {
      "citation_id": "48",
      "title": "Facial expressions in eeg/emg recordings",
      "authors": [
        "L Boot"
      ],
      "year": "2009",
      "venue": "Facial expressions in eeg/emg recordings"
    },
    {
      "citation_id": "49",
      "title": "What's in a smile? neural correlates of facial embodiment during social interaction",
      "authors": [
        "L Schilbach",
        "S Eickhoff",
        "A Mojzisch",
        "K Vogeley"
      ],
      "year": "2008",
      "venue": "Social Neuroscience"
    },
    {
      "citation_id": "50",
      "title": "A survey on affective and cognitive vr",
      "authors": [
        "T Luong",
        "A Lecuyer",
        "N Martin",
        "F Argelaguet"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Visualization and Graphics"
    },
    {
      "citation_id": "51",
      "title": "A vr game-based system for multimodal emotion data collection",
      "authors": [
        "C Bassano",
        "G Ballestin",
        "E Ceccaldi",
        "F Larradet",
        "M Mancini",
        "E Volta",
        "R Niewiadomski"
      ],
      "year": "2019",
      "venue": "A vr game-based system for multimodal emotion data collection"
    },
    {
      "citation_id": "52",
      "title": "Computational analysis of valence and arousal in virtual reality gaming using lower arm electromyograms",
      "authors": [
        "I Shumailov",
        "H Gunes"
      ],
      "year": "2017",
      "venue": "2017 Seventh International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "53",
      "title": "Geneva Emotion Wheel Rating Study",
      "authors": [
        "V Shuman",
        "K Schlegel",
        "K Scherer"
      ],
      "year": "2015",
      "venue": "Geneva Emotion Wheel Rating Study"
    },
    {
      "citation_id": "54",
      "title": "Physiological correlates of subjective emotional valence and arousal dynamics while viewing films",
      "authors": [
        "W Sato",
        "T Kochiyama",
        "S Yoshikawa"
      ],
      "year": "2020",
      "venue": "Biological Psychology"
    },
    {
      "citation_id": "55",
      "title": "emteqpro-fully integrated biometric sensing array for non-invasive biomedical research in virtual reality",
      "authors": [
        "M Gnacek",
        "J Broulidakis",
        "I Mavridou",
        "M Fatoorechi",
        "E Seiss",
        "T Kostoulas",
        "E Balaguer-Ballester",
        "I Kiprijanovska",
        "C Rosten",
        "C Nduka"
      ],
      "year": "2022",
      "venue": "emteqpro-fully integrated biometric sensing array for non-invasive biomedical research in virtual reality"
    },
    {
      "citation_id": "56",
      "title": "Stressfoot: Uncovering the potential of the foot for acute stress sensing in sitting posture",
      "authors": [
        "D Elvitigala",
        "D Matthies",
        "S Nanayakkara"
      ],
      "year": "2020",
      "venue": "Stressfoot: Uncovering the potential of the foot for acute stress sensing in sitting posture"
    },
    {
      "citation_id": "57",
      "title": "Mssq-short norms may underestimate highly susceptible individuals",
      "authors": [
        "S Lamb",
        "K Kwok"
      ],
      "year": "2014",
      "venue": "Human Factors: The Journal of the Human Factors and Ergonomics Society"
    },
    {
      "citation_id": "58",
      "title": "Psychopy-psychophysics software in python",
      "authors": [
        "J Peirce"
      ],
      "year": "2007",
      "venue": "Journal of neuroscience methods"
    },
    {
      "citation_id": "59",
      "title": "Mastering blender",
      "authors": [
        "T Mullen"
      ],
      "year": "2011",
      "venue": "Mastering blender"
    },
    {
      "citation_id": "60",
      "title": "",
      "authors": [
        "Online"
      ],
      "venue": ""
    },
    {
      "citation_id": "61",
      "title": "Basic emotions, natural kinds, emotion schemas, and a new paradigm",
      "authors": [
        "C Izard"
      ],
      "year": "2007",
      "venue": "Basic emotions, natural kinds, emotion schemas, and a new paradigm"
    },
    {
      "citation_id": "62",
      "title": "Emotion theory and research: Highlights, unanswered questions, and emerging issues",
      "year": "2009",
      "venue": "Annual review of psychology"
    },
    {
      "citation_id": "63",
      "title": "Towards understanding emotional experience in a componential framework",
      "authors": [
        "G Mohammadi",
        "K Lin",
        "P Vuilleumier"
      ],
      "year": "2019",
      "venue": "2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "64",
      "title": "A cross-cultural study of a circumplex model of affect",
      "authors": [
        "J Russell",
        "M Lewicka",
        "T Niit"
      ],
      "year": "1989",
      "venue": "Journal of personality and psychology, social"
    },
    {
      "citation_id": "65",
      "title": "Assessing the effectiveness of a large database of emotion-eliciting films: A new tool for emotion researchers",
      "authors": [
        "X Schaefer",
        "Frédéric Nils"
      ],
      "year": "2010",
      "venue": "Cognition & Emotion"
    },
    {
      "citation_id": "66",
      "title": "The world of emotions is not two-dimensional",
      "authors": [
        "J Fontaine",
        "K Scherer",
        "E Roesch",
        "P Ellsworth"
      ],
      "year": "2007",
      "venue": "Psychol Sci"
    },
    {
      "citation_id": "67",
      "title": "The GRID meets the Wheel: Assessing emotional feeling via self-report1",
      "authors": [
        "K Scherer",
        "V Shuman",
        "J Fontaine",
        "C Soriano"
      ],
      "year": "2013",
      "venue": "The GRID meets the Wheel: Assessing emotional feeling via self-report1"
    },
    {
      "citation_id": "68",
      "title": "Brain networks subserving functional core processes of emotions identified with componential modeling",
      "authors": [
        "G Mohammadi",
        "D Van De Ville",
        "P Vuilleumier"
      ],
      "year": "2023",
      "venue": "Cerebral Cortex"
    },
    {
      "citation_id": "69",
      "title": "Dimensions and clusters of aesthetic emotions: A semantic profile analysis",
      "authors": [
        "U Beermann",
        "G Hosoya",
        "I Schindler",
        "K Scherer",
        "M Eid",
        "V Wagner",
        "W Menninghaus"
      ],
      "year": "1949",
      "venue": "Dimensions and clusters of aesthetic emotions: A semantic profile analysis"
    },
    {
      "citation_id": "70",
      "title": "Subjectively salient dimensions of emotional appraisal",
      "authors": [
        "R Reisenzein",
        "C Spielhofer"
      ],
      "year": "1994",
      "venue": "Motivation and Emotion"
    },
    {
      "citation_id": "71",
      "title": "Reading emotions from and into faces: Resurrecting a dimensional-contextual perspective, ser. Studies in emotion and social interaction, 2nd series",
      "authors": [
        "J Russell"
      ],
      "year": "1997",
      "venue": "Editions de la Maison des Sciences de l'Homme"
    },
    {
      "citation_id": "72",
      "title": "Facial emg sensing for monitoring affect using a wearable device",
      "authors": [
        "M Gjoreski",
        "I Kiprijanovska",
        "S Stankoski",
        "I Mavridou",
        "M Broulidakis",
        "H Gjoreski",
        "C Nduka"
      ],
      "year": "2022",
      "venue": "Scientific reports"
    },
    {
      "citation_id": "73",
      "title": "Facial action coding system",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1978",
      "venue": "Environmental Psychology & Nonverbal Behavior"
    },
    {
      "citation_id": "74",
      "title": "Affect elicitation for affective computing",
      "authors": [
        "J Kory",
        "S D'mello"
      ],
      "venue": "Conference Proceedings"
    },
    {
      "citation_id": "75",
      "title": "An empirical study of players' emotions in vr racing games based on a dataset of physiological data",
      "authors": [
        "M Granato",
        "D Gadia",
        "D Maggiorini",
        "L Ripamonti"
      ],
      "year": "2020",
      "venue": "An empirical study of players' emotions in vr racing games based on a dataset of physiological data"
    },
    {
      "citation_id": "76",
      "title": "Human amygdala tracks a feature-based valence signal embedded within the facial expression of surprise",
      "authors": [
        "M Kim",
        "A Mattek",
        "R Bennett",
        "K Solomon",
        "J Shin",
        "P Whalen"
      ],
      "year": "2017",
      "venue": "Journal of Neuroscience"
    },
    {
      "citation_id": "77",
      "title": "Behavioral markers and recognizability of the smile of enjoyment",
      "authors": [
        "M Frank",
        "P Ekman",
        "W Friesen"
      ],
      "year": "1993",
      "venue": "Journal of personality and psychology, social"
    },
    {
      "citation_id": "78",
      "title": "Authentic and posed emotional vocalizations trigger distinct facial responses",
      "authors": [
        "C Lima",
        "P Arriaga",
        "A Anikin",
        "A Pires",
        "S Frade",
        "L Neves",
        "S Scott"
      ],
      "year": "2021",
      "venue": "Cortex"
    },
    {
      "citation_id": "79",
      "title": "Emotionally assisted human-robot interaction using a wearable device for reading facial expressions",
      "authors": [
        "A Gruebler",
        "V Berenz",
        "K Suzuki"
      ],
      "year": "2012",
      "venue": "Advanced Robotics"
    },
    {
      "citation_id": "80",
      "title": "A wearable high-resolution facial electromyography for long term recordings in freely behaving humans",
      "authors": [
        "L Inzelberg",
        "D Rand",
        "S Steinberg",
        "M David-Pur",
        "Y Hanein"
      ],
      "year": "2018",
      "venue": "Scientific reports"
    },
    {
      "citation_id": "81",
      "title": "Comparative study of physiological signals from empatica e4 wristband for stress classification",
      "authors": [
        "V Chandra",
        "A Priyarup",
        "D Sethia"
      ],
      "venue": "International Conference on Advances in Computing and Data Sciences"
    },
    {
      "citation_id": "82",
      "title": "Smoothing and differentiation of data by simplified least squares procedures",
      "authors": [
        "A Savitzky",
        "M Golay"
      ],
      "year": "1964",
      "venue": "Analytical Chemistry"
    },
    {
      "citation_id": "83",
      "title": "Discrimination of simultaneous psychological and physical stressors using wristband biosignals",
      "authors": [
        "M Sevil",
        "M Rashid",
        "I Hajizadeh",
        "M Askari",
        "N Hobbs",
        "R Brandt",
        "M Park",
        "L Quinn",
        "A Cinar"
      ],
      "year": "2021",
      "venue": "Computer Methods and Biomedicine, Programs in"
    },
    {
      "citation_id": "84",
      "title": "Smote: synthetic minority over-sampling technique",
      "authors": [
        "N Chawla",
        "K Bowyer",
        "L Hall",
        "W Kegelmeyer"
      ],
      "year": "2002",
      "venue": "Journal of artificial intelligence research"
    },
    {
      "citation_id": "85",
      "title": "Profiles of appraisal, motivation, and coping for positive emotions",
      "authors": [
        "J Yih",
        "L Kirby",
        "C Smith"
      ],
      "year": "2020",
      "venue": "Cognition and Emotion"
    }
  ]
}