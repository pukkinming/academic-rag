{
  "paper_id": "2404.00403v2",
  "title": "Unimeec: Towards Unified Multimodal Emotion Recognition And Emotion Cause",
  "published": "2024-03-30T15:59:17Z",
  "authors": [
    "Guimin Hu",
    "Zhihong Zhu",
    "Daniel Hershcovich",
    "Lijie Hu",
    "Hasti Seifi",
    "Jiayuan Xie"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal emotion recognition in conversation (MERC) and multimodal emotion-cause pair extraction (MECPE) have recently garnered significant attention. Emotions are the expression of affect or feelings; responses to specific events, or situations -known as emotion causes. Both collectively explain the causality between human emotion and intents. However, existing works treat emotion recognition and emotion cause extraction as two individual problems, ignoring their natural causality. In this paper, we propose a Unified Multimodal Emotion recognition and Emotion-Cause analysis framework (UniMEEC) to explore the causality between emotion and emotion cause. Concretely, UniMEEC reformulates the MERC and MECPE tasks as mask prediction problems and unifies them with a causal prompt template. To differentiate the modal effects, UniMEEC proposes a multimodal causal prompt to probe the pre-trained knowledge specified to modality and implements crosstask and cross-modality interactions under taskoriented settings. Experiment results on four public benchmark datasets verify the model performance on MERC and MECPE tasks and achieve consistent improvements compared with the previous state-of-the-art methods.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Recently, multimodal emotion recognition in conversations (MERC) and multimodal emotion-cause pair extraction (MECPE) have attracted increasing attention  (Zhang et al., 2021a,b; Hu et al., 2021a,b) . Both task play crucial roles in dialog systems, especially in empathetic response generation in a conversation  (Fu et al., 2023; Qian et al., 2023; Tian et al., 2022; Hu et al., 2024) . MERC detects the emotion category of each utterance in a conversation, while MECPE finds the reasons that trigger a certain emotion for the utterance. Both tasks are tightly related in practice and theory (Baumeister Figure  1 : Illustration of the causal inference between emotion and emotion cause, which unifies MECPE and MERC tasks. \"response\" denotes the speaker's reaction to the event and \"event\" denotes the event that triggers emotion.  and Cooper, 1981; Dirven, 1997; Russell, 1990; Lee et al., 2019) . However, the existing works treat MERC and MECPE as two separate tasks and ignore their causality. On the one hand, emotions are responses to emotion causes (e.g., specific events)  (Marks, 1982; Cabanac, 2002) . On the other hand, emotion and its emotion causes are interdependent and mutually influential  (Russell, 1990; Lee et al., 2019) . The two serve as reflections for each other and together provide a causal story of human behavior and intents. Figure  1  illustrates the causal alignment between emotion category and emotion cause  (Baumeister and Cooper, 1981; Dirven, 1997) .\n\nFor example, the emotion causes of \"happiness\" generally are positive events, such as \"being praised\". Similarly, the emotion causes of \"sad\" generally are negative events, such as \"being criticized\". We view the mapping between the specific events (e.g., emotion cause) and response (e.g., emotion label) as the emotion-cause causality. From the causal perspective,  Lyu et al. (2024)  proposes the idea of causal prompts, which are prompts that describe the causal story behind the sentiment rating and reviews, further demonstrating that Pretrained Language Model (PLM) is able to be aware of the underlying causality. A natural question arises: How should we perform causality between emotions and their causes in a unified architecture?\n\nRecently, the unification of related but different tasks into a framework has achieved significant progress  (Chen et al., 2022; Xie et al., 2022; Zhang et al., 2022) . For example, UniMSE  (Hu et al., 2022b)  unifies emotion and sentiment into a single architecture to share complementary knowledge between them. Different from UniMSE which focuses on the unification of emotion and sentiment in a generative way, we propose a multimodal causal prompt to unify MERC and MECPE tasks, thereby capturing the causal nature between emotion and emotion cause. In this paper, we propose a Unified Multimodal Emotion recognition and Emotion-Cause pair extraction framework (UniMEEC) to explore the causality between emotion and emotion cause. As  Lyu et al. (2024)  illustrated, PLM can capture the causal stories with the causal prompts. Starting from this perspective, UniMEEC reformulates MERC and MECPE as two mask prediction tasks and unifies the two tasks using a causal prompt, aiming to capture the understanding of PLM to emotion-cause causlity. In order to differentiate the modal effects, UniMEEC probes modal features from PLM using the multimodal causal prompt, and meanwhile, UniMEEC captures the emotion-specific, cause-specific, and utterance-specific contexts in a hierarchical way. The main contributions are summarized as follows:\n\n• We propose a Unified Multimodal Emotion recognition and Emotion Cause pair extraction framework (UniMEEC) 1 , which uses the causal prompt to unify the MERC and MECPE tasks for causal relation between emotion and emotion cause.\n\n• UniMEEC formalizes MERC and MEEC tasks into mask prediction problems and constructs the multimodal causal prompt to probe the knowledge from PLM. Meanwhile, UniMEEC proposes task-specific context aggregation to orderly capture the contexts oriented to specific tasks.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Multimodal Emotion Recognition in Conversations (MERC) We categorize the works of MERC into three main groups: multimodal fusion, context-aware models, and external-knowledge models. The first group focuses on the fusion representation in which some works  (Hu et al., 2022a (Hu et al., , 2021c;; Joshi et al., 2022)  employed the graph neural networks to model the inter/intra dependencies of utterances information, and some works proposed cross-attention Transformer  (Vaswani et al., 2017)     (Hazarika et al., 2019; Lee and Lee, 2021) , commonsense knowledge  (Ghosal et al., 2020) , multitask learning  (Akhtar et al., 2019) , and external information  (Zhu et al., 2021)  to introduce more auxiliary information to help model understand conversation.\n\nMultimodal Emotion-Cause Pair Extraction (MECPE) As more and more NLP tasks extend to the multimodal paradigm  (Zhu et al., 2024; Li et al., 2024; ?) ,  Wang et al. (2021)  defined multimodal emotion-cause pair extraction (MECPE) and constructed Emotion-Cause-in-Friends (ECF) dataset based on MELD  (Poria et al., 2019) .  Li et al. (2022a)  built an English conversational emotioncause pair extraction multimodal dataset based on IEMOCAP  (Busso et al., 2008) . With MECPE only emerging for a relatively short time, there are a few baseline methods in this field. Previous studie  (Wang et al., 2021; Li et al., 2022a)  integrated multimodal features to tackle the MECPE task based on the baselines of ECPE  (Xia and Ding, 2019) , overlooking the importance of interutterance context and multimodal fusion in understanding emotion cause.\n\nPrompt-tuning Prompt-tuning  (Li and Liang, 2021; Liu et al., 2021; Su et al., 2021) , inspired  by  GPT-3 (Ding et al., 2023) , is a new paradigm to fine-tuning, particularly geared towards addressing few-shot scenarios. Recently, prompt-tuning has been widely used in addressing NLP tasks and achieved remarkable performances  (Zheng et al., 2022; Li et al., 2021a; Yang et al., 2023; Su et al., 2021; Sun et al., 2022) . The initial input X undergoes modification through a template to form a textual string prompt X ′ with unfilled slots. Subsequently, the language model is employed to probabilistically fill in the missing information, resulting in a final string X from which the model outputs y  (Liu et al., 2023) . The prompt template contains manual template engineering and automated template learning  (Liu et al., 2023) . The manual template is to manually create intuitive templates and the auto-prompt template  (Li and Liang, 2021; Liu et al., 2021; Su et al., 2021)  includes discrete prompts, represented by actual text strings, and continuous prompts, described directly within the embedding space of the underlying language model. In this work, UniMEEC constructs causal prompts to unify MERC and MECPE, where causal prompt connects emotion and corresponding emotion cause to ensure the causal coherence.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Task Formalization",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Given A Multi-Turn Conversation",
      "text": "i , I a i , I v i } contains three modalities, where I m i , m ∈ {t, a, v} represent uni-modal feature extracted from video fragment i, and {t, a, v} denote the three types of modalities-text, acoustic and visual, respectively. Multimodal emotion recognition (MERC) predicts the emotion category of u i , and multimodal emotion-cause pair extraction (MECPE) aims to predict the corresponding cause utterance ID (e.g., \"u 1 \", \"u 2 \") for non-neutral utterance u i . To unify MERC and MECPE, we formalize MERC and MECPE as two mask prediction problems in the causal prompt and leverage the language model to probabilistically fill the unfilled slots, thereby predicting the results of MERC and MECPE tasks respectively.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Multimodal Causal Prompt (Mcp)",
      "text": "In order to differentiate the modal effects, we set causal prompt for each modality to probe the modality-specific features from PLM. Multimodal causal prompts share auxiliary prompt tokens in the prompt template, which enables inter-modality and inter-task semantic interaction in representation learning.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Causal Prompt Construction",
      "text": "We manually design the modality-specific prompt template, and it consists of a modal input [X], the emotion category slot [M] 1 , the cause slot [M] 2 and auxiliary prompt part, where [X] is the slot filled with modal feature of target utterance, [M] 1 indicates the emotion category of target utterance, e.g., \"happy\" or \"sad\", and [M] 2 indicates the cause utterance ID of target utterance, e.g., \"u 1 \", \"u 2 \".\n\n[M] 1 and  [M]  2 are unfilled answer slots and are separately predicted as the results of MERC and MECPE. Given text modality I t i , i ∈ {1, • • • , |U |}, we designed the causal prompt template like \"the emotion of utterance I t i is [M] 1 , and its emotion cause is [M] 2 \" as text-specific prompt, where the textual strings \"For conversation\", \"the emotion category of\", \"is\", and \"the reason for this emotion is\" are auxiliary prompt parts. For audio-specific and vision-specific prompts, we replace the [X] part of the prompt with the acoustic and visual representations to construct audio-specific and vision-specific prompts, respectively.\n\nWe use X i,m , X i,m ∈ R lm×dm to represent the modal representation after modal alignment  (Tsai et al., 2019) , l m and d m are the sequence length and the representation dimension of modality m, respectively. Specifically, we obtain X i,t with the word embedding layer of the model and we processed raw acoustic input into numerical sequential vectors by librosa 2  to extract Mel-spectrogram as X i,a . For vision modality, we use effecientNet (Tan and Le, 2019) pre-trained (supervised) on VGGface 3  and AFEW dataset to extract X i,v .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Causal Prompt Encoder",
      "text": "We take Transformer-based model (e.g., BERT  (Devlin et al., 2019) ) as the backbone of the multimodal causal prompt. The stacked Transformer contains multiple Transformer layers, and each layer contains a self-attention module, FFN, and layer normalization  (Ba et al., 2016) . We take the former N t Transformer layers as the text-specific prompt encoder and take the latter N a and N v Transformer layers as the visual-and acoustic prompt encoders, respectively. First, text-specific prompt is fed into the text-specific prompt encoder to get the text-specific representations of [X], auxiliary prompt part, and [M] 1 and [M] 2 , with the supervision of real ground answers of slots. After that, we obtain the text-specific prompt sequence, which contains the hidden states of h P 1,l 1 , X i,t , h P l 2 ,l 3 , h [M] 1 , h P l 4 ,l 5 and h [M] 2 , where h (•) denotes the representation of token or token sequence, h P 1,l 1 , h P l 2 ,l 3 and h P l 4 ,l 5 denote the representations of auxiliary prompt parts.\n\nDue to the dimensions and sequence lengths of audio and vision modalities being less than the dimensions and sequence length of text modality, we pad the audio and vision feature with zero to achieve consistency with the representation of text modality. We take Xi,a and Xi,v to represent audio and vision representations after padding, respectively. For audio-specific prompt, we replace [X] part of the prompt representation with Xi,a . For vision-specific prompt, we replace [X] part of the prompt representation with Xi,v after N t Transformer layers. After that, we feed audiospecific and vision-specific prompts into N a and N v Transformer layers respectively. For (n-1)-th Transformer layer, the modality-specific prompt learning is given by:\n\nwhere P n-1 i,m denotes the prompt representation of utterance u i under the modality m. Specifically, P n-1 i,m is composed by the hidden states of [X], [M] 1 [M] 2 , and auxiliary prompt strings. X 0 i,t = X i,t , X 0 i,a = Xi,a , and\n\nAfter the multimodal causal prompt, we obtain the modal fusion representations of mask tokens [M] 1 and [M] 2 via concatenation, respectively. Similarly, we obtain the fusion representation of u i via the concatenation of X Nt i,t , X Na i,a and X Nv i,v :\n\nwhere X Nt i,t , X Na i,a and X Nv i,v are text, audio and video representations of u i encoded by N t , N a and N v Transformer layers respectively.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Task-Specific Hierarchical Context (Thc)",
      "text": "The learned representations of [M] 1 (i.e., h f\n\n[M] 1 ) and [M] 2 (i.e., h f\n\n[M] 2 ) fail to capture the context information in a conversation, which inspires us to build a hierarchical context aggregation structure to control the direction of context aggregation in a conversation. In order to avoid the noise information in representation learning, we set the context windows for each utterance to incorporate the information around target utterance.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Hierarchical Graph Construction",
      "text": "We construct a 3-level graph attention network (GAT)  (Velickovic et al., 2018)  as the encoder of contexts, which includes top, middle, and bottom levels. Each level has a context window to focus on the local context of utterance. Formally, we define a graph G = (V, E), V and E denote the node and edge sets respectively. We take the utterancelevel representation h u as the bottom node, causespecific token representation h f\n\n[M] 2 as the middle node, and the emotion-specific token representation h f\n\n[M] 1 as the top node. For the intra-level nodes, we set undirected edges for any two adjacent nodes in the context window of the same level. For the interlevel nodes, we set the undirected edges between the top nodes and middle nodes. In general, we set the directed edges from the bottom to the middle nodes in the context window, aiming to control the direction of the information flow among nodes.\n\nConsidering that graph G contains multiple type node representations, we set five edge types respectively to model the dependency relations among different nodes. The former three edges are constructed between the slot nodes to slot nodes, i.e.,\n\nwhich are represented with t ee , t ec and t cc respectively. The fourth edge type is constructed from utterance node to slot node, i.e., h u ↔ h [M] 2 , represented by t uc . The last is from utterance node to utterance node, i.e., h u ↔ h u , denoted by t uu . The subscripts \"e\" and \"c\" in edge type represent [M] 1 and [M] 2 , respectively, and \"u\" represents the utterance. For one edge type t ∈ {t ee , t ec , t cc , t uc , t uu }, its adjacent matrix is given as:\n\nwhere a t i,j ∈ A, A ∈ R V * V . V denotes the number of utterances in a conversation. |w| denotes the size of the context window. i and j represent the indexes of utterances in a conversation, and they are located on the same or adjacent levels of THC.",
      "page_start": 1,
      "page_end": 5
    },
    {
      "section_name": "Task-Specific Context Aggregation",
      "text": "We set a contextual window for each node at each level to ensure that the model only aggregates the node representations in its contextual window. This operation reduces the computational cost and avoids introducing noise to the representation learning. Given an utterance u i , the prediction slots of emotion and emotion cause are [M] i,1 and [M] i,2 respectively. We aggregate the representation from the bottom to top levels in the graph, and the representations of bottom nodes are not updated by aggregating the representations of the top or middle nodes to them. For the bottom node u i , its representation is aggregated by the bottom nodes in the context window:\n\nwhere N u i denotes the neighbor nodes of utterance u i and h 0 u j = h f u j . When the model comes to the middle node [M] i,2 , the representations is aggregated by the top and middle nodes in the context window, which is given by:\n\nwhere\n\n. When the model comes to the top node [M] i,1 , its representation is aggregated by the top, and the middle nodes in the context window, which is given by:\n\nWe stacked N task-specific context aggregation modules and then use h",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Grounding Mask Predictions To Merc And Mecpe",
      "text": "We use h N [M] i,1 to predict MERC task, i.e., the answers of slot [M] 1 , and use h N\n\n[M] i,2 to predict MECPE task, i.e., the answers of slot [M] 2 . The predictions of [M] 1 (i.e., ŷe i ) and [M] 1 (i.e., ŷc i ) are given as respectively:\n\nwhere {ŷ e i , ŷc i } denote the prediction results for MERC and MECPE tasks, respectively. Based on the predictions, we use the sum of the cross-entropy losses of MERC and MECPE tasks as the objective loss of UniMEEC.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Datasets",
      "text": "We conduct experiments on four publicly available benchmark datasets of MERC and MECPE. For MERC task, its benchmark datasets include multimodal emotionLines dataset (MELD)  (Poria et al., 2019) , interactive emotional dyadic motion capture database (IEMOCAP)  (Busso et al., 2008) . IEMOCAP consists of 7532 samples, and each sample is labeled with six emotions for emotion recognition, including happiness, sadness, anger, neutral, excitement, and frustration. MELD contains 13,707 video clips of multi-party conversations, with labels following Ekman's six universal emotions, including joy, sadness, fear, angry, surprise and disgust. For MECPE task, its benchmark datasets include ConvECPE  (Li et al., 2022a) , and emotion-cause-in-friends (ECF)  (Wang et al., 2021) . ConvECPE is a multimodal emotion cause dataset constructed based on IEMOCAP, in which each non-neutral utterance is labeled with the emotion cause. It contains 151 dialogues with 7,433 utterances. Similarly,  (Wang et al., 2021)  annotated the emotion cause of each sample in MELD and then constructed multimodal emotion cause dataset ECF. ECF contains 1,344 conversations and 13,509 utterances. The detailed statistics of four datasets are shown in Table  1 . For datasets IEMOCAP and MELD, we follow previous works  (Li et al., 2021c; Lu et al., 2020) , and we use accuracy (ACC) and weighted F1 (WF1) as the evaluation metric for the MERC task. For datasets ECF and ConvECPE, we use precision (P), recall (R), and F1 as the evaluation metric for the MECPE task.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Baselines",
      "text": "For MERC, the baselines can be grouped into three categories: 1)the methods focusing on emotion cues like EmoCaps  (Li et al., 2022b) , FacialMMT-RoBERTa  (Zheng et al., 2023) , MVN  (Li et al., 2021c) . These works aim to improve model performance by tracking emotional states in a conversation, and 2)the methods fusing multimodal information like QMNN  (Li et al., 2021c) , GA2MIF  (Li et al., 2023) ,MALN  (Ren et al., 2023) , Multi-EMO  (Shi and Huang, 2023) , and UniMSE  (Hu et al., 2022b) . These works focus on better multimodal fusion, and 3)the methods incorporating context information like DialogueGCN  (Ghosal et al., 2019) , MMGCN  (Hu et al., 2021c) , MM-DFN  (Hu et al., 2022a) , BC-LSTM  (Poria et al., 2017) , DialogueRNN  (Majumder et al., 2019)  and Itera-tiveERC  (Lu et al., 2020) . These works aggregate the context to understand the whole conversation.\n\nMECPE has a few baselines due to MECPE only emerging for a relatively short time. Most baselines address MECPE tasks based on twostep frameworks of emotion-cause pair extraction in text, like Joint-GCN  (Li et al., 2022a) , Joint-Xatt  (Li et al., 2022a)  and Inter-EC  (Li et al., 2022a) . C Multi-Bernoulli  (Wang et al., 2021)  carries out a binary decision for each relative position to determine the cause utterance. C Multinomial  (Wang et al., 2021)  randomly selects a relative position from all relative positions as the feature to extract emotion-cause pair. We produce some typical multimodal methods based on their open source codes, including MuLT  (Tsai et al., 2019) , MMGCN  (Hu et al., 2021c) , MMDFN  (Hu et al., 2022a) , UniMSE  (Hu et al., 2022b)  and GA2MIF  (Li et al., 2023) .",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Experimental Settings",
      "text": "We use pre-trained BERT as the encoder of multimodal causal prompt. spectively, so we integrate the emotion and cause labels of IEMOCAP, MELD, ConvECPE and ECF to train the model. The batch size is 64, the learning rate for BERT fine-tuning is set at 3e-4, and the learning rate for UniMEEC is set to 0.0001. The hidden dimension of acoustic and visual representation is 64, the BERT embedding size is 768, and the fusion vector size is 768. We use the former 9 Transformer layers of BERT as the text-specific prompt encoder, the following 10th and 11th as the audio-specific prompt encoder, and the last Transformer layer of BERT as the video-specific prompt encoder. The THC module stacks two graph network layers, where the first layer has one attention head and the second layer has four attention heads.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Experimental Environment",
      "text": "All experiments are conducted in the NVIDIA RTX A100. We take BERT as the Transformer-based model, which has 110M parameters, including 12 layers, 768 hidden dimensions, and 12 heads. We use the former N t = 9 Transformer layers as the text-specific encoder, use the following N a = 2 and N v = 1 Transformer layers as the audiospecific and video-specific encoders respectively. The value of N t , N a and N v are determined by the model performance on valid test. Furthermore, we employ a linear decay learning rate schedule with a warm-up strategy.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Results Of Emotion Recognition",
      "text": "We compare UniMEEC with the baselines of MERC on IEMOCAP and MELD datasets, and the comparative results are shown in Table  2 . UniMEEC significantly outperforms SOTA in all metrics on IEMOCAP, and MELD, and improves WF1 scores of IEMOCAP and MELD by 1.99% and 2.06%, respectively.\n\nRecent methods like MultiEMO, MALN, and GA2MF achieve low performance in recognizing the label \"Happiness\" for the IEMOCAP dataset and recognizing the label \"Fear\" for the MELD dataset. The low performance is caused by the label imbalance of the benchmark. UniMEEC significantly improves the emotion recognition performance on most emotion categories for two datasets. On the one hand, the unified framework offers model auxiliary information, enhancing the interaction between emotion and emotion cause, thereby alleviating the label imbalance of the benchmark. On the other hand, UniMEEC unifies the annotated labels of MERC and MECPE tasks with a causal prompt, which probes the causal story between response (emotion) and event (emotion cause). In summary, UniMEEC consistently surpasses the state-of-the-art (SOTA) in most emotion category recognition on both datasets. These results indicate the superiority of UniMEEC to MERC and MECPE and illustrate the unified framework of modeling emotion-cause causality brings improvements to emotion recognition. Furthermore, we explore the impact of different PLMs, i.e., BART  (Lewis et al., 2020) , T5  (Raffel et al., 2020)  and LLaMa  (Touvron et al., 2023)  on UniMEEC performance. We report the result on IEMOCAP and MELD datasets when we take BART, T5 and LLaMA as the PLM of UniMEEC. The experimental results are shown in Table  3 .",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Methods",
      "text": "",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Cause Recognition",
      "text": "Pair Extraction P R F1 P R F1 WF1\n\nE True + C Multi-Bernoulli  (Wang et al., 2021) 55.69 57.20 55.47 49.40 25.22 33. 39 -E True +C Multinomial  (Wang et al., 2021)  57.  21 56.38 56.85 49.33 25.18  33.34 -MC-ECPE-2steps  (Wang et al., 2021)  57.  76 56.71 57.09 49.43 53.76 51.32 30 .00 MuLT*  (Tsai et al., 2019)  55.  19 53.43 54.79 30.48 37.85  39.02 -MMGCN*  (Hu et al., 2021c) 56.51 54.82 55.30 35.43 38.19 37.48  54.65 MM-DFN*  (Hu et al., 2022a)  54.  28 56.35 55.17 37.90      4  and Table 5 ), outperforming at least 5.34% and 2.12% improvements by the competitive baselines on ECF and ConvECPE, respectively. We summarize the improvements into two aspects: 1) UniMEEC achieves SOTA on emotion recognition, cause recognition, and emotion-cause pair extraction on the benchmarks of MERC and MECPE, and 2)UniMEEC significantly outperforms SOTA in most cases. The improvements illustrate jointly training emotion and emotion cause can benefit the two tasks, and the unified framework in modeling causality between emotion and emotion cause can bring prior knowledge to MERC and MECPE training.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Ablation Study",
      "text": "We conducted extensive ablation studies on IEMO-CAP and MELD datasets and experimental results are shown in Table  6 . First, we remove the MECPE part in the prompt template, and then train UniMEEC just using the emotion label as the supervision signal. The removal of MECPE from UniMEEC results in a performance drop by 3.57% and 1.96% on IEMOCAP and MELD respectively, demonstrating that jointly training MERC and MECPE can bring improvements for MERC tasks.\n\nThen we remove one or two modalities from MCP by replacing MCP with unimodal and bimodal prompt templates, where unimodal and bimodal prompt templates denote the prompt template containing one and two modalities, respectively. We feed the unimodal and bimodal prompts into PLM and their performances significantly decline on two datasets. We can find that removing acoustic, visual, and textual modalities or one of them all leads to performance degradation, further demonstrating the effectiveness and necessity of multimodal prompt learning to model performance. For example, we eliminate acoustic, visual, and both modalities from the multimodal prompt template, resulting in performance degradation by 2.75%, 1.96%, and 3.56%, respectively, on WF1 for IEMOCAP. Similarly, the performance also drops for the MELD dataset after removing acoustic, visual, and both. For the context aggregation module, we first remove THC from the model, which leads to 1.99% and 3.54% drops on two datasets respectively. Next, we disorder the positions of utterance-specific, cause-specific, and emotion-specific nodes in the THC module, disrupting the hierarchical structure of context aggregation, which results in 1.79% and 1.94% drops on IEMOCAP and MELD respectively. Additionally, It can be found that removing the restriction of the context window when we construct the edges between nodes leads to the drop in ACC and WF1 on two datasets. Overall, MCP and THC are necessary to improve model performance, and introducing MERC and MECPE into a unified framework can bring improvements.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Conclusion",
      "text": "This paper presents a unified multimodal emotion recognition and emotion-cause analysis framework, which aims to explore the emotion-cause causality by jointly modeling multimodal emotion recognition and emotion-cause pair extraction. UniMEEC reformulates MERC and MECPE tasks as two mask prediction problems, tunes PLM via multimodal causal prompts specific to uni-modality, and aggregates task-specific context in a conversation. Experiments on IEMOCAP, MELD, Con-vECPE, and ECF consistently gain significant improvements on most metrics compared to the previous SOTA, further demonstrating the effectiveness of UniMEEC in addressing MERC and MEPCE.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Limitations",
      "text": "Due to the dimensions and sequence lengths of audio and vision modalities being less than the dimensions and sequence length of text modality, UniMEEC pads the audio and vision feature with zero to achieve consistency with the representation of text modality. This operation might introduce some unnecessary information in fusion representation learning. Furthermore, UniMEEC is set up to detect emotion and emotion cause in multimodal scenarios, fails to effectively address MERC and MECPE in text, which will also be solved in our future work.",
      "page_start": 14,
      "page_end": 14
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Illustration of the causal inference between",
      "page": 1
    },
    {
      "caption": "Figure 1: illustrates",
      "page": 1
    },
    {
      "caption": "Figure 2: The overview of UniMEEC. The outputs “disgust” and “u3” denote the emotion category and the emotion",
      "page": 3
    },
    {
      "caption": "Figure 2: , UniMEEC is composed",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "rice.hu.x@gmail.com, dh@di.ku.dk, hasti.seifi@asu.edu": "Abstract"
        },
        {
          "rice.hu.x@gmail.com, dh@di.ku.dk, hasti.seifi@asu.edu": ""
        },
        {
          "rice.hu.x@gmail.com, dh@di.ku.dk, hasti.seifi@asu.edu": "Multimodal emotion recognition in conversa-"
        },
        {
          "rice.hu.x@gmail.com, dh@di.ku.dk, hasti.seifi@asu.edu": "tion (MERC) and multimodal emotion-cause"
        },
        {
          "rice.hu.x@gmail.com, dh@di.ku.dk, hasti.seifi@asu.edu": ""
        },
        {
          "rice.hu.x@gmail.com, dh@di.ku.dk, hasti.seifi@asu.edu": "pair extraction (MECPE) have recently gar-"
        },
        {
          "rice.hu.x@gmail.com, dh@di.ku.dk, hasti.seifi@asu.edu": "nered significant attention. Emotions are the"
        },
        {
          "rice.hu.x@gmail.com, dh@di.ku.dk, hasti.seifi@asu.edu": ""
        },
        {
          "rice.hu.x@gmail.com, dh@di.ku.dk, hasti.seifi@asu.edu": "expression of\naffect or\nfeelings;\nresponses"
        },
        {
          "rice.hu.x@gmail.com, dh@di.ku.dk, hasti.seifi@asu.edu": ""
        },
        {
          "rice.hu.x@gmail.com, dh@di.ku.dk, hasti.seifi@asu.edu": "to specific events, or\nsituations – known as"
        },
        {
          "rice.hu.x@gmail.com, dh@di.ku.dk, hasti.seifi@asu.edu": ""
        },
        {
          "rice.hu.x@gmail.com, dh@di.ku.dk, hasti.seifi@asu.edu": "emotion causes.\nBoth collectively explain"
        },
        {
          "rice.hu.x@gmail.com, dh@di.ku.dk, hasti.seifi@asu.edu": ""
        },
        {
          "rice.hu.x@gmail.com, dh@di.ku.dk, hasti.seifi@asu.edu": "the causality between human emotion and in-"
        },
        {
          "rice.hu.x@gmail.com, dh@di.ku.dk, hasti.seifi@asu.edu": ""
        },
        {
          "rice.hu.x@gmail.com, dh@di.ku.dk, hasti.seifi@asu.edu": "tents. However, existing works treat emotion"
        },
        {
          "rice.hu.x@gmail.com, dh@di.ku.dk, hasti.seifi@asu.edu": "recognition and emotion cause extraction as"
        },
        {
          "rice.hu.x@gmail.com, dh@di.ku.dk, hasti.seifi@asu.edu": "two individual problems, ignoring their natural"
        },
        {
          "rice.hu.x@gmail.com, dh@di.ku.dk, hasti.seifi@asu.edu": "causality. In this paper, we propose a Unified"
        },
        {
          "rice.hu.x@gmail.com, dh@di.ku.dk, hasti.seifi@asu.edu": "Multimodal Emotion recognition and Emotion-"
        },
        {
          "rice.hu.x@gmail.com, dh@di.ku.dk, hasti.seifi@asu.edu": "Cause analysis framework (UniMEEC) to ex-"
        },
        {
          "rice.hu.x@gmail.com, dh@di.ku.dk, hasti.seifi@asu.edu": "plore the causality between emotion and emo-"
        },
        {
          "rice.hu.x@gmail.com, dh@di.ku.dk, hasti.seifi@asu.edu": "tion cause. Concretely, UniMEEC reformulates"
        },
        {
          "rice.hu.x@gmail.com, dh@di.ku.dk, hasti.seifi@asu.edu": "the MERC and MECPE tasks as mask predic-"
        },
        {
          "rice.hu.x@gmail.com, dh@di.ku.dk, hasti.seifi@asu.edu": "tion problems and unifies them with a causal"
        },
        {
          "rice.hu.x@gmail.com, dh@di.ku.dk, hasti.seifi@asu.edu": ""
        },
        {
          "rice.hu.x@gmail.com, dh@di.ku.dk, hasti.seifi@asu.edu": "prompt template. To differentiate the modal ef-"
        },
        {
          "rice.hu.x@gmail.com, dh@di.ku.dk, hasti.seifi@asu.edu": ""
        },
        {
          "rice.hu.x@gmail.com, dh@di.ku.dk, hasti.seifi@asu.edu": "fects, UniMEEC proposes a multimodal causal"
        },
        {
          "rice.hu.x@gmail.com, dh@di.ku.dk, hasti.seifi@asu.edu": ""
        },
        {
          "rice.hu.x@gmail.com, dh@di.ku.dk, hasti.seifi@asu.edu": "prompt\nto probe the pre-trained knowledge"
        },
        {
          "rice.hu.x@gmail.com, dh@di.ku.dk, hasti.seifi@asu.edu": ""
        },
        {
          "rice.hu.x@gmail.com, dh@di.ku.dk, hasti.seifi@asu.edu": "specified to modality and implements cross-"
        },
        {
          "rice.hu.x@gmail.com, dh@di.ku.dk, hasti.seifi@asu.edu": ""
        },
        {
          "rice.hu.x@gmail.com, dh@di.ku.dk, hasti.seifi@asu.edu": "task and cross-modality interactions under task-"
        },
        {
          "rice.hu.x@gmail.com, dh@di.ku.dk, hasti.seifi@asu.edu": "oriented settings. Experiment results on four"
        },
        {
          "rice.hu.x@gmail.com, dh@di.ku.dk, hasti.seifi@asu.edu": "public benchmark datasets verify the model"
        },
        {
          "rice.hu.x@gmail.com, dh@di.ku.dk, hasti.seifi@asu.edu": "performance on MERC and MECPE tasks"
        },
        {
          "rice.hu.x@gmail.com, dh@di.ku.dk, hasti.seifi@asu.edu": "and achieve consistent improvements compared"
        },
        {
          "rice.hu.x@gmail.com, dh@di.ku.dk, hasti.seifi@asu.edu": ""
        },
        {
          "rice.hu.x@gmail.com, dh@di.ku.dk, hasti.seifi@asu.edu": "with the previous state-of-the-art methods."
        },
        {
          "rice.hu.x@gmail.com, dh@di.ku.dk, hasti.seifi@asu.edu": ""
        },
        {
          "rice.hu.x@gmail.com, dh@di.ku.dk, hasti.seifi@asu.edu": ""
        },
        {
          "rice.hu.x@gmail.com, dh@di.ku.dk, hasti.seifi@asu.edu": "1\nIntroduction"
        },
        {
          "rice.hu.x@gmail.com, dh@di.ku.dk, hasti.seifi@asu.edu": ""
        },
        {
          "rice.hu.x@gmail.com, dh@di.ku.dk, hasti.seifi@asu.edu": "Recently, multimodal emotion recognition in con-"
        },
        {
          "rice.hu.x@gmail.com, dh@di.ku.dk, hasti.seifi@asu.edu": "versations (MERC) and multimodal emotion-cause"
        },
        {
          "rice.hu.x@gmail.com, dh@di.ku.dk, hasti.seifi@asu.edu": "pair extraction (MECPE) have attracted increasing"
        },
        {
          "rice.hu.x@gmail.com, dh@di.ku.dk, hasti.seifi@asu.edu": "attention (Zhang et al., 2021a,b; Hu et al., 2021a,b)."
        },
        {
          "rice.hu.x@gmail.com, dh@di.ku.dk, hasti.seifi@asu.edu": "Both task play crucial roles in dialog systems, espe-"
        },
        {
          "rice.hu.x@gmail.com, dh@di.ku.dk, hasti.seifi@asu.edu": "cially in empathetic response generation in a con-"
        },
        {
          "rice.hu.x@gmail.com, dh@di.ku.dk, hasti.seifi@asu.edu": "versation (Fu et al., 2023; Qian et al., 2023; Tian"
        },
        {
          "rice.hu.x@gmail.com, dh@di.ku.dk, hasti.seifi@asu.edu": "et al., 2022; Hu et al., 2024). MERC detects the"
        },
        {
          "rice.hu.x@gmail.com, dh@di.ku.dk, hasti.seifi@asu.edu": "emotion category of each utterance in a conversa-"
        },
        {
          "rice.hu.x@gmail.com, dh@di.ku.dk, hasti.seifi@asu.edu": "tion, while MECPE finds the reasons that trigger"
        },
        {
          "rice.hu.x@gmail.com, dh@di.ku.dk, hasti.seifi@asu.edu": "a certain emotion for the utterance. Both tasks are"
        },
        {
          "rice.hu.x@gmail.com, dh@di.ku.dk, hasti.seifi@asu.edu": "tightly related in practice and theory (Baumeister"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "and achieve consistent improvements compared": ""
        },
        {
          "and achieve consistent improvements compared": "with the previous state-of-the-art methods."
        },
        {
          "and achieve consistent improvements compared": ""
        },
        {
          "and achieve consistent improvements compared": ""
        },
        {
          "and achieve consistent improvements compared": "1\nIntroduction"
        },
        {
          "and achieve consistent improvements compared": ""
        },
        {
          "and achieve consistent improvements compared": "Recently, multimodal emotion recognition in con-"
        },
        {
          "and achieve consistent improvements compared": "versations (MERC) and multimodal emotion-cause"
        },
        {
          "and achieve consistent improvements compared": "pair extraction (MECPE) have attracted increasing"
        },
        {
          "and achieve consistent improvements compared": "attention (Zhang et al., 2021a,b; Hu et al., 2021a,b)."
        },
        {
          "and achieve consistent improvements compared": "Both task play crucial roles in dialog systems, espe-"
        },
        {
          "and achieve consistent improvements compared": "cially in empathetic response generation in a con-"
        },
        {
          "and achieve consistent improvements compared": "versation (Fu et al., 2023; Qian et al., 2023; Tian"
        },
        {
          "and achieve consistent improvements compared": "et al., 2022; Hu et al., 2024). MERC detects the"
        },
        {
          "and achieve consistent improvements compared": "emotion category of each utterance in a conversa-"
        },
        {
          "and achieve consistent improvements compared": "tion, while MECPE finds the reasons that trigger"
        },
        {
          "and achieve consistent improvements compared": "a certain emotion for the utterance. Both tasks are"
        },
        {
          "and achieve consistent improvements compared": "tightly related in practice and theory (Baumeister"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "captures the emotion-specific, cause-specific, and": ""
        },
        {
          "captures the emotion-specific, cause-specific, and": "utterance-specific contexts in a hierarchical way."
        },
        {
          "captures the emotion-specific, cause-specific, and": ""
        },
        {
          "captures the emotion-specific, cause-specific, and": "The main contributions are summarized as follows:"
        },
        {
          "captures the emotion-specific, cause-specific, and": ""
        },
        {
          "captures the emotion-specific, cause-specific, and": "• We propose a Unified Multimodal Emotion"
        },
        {
          "captures the emotion-specific, cause-specific, and": "recognition and Emotion Cause pair extrac-"
        },
        {
          "captures the emotion-specific, cause-specific, and": "tion framework (UniMEEC)1, which uses"
        },
        {
          "captures the emotion-specific, cause-specific, and": "the causal prompt\nto unify the MERC and"
        },
        {
          "captures the emotion-specific, cause-specific, and": "MECPE tasks\nfor\ncausal\nrelation between"
        },
        {
          "captures the emotion-specific, cause-specific, and": "emotion and emotion cause."
        },
        {
          "captures the emotion-specific, cause-specific, and": ""
        },
        {
          "captures the emotion-specific, cause-specific, and": "• UniMEEC formalizes MERC and MEEC"
        },
        {
          "captures the emotion-specific, cause-specific, and": ""
        },
        {
          "captures the emotion-specific, cause-specific, and": "tasks\ninto mask\nprediction\nproblems\nand"
        },
        {
          "captures the emotion-specific, cause-specific, and": ""
        },
        {
          "captures the emotion-specific, cause-specific, and": "constructs the multimodal causal prompt\nto"
        },
        {
          "captures the emotion-specific, cause-specific, and": ""
        },
        {
          "captures the emotion-specific, cause-specific, and": "probe the knowledge from PLM. Meanwhile,"
        },
        {
          "captures the emotion-specific, cause-specific, and": ""
        },
        {
          "captures the emotion-specific, cause-specific, and": "UniMEEC proposes task-specific context ag-"
        },
        {
          "captures the emotion-specific, cause-specific, and": ""
        },
        {
          "captures the emotion-specific, cause-specific, and": "gregation to orderly capture the contexts ori-"
        },
        {
          "captures the emotion-specific, cause-specific, and": ""
        },
        {
          "captures the emotion-specific, cause-specific, and": "ented to specific tasks."
        },
        {
          "captures the emotion-specific, cause-specific, and": ""
        },
        {
          "captures the emotion-specific, cause-specific, and": ""
        },
        {
          "captures the emotion-specific, cause-specific, and": "• Experimental\nresults\ndemonstrate\nthat"
        },
        {
          "captures the emotion-specific, cause-specific, and": "UniMEEC achieves a new state-of-the-art per-"
        },
        {
          "captures the emotion-specific, cause-specific, and": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "sentiment rating and reviews, further demonstrat-": "ing that Pretrained Language Model (PLM) is able",
          "formance on MELD, IEMOCAP, ConvECPE": "and ECF datasets, further demonstrating the"
        },
        {
          "sentiment rating and reviews, further demonstrat-": "to be aware of the underlying causality. A natural",
          "formance on MELD, IEMOCAP, ConvECPE": "effectiveness of a unified causal framework"
        },
        {
          "sentiment rating and reviews, further demonstrat-": "question arises: How should we perform causal-",
          "formance on MELD, IEMOCAP, ConvECPE": "for MERC and MECPE."
        },
        {
          "sentiment rating and reviews, further demonstrat-": "ity between emotions and their causes in a unified",
          "formance on MELD, IEMOCAP, ConvECPE": ""
        },
        {
          "sentiment rating and reviews, further demonstrat-": "",
          "formance on MELD, IEMOCAP, ConvECPE": "2\nRelated Work"
        },
        {
          "sentiment rating and reviews, further demonstrat-": "architecture?",
          "formance on MELD, IEMOCAP, ConvECPE": ""
        },
        {
          "sentiment rating and reviews, further demonstrat-": "Recently, the unification of related but different",
          "formance on MELD, IEMOCAP, ConvECPE": "Multimodal Emotion Recognition in Conver-"
        },
        {
          "sentiment rating and reviews, further demonstrat-": "tasks into a framework has achieved significant",
          "formance on MELD, IEMOCAP, ConvECPE": "sations\n(MERC)\nWe categorize the works of"
        },
        {
          "sentiment rating and reviews, further demonstrat-": "progress (Chen et al., 2022; Xie et al., 2022; Zhang",
          "formance on MELD, IEMOCAP, ConvECPE": "MERC into three main groups: multimodal fusion,"
        },
        {
          "sentiment rating and reviews, further demonstrat-": "et al., 2022).\nFor example, UniMSE (Hu et al.,",
          "formance on MELD, IEMOCAP, ConvECPE": "context-aware models,\nand external-knowledge"
        },
        {
          "sentiment rating and reviews, further demonstrat-": "2022b) unifies emotion and sentiment into a single",
          "formance on MELD, IEMOCAP, ConvECPE": "models. The first group focuses on the fusion rep-"
        },
        {
          "sentiment rating and reviews, further demonstrat-": "architecture to share complementary knowledge",
          "formance on MELD, IEMOCAP, ConvECPE": "resentation in which some works (Hu et al., 2022a,"
        },
        {
          "sentiment rating and reviews, further demonstrat-": "between them.\nDifferent\nfrom UniMSE which",
          "formance on MELD, IEMOCAP, ConvECPE": "2021c; Joshi et al., 2022) employed the graph neu-"
        },
        {
          "sentiment rating and reviews, further demonstrat-": "focuses on the unification of emotion and senti-",
          "formance on MELD, IEMOCAP, ConvECPE": "ral networks to model the inter/intra dependencies"
        },
        {
          "sentiment rating and reviews, further demonstrat-": "ment\nin a generative way, we propose a multi-",
          "formance on MELD, IEMOCAP, ConvECPE": "of utterances information, and some works pro-"
        },
        {
          "sentiment rating and reviews, further demonstrat-": "modal causal prompt to unify MERC and MECPE",
          "formance on MELD, IEMOCAP, ConvECPE": "posed cross-attention Transformer (Vaswani et al.,"
        },
        {
          "sentiment rating and reviews, further demonstrat-": "tasks, thereby capturing the causal nature between",
          "formance on MELD, IEMOCAP, ConvECPE": "2017)\nto model cross-modality interaction. Ad-"
        },
        {
          "sentiment rating and reviews, further demonstrat-": "emotion and emotion cause.\nIn this paper, we",
          "formance on MELD, IEMOCAP, ConvECPE": "dressing context incorporation, Sun et al. (2021);"
        },
        {
          "sentiment rating and reviews, further demonstrat-": "propose a Unified Multimodal Emotion recogni-",
          "formance on MELD, IEMOCAP, ConvECPE": "Li et al.\n(2021b); Ghosal et al.\n(2019) construct"
        },
        {
          "sentiment rating and reviews, further demonstrat-": "tion and Emotion-Cause pair extraction framework",
          "formance on MELD, IEMOCAP, ConvECPE": "graph structures to represent contexts and further"
        },
        {
          "sentiment rating and reviews, further demonstrat-": "(UniMEEC) to explore the causality between emo-",
          "formance on MELD, IEMOCAP, ConvECPE": "model\ninter-utterance dependencies, while Mao"
        },
        {
          "sentiment rating and reviews, further demonstrat-": "tion and emotion cause. As Lyu et al. (2024) il-",
          "formance on MELD, IEMOCAP, ConvECPE": "et al.\n(2021)\nintroduces the concept of emotion"
        },
        {
          "sentiment rating and reviews, further demonstrat-": "lustrated, PLM can capture the causal stories with",
          "formance on MELD, IEMOCAP, ConvECPE": "dynamics to capture context.\nIn the last group,"
        },
        {
          "sentiment rating and reviews, further demonstrat-": "the causal prompts. Starting from this perspective,",
          "formance on MELD, IEMOCAP, ConvECPE": "advanced MERC studies integrate external knowl-"
        },
        {
          "sentiment rating and reviews, further demonstrat-": "UniMEEC reformulates MERC and MECPE as",
          "formance on MELD, IEMOCAP, ConvECPE": "edge, employing techniques such as transfer learn-"
        },
        {
          "sentiment rating and reviews, further demonstrat-": "two mask prediction tasks and unifies the two tasks",
          "formance on MELD, IEMOCAP, ConvECPE": "ing (Hazarika et al., 2019; Lee and Lee, 2021), com-"
        },
        {
          "sentiment rating and reviews, further demonstrat-": "using a causal prompt, aiming to capture the un-",
          "formance on MELD, IEMOCAP, ConvECPE": "monsense knowledge (Ghosal et al., 2020), multi-"
        },
        {
          "sentiment rating and reviews, further demonstrat-": "derstanding of PLM to emotion-cause causlity. In",
          "formance on MELD, IEMOCAP, ConvECPE": "task learning (Akhtar et al., 2019), and external"
        },
        {
          "sentiment rating and reviews, further demonstrat-": "order to differentiate the modal effects, UniMEEC",
          "formance on MELD, IEMOCAP, ConvECPE": "information\n(Zhu et al., 2021) to introduce more"
        },
        {
          "sentiment rating and reviews, further demonstrat-": "probes modal features from PLM using the multi-",
          "formance on MELD, IEMOCAP, ConvECPE": "auxiliary information to help model understand"
        },
        {
          "sentiment rating and reviews, further demonstrat-": "modal causal prompt, and meanwhile, UniMEEC",
          "formance on MELD, IEMOCAP, ConvECPE": "conversation."
        },
        {
          "sentiment rating and reviews, further demonstrat-": "captures the emotion-specific, cause-specific, and",
          "formance on MELD, IEMOCAP, ConvECPE": ""
        },
        {
          "sentiment rating and reviews, further demonstrat-": "",
          "formance on MELD, IEMOCAP, ConvECPE": "Multimodal Emotion-Cause Pair Extraction"
        },
        {
          "sentiment rating and reviews, further demonstrat-": "utterance-specific contexts in a hierarchical way.",
          "formance on MELD, IEMOCAP, ConvECPE": ""
        },
        {
          "sentiment rating and reviews, further demonstrat-": "",
          "formance on MELD, IEMOCAP, ConvECPE": "(MECPE)\nAs more and more NLP tasks extend"
        },
        {
          "sentiment rating and reviews, further demonstrat-": "The main contributions are summarized as follows:",
          "formance on MELD, IEMOCAP, ConvECPE": ""
        },
        {
          "sentiment rating and reviews, further demonstrat-": "",
          "formance on MELD, IEMOCAP, ConvECPE": "to the multimodal paradigm (Zhu et al., 2024; Li"
        },
        {
          "sentiment rating and reviews, further demonstrat-": "• We propose a Unified Multimodal Emotion",
          "formance on MELD, IEMOCAP, ConvECPE": "et al., 2024; ?), Wang et al. (2021) defined mul-"
        },
        {
          "sentiment rating and reviews, further demonstrat-": "recognition and Emotion Cause pair extrac-",
          "formance on MELD, IEMOCAP, ConvECPE": "timodal emotion-cause pair extraction (MECPE)"
        },
        {
          "sentiment rating and reviews, further demonstrat-": "tion framework (UniMEEC)1, which uses",
          "formance on MELD, IEMOCAP, ConvECPE": "and constructed Emotion-Cause-in-Friends (ECF)"
        },
        {
          "sentiment rating and reviews, further demonstrat-": "the causal prompt\nto unify the MERC and",
          "formance on MELD, IEMOCAP, ConvECPE": "dataset based on MELD (Poria et al., 2019). Li et al."
        },
        {
          "sentiment rating and reviews, further demonstrat-": "MECPE tasks\nfor\ncausal\nrelation between",
          "formance on MELD, IEMOCAP, ConvECPE": "(2022a) built an English conversational emotion-"
        },
        {
          "sentiment rating and reviews, further demonstrat-": "emotion and emotion cause.",
          "formance on MELD, IEMOCAP, ConvECPE": "cause pair extraction multimodal dataset based on"
        },
        {
          "sentiment rating and reviews, further demonstrat-": "",
          "formance on MELD, IEMOCAP, ConvECPE": "IEMOCAP (Busso et al., 2008). With MECPE"
        },
        {
          "sentiment rating and reviews, further demonstrat-": "• UniMEEC formalizes MERC and MEEC",
          "formance on MELD, IEMOCAP, ConvECPE": ""
        },
        {
          "sentiment rating and reviews, further demonstrat-": "",
          "formance on MELD, IEMOCAP, ConvECPE": "only emerging for a relatively short\ntime,\nthere"
        },
        {
          "sentiment rating and reviews, further demonstrat-": "tasks\ninto mask\nprediction\nproblems\nand",
          "formance on MELD, IEMOCAP, ConvECPE": ""
        },
        {
          "sentiment rating and reviews, further demonstrat-": "",
          "formance on MELD, IEMOCAP, ConvECPE": "are a few baseline methods in this field. Previous"
        },
        {
          "sentiment rating and reviews, further demonstrat-": "constructs the multimodal causal prompt\nto",
          "formance on MELD, IEMOCAP, ConvECPE": ""
        },
        {
          "sentiment rating and reviews, further demonstrat-": "",
          "formance on MELD, IEMOCAP, ConvECPE": "studie (Wang et al., 2021; Li et al., 2022a)\ninte-"
        },
        {
          "sentiment rating and reviews, further demonstrat-": "probe the knowledge from PLM. Meanwhile,",
          "formance on MELD, IEMOCAP, ConvECPE": ""
        },
        {
          "sentiment rating and reviews, further demonstrat-": "",
          "formance on MELD, IEMOCAP, ConvECPE": "grated multimodal features to tackle the MECPE"
        },
        {
          "sentiment rating and reviews, further demonstrat-": "UniMEEC proposes task-specific context ag-",
          "formance on MELD, IEMOCAP, ConvECPE": ""
        },
        {
          "sentiment rating and reviews, further demonstrat-": "",
          "formance on MELD, IEMOCAP, ConvECPE": "task based on the baselines of ECPE (Xia and"
        },
        {
          "sentiment rating and reviews, further demonstrat-": "gregation to orderly capture the contexts ori-",
          "formance on MELD, IEMOCAP, ConvECPE": ""
        },
        {
          "sentiment rating and reviews, further demonstrat-": "",
          "formance on MELD, IEMOCAP, ConvECPE": "Ding, 2019), overlooking the importance of inter-"
        },
        {
          "sentiment rating and reviews, further demonstrat-": "ented to specific tasks.",
          "formance on MELD, IEMOCAP, ConvECPE": ""
        },
        {
          "sentiment rating and reviews, further demonstrat-": "",
          "formance on MELD, IEMOCAP, ConvECPE": "utterance context and multimodal fusion in under-"
        },
        {
          "sentiment rating and reviews, further demonstrat-": "",
          "formance on MELD, IEMOCAP, ConvECPE": "standing emotion cause."
        },
        {
          "sentiment rating and reviews, further demonstrat-": "• Experimental\nresults\ndemonstrate\nthat",
          "formance on MELD, IEMOCAP, ConvECPE": ""
        },
        {
          "sentiment rating and reviews, further demonstrat-": "UniMEEC achieves a new state-of-the-art per-",
          "formance on MELD, IEMOCAP, ConvECPE": ""
        },
        {
          "sentiment rating and reviews, further demonstrat-": "",
          "formance on MELD, IEMOCAP, ConvECPE": "Prompt-tuning\nPrompt-tuning (Li and Liang,"
        },
        {
          "sentiment rating and reviews, further demonstrat-": "1https://github.com/LeMei/causal-unimeec",
          "formance on MELD, IEMOCAP, ConvECPE": "2021; Liu et al., 2021; Su et al., 2021),\ninspired"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Chris says they": "are closing down \ncause-specific feature"
        },
        {
          "Chris says they": "the bar."
        },
        {
          "Chris says they": ""
        },
        {
          "Chris says they": "emotion cause utterance"
        },
        {
          "Chris says they": "utterance-specific feature"
        },
        {
          "Chris says they": "…"
        },
        {
          "Chris says they": ""
        },
        {
          "Chris says they": ""
        },
        {
          "Chris says they": "tion [X], auxiliary prompt\ntokens P(·), and mask"
        },
        {
          "Chris says they": "tem-\ntokens [M]1 and [M]2. We feed the causal"
        },
        {
          "Chris says they": "plate into PLM to encode [X], [M]1 and [M]2 into"
        },
        {
          "Chris says they": "vectors. THC takes the emotion-specific, cause-"
        },
        {
          "Chris says they": "specific, and utterance-specific representations as"
        },
        {
          "Chris says they": "nodes and models their dependencies in the context"
        },
        {
          "Chris says they": "window. Finally, UniMEEC predicts the emotion"
        },
        {
          "Chris says they": "category and the position of cause utterance in a"
        },
        {
          "Chris says they": "conversation based on the representations of [M]1"
        },
        {
          "Chris says they": "and [M]2 respectively."
        },
        {
          "Chris says they": ""
        },
        {
          "Chris says they": "3.2\nTask Formalization"
        },
        {
          "Chris says they": ""
        },
        {
          "Chris says they": "U\n=\nGiven\na\nmulti-turn\nconversation"
        },
        {
          "Chris says they": "{u1, u2, · · ·\n, u|U |}, U has |U | utterances and each"
        },
        {
          "Chris says they": "utterance ui = {I t\ni , I a\ni , I v\ni } contains three modali-"
        },
        {
          "Chris says they": "ties, where I m\n, m ∈ {t, a, v} represent uni-modal"
        },
        {
          "Chris says they": "i"
        },
        {
          "Chris says they": "feature\nextracted\nfrom video\nfragment\ni,\nand"
        },
        {
          "Chris says they": "{t, a, v} denote the three types of modalities—text,"
        },
        {
          "Chris says they": "acoustic\nand visual,\nrespectively.\nMultimodal"
        },
        {
          "Chris says they": "emotion recognition (MERC) predicts the emotion"
        },
        {
          "Chris says they": "and multimodal\nemotion-cause\ncategory of ui,"
        },
        {
          "Chris says they": "pair\nextraction\n(MECPE)\naims\nto\npredict\nthe"
        },
        {
          "Chris says they": "corresponding cause utterance ID (e.g., “u1”, “u2”)"
        },
        {
          "Chris says they": "for non-neutral utterance ui. To unify MERC and"
        },
        {
          "Chris says they": "MECPE, we formalize MERC and MECPE as two"
        },
        {
          "Chris says they": "mask prediction problems in the causal prompt and"
        },
        {
          "Chris says they": "leverage the language model\nto probabilistically"
        },
        {
          "Chris says they": "fill the unfilled slots, thereby predicting the results"
        },
        {
          "Chris says they": ""
        },
        {
          "Chris says they": "of MERC and MECPE tasks respectively."
        },
        {
          "Chris says they": ""
        },
        {
          "Chris says they": "3.3\nMultimodal Causal Prompt (MCP)"
        },
        {
          "Chris says they": ""
        },
        {
          "Chris says they": "In order\nto differentiate\nthe modal\neffects, we"
        },
        {
          "Chris says they": "set causal prompt for each modality to probe the"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "modality-specific features from PLM. Multimodal": "causal prompts share auxiliary prompt\ntokens in",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": "prompt encoders, respectively. First, text-specific"
        },
        {
          "modality-specific features from PLM. Multimodal": "the prompt template, which enables inter-modality",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": "prompt is fed into the text-specific prompt encoder"
        },
        {
          "modality-specific features from PLM. Multimodal": "and inter-task semantic interaction in representa-",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": "to get the text-specific representations of [X], aux-"
        },
        {
          "modality-specific features from PLM. Multimodal": "tion learning.",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": "iliary prompt part, and [M]1 and [M]2, with the su-"
        },
        {
          "modality-specific features from PLM. Multimodal": "",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": "pervision of real ground answers of slots. After that,"
        },
        {
          "modality-specific features from PLM. Multimodal": "3.3.1\nCausal Prompt Construction",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": ""
        },
        {
          "modality-specific features from PLM. Multimodal": "",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": "we obtain the text-specific prompt sequence, which"
        },
        {
          "modality-specific features from PLM. Multimodal": "We manually design the modality-specific prompt",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": ""
        },
        {
          "modality-specific features from PLM. Multimodal": "",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": ",\ncontains the hidden states of hP1,l1\n, Xi,t, hPl2,l3"
        },
        {
          "modality-specific features from PLM. Multimodal": "template, and it consists of a modal input [X], the",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": ""
        },
        {
          "modality-specific features from PLM. Multimodal": "",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": "the\nand h[M]2, where h(·) denotes\nh[M]1, hPl4,l5"
        },
        {
          "modality-specific features from PLM. Multimodal": "emotion category slot\nthe cause slot\n[M]1,\n[M]2",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": ","
        },
        {
          "modality-specific features from PLM. Multimodal": "",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": "representation of token or token sequence, hP1,l1"
        },
        {
          "modality-specific features from PLM. Multimodal": "and auxiliary prompt part, where [X]\nis the slot",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": ""
        },
        {
          "modality-specific features from PLM. Multimodal": "",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": "denote the representations of\nhPl2,l3\nand hPl4,l5"
        },
        {
          "modality-specific features from PLM. Multimodal": "filled with modal feature of target utterance, [M]1",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": "auxiliary prompt parts."
        },
        {
          "modality-specific features from PLM. Multimodal": "indicates the emotion category of target utterance,",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": "Due to the dimensions and sequence lengths of"
        },
        {
          "modality-specific features from PLM. Multimodal": "e.g., “happy” or “sad”, and [M]2 indicates the cause",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": "audio and vision modalities being less than the"
        },
        {
          "modality-specific features from PLM. Multimodal": "utterance ID of target utterance, e.g., “u1”, “u2”.",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": "dimensions and sequence length of text modality,"
        },
        {
          "modality-specific features from PLM. Multimodal": "[M]1 and [M]2 are unfilled answer slots and are",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": "we pad the audio and vision feature with zero to"
        },
        {
          "modality-specific features from PLM. Multimodal": "separately predicted as the results of MERC and",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": "achieve consistency with the representation of text"
        },
        {
          "modality-specific features from PLM. Multimodal": "MECPE. Given text modality I t",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": "ˆ\nˆ"
        },
        {
          "modality-specific features from PLM. Multimodal": "i , i ∈ {1, · · ·",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": "modality. We take\nXi,a and\nXi,v to represent au-"
        },
        {
          "modality-specific features from PLM. Multimodal": "we designed the causal prompt template like “the",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": "dio and vision representations after padding,\nre-"
        },
        {
          "modality-specific features from PLM. Multimodal": "emotion of utterance I t\nis [M]1, and its emotion",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": "spectively. For audio-specific prompt, we replace"
        },
        {
          "modality-specific features from PLM. Multimodal": "i",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": ""
        },
        {
          "modality-specific features from PLM. Multimodal": "",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": "ˆ"
        },
        {
          "modality-specific features from PLM. Multimodal": "cause is [M]2” as text-specific prompt, where the",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": "[X] part of\nthe prompt\nrepresentation with\nXi,a."
        },
        {
          "modality-specific features from PLM. Multimodal": "textual strings “For conversation”, “the emotion cat-",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": "For vision-specific prompt, we replace [X] part"
        },
        {
          "modality-specific features from PLM. Multimodal": "egory of”, “is”, and “the reason for this emotion is”",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": "ˆ"
        },
        {
          "modality-specific features from PLM. Multimodal": "",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": "of\nthe prompt\nrepresentation with\nXi,v after Nt"
        },
        {
          "modality-specific features from PLM. Multimodal": "are auxiliary prompt parts. For audio-specific and",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": "Transformer\nlayers.\nAfter\nthat, we feed audio-"
        },
        {
          "modality-specific features from PLM. Multimodal": "vision-specific prompts, we replace the [X] part of",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": ""
        },
        {
          "modality-specific features from PLM. Multimodal": "",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": "specific and vision-specific prompts into Na and"
        },
        {
          "modality-specific features from PLM. Multimodal": "the prompt with the acoustic and visual representa-",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": ""
        },
        {
          "modality-specific features from PLM. Multimodal": "",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": "Nv Transformer layers respectively. For (n-1)-th"
        },
        {
          "modality-specific features from PLM. Multimodal": "tions to construct audio-specific and vision-specific",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": "Transformer\nlayer,\nthe modality-specific prompt"
        },
        {
          "modality-specific features from PLM. Multimodal": "prompts, respectively.",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": "learning is given by:"
        },
        {
          "modality-specific features from PLM. Multimodal": "We use Xi,m, Xi,m ∈ Rlm×dm to represent the",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": ""
        },
        {
          "modality-specific features from PLM. Multimodal": "modal representation after modal alignment (Tsai",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": ""
        },
        {
          "modality-specific features from PLM. Multimodal": "",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": ", hm\nP n−1\n, X n−1\n, hm"
        },
        {
          "modality-specific features from PLM. Multimodal": "",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": "[M]2 ]\n[M]1 , hPl4,l5\ni,m = [hP1,l1\ni,m , hPl2,l3"
        },
        {
          "modality-specific features from PLM. Multimodal": "et al., 2019),\nlm and dm are the sequence length",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": ""
        },
        {
          "modality-specific features from PLM. Multimodal": "",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": "(1)"
        },
        {
          "modality-specific features from PLM. Multimodal": "",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": "i,m = Transformer(P n−1\ni,m , P n−1\ni,m , P n−1\ni,m )"
        },
        {
          "modality-specific features from PLM. Multimodal": "and the representation dimension of modality m,",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": ""
        },
        {
          "modality-specific features from PLM. Multimodal": "",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": "X n\ni,m = P n\ni,m, m ∈ {t, a, v}"
        },
        {
          "modality-specific features from PLM. Multimodal": "respectively. Specifically, we obtain Xi,t with the",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": ""
        },
        {
          "modality-specific features from PLM. Multimodal": "word embedding layer of the model and we pro-",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": ""
        },
        {
          "modality-specific features from PLM. Multimodal": "",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": "where P n−1\ndenotes the prompt\nrepresentation"
        },
        {
          "modality-specific features from PLM. Multimodal": "",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": "i,m"
        },
        {
          "modality-specific features from PLM. Multimodal": "cessed raw acoustic input into numerical sequential",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": ""
        },
        {
          "modality-specific features from PLM. Multimodal": "",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": "of utterance ui under the modality m. Specifically,"
        },
        {
          "modality-specific features from PLM. Multimodal": "vectors by librosa 2 to extract Mel-spectrogram as",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": ""
        },
        {
          "modality-specific features from PLM. Multimodal": "",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": "P n−1"
        },
        {
          "modality-specific features from PLM. Multimodal": "",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": "i,m is composed by the hidden states of [X], [M]1"
        },
        {
          "modality-specific features from PLM. Multimodal": "Xi,a. For vision modality, we use effecientNet (Tan",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": ""
        },
        {
          "modality-specific features from PLM. Multimodal": "",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": "[M]2, and auxiliary prompt strings. X 0\ni,t = Xi,t,"
        },
        {
          "modality-specific features from PLM. Multimodal": "and Le, 2019) pre-trained (supervised) on VGGface",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": ""
        },
        {
          "modality-specific features from PLM. Multimodal": "3 and AFEW dataset to extract Xi,v.",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": "X 0\n[·, ·] denotes the\ni,a = ˆXi,a, and X 0\ni,v = ˆXi,v."
        },
        {
          "modality-specific features from PLM. Multimodal": "",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": "concatenation operation."
        },
        {
          "modality-specific features from PLM. Multimodal": "3.3.2\nCausal Prompt Encoder",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": "After\nthe multimodal\ncausal prompt, we ob-"
        },
        {
          "modality-specific features from PLM. Multimodal": "",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": "tain the modal fusion representations of mask to-"
        },
        {
          "modality-specific features from PLM. Multimodal": "We take Transformer-based model (e.g., BERT (De-",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": ""
        },
        {
          "modality-specific features from PLM. Multimodal": "",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": "kens [M]1 and [M]2 via concatenation, respectively."
        },
        {
          "modality-specific features from PLM. Multimodal": "vlin et al., 2019)) as the backbone of\nthe multi-",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": "Similarly, we obtain the fusion representation of ui"
        },
        {
          "modality-specific features from PLM. Multimodal": "",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": "via the concatenation of X Nt\n, X Na\nand X Nv"
        },
        {
          "modality-specific features from PLM. Multimodal": "modal causal prompt. The stacked Transformer",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": "i,t\ni,a\ni,v :"
        },
        {
          "modality-specific features from PLM. Multimodal": "contains multiple Transformer\nlayers,\nand each",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": ""
        },
        {
          "modality-specific features from PLM. Multimodal": "",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": "hf"
        },
        {
          "modality-specific features from PLM. Multimodal": "layer contains a self-attention module, FFN, and",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": "[M]1 , ha\n[M]1 , hv\n[M]1 ]\n[M]1"
        },
        {
          "modality-specific features from PLM. Multimodal": "",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": "hf\n= [ht"
        },
        {
          "modality-specific features from PLM. Multimodal": "layer normalization (Ba et al., 2016). We take the",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": "(2)\n[M]2 , ha\n[M]2 , hv\n[M]2 ]\n[M]2"
        },
        {
          "modality-specific features from PLM. Multimodal": "former Nt Transformer layers as the text-specific",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": ""
        },
        {
          "modality-specific features from PLM. Multimodal": "",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": "hf\ni,v ]\ni,a , X Nv\ni,t , X Na\nui = [X Nt"
        },
        {
          "modality-specific features from PLM. Multimodal": "prompt encoder and take the latter Na\nand Nv",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": ""
        },
        {
          "modality-specific features from PLM. Multimodal": "",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": "where X Nt\n, X Na\nand X Nv\nare text, audio and"
        },
        {
          "modality-specific features from PLM. Multimodal": "",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": "i,t\ni,a\ni,v"
        },
        {
          "modality-specific features from PLM. Multimodal": "2https://github.com/librosa/librosa",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": ""
        },
        {
          "modality-specific features from PLM. Multimodal": "3https://www.robots.ox.ac.uk/~vgg/software/",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": "video representations of ui encoded by Nt, Na and"
        },
        {
          "modality-specific features from PLM. Multimodal": "vgg_face/",
          "Transformer\nlayers\nas\nthe visual-\nand acoustic": "Nv Transformer layers respectively."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "",
          "indexes of utterances in a conversation, and they": "are located on the same or adjacent levels of THC."
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "The learned representations of\n(i.e., hf\n)\n[M]1",
          "indexes of utterances in a conversation, and they": ""
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "[M]1",
          "indexes of utterances in a conversation, and they": ""
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "(i.e., hf\n)\nfail\nto capture the context\nand [M]2",
          "indexes of utterances in a conversation, and they": "3.4.2\nTask-specific Context Aggregation"
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "[M]2",
          "indexes of utterances in a conversation, and they": ""
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "information in a conversation, which inspires us",
          "indexes of utterances in a conversation, and they": ""
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "",
          "indexes of utterances in a conversation, and they": "We set a contextual window for each node at each"
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "to build a hierarchical context aggregation struc-",
          "indexes of utterances in a conversation, and they": ""
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "",
          "indexes of utterances in a conversation, and they": "level\nto ensure that\nthe model only aggregates"
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "ture to control the direction of context aggregation",
          "indexes of utterances in a conversation, and they": ""
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "",
          "indexes of utterances in a conversation, and they": "the node representations in its contextual window."
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "in a conversation.\nIn order to avoid the noise in-",
          "indexes of utterances in a conversation, and they": ""
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "",
          "indexes of utterances in a conversation, and they": "This operation reduces the computational cost and"
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "formation in representation learning, we set\nthe",
          "indexes of utterances in a conversation, and they": ""
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "",
          "indexes of utterances in a conversation, and they": "avoids introducing noise to the representation learn-"
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "context windows for each utterance to incorporate",
          "indexes of utterances in a conversation, and they": ""
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "",
          "indexes of utterances in a conversation, and they": "ing. Given an utterance ui, the prediction slots of"
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "the information around target utterance.",
          "indexes of utterances in a conversation, and they": ""
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "",
          "indexes of utterances in a conversation, and they": "emotion and emotion cause are [M]i,1 and [M]i,2"
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "",
          "indexes of utterances in a conversation, and they": "respectively. We aggregate the representation from"
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "3.4.1\nHierarchical Graph Construction",
          "indexes of utterances in a conversation, and they": ""
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "",
          "indexes of utterances in a conversation, and they": "the bottom to top levels in the graph, and the rep-"
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "We construct a 3-level graph attention network",
          "indexes of utterances in a conversation, and they": ""
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "",
          "indexes of utterances in a conversation, and they": "resentations of bottom nodes are not updated by"
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "(GAT) (Velickovic et al., 2018) as the encoder of",
          "indexes of utterances in a conversation, and they": ""
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "",
          "indexes of utterances in a conversation, and they": "aggregating the representations of the top or middle"
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "contexts, which includes top, middle, and bottom",
          "indexes of utterances in a conversation, and they": ""
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "",
          "indexes of utterances in a conversation, and they": "its repre-\nnodes to them. For the bottom node ui,"
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "levels. Each level has a context window to focus on",
          "indexes of utterances in a conversation, and they": ""
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "",
          "indexes of utterances in a conversation, and they": "sentation is aggregated by the bottom nodes in the"
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "the local context of utterance. Formally, we define",
          "indexes of utterances in a conversation, and they": ""
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "",
          "indexes of utterances in a conversation, and they": "context window:"
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "a graph G = (V, E), V and E denote the node",
          "indexes of utterances in a conversation, and they": ""
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "and edge sets respectively. We take the utterance-",
          "indexes of utterances in a conversation, and they": ""
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "level representation hu as the bottom node, cause-",
          "indexes of utterances in a conversation, and they": ""
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "",
          "indexes of utterances in a conversation, and they": " \n \n(cid:88)"
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "",
          "indexes of utterances in a conversation, and they": "hn\n+ bn−1\natuu\n(4)\ni,j W uu,n−1hn−1"
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "specific token representation hf\nas the middle",
          "indexes of utterances in a conversation, and they": "ui = ReLU"
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "[M]2",
          "indexes of utterances in a conversation, and they": ""
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "",
          "indexes of utterances in a conversation, and they": "j∈Nui"
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "node, and the emotion-specific token representation",
          "indexes of utterances in a conversation, and they": ""
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "hf\nas the top node. For the intra-level nodes, we",
          "indexes of utterances in a conversation, and they": ""
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "[M]1",
          "indexes of utterances in a conversation, and they": "where Nui denotes the neighbor nodes of utterance"
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "set undirected edges for any two adjacent nodes in",
          "indexes of utterances in a conversation, and they": ""
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "",
          "indexes of utterances in a conversation, and they": "ui and h0\nuj . When the model comes to the\nuj = hf"
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "the context window of the same level. For the inter-",
          "indexes of utterances in a conversation, and they": ""
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "",
          "indexes of utterances in a conversation, and they": "the representations is aggre-\nmiddle node [M]i,2,"
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "level nodes, we set the undirected edges between",
          "indexes of utterances in a conversation, and they": ""
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "",
          "indexes of utterances in a conversation, and they": "gated by the top and middle nodes in the context"
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "the top nodes and middle nodes. In general, we set",
          "indexes of utterances in a conversation, and they": ""
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "",
          "indexes of utterances in a conversation, and they": "window, which is given by:"
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "the directed edges from the bottom to the middle",
          "indexes of utterances in a conversation, and they": ""
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "nodes in the context window, aiming to control the",
          "indexes of utterances in a conversation, and they": ""
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "",
          "indexes of utterances in a conversation, and they": "(cid:88)"
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "",
          "indexes of utterances in a conversation, and they": "hn\natcc\ni,j W cc,n−1hn−1"
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "direction of the information flow among nodes.",
          "indexes of utterances in a conversation, and they": "[M]i,2 = ReLU(\n[M]j,2"
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "",
          "indexes of utterances in a conversation, and they": "j∈N[M]i,2"
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "Considering that graph G contains multiple type",
          "indexes of utterances in a conversation, and they": ""
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "",
          "indexes of utterances in a conversation, and they": "(cid:88)"
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "",
          "indexes of utterances in a conversation, and they": "+\n)\natec\ni,j W mec,n−1hn−1"
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "node representations, we set five edge types respec-",
          "indexes of utterances in a conversation, and they": "[M]j,1"
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "",
          "indexes of utterances in a conversation, and they": "(5)"
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "",
          "indexes of utterances in a conversation, and they": "j∈N[M]i,1"
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "tively to model\nthe dependency relations among",
          "indexes of utterances in a conversation, and they": ""
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "",
          "indexes of utterances in a conversation, and they": "(cid:88)"
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "different nodes. The former three edges are con-",
          "indexes of utterances in a conversation, and they": "+\natuc\n+ bn−1)\ni,j W uc,n−1hn−1"
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "structed between the slot nodes to slot nodes, i.e.,",
          "indexes of utterances in a conversation, and they": "j∈Nui"
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "h[M]1 ↔ h[M]1, h[M]1 ↔ h[M]2 and h[M]2 ↔ h[M]2,",
          "indexes of utterances in a conversation, and they": ""
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "",
          "indexes of utterances in a conversation, and they": "where\ndenote\nthe\nneighbor\n{N[M]i,1, N[M]i,2}"
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "which are represented with tee, tec and tcc respec-",
          "indexes of utterances in a conversation, and they": ""
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "",
          "indexes of utterances in a conversation, and they": "nodes\nof\ntokens\nand\nrespectively.\n[M]1\n[M]2"
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "tively. The fourth edge type is constructed from",
          "indexes of utterances in a conversation, and they": ""
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "",
          "indexes of utterances in a conversation, and they": "= hf\n= hf\nh0\n, h0\n. When the"
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "",
          "indexes of utterances in a conversation, and they": "[M]j,1\n[M]j,2"
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "utterance node to slot node, i.e., hu ↔ h[M]2, rep-",
          "indexes of utterances in a conversation, and they": "[M]j,1\n[M]j,2"
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "",
          "indexes of utterances in a conversation, and they": "model comes to the top node [M]i,1, its representa-"
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "resented by tuc. The last is from utterance node to",
          "indexes of utterances in a conversation, and they": ""
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "",
          "indexes of utterances in a conversation, and they": "tion is aggregated by the top, and the middle nodes"
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "utterance node, i.e., hu ↔ hu, denoted by tuu. The",
          "indexes of utterances in a conversation, and they": ""
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "",
          "indexes of utterances in a conversation, and they": "in the context window, which is given by:"
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "subscripts “e” and “c” in edge type represent [M]1",
          "indexes of utterances in a conversation, and they": ""
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "and [M]2, respectively, and “u” represents the utter-",
          "indexes of utterances in a conversation, and they": ""
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "",
          "indexes of utterances in a conversation, and they": "(cid:88)"
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "ance. For one edge type t ∈ {tee, tec, tcc, tuc, tuu},",
          "indexes of utterances in a conversation, and they": "hn\natee\ni,j W ee,n−1hn−1"
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "",
          "indexes of utterances in a conversation, and they": "[M]i,1 = ReLU(\n[M]j,1"
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "its adjacent matrix is given as:",
          "indexes of utterances in a conversation, and they": "j∈N[M]i,1"
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "",
          "indexes of utterances in a conversation, and they": "(6)"
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "",
          "indexes of utterances in a conversation, and they": "(cid:88)"
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "",
          "indexes of utterances in a conversation, and they": "+\natec\n+ bn−1)\ni,j W ec,n−1hn−1"
        },
        {
          "3.4\nTask-specific Hierarchical Context (THC)": "(cid:40)",
          "indexes of utterances in a conversation, and they": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 1: The statistics of MELD, IEMOCAP, Con- Luetal.,2020),andweuseaccuracy(ACC)and",
      "data": [
        {
          "MELD\n9989\n1108\n2610\n13707": "IEMOCAP\n5354\n528\n1650\n7532",
          "ECF. ECF contains 1,344 conversations and 13,509": "utterances. The detailed statistics of four datasets"
        },
        {
          "MELD\n9989\n1108\n2610\n13707": "ConvECPE\n5303\n486\n1644\n7433",
          "ECF. ECF contains 1,344 conversations and 13,509": ""
        },
        {
          "MELD\n9989\n1108\n2610\n13707": "",
          "ECF. ECF contains 1,344 conversations and 13,509": "are shown in Table 1. For datasets IEMOCAP and"
        },
        {
          "MELD\n9989\n1108\n2610\n13707": "ECF\n9457\n1351\n2701\n13509",
          "ECF. ECF contains 1,344 conversations and 13,509": ""
        },
        {
          "MELD\n9989\n1108\n2610\n13707": "",
          "ECF. ECF contains 1,344 conversations and 13,509": "MELD, we follow previous works (Li et al., 2021c;"
        },
        {
          "MELD\n9989\n1108\n2610\n13707": "Table 1:\nThe statistics of MELD,\nIEMOCAP, Con-",
          "ECF. ECF contains 1,344 conversations and 13,509": "Lu et al., 2020), and we use accuracy (ACC) and"
        },
        {
          "MELD\n9989\n1108\n2610\n13707": "vECPE, and ECF.",
          "ECF. ECF contains 1,344 conversations and 13,509": "weighted F1 (WF1) as the evaluation metric for the"
        },
        {
          "MELD\n9989\n1108\n2610\n13707": "",
          "ECF. ECF contains 1,344 conversations and 13,509": "MERC task. For datasets ECF and ConvECPE, we"
        },
        {
          "MELD\n9989\n1108\n2610\n13707": "",
          "ECF. ECF contains 1,344 conversations and 13,509": "use precision (P), recall (R), and F1 as the evalua-"
        },
        {
          "MELD\n9989\n1108\n2610\n13707": "3.5\nGrounding Mask Predictions to MERC",
          "ECF. ECF contains 1,344 conversations and 13,509": ""
        },
        {
          "MELD\n9989\n1108\n2610\n13707": "",
          "ECF. ECF contains 1,344 conversations and 13,509": "tion metric for the MECPE task."
        },
        {
          "MELD\n9989\n1108\n2610\n13707": "and MECPE",
          "ECF. ECF contains 1,344 conversations and 13,509": ""
        },
        {
          "MELD\n9989\n1108\n2610\n13707": "",
          "ECF. ECF contains 1,344 conversations and 13,509": "4.2\nBaselines"
        },
        {
          "MELD\n9989\n1108\n2610\n13707": "We use hN\nto predict MERC task,\ni.e.,\nthe",
          "ECF. ECF contains 1,344 conversations and 13,509": ""
        },
        {
          "MELD\n9989\n1108\n2610\n13707": "[M]i,1",
          "ECF. ECF contains 1,344 conversations and 13,509": ""
        },
        {
          "MELD\n9989\n1108\n2610\n13707": "",
          "ECF. ECF contains 1,344 conversations and 13,509": "For MERC, the baselines can be grouped into three"
        },
        {
          "MELD\n9989\n1108\n2610\n13707": "answers of slot\nto predict\n[M]1, and use hN",
          "ECF. ECF contains 1,344 conversations and 13,509": ""
        },
        {
          "MELD\n9989\n1108\n2610\n13707": "[M]i,2",
          "ECF. ECF contains 1,344 conversations and 13,509": ""
        },
        {
          "MELD\n9989\n1108\n2610\n13707": "",
          "ECF. ECF contains 1,344 conversations and 13,509": "categories:\n1)the methods focusing on emotion"
        },
        {
          "MELD\n9989\n1108\n2610\n13707": "MECPE task,\ni.e.,\nthe answers of slot [M]2. The",
          "ECF. ECF contains 1,344 conversations and 13,509": ""
        },
        {
          "MELD\n9989\n1108\n2610\n13707": "",
          "ECF. ECF contains 1,344 conversations and 13,509": "cues like EmoCaps (Li et al., 2022b), FacialMMT-"
        },
        {
          "MELD\n9989\n1108\n2610\n13707": "predictions of [M]1 (i.e., ˆye\ni ) and [M]1 (i.e., ˆyc\ni ) are",
          "ECF. ECF contains 1,344 conversations and 13,509": ""
        },
        {
          "MELD\n9989\n1108\n2610\n13707": "",
          "ECF. ECF contains 1,344 conversations and 13,509": "RoBERTa (Zheng et al., 2023), MVN (Li et al.,"
        },
        {
          "MELD\n9989\n1108\n2610\n13707": "given as respectively:",
          "ECF. ECF contains 1,344 conversations and 13,509": ""
        },
        {
          "MELD\n9989\n1108\n2610\n13707": "",
          "ECF. ECF contains 1,344 conversations and 13,509": "2021c). These works aim to improve model perfor-"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 1: The statistics of MELD, IEMOCAP, Con- Luetal.,2020),andweuseaccuracy(ACC)and",
      "data": [
        {
          "Train\nValid\nTest\nAll\nDatasets": "MELD\n9989\n1108\n2610\n13707",
          "then constructed multimodal emotion cause dataset": "ECF. ECF contains 1,344 conversations and 13,509"
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "IEMOCAP\n5354\n528\n1650\n7532",
          "then constructed multimodal emotion cause dataset": "utterances. The detailed statistics of four datasets"
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "ConvECPE\n5303\n486\n1644\n7433",
          "then constructed multimodal emotion cause dataset": ""
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "",
          "then constructed multimodal emotion cause dataset": "are shown in Table 1. For datasets IEMOCAP and"
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "ECF\n9457\n1351\n2701\n13509",
          "then constructed multimodal emotion cause dataset": ""
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "",
          "then constructed multimodal emotion cause dataset": "MELD, we follow previous works (Li et al., 2021c;"
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "Table 1:\nThe statistics of MELD,\nIEMOCAP, Con-",
          "then constructed multimodal emotion cause dataset": "Lu et al., 2020), and we use accuracy (ACC) and"
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "vECPE, and ECF.",
          "then constructed multimodal emotion cause dataset": "weighted F1 (WF1) as the evaluation metric for the"
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "",
          "then constructed multimodal emotion cause dataset": "MERC task. For datasets ECF and ConvECPE, we"
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "",
          "then constructed multimodal emotion cause dataset": "use precision (P), recall (R), and F1 as the evalua-"
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "3.5\nGrounding Mask Predictions to MERC",
          "then constructed multimodal emotion cause dataset": ""
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "",
          "then constructed multimodal emotion cause dataset": "tion metric for the MECPE task."
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "and MECPE",
          "then constructed multimodal emotion cause dataset": ""
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "",
          "then constructed multimodal emotion cause dataset": "4.2\nBaselines"
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "We use hN\nto predict MERC task,\ni.e.,\nthe",
          "then constructed multimodal emotion cause dataset": ""
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "[M]i,1",
          "then constructed multimodal emotion cause dataset": ""
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "",
          "then constructed multimodal emotion cause dataset": "For MERC, the baselines can be grouped into three"
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "answers of slot\nto predict\n[M]1, and use hN",
          "then constructed multimodal emotion cause dataset": ""
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "[M]i,2",
          "then constructed multimodal emotion cause dataset": ""
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "",
          "then constructed multimodal emotion cause dataset": "categories:\n1)the methods focusing on emotion"
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "MECPE task,\ni.e.,\nthe answers of slot [M]2. The",
          "then constructed multimodal emotion cause dataset": ""
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "",
          "then constructed multimodal emotion cause dataset": "cues like EmoCaps (Li et al., 2022b), FacialMMT-"
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "predictions of [M]1 (i.e., ˆye\ni ) and [M]1 (i.e., ˆyc\ni ) are",
          "then constructed multimodal emotion cause dataset": ""
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "",
          "then constructed multimodal emotion cause dataset": "RoBERTa (Zheng et al., 2023), MVN (Li et al.,"
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "given as respectively:",
          "then constructed multimodal emotion cause dataset": ""
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "",
          "then constructed multimodal emotion cause dataset": "2021c). These works aim to improve model perfor-"
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "",
          "then constructed multimodal emotion cause dataset": "mance by tracking emotional states in a conversa-"
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "i = f (W ehN\n[M]i,1",
          "then constructed multimodal emotion cause dataset": "tion, and 2)the methods fusing multimodal\ninfor-"
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "(7)",
          "then constructed multimodal emotion cause dataset": ""
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "",
          "then constructed multimodal emotion cause dataset": "mation like QMNN (Li et al., 2021c), GA2MIF"
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "i = f (W chN\n[M]i,2",
          "then constructed multimodal emotion cause dataset": ""
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "",
          "then constructed multimodal emotion cause dataset": "(Li et al., 2023),MALN(Ren et al., 2023), Multi-"
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "",
          "then constructed multimodal emotion cause dataset": "EMO (Shi and Huang, 2023), and UniMSE (Hu"
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "i , ˆyc\ni } denote the prediction results for",
          "then constructed multimodal emotion cause dataset": ""
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "",
          "then constructed multimodal emotion cause dataset": "et al., 2022b). These works focus on better multi-"
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "MERC and MECPE tasks, respectively. Based on",
          "then constructed multimodal emotion cause dataset": ""
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "",
          "then constructed multimodal emotion cause dataset": "modal fusion, and 3)the methods incorporating con-"
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "the predictions, we use the sum of the cross-entropy",
          "then constructed multimodal emotion cause dataset": ""
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "",
          "then constructed multimodal emotion cause dataset": "text information like DialogueGCN (Ghosal et al.,"
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "losses of MERC and MECPE tasks as the objective",
          "then constructed multimodal emotion cause dataset": ""
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "",
          "then constructed multimodal emotion cause dataset": "2019), MMGCN (Hu et al., 2021c), MM-DFN"
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "loss of UniMEEC.",
          "then constructed multimodal emotion cause dataset": ""
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "",
          "then constructed multimodal emotion cause dataset": "(Hu et al., 2022a), BC-LSTM (Poria et al., 2017),"
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "4\nExperiments",
          "then constructed multimodal emotion cause dataset": "DialogueRNN (Majumder et al., 2019) and Itera-"
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "",
          "then constructed multimodal emotion cause dataset": "tiveERC (Lu et al., 2020). These works aggregate"
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "4.1\nDatasets",
          "then constructed multimodal emotion cause dataset": ""
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "",
          "then constructed multimodal emotion cause dataset": "the context to understand the whole conversation."
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "We conduct experiments on four publicly available",
          "then constructed multimodal emotion cause dataset": "MECPE has a few baselines due to MECPE"
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "benchmark datasets of MERC and MECPE. For",
          "then constructed multimodal emotion cause dataset": "only emerging for a relatively short\ntime. Most"
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "MERC task, its benchmark datasets include multi-",
          "then constructed multimodal emotion cause dataset": "baselines\naddress MECPE tasks based on two-"
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "modal emotionLines dataset (MELD) (Poria et al.,",
          "then constructed multimodal emotion cause dataset": "step frameworks of\nemotion-cause pair\nextrac-"
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "2019),\ninteractive emotional dyadic motion cap-",
          "then constructed multimodal emotion cause dataset": "tion in text,\nlike Joint-GCN (Li et al., 2022a),"
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "ture database (IEMOCAP) (Busso et al., 2008).",
          "then constructed multimodal emotion cause dataset": "Joint-Xatt(Li et al., 2022a) and Inter-EC(Li et al.,"
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "IEMOCAP consists of 7532 samples, and each",
          "then constructed multimodal emotion cause dataset": "2022a). CMulti-Bernoulli(Wang et al., 2021) carries"
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "sample is labeled with six emotions for emotion",
          "then constructed multimodal emotion cause dataset": "out a binary decision for each relative position to"
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "recognition,\nincluding happiness, sadness, anger,",
          "then constructed multimodal emotion cause dataset": "determine the cause utterance. CMultinomial (Wang"
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "neutral, excitement, and frustration. MELD con-",
          "then constructed multimodal emotion cause dataset": "et al., 2021) randomly selects a relative position"
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "tains 13,707 video clips of multi-party conversa-",
          "then constructed multimodal emotion cause dataset": "from all relative positions as the feature to extract"
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "tions, with labels following Ekman’s six universal",
          "then constructed multimodal emotion cause dataset": "emotion-cause pair. We produce some typical mul-"
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "emotions, including joy, sadness, fear, angry, sur-",
          "then constructed multimodal emotion cause dataset": "timodal methods based on their open source codes,"
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "prise and disgust. For MECPE task, its benchmark",
          "then constructed multimodal emotion cause dataset": "including MuLT (Tsai et al., 2019), MMGCN"
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "datasets\ninclude ConvECPE (Li et al., 2022a),",
          "then constructed multimodal emotion cause dataset": "(Hu et al., 2021c), MMDFN (Hu et al., 2022a),"
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "and emotion-cause-in-friends (ECF) (Wang et al.,",
          "then constructed multimodal emotion cause dataset": "UniMSE (Hu et al., 2022b) and GA2MIF (Li et al.,"
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "2021). ConvECPE is a multimodal emotion cause",
          "then constructed multimodal emotion cause dataset": "2023)."
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "dataset constructed based on IEMOCAP, in which",
          "then constructed multimodal emotion cause dataset": ""
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "",
          "then constructed multimodal emotion cause dataset": "4.3\nExperimental Settings"
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "each non-neutral utterance is labeled with the emo-",
          "then constructed multimodal emotion cause dataset": ""
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "tion cause.\nIt contains 151 dialogues with 7,433",
          "then constructed multimodal emotion cause dataset": "We use pre-trained BERT as the encoder of mul-"
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "utterances. Similarly, (Wang et al., 2021) annotated",
          "then constructed multimodal emotion cause dataset": "timodal causal prompt. ConvECPE and ECF are"
        },
        {
          "Train\nValid\nTest\nAll\nDatasets": "the emotion cause of each sample in MELD and",
          "then constructed multimodal emotion cause dataset": "constructed based on IEMOCAP and MELD re-"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 2: LLaMA 74.67 75.16 75.02 69.15",
      "data": [
        {
          "IEMOCAP": "",
          "MELD": ""
        },
        {
          "IEMOCAP": "Anger",
          "MELD": "Sadness"
        },
        {
          "IEMOCAP": "56.73",
          "MELD": "25.1"
        },
        {
          "IEMOCAP": "65.28",
          "MELD": "26.33"
        },
        {
          "IEMOCAP": "62.26",
          "MELD": "24.32"
        },
        {
          "IEMOCAP": "61.45",
          "MELD": "23.62"
        },
        {
          "IEMOCAP": "62.58",
          "MELD": "16.50"
        },
        {
          "IEMOCAP": "69.00",
          "MELD": "-"
        },
        {
          "IEMOCAP": "69.77",
          "MELD": "22.93"
        },
        {
          "IEMOCAP": "65.96",
          "MELD": "21.82"
        },
        {
          "IEMOCAP": "-",
          "MELD": "-"
        },
        {
          "IEMOCAP": "68.99",
          "MELD": "42.52"
        },
        {
          "IEMOCAP": "70.29",
          "MELD": "27.18"
        },
        {
          "IEMOCAP": "-",
          "MELD": "41.99"
        },
        {
          "IEMOCAP": "69.10",
          "MELD": "43.00"
        },
        {
          "IEMOCAP": "69.88",
          "MELD": "41.51"
        },
        {
          "IEMOCAP": "72.63",
          "MELD": "43.31"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 2: LLaMA 74.67 75.16 75.02 69.15",
      "data": [
        {
          "underline denote the previous SOTA performance.": ""
        },
        {
          "underline denote the previous SOTA performance.": "IEMOCAP\nMELD"
        },
        {
          "underline denote the previous SOTA performance.": "ACC\nWF1\nACC\nWF1"
        },
        {
          "underline denote the previous SOTA performance.": ""
        },
        {
          "underline denote the previous SOTA performance.": "BART\n73.59\n74.46\n74.69\n68.84"
        },
        {
          "underline denote the previous SOTA performance.": "T5\n74.32\n75.09\n74.93\n69.06"
        },
        {
          "underline denote the previous SOTA performance.": ""
        },
        {
          "underline denote the previous SOTA performance.": "LLaMA\n74.67\n75.16\n75.02\n69.15"
        },
        {
          "underline denote the previous SOTA performance.": ""
        },
        {
          "underline denote the previous SOTA performance.": ""
        },
        {
          "underline denote the previous SOTA performance.": "Table 3: Experimental results on IEMOCAP and MELD"
        },
        {
          "underline denote the previous SOTA performance.": ""
        },
        {
          "underline denote the previous SOTA performance.": "datasets with BART, T5 and LLaMA as backbone."
        },
        {
          "underline denote the previous SOTA performance.": ""
        },
        {
          "underline denote the previous SOTA performance.": ""
        },
        {
          "underline denote the previous SOTA performance.": "spectively, so we integrate the emotion and cause"
        },
        {
          "underline denote the previous SOTA performance.": "labels of IEMOCAP, MELD, ConvECPE and ECF"
        },
        {
          "underline denote the previous SOTA performance.": "to train the model. The batch size is 64, the learn-"
        },
        {
          "underline denote the previous SOTA performance.": "ing rate for BERT fine-tuning is set at 3e-4, and the"
        },
        {
          "underline denote the previous SOTA performance.": "learning rate for UniMEEC is set to 0.0001. The"
        },
        {
          "underline denote the previous SOTA performance.": "hidden dimension of acoustic and visual represen-"
        },
        {
          "underline denote the previous SOTA performance.": "tation is 64, the BERT embedding size is 768, and"
        },
        {
          "underline denote the previous SOTA performance.": "the fusion vector size is 768. We use the former"
        },
        {
          "underline denote the previous SOTA performance.": "9 Transformer layers of BERT as the text-specific"
        },
        {
          "underline denote the previous SOTA performance.": "prompt encoder, the following 10th and 11th as the"
        },
        {
          "underline denote the previous SOTA performance.": "audio-specific prompt encoder, and the last Trans-"
        },
        {
          "underline denote the previous SOTA performance.": "former layer of BERT as the video-specific prompt"
        },
        {
          "underline denote the previous SOTA performance.": "encoder. The THC module stacks two graph net-"
        },
        {
          "underline denote the previous SOTA performance.": "work layers, where the first layer has one attention"
        },
        {
          "underline denote the previous SOTA performance.": "head and the second layer has four attention heads."
        },
        {
          "underline denote the previous SOTA performance.": ""
        },
        {
          "underline denote the previous SOTA performance.": "4.4\nExperimental Environment"
        },
        {
          "underline denote the previous SOTA performance.": ""
        },
        {
          "underline denote the previous SOTA performance.": ""
        },
        {
          "underline denote the previous SOTA performance.": "All experiments are conducted in the NVIDIA RTX"
        },
        {
          "underline denote the previous SOTA performance.": ""
        },
        {
          "underline denote the previous SOTA performance.": "A100. We take BERT as the Transformer-based"
        },
        {
          "underline denote the previous SOTA performance.": ""
        },
        {
          "underline denote the previous SOTA performance.": "model, which has 110M parameters, including 12"
        },
        {
          "underline denote the previous SOTA performance.": ""
        },
        {
          "underline denote the previous SOTA performance.": "layers, 768 hidden dimensions, and 12 heads. We"
        },
        {
          "underline denote the previous SOTA performance.": ""
        },
        {
          "underline denote the previous SOTA performance.": "use the former Nt = 9 Transformer layers as the"
        },
        {
          "underline denote the previous SOTA performance.": "text-specific encoder, use the following Na = 2"
        },
        {
          "underline denote the previous SOTA performance.": "layers as\nthe audio-\nand Nv = 1 Transformer"
        },
        {
          "underline denote the previous SOTA performance.": "specific and video-specific encoders respectively."
        },
        {
          "underline denote the previous SOTA performance.": "The value of Nt, Na and Nv are determined by the"
        },
        {
          "underline denote the previous SOTA performance.": "model performance on valid test. Furthermore, we"
        },
        {
          "underline denote the previous SOTA performance.": "employ a linear decay learning rate schedule with"
        },
        {
          "underline denote the previous SOTA performance.": "a warm-up strategy."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 6: First, we remove the",
      "data": [
        {
          "Cause Recognition": "",
          "Pair Extraction": ""
        },
        {
          "Cause Recognition": "R",
          "Pair Extraction": "R"
        },
        {
          "Cause Recognition": "57.20",
          "Pair Extraction": "25.22"
        },
        {
          "Cause Recognition": "56.38",
          "Pair Extraction": "25.18"
        },
        {
          "Cause Recognition": "56.71",
          "Pair Extraction": "53.76"
        },
        {
          "Cause Recognition": "53.43",
          "Pair Extraction": "37.85"
        },
        {
          "Cause Recognition": "54.82",
          "Pair Extraction": "38.19"
        },
        {
          "Cause Recognition": "56.35",
          "Pair Extraction": "39.08"
        },
        {
          "Cause Recognition": "57.09",
          "Pair Extraction": "54.25"
        },
        {
          "Cause Recognition": "58.33",
          "Pair Extraction": "54.26"
        },
        {
          "Cause Recognition": "58.85",
          "Pair Extraction": "59.29"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 6: First, we remove the",
      "data": [
        {
          "Table 4: Results on ECF dataset. Cause recognition is to predict the location of cause utterance and pair extraction": ""
        },
        {
          "Table 4: Results on ECF dataset. Cause recognition is to predict the location of cause utterance and pair extraction": "The baselines with * are reproduced with their open sources."
        },
        {
          "Table 4: Results on ECF dataset. Cause recognition is to predict the location of cause utterance and pair extraction": ""
        },
        {
          "Table 4: Results on ECF dataset. Cause recognition is to predict the location of cause utterance and pair extraction": "Methods"
        },
        {
          "Table 4: Results on ECF dataset. Cause recognition is to predict the location of cause utterance and pair extraction": ""
        },
        {
          "Table 4: Results on ECF dataset. Cause recognition is to predict the location of cause utterance and pair extraction": "Joint-GCN(Joint-EC)(Li et al., 2022a)"
        },
        {
          "Table 4: Results on ECF dataset. Cause recognition is to predict the location of cause utterance and pair extraction": "Joint-Xatt(Joint-EC)(Li et al., 2022a)"
        },
        {
          "Table 4: Results on ECF dataset. Cause recognition is to predict the location of cause utterance and pair extraction": "Inter-EC(Li et al., 2022a)"
        },
        {
          "Table 4: Results on ECF dataset. Cause recognition is to predict the location of cause utterance and pair extraction": "MuLT*(Tsai et al., 2019)"
        },
        {
          "Table 4: Results on ECF dataset. Cause recognition is to predict the location of cause utterance and pair extraction": "MMGCN*(Hu et al., 2021c)"
        },
        {
          "Table 4: Results on ECF dataset. Cause recognition is to predict the location of cause utterance and pair extraction": "MM-DFN*(Hu et al., 2022a)"
        },
        {
          "Table 4: Results on ECF dataset. Cause recognition is to predict the location of cause utterance and pair extraction": "UniMSE*(Hu et al., 2022b)"
        },
        {
          "Table 4: Results on ECF dataset. Cause recognition is to predict the location of cause utterance and pair extraction": "GA2MIF*(Zheng et al., 2023)"
        },
        {
          "Table 4: Results on ECF dataset. Cause recognition is to predict the location of cause utterance and pair extraction": "UniMEEC(Ours)"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 6: First, we remove the",
      "data": [
        {
          "87.21\nUniMEEC(Ours)": "Table 5: Results on ConvECPE dataset. The baselines with italics indicate it only uses textual modality.",
          "92.95\n89.88\n50.61\n50.83\n69.48\n50.41": ""
        },
        {
          "87.21\nUniMEEC(Ours)": "4.6\nResults of Emotion-Cause Pair Extraction",
          "92.95\n89.88\n50.61\n50.83\n69.48\n50.41": "recognition, cause recognition, and emotion-cause"
        },
        {
          "87.21\nUniMEEC(Ours)": "",
          "92.95\n89.88\n50.61\n50.83\n69.48\n50.41": "pair extraction on the benchmarks of MERC and"
        },
        {
          "87.21\nUniMEEC(Ours)": "The results of cause recognition, pair extraction,",
          "92.95\n89.88\n50.61\n50.83\n69.48\n50.41": ""
        },
        {
          "87.21\nUniMEEC(Ours)": "",
          "92.95\n89.88\n50.61\n50.83\n69.48\n50.41": "MECPE,\nand 2)UniMEEC significantly outper-"
        },
        {
          "87.21\nUniMEEC(Ours)": "and emotion recognition on ECF and ConvECPE",
          "92.95\n89.88\n50.61\n50.83\n69.48\n50.41": ""
        },
        {
          "87.21\nUniMEEC(Ours)": "",
          "92.95\n89.88\n50.61\n50.83\n69.48\n50.41": "forms SOTA in most cases. The improvements il-"
        },
        {
          "87.21\nUniMEEC(Ours)": "datasets are shown in Table 4 and Table 5, respec-",
          "92.95\n89.88\n50.61\n50.83\n69.48\n50.41": ""
        },
        {
          "87.21\nUniMEEC(Ours)": "",
          "92.95\n89.88\n50.61\n50.83\n69.48\n50.41": "lustrate jointly training emotion and emotion cause"
        },
        {
          "87.21\nUniMEEC(Ours)": "tively. UniMEEC significantly outperforms SOTA",
          "92.95\n89.88\n50.61\n50.83\n69.48\n50.41": ""
        },
        {
          "87.21\nUniMEEC(Ours)": "",
          "92.95\n89.88\n50.61\n50.83\n69.48\n50.41": "can benefit\nthe two tasks, and the unified frame-"
        },
        {
          "87.21\nUniMEEC(Ours)": "in all metrics on ECF and most metrics on Con-",
          "92.95\n89.88\n50.61\n50.83\n69.48\n50.41": ""
        },
        {
          "87.21\nUniMEEC(Ours)": "",
          "92.95\n89.88\n50.61\n50.83\n69.48\n50.41": "work in modeling causality between emotion and"
        },
        {
          "87.21\nUniMEEC(Ours)": "vECPE datasets. For the ECF dataset, UniMEEC",
          "92.95\n89.88\n50.61\n50.83\n69.48\n50.41": ""
        },
        {
          "87.21\nUniMEEC(Ours)": "",
          "92.95\n89.88\n50.61\n50.83\n69.48\n50.41": "emotion cause can bring prior knowledge to MERC"
        },
        {
          "87.21\nUniMEEC(Ours)": "improves metrics P, R, and F of cause recognition",
          "92.95\n89.88\n50.61\n50.83\n69.48\n50.41": ""
        },
        {
          "87.21\nUniMEEC(Ours)": "",
          "92.95\n89.88\n50.61\n50.83\n69.48\n50.41": "and MECPE training."
        },
        {
          "87.21\nUniMEEC(Ours)": "by 2.11%, 0.52%, and 2.09%,\nrespectively, and",
          "92.95\n89.88\n50.61\n50.83\n69.48\n50.41": ""
        },
        {
          "87.21\nUniMEEC(Ours)": "P, R, and F of pair recognition by 0.45%, 5.03%,",
          "92.95\n89.88\n50.61\n50.83\n69.48\n50.41": ""
        },
        {
          "87.21\nUniMEEC(Ours)": "",
          "92.95\n89.88\n50.61\n50.83\n69.48\n50.41": "4.7\nAblation Study"
        },
        {
          "87.21\nUniMEEC(Ours)": "and 3.29% respectively. For the ConvECPE dataset,",
          "92.95\n89.88\n50.61\n50.83\n69.48\n50.41": ""
        },
        {
          "87.21\nUniMEEC(Ours)": "multimodal methods perform better than text-based",
          "92.95\n89.88\n50.61\n50.83\n69.48\n50.41": ""
        },
        {
          "87.21\nUniMEEC(Ours)": "",
          "92.95\n89.88\n50.61\n50.83\n69.48\n50.41": "We conducted extensive ablation studies on IEMO-"
        },
        {
          "87.21\nUniMEEC(Ours)": "ones. UniMEEC improves by at least 2% on most",
          "92.95\n89.88\n50.61\n50.83\n69.48\n50.41": ""
        },
        {
          "87.21\nUniMEEC(Ours)": "",
          "92.95\n89.88\n50.61\n50.83\n69.48\n50.41": "CAP and MELD datasets and experimental\nre-"
        },
        {
          "87.21\nUniMEEC(Ours)": "metrics for cause recognition and pair extraction.",
          "92.95\n89.88\n50.61\n50.83\n69.48\n50.41": ""
        },
        {
          "87.21\nUniMEEC(Ours)": "",
          "92.95\n89.88\n50.61\n50.83\n69.48\n50.41": "sults are shown in Table 6. First, we remove the"
        },
        {
          "87.21\nUniMEEC(Ours)": "Furthermore, we report the UniMEEC performance",
          "92.95\n89.88\n50.61\n50.83\n69.48\n50.41": ""
        },
        {
          "87.21\nUniMEEC(Ours)": "",
          "92.95\n89.88\n50.61\n50.83\n69.48\n50.41": "MECPE part\nin the prompt\ntemplate,\nand then"
        },
        {
          "87.21\nUniMEEC(Ours)": "of the emotion recognition task on two datasets (see",
          "92.95\n89.88\n50.61\n50.83\n69.48\n50.41": ""
        },
        {
          "87.21\nUniMEEC(Ours)": "",
          "92.95\n89.88\n50.61\n50.83\n69.48\n50.41": "train UniMEEC just using the emotion label as"
        },
        {
          "87.21\nUniMEEC(Ours)": "WF1 in Table 4 and Table 5), outperforming at least",
          "92.95\n89.88\n50.61\n50.83\n69.48\n50.41": ""
        },
        {
          "87.21\nUniMEEC(Ours)": "",
          "92.95\n89.88\n50.61\n50.83\n69.48\n50.41": "the supervision signal. The removal of MECPE"
        },
        {
          "87.21\nUniMEEC(Ours)": "5.34% and 2.12% improvements by the competitive",
          "92.95\n89.88\n50.61\n50.83\n69.48\n50.41": ""
        },
        {
          "87.21\nUniMEEC(Ours)": "",
          "92.95\n89.88\n50.61\n50.83\n69.48\n50.41": "from UniMEEC results\nin a performance drop"
        },
        {
          "87.21\nUniMEEC(Ours)": "baselines on ECF and ConvECPE, respectively.",
          "92.95\n89.88\n50.61\n50.83\n69.48\n50.41": ""
        },
        {
          "87.21\nUniMEEC(Ours)": "",
          "92.95\n89.88\n50.61\n50.83\n69.48\n50.41": "by 3.57% and 1.96% on IEMOCAP and MELD"
        },
        {
          "87.21\nUniMEEC(Ours)": "We summarize the improvements into two as-",
          "92.95\n89.88\n50.61\n50.83\n69.48\n50.41": "respectively, demonstrating that\njointly training"
        },
        {
          "87.21\nUniMEEC(Ours)": "pects: 1) UniMEEC achieves SOTA on emotion",
          "92.95\n89.88\n50.61\n50.83\n69.48\n50.41": "MERC and MECPE can bring improvements for"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "multi-modal emotion recognition and sentiment anal-",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": "ria, Niyati Chhaya, and Alexander F. Gelbukh. 2019."
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "ysis.\nIn Proceedings of the 2019 Conference of the",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": "Dialoguegcn: A graph convolutional neural network"
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "North American Chapter of the Association for Com-",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": "for emotion recognition in conversation.\nIn Proceed-"
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "putational Linguistics: Human Language Technolo-",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": "ings of the 2019 Conference on Empirical Methods"
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "gies, NAACL-HLT 2019, Minneapolis, MN, USA,",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": "in Natural Language Processing and the 9th Inter-"
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "June 2-7, 2019, Volume 1 (Long and Short Papers),",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": "national Joint Conference on Natural Language Pro-"
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "pages 370–379. Association for Computational Lin-",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": "cessing, EMNLP-IJCNLP 2019, Hong Kong, China,"
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "guistics.",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": "November 3-7, 2019, pages 154–164. Association for"
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": "Computational Linguistics."
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "Lei\nJimmy Ba,\nJamie Ryan Kiros,\nand Geoffrey E.",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": ""
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "Hinton.\n2016.\nLayer\nnormalization.\nCoRR,",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": ""
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": "Devamanyu Hazarika, Soujanya Poria, Roger Zim-"
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "abs/1607.06450.",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": ""
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": "mermann,\nand Rada Mihalcea. 2019.\nEmotion"
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": "recognition in conversations with transfer learning"
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "Roy F Baumeister and Joel Cooper. 1981.\nCan the",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": ""
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": "from generative\nconversation modeling.\nCoRR,"
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "public expectation of emotion cause that emotion? 1.",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": ""
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": "abs/1910.04980."
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "Journal of Personality, 49(1):49–59.",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": ""
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": "Dou Hu, Xiaolong Hou, Lingwei Wei, Lian-Xin Jiang,"
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "Kazemzadeh, Emily Mower, Samuel Kim,\nJean-",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": "and Yang Mo. 2022a. MM-DFN: multimodal dy-"
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "nette N. Chang, Sungbok Lee,\nand Shrikanth S.",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": "namic fusion network for emotion recognition in con-"
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "Narayanan. 2008.\nIEMOCAP: interactive emotional",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": "versations.\nIn IEEE International Conference on"
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "dyadic motion capture database. Lang. Resour. Eval-",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": "Acoustics, Speech and Signal Processing,\nICASSP"
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "uation, 42(4):335–359.",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": "2022, Virtual and Singapore, 23-27 May 2022, pages"
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": "7037–7041."
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "Michel Cabanac. 2002. What is emotion? Behavioural",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": ""
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "processes, 60(2):69–83.",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": ""
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": "Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu,"
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": "Yuchuan Wu, and Yongbin Li. 2022b. Unimse: To-"
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "Zhi Chen, Lu Chen, Bei Chen, Libo Qin, Yuncong Liu,",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": ""
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": "wards unified multimodal\nsentiment analysis and"
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "Su Zhu, Jian-Guang Lou, and Kai Yu. 2022. Unidu:",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": ""
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": "the 2022\nemotion recognition.\nIn Proceedings of"
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "Towards A unified generative dialogue understanding",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": ""
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": "Conference on Empirical Methods in Natural Lan-"
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "framework. CoRR, abs/2204.04637.",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": ""
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": "guage Processing, EMNLP 2022, Abu Dhabi, United"
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": "Arab Emirates, December 7-11, 2022, pages 7837–"
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "Jacob Devlin, Ming-Wei Chang, Kenton Lee,\nand",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": ""
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": "7851."
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "Kristina Toutanova. 2019.\nBERT: Pre-training of",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": ""
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "deep bidirectional transformers for language under-",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": ""
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": "Guimin Hu, Guangming Lu, and Yi Zhao. 2021a. Bidi-"
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "standing.\nIn Proceedings of the 2019 Conference of",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": ""
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": "rectional hierarchical attention networks based on"
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "the North American Chapter of the Association for",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": ""
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": "document-level context for emotion cause extraction."
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "Computational Linguistics: Human Language Tech-",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": ""
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": "In Findings of the Association for Computational Lin-"
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "nologies, Volume 1 (Long and Short Papers), pages",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": ""
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": "guistics: EMNLP 2021, Virtual Event / Punta Cana,"
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "4171–4186, Minneapolis, Minnesota. Association for",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": ""
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": "Dominican Republic, 16-20 November, 2021, pages"
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "Computational Linguistics.",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": ""
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": "558–568."
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "Bosheng Ding, Chengwei Qin, Linlin Liu, Yew Ken",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": ""
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "Chia, Boyang Li, Shafiq Joty, and Lidong Bing. 2023.",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": "Guimin Hu, Guangming Lu, and Yi Zhao. 2021b. FSS-"
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "Is GPT-3 a good data annotator?\nIn Proceedings",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": "GCN: A graph convolutional networks with fusion"
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "of\nthe 61st Annual Meeting of\nthe Association for",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": "of semantic and structure for emotion cause analysis."
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "Computational Linguistics (Volume 1: Long Papers),",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": "Knowl. Based Syst., 212:106584."
        },
        {
          "pak Bhattacharyya. 2019. Multi-task learning for": "ACL 2023, Toronto, Canada, July 9-14, 2023, pages",
          "Deepanway Ghosal, Navonil Majumder, Soujanya Po-": ""
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "Singh, and Ashutosh Modi. 2022. COGMEN: con-",
          "Proceedings of the 59th Annual Meeting of the Asso-": "ciation for Computational Linguistics and the 11th"
        },
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "textualized GNN based multimodal emotion recogni-",
          "Proceedings of the 59th Annual Meeting of the Asso-": "International Joint Conference on Natural Language"
        },
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "tion. CoRR, abs/2205.02455.",
          "Proceedings of the 59th Annual Meeting of the Asso-": "Processing, ACL/IJCNLP 2021,\n(Volume 1: Long"
        },
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "",
          "Proceedings of the 59th Annual Meeting of the Asso-": "Papers), Virtual Event, August 1-6, 2021, pages 4582–"
        },
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "Joosung Lee and Wooin Lee. 2021. Compm: Context",
          "Proceedings of the 59th Annual Meeting of the Asso-": "4597."
        },
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "modeling with speaker’s pre-trained memory track-",
          "Proceedings of the 59th Annual Meeting of the Asso-": ""
        },
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "ing for emotion recognition in conversation. CoRR,",
          "Proceedings of the 59th Annual Meeting of the Asso-": "Zaijing Li, Fengxiao Tang, Ming Zhao, and Yusen Zhu."
        },
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "abs/2108.11626.",
          "Proceedings of the 59th Annual Meeting of the Asso-": "2022b. Emocaps: Emotion capsule based model for"
        },
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "",
          "Proceedings of the 59th Annual Meeting of the Asso-": "conversational emotion recognition. arXiv preprint"
        },
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "S Lee, Sophia Yat Mei Lee, and Zhu. 2019. Emotion",
          "Proceedings of the 59th Annual Meeting of the Asso-": "arXiv:2203.13504."
        },
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "and Cause. Springer.",
          "Proceedings of the 59th Annual Meeting of the Asso-": ""
        },
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "",
          "Proceedings of the 59th Annual Meeting of the Asso-": "Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,"
        },
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "Mike Lewis, Yinhan Liu, Naman Goyal, Marjan",
          "Proceedings of the 59th Annual Meeting of the Asso-": "Hiroaki Hayashi, and Graham Neubig. 2023.\nPre-"
        },
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "Ghazvininejad, Abdelrahman Mohamed, Omer Levy,",
          "Proceedings of the 59th Annual Meeting of the Asso-": "train, prompt, and predict: A systematic survey of"
        },
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "Veselin\nStoyanov,\nand Luke Zettlemoyer.\n2020.",
          "Proceedings of the 59th Annual Meeting of the Asso-": "prompting methods in natural language processing."
        },
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "BART: denoising sequence-to-sequence pre-training",
          "Proceedings of the 59th Annual Meeting of the Asso-": "ACM Computing Surveys, 55(9):1–35."
        },
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "for natural language generation, translation, and com-",
          "Proceedings of the 59th Annual Meeting of the Asso-": ""
        },
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "prehension.\nIn Proceedings of the 58th Annual Meet-",
          "Proceedings of the 59th Annual Meeting of the Asso-": "Xiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du, Zhilin"
        },
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "ing of\nthe Association for Computational Linguis-",
          "Proceedings of the 59th Annual Meeting of the Asso-": "Yang, and Jie Tang. 2021.\nP-tuning v2:\nPrompt"
        },
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "tics, ACL 2020, Online, July 5-10, 2020, pages 7871–",
          "Proceedings of the 59th Annual Meeting of the Asso-": "tuning can be comparable to fine-tuning universally"
        },
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "7880.",
          "Proceedings of the 59th Annual Meeting of the Asso-": "across scales and tasks. CoRR, abs/2110.07602."
        },
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "Chengxi Li, Feiyu Gao, Jiajun Bu, Lu Xu, Xiang Chen,",
          "Proceedings of the 59th Annual Meeting of the Asso-": "Xin Lu, Yanyan Zhao, Yang Wu, Yijian Tian, Huipeng"
        },
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "Yu Gu, Zirui Shao, Qi Zheng, Ningyu Zhang, Yong-",
          "Proceedings of the 59th Annual Meeting of the Asso-": "Chen, and Bing Qin. 2020.\nAn iterative emotion"
        },
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "pan Wang, and Zhi Yu. 2021a. Sentiprompt: Senti-",
          "Proceedings of the 59th Annual Meeting of the Asso-": "interaction network for emotion recognition in con-"
        },
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "ment knowledge enhanced prompt-tuning for aspect-",
          "Proceedings of the 59th Annual Meeting of the Asso-": "versations.\nIn Proceedings of the 28th international"
        },
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "based sentiment analysis. CoRR, abs/2109.08306.",
          "Proceedings of the 59th Annual Meeting of the Asso-": "conference on computational linguistics, pages 4078–"
        },
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "",
          "Proceedings of the 59th Annual Meeting of the Asso-": "4088."
        },
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang",
          "Proceedings of the 59th Annual Meeting of the Asso-": ""
        },
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "Zeng. 2023.\nGa2mif: Graph and attention based",
          "Proceedings of the 59th Annual Meeting of the Asso-": "Zhiheng Lyu, Zhijing Jin, Fernando Gonzalez, Rada Mi-"
        },
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "two-stage multi-source information fusion for con-",
          "Proceedings of the 59th Annual Meeting of the Asso-": "halcea, Bernhard Schölkopf, and Mrinmaya Sachan."
        },
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "IEEE Transactions on\nversational emotion detection.",
          "Proceedings of the 59th Annual Meeting of the Asso-": "2024. CoRR, abs/2404.11055."
        },
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "Affective Computing.",
          "Proceedings of the 59th Annual Meeting of the Asso-": ""
        },
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "",
          "Proceedings of the 59th Annual Meeting of the Asso-": "Hui Ma, Jian Wang, Hongfei Lin, Xuejun Pan, Yijia"
        },
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "Jiangnan Li, Zheng Lin, Peng Fu, and Weiping Wang.",
          "Proceedings of the 59th Annual Meeting of the Asso-": "Zhang, and Zhihao Yang. 2022. A multi-view net-"
        },
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "2021b.\nPast, present, and future: Conversational",
          "Proceedings of the 59th Annual Meeting of the Asso-": "work for real-time emotion recognition in conversa-"
        },
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "emotion recognition through structural modeling of",
          "Proceedings of the 59th Annual Meeting of the Asso-": "tions. Knowledge-Based Systems, 236:107751."
        },
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "psychological knowledge.\nIn Findings of the Associ-",
          "Proceedings of the 59th Annual Meeting of the Asso-": ""
        },
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "ation for Computational Linguistics: EMNLP 2021,",
          "Proceedings of the 59th Annual Meeting of the Asso-": "Navonil Majumder, Soujanya Poria, Devamanyu Haz-"
        },
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "Virtual Event / Punta Cana, Dominican Republic, 16-",
          "Proceedings of the 59th Annual Meeting of the Asso-": "arika, Rada Mihalcea, Alexander F. Gelbukh, and"
        },
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "20 November, 2021, pages 1204–1214. Association",
          "Proceedings of the 59th Annual Meeting of the Asso-": "Erik Cambria. 2019.\nDialoguernn: An attentive"
        },
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "for Computational Linguistics.",
          "Proceedings of the 59th Annual Meeting of the Asso-": "RNN for emotion detection in conversations.\nIn The"
        },
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "",
          "Proceedings of the 59th Annual Meeting of the Asso-": "Thirty-Third AAAI Conference on Artificial Intelli-"
        },
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "Qiuchi Li, Dimitris Gkoumas, Alessandro Sordoni, Jian-",
          "Proceedings of the 59th Annual Meeting of the Asso-": "gence, AAAI 2019, The Thirty-First Innovative Ap-"
        },
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "Yun Nie, and Massimo Melucci. 2021c. Quantum-",
          "Proceedings of the 59th Annual Meeting of the Asso-": "plications of Artificial Intelligence Conference, IAAI"
        },
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "inspired neural network for conversational emotion",
          "Proceedings of the 59th Annual Meeting of the Asso-": "2019, The Ninth AAAI Symposium on Educational"
        },
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "recognition.\nIn Proceedings of the AAAI Conference",
          "Proceedings of the 59th Annual Meeting of the Asso-": "Advances in Artificial Intelligence, EAAI 2019, Hon-"
        },
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "on Artificial Intelligence, volume 35, pages 13270–",
          "Proceedings of the 59th Annual Meeting of the Asso-": "olulu, Hawaii, USA, January 27 - February 1, 2019,"
        },
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "13278.",
          "Proceedings of the 59th Annual Meeting of the Asso-": "pages 6818–6825."
        },
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "Wei Li, Yang Li, Vlad Pandelea, Mengshi Ge, Luyao",
          "Proceedings of the 59th Annual Meeting of the Asso-": "Yuzhao Mao, Guang Liu, Xiaojie Wang, Weiguo Gao,"
        },
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "Zhu, and Erik Cambria. 2022a.\nEcpec: Emotion-",
          "Proceedings of the 59th Annual Meeting of the Asso-": "and Xuan Li. 2021. Dialoguetrm: Exploring multi-"
        },
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "IEEE Trans-\ncause pair extraction in conversations.",
          "Proceedings of the 59th Annual Meeting of the Asso-": "modal emotional dynamics in a conversation.\nIn"
        },
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "actions on Affective Computing, pages 1–12.",
          "Proceedings of the 59th Annual Meeting of the Asso-": "Findings of the Association for Computational Lin-"
        },
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "",
          "Proceedings of the 59th Annual Meeting of the Asso-": "guistics: EMNLP 2021, Virtual Event / Punta Cana,"
        },
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "Wenyan Li, Xinyu Zhang,\nJiaang Li, Qiwei Peng,",
          "Proceedings of the 59th Annual Meeting of the Asso-": "Dominican Republic, 16-20 November, 2021, pages"
        },
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "Raphael Tang, Li Zhou, Weijia Zhang, Guimin Hu,",
          "Proceedings of the 59th Annual Meeting of the Asso-": "2694–2704."
        },
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "Yifei Yuan, Anders Søgaard,\net al. 2024.\nFood-",
          "Proceedings of the 59th Annual Meeting of the Asso-": ""
        },
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "ieqa: A multimodal dataset for fine-grained under-",
          "Proceedings of the 59th Annual Meeting of the Asso-": "Joel Marks. 1982. A theory of emotion. Philosophical"
        },
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "arXiv preprint\nstanding of chinese food culture.",
          "Proceedings of the 59th Annual Meeting of the Asso-": "Studies: An International Journal for Philosophy in"
        },
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "arXiv:2406.11030.",
          "Proceedings of the 59th Annual Meeting of the Asso-": "the Analytic Tradition, 42(2):227–242."
        },
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "Xiang Lisa Li and Percy Liang. 2021.\nPrefix-tuning:",
          "Proceedings of the 59th Annual Meeting of the Asso-": "Soujanya Poria, Erik Cambria, Devamanyu Hazarika,"
        },
        {
          "Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Vikram": "Optimizing continuous prompts for generation.\nIn",
          "Proceedings of the 59th Annual Meeting of the Asso-": "Navonil Majumder, Amir Zadeh, and Louis-Philippe"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Morency. 2017. Context-dependent sentiment anal-": "ysis in user-generated videos.\nIn Proceedings of the",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": "thinking model scaling for convolutional neural net-"
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "55th Annual Meeting of the Association for Compu-",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": "works.\nIn Proceedings of the 36th International Con-"
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "tational Linguistics, ACL 2017, Vancouver, Canada,",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": "ference on Machine Learning, ICML 2019, 9-15 June"
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "July 30 - August 4, Volume 1: Long Papers, pages",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": "2019, Long Beach, California, USA, volume 97 of"
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "873–883.",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": "Proceedings of Machine Learning Research, pages"
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": "6105–6114. PMLR."
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": ""
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "jumder, Gautam Naik, Erik Cambria, and Rada Mi-",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": ""
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": "Zhiliang Tian, Yinliang Wang, Yiping Song, Chi Zhang,"
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "halcea. 2019. MELD: A multimodal multi-party",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": ""
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": "Dongkyu Lee, Yingxiu Zhao, Dongsheng Li, and"
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "dataset for emotion recognition in conversations.\nIn",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": ""
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": "Nevin L Zhang. 2022.\nEmpathetic and emotion-"
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "Proceedings of the 57th Conference of the Associa-",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": ""
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": "ally positive conversation systems with an emotion-"
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "tion for Computational Linguistics, ACL 2019, Flo-",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": ""
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": "specific query-response memory.\nIn Findings of the"
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "rence, Italy, July 28- August 2, 2019, Volume 1: Long",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": ""
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": "Association for Computational Linguistics: EMNLP"
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "Papers, pages 527–536. Association for Computa-",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": ""
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": "2022, pages 6364–6376."
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "tional Linguistics.",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": ""
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier"
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "Yushan Qian, Bo Wang, Ting-En Lin, Yinhe Zheng,",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": ""
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": "Martinet, Marie-Anne Lachaux, Timothée Lacroix,"
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "Ying Zhu, Dongming Zhao, Yuexian Hou, Yuchuan",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": ""
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": "Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal"
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "Wu, and Yongbin Li. 2023.\nEmpathetic response",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": ""
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": "Azhar, Aurélien Rodriguez, Armand Joulin, Edouard"
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "generation via emotion cause transition graph.\nIn",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": ""
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": "Grave, and Guillaume Lample. 2023. Llama: Open"
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "ICASSP\n2023-2023\nIEEE\nInternational Confer-",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": ""
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": "and efficient\nfoundation language models.\nCoRR,"
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "ence on Acoustics, Speech and Signal Processing",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": ""
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": "abs/2302.13971."
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "(ICASSP), pages 1–5. IEEE.",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": ""
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": "Yao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang,"
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": ""
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": "J. Zico Kolter, Louis-Philippe Morency, and Ruslan"
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "Lee, Sharan Narang, Michael Matena, Yanqi Zhou,",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": ""
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": "Salakhutdinov. 2019. Multimodal\ntransformer\nfor"
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "Wei Li, and Peter J. Liu. 2020. Exploring the limits",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": ""
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": "unaligned multimodal language sequences.\nIn Pro-"
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "of transfer learning with a unified text-to-text trans-",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": ""
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": "ceedings of the 57th Conference of the Association"
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "former. J. Mach. Learn. Res., 21:140:1–140:67.",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": ""
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": "for Computational Linguistics, ACL 2019, Florence,"
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "Minjie Ren, Xiangdong Huang, Jing Liu, Ming Liu, Xu-",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": "Italy, July 28- August 2, 2019, Volume 1: Long Pa-"
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "anya Li, and An-An Liu. 2023. MALN: multimodal",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": "pers, pages 6558–6569. Association for Computa-"
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "adversarial learning network for conversational emo-",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": "tional Linguistics."
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "IEEE Trans. Circuits Syst. Video\ntion recognition.",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": ""
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "Technol., 33(11):6965–6980.",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob"
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": "Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz"
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "James A Russell. 1990. The preschooler’s understand-",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": "Kaiser, and Illia Polosukhin. 2017. Attention is all"
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "ing of the causes and consequences of emotion. Child",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": "you need.\nIn Advances in Neural Information Pro-"
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "Development, 61(6):1872–1881.",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": "cessing Systems 30: Annual Conference on Neural"
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": "Information Processing Systems 2017, December 4-9,"
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "Tao Shi and Shao-Lun Huang. 2023. Multiemo: An",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": ""
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": "2017, Long Beach, CA, USA, pages 5998–6008."
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "attention-based correlation-aware multimodal fusion",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": ""
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "framework for emotion recognition in conversations.",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": ""
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": "Petar Velickovic, Guillem Cucurull, Arantxa Casanova,"
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "the 61st Annual Meeting of\nthe\nIn Proceedings of",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": ""
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": "Adriana Romero, Pietro Liò, and Yoshua Bengio."
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "Association for Computational Linguistics (Volume",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": ""
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": "2018. Graph attention networks.\nIn 6th International"
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "1: Long Papers), ACL 2023, Toronto, Canada, July",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": ""
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": "Conference on Learning Representations, ICLR 2018,"
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "9-14, 2023, pages 14752–14766.",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": ""
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": "Vancouver, BC, Canada, April 30 - May 3, 2018,"
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": "Conference Track Proceedings."
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "Yusheng Su, Xiaozhi Wang, Yujia Qin, Chi-Min Chan,",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": ""
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "Yankai Lin, Huadong Wang, Kaiyue Wen, Zhiyuan",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": ""
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": "Fanfan Wang, Zixiang Ding, Rui Xia, Zhaoyu Li, and"
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "Liu, Peng Li, Juanzi Li, et al. 2021. On transferability",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": ""
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": "Jianfei Yu. 2021. Multimodal emotion-cause pair"
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "of prompt\ntuning for natural\nlanguage processing.",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": ""
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": "extraction in conversations. CoRR, abs/2110.08020."
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "arXiv preprint arXiv:2111.06719.",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": ""
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": "Rui Xia and Zixiang Ding. 2019. Emotion-cause pair"
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "Yang Sun, Nan Yu, and Guohong Fu. 2021. A discourse-",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": ""
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": "extraction: A new task to emotion analysis in texts."
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "aware graph neural network for emotion recognition",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": ""
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": "In Proceedings of the 57th Conference of the Asso-"
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "in multi-party conversation.\nIn Findings of the Asso-",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": ""
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": "ciation for Computational Linguistics, ACL 2019,"
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "ciation for Computational Linguistics: EMNLP 2021,",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": ""
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": "Florence, Italy, July 28- August 2, 2019, Volume 1:"
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "Virtual Event / Punta Cana, Dominican Republic, 16-",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": ""
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": "Long Papers, pages 1003–1012."
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "20 November, 2021, pages 2949–2958. Association",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": ""
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "for Computational Linguistics.",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": ""
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": "Tianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong,"
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "Yueqing Sun, Yu Zhang, Le Qi, and Qi Shi. 2022. Tsgp:",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": "Torsten Scholak, Michihiro Yasunaga, Chien-Sheng"
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "Two-stage generative prompting for unsupervised",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": "Wu, Ming Zhong, Pengcheng Yin, Sida I. Wang, Vic-"
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "arXiv preprint\ncommonsense question answering.",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": "tor Zhong, Bailin Wang, Chengzu Li, Connor Boyle,"
        },
        {
          "Morency. 2017. Context-dependent sentiment anal-": "arXiv:2211.13515.",
          "Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-": "Ansong Ni, Ziyu Yao, Dragomir R. Radev, Caiming"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith,": "Luke Zettlemoyer, and Tao Yu. 2022. Unifiedskg:"
        },
        {
          "Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith,": "Unifying and multi-tasking structured knowledge"
        },
        {
          "Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith,": "grounding with text-to-text language models. CoRR,"
        },
        {
          "Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith,": "abs/2201.05966."
        },
        {
          "Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith,": "Xiaocui Yang, Shi Feng, Daling Wang, Sun Qi, Wenfang"
        },
        {
          "Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith,": "Wu, Yifei Zhang, Pengfei Hong, and Soujanya Poria."
        },
        {
          "Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith,": "2023. Few-shot\njoint multimodal aspect-sentiment"
        },
        {
          "Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith,": "analysis based on generative multimodal prompt."
        },
        {
          "Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith,": "arXiv preprint arXiv:2305.10169."
        },
        {
          "Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith,": "Hanlei Zhang, Hua Xu, and Ting-En Lin. 2021a. Deep"
        },
        {
          "Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith,": "open intent\nclassification with\nadaptive\ndecision"
        },
        {
          "Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith,": "boundary. Proceedings of the AAAI Conference on"
        },
        {
          "Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith,": "Artificial Intelligence, 35(16):14374–1438."
        },
        {
          "Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith,": "Hanlei Zhang, Hua Xu, Ting-En Lin,\nand Rui Lyu."
        },
        {
          "Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith,": "2021b. Discovering new intents with deep aligned"
        },
        {
          "Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith,": "clustering. Proceedings of the AAAI Conference on"
        },
        {
          "Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith,": "Artificial Intelligence, 35(16):14365–14373."
        },
        {
          "Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith,": "Zhengkun Zhang, Xiaojun Meng, Yasheng Wang, Xin"
        },
        {
          "Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith,": "Jiang, Qun Liu, and Zhenglu Yang. 2022. Unims:"
        },
        {
          "Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith,": "A unified framework for multimodal summarization"
        },
        {
          "Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith,": "with knowledge distillation.\nIn Thirty-Sixth AAAI"
        },
        {
          "Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith,": "Conference on Artificial\nIntelligence, AAAI 2022,"
        },
        {
          "Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith,": "Thirty-Fourth Conference on Innovative Applications"
        },
        {
          "Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith,": "of Artificial Intelligence,\nIAAI 2022, The Twelveth"
        },
        {
          "Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith,": "Symposium on Educational Advances in Artificial In-"
        },
        {
          "Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith,": "telligence, EAAI 2022 Virtual Event, February 22 -"
        },
        {
          "Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith,": "March 1, 2022, pages 11757–11764."
        },
        {
          "Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith,": "Wenjie Zheng, Jianfei Yu, Rui Xia, and Shijin Wang."
        },
        {
          "Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith,": "2023. A facial expression-aware multimodal multi-"
        },
        {
          "Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith,": "task learning framework for emotion recognition in"
        },
        {
          "Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith,": "multi-party conversations.\nIn Proceedings of the 61st"
        },
        {
          "Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith,": "Annual Meeting of the Association for Computational"
        },
        {
          "Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith,": "Linguistics (Volume 1: Long Papers), pages 15445–"
        },
        {
          "Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith,": "15459."
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Cause Recognition": "R",
          "Pair Extraction": "R"
        },
        {
          "Cause Recognition": "54.39",
          "Pair Extraction": "58.68"
        },
        {
          "Cause Recognition": "",
          "Pair Extraction": ""
        },
        {
          "Cause Recognition": "56.28",
          "Pair Extraction": "56.57"
        },
        {
          "Cause Recognition": "56.41",
          "Pair Extraction": "56.47"
        },
        {
          "Cause Recognition": "58.54",
          "Pair Extraction": "58.36"
        },
        {
          "Cause Recognition": "56.77",
          "Pair Extraction": "56.54"
        },
        {
          "Cause Recognition": "59.47",
          "Pair Extraction": "59.06"
        },
        {
          "Cause Recognition": "59.63",
          "Pair Extraction": "58.32"
        },
        {
          "Cause Recognition": "56.68",
          "Pair Extraction": "56.10"
        },
        {
          "Cause Recognition": "56.36",
          "Pair Extraction": "57.26"
        },
        {
          "Cause Recognition": "58.33",
          "Pair Extraction": "57.48"
        },
        {
          "Cause Recognition": "56.63",
          "Pair Extraction": "56.41"
        }
      ],
      "page": 14
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Multi-task learning for multi-modal emotion recognition and sentiment analysis",
      "authors": [
        "Shad Akhtar",
        "Dushyant Singh Chauhan",
        "Deepanway Ghosal",
        "Soujanya Poria",
        "Asif Ekbal",
        "Push-Pak Bhattacharyya"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019",
      "doi": "10.18653/v1/n19-1034"
    },
    {
      "citation_id": "2",
      "title": "",
      "authors": [
        "Corr"
      ],
      "venue": ""
    },
    {
      "citation_id": "3",
      "title": "Can the public expectation of emotion cause that emotion? 1",
      "authors": [
        "F Roy",
        "Joel Baumeister",
        "Cooper"
      ],
      "year": "1981",
      "venue": "Journal of Personality"
    },
    {
      "citation_id": "4",
      "title": "IEMOCAP: interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth Narayanan"
      ],
      "year": "2008",
      "venue": "Lang. Resour. Evaluation",
      "doi": "10.1007/s10579-008-9076-6"
    },
    {
      "citation_id": "5",
      "title": "What is emotion? Behavioural processes",
      "authors": [
        "Michel Cabanac"
      ],
      "year": "2002",
      "venue": "What is emotion? Behavioural processes"
    },
    {
      "citation_id": "6",
      "title": "Unidu: Towards A unified generative dialogue understanding framework",
      "authors": [
        "Zhi Chen",
        "Lu Chen",
        "Bei Chen",
        "Libo Qin",
        "Yuncong Liu",
        "Su Zhu",
        "Jian-Guang Lou",
        "Kai Yu"
      ],
      "year": "2022",
      "venue": "Unidu: Towards A unified generative dialogue understanding framework",
      "doi": "10.48550/arXiv.2204.04637"
    },
    {
      "citation_id": "7",
      "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/N19-1423"
    },
    {
      "citation_id": "8",
      "title": "Is GPT-3 a good data annotator?",
      "authors": [
        "Bosheng Ding",
        "Chengwei Qin",
        "Linlin Liu",
        "Ken Yew",
        "Boyang Chia",
        "Shafiq Li",
        "Lidong Joty",
        "Bing"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/V1/2023.ACL-LONG.626"
    },
    {
      "citation_id": "9",
      "title": "Emotions as cause and the cause of emotions. The language of emotions",
      "authors": [
        "René Dirven"
      ],
      "year": "1997",
      "venue": "Emotions as cause and the cause of emotions. The language of emotions"
    },
    {
      "citation_id": "10",
      "title": "E-core: Emotion correlation enhanced empathetic dialogue generation",
      "authors": [
        "Fengyi Fu",
        "Lei Zhang",
        "Quan Wang",
        "Zhendong Mao"
      ],
      "year": "2023",
      "venue": "E-core: Emotion correlation enhanced empathetic dialogue generation",
      "arxiv": "arXiv:2311.15016"
    },
    {
      "citation_id": "11",
      "title": "COSMIC: commonsense knowledge for emotion identification in conversations",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Alexander Gelbukh",
        "Rada Mihalcea",
        "Soujanya Poria"
      ],
      "year": "2020",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020",
      "doi": "10.18653/v1/2020.findings-emnlp.224"
    },
    {
      "citation_id": "12",
      "title": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Soujanya Poria",
        "Niyati Chhaya",
        "Alexander Gelbukh"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019",
      "doi": "10.18653/v1/D19-1015"
    },
    {
      "citation_id": "13",
      "title": "Emotion recognition in conversations with transfer learning from generative conversation modeling",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Roger Zimmermann",
        "Rada Mihalcea"
      ],
      "year": "2019",
      "venue": "Emotion recognition in conversations with transfer learning from generative conversation modeling"
    },
    {
      "citation_id": "14",
      "title": "2022a. MM-DFN: multimodal dynamic fusion network for emotion recognition in conversations",
      "authors": [
        "Dou Hu",
        "Xiaolong Hou",
        "Lingwei Wei",
        "Lian-Xin Jiang",
        "Yang Mo"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2022, Virtual and Singapore",
      "doi": "10.1109/ICASSP43922.2022.9747397"
    },
    {
      "citation_id": "15",
      "title": "2022b. Unimse: Towards unified multimodal sentiment analysis and emotion recognition",
      "authors": [
        "Guimin Hu",
        "Ting-En Lin",
        "Yi Zhao",
        "Guangming Lu",
        "Yuchuan Wu",
        "Yongbin Li"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/V1/2022.EMNLP-MAIN.534"
    },
    {
      "citation_id": "16",
      "title": "2021a. Bidirectional hierarchical attention networks based on document-level context for emotion cause extraction",
      "authors": [
        "Guimin Hu",
        "Guangming Lu",
        "Yi Zhao"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana",
      "doi": "10.18653/v1/2021.findings-emnlp.51"
    },
    {
      "citation_id": "17",
      "title": "2021b. FSS-GCN: A graph convolutional networks with fusion of semantic and structure for emotion cause analysis",
      "authors": [
        "Guimin Hu",
        "Guangming Lu",
        "Yi Zhao"
      ],
      "venue": "Knowl. Based Syst",
      "doi": "10.1016/j.knosys.2020.106584"
    },
    {
      "citation_id": "18",
      "title": "Recent trends of multimodal affective computing: A survey from nlp perspective",
      "authors": [
        "Guimin Hu",
        "Yi Xin",
        "Weimin Lyu",
        "Haojian Huang",
        "Chang Sun",
        "Zhihong Zhu",
        "Lin Gui",
        "Ruichu Cai"
      ],
      "year": "2024",
      "venue": "Recent trends of multimodal affective computing: A survey from nlp perspective",
      "arxiv": "arXiv:2409.07388"
    },
    {
      "citation_id": "19",
      "title": "MMGCN: multimodal fusion via deep graph convolution network for emotion recognition in conversation",
      "authors": [
        "Jingwen Hu",
        "Yuchen Liu",
        "Jinming Zhao",
        "Qin Jin"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021",
      "doi": "10.18653/v1/2021.acl-long.440"
    },
    {
      "citation_id": "20",
      "title": "COGMEN: contextualized GNN based multimodal emotion recognition",
      "authors": [
        "Abhinav Joshi",
        "Ashwani Bhat",
        "Ayush Jain"
      ],
      "year": "2022",
      "venue": "COGMEN: contextualized GNN based multimodal emotion recognition",
      "doi": "10.48550/arXiv.2205.02455"
    },
    {
      "citation_id": "21",
      "title": "Compm: Context modeling with speaker's pre-trained memory tracking for emotion recognition in conversation",
      "authors": [
        "Joosung Lee",
        "Wooin Lee"
      ],
      "year": "2021",
      "venue": "Compm: Context modeling with speaker's pre-trained memory tracking for emotion recognition in conversation"
    },
    {
      "citation_id": "22",
      "title": "Emotion and Cause",
      "authors": [
        "Sophia Lee",
        "Mei Lee"
      ],
      "year": "2019",
      "venue": "Emotion and Cause"
    },
    {
      "citation_id": "23",
      "title": "BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
      "authors": [
        "Mike Lewis",
        "Yinhan Liu",
        "Naman Goyal",
        "Marjan Ghazvininejad",
        "Abdelrahman Mohamed",
        "Omer Levy",
        "Veselin Stoyanov",
        "Luke Zettlemoyer"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020",
      "doi": "10.18653/v1/2020.acl-main.703"
    },
    {
      "citation_id": "24",
      "title": "2021a. Sentiprompt: Sentiment knowledge enhanced prompt-tuning for aspectbased sentiment analysis",
      "authors": [
        "Chengxi Li",
        "Feiyu Gao",
        "Jiajun Bu",
        "Lu Xu",
        "Xiang Chen",
        "Yu Gu",
        "Zirui Shao",
        "Qi Zheng",
        "Ningyu Zhang",
        "Yongpan Wang",
        "Zhi Yu"
      ],
      "venue": "2021a. Sentiprompt: Sentiment knowledge enhanced prompt-tuning for aspectbased sentiment analysis"
    },
    {
      "citation_id": "25",
      "title": "Ga2mif: Graph and attention based two-stage multi-source information fusion for conversational emotion detection",
      "authors": [
        "Jiang Li",
        "Xiaoping Wang",
        "Guoqing Lv",
        "Zhigang Zeng"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "26",
      "title": "2021b. Past, present, and future: Conversational emotion recognition through structural modeling of psychological knowledge",
      "authors": [
        "Jiangnan Li",
        "Zheng Lin",
        "Peng Fu",
        "Weiping Wang"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana",
      "doi": "10.18653/v1/2021.findings-emnlp.104"
    },
    {
      "citation_id": "27",
      "title": "2021c. Quantuminspired neural network for conversational emotion recognition",
      "authors": [
        "Qiuchi Li",
        "Dimitris Gkoumas",
        "Alessandro Sordoni",
        "Jian-Yun Nie",
        "Massimo Melucci"
      ],
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "28",
      "title": "Ecpec: Emotioncause pair extraction in conversations",
      "authors": [
        "Wei Li",
        "Yang Li",
        "Vlad Pandelea",
        "Mengshi Ge",
        "Luyao Zhu",
        "Erik Cambria"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2022.3216551"
    },
    {
      "citation_id": "29",
      "title": "Foodieqa: A multimodal dataset for fine-grained understanding of chinese food culture",
      "authors": [
        "Wenyan Li",
        "Xinyu Zhang",
        "Jiaang Li",
        "Qiwei Peng",
        "Raphael Tang",
        "Li Zhou",
        "Weijia Zhang",
        "Guimin Hu",
        "Yifei Yuan",
        "Anders Søgaard"
      ],
      "year": "2024",
      "venue": "Foodieqa: A multimodal dataset for fine-grained understanding of chinese food culture",
      "arxiv": "arXiv:2406.11030"
    },
    {
      "citation_id": "30",
      "title": "Prefix-tuning: Optimizing continuous prompts for generation",
      "authors": [
        "Lisa Xiang",
        "Percy Li",
        "Liang"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021",
      "doi": "10.18653/V1/2021.ACL-LONG.353"
    },
    {
      "citation_id": "31",
      "title": "2022b. Emocaps: Emotion capsule based model for conversational emotion recognition",
      "authors": [
        "Zaijing Li",
        "Fengxiao Tang",
        "Ming Zhao",
        "Yusen Zhu"
      ],
      "venue": "2022b. Emocaps: Emotion capsule based model for conversational emotion recognition",
      "arxiv": "arXiv:2203.13504"
    },
    {
      "citation_id": "32",
      "title": "Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing",
      "authors": [
        "Pengfei Liu",
        "Weizhe Yuan",
        "Jinlan Fu",
        "Zhengbao Jiang",
        "Hiroaki Hayashi",
        "Graham Neubig"
      ],
      "year": "2023",
      "venue": "ACM Computing Surveys"
    },
    {
      "citation_id": "33",
      "title": "P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks",
      "authors": [
        "Xiao Liu",
        "Kaixuan Ji",
        "Yicheng Fu",
        "Zhengxiao Du",
        "Zhilin Yang",
        "Jie Tang"
      ],
      "year": "2021",
      "venue": "P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks"
    },
    {
      "citation_id": "34",
      "title": "An iterative emotion interaction network for emotion recognition in conversations",
      "authors": [
        "Xin Lu",
        "Yanyan Zhao",
        "Yang Wu",
        "Yijian Tian",
        "Huipeng Chen",
        "Bing Qin"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th international conference on computational linguistics"
    },
    {
      "citation_id": "35",
      "title": "",
      "authors": [
        "Zhiheng Lyu",
        "Zhijing Jin",
        "Fernando Gonzalez",
        "Rada Mihalcea",
        "Bernhard Schölkopf",
        "Mrinmaya Sachan"
      ],
      "year": "2024",
      "venue": "",
      "doi": "10.48550/ARXIV.2404.11055"
    },
    {
      "citation_id": "36",
      "title": "A multi-view network for real-time emotion recognition in conversations",
      "authors": [
        "Hui Ma",
        "Jian Wang",
        "Hongfei Lin",
        "Xuejun Pan",
        "Yijia Zhang",
        "Zhihao Yang"
      ],
      "year": "2022",
      "venue": "A multi-view network for real-time emotion recognition in conversations"
    },
    {
      "citation_id": "37",
      "title": "Dialoguernn: An attentive RNN for emotion detection in conversations",
      "authors": [
        "Navonil Majumder",
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Rada Mihalcea",
        "Alexander Gelbukh",
        "Erik Cambria"
      ],
      "year": "2019",
      "venue": "The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence",
      "doi": "10.1609/aaai.v33i01.33016818"
    },
    {
      "citation_id": "38",
      "title": "Dialoguetrm: Exploring multimodal emotional dynamics in a conversation",
      "authors": [
        "Yuzhao Mao",
        "Guang Liu",
        "Xiaojie Wang",
        "Weiguo Gao",
        "Xuan Li"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana",
      "doi": "10.18653/v1/2021.findings-emnlp.229"
    },
    {
      "citation_id": "39",
      "title": "A theory of emotion",
      "authors": [
        "Joel Marks"
      ],
      "year": "1982",
      "venue": "Philosophical Studies: An International Journal for Philosophy in the Analytic Tradition"
    },
    {
      "citation_id": "40",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017",
      "doi": "10.18653/v1/P17-1081"
    },
    {
      "citation_id": "41",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Gautam Naik",
        "Erik Cambria",
        "Rada Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019",
      "doi": "10.18653/v1/p19-1050"
    },
    {
      "citation_id": "42",
      "title": "Empathetic response generation via emotion cause transition graph",
      "authors": [
        "Yushan Qian",
        "Bo Wang",
        "Ting-En Lin",
        "Yinhe Zheng",
        "Ying Zhu",
        "Dongming Zhao",
        "Yuexian Hou",
        "Yuchuan Wu",
        "Yongbin Li"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "43",
      "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "authors": [
        "Colin Raffel",
        "Noam Shazeer",
        "Adam Roberts",
        "Katherine Lee",
        "Sharan Narang",
        "Michael Matena",
        "Yanqi Zhou",
        "Wei Li",
        "Peter Liu"
      ],
      "year": "2020",
      "venue": "J. Mach. Learn. Res"
    },
    {
      "citation_id": "44",
      "title": "MALN: multimodal adversarial learning network for conversational emotion recognition",
      "authors": [
        "Minjie Ren",
        "Xiangdong Huang",
        "Jing Liu",
        "Ming Liu",
        "Xuanya Li",
        "An-An Liu"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Circuits Syst. Video Technol",
      "doi": "10.1109/TCSVT.2023.3273577"
    },
    {
      "citation_id": "45",
      "title": "The preschooler's understanding of the causes and consequences of emotion",
      "authors": [
        "Russell James"
      ],
      "year": "1990",
      "venue": "Child Development"
    },
    {
      "citation_id": "46",
      "title": "Multiemo: An attention-based correlation-aware multimodal fusion framework for emotion recognition in conversations",
      "authors": [
        "Tao Shi",
        "Shao-Lun Huang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/V1/2023.ACL-LONG.824"
    },
    {
      "citation_id": "47",
      "title": "On transferability of prompt tuning for natural language processing",
      "authors": [
        "Yusheng Su",
        "Xiaozhi Wang",
        "Yujia Qin",
        "Chi-Min Chan",
        "Yankai Lin",
        "Huadong Wang",
        "Kaiyue Wen",
        "Zhiyuan Liu",
        "Peng Li",
        "Juanzi Li"
      ],
      "year": "2021",
      "venue": "On transferability of prompt tuning for natural language processing",
      "arxiv": "arXiv:2111.06719"
    },
    {
      "citation_id": "48",
      "title": "A discourseaware graph neural network for emotion recognition in multi-party conversation",
      "authors": [
        "Yang Sun",
        "Nan Yu",
        "Guohong Fu"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana",
      "doi": "10.18653/v1/2021.findings-emnlp.252"
    },
    {
      "citation_id": "49",
      "title": "Efficientnet: Rethinking model scaling for convolutional neural networks",
      "authors": [
        "Yueqing Sun",
        "Yu Zhang",
        "Le Qi",
        "Qi Shi",
        "Mingxing Tan",
        "V Quoc",
        "Le"
      ],
      "year": "2019",
      "venue": "Tsgp: Two-stage generative prompting for unsupervised commonsense question answering",
      "arxiv": "arXiv:2211.13515"
    },
    {
      "citation_id": "50",
      "title": "Empathetic and emotionally positive conversation systems with an emotionspecific query-response memory",
      "authors": [
        "Zhiliang Tian",
        "Yinliang Wang",
        "Yiping Song",
        "Chi Zhang",
        "Dongkyu Lee",
        "Yingxiu Zhao",
        "Dongsheng Li",
        "Nevin Zhang"
      ],
      "year": "2022",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022"
    },
    {
      "citation_id": "51",
      "title": "Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models",
      "authors": [
        "Hugo Touvron",
        "Thibaut Lavril",
        "Gautier Izacard",
        "Xavier Martinet",
        "Marie-Anne Lachaux",
        "Timothée Lacroix",
        "Baptiste Rozière",
        "Naman Goyal",
        "Eric Hambro",
        "Faisal Azhar",
        "Aurélien Rodriguez",
        "Armand Joulin"
      ],
      "venue": "Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models",
      "doi": "10.48550/ARXIV.2302.13971"
    },
    {
      "citation_id": "52",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Shaojie Bai",
        "Paul Pu Liang",
        "J Kolter",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019",
      "doi": "10.18653/v1/p19-1656"
    },
    {
      "citation_id": "53",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Lukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "54",
      "title": "Graph attention networks",
      "authors": [
        "Petar Velickovic",
        "Guillem Cucurull",
        "Arantxa Casanova",
        "Adriana Romero",
        "Pietro Liò",
        "Yoshua Bengio"
      ],
      "year": "2018",
      "venue": "6th International Conference on Learning Representations, ICLR 2018"
    },
    {
      "citation_id": "55",
      "title": "Multimodal emotion-cause pair extraction in conversations",
      "authors": [
        "Fanfan Wang",
        "Zixiang Ding",
        "Rui Xia",
        "Zhaoyu Li",
        "Jianfei Yu"
      ],
      "year": "2021",
      "venue": "Multimodal emotion-cause pair extraction in conversations"
    },
    {
      "citation_id": "56",
      "title": "Emotion-cause pair extraction: A new task to emotion analysis in texts",
      "authors": [
        "Rui Xia",
        "Zixiang Ding"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019"
    },
    {
      "citation_id": "57",
      "title": "",
      "authors": [
        "Tianbao Xie",
        "Chen Wu",
        "Peng Shi",
        "Ruiqi Zhong",
        "Torsten Scholak",
        "Michihiro Yasunaga",
        "Chien-Sheng Wu",
        "Ming Zhong",
        "Pengcheng Yin",
        "I Sida",
        "Victor Wang",
        "Bailin Zhong",
        "Chengzu Wang",
        "Li"
      ],
      "venue": ""
    },
    {
      "citation_id": "58",
      "title": "Unifiedskg: Unifying and multi-tasking structured knowledge grounding with text-to-text language models",
      "authors": [
        "Lingpeng Xiong",
        "Rui Kong",
        "Noah Zhang",
        "Luke Smith",
        "Tao Zettlemoyer",
        "Yu"
      ],
      "year": "2022",
      "venue": "Unifiedskg: Unifying and multi-tasking structured knowledge grounding with text-to-text language models"
    },
    {
      "citation_id": "59",
      "title": "Few-shot joint multimodal aspect-sentiment analysis based on generative multimodal prompt",
      "authors": [
        "Xiaocui Yang",
        "Shi Feng",
        "Daling Wang",
        "Sun Qi",
        "Wenfang Wu",
        "Yifei Zhang",
        "Pengfei Hong",
        "Soujanya Poria"
      ],
      "year": "2023",
      "venue": "Few-shot joint multimodal aspect-sentiment analysis based on generative multimodal prompt",
      "arxiv": "arXiv:2305.10169"
    },
    {
      "citation_id": "60",
      "title": "2021a. Deep open intent classification with adaptive decision boundary",
      "authors": [
        "Hanlei Zhang",
        "Hua Xu",
        "Ting-En Lin"
      ],
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "61",
      "title": "2021b. Discovering new intents with deep aligned clustering",
      "authors": [
        "Hanlei Zhang",
        "Hua Xu",
        "Rui Ting-En Lin",
        "Lyu"
      ],
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "62",
      "title": "Unims: A unified framework for multimodal summarization with knowledge distillation",
      "authors": [
        "Zhengkun Zhang",
        "Xiaojun Meng",
        "Yasheng Wang",
        "Xin Jiang",
        "Qun Liu",
        "Zhenglu Yang"
      ],
      "year": "2022",
      "venue": "Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Conference on Innovative Applications of Artificial Intelligence, IAAI 2022, The Twelveth Symposium on Educational Advances in Artificial Intelligence, EAAI 2022 Virtual Event"
    },
    {
      "citation_id": "63",
      "title": "A facial expression-aware multimodal multitask learning framework for emotion recognition in multi-party conversations",
      "authors": [
        "Wenjie Zheng",
        "Jianfei Yu",
        "Rui Xia",
        "Shijin Wang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "64",
      "title": "Ueca-prompt: Universal prompt for emotion cause analysis",
      "authors": [
        "Xiaopeng Zheng",
        "Zhiyue Liu",
        "Zizhen Zhang",
        "Zhaoyang Wang",
        "Jiahai Wang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 29th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "65",
      "title": "Topic-driven and knowledgeaware transformer for dialogue emotion detection",
      "authors": [
        "Lixing Zhu",
        "Gabriele Pergola",
        "Lin Gui",
        "Deyu Zhou",
        "Yulan He"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021",
      "doi": "10.18653/v1/2021.acl-long.125"
    },
    {
      "citation_id": "66",
      "title": "Towards multimodal sarcasm detection via disentangled multigrained multi-modal distilling",
      "authors": [
        "Zhihong Zhu",
        "Xuxin Cheng",
        "Guimin Hu",
        "Yaowei Li",
        "Zhiqi Huang",
        "Yuexian Zou"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)"
    },
    {
      "citation_id": "67",
      "title": "Ablation study of UniMEEC on ECF dataset on cause recognition and pair extraction. T, V and A represent textual, visual and acoustic modalities, respectively. UPL and BPL denotes unimodal and bimodal causal prompts",
      "venue": "Ablation study of UniMEEC on ECF dataset on cause recognition and pair extraction. T, V and A represent textual, visual and acoustic modalities, respectively. UPL and BPL denotes unimodal and bimodal causal prompts"
    }
  ]
}