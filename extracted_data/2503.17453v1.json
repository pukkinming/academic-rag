{
  "paper_id": "2503.17453v1",
  "title": "Feature-Based Dual Visual Feature Extraction Model For Compound Multimodal Emotion Recognition",
  "published": "2025-03-21T18:03:44Z",
  "authors": [
    "Ran Liu",
    "Fengyu Zhang",
    "Cong Yu",
    "Longjiang Yang",
    "Zhuofan Wen",
    "Siyuan Zhang",
    "Hailiang Yao",
    "Shun Chen",
    "Zheng Lian",
    "Bin Liu"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This article presents our results for the eighth Affective Behavior Analysis in-the-wild (ABAW[1] [2]) competition.Multimodal emotion recognition (ER) has important applications in affective computing and human-computer interaction. However, in the real world, compound emotion recognition faces greater issues of uncertainty and modal conflicts. For the Compound Expression (CE) Recognition Challenge,this paper proposes a multimodal emotion recognition method that fuses the features of Vision Transformer (ViT) and Residual Network (ResNet). We conducted experiments on the C-EXPR-DB and MELD datasets. The results show that in scenarios with complex visual and audio cues (such as C-EXPR-DB), the model that fuses the features of ViT and ResNet exhibits superior performance.Our code are avalible onhttps: //github.com/MyGitHub-ax/8th_ABAW.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Accurate emotion analysis is crucial for the development of human-centered technologies. In recent years, significant progress has been made in affective behavior analysis in real-world, unconstrained environments. However, traditional methods still face challenges in the in-the-wild context. Multimodal emotion recognition (ER) has important applications in affective computing and human-computer interaction. However, in the real world, compound emotion recognition faces greater issues of uncertainty and modal conflicts. For the Compound Expression (CE) Recognition Challenge,this paper proposes a multimodal emotion recognition method that fuses the features of Vision Transformer (ViT) and Residual Network (ResNet). We conducted experiments on the C-EXPR-DB  [3]  and MELD  [4]  datasets.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Method",
      "text": "Emotion recognition has a wide range of applications in fields such as automatic behavior analysis, human-computer interaction, and health monitoring  [5] . Traditional research mainly focuses on basic emotions (such as anger, surprise, fear, etc.)  [6] . However, in real-world scenarios, human emotions are usually compound, that is, composed of multiple basic emotions  [7, 8] . For example, \"Fearfully Surprised\" is a combination of surprise and fear. Nevertheless, the difficulty of compound emotion recognition lies in its cross-modal uncertainty and conflict  [9] . This paper proposes a multimodal method that combines the features of ViT and ResNet. We propose a method to fuse the visual features of ViT and ResNet, which enhances the representational ability of visual features.This paper is based on the recent work on feature-based methods  [10] . It combines visual, audio, and textual information  [11] . In the visual modality, ResNet  [12]  is used for feature extraction. For the audio modality, the VGGish  [13]  model is employed to extract spectral features. In the textual modality, the encoding relies on BERT  [14] .\n\n• We propose a method to fuse the visual features of ViT and ResNet, which enhances the representational ability of visual features.\n\n• We conduct experiments on the C-EXPR-DB and MELD datasets, and systematically compare the feature-based and text-based methods.\n\n• By integrating a multimodal fusion strategy, we improve the robustness of the model in the compound emotion recognition task.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Dual Feature Extraction Module With Vision Transformer (Vit) And Resnet50",
      "text": "In recent years, ViT  [15]  has achieved breakthroughs in image understanding tasks. Compared with traditional CNNs, ViT can better capture global information. However, in emotion recognition, directly using ViT may lead to the loss of local details. Therefore, combining ResNet for multi -scale feature fusion has become an effective strategy. This paper utilizes ViT-Base and ResNet50 to extract visual features. ViT-Base extracts the feature F vit , where F vit ∈ R b×768 , with b representing the batch size. ResNet50 extracts the feature F resnet , where F resnet ∈ R b×512 . Finally, the two features are fused using concatenation:\n\nFinally, a linear projection layer is applied to reduce the fused feature dimension to 512:",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Feature-Based Modeling",
      "text": "This paper follows the latest research  [10]  to experiment with a feature-based approach for the Compound Emotion (CE) Recognition task (as shown in Fig.  1 ). Their method achieved excellent results in the recent ABAW challenge for the Compound Expression (CE) Recognition Challenge in videos. In our study, we modified the model to better suit our task.\n\nSpecifically, for the visual modality, we use ResNet50  [12]  and ViT  [15] . The ResNet50 model is pre-trained on the MS-CELEB1M  [16]  and FER+  [5]  datasets, while the ViT model is pre-trained on the ImageNet dataset. For the audio modality, we employ VGGish  [13] , and for the text modality, we utilize BERT  [14] . Additionally, we introduce a Temporal Convolutional Network (TCN)  [17]  after each feature extractor to further exploit temporal information.\n\nWe adopt the co-attention module  [10]  to attend to features from different modalities. This module constructs a single embedding per frame while leveraging a contextual window. The per-frame feature is then fed into a classifier head to predict emotions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Training Setting",
      "text": "We trained our model on the basic emotion dataset MELD and then tested it on the competition dataset C-EXPR-DB.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Experiment",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Dataset",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Evaluation Metric",
      "text": "For CE Challenge,We evaluate the performance of compound expressions recognition by the average F1 Score across all 7:\n\nAccording to the literature  [10] , we adopt the weighted F1 score as the evaluation metric for the MELD dataset. This metric can effectively mitigate the issue of class imbalance, similar to the approach used for the C-EXPR-DB dataset. It should be specifically noted that C-EXPR-DB follows the frame -level evaluation paradigm of C-EXPR-DB. In contrast, the MELD dataset only provides global video-level annotations (i.e., an entire video shares a single class label), so its evaluation needs to be conducted at the video level.\n\nFor the video-level prediction task, a three-stage post-processing strategy is used to integrate the framelevel prediction results:\n\n• Majority Voting Mechanism: The distribution of predicted classes for all frames is counted, and the class with the highest frequency is selected as the final video label.\n\n• Logits Mean Aggregation: The mean of logits for all frames is calculated for each class to construct a video -level logits vector. The class corresponding to the maximum value is taken as the prediction result.\n\n• Probability Mean Aggregation: The principle is similar to logits aggregation, but the operation is performed on the normalized class probability values.\n\n• Frame -Level Ensemble Technique In the ABAW CER competition, we utilize a time -windowbased cross -model ensemble method. The specific process is as follows: First, obtain the independent prediction results of each model on a single frame. Then, for each target frame t, construct a sliding window that covers the current frame and its previous 9 frames (with a total length of 10). By counting the distribution of classes predicted by all models within the window, the majority voting mechanism is used to determine the final label of frame t. This strategy can effectively integrate the temporal context information and the complementary advantages of multiple models.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we outline the approach we adopted for the Compound Expression (CE) Recognition Challenge of the 8th Asian Conference on Automatic Face and Gesture Recognition (ABAW). We have studied various pre-trained features from three common modalities: audio, visual, and text. We explored the application of pre-trained feature extraction. Specifically, we introduced a dual-feature extraction method that uses the Vision Transformer (ViT) and ResNet50 to extract visual features, employs BERT to extract text features, and utilizes VGGish to extract audio features. These features are then processed through a Temporal Convolutional Network (TCN) and fused at the feature level to obtain the final results.",
      "page_start": 1,
      "page_end": 1
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Feature-based Modeling.",
      "page": 2
    },
    {
      "caption": "Figure 1: ). Their method achieved excellent results in the",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Abstract": "This article presents our results for the eighth Af-",
          "MELD[4] datasets.": ""
        },
        {
          "Abstract": "fective Behavior Analysis\nin-the-wild (ABAW[1]\n[2])",
          "MELD[4] datasets.": "2\nMethod"
        },
        {
          "Abstract": "competition.Multimodal emotion recognition (ER) has",
          "MELD[4] datasets.": ""
        },
        {
          "Abstract": "",
          "MELD[4] datasets.": "Emotion recognition has\na wide\nrange\nof\nappli-"
        },
        {
          "Abstract": "important\napplications\nin\naffective\ncomputing\nand",
          "MELD[4] datasets.": ""
        },
        {
          "Abstract": "",
          "MELD[4] datasets.": "cations\nin fields\nsuch as automatic behavior analysis,"
        },
        {
          "Abstract": "human-computer\ninteraction.\nHowever,\nin\nthe\nreal",
          "MELD[4] datasets.": ""
        },
        {
          "Abstract": "",
          "MELD[4] datasets.": "human-computer\ninteraction,\nand\nhealth monitoring"
        },
        {
          "Abstract": "world, compound emotion recognition faces greater is-",
          "MELD[4] datasets.": ""
        },
        {
          "Abstract": "",
          "MELD[4] datasets.": "[5]. Traditional research mainly focuses on basic emo-"
        },
        {
          "Abstract": "sues of uncertainty and modal conflicts. For the Com-",
          "MELD[4] datasets.": ""
        },
        {
          "Abstract": "",
          "MELD[4] datasets.": "tions (such as anger, surprise,\nfear, etc.)\n[6]. However,"
        },
        {
          "Abstract": "pound Expression (CE) Recognition Challenge,this pa-",
          "MELD[4] datasets.": ""
        },
        {
          "Abstract": "",
          "MELD[4] datasets.": "in real-world scenarios,\nhuman emotions\nare usually"
        },
        {
          "Abstract": "per proposes a multimodal emotion recognition method",
          "MELD[4] datasets.": ""
        },
        {
          "Abstract": "",
          "MELD[4] datasets.": "compound,\nthat\nis,\ncomposed of multiple basic\nemo-"
        },
        {
          "Abstract": "that\nfuses\nthe\nfeatures\nof Vision Transformer\n(ViT)",
          "MELD[4] datasets.": ""
        },
        {
          "Abstract": "",
          "MELD[4] datasets.": "tions\n[7, 8].\nFor\nexample, ”Fearfully Surprised” is a"
        },
        {
          "Abstract": "and Residual Network (ResNet). We\nconducted ex-",
          "MELD[4] datasets.": ""
        },
        {
          "Abstract": "",
          "MELD[4] datasets.": "combination of\nsurprise\nand fear.\nNevertheless,\nthe"
        },
        {
          "Abstract": "periments on the C-EXPR-DB and MELD datasets.",
          "MELD[4] datasets.": ""
        },
        {
          "Abstract": "",
          "MELD[4] datasets.": "difficulty of compound emotion recognition lies\nin its"
        },
        {
          "Abstract": "The\nresults\nshow that\nin scenarios with complex vi-",
          "MELD[4] datasets.": ""
        },
        {
          "Abstract": "",
          "MELD[4] datasets.": "cross-modal uncertainty and conflict\n[9].\nThis paper"
        },
        {
          "Abstract": "sual and audio cues (such as C-EXPR-DB), the model",
          "MELD[4] datasets.": ""
        },
        {
          "Abstract": "",
          "MELD[4] datasets.": "proposes a multimodal method that combines the fea-"
        },
        {
          "Abstract": "that\nfuses\nthe\nfeatures\nof ViT and ResNet\nexhibits",
          "MELD[4] datasets.": ""
        },
        {
          "Abstract": "",
          "MELD[4] datasets.": "tures of ViT and ResNet. We propose a method to fuse"
        },
        {
          "Abstract": "superior performance.Our code are avalible onhttps:",
          "MELD[4] datasets.": ""
        },
        {
          "Abstract": "",
          "MELD[4] datasets.": "the visual features of ViT and ResNet, which enhances"
        },
        {
          "Abstract": "//github.com/MyGitHub-ax/8th_ABAW.",
          "MELD[4] datasets.": ""
        },
        {
          "Abstract": "",
          "MELD[4] datasets.": "the representational ability of visual\nfeatures.This pa-"
        },
        {
          "Abstract": "",
          "MELD[4] datasets.": "per is based on the recent work on feature-based meth-"
        },
        {
          "Abstract": "1\nIntroduction",
          "MELD[4] datasets.": "ods\n[10].\nIt\ncombines visual,\naudio,\nand textual\nin-"
        },
        {
          "Abstract": "",
          "MELD[4] datasets.": "formation [11].\nIn the visual modality, ResNet\n[12]\nis"
        },
        {
          "Abstract": "Accurate\nemotion analysis\nis\ncrucial\nfor\nthe de-",
          "MELD[4] datasets.": "used for\nfeature\nextraction.\nFor\nthe audio modality,"
        },
        {
          "Abstract": "velopment of human-centered technologies.\nIn recent",
          "MELD[4] datasets.": "the VGGish [13] model\nis employed to extract spectral"
        },
        {
          "Abstract": "years,\nsignificant progress has been made in affective",
          "MELD[4] datasets.": "features.\nIn the textual modality,\nthe encoding relies"
        },
        {
          "Abstract": "behavior analysis in real-world, unconstrained environ-",
          "MELD[4] datasets.": "on BERT [14]."
        },
        {
          "Abstract": "ments.\nHowever,\ntraditional methods\nstill\nface\nchal-",
          "MELD[4] datasets.": ""
        },
        {
          "Abstract": "",
          "MELD[4] datasets.": "• We propose a method to fuse the visual\nfeatures"
        },
        {
          "Abstract": "lenges\nin the\nin-the-wild context.\nMultimodal\nemo-",
          "MELD[4] datasets.": ""
        },
        {
          "Abstract": "",
          "MELD[4] datasets.": "of ViT and ResNet, which enhances the represen-"
        },
        {
          "Abstract": "tion recognition (ER) has\nimportant\napplications\nin",
          "MELD[4] datasets.": ""
        },
        {
          "Abstract": "",
          "MELD[4] datasets.": "tational ability of visual\nfeatures."
        },
        {
          "Abstract": "affective computing and human-computer\ninteraction.",
          "MELD[4] datasets.": ""
        },
        {
          "Abstract": "However, in the real world, compound emotion recogni-",
          "MELD[4] datasets.": ""
        },
        {
          "Abstract": "",
          "MELD[4] datasets.": "• We conduct experiments on the C-EXPR-DB and"
        },
        {
          "Abstract": "tion faces greater issues of uncertainty and modal con-",
          "MELD[4] datasets.": ""
        },
        {
          "Abstract": "",
          "MELD[4] datasets.": "MELD datasets, and systematically compare the"
        },
        {
          "Abstract": "flicts. For the Compound Expression (CE) Recognition",
          "MELD[4] datasets.": ""
        },
        {
          "Abstract": "",
          "MELD[4] datasets.": "feature-based and text-based methods."
        },
        {
          "Abstract": "Challenge,this paper proposes a multimodal\nemotion",
          "MELD[4] datasets.": ""
        },
        {
          "Abstract": "recognition method that\nfuses\nthe\nfeatures of Vision",
          "MELD[4] datasets.": "• By integrating a multimodal\nfusion strategy, we"
        },
        {
          "Abstract": "Transformer\n(ViT)\nand Residual Network\n(ResNet).",
          "MELD[4] datasets.": "improve the robustness of the model\nin the com-"
        },
        {
          "Abstract": "We conducted experiments on the C-EXPR-DB[3] and",
          "MELD[4] datasets.": "pound emotion recognition task."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 1: Feature-based Modeling.": "2.1\nDual\nFeature\nExtraction Module"
        },
        {
          "Figure 1: Feature-based Modeling.": ""
        },
        {
          "Figure 1: Feature-based Modeling.": "with Vision Transformer (ViT) and"
        },
        {
          "Figure 1: Feature-based Modeling.": ""
        },
        {
          "Figure 1: Feature-based Modeling.": "ResNet50"
        },
        {
          "Figure 1: Feature-based Modeling.": ""
        },
        {
          "Figure 1: Feature-based Modeling.": "In\nrecent\nyears, ViT [15]\nhas\nachieved\nbreak-"
        },
        {
          "Figure 1: Feature-based Modeling.": "throughs\nin\nimage\nunderstanding\ntasks.\nCompared"
        },
        {
          "Figure 1: Feature-based Modeling.": "with traditional CNNs, ViT can better capture global"
        },
        {
          "Figure 1: Feature-based Modeling.": "information. However,\nin emotion recognition, directly"
        },
        {
          "Figure 1: Feature-based Modeling.": "using ViT may lead to the loss of\nlocal details. There-"
        },
        {
          "Figure 1: Feature-based Modeling.": "fore,\ncombining ResNet\nfor multi\n-\nscale\nfeature\nfu-"
        },
        {
          "Figure 1: Feature-based Modeling.": "sion\nhas\nbecome\nan\neffective\nstrategy.\nThis\npaper"
        },
        {
          "Figure 1: Feature-based Modeling.": "utilizes ViT-Base and ResNet50 to extract visual\nfea-"
        },
        {
          "Figure 1: Feature-based Modeling.": "tures. ViT-Base extracts the feature Fvit, where Fvit ∈"
        },
        {
          "Figure 1: Feature-based Modeling.": "Rb×768, with b representing the batch size. ResNet50"
        },
        {
          "Figure 1: Feature-based Modeling.": "extracts the feature Fresnet, where Fresnet ∈ Rb×512. Fi-"
        },
        {
          "Figure 1: Feature-based Modeling.": "nally, the two features are fused using concatenation:"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 1: and Table 2 respectively show our results",
      "data": [
        {
          "emotions.": "",
          "length of 10).\nBy counting the distribution of": "classes predicted by all models within the win-"
        },
        {
          "emotions.": "2. MELD [4]:This\nis a basic emotion dataset. The",
          "length of 10).\nBy counting the distribution of": ""
        },
        {
          "emotions.": "",
          "length of 10).\nBy counting the distribution of": "dow,\nthe majority voting mechanism is used to"
        },
        {
          "emotions.": "dataset\nis\na multimodal\nemotion\nrecognition",
          "length of 10).\nBy counting the distribution of": ""
        },
        {
          "emotions.": "",
          "length of 10).\nBy counting the distribution of": "determine the final\nlabel of\nframe t. This strat-"
        },
        {
          "emotions.": "dataset primarily used for\naffective\ncomputing",
          "length of 10).\nBy counting the distribution of": ""
        },
        {
          "emotions.": "",
          "length of 10).\nBy counting the distribution of": "egy can effectively integrate the temporal context"
        },
        {
          "emotions.": "and multimodal\nemotion analysis\ntasks.\nIt\nis",
          "length of 10).\nBy counting the distribution of": ""
        },
        {
          "emotions.": "",
          "length of 10).\nBy counting the distribution of": "information and the complementary advantages"
        },
        {
          "emotions.": "extracted from the dialogues\nof\nthe TV series",
          "length of 10).\nBy counting the distribution of": ""
        },
        {
          "emotions.": "",
          "length of 10).\nBy counting the distribution of": "of multiple models."
        },
        {
          "emotions.": "*Friends*\nand contains\ninformation from three",
          "length of 10).\nBy counting the distribution of": ""
        },
        {
          "emotions.": "",
          "length of 10).\nBy counting the distribution of": "Table 1 and Table 2 respectively show our results"
        },
        {
          "emotions.": "modalities: visual, textual, and audio. The train-",
          "length of 10).\nBy counting the distribution of": ""
        },
        {
          "emotions.": "",
          "length of 10).\nBy counting the distribution of": "on the MELD and C-EXPR-DB datasets."
        },
        {
          "emotions.": "ing\nset,\nvalidation set,\nand test\nset\nconsist\nof",
          "length of 10).\nBy counting the distribution of": ""
        },
        {
          "emotions.": "9,988, 1,108, and 2,610 utterances, respectively.",
          "length of 10).\nBy counting the distribution of": ""
        },
        {
          "emotions.": "",
          "length of 10).\nBy counting the distribution of": "Prediction Method\nFeature-based"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: and Table 2 respectively show our results",
      "data": [
        {
          "tion result.": "• Probability Mean Aggregation: The principle is"
        },
        {
          "tion result.": ""
        },
        {
          "tion result.": "similar\nto logits aggregation, but\nthe operation"
        },
        {
          "tion result.": "is performed on the normalized class probability"
        },
        {
          "tion result.": "values."
        },
        {
          "tion result.": ""
        },
        {
          "tion result.": "• Frame - Level Ensemble Technique In the ABAW"
        },
        {
          "tion result.": ""
        },
        {
          "tion result.": "CER competition, we utilize a time - window -"
        },
        {
          "tion result.": ""
        },
        {
          "tion result.": "based cross - model ensemble method. The spe-"
        },
        {
          "tion result.": ""
        },
        {
          "tion result.": "cific process\nis as\nfollows:\nFirst, obtain the\nin-"
        },
        {
          "tion result.": "dependent prediction results of each model on a"
        },
        {
          "tion result.": "single frame. Then,\nfor each target frame t, con-"
        },
        {
          "tion result.": "struct a sliding window that covers\nthe current"
        },
        {
          "tion result.": "frame\nand its previous\n9\nframes\n(with a\ntotal"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "Aslam. Advancements\nin Affective and Behavior",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": "learning for image recognition.\nIn CVPR, 2016."
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "Analysis: The 8th ABAW Workshop and Compe-",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": ""
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": "[13] S. Hershey,\nS. Chaudhuri,\nD.\nEllis,\nJ. Gem-"
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "tition. 3 2025.",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": ""
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": "meke, A. Jansen, R. Moore, M. Plakal, D. Platt,"
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "[3] D. Kollias.\nMulti-label\ncompound\nexpression",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": "R. Saurous, B. Seybold, M. Slaney, R. Weiss, and"
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "recognition:\nC-expr\ndatabase & network.\nIn",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": "K. Wilson. Cnn architectures for large-scale audio"
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "CVPR, 2023.",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": "classification.\nIn ICASSP, 2017."
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": "[14] J. Devlin, M. Chang, K. Lee, and K. Toutanova."
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "[4] S. Poria, D. Hazarika, N. Majumder, G. Naik,",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": ""
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": "Bert: pre-training of deep bidirectional transform-"
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "E. Cambria,\nand R. Mihalcea. Meld: A multi-",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": ""
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": "ers for language understanding.\nIn NAACL-HLT,"
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "modal multi\n- party dataset\nfor emotion recogni-",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": ""
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": "pages 4171–4186, 2019."
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "tion in conversations.\nIn Conference of\nthe Asso-",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": ""
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "ciation for Computational Linguistics, pages 527–",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": ""
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": "[15] Bichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin"
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "536, 2019.",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": ""
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": "Wan, Peizhao Zhang, Masayoshi Tomizuka, Kurt"
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": "Keutzer, and P´eter Vajda.\nVisual\ntransformers:"
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "[5] K. Ezzameli and H. Mahersia. Emotion recogni-",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": ""
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": "Token-based image representation and processing"
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "tion from unimodal to multimodal analysis: A re-",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": ""
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": "for computer vision. ArXiv, abs/2006.03677, 2020."
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "view.\nInformation Fusion, 99:101847, 2023.",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": ""
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": "[16] Y. Guo, L. Zhang, Y. Hu, X. He,\nand J. Gao."
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "[6] S. Belharbi, M. Pedersoli, A. L. Koerich, S. Bacon,",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": ""
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": "Msceleb-1m: A dataset and benchmark for large-"
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "and E. Granger. Guided interpretable facial ex-",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": ""
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": "scale face recognition.\nIn ECCV, 2016."
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "pression recognition via spatial action unit cues. In",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": ""
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "International Conference on Automatic Face and",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": ""
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": "[17] S. Bai,\nJ. Kolter,\nand V. Koltun.\nAn\nempir-"
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "Gesture Recognition, 2024.",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": ""
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": "ical\nevaluation of\ngeneric\nconvolutional\nand re-"
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": "current networks\nfor\nsequence modeling.\nCoRR,"
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "[7] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia,",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": ""
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": "abs/1803.01271, 2018."
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "Abhinav Dhall, Shreya Ghosh, Chunchang Shao,",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": ""
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "and Guanyu Hu.\n7th abaw competition: Multi-",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": ""
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "task learning and compound expression recogni-",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": ""
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "tion. arXiv preprint arXiv:2407.03835, 2024.",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": ""
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "[8] Dimitrios Kollias, Viktoriia Sharmanska, and Ste-",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": ""
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "fanos Zafeiriou. Distribution matching for multi-",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": ""
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "task learning of classification tasks:\na large-scale",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": ""
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "study on faces & beyond.\nIn Proceedings of\nthe",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": ""
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "AAAI Conference on Artificial\nIntelligence, vol-",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": ""
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "ume 38, pages 2813–2821, 2024.",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": ""
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "[9] Y.\nJi,\nJ. Wang, Y. Gong,\nL. Zhang, Y. Zhu,",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": ""
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "H. Wang,\nJ.\nZhang,\nT.\nSakai,\nand Y. Yang.",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": ""
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "Map:\nmultimodal\nuncertainty-aware\nvision-",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": ""
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "language pre-training model.\nIn CVPR, 2023.",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": ""
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "[10] Nicolas\nRichet,\nSoufiane\nBelharbi,\nHaseeb",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": ""
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "Aslam, Meike Emilie Schadt, Manuela Gonz’alez-",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": ""
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "Gonz’alez, Gustave Cortal, Alessandro Lameiras",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": ""
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "Koerich,\nMarco\nPedersoli,\nAlain\nFinkel,\nSi-",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": ""
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "mon\nBacon,\nand\nEric Granger.\nTextualized",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": ""
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "and\nfeature-based models\nfor\ncompound multi-",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": ""
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "modal\nemotion recognition in the wild.\nArXiv,",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": ""
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "abs/2407.12927, 2024.",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": ""
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "[11] M. Aslam, M. Zeeshan, S. Belharbi, M. Pedersoli,",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": ""
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "A. Koerich, S. Bacon, and E. Granger. Distilling",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": ""
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "privileged multimodal\ninformation for expression",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": ""
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "recognition using optimal\ntransport.\nIn Interna-",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": ""
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "tional Conference on Automatic Face and Gesture",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": ""
        },
        {
          "Hu, Soufiane Belharbi,\nand Muhammad Haseeb": "Recognition, 2024.",
          "[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual": ""
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Advancements in affective and behavior analysis: The 8th abaw workshop and competition",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "Alan Cowen",
        "Irene Kotsia",
        "Eric Cogitat",
        "Marco Granger",
        "Simon Pedersoli",
        "Alice Bacon",
        "Chunchang Baird",
        "Shao"
      ],
      "venue": "Advancements in affective and behavior analysis: The 8th abaw workshop and competition"
    },
    {
      "citation_id": "2",
      "title": "Advancements in Affective and Behavior Analysis: The 8th ABAW Workshop and Competition",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "Alan Cowen",
        "Stefanos Zafeiriou",
        "Irene Kotsia",
        "Eric Granger",
        "Marco Pedersoli",
        "Simon Bacon",
        "Alice Baird",
        "Chris Gagne",
        "Chunchang Shao",
        "Guanyu Hu",
        "Soufiane Belharbi",
        "Muhammad Haseeb"
      ],
      "year": "2025",
      "venue": "Advancements in Affective and Behavior Analysis: The 8th ABAW Workshop and Competition"
    },
    {
      "citation_id": "3",
      "title": "Multi-label compound expression recognition: C-expr database & network",
      "authors": [
        "D Kollias"
      ],
      "year": "2023",
      "venue": "CVPR"
    },
    {
      "citation_id": "4",
      "title": "Meld: A multimodal multi -party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "Conference of the Association for Computational Linguistics"
    },
    {
      "citation_id": "5",
      "title": "Emotion recognition from unimodal to multimodal analysis: A review",
      "authors": [
        "K Ezzameli",
        "H Mahersia"
      ],
      "year": "2023",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "6",
      "title": "Guided interpretable facial expression recognition via spatial action unit cues",
      "authors": [
        "S Belharbi",
        "M Pedersoli",
        "A Koerich",
        "S Bacon",
        "E Granger"
      ],
      "year": "2024",
      "venue": "International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "7",
      "title": "7th abaw competition: Multitask learning and compound expression recognition",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou",
        "Irene Kotsia",
        "Abhinav Dhall",
        "Shreya Ghosh",
        "Chunchang Shao",
        "Guanyu Hu"
      ],
      "year": "2024",
      "venue": "7th abaw competition: Multitask learning and compound expression recognition",
      "arxiv": "arXiv:2407.03835"
    },
    {
      "citation_id": "8",
      "title": "Distribution matching for multitask learning of classification tasks: a large-scale study on faces & beyond",
      "authors": [
        "Dimitrios Kollias",
        "Viktoriia Sharmanska",
        "Stefanos Zafeiriou"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "9",
      "title": "Map: multimodal uncertainty-aware visionlanguage pre-training model",
      "authors": [
        "Y Ji",
        "J Wang",
        "Y Gong",
        "L Zhang",
        "Y Zhu",
        "H Wang",
        "J Zhang",
        "T Sakai",
        "Y Yang"
      ],
      "year": "2023",
      "venue": "CVPR"
    },
    {
      "citation_id": "10",
      "title": "Textualized and feature-based models for compound multimodal emotion recognition in the wild",
      "authors": [
        "Nicolas Richet",
        "Soufiane Belharbi",
        "Haseeb Aslam",
        "Emilie Meike",
        "Manuela Schadt",
        "Gustave Gonz'alez-Gonz'alez",
        "Alessandro Cortal",
        "Marco Koerich",
        "Alain Pedersoli",
        "Simon Finkel",
        "Eric Bacon",
        "Granger"
      ],
      "year": "2024",
      "venue": "Textualized and feature-based models for compound multimodal emotion recognition in the wild"
    },
    {
      "citation_id": "11",
      "title": "Distilling privileged multimodal information for expression recognition using optimal transport",
      "authors": [
        "M Aslam",
        "M Zeeshan",
        "S Belharbi",
        "M Pedersoli",
        "A Koerich",
        "S Bacon",
        "E Granger"
      ],
      "year": "2024",
      "venue": "International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "12",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "CVPR"
    },
    {
      "citation_id": "13",
      "title": "Cnn architectures for large-scale audio classification",
      "authors": [
        "S Hershey",
        "S Chaudhuri",
        "D Ellis",
        "J Gemmeke",
        "A Jansen",
        "R Moore",
        "M Plakal",
        "D Platt",
        "R Saurous",
        "B Seybold",
        "M Slaney",
        "R Weiss",
        "K Wilson"
      ],
      "year": "2017",
      "venue": "ICASSP"
    },
    {
      "citation_id": "14",
      "title": "Bert: pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "NAACL-HLT"
    },
    {
      "citation_id": "15",
      "title": "Visual transformers: Token-based image representation and processing for computer vision",
      "authors": [
        "Bichen Wu",
        "Chenfeng Xu",
        "Xiaoliang Dai",
        "Alvin Wan",
        "Peizhao Zhang",
        "Masayoshi Tomizuka",
        "Kurt Keutzer",
        "Péter Vajda"
      ],
      "year": "2020",
      "venue": "Visual transformers: Token-based image representation and processing for computer vision"
    },
    {
      "citation_id": "16",
      "title": "Msceleb-1m: A dataset and benchmark for largescale face recognition",
      "authors": [
        "Y Guo",
        "L Zhang",
        "Y Hu",
        "X He",
        "J Gao"
      ],
      "year": "2016",
      "venue": "ECCV"
    },
    {
      "citation_id": "17",
      "title": "An empirical evaluation of generic convolutional and recurrent networks for sequence modeling",
      "authors": [
        "S Bai",
        "J Kolter",
        "V Koltun"
      ],
      "year": "2018",
      "venue": "An empirical evaluation of generic convolutional and recurrent networks for sequence modeling"
    }
  ]
}