{
  "paper_id": "2203.14865v1",
  "title": "Towards Transferable Speech Emotion Representation: On Loss Functions For Cross-Lingual Latent Representations",
  "published": "2022-03-28T16:14:08Z",
  "authors": [
    "Sneha Das",
    "Nicole Nadine Lønfeldt",
    "Anne Katrine Pagsberg",
    "Line H. Clemmensen"
  ],
  "keywords": [
    "cross-lingual",
    "latent representation",
    "loss functions",
    "speech emotion recognition (SER)",
    "transfer learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In recent years, speech emotion recognition (SER) has been used in wide ranging applications, from healthcare to the commercial sector. In addition to signal processing approaches, methods for SER now also use deep learning techniques which provide transfer learning possibilities. However, generalizing over languages, corpora and recording conditions is still an open challenge. In this work we address this gap by exploring loss functions that aid in transferability, specifically to non-tonal languages. We propose a variational autoencoder (VAE) with KL annealing and a semi-supervised VAE to obtain more consistent latent embedding distributions across data sets. To ensure transferability, the distribution of the latent embedding should be similar across non-tonal languages (data sets). We start by presenting a low-complexity SER based on a denoisingautoencoder, which achieves an unweighted classification accuracy of over 52.09% for four-class emotion classification. This performance is comparable to that of similar baseline methods. Following this, we employ a VAE, the semi-supervised VAE and the VAE with KL annealing to obtain a more regularized latent space. We show that while the DAE has the highest classification accuracy among the methods, the semi-supervised VAE has a comparable classification accuracy and a more consistent latent embedding distribution over data sets. 1",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech emotion recognition (SER) refers to a group of algorithms that deduce the emotional state of an individual from their speech utterances. SER combined with affect recognition using other modalities, like vision and physiological signals, are deployed in a wide range of applications. For instance, in the detection and intervention of disorders in healthcare, monitoring the attentiveness of students in schools, risk assessment within the criminal justice system, and for commercial applications, like detecting customer satisfaction in call-centers and by employment agencies to find suitable candidates.\n\nState-of-the-art SER techniques have evolved from the more conventional signal processing and machine learning based methods to deep neural network based solutions  [1] . Classical methods were based on hidden Markov models (HMM), Gaussian mixture models (GMM), support vector machines (SVM) and decision trees. Contributions based on HMM employed energy and pitch features  [2]  showed high classification accuracy. Spectral, prosodic and energy features in tandem with a GMM and SVM were used 1 Link to code: https://bit.ly/34CgkSZ",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Input-Layer",
      "text": "Output-layer Bottleneck Fig.  1 . Illustration of the architecture employed for all the models explored in this work.\n\nto recognize emotions from Basque and Chinese datasets  [3, 4] , while pitch based input features were used to obtain a GMM and tested on more heterogeneous speech corpora  [5] . SVM is another popular method, either used as the primary classification tool, or in coordination with other techniques to predict the affect classes  [6] .\n\nSince the successful re-emergence of neural networks, more advanced methods that use deep learning have been used for SER. Long short-term memory (LSTM), bidirectional LSTM and recurrent neural networks (RNN) were used to predict the quadrant in the dimensional emotional model  [7] . Convolutional neural networks, having had immense success in computer vision, are a common architecture choice for neural network based SERs. Furthermore, unsupervision and semi-supervision techniques using autoencoders (AEs) and its regularized variants were used to learn a lower dimensional latent representation for the emotions, that were then employed at various levels to classify speech into emotional categories  [8, 9, 10] .\n\nLatent representation based methods, like the AE, are useful in the modelling of emotions from speech signals, because they can compress the input features to a smaller and ideally more target relevant latent embedding. This can also lead to better knowledge transfer between data sets by transferring generic emotions representations to unseen languages and corpora, hence addressing label shortage and in the process making the models relatively more interpretable. DAE was one of the earliest deep learning based unsupervised learning techniques for SER  [9] . This was followed by the use of sparse AE for feature transfer  [8]  and for SER on spontaneous data set  [11] . Furthermore, end-to-end representation learning for affect recognition from speech was proposed and showed performance comparable to existing methods  [12] . In recent years, techniques like variational and adversarial AEs and adversarial vari-arXiv:2203.14865v1 [eess.AS] 28 Mar 2022 ational Bayes have been exploited to learn the latent representations of speech emotions with input features ranging from the raw signals to hand crafted features  [13, 14, 15, 16] .\n\nTo successfully model the emotion representations, 1. the latent space should be discriminative of the emotion classes, 2. distribution of the latent embedding should be similar over different languages and corpora, such that the latent space is invariant to emotion-irrelevant factors while preserving linguistic and cultural differences in expressing emotions. Despite the use of AEs for SER in existing literature, few methods provide insights beyond classification accuracy on within and cross-corpus datasets. The contribution of this work is to present a method that fares well on both the aforementioned factors. We begin by presenting a low-complexity DAE for SER that achieves a performance similar to existing methods, as listed in Tab. 1. We show that while the DAE discovers a class-discriminative latent space, the distribution of the latent embedding is considerably different for different languages. Hence, to obtain a more regularized latent space, we employ a variational autoencoder (VAE). Since the latent space of a VAE is continuous and probabilistic, this approach is a step further towards a dimensional model of emotions, which proposes that a common process generates all affective states, thereby rendering emotions as a more continuous process  [17] . While a vanilla-VAE yields consistent latent distributions over cross-lingual corpora, the latent embedding is not discriminative of the emotion classes due to a dominant KL-loss. Therefore, we address the question: what is the optimal balance between the reconstruction loss and the regularization loss to obtain a class-discriminative and distribution-consistent latent representation. Towards that end, we explore 1. a VAE with KL-loss annealing using cyclic scheduling to improve class discrimination. 2. To further improve performance, we employ semi-supervised training of the VAE by incorporating a clustering loss in the learning function.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Methodology",
      "text": "The standard DAE and VAE: A denoising autoencoder can be interpreted as a regularized autoencoder, that directs the network to learn a latent embedding in a noise-free subspace that is representative of the clean input data, from a noisy input. The standard formulation of a DAE loss is:\n\n, the latent embedding is obtained as z = f θ (xn) and the reconstructed signal x = g φ (z), whereby f θ , g φ are the encoder and decoder, respectively. In a DAE, the latent representation is a point estimate of the training data, whereby the latent space is mostly discrete. According to the dimensional model of emotions and recent methods  [17, 18] , it is more appropriate to model emotions as a more continuous or an ordinal process rather than a categorical process. In order to obtain a more continuous representation of the emotions in the latent space, we employ a VAE. While a VAE is architecturally similar to an autoencoder, the learning goals are considerably different between the two. In other words, in addition to a reconstruction loss, which ensures a latent space representative of the input, a VAE uses the Kullback-Leibler (KL) loss to minimize the distance between the true and approximated posterior distribution using the KL-divergence (DKL). Optimal VAE parameters are obtained by\n\nwhere q θ (z|x), p(z) are the estimated posterior and the prior distributions, respectively.\n\nVAE with KL-annealing: Vanishing KL, also known as posterior collapse occurs when the KL-loss solely directs the minimization of the overall loss. In other words, the reconstruction loss remains consistent whereas the KL-loss reduces incrementally, eventually reducing to a small value. As a consequence, while the posterior distribution resembles the prior distribution with high fidelity (here a standard Gaussian), z is unrepresentative of data. This can be seen in Fig.  2 (b ). KL cost annealing with linear or cyclical scheduling is commonly employed to mitigate this issue, wherein a variable weight is added to the KL-loss term  [19] . We use cyclical annealing in this paper, and the VAE-annealing is expressed as:\n\nwhere the standard formulation of βe:\n\ne being the training epoch, the total number of cycles given by M = 2, T = 50 is the total number of epochs and R = 0.5 being the proportion of monotonic increase in an annealing cycle.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Vae With Semi-Supervision:",
      "text": "To obtain a latent representation where the emotion classes have improved discrimination while maintaining the regularized space, we incorporate a cluster loss, Lclus within the VAE. The cluster loss is obtained as the ratio of intra-class and inter-class distances of the latent embedding and minimizing the overall loss will lead to minimize the intra-class distances and maximize the inter-class distance. This is often referred to as the center-loss  [20] . The overall optimization problem is hence written as:\n\nLrec + βeLKL + γLclus,\n\nwhere k is the class under consideration, K is the total number of classes, z k is the mean z for class k and γ = 0.5.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiments And Results",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Datasets And Input Features:",
      "text": "We use the IEMOCAP data set, an audio-visual affect data set, to train and validate the models  [21] .\n\nThe data set comprises of annotations representing both the categorical and dimensional emotional model  [17] . To study how the latent representations are transferred between corpora, we use 1. the Surrey Audio-Visual Expresses Emotion (SAVEE) database that is primarily English and consists of male speakers only, 2. the Berlin Database of Emotional Speech (Emo-DB) recorded in German and, 3. the Canadian French Emotional (CaFE) speech database comprising of French audio samples  [22, 23, 24] ., the URDU-dataset consisting of Urdu speech, and the Acted Emotional Speech Dynamic (AESD) Database comprising of Greek speech  [25, 26] . Note that AESD does not comprise of samples from neutral emotion. In this work, we utilize the audio modality only, and train the models with data from the emotional categories neutral (N), sad (S), happy (H), angry (A). We use the extended Geneva minimalistic acoustic parameter set (eGeMAPS)  [27] , specifically the functionals of lower-level features. Each speech sample yields a feature vector comprised of 88 features and we use the OpenSmile toolkit to extract the features  [28] .\n\nSystem architecture: While AEs have been employed for SER in existing literature, their main focus was to propose novel network architectures that provide better classification accuracy  [11, 13, 16, 14, 9] . Since our goal is to explore the cross-lingual transferability of the latent representations, we adhere to the simplest architecture that provides comparable results to previous works. For a fair comparison between the performance of the models, we design all systems with identical architectures. The input feature dimension is 88 and we maintain the latent dimension size at z ∈ R 2×1 . In relation to past works, the primary source of low-complexity in our proposed architecture is from the latent dimension size. Since the functionals do not have a temporal correlation and the spatial correlation exists between the statistical parameters of a feature only, we use a fully connected neural network (NN) instead of a recurrent or convolutional type NN. The network architecture is illustrated in Fig.  1 . Preprocessing: Prior to using the data sets for training and testing, we remove the outliers by computing the z-score and eliminating the data samples that have a z-score, -10 > z > 10. We chose a threshold of 10 instead of the standard value of 3 because the goal of this work is to understand the behavior of the models for both typical and atypical rendition of emotions in speech. Therefore, we only remove the extreme outliers. Following this, the data sets are standardized to obtain a normal feature distribution.\n\nMethods and evaluation: We train and validate the four models: 1. DAE-vanilla representing a standard denoising autoencoder, 2. VAE-vanilla is a standard variational autoencoder, 3. VAE-annealing using KL-loss annealing from Eq. 3 and 4. VAE-ss employing semi-supervision from Eq. 5, using 5-fold cross-validation on the IEMOCAP database while the transfer data sets are identical over the iterations. The models were trained over 50 epochs and a batch size of 64, and we used the Adam optimizer with the learning rate set to 1e-3. In the following parts, we evaluate the latent embedding by using them as the input features to classify the speech samples into emotional categories using the support vector classifier (SVC) with a linear kernel. To evaluate the methods, we first present the overall classification accuracy to understand how our presented systems compare to each other. Following that, we compute the similarity of the latent distribution between the training and the cross-lingual transfer data sets.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Method Features+Dataset",
      "text": "Classes Accuracy GAN  [29]  eGeMAPS  [27] +EMO-DB 2 66% (UAR) FLUDA  [30]  IS10  [31] +IEMOCAP 4 50% (UA) VAE+LSTM  [13]  LogMel+IEMOCAP 4 56.08% (UA) AE+LSTM  [13]  LogMel+IEMOCAP 4 55.42% (UA) Stacked-AE+BLSTM-RNN  [12]  COVAREP+IEMOCAP  [   on a similar concept as that in this paper and their performance in Tab. 1. Classification and consistency results are provided in the following sections.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Classification Of Emotion Classes:",
      "text": "To study the limits of the methods, we evaluate the systems under the following two conditions wherein: 1) the transfer data sets are completely unseen, (2) we have access to 20% of the unlabeled transfer data sets, whereby we use them to normalize the input feature space. The mean and 95% CI of the balanced accuracy over 5-folds for conditions 1 and 2 are shown in Fig.  3  (a, b) and Fig.  3 (c, d ) respectively. As can be observed from the plot, the DAE-vanilla consistently performs well and is closely followed by VAE with semi-supervision, specifically under condition 2. This indicates that DAE is best at forming a discriminating latent space. We also observe that using a minor portion of the target data set to normalize the input feature space considerably improves the classification accuracy. This is a realistic assumption because open unlabelled language specific data sets can be used for this purpose. Furthermore, VAE-vanilla seems to have no better performance than a random classifier, which is due to posterior collapse as discussed in Sec. 2. VAE-annealing seems to perform better under the condition 1 for all transfer data sets, except for the URDU. On a general level, we observe that the models are able to discriminate between neutral, sad and anger better than between anger and happy. In other words, the models are unable to effectively discriminate between anger and happy, although they represent negative and positive emotions, respectively. This can be further observed in Fig.  2 (d) , wherein happy and anger samples overlap in the latent space. A probable reason for this could be that the latent representations correspond to activation more than valence.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Consistency Of Latent Distribution",
      "text": "We use the Bhattacharya distance to quantitatively evaluate the similarity between the latent embedding distribution. The Bhattacharya distance is given as BD = -log( x∈Z pref(z)pi(z)) between the kernel density estimates (KDEs) of the reference (training) dataset and the transfer dataset i. We present the logarithm of the BD in Fig.  4 ; lower BD value implies that the transfer data set is more similar to the reference data set. Furthermore, the scatter plots of the latent embedding for the four methods over the training and transfer data sets are illustrated in Fig.  5 . VAE-vanilla and VAE-ss are shown to have the lowest BD with respect to the training data set and this can also be observed in Fig.  5 (b, d ), where the scatter plots of all the transfer data sets overlap. While VAE-annealing and VAE-ss indicate an offset between the data sets in Fig.  5 (c, d ), the covariance for the data",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusions",
      "text": "In this work, we explored latent representation based methods for their ability to transfer emotion representations across cross-lingual data sets for non-tonal languages. Our objective was to find representations motivated from the dimensional emotion model, wherein emotions are processed on the relative and continuous scale. We evaluated the methods on class separability in the latent space and the consistency of the latent distribution over the data sets. We present a VAE with KL annealing and semi supervision incorporating cluster distance within the loss. The standard DAE yielded the latent embedding with highest classification accuracy implying high class discrimination. However, VAE with semi-supervision not only provided a comparable classification accuracy to the DAE, but also a more consistent latent space over the transfer datasets. Furthermore, a natural progression of this work would be to employ semisupervision using a continuous distance metric instead of a classification loss and is left for future work.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Illustration of the architecture employed for all the models",
      "page": 1
    },
    {
      "caption": "Figure 2: Scatterplots of the latent embedding of the training IEMO-",
      "page": 2
    },
    {
      "caption": "Figure 2: (b). KL cost annealing with linear or cyclical schedul-",
      "page": 2
    },
    {
      "caption": "Figure 1: Fig. 3. (1) Balanced accuracy on unseen transfer data sets using",
      "page": 3
    },
    {
      "caption": "Figure 4: Plot presents the mean of the logarithm of the Bhattacharya",
      "page": 4
    },
    {
      "caption": "Figure 3: (a, b) and Fig. 3 (c, d) respectively. As can be observed",
      "page": 4
    },
    {
      "caption": "Figure 2: (d), wherein happy and anger samples overlap in the latent",
      "page": 4
    },
    {
      "caption": "Figure 4: ; lower BD",
      "page": 4
    },
    {
      "caption": "Figure 5: VAE-vanilla and VAE-ss are shown to have the",
      "page": 4
    },
    {
      "caption": "Figure 5: (b, d), where the scatter plots of all the transfer",
      "page": 4
    },
    {
      "caption": "Figure 5: (c, d), the covariance for the data",
      "page": 4
    },
    {
      "caption": "Figure 5: Scatter plots depicting the overlap between the latent embed-",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "In recent years, speech emotion recognition (SER) has been used in"
        },
        {
          "ABSTRACT": "wide ranging applications,\nfrom healthcare to the commercial sec-"
        },
        {
          "ABSTRACT": "tor.\nIn addition to signal processing approaches, methods for SER"
        },
        {
          "ABSTRACT": "now also use deep learning techniques which provide transfer learn-"
        },
        {
          "ABSTRACT": "ing possibilities. However, generalizing over languages, corpora and"
        },
        {
          "ABSTRACT": "recording conditions is still an open challenge.\nIn this work we ad-"
        },
        {
          "ABSTRACT": "dress this gap by exploring loss functions that aid in transferabil-"
        },
        {
          "ABSTRACT": "ity,\nspeciﬁcally to non-tonal\nlanguages. We propose a variational"
        },
        {
          "ABSTRACT": "autoencoder (VAE) with KL annealing and a semi-supervised VAE"
        },
        {
          "ABSTRACT": "to obtain more consistent latent embedding distributions across data"
        },
        {
          "ABSTRACT": "sets. To ensure transferability,\nthe distribution of the latent embed-"
        },
        {
          "ABSTRACT": "ding should be similar across non-tonal\nlanguages (data sets). We"
        },
        {
          "ABSTRACT": "start by presenting a low-complexity SER based on a denoising-"
        },
        {
          "ABSTRACT": "autoencoder, which achieves an unweighted classiﬁcation accuracy"
        },
        {
          "ABSTRACT": "of over 52.09% for\nfour-class emotion classiﬁcation.\nThis perfor-"
        },
        {
          "ABSTRACT": "mance is comparable to that of similar baseline methods. Following"
        },
        {
          "ABSTRACT": "this, we employ a VAE, the semi-supervised VAE and the VAE with"
        },
        {
          "ABSTRACT": "KL annealing to obtain a more regularized latent space. We show"
        },
        {
          "ABSTRACT": "that while the DAE has the highest classiﬁcation accuracy among"
        },
        {
          "ABSTRACT": "the methods,\nthe semi-supervised VAE has a comparable classiﬁca-"
        },
        {
          "ABSTRACT": "tion accuracy and a more consistent\nlatent embedding distribution"
        },
        {
          "ABSTRACT": "over data sets.1"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "Index Terms— cross-lingual,\nlatent\nrepresentation,\nloss func-"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "tions, speech emotion recognition (SER), transfer learning"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "1.\nINTRODUCTION"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "Speech emotion recognition (SER)\nrefers to a group of algorithms"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "that deduce the emotional state of an individual from their speech ut-"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "terances. SER combined with affect recognition using other modal-"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "ities,\nlike vision and physiological signals, are deployed in a wide"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "range of applications. For instance, in the detection and intervention"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "of disorders in healthcare, monitoring the attentiveness of students"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "in schools,\nrisk assessment within the criminal\njustice system, and"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "for commercial applications,\nlike detecting customer satisfaction in"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "call-centers and by employment agencies to ﬁnd suitable candidates."
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "State-of-the-art SER techniques have\nevolved from the more"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "conventional signal processing and machine learning based meth-"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "ods to deep neural network based solutions [1]. Classical methods"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "were\nbased\non\nhidden Markov models\n(HMM), Gaussian mix-"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "ture models (GMM), support vector machines (SVM) and decision"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "trees.\nContributions based on HMM employed energy and pitch"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "features [2] showed high classiﬁcation accuracy. Spectral, prosodic"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "and energy features in tandem with a GMM and SVM were used"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "1Link to code: https://bit.ly/34CgkSZ"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ational Bayes have been exploited to learn the latent representations": "of speech emotions with input features ranging from the raw signals"
        },
        {
          "ational Bayes have been exploited to learn the latent representations": "to hand crafted features [13, 14, 15, 16]."
        },
        {
          "ational Bayes have been exploited to learn the latent representations": "To successfully model the emotion representations, 1. the latent"
        },
        {
          "ational Bayes have been exploited to learn the latent representations": "space should be discriminative of\nthe emotion classes, 2. distribu-"
        },
        {
          "ational Bayes have been exploited to learn the latent representations": "tion of\nthe latent embedding should be similar over different\nlan-"
        },
        {
          "ational Bayes have been exploited to learn the latent representations": "guages and corpora, such that\nthe latent space is invariant\nto emo-"
        },
        {
          "ational Bayes have been exploited to learn the latent representations": "tion-irrelevant\nfactors while preserving linguistic and cultural dif-"
        },
        {
          "ational Bayes have been exploited to learn the latent representations": "ferences in expressing emotions. Despite the use of AEs for SER"
        },
        {
          "ational Bayes have been exploited to learn the latent representations": "in existing literature, few methods provide insights beyond classiﬁ-"
        },
        {
          "ational Bayes have been exploited to learn the latent representations": "cation accuracy on within and cross-corpus datasets. The contribu-"
        },
        {
          "ational Bayes have been exploited to learn the latent representations": ""
        },
        {
          "ational Bayes have been exploited to learn the latent representations": "tion of this work is to present a method that fares well on both the"
        },
        {
          "ational Bayes have been exploited to learn the latent representations": "aforementioned factors. We begin by presenting a low-complexity"
        },
        {
          "ational Bayes have been exploited to learn the latent representations": "DAE for SER that achieves a performance similar to existing meth-"
        },
        {
          "ational Bayes have been exploited to learn the latent representations": "ods, as listed in Tab. 1. We show that while the DAE discovers a"
        },
        {
          "ational Bayes have been exploited to learn the latent representations": "class-discriminative latent space,\nthe distribution of\nthe latent em-"
        },
        {
          "ational Bayes have been exploited to learn the latent representations": "bedding is considerably different\nfor different\nlanguages.\nHence,"
        },
        {
          "ational Bayes have been exploited to learn the latent representations": "to obtain a more regularized latent space, we employ a variational"
        },
        {
          "ational Bayes have been exploited to learn the latent representations": "autoencoder\n(VAE). Since the latent space of a VAE is continuous"
        },
        {
          "ational Bayes have been exploited to learn the latent representations": "and probabilistic,\nthis approach is a step further\ntowards a dimen-"
        },
        {
          "ational Bayes have been exploited to learn the latent representations": "sional model of emotions, which proposes that a common process"
        },
        {
          "ational Bayes have been exploited to learn the latent representations": ""
        },
        {
          "ational Bayes have been exploited to learn the latent representations": "generates all affective states,\nthereby rendering emotions as a more"
        },
        {
          "ational Bayes have been exploited to learn the latent representations": ""
        },
        {
          "ational Bayes have been exploited to learn the latent representations": "continuous process [17]. While a vanilla-VAE yields consistent\nla-"
        },
        {
          "ational Bayes have been exploited to learn the latent representations": ""
        },
        {
          "ational Bayes have been exploited to learn the latent representations": "tent distributions over cross-lingual corpora, the latent embedding is"
        },
        {
          "ational Bayes have been exploited to learn the latent representations": ""
        },
        {
          "ational Bayes have been exploited to learn the latent representations": "not discriminative of the emotion classes due to a dominant KL-loss."
        },
        {
          "ational Bayes have been exploited to learn the latent representations": "Therefore, we address the question: what is the optimal balance be-"
        },
        {
          "ational Bayes have been exploited to learn the latent representations": "tween the reconstruction loss and the regularization loss to obtain"
        },
        {
          "ational Bayes have been exploited to learn the latent representations": ""
        },
        {
          "ational Bayes have been exploited to learn the latent representations": "a class-discriminative and distribution-consistent\nlatent\nrepresenta-"
        },
        {
          "ational Bayes have been exploited to learn the latent representations": ""
        },
        {
          "ational Bayes have been exploited to learn the latent representations": "tion. Towards that end, we explore 1. a VAE with KL-loss annealing"
        },
        {
          "ational Bayes have been exploited to learn the latent representations": ""
        },
        {
          "ational Bayes have been exploited to learn the latent representations": "using cyclic scheduling to improve class discrimination. 2. To fur-"
        },
        {
          "ational Bayes have been exploited to learn the latent representations": "ther\nimprove performance, we employ semi-supervised training of"
        },
        {
          "ational Bayes have been exploited to learn the latent representations": "the VAE by incorporating a clustering loss in the learning function."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "e being the training epoch, the total number of cycles given by M =": "2, T = 50 is the total number of epochs and R = 0.5 being the"
        },
        {
          "e being the training epoch, the total number of cycles given by M =": "proportion of monotonic increase in an annealing cycle."
        },
        {
          "e being the training epoch, the total number of cycles given by M =": "VAE with Semi-supervision:\nTo obtain a\nlatent\nrepresentation"
        },
        {
          "e being the training epoch, the total number of cycles given by M =": "where\nthe\nemotion\nclasses\nhave\nimproved\ndiscrimination while"
        },
        {
          "e being the training epoch, the total number of cycles given by M =": "maintaining the regularized space, we\nincorporate\na\ncluster\nloss,"
        },
        {
          "e being the training epoch, the total number of cycles given by M =": "loss\nis obtained as\nthe ratio of\nLclus within the VAE. The cluster"
        },
        {
          "e being the training epoch, the total number of cycles given by M =": "intra-class\nand inter-class distances of\nthe\nlatent\nembedding and"
        },
        {
          "e being the training epoch, the total number of cycles given by M =": "minimizing the overall loss will lead to minimize the intra-class dis-"
        },
        {
          "e being the training epoch, the total number of cycles given by M =": "tances and maximize the inter-class distance. This is often referred"
        },
        {
          "e being the training epoch, the total number of cycles given by M =": "to as the center-loss [20]. The overall optimization problem is hence"
        },
        {
          "e being the training epoch, the total number of cycles given by M =": "written as:"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "arg min\nLrec + βeLKL + γLclus,": "θ,φ"
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": "(cid:80)"
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": "K(cid:80) k\nD(zi, zk)"
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": "(5)"
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": "=1\n∀i∈k\nDintra"
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": "=\n,\nLclus ="
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": "K(cid:80)\nK−1\nDinter"
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": "(cid:80) k\nD(zk, zj)"
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": ""
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": "=1\nj=k+1"
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": ""
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": "where k is the class under consideration, K is the total number of"
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": ""
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": "classes, zk is the mean z for class k and γ = 0.5."
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": ""
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": "3. EXPERIMENTS AND RESULTS"
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": ""
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": ""
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": "Datasets and input features:\nWe use the IEMOCAP data set, an"
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": ""
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": "audio-visual affect data set,\nto train and validate the models [21]."
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": ""
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": "The data set comprises of annotations representing both the cate-"
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": ""
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": "gorical and dimensional emotional model\n[17].\nTo study how the"
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": ""
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": "latent representations are transferred between corpora, we use 1. the"
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": ""
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": "Surrey Audio-Visual Expresses Emotion (SAVEE) database that\nis"
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": "primarily English and consists of male speakers only, 2.\nthe Berlin"
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": ""
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": "Database of Emotional Speech (Emo-DB) recorded in German and,"
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": ""
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": "3.\nthe Canadian French Emotional\n(CaFE)\nspeech database com-"
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": ""
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": "prising of French audio samples [22, 23, 24].,\nthe URDU-dataset"
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": ""
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": "consisting of Urdu speech, and the Acted Emotional Speech Dy-"
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": ""
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": "namic (AESD) Database comprising of Greek speech [25, 26]. Note"
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": ""
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": "that AESD does not comprise of samples from neutral emotion.\nIn"
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": ""
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": "this work, we utilize the audio modality only, and train the mod-"
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": ""
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": "(N),\nsad (S),\nels with data from the emotional categories neutral"
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": ""
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": "happy (H), angry (A). We use the extended Geneva minimalistic"
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": ""
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": "acoustic parameter set (eGeMAPS) [27], speciﬁcally the functionals"
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": ""
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": "of lower-level features. Each speech sample yields a feature vector"
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": ""
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": "comprised of 88 features and we use the OpenSmile toolkit to extract"
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": ""
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": "the features [28]."
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": ""
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": ""
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": "System architecture:\nWhile AEs have been employed for SER in"
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": ""
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": "existing literature,\ntheir main focus was to propose novel network"
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": "architectures that provide better classiﬁcation accuracy [11, 13, 16,"
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": ""
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": "14, 9]. Since our goal is to explore the cross-lingual transferability of"
        },
        {
          "arg min\nLrec + βeLKL + γLclus,": "the latent representations, we adhere to the simplest architecture that"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "transfer data sets.": ""
        },
        {
          "transfer data sets.": ""
        },
        {
          "transfer data sets.": "3.1. Results"
        },
        {
          "transfer data sets.": ""
        },
        {
          "transfer data sets.": ""
        },
        {
          "transfer data sets.": "Method"
        },
        {
          "transfer data sets.": "GAN [29]"
        },
        {
          "transfer data sets.": ""
        },
        {
          "transfer data sets.": "FLUDA [30]"
        },
        {
          "transfer data sets.": "VAE+LSTM [13]"
        },
        {
          "transfer data sets.": "AE+LSTM [13]"
        },
        {
          "transfer data sets.": "Stacked-AE+BLSTM-RNN [12]"
        },
        {
          "transfer data sets.": "DAE+Linear-SVM (proposed)"
        },
        {
          "transfer data sets.": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": "distance (BD) of\nthe distribution of the latent embedding from the"
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": "methods over the transfer datasets with respect\nto the training data"
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": "set. Lower BD implies more similarity between distributions."
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": "on a similar concept as that\nin this paper and their performance in"
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": "Tab. 1.\nClassiﬁcation and consistency results are provided in the"
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": "following sections."
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": "Classiﬁcation of emotion classes:\nTo study the limits of the meth-"
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": "ods, we evaluate the systems under\nthe following two conditions"
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": "wherein: 1) the transfer data sets are completely unseen, (2) we have"
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": "access to 20% of the unlabeled transfer data sets, whereby we use"
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": "them to normalize the input feature space. The mean and 95% CI of"
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": "the balanced accuracy over 5-folds for conditions 1 and 2 are shown"
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": "in Fig. 3 (a, b) and Fig. 3 (c, d)\nrespectively. As can be observed"
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": "from the plot,\nthe DAE-vanilla consistently performs well and is"
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": "closely followed by VAE with semi-supervision, speciﬁcally under"
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": "condition 2. This indicates that DAE is best at forming a discrimi-"
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": "nating latent space. We also observe that using a minor portion of"
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": "the target data set to normalize the input feature space considerably"
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": "improves the classiﬁcation accuracy. This is a realistic assumption"
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": "because open unlabelled language speciﬁc data sets can be used for"
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": ""
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": "this purpose. Furthermore, VAE-vanilla seems to have no better per-"
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": "formance than a random classiﬁer, which is due to posterior collapse"
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": ""
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": "as discussed in Sec. 2. VAE-annealing seems to perform better under"
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": ""
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": "the condition 1 for all transfer data sets, except for the URDU."
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": ""
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": "On a general\nlevel, we observe that\nthe models are able to dis-"
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": ""
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": "criminate between neutral, sad and anger better than between anger"
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": ""
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": "and happy.\nIn other words, the models are unable to effectively dis-"
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": ""
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": "criminate between anger and happy, although they represent negative"
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": ""
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": "and positive emotions, respectively. This can be further observed in"
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": ""
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": "Fig. 2 (d), wherein happy and anger samples overlap in the latent"
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": ""
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": "space. A probable reason for this could be that the latent representa-"
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": ""
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": "tions correspond to activation more than valence."
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": ""
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": ""
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": "Consistency of latent distribution\nWe use the Bhattacharya dis-"
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": "tance\nto quantitatively evaluate\nthe\nsimilarity between the\nlatent"
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": "embedding\ndistribution.\nThe Bhattacharya\ndistance\nis\ngiven\nas"
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": "BD = − log( (cid:80)\n(cid:112)pref(z)pi(z)) between the kernel density es-"
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": "x∈Z"
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": "timates (KDEs) of\nthe reference (training) dataset and the transfer"
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": "dataset i. We present\nthe logarithm of the BD in Fig. 4;\nlower BD"
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": ""
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": "value implies that\nthe transfer data set\nis more similar to the refer-"
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": "ence data set. Furthermore, the scatter plots of the latent embedding"
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": ""
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": "for\nthe four methods over\nthe training and transfer data sets are"
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": ""
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": "illustrated in Fig. 5. VAE-vanilla and VAE-ss are shown to have the"
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": ""
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": "lowest BD with respect\nto the training data set and this can also be"
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": ""
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": "observed in Fig. 5 (b, d), where the scatter plots of all\nthe transfer"
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": "data sets overlap. While VAE-annealing and VAE-ss indicate an off-"
        },
        {
          "Fig. 4. Plot presents the mean of the logarithm of the Bhattacharya": "set between the data sets in Fig. 5 (c, d), the covariance for the data"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "markov models,”\nin Seventh European Conference on Speech": "Communication and Technology, 2001.",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": "arousal, dominance: Mehrabian and russell revisited,” Current"
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": "Psychology, vol. 33, no. 3, pp. 405–421, 2014."
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "[3]\nIker\nLuengo,\nEva Navas,\nInmaculada Hern´aez,\nand\nJon",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": ""
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "S´anchez,\n“Automatic emotion recognition using prosodic pa-",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": "[18] Hector P Martinez, Georgios N Yannakakis, and John Hallam,"
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "rameters,” in Ninth European conference on speech communi-",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": "IEEE transac-\n“Don’t classify ratings of affect;\nrank them!,”"
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "cation and technology, 2005.",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": "tions on affective computing, vol. 5, no. 3, pp. 314–326, 2014."
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": "[19] Hao Fu, Chunyuan Li, Xiaodong Liu, Jianfeng Gao, Asli Ce-"
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "[4] H. Hu, M. Xu, and W. Wu, “Gmm supervector based svm with",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": ""
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": "likyilmaz, and Lawrence Carin, “Cyclical annealing schedule:"
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "spectral features for speech emotion recognition,” in IEEE In-",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": ""
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": "A simple approach to mitigating kl vanishing,” arXiv preprint"
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "ternational Conference on Acoustics, Speech and Signal Pro-",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": ""
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": "arXiv:1903.10145, 2019."
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "cessing (ICASSP), 2007, vol. 4, pp. IV–413–IV–416.",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": ""
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": "[20] Klas Rydhmer and Raghavendra Selvan, “Dynamic β-vaes for"
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "[5] C. Busso, S. Lee, and S. Narayanan,\n“Analysis of emotion-",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": ""
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": "quantifying biodiversity by clustering optically recorded insect"
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "ally salient aspects of fundamental frequency for emotion de-",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": ""
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": "signals,” Ecological Informatics, vol. 66, pp. 101456, 2021."
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "IEEE Transactions on Audio, Speech, and Language\ntection,”",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": ""
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "Processing, vol. 17, no. 4, pp. 582–596, 2009.",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": "[21] Carlos Busso, Murtaza Bulut, and et al.,\n“Iemocap:\nInterac-"
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": "tive emotional dyadic motion capture database,” Language re-"
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "[6]\nJerome Friedman, Trevor Hastie, Robert Tibshirani, et al., The",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": ""
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": "sources and evaluation, vol. 42, no. 4, pp. 335–359, 2008."
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "elements of statistical learning, vol. 1, Springer series in statis-",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": ""
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": "[22]\nPhilip Jackson and et al., “Surrey audio-visual expressed emo-"
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "tics New York, 2001.",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": ""
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": "tion (savee) database,” University of Surrey: Guildford, UK,"
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "[7] M. W¨ollmer, B. Schuller, F. Eyben, and G. Rigoll,\n“Combin-",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": ""
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": "2014."
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "ing long short-term memory and dynamic bayesian networks",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": ""
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": "[23]\nFelix Burkhardt, Astrid Paeschke, Miriam Rolfes, Walter F"
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "IEEE\nfor\nincremental emotion-sensitive artiﬁcial\nlistening,”",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": ""
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": "Sendlmeier, and Benjamin Weiss, “A database of german emo-"
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "Journal of Selected Topics in Signal Processing, vol. 4, no. 5,",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": ""
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": "tional speech,” in Ninth European Conference on Speech Com-"
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "pp. 867–881, 2010.",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": ""
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": "munication and Technology, 2005."
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "[8]\nJun Deng, Zixing Zhang, and et al., “Sparse autoencoder-based",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": ""
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": "[24] Philippe Gournay, Olivier Lahaie,\nand et al.,\n“A canadian"
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "feature transfer\nlearning for speech emotion recognition,”\nin",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": ""
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": "the 9th\nfrench emotional speech dataset,”\nin Proceedings of"
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "Humaine association conference on affective computing and",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": ""
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": "ACM Multimedia Systems Conference, 2018, pp. 399–402."
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "intelligent interaction. IEEE, 2013, pp. 511–516.",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": ""
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": "[25]\nSiddique Latif, Adnan Qayyum, Muhammad Usman, and Ju-"
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "[9] Rui Xia and Yang Liu, “Using denoising autoencoder for emo-",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": ""
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": "naid Qadir,\n“Cross lingual speech emotion recognition: Urdu"
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "tion recognition.,” in Interspeech, 2013, pp. 2886–2889.",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": ""
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": "vs. western languages,”\nin International Conference on Fron-"
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "[10]\nJ. Deng, Z. Zhang, F. Eyben, and B. Schuller,\n“Autoencoder-",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": "tiers of Information Technology (FIT). IEEE, 2018, pp. 88–93."
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "based\nunsupervised\ndomain\nadaptation\nfor\nspeech\nemotion",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": ""
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": "[26] Nikolaos Vryzas, Rigas Kotsakis, Aikaterini Liatsou, Char-"
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "recognition,”\nIEEE Signal Processing Letters, vol. 21, no. 9,",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": ""
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": "alampos A Dimoulas, and George Kalliris,\n“Speech emotion"
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "pp. 1068–1072, 2014.",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": ""
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": "recognition for performance interaction,” Journal of the Audio"
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "[11] Vipula Dissanayake, Haimo Zhang, Mark Billinghurst,\nand",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": "Engineering Society, vol. 66, no. 6, pp. 457–467, 2018."
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "Suranga Nanayakkara,\n“Speech emotion recognition ‘in the",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": ""
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": "[27]\nFlorian Eyben, Klaus R Scherer,\nand et al. Schuller,\n“The"
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "wild’ using an autoencoder,” Interspeech, pp. 526–530, 2020.",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": ""
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": "geneva minimalistic acoustic parameter set (gemaps) for voice"
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": "IEEE transactions on af-\nresearch and affective computing,”"
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "[12]\nSayan Ghosh, Eugene Laksana, Louis-Philippe Morency, and",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": ""
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "Stefan Scherer,\n“Representation learning for speech emotion",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": "fective computing, vol. 7, no. 2, pp. 190–202, 2015."
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "recognition.,” in Interspeech, 2016, pp. 3603–3607.",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": "[28]\nFlorian Eyben, Martin W¨ollmer, and Bj¨orn Schuller,\n“Opens-"
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": "mile:\nthe munich versatile and fast open-source audio feature"
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "[13]\nSiddique Latif, Rajib Rana,\nJunaid Qadir,\nand Julien Epps,",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": ""
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": "extractor,” in Proceedings of the 18th ACM international con-"
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "“Variational autoencoders for learning latent representations of",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": ""
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": "ference on Multimedia, 2010, pp. 1459–1462."
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "Interspeech Proceed-\nspeech emotion:\na preliminary study,”",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": ""
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "ings, pp. 3107–3111, 2018.",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": "[29]\nSiddique Latif, Junaid Qadir, and Muhammad Bilal, “Unsuper-"
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": "vised adversarial domain adaptation for cross-lingual speech"
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "[14]\nSrinivas Parthasarathy, Viktor Rozgic, Ming Sun,\nand Chao",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": ""
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": "emotion recognition,”\nin 8th International Conference on Af-"
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "Wang,\n“Improving emotion classiﬁcation through variational",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": ""
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": "fective Computing and Intelligent\nInteraction (ACII).\nIEEE,"
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "inference of\nlatent variables,”\nin IEEE International Confer-",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": ""
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": "2019, pp. 732–737."
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "ence on Acoustics, Speech and Signal Processing (ICASSP).",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": ""
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "IEEE, 2019, pp. 7410–7414.",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": "[30] Youngdo Ahn, Sung Joo Lee, and et al., “Cross-corpus speech"
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": "emotion recognition based on few-shot\nlearning and domain"
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "[15]\nSeﬁk Emre Eskimez,\nZhiyao Duan,\nand Wendi Heinzel-",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": ""
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": "adaptation,” IEEE Signal Processing Letters, 2021."
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "man,\n“Unsupervised learning approach to feature\nanalysis",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": ""
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": "[31] Bj¨orn Schuller, Stefan Steidl, Anton Batliner, and et al.,\n“The"
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "for automatic speech emotion recognition,”\nin IEEE Interna-",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": ""
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": "INTER-\ninterspeech 2010 paralinguistic challenge,”\nin Proc."
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "tional Conference on Acoustics, Speech and Signal Processing",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": ""
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": "SPEECH, Makuhari, Japan, 2010, pp. 2794–2797."
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "(ICASSP). IEEE, 2018, pp. 5099–5103.",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": ""
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": "[32] Gilles Degottex, John Kane, Thomas Drugman, Tuomo Raitio,"
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "[16] Michael Neumann and Ngoc Thang Vu,\n“Improving speech",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": ""
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": "and Stefan Scherer,\n“Covarep—a collaborative voice analy-"
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "emotion recognition with unsupervised representation learn-",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": ""
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": "sis repository for speech technologies,”\nin International con-"
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "ing on unlabeled speech,”\nin IEEE International Conference",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": ""
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": "ference on acoustics, speech and signal processing (ICASSP)."
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "on Acoustics, Speech and Signal Processing (ICASSP). IEEE,",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": ""
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": "IEEE, 2014, pp. 960–964."
        },
        {
          "markov models,”\nin Seventh European Conference on Speech": "2019, pp. 7390–7394.",
          "[17]\nIris Bakker, Theo Van Der Voordt,\nand et\nal.,\n“Pleasure,": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Speech emotion recognition: Emotional models, databases, features, preprocessing methods, supporting modalities, and classifiers",
      "authors": [
        "Mehmet Berkehan",
        "Akc ¸ay",
        "Kaya Oguz"
      ],
      "year": "2020",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "3",
      "title": "Speech emotion recognition using hidden markov models",
      "authors": [
        "Albino Nogueiras",
        "Asunción Moreno",
        "Antonio Bonafonte",
        "José Mariño"
      ],
      "year": "2001",
      "venue": "Seventh European Conference on Speech Communication and Technology"
    },
    {
      "citation_id": "4",
      "title": "Automatic emotion recognition using prosodic parameters",
      "authors": [
        "Iker Luengo",
        "Eva Navas",
        "Inmaculada Hernáez",
        "Jon Sánchez"
      ],
      "year": "2005",
      "venue": "Ninth European conference on speech communication and technology"
    },
    {
      "citation_id": "5",
      "title": "Gmm supervector based svm with spectral features for speech emotion recognition",
      "authors": [
        "H Hu",
        "M Xu",
        "W Wu"
      ],
      "year": "2007",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "6",
      "title": "Analysis of emotionally salient aspects of fundamental frequency for emotion detection",
      "authors": [
        "C Busso",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2009",
      "venue": "IEEE Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "7",
      "title": "The elements of statistical learning",
      "authors": [
        "Jerome Friedman",
        "Trevor Hastie",
        "Robert Tibshirani"
      ],
      "year": "2001",
      "venue": "Springer series in statistics"
    },
    {
      "citation_id": "8",
      "title": "Combining long short-term memory and dynamic bayesian networks for incremental emotion-sensitive artificial listening",
      "authors": [
        "M Wöllmer",
        "B Schuller",
        "F Eyben",
        "G Rigoll"
      ],
      "year": "2010",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "9",
      "title": "Sparse autoencoder-based feature transfer learning for speech emotion recognition",
      "authors": [
        "Jun Deng",
        "Zixing Zhang"
      ],
      "year": "2013",
      "venue": "Humaine association conference on affective computing and intelligent interaction"
    },
    {
      "citation_id": "10",
      "title": "Using denoising autoencoder for emotion recognition",
      "authors": [
        "Rui Xia",
        "Yang Liu"
      ],
      "year": "2013",
      "venue": "Using denoising autoencoder for emotion recognition"
    },
    {
      "citation_id": "11",
      "title": "Autoencoderbased unsupervised domain adaptation for speech emotion recognition",
      "authors": [
        "J Deng",
        "Z Zhang",
        "F Eyben",
        "B Schuller"
      ],
      "year": "2014",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "12",
      "title": "Speech emotion recognition 'in the wild' using an autoencoder",
      "authors": [
        "Haimo Vipula Dissanayake",
        "Mark Zhang",
        "Suranga Billinghurst",
        "Nanayakkara"
      ],
      "year": "2020",
      "venue": "Speech emotion recognition 'in the wild' using an autoencoder"
    },
    {
      "citation_id": "13",
      "title": "Representation learning for speech emotion recognition",
      "authors": [
        "Sayan Ghosh",
        "Eugene Laksana",
        "Louis-Philippe Morency",
        "Stefan Scherer"
      ],
      "year": "2016",
      "venue": "Representation learning for speech emotion recognition"
    },
    {
      "citation_id": "14",
      "title": "Variational autoencoders for learning latent representations of speech emotion: a preliminary study",
      "authors": [
        "Siddique Latif",
        "Rajib Rana",
        "Junaid Qadir",
        "Julien Epps"
      ],
      "year": "2018",
      "venue": "Interspeech Proceedings"
    },
    {
      "citation_id": "15",
      "title": "Improving emotion classification through variational inference of latent variables",
      "authors": [
        "Srinivas Parthasarathy",
        "Viktor Rozgic",
        "Ming Sun",
        "Chao Wang"
      ],
      "year": "2019",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "16",
      "title": "Unsupervised learning approach to feature analysis for automatic speech emotion recognition",
      "authors": [
        "Zhiyao Sefik Emre Eskimez",
        "Wendi Duan",
        "Heinzelman"
      ],
      "year": "2018",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "17",
      "title": "Improving speech emotion recognition with unsupervised representation learning on unlabeled speech",
      "authors": [
        "Michael Neumann",
        "Ngoc Vu"
      ],
      "year": "2019",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "18",
      "title": "Pleasure, arousal, dominance: Mehrabian and russell revisited",
      "authors": [
        "Iris Bakker",
        "Theo Van Der",
        "Voordt"
      ],
      "year": "2014",
      "venue": "Current Psychology"
    },
    {
      "citation_id": "19",
      "title": "Don't classify ratings of affect; rank them!",
      "authors": [
        "Georgios Hector P Martinez",
        "John Yannakakis",
        "Hallam"
      ],
      "year": "2014",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "20",
      "title": "Cyclical annealing schedule: A simple approach to mitigating kl vanishing",
      "authors": [
        "Hao Fu",
        "Chunyuan Li",
        "Xiaodong Liu",
        "Jianfeng Gao",
        "Asli Celikyilmaz",
        "Lawrence Carin"
      ],
      "year": "2019",
      "venue": "Cyclical annealing schedule: A simple approach to mitigating kl vanishing",
      "arxiv": "arXiv:1903.10145"
    },
    {
      "citation_id": "21",
      "title": "Dynamic β-vaes for quantifying biodiversity by clustering optically recorded insect signals",
      "authors": [
        "Klas Rydhmer",
        "Raghavendra Selvan"
      ],
      "year": "2021",
      "venue": "Ecological Informatics"
    },
    {
      "citation_id": "22",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut"
      ],
      "year": "2008",
      "venue": "Iemocap: Interactive emotional dyadic motion capture database"
    },
    {
      "citation_id": "23",
      "title": "Surrey audio-visual expressed emotion (savee) database",
      "authors": [
        "Philip Jackson"
      ],
      "year": "2014",
      "venue": "Surrey audio-visual expressed emotion (savee) database"
    },
    {
      "citation_id": "24",
      "title": "A database of german emotional speech",
      "authors": [
        "Felix Burkhardt",
        "Astrid Paeschke",
        "Miriam Rolfes",
        "Walter Sendlmeier",
        "Benjamin Weiss"
      ],
      "year": "2005",
      "venue": "Ninth European Conference on Speech Communication and Technology"
    },
    {
      "citation_id": "25",
      "title": "A canadian french emotional speech dataset",
      "authors": [
        "Philippe Gournay",
        "Olivier Lahaie"
      ],
      "year": "2018",
      "venue": "Proceedings of the 9th ACM Multimedia Systems Conference"
    },
    {
      "citation_id": "26",
      "title": "Cross lingual speech emotion recognition: Urdu vs. western languages",
      "authors": [
        "Siddique Latif",
        "Adnan Qayyum",
        "Muhammad Usman",
        "Junaid Qadir"
      ],
      "year": "2018",
      "venue": "International Conference on Frontiers of Information Technology (FIT)"
    },
    {
      "citation_id": "27",
      "title": "Speech emotion recognition for performance interaction",
      "authors": [
        "Nikolaos Vryzas",
        "Rigas Kotsakis",
        "Aikaterini Liatsou",
        "Charalampos Dimoulas",
        "George Kalliris"
      ],
      "year": "2018",
      "venue": "Journal of the Audio Engineering Society"
    },
    {
      "citation_id": "28",
      "title": "The geneva minimalistic acoustic parameter set (gemaps) for voice research and affective computing",
      "authors": [
        "Florian Eyben",
        "Klaus Scherer"
      ],
      "year": "2015",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "29",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "Florian Eyben",
        "Martin Wöllmer",
        "Björn Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "30",
      "title": "Unsupervised adversarial domain adaptation for cross-lingual speech emotion recognition",
      "authors": [
        "Siddique Latif",
        "Junaid Qadir",
        "Muhammad Bilal"
      ],
      "year": "2019",
      "venue": "8th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "31",
      "title": "Cross-corpus speech emotion recognition based on few-shot learning and domain adaptation",
      "authors": [
        "Youngdo Ahn",
        "Sung Lee"
      ],
      "year": "2021",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "32",
      "title": "The interspeech 2010 paralinguistic challenge",
      "authors": [
        "Björn Schuller",
        "Stefan Steidl",
        "Anton Batliner"
      ],
      "year": "2010",
      "venue": "Proc. INTER-SPEECH, Makuhari, Japan"
    },
    {
      "citation_id": "33",
      "title": "Covarep-a collaborative voice analysis repository for speech technologies",
      "authors": [
        "Gilles Degottex",
        "John Kane",
        "Thomas Drugman",
        "Tuomo Raitio",
        "Stefan Scherer"
      ],
      "year": "2014",
      "venue": "International conference on acoustics, speech and signal processing (ICASSP)"
    }
  ]
}