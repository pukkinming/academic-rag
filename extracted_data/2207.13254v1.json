{
  "paper_id": "2207.13254v1",
  "title": "Contextual Information And Commonsense Based Prompt For Emotion Recognition In Conversation",
  "published": "2022-07-27T02:34:05Z",
  "authors": [
    "Jingjie Yi",
    "Deqing Yang",
    "Siyu Yuan",
    "Caiyan Cao",
    "Zhiyao Zhang",
    "Yanghua Xiao"
  ],
  "keywords": [
    "emotion recognition",
    "prompt",
    "pre-trained language model"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition in conversation (ERC) aims to detect the emotion for each utterance in a given conversation. The newly proposed ERC models have leveraged pre-trained language models (PLMs) with the paradigm of pre-training and fine-tuning to obtain good performance. However, these models seldom exploit PLMs' advantages thoroughly, and perform poorly for the conversations lacking explicit emotional expressions. In order to fully leverage the latent knowledge related to the emotional expressions in utterances, we propose a novel ERC model CIS-PER with the new paradigm of prompt and language model (LM) tuning. Specifically, CISPER is equipped with the prompt blending the contextual information and commonsense related to the interlocutor's utterances, to achieve ERC more effectively. Our extensive experiments demonstrate CISPER's superior performance over the state-of-the-art ERC models, and the effectiveness of leveraging these two kinds of significant prompt information for performance gains. To reproduce our experimental results conveniently, CISPER's sourcecode and the datasets have been shared at https: // github. com/ DeqingYang/ CISPER .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition in conversation (ERC) aims to judge the emotion category expressed by each interlocutor in a given conversation. In recent years, ERC has been widely studied in natural language processing (NLP), and applied in many fields, including dialogue robots (such as chat and self-help psychological diagnosis), sentiment and opinion mining in the conversations on social media.\n\nMost of previous ERC models are implemented through encoding the dialogue's text into semantic embeddings at first, followed by regarding each round of dialogue as a step or node. Then, they employ recurrent neural networks (RNNs)  [21]  or graph neural networks (GNNs)  [5, 39]  to obtain utterance representations for the final sentiment prediction. The encoders of dialogue texts in the earlier models include Glove  [24]  and Word2Vec  [23] . Recently, inspired by the power of pre-trained language models (PLMs)  [4, 19]  on encoding text semantics, PLMs are also employed as the encoders to obtain enhanced recognition performance  [27, 6] .\n\nDespite the achievements, the previous PLM-based ERC models seldom fully exploit PLMs' latent knowledge, resulting in limited performance gains. More recently, some researchers have proposed the prompt-based learning paradigm to utilize PLMs on various downstream NLP tasks, in which an appropriate prompt is designed to guide the PLM to better take advantage of the knowledge related to the downstream task. As a result, the PLM's performance on the downstream task is improved. Given that PLMs also contain rich semantic and emotional knowledge related to the utterances in a human dialogue at pre-training stage, we are inspired to leverage the prompt about such knowledge to guide the PLM to achieve ERC task more effectively.\n\nHowever, it is nontrivial to apply the prompt-based learning paradigm on a PLM to achieve ERC. Although prompt-based PLMs have been employed for generic sentimental analysis successfully  [14] , ERC is an entirely different task posing new challenges. In ERC, there are multiple utterances in a conversation, and these utterances are semantically similar to or logically correlated with each other. Thus the contextual information is helpful to the emotion recognition of the current utterance in a conversation  [21] . Besides, due to the lack of the commonsense related to emotional expressions, the colloquial and obscure expressions in a conversation make it difficult for PLMs to understand the real utterance emotions. We use a conversation example in Fig.  1  to explain the significance of these two kinds of significant information. Without any prompt, it is difficult to identify Anger is the real emotion of Speaker B's utterance \"What a good thing you've done!\", because \"good thing\" is obscure that is actually an irony in this conversation. While it would be recognized correctly if the cues from contextual utterances are provided, such as \"so messy\" and \"playing a game\". Furthermore, the states of Speaker A and B when expressing these utterances are also helpful to identify the emotion. Therefore, it is challenging but crucial for ERC to design a valid and effective prompt to leverage the contextual information and commonsense. To tackle this challenge, we propose a PLM-based ERC model with prompt, namely CIS-PER (Contextual Information and commonSense based Prompt for Emotion Recognition). Specifically, we adopt the trainable embeddings of pseudo-tokens as the continuous prompt to cue the PLM, which blends two kinds of significant information. One is the contextual information in the conversation, and the other is the inferential commonsense related to the emotional expression in the utterance, which is extracted from a famous commonsense base ATOMIC  [28] . Compared with the explicit discrete prompt  [29]  in previous models, the trainable continuous prompt in CISPER blends these two kinds of information more flexibly, and makes the model converge more quickly with the learning paradigm of prompt + LM tuning (language model tuning). In fact, these prompt embeddings can be regarded as some informative \"sentences\" with crucial emotional cues of the conversation, which are then attached with the utterance text and fed into the PLM to achieve ERC.\n\nIn summary, the main contributions of our paper include: 1. To the best of our knowledge, this is the first to practice the prompt-based learning paradigm on ERC task successfully. Unlike previous work focusing on task-specific model design, we pay more attention to prompt template mining.\n\n2. We propose a novel ERC model built with the trainable continuous prompt from the contextual information and commonsense related to the emotional expressions in utterances. The prompt provides the model with significant cues, and thus enhances the model's ERC performance effectively.\n\n3. Our extensive experimental results on two benchmark ERC datasets prove that, our CISPER outperforms the state-of-the-art (SOTA) baselines, especially in the emotion categories with fewer instances. Meanwhile, the rationality of incorporating contextual information and commonsense for enhanced ERC performance is also verified.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Emotion Recognition in Conversation Emotion recognition (including ERC) has been widely applied in many fields, such as man-machine dialogue and psychological and emotional intervention  [26] . Previous work in ERC generally adopts fine-tuning paradigm. Specifically, the utterance embeddings are first extracted by PLMs (such as Bert  [4]  and Roberta  [19] ), and then fed into the ERC model for emotion identification. Most of previous works based on finetuning paradigm design sophisticated deep neural networks to model various hidden states in the conversation, which can be divided into RNN-based methods  [21] , and GNN-based methods  [5, 10] . However, those methods with fine-tuning focus on identifying utterance emotions through downstream model designing, that implicitly model related elements in a conversation but ignore incorporating the latent knowledge in the PLM.\n\nCommonsense Knowledge Commonsense knowledge is beneficial for many NLP tasks such as dialogue generation  [35]  and story ending generation  [7] .\n\nWidely used commonsense knowledge graphs (CKGs) include ATOMIC  [28] , ConceptNet  [33] , etc. Commonsense knowledge is particularly important for ERC, since the colloquial expressions often occur in a conversation, making it difficult for the model to understand the semantics of sentences. Therefore, the CKGs containing abundant commonsense, are leveraged to incorporate such commonsense into the ERC model to improve ERC performance. For example, COSMIC  [6]  adopts COMET  [2]  to generate several types of commonsense for each utterance from ATOMIC, and achieves SOTA performance. Inspired by those works, we also incorporate commonsense knowledge into our ERC model.\n\nLanguage Prompting In recent years, as a new paradigm, \"pre-training, prompting, and predicting\" has been proposed to directly exploit the knowledge in pre-trained language models (PLMs), which greatly bridges the gap between the pre-training and fine-tuning of PLMs in downstream tasks. The construction methods of language prompts can be classified into manual constructed prompts and automatic constructed prompts  [17] . Manual constructed prompts are manually created based on human insights into the task and widely used in machine translation, text classification  [30, 29] . Constructing an appropriate prompt template for a certain downstream task is still a challenge even for the experienced prompt designers. Automatic constructed prompts are automatically generated to address the shortcomings of manual prompts. Some efforts have exploited natural language phrases to discover discrete prompts  [11, 37] . In addition, given the inherent continuous characteristics of neural networks, others focused on implementing prompts directly in vector spaces rather than designing human-interpretable template of prompts  [16, 18] . These continuous prompts are trainable and, therefore, optimal for downstream tasks. The training strategies of the prompt-based models can be divided into four categories: Tuning-free Prompting  [3] , Fixed-LM Prompt Tuning  [16, 8] , Fixed-prompt LM Tuning  [29, 30]  and Prompt+LM Tuning  [1, 18] . The third category does not need to train the prompts, and the last category takes the prompts as the parameters to fine-tune. In our CISPER, we also adopt Prompt+LM Tuning paradigm to train the model given its good flexibility and performance on ERC.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Task Formalization",
      "text": "Given a conversation containing L utterances {u 1 , u 2 , ..., u L }, suppose that the t-th utterance u t (1 ≤ t ≤ L) is spoken by the speaker q t and has K t words, i.e., u t = {w t 1 , w t 2 , ..., w t Kt }. The task of ERC is to identify each utterance u t 's emotion m t based on the features of u t and q t , as well as any other important cues. In other words, ERC is achieved at the utterance level.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Framework",
      "text": "Compared with the previous of ERC models with the fine-tuning paradigm, we adopt the prompts+LM-tuning paradigm for our CISPER, and focus more on how to mine an appropriate and effective prompt template to guide the PLM to achieve better ERC. As we claimed before, although the emotional expressions seldom appear in most conversations, the potential information derived from contextual utterances and commonsense reasoning are highly related to the emotional expression of the current utterance. It implies that these two kinds of information are informative for the PLM to infer current utterance's emotion. Therefore, we pay more attention to the generation of the appropriate prompt based on these two kinds of significant information. To this end, we adopt a trainable continuous prompt that can be updated during the training stage to blend contextual information and commonsense better. Our CISPER's architecture is depicted in Fig.  2 , of which the pipeline can be mainly divided into the following three steps (components):",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Input",
      "text": "1. Feature Extraction: The information features related to a conversation are extracted by the language models at first, including the semantics of the utterances in the conversation and the various inferential relations of commonsense.\n\n2. Prompt Generation: The trainable continuous prompt in CISPER are generated based on the features extracted in the first step.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Emotion Prediction:",
      "text": "The continuous prompt embeddings generated in the previous step and the target utterance's embeddings are together fed into the PLM to predict the token indicating the utterance's emotion.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Information Feature Extraction",
      "text": "This step aims to obtain the embeddings encoding the semantics of the utterances and the commonsense related to the utterances. These semantic embeddings will be subsequently used to generate the prompt in our model.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Semantic Features Extraction",
      "text": "For each utterance in a conversation, we directly use a PLM to generate its semantic embeddings. In our experiments, we adopted a RoBERTa-large model  [20]  as the PLM in this step, which consists of 24 Transformer encoder layers with 16-head self-attentions. Specifically, we append two special tokens [CLS], [SEP] to the token sequence of a given utterance u t = {w t 1 , w t 2 , ..., w t Kt }, to constitute RoBERTa's input se-   [6] , the special token [CLS] in such input format generally encodes the whole sequence's semantics through the PLM's encoding. Thus, among the output embeddings of RoBERTa, we only use [CLS]'s embeddings in the last 4 layers, denoted as v 1 , v 2 , v 3 and v 4 . Then, we average these 4 embeddings as u t 's semantic embedding x t ∈ R du . All utterances' semantic embeddings are obtained by this method.\n\nCommonsense Features Extraction Similar to COSMIC  [6] , the commonsense related to the utterances in the conversation is extracted from COMET  [2] . COMET is a Transformer-based model that constructs commonsense through training the language model on a seed set of knowledge triplets from ATOMIC  [28] . ATOMIC is one representative commonsense graph with 880K triplets of everyday inferential knowledge, covering 9 relations about entities and events. In CISPER, we select the 9 relation types of commonsense from ATOMIC, as listed in Table  1 . In these types, the former six types are related to the inference of different states of the speaker in the conversion, while the latter three types are related to the states of the listener.\n\nThe procedure of extracting the features of commonsense is presented as follows. Suppose r j (1 ≤ j ≤ 9) is the token of one relation type in the 9 inferential commonsense types, we concatenate it with the token sequence of given utterance u t and feed it into the COMET encoder. Then, we extract the hidden state (embedding) of the encoder's last layer, namely c t j ∈ R dc , as the embedding of the j-th commonsense type for u t . So we have\n\nAll embeddings of the 9 commonsense types are used together with x t for generating the prompt in CISPER in the next step.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Continuous Prompt Generation",
      "text": "In general, there are two types of language prompts, i.e., discrete prompt and continuous prompt. As we mentioned in Sec. 2, a continuous prompt may be more appropriate and effective for deep models, since deep neural networks are inherently continuous. Inspired by P-tuning  [18] , we also adopt some trainable embeddings as the continuous prompt in CISPER. These trainable embeddings are generated by the encoders fed with the contextual information and commonsense related to the current utterance in the conversation.\n\nPrevious research  [21]  has found that, the utterance emotion is highly related to the states of this utterance's speaker (such as speaker's intent, reaction, etc.) and listener (listeners' effect, reaction, etc.), which has also been illustrated in Figure  1 . Inspired by it, we generate two groups of continuous prompt embeddings from the perspective of speaker and listener, respectively, which are denoted as E and P. E corresponds to the speaker-related conversational information while P corresponds to the listener-related conversational information. Furthermore, the inferential commonsense related to speaker and listener are blended with the contextual information in the conversation and encoded into these embeddings, which are finally leveraged as the emotional prompts for the PLM to predict the utterance's emotion. The details of the generation of these prompt embeddings are described as follows.\n\nEncoding Contextual Information and Commonsense At first, we build a Transformer encoder to encode the contextual information and commonsense related to a conversation, which is fed with the semantic embeddings and the commonsense type embeddings of the utterances in the conversation obtained in the previous step.\n\nSpecifically, given a conversation consisting of L utterances, for each commonsense type j(1 ≤ j ≤ 9), we concatenate its embeddings related to all utterances that are computed by Eq. 1, as\n\nwhere ⊕ is concatenation operation. Then, suppose x = x 1 ⊕ x 2 ⊕ ... ⊕ x L ∈ R Ldu represent this conversation's contextual information, the two hidden embedding matrices about the conversation are obtained as\n\nwhere W e , W p are two linear projection matrices, and d T is the dimension of hidden embeddings. The encoding operations from Eq. 1 to Eq. 3 indicate that all contextual information in the conversation and the commonsense are blended and encoded into H e and H p with respect to (w.r.t.) speaker and listener, respectively, which are subsequently used as the basis of generating the final prompt embeddings.\n\nGenerating Prompt Embeddings of Pseudo Tokens In the last prediction step of CISPER, the target utterance's emotion is identified by a PLM through predicting the middle special token based on its surrounding (contextual) tokens' embeddings. In order to better fit with such prediction mechanism, we adopt a symmetrical prompt template to simultaneously insert the pseudo (prompt) tokens of the same number on the left side and the right side of utterance tokens.\n\nAccordingly, based on either H e or H p , we respectively generate two sets of prompt embeddings of the pseudo tokens by a multi-layer perceptron (MLP) followed by reshape operation. Specifically, suppose E ∈ R L×(2Ned T ) , P ∈ R L×(2Npd T ) are the continuous embedding matrices containing the speaker-related and listenerrelated conversational information, respectively, where N e and N p are the number of prompt embeddings. Then, we have E = [E l , E r ] = Reshape e MLP e (H e ) , P = [P l , P r ] = Reshape p MLP p (H p ) (  4 ) where E l (E r ) ∈ R L×(Ned T ) is the left (right) half of E used as the continuous embeddings for the left (right) pseudo tokens. So is P l (P r ).\n\nFinally, for utterance u t (1 ≤ t ≤ L), we take the t-th vectors in the continuous embedding matrices to constitute its hidden prompt embeddings of pseudo tokens, denoted as e l t , p l t , p r t , e r t . Note that the current continuous prompt embeddings are not encoded with sequential correlations among the tokens. It is not satisfied with the requirement that the input token embeddings of PLMs should encode sequential features. As a result, we further use Bi-LSTM  [9]   (5)",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Utterance Emotion Prediction",
      "text": "Recall that the ERC task is to identify emotion of a given conversation {u 1 , ..., u L } at the utterance level. In the last step, we leverage a PLM to predict the utterance emotions. To guide the PLM to better take advantage of the knowledge related to the utterances which is obtained from its pre-training, we convert the original emotion recognition task into a cloze task that meets the masked PLM's pretraining task. Specifically, in the PLM pre-training, some tokens in the original corpus are masked by a special token [MASK] with a certain probability. Then, the PLM predicts the masked tokens based on their contextual tokens.\n\nAccording to this task's principle, we feed a [MASK] corresponding to m t along with u t 's token sequence and the prompt pseudo tokens, into a RoBERTa with the following format as\n\nwhere\n\nwhere P ([M ASK] = w) is the predicted probability of w appearing at the position of [MASK] and w is one word in the tokenizer's vocabulary V. Since the predicted word may be any word in the vocabulary, we maintain a thesaurus to map the predicted word ŵ into one emotion category, i.e., m t . Hence, the prediction of u t 's emotion is achieved.\n\nPlease note that, in order to exert the continuous prompt's effect, the embeddings of [E",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Model Training",
      "text": "We adopt the cross entropy loss to train our ERC model as follows,\n\nwhere q is one conversation from the training set Q, and L q is the utterance number in q. w t is the word corresponding to the true emotion category of utterance u t , while P (w t ) is the estimated probability of w t appearing at the position of [MASK] for u t . In addition, we use ADAM  [13]  as the optimizer to update the model's parameters based on the error inverse propagation strategy.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Datasets",
      "text": "MELD  [25] : It has 1,432 conversations with more than 13,000 utterances in total, which were extracted from the famous TV show Friends. All utterances are labeled with seven emotion categories: anger, disgust, sadness, joy, surprise, fear and neutral, as well as three sentiment classes of positive, negative or neutral. We only evaluated the models' performance of recognizing the emotion categories.\n\nEmoryNLP  [38] : It is another dataset also extracted from the TV show Friends.\n\nThe utterances in this dataset are also annotated on seven emotion categories and three sentiment classes. The emotion categories are neutral, joyful, peaceful, powerful, scared, mad and sad. To create three sentiment classes, joyful, peaceful, and powerful are grouped to constitute the positive class; scared, mad and sad are grouped to constitute the negative class; and neutral is the rest class. We divided the two datasets into training, validation and test set according to the size the same as the previous work  [6] . Table  2  lists the sample number statistics of the three sample sets in these two datasets. DialogXL  [31] : It modifies the recurrence mechanism in XLNet  [36] , and uses the dialog-aware self-attention to model conversational data better.\n\nDialogueTRM  [22] : It first utilizes hierarchical transformer to generate features maintaining utterance-level and individual context, and then utilizes multi-modal transformer for Multi-Grained Interactive Fusion in ERC task. DAG-ERC  [32] : It proposes a directed acyclic graph network to better simulate the internal structure of a conversation, which provides a more intuitive way to model the information flow between the the background of the conversation and nearby context. COSMIC  [6] : It utilizes commonsense Transformer COMET  [2]  to extract commonsense from ATOMIC  [28]  graph for each utterance, and uses RNNs to blend those knowledge with contextual information. P-tuning  [18] : It is a framework using Bi-LSTM to generate trainable continuous prompt that would be fed along with utterances into the PLM. We apply this baseline in ERC to examine its difference to our model. We also specially designed several methods with fixed prompt templates to be compared with CISPER. Notice that we have tested some manual templates and finally chose the best effective fixed template \"my emotion is [MASK]\" as the prompt template for a prompt-based baseline, denoted as FixedTemplate. In Sec. 3.4, we have mentioned the reason of adopting a symmetrical prompt template in CISPER. To justify such symmetrical template's advantage, we further compared CISPER with its two variants which are equipped with the same size prompt only on the left or right side, denoted as CISPER (left) and CISPER (right).",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Important Settings",
      "text": "We used the following score as the metric to evaluate all compared models as,\n\nwhere M is the set of all emotion categories, N m is the number of utterances with emotion category m, and F1(m) is the F1-score on m. In CISPER, we adopted the Roberta-large model from https: // huggingface. co/ , and used the Transformer comes from https: // pytorch. org/ as the encoder in Eq. 3. We used ADAM  [13]  as the optimizer to update our model's parameters and set the learning rate and weight decay to 5 × 10 -6 and 10 -2 respectively. The batch size was set to 64. In addition, we set N e = N p = 3, which was determined by our tuning studies displayed subsequently. In addition, the dimension of commonsense type embedding d C is 768, the dimensions of utterance's semantic embedding d u and prompt embeddings d T were both set to 1,024. All these settings were decided as the optimum through our tuning studies.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Erc Performance Comparisons",
      "text": "We compared our CISPER with the baselines in terms of macro (overall) ERC performance and micro performance on each emotion category level.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Macro Comparisons",
      "text": "We first display the overall performance (weighted-F1) of all compared models on the two datasets in Table  3 , where all models are divided into three groups according to their learning paradigms and used language models. For the baselines in the first group except for DialogueGCN, and the baselines in the second group except for DialogXL, their performance scores were directly obtained from  [6] . For the rest models, we ran each one for 4 times and reported its average scores.\n\nAs we mentioned before, MELD was extracted from the famous TV show Friends where the utterances are very colloquial hardly contain explicit emotional expressions. As shown in Table  3 , all types of prompts can help the PLM obtain good ERC performance, since all prompt-based models in the third group almost outperform the rest baselines. CISPER and CISPER(left/right) both outperform FixedTemplate, justifying the advantage of the trainable continuous prompt over Fig.  3 . Micro performance comparisons of CISPER and COSMIC on emotion category level (better viewed in color). It shows that the two models perform better in the categories with more samples, while our CISPER outperforms COSMIC in the categories with fewer samples, justifying its capability of few-shot learning.\n\nthe fixed prompt template. Specifically, our CISPER has a performance improvement of 0.89% over COSMIC, which is the current SOTA ERC model except for the prompt-based ones. We attribute this improvement to the employment of prompt in the ERC task and the effective way of incorporating contextual information and commonsense into the prompt.\n\nCompared with MELD, all models' performance on EmoryNLP declines apparently, due to the more \"obscure\" emotional expressions in the utterances of this dataset. Nonetheless, our CISPER outperforms COSMIC with the improvement of 1.75%, which is more significant than that in MELD. Please note that COSMIC also leverages contextual information and commonsense as CISPER. Thus, CISPER's superior performance shows that leveraging these two significant information through our proposed prompt is more effective than the solution in COSMIC on enhancing ERC performance. Especially for the conversations with more \"obscure\" emotional expressions as MELD, the prompt in CISPER can guide the PLM to recall its latent knowledge related to the emotional cues, which has been learned in the PLM's pre-training.\n\nMicro Comparisons Since COSMIC is the current SOTA model, we further compared the performance of CISPER and COSMIC on the level of seven emotion categories. In Fig.  3 , each sector of a certain color corresponds to a certain emotion category, of which the size of the sectorial area quantifies the sample number. Meanwhile, the sample proportion and corresponding number of each category are also listed beside the sector. Fig.  3  shows that, on MELD, our CISPER has the performance nearly equivalent to COSMIC on neutral, surprise, joy and anger, while has better performance on sadness, fear and disgust. CISPER's advantage is more obvious in the categories of fear (+13.17%) and disgust (+12.02%).Compared with MELD, CISPER's performance on EmoryNLP is better than COSMIC in more emotion categories, i.e., powerful (+1.47%), peaceful (+7.28%), mad (+2.77%) and scared (+4.87%).\n\nIn addition, from Fig.  3  we can easily find that both of the two compared models have high weighted-F1 on neutral, joy, anger, mad, surprise and scared. It is because in general these popular categories of emotions are obviously expressed in the utterances. Both models perform poorly on sad (sadness) fear, disgust, peaceful and powerful. The reason is two-fold: on the one hand, the utterances' emotions are inherently obscure. On the other hand, the utterances of these emotions are relatively rare in the conversations, so the models cannot obtain satisfactory performance only with sparse training data. Notably, with the help of contextual semantics and commonsense-based prompt, our CISPER can take full advantage of its latent knowledge related to the emotional expressions in utterances. Particularly, CISPER outperforms COSMIC especially in the emotion categories with fewer samples. Such results also prove CISPER's capability of few-shot learning, which is consistent with the findings in previous work  [17]  about prompt-based models' advantage in few-shot learning tasks.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Ablation Studies",
      "text": "The main innovation of our work is to use two Transformer encoders to blend and encode two types of significant information, i.e., contextual information and commonsense, to generate an effective continuous prompt that guides the PLM to achieve ERC better. To verify the effectiveness of either type of prompt information, we added three ablated variants of CISPER into performance comparisons, as shown in Table  4  where all models' weighted-F1 scores and the improvements w.r.t. that of the variant without prompt are both listed. If one type of prompt information is not incorporated, we use randomly initialized embeddings (denoted as \"no\") to replace our proposed continuous prompt (denoted as \"yes\"). The results in Table  4  show that, although the random embeddings have the same amount of parameters as the continuous prompt, they can not help the model sufficiently since they contain no meaningful information. We also find that either contextual information or commonsense is helpful for the model to improve ERC performance on these two datasets. Specifically, contextual information brings a more apparent performance improvement than commonsense on both of the two datasets. Furthermore, incorporating these two types of information results in more performance improvement. Even without those two types of information, our model still outperforms P-tuning, justifying the advantage of our model structure. In addition, as shown in Table  3 , CISPER's superiority over CISPER(left) and CISPER(right) shows that the symmetrical prompt structure is better than the one side structure.",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "Prompt Length Decision",
      "text": "Unlike manually designed prompt templates with fixed length and explicit semantics, the continuous prompt in our model is in fact a group of embeddings and has no explicit semantics. To investigate the influence of prompt length on model performance, we compared CISPER's performance when setting N e = N p = 1 ∼ 5 (the corresponding pseudo token number is  4, 8, 12, 16, 20) . According to the results in Fig.  4 , we set N e = N p = 3 when comparing CISPER with the baselines.\n\nIn fact, the small value of N e /N p can not ensure the prompt to bring adequate emotional cues for the PLM. While the large value of N e /N p may result in redundant information that disturbs the model.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Case Study",
      "text": "In actual conversation scenarios, many utterances contain very few words, making their real emotions hard to be recognized. We illustrate such a situation by an actual case from our test set, as shown in Fig.  5  where the emotion of Speaker A's utterance: \"Ugh!\" is disgust in fact. Obviously, it is tough to identify Speaker A's emotion expressed on this utterance only with such a single word. For this case, COSMIC failed to recognize the emotion of \"Ugh!\", although it has leveraged contextual semantics and commonsense information. Comparatively, CISPER can exploit the contextual semantics in the conversation and the speaker/listener's state thoroughly by the sophisticated prompt generation. Furthermore, with the prompt+LM tuning paradigm, CISPER successfully identifies the emotion of this utterance as disgust.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we propose an ERC model CISPER which blends contextual information and common sense related to the utterances in a conversation into continuous prompt for enhanced ERC performance. Unlike previous ERC methods adopting fine-tuning paradigm, our CISPER achieves ERC with the paradigm of prompt+LM tuning, which explicitly brings the information related to emotional expressions in the conversation to the PLM. With the help of contextual information and common sense based prompt, our model can well handle the challenge of recognizing the implicit emotional expressions in the utterances. Our experiments show that, our CISPER significantly outperforms the state-of-the-art ERC models especially on some critical emotion categories.",
      "page_start": 15,
      "page_end": 15
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: A toy example of recognizing utterance emotion based on the cues from the",
      "page": 2
    },
    {
      "caption": "Figure 1: to explain the",
      "page": 2
    },
    {
      "caption": "Figure 2: The overall framework of our proposed CISPER. It has three main steps: feature",
      "page": 5
    },
    {
      "caption": "Figure 2: , of which the pipeline can be mainly divided into the following",
      "page": 5
    },
    {
      "caption": "Figure 1: Inspired by it, we generate two groups of continuous prompt embeddings",
      "page": 7
    },
    {
      "caption": "Figure 3: Micro performance comparisons of CISPER and COSMIC on emotion category",
      "page": 12
    },
    {
      "caption": "Figure 3: , each sector of a certain color corresponds to a",
      "page": 12
    },
    {
      "caption": "Figure 3: shows that, on MELD,",
      "page": 12
    },
    {
      "caption": "Figure 3: we can easily ﬁnd that both of the two compared",
      "page": 13
    },
    {
      "caption": "Figure 4: , we set Ne = Np = 3 when comparing CISPER with the baselines.",
      "page": 14
    },
    {
      "caption": "Figure 4: CISPER’s performance with diﬀerent prompt lengths. The X-axis is the value",
      "page": 14
    },
    {
      "caption": "Figure 5: A conversation case of MELD. The baseline COSMIC can not correctly identify",
      "page": 14
    },
    {
      "caption": "Figure 5: where the emotion of Speaker A’s",
      "page": 14
    }
  ],
  "tables": [
    {
      "caption": "Table 1: 9 relation types of commonsense used in CISPER.",
      "data": [
        {
          "Notation Type token Relation meaning": "xIntent\nxAttr\nxNeed\nxWant\nxEﬀect\nxReact\noWant\noEﬀect\noReact"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 3: ERC performance (weighted-F1) comparisons of all compared models.",
      "data": [
        {
          "Paradigm": "Fine-tuning",
          "Language Model ERC Model": "Glove-based",
          "MELD EmoryNLP": "32.59%\n34.19%\n-\n31.70%\n-\n35.45%"
        },
        {
          "Paradigm": "Fine-tuning",
          "Language Model ERC Model": "BERT&RoBERTa DialogueTRM\n-based",
          "MELD EmoryNLP": "35.92%\n34.73%\n-\n37.44%\n39.02%\n38.11%"
        },
        {
          "Paradigm": "Prompt+LM tuning",
          "Language Model ERC Model": "RoBERTa-based",
          "MELD EmoryNLP": "38.67%\n37.97%\n39.46%\n39.39%\n39.86%"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 4: where all models’ weighted-F1 scores and the",
      "data": [
        {
          "Commonsense Contextual Info. MELD": "no\nyes\nno\nyes",
          "EmoryNLP": "38.02%\n39.42% (+1.40%)\n38.97% (+0.95%)"
        }
      ],
      "page": 13
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Pada: A prompt-based autoregressive approach for adaptation to unseen domains",
      "authors": [
        "E Ben-David",
        "N Oved",
        "R Reichart"
      ],
      "year": "2021",
      "venue": "Pada: A prompt-based autoregressive approach for adaptation to unseen domains",
      "arxiv": "arXiv:2102.12206"
    },
    {
      "citation_id": "2",
      "title": "Comet: Commonsense transformers for automatic knowledge graph construction",
      "authors": [
        "A Bosselut",
        "H Rashkin"
      ],
      "year": "2019",
      "venue": "Comet: Commonsense transformers for automatic knowledge graph construction",
      "arxiv": "arXiv:1906.05317"
    },
    {
      "citation_id": "3",
      "title": "Language models are few-shot learners",
      "authors": [
        "T Brown",
        "B Mann"
      ],
      "year": "2020",
      "venue": "Language models are few-shot learners",
      "arxiv": "arXiv:2005.14165"
    },
    {
      "citation_id": "4",
      "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M Chang"
      ],
      "year": "2018",
      "venue": "BERT: pre-training of deep bidirectional transformers for language understanding"
    },
    {
      "citation_id": "5",
      "title": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "D Ghosal",
        "N Majumder"
      ],
      "year": "2019",
      "venue": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "arxiv": "arXiv:1908.11540"
    },
    {
      "citation_id": "6",
      "title": "Cosmic: Commonsense knowledge for emotion identification in conversations",
      "authors": [
        "D Ghosal",
        "N Majumder"
      ],
      "year": "2020",
      "venue": "Cosmic: Commonsense knowledge for emotion identification in conversations",
      "arxiv": "arXiv:2010.02795"
    },
    {
      "citation_id": "7",
      "title": "Story ending generation with incremental encoding and commonsense knowledge",
      "authors": [
        "J Guan",
        "Y Wang",
        "M Huang"
      ],
      "year": "2019",
      "venue": "Proc. of AAAI"
    },
    {
      "citation_id": "8",
      "title": "Warp: Word-level adversarial reprogramming",
      "authors": [
        "K Hambardzumyan",
        "H Khachatrian",
        "J May"
      ],
      "year": "2021",
      "venue": "Warp: Word-level adversarial reprogramming",
      "arxiv": "arXiv:2101.00121"
    },
    {
      "citation_id": "9",
      "title": "Long short-term memory",
      "authors": [
        "S Hochreiter",
        "J Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural Computation"
    },
    {
      "citation_id": "10",
      "title": "Mmgcn: Multimodal fusion via deep graph convolution network for emotion recognition in conversation",
      "authors": [
        "J Hu",
        "Y Liu"
      ],
      "year": "2021",
      "venue": "Mmgcn: Multimodal fusion via deep graph convolution network for emotion recognition in conversation",
      "arxiv": "arXiv:2107.06779"
    },
    {
      "citation_id": "11",
      "title": "How can we know what language models know?",
      "authors": [
        "Z Jiang",
        "F Xu"
      ],
      "year": "2020",
      "venue": "How can we know what language models know?"
    },
    {
      "citation_id": "12",
      "title": "Convolutional neural networks for sentence classification",
      "authors": [
        "Y Kim"
      ],
      "year": "2014",
      "venue": "Convolutional neural networks for sentence classification"
    },
    {
      "citation_id": "13",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "J Kingma",
        "J Ba"
      ],
      "year": "2015",
      "venue": "Proc. of ICLR"
    },
    {
      "citation_id": "14",
      "title": "Sentiprompt: Sentiment knowledge enhanced prompt-tuning for aspect-based sentiment analysis",
      "authors": [
        "C Li",
        "F Gao"
      ],
      "year": "2021",
      "venue": "Sentiprompt: Sentiment knowledge enhanced prompt-tuning for aspect-based sentiment analysis"
    },
    {
      "citation_id": "15",
      "title": "Multi-task learning with auxiliary speaker identification for conversational emotion recognition",
      "authors": [
        "J Li",
        "M Zhang"
      ],
      "year": "2020",
      "venue": "Multi-task learning with auxiliary speaker identification for conversational emotion recognition"
    },
    {
      "citation_id": "16",
      "title": "Prefix-tuning: Optimizing continuous prompts for generation",
      "authors": [
        "X Li",
        "P Liang"
      ],
      "year": "2021",
      "venue": "Prefix-tuning: Optimizing continuous prompts for generation",
      "arxiv": "arXiv:2101.00190"
    },
    {
      "citation_id": "17",
      "title": "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing",
      "authors": [
        "P Liu",
        "W Yuan"
      ],
      "year": "2021",
      "venue": "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing"
    },
    {
      "citation_id": "18",
      "title": "Gpt understands, too",
      "authors": [
        "X Liu",
        "Y Zheng"
      ],
      "year": "2021",
      "venue": "Gpt understands, too",
      "arxiv": "arXiv:2103.10385"
    },
    {
      "citation_id": "19",
      "title": "Roberta: A robustly optimized BERT pretraining approach",
      "authors": [
        "Y Liu",
        "M Ott"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized BERT pretraining approach"
    },
    {
      "citation_id": "20",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Y Liu",
        "M Ott"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "21",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "N Majumder",
        "S Poria"
      ],
      "year": "2019",
      "venue": "Proc. of AAAI"
    },
    {
      "citation_id": "22",
      "title": "Dialoguetrm: Exploring the intra-and inter-modal emotional behaviors in the conversation",
      "authors": [
        "Y Mao",
        "Q Sun"
      ],
      "year": "2020",
      "venue": "Dialoguetrm: Exploring the intra-and inter-modal emotional behaviors in the conversation",
      "arxiv": "arXiv:2010.07637"
    },
    {
      "citation_id": "23",
      "title": "Efficient estimation of word representations in vector space",
      "authors": [
        "T Mikolov",
        "K Chen",
        "G Corrado",
        "J Dean"
      ],
      "year": "2013",
      "venue": "Efficient estimation of word representations in vector space",
      "arxiv": "arXiv:1301.3781"
    },
    {
      "citation_id": "24",
      "title": "Glove: Global vectors for word representation",
      "authors": [
        "J Pennington",
        "R Socher",
        "C Manning"
      ],
      "year": "2014",
      "venue": "Proc. of EMNLP"
    },
    {
      "citation_id": "25",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika"
      ],
      "year": "2018",
      "venue": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "26",
      "title": "Emotion recognition in conversation: Research challenges, datasets, and recent advances",
      "authors": [
        "S Poria",
        "N Majumder"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "27",
      "title": "Dcr-net: A deep co-interactive relation network for joint dialog act recognition and sentiment classification",
      "authors": [
        "L Qin",
        "W Che"
      ],
      "year": "2020",
      "venue": "Proc. of AAAI"
    },
    {
      "citation_id": "28",
      "title": "Atomic: An atlas of machine commonsense for if-then reasoning",
      "authors": [
        "M Sap",
        "R Le Bras"
      ],
      "year": "2019",
      "venue": "Proc. of AAAI"
    },
    {
      "citation_id": "29",
      "title": "Exploiting cloze questions for few shot text classification and natural language inference",
      "authors": [
        "T Schick",
        "H Schütze"
      ],
      "year": "2020",
      "venue": "Exploiting cloze questions for few shot text classification and natural language inference",
      "arxiv": "arXiv:2001.07676"
    },
    {
      "citation_id": "30",
      "title": "Few-shot text generation with pattern-exploiting training",
      "authors": [
        "T Schick",
        "H Schütze"
      ],
      "year": "2020",
      "venue": "Few-shot text generation with pattern-exploiting training",
      "arxiv": "arXiv:2012.11926"
    },
    {
      "citation_id": "31",
      "title": "Dialogxl: All-in-one xlnet for multi-party conversation emotion recognition",
      "authors": [
        "W Shen",
        "J Chen"
      ],
      "year": "2020",
      "venue": "Dialogxl: All-in-one xlnet for multi-party conversation emotion recognition",
      "arxiv": "arXiv:2012.08695"
    },
    {
      "citation_id": "32",
      "title": "Directed acyclic graph network for conversational emotion recognition",
      "authors": [
        "W Shen",
        "S Wu"
      ],
      "year": "2021",
      "venue": "Directed acyclic graph network for conversational emotion recognition",
      "arxiv": "arXiv:2105.12907"
    },
    {
      "citation_id": "33",
      "title": "Conceptnet 5.5: An open multilingual graph of general knowledge",
      "authors": [
        "R Speer",
        "J Chin",
        "C Havasi"
      ],
      "year": "2017",
      "venue": "Proc. of AAAI"
    },
    {
      "citation_id": "34",
      "title": "Context-and sentiment-aware networks for emotion recognition in conversation",
      "authors": [
        "G Tu",
        "J Wen",
        "C Liu",
        "D Jiang",
        "E Cambria"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Artificial Intelligence"
    },
    {
      "citation_id": "35",
      "title": "Diverse and informative dialogue generation with contextspecific commonsense knowledge awareness",
      "authors": [
        "S Wu",
        "Y Li"
      ],
      "year": "2020",
      "venue": "Proc. of ACL"
    },
    {
      "citation_id": "36",
      "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
      "authors": [
        "Z Yang",
        "Z Dai"
      ],
      "year": "2019",
      "venue": "Proc. of NeurIPS"
    },
    {
      "citation_id": "37",
      "title": "Bartscore: Evaluating generated text as text generation",
      "authors": [
        "W Yuan",
        "G Neubig",
        "P Liu"
      ],
      "year": "2021",
      "venue": "Bartscore: Evaluating generated text as text generation",
      "arxiv": "arXiv:2106.11520"
    },
    {
      "citation_id": "38",
      "title": "Emotion detection on tv show transcripts with sequencebased convolutional neural networks",
      "authors": [
        "S Zahiri",
        "J Choi"
      ],
      "year": "2018",
      "venue": "Proc. of AAAI"
    },
    {
      "citation_id": "39",
      "title": "Modeling both context-and speaker-sensitive dependence for emotion detection in multi-speaker conversations",
      "authors": [
        "D Zhang",
        "L Wu"
      ],
      "year": "2019",
      "venue": "Proc. of IJCAI"
    },
    {
      "citation_id": "40",
      "title": "Knowledge-enriched transformer for emotion detection in textual conversations",
      "authors": [
        "P Zhong",
        "D Wang",
        "C Miao"
      ],
      "year": "2019",
      "venue": "Knowledge-enriched transformer for emotion detection in textual conversations",
      "arxiv": "arXiv:1909.10681"
    }
  ]
}