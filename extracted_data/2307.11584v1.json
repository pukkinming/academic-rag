{
  "paper_id": "2307.11584v1",
  "title": "A Change Of Heart: Improving Speech Emotion Recognition Through Speech-To-Text Modality Conver-Sion",
  "published": "2023-07-21T13:48:11Z",
  "authors": [
    "Zeinab Sadat Taghavi",
    "Ali Satvaty",
    "Hossein Sameti"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech Emotion Recognition (SER) is a challenging task. In this paper, we introduce a modality conversion concept aimed at enhancing emotion recognition performance on the MELD dataset. We assess our approach through two experiments: first, a method named Modality-Conversion that employs automatic speech recognition (ASR) systems, followed by a text classifier; second, we assume perfect ASR output and investigate the impact of modality conversion on SER, this method is called Modality-Conversion++. Our findings indicate that the first method yields substantial results, while the second method outperforms stateof-the-art (SOTA) speech-based approaches in terms of SER weighted-F1 (WF1) score on the MELD dataset. This research highlights the potential of modality conversion for tasks that can be conducted in alternative modalities.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition from speech is challenging due to audio complexity and expression variability. SOTA baseline approaches rely on a one-modality approach (4). However, if there is one modality that performs better than speech  (10) , can converting the modality improve results when using the modality that had lower performance on the same dataset? In this paper, we examine this idea with two experiments: first, we introduce a model named Modality-Conversion that employs ASR and then uses a text classifier; second, we assume a perfect ASR output and examine the impact of modality conversion on emotion recognition; we call this method Modality-Conversion++. While the first method achieved significant results compared to SOTA, the second method improves emotion recognition performance even further, compared to SOTA speech-based approaches on the MELD dataset from the TV series Friends (4). The study demonstrates the potential of modality conversion and text-based techniques for enhancing emotion recognition from speech.  1",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Works",
      "text": "Emotion recognition from speech has been studied extensively, with many approaches focusing on acoustic features extracted from speech signals (3) (5) (6) (4)  (10) . In the context of emotion recognition, researchers have also explored the use of text-based features (13)  (8) . Multi-modality approaches, which combine both speech and text-based features, have also been proposed and have shown promising results in emotion recognition and show that on the same dataset, text-based approaches achieve better results than speech-based approaches (7) (10) (12) (16)  (11)    (14)  (2) (17)  (15) . However, to the best of our knowledge, no one has explored using modality conversion to improve emotion recognition from any of the modalities on the MELD dataset.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Method",
      "text": "In this study, we utilized the MELD dataset consisting of audio-visual clips. In the first method, we extracted the audio from the clips and then used the Vosk API 2  for ASR to convert the audio clips to text  (9) . In the second method, we assumed an ideal ASR with optimal accuracy and used the gold transcript of each speech track. For both methods, we used a RoBERTa-base text classification model, fine-tuned on the modality converted text transcripts with emotion labels (1). We evaluated the performance of our proposed modality conversion approach, as shown in Figure  1 , using the WF1 metric.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Results",
      "text": "We compared the performance of our proposed modality conversion approach with the current SOTA speech-based models on the MELD dataset. The results, as presented in Table  1 , show that our first method achieved a competitive WF1 score of 43.1%, which is even higher than SpeechFormer (4). Moreover, our second method outperforms the baseline models, achieving a WF1 score of 60.4%. These results demonstrate the effectiveness of our proposed modality conversion approach for emotion recognition from speech.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Method",
      "text": "Input Modality Using Modality Conversion WF1(%) SpeechFormer (  7",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Conclusion",
      "text": "Based on our experiments on the MELD dataset, we proposed an alternative to traditional approaches: a modality conversion idea for tasks that have better performance on one modality than others, especially for SER. We examined this idea with two methods: first, the Vosk API for ASR as modality conversion, and second, considering an ideal modality conversion stage by using gold text. Finally, both methods employed a pre-trained RoBERTa language model for emotion recognition on the text transcripts. Our approach resulted in a WF1 score of 43.1% for the first method and 60.4% for the second method, which is an 11.6% improvement over the SOTA speech-based models on the same dataset, showing the potential of modality conversion idea.",
      "page_start": 2,
      "page_end": 2
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , using the",
      "page": 2
    },
    {
      "caption": "Figure 1: Our proposed idea about the emotion recognition pipeline consists of two main steps:",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table 1: , show that our",
      "data": [
        {
          "Method": "SpeechFormer (7)\nSpeechFormer++ (5)\nDWFormer (3)\nDST (6)",
          "Input Modality": "Speech\nSpeech\nSpeech\nSpeech",
          "Using Modality Conversion": "-\n-\n-\n-",
          "WF1(%)": "41.9\n47.0\n48.5\n48.8"
        },
        {
          "Method": "Modality-Conversion\nModality-Conversion++",
          "Input Modality": "Speech\nSpeech",
          "Using Modality Conversion": "Converting to Text Modality\nConverting to Text Modality",
          "WF1(%)": "43.1\n60.4"
        }
      ],
      "page": 2
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "TweetEval: Unified benchmark and comparative evaluation for tweet classification",
      "authors": [
        "Francesco Barbieri",
        "Jose Camacho-Collados",
        "Luis Espinosa Anke",
        "Leonardo Neves"
      ],
      "year": "2020",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020",
      "doi": "10.18653/v1/2020.findings-emnlp.148"
    },
    {
      "citation_id": "2",
      "title": "Whose emotion matters? speaking activity localisation without prior knowledge",
      "authors": [
        "Hugo Carneiro",
        "Cornelius Weber",
        "Stefan Wermter"
      ],
      "year": "2022",
      "venue": "Whose emotion matters? speaking activity localisation without prior knowledge"
    },
    {
      "citation_id": "3",
      "title": "Dwformer: Dynamic window transformer for speech emotion recognition",
      "authors": [
        "Shuaiqi Chen",
        "Xiaofen Xing",
        "Weibin Zhang",
        "Weidong Chen",
        "Xiangmin Xu"
      ],
      "year": "2023",
      "venue": "Dwformer: Dynamic window transformer for speech emotion recognition"
    },
    {
      "citation_id": "4",
      "title": "Speechformer: A hierarchical efficient framework incorporating the characteristics of speech",
      "authors": [
        "Weidong Chen",
        "Xiaofen Xing",
        "Xiangmin Xu",
        "Jianxin Pang",
        "Lan Du"
      ],
      "year": "2022",
      "venue": "Speechformer: A hierarchical efficient framework incorporating the characteristics of speech"
    },
    {
      "citation_id": "5",
      "title": "Speechformer++: A hierarchical efficient framework for paralinguistic speech processing",
      "authors": [
        "Weidong Chen",
        "Xiaofen Xing",
        "Xiangmin Xu",
        "Jianxin Pang",
        "Lan Du"
      ],
      "year": "2023",
      "venue": "Speechformer++: A hierarchical efficient framework for paralinguistic speech processing",
      "doi": "10.48550/ARXIV.2302.14638"
    },
    {
      "citation_id": "6",
      "title": "Dst: Deformable speech transformer for emotion recognition",
      "authors": [
        "Weidong Chen",
        "Xiaofen Xing",
        "Xiangmin Xu",
        "Jianxin Pang",
        "Lan Du"
      ],
      "year": "2023",
      "venue": "Dst: Deformable speech transformer for emotion recognition"
    },
    {
      "citation_id": "7",
      "title": "M2fnet: Multi-modal fusion network for emotion recognition in conversation",
      "authors": [
        "Purbayan Vishal Chudasama",
        "Ashish Kar",
        "Nirmesh Gudmalwar",
        "Pankaj Shah",
        "Naoyuki Wasnik",
        "Onoe"
      ],
      "year": "2022",
      "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
      "doi": "10.1109/CVPRW56347.2022.00511"
    },
    {
      "citation_id": "8",
      "title": "CoMPM: Context modeling with speaker's pre-trained memory tracking for emotion recognition in conversation",
      "authors": [
        "Joosung Lee",
        "Wooin Lee"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/2022.naacl-main.416"
    },
    {
      "citation_id": "9",
      "title": "Vosk speech recognition toolkit: Offline speech recognition api for android, ios, raspberry pi and servers with python, java, c# and node",
      "authors": [
        "Llc Multimedia"
      ],
      "year": "2020",
      "venue": "Vosk speech recognition toolkit: Offline speech recognition api for android, ios, raspberry pi and servers with python, java, c# and node"
    },
    {
      "citation_id": "10",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Gautam Naik",
        "Erik Cambria",
        "Rada Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P19-1050"
    },
    {
      "citation_id": "11",
      "title": "Multimodal speech emotion recognition and ambiguity resolution",
      "authors": [
        "Gaurav Sahu"
      ],
      "year": "2019",
      "venue": "Multimodal speech emotion recognition and ambiguity resolution"
    },
    {
      "citation_id": "12",
      "title": "Jointly Fine-Tuning \"BERT-Like\" Self Supervised Models to Improve Multimodal Speech Emotion Recognition",
      "authors": [
        "Shamane Siriwardhana",
        "Andrew Reis",
        "Rivindu Weerasekera",
        "Suranga Nanayakkara"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech 2020",
      "doi": "10.21437/Interspeech.2020-1212"
    },
    {
      "citation_id": "13",
      "title": "Supervised prototypical contrastive learning for emotion recognition in conversation",
      "authors": [
        "Xiaohui Song",
        "Longtao Huang",
        "Hui Xue",
        "Songlin Hu"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "14",
      "title": "Using auxiliary tasks in multimodal fusion of wav2vec 2.0 and bert for multimodal emotion recognition",
      "authors": [
        "Dekai Sun",
        "Yancheng He",
        "Jiqing Han"
      ],
      "year": "2023",
      "venue": "Using auxiliary tasks in multimodal fusion of wav2vec 2.0 and bert for multimodal emotion recognition"
    },
    {
      "citation_id": "15",
      "title": "Multimodal cross-and self-attention network for speech emotion recognition",
      "authors": [
        "Licai Sun",
        "Bin Liu",
        "Jianhua Tao",
        "Zheng Lian"
      ],
      "year": "2021",
      "venue": "ICASSP 2021 -2021 IEEE International Conference on Acoustics, Speech and Signal Processing",
      "doi": "10.1109/icassp39728.2021.9414654"
    },
    {
      "citation_id": "16",
      "title": "Multimodal speech emotion recognition using audio and text",
      "authors": [
        "Seunghyun Yoon",
        "Seokhyun Byun",
        "Kyomin Jung"
      ],
      "year": "2018",
      "venue": "2018 IEEE Spoken Language Technology Workshop (SLT)",
      "doi": "10.1109/slt.2018.8639583"
    },
    {
      "citation_id": "17",
      "title": "Knowledge-aware bayesian co-attention for multimodal emotion recognition",
      "authors": [
        "Zihan Zhao",
        "Yu Wang",
        "Yanfeng Wang"
      ],
      "year": "2023",
      "venue": "Knowledge-aware bayesian co-attention for multimodal emotion recognition"
    }
  ]
}