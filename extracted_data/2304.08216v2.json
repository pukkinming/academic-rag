{
  "paper_id": "2304.08216v2",
  "title": "Context-Dependent Embedding Utterance Representations For Emotion Recognition In Conversations",
  "published": "2023-04-17T12:37:57Z",
  "authors": [
    "Patrícia Pereira",
    "Helena Moniz",
    "Isabel Dias",
    "Joao Paulo Carvalho"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion Recognition in Conversations (ERC) has been gaining increasing importance as conversational agents become more and more common. Recognizing emotions is key for effective communication, being a crucial component in the development of effective and empathetic conversational agents. Knowledge and understanding of the conversational context are extremely valuable for identifying the emotions of the interlocutor. We thus approach Emotion Recognition in Conversations leveraging the conversational context, i.e., taking into attention previous conversational turns. The usual approach to model the conversational context has been to produce context-independent representations of each utterance and subsequently perform contextual modeling of these. Here we propose context-dependent embedding representations of each utterance by leveraging the contextual representational power of pretrained transformer language models. In our approach, we feed the conversational context appended to the utterance to be classified as input to the RoBERTa encoder, to which we append a simple classification module, thus discarding the need to deal with context after obtaining the embeddings since these constitute already an efficient representation of such context. We also investigate how the number of introduced conversational turns influences our model performance. The effectiveness of our approach is validated on the open-domain DailyDialog dataset and on the task-oriented EmoWOZ dataset.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion Recognition in Conversations (ERC) is useful in automatic opinion mining, emotion-aware conversational agents and assisting modules for therapeutic practices. There is thus an increasing interest in endowing machines with efficient emotion recognition modules. Knowledge and understanding of the conversational context, i.e., of the previous conversation turns, are extremely valuable in identifying the emotions of the interlocutors  (Poria et al., 2019 )  (Chatterjee et al., 2019)    (Pereira et al., 2022) .\n\nResearch in automatic emotion recognition using machine learning techniques dates back to the end of the 20th century. However, the use of the conversational context as an auxiliary information for the classifiers, did not appear until publicly available conversational datasets became more common.\n\nState-of-the-art ERC works leverage not only state-of-the-art pre-trained-language models such as BERT  (Devlin et al., 2018)  and RoBERTa  (Liu et al., 2019) , but also deep, complex architectures to model several factors that influence the emotions in the conversation  (Pereira et al., 2022) . Such fac-1 arXiv:2304.08216v2 [cs.CL] 3 Jun 2023 tors usually pertain to self and inter-speaker emotional influence and the context and emotion of preceeding utterances.\n\nIn this paper we argue that the powerful representation capabilities of pre-trained language models can be leveraged to model context without the need of additional elaborate classifier architectures, allowing for much simpler and efficient architectures. Furthermore, it is our contention that the Transformer, the backbone of our chosen language model, is better at preserving the contextual information since it has a shorter path of information flow than the RNNs typically used for context modelling. In this line, we rely on the RoBERTa language model and resort to a simple classification module to preserve the contextual information.\n\nThe usual approach to model the conversational context has been to produce context independent representations of each utterance and subsequently perform contextual modeling of those representations. State-of-the art approaches start by resorting to embedding representations from language models and employ gated or graph neural network architectures to perform contextual modelling of these embedding representations at a later step. In our much simpler and efficient proposed approach, we produce context-dependent embedding representations of each utterance, by feeding not only the utterance but also its conversational context to the language model. We thus discard the need to deal with context after obtaining the embeddings since these constitute already an efficient representation of such context.\n\nOur experiments show that by leveraging context in this way, one can obtain state-of-the-art results with RoBERTa and a simple classification module, surpassing more complex state-of-the-art models.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Amongst the first works considering contextual interdependences among utterances is the one by  Poria et al. (Poria et al., 2017) . It uses LSTMs to extract contextual features from the utterances. These gated recurrent networks make it possible to share information between consecutive utterances while preserving its order.\n\nA more elaborate model also leveraging gated recurrent networks is DialogueRNN  (Majumder et al., 2019) , which uses GRUs to model the speaker, context and emotion of preceding utterances by keeping a party state and a global state that are used to model the final emotion representation.\n\nGated recurrent networks have a long path of information flow which makes it difficult to capture long term dependencies. These can be better captured with the Transformer which a has shorter path of information flow. Its invention in 2017  (Vaswani et al., 2017)  led to a new state-of-the-art in several Natural Language Processing tasks.\n\nAmongst the first works leveraging the Transformer is the Knowledge-Enriched Transformer (KET)  (Zhong et al., 2019) . It uses its self-attention to model context and response. It also makes use of an external knowledge base, a graph of concepts that is retrieved for each word.\n\nFollowing the invention of Transformers, pretrained language models brought about another new state-of-the art in 2019. Since their invention, most state-of-the art ERC works resorted to encoder pre-trained language models  (Shen et al., 2021a)    (Ghosal et al., 2020)    (Li et al., 2021) .\n\nCOSMIC  (Ghosal et al., 2020)  leverages RoBERTa Large as feature extractor. Furthermore, it makes use of the commonsense transformer model COMET  (Bosselut et al., 2019)  in order to extract commonsense features. Five bi-directional GRUs model a context state, internal state, external state, intent state, and emotion state that influence the final emotion classification.\n\nPsychological  (Li et al., 2021 ) also uses RoBERTa Large for utterance encoding and COMET. For conversation-level encoding it constructs a graph of utterances to model the actions and intentions of the speaker along with the interactions with other utterances. It uses COMET to introduce commonsense knowledge into the graph edge representations and processes this graph using a graph transformer network.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methodology",
      "text": "We describe how we obtain a contextual embedding representation of the sentence and its context with RoBERTa, how we pool the contextual embeddings, our classification module and how we obtain the emotion labels. These processes can be observed in Figure  2 .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Task Definition",
      "text": "Given a conversation, a sequence of u i utterances with corresponding emotion i from a predefined set of emotions, the aim of the task of ERC is to correctly assign an emotion to each utterance of the conversation. An utterance consists in a sequence of w it tokens representing its T i words\n\nThe usual approach for this task has been to produce context independent representations of each utterance and perform contextual modeling of these. In our approach we produce context-dependent representations of each utterance that represent not only the utterance but also a given number of previous utterances from the conversation.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Context-Dependent Feature Extraction",
      "text": "For context-dependent feature extraction, we feed as input to RoBERTa the utterance we intend to classify, u i , concatenated with its conversational context corresponding to the number c of previous utterances in the conversation, (u i-1 , u i-2 , ..., u i-c ). Concretely, we feed u i to the model, preceded by the [CLS] token and suceded by the [SEP] token, followed by the previous turns u i-1 up to u i-c , separated by the [SEP] token.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Pooling",
      "text": "The RoBERTa encoder outputs several layers of embeddings representing the utterance, and in our approach, also the preceding utterances it receives as input. Each layer comprises several tokens, being the number of tokens the same as the number of input tokens. Each token is a vector with dimension corresponding to the RoBERTa hidden size.\n\nFrom these embeddings one can extract a suitable representation for the sentence. Choosing all tokens from all layers would yield an extremely memory demanding classification layer and may not yield the best model performance. Thus we choose the first embedding from the last layer L, the [CLS] which is used for classification, as in Equation  2 .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Emotion Classification",
      "text": "The classification module that follows RoBERTa is a linear fully connected layer, applying a linear transformation to the pooled encoder output data. Its input size is the RoBERTa encoder hidden size and its output size is the number of emotion classes.\n\nThe final label probability distribution is yielded by applying the softmax operation to the output of the classification head and the predicted label is the one with the highest probability:\n\n(3)\n\n4 Experimental Setup",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Training",
      "text": "Our model is based on RoBERTa-base from the Transformers library by Hugging Face  (Wolf et al., 2020) . It is trained with the cross-entropy loss with logits. The Adam (Kingma and Ba, 2014) optimizer is used with an initial learning rate of 1e-5 and 5e-5, for the encoder and the classification head, respectively with a layer-wise decay rate of 0.95 after each training epoch for the encoder. The encoder is frozen for the first epoch. The batch size is set to 4. Gradient clipping is set to 1.0. As stopping criteria, early stopping is used to terminate training if there is no improvement after 5 consecutive epochs on the validation set over macro-F1, for a maximum of 10 epochs. The checkpoint used in testing is the one that achieves the highest macro-F1 score on the validation set.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Evaluation",
      "text": "We evaluate the performance of our model with the macro F1-score. The reported results are yielded from an average of 5 runs corresponding to 5 distinct random seeds that are kept for a meaningful comparison of all experiments. This average is motivated by the fact that results for the same experiment obtained with different random seeds can have a variability of about 3 in macro F1-score which is a large deviation given that our proposed approach yields an improvement of that magnitude and comparison between state-of-the-art models are based on improvements of less than 1 F1score. This procedure is in line with several authors that also resort to 5 run averages  (Li et al., 2021 )  (Zhong et al., 2019 )  (Shen et al., 2021a )  (Shen et al., 2021b) .\n\nOur code is publicly available 1 .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Datasets",
      "text": "We evaluate our approach on the chit-chat Daily-Dialog  (Li et al., 2017)  dataset and on the taskoriented EmoWOZ  (Feng et al., 2022)  dataset.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Dailydialog",
      "text": "DailyDialog is built from websites used to practice English dialogue in daily life. It is labelled with the six Ekman's basic emotions  (Ekman, 1999) , anger, disgust, fear, happiness, sadness and surprise, or neutral. The publicly available splits of Yanran are used.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Emowoz",
      "text": "EmoWOZ is derived from  MultiWOZ et al., 2018) , one of the largest multi-domain corpora benchmark dataset for various dialogue tasks. User utterances are annotated with either fear, dissatisfaction, apologetic, abusive, excited, satisfied or neutral emotions.\n\nThe statistics and proportion of labels in the datasets are presented in Tables  1  and 2 , respectively. From Table  2  it can be observed that both datasets are imbalanced, not only for its dominant majority neutral class, but also for the relative imbalance between minority classes. Therefore, we have opted to use the macro-F1 score for evaluation in order to promote consistent performance across all classes.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Results And Analysis",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Iterating Towards The Ideal Approach",
      "text": "We have performed extensive experiments in order to obtain our ideal model architecture. From experimenting different approaches to pool the various layers of embeddings RoBERTa provides to choosing which classification module to employ withing a wide variety of deep learning architectures, we put forward our experiments in this subsection.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Fine-Tuning",
      "text": "Fine-tuning, the modification of the pre-trained RoBERTa's weights along with the classification head during training with the target dataset, is a determinant procedure for the success of our approach.\n\nIn our experiments we observed that if we did not fine-tune the language model and just trained the classification head, the model would always predict the majority neutral class. This supports the notion that pre-trained-language models are useful for a wide variety of tasks but need to be fine-tuned for the specific task at hand.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Pooling",
      "text": "We have performed experiments with several pooling alternatives. From average pooling, max pool-ing, concatenation of the CLS token of more than 1 last layers to the concatenation of the CLS token with the result from average pooling. All these pooling alternatives resulted in lower performance than choosing the CLS token of the last layer. This might suggest a high representative power for the CLS token, which is proposed for classification, and discards the need for directly considering other tokens for this task.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Classification Module",
      "text": "We have also performed alternative experiments with other classification modules than our simple classification head. These consisted in passing the pooled embeddings through Recurrent Neural Networks  (Elman, 1991) , uni  (Hochreiter and Schmidhuber, 1997 ) and bi-directional  (Graves et al., 2005)  Long Short-Term Memory Networks and a Conditional Random Field  (Lafferty et al., 2001)  before feeding them to the classification head. Performance was lower in all alternative experiments when compared to our main approach of using a simple classification head. These results may indicate that our approach leveraging RoBERTa's representational power for context suffices and there is no apparent need for modelling the context with complex classification modules, after obtaining our context-dependent embedding utterance representations.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Overall Performance",
      "text": "For each of the datasets, we have performed experiments without introducing any context (c = 0) to introducing 4 previous conversation turns (c = 4), for which the overal performance operationalized by the macro-F1 metric is reported in Table  3 . Our results are an average of 5 runs. It can be observed that introducing previous conversational context turns leads to an increase in macro-F1 score. As hypothesised, providing no context is never the best option. This shows that the introduction of an adequate number of context turns directly as the language model input significantly improves model performance. In general performance increases with the introduction of each additional context turn up to the ideal number of turns and then it decreases. Overall, it can be concluded that the ideal number of introduced context turns for ERC in both datasets is 3.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Performance On Each Emotion Label",
      "text": "For each dataset, we report the results on each individual emotion label and also present the confusion matrices for the best determined c value. Our results are an average of 5 runs.\n\nThe individual emotion label F1-scores for the DailyDialog dataset are presented in Table  4 .\n\nIt can be observed that for more than half of the labels, Anger, Fear, Sadness and Neutral, the ideal context to be provided is 3 turns which maximise their F1-scores, and also the macro-F1 score on Table  3 , and for the other labels the ideal context is 4 turns for Disgust, 2 turns for Happiness and 1 turn for Surprise. As expected, providing no context is never the best option.\n\nThe confusion matrix for c = 3 corresponding to the highest macro-F1 score is displayed on Figure  3 , in which the label nomenclature and order is the same as in table 4 but with neutral as the first label.\n\nThis matrix indicates that majority of the errors are due to classifying utterances as neutral instead of assigning a non-neutral emotion. The classifier also displays some confusion in discerning between Happiness and Surprised.\n\nThe individual emotion label F1-scores for the EmoWOZ dataset are presented in Table  5 .\n\nIt can be observed that for 4 of the labels, Dissatistfied, Excited, Satisfied and Neutral, the ideal context to be provided is 4 turns which maximise their F1-scores. Regarding the other labels the ideal context is 2 for Fear, 3 for Abusive, and surprisingly 0 turns for Apologetic, which might indicate that this emotion is very explicit in this dataset.\n\nThe confusion matrix for c = 3 corresponding to the highest macro-F1 score is displayed on Figure  4 , in which the label nomenclature and order is the same as in table 5 but with neutral as the first label.\n\nThis matrix indicates that majority of the errors are due to classifying utterances as neutral instead of assigning a non-neutral emotion, as in happens with the DailyDialog dataset.   c=0 37.47 32.32 36.69 59.42 33.16 49.60 90.99 c=1 40.18 29.28 39.43 61.26 38.30 52.66 91.06 c=2 43.26 33.91 36.52 61.98 33.63 52.23 91.12 c=3 43.51 33.22 39.44 61.12 38.43 51.50 91.42 c=4 42.00 34.52 34.65 61.97 37.18 51.70 91.18    It can be observed that for 4 of the labels, Dissatistfied, Excited, Satisfied and Neutral, the ideal context to be provided is 4 turns which maximise their F1-scores. Regarding the other labels the ideal context is 2 for Fear, 3 for Abusive, and surprisingly 0 turns for Apologetic, which might indicate that this emotion is very explicit in this dataset.\n\nThe confusion matrix for c = 3 corresponding to the highest macro-F1 score is displayed on Figure  4 , in which the label nomenclature and order is the same as in table V but with neutral as the first label.\n\nThis matrix indicates that majority of the errors are due to classifying utterances as neutral instead of assigning a nonneutral emotion, as in happens with the DailyDialog dataset.  great performance increases when compared to using other means of utterance feature extraction. Regarding DailyDialog results, we compare our approach to COSMIC [8], RoBERTa and RoBERTa DialogueRNN, implemented by the authors of COSMIC, and the Psychological model [13], all models described in Section II. Concerning the performance on the EmoWOZ dataset, we compare out approach to COSMIC, BERT and BERT DialogueRNN, tested by the authors of  EmoWOZ [7] , since for this dataset the authors obtained a most suitable uterrance representation using BERT instead of RoBERTa. Results are displayed on table VI and are an average of 5 runs.  It is worth noting that our results are an average of 5 runs and the final model is determined via performance on the validation set. Therefore, the fluctuation in individual label F1-scores does not hinder the representativity of our results and these fluctuations may occur between results from the other reported state-of-the-art models.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Comparison With State-Of-The-Art",
      "text": "We further compare our approach to other state-ofthe-art approaches that also resort to the RoBERTa or BERT pre-trained-language models. This allows for a fair comparison between approaches  It can be observed that for 4 of the labels, Dissatistfied, Excited, Satisfied and Neutral, the ideal context to be provided is 4 turns which maximise their F1-scores. Regarding the other labels the ideal context is 2 for Fear, 3 for Abusive, and surprisingly 0 turns for Apologetic, which might indicate that this emotion is very explicit in this dataset.\n\nThe confusion matrix for c = 3 corresponding to the highest macro-F1 score is displayed on Figure  4 , in which the label nomenclature and order is the same as in table V but with neutral as the first label.\n\nThis matrix indicates that majority of the errors are due to classifying utterances as neutral instead of assigning a nonneutral emotion, as in happens with the DailyDialog dataset.   given that using this language model brings great performance increases when compared to using other means of utterance feature extraction. Regarding DailyDialog results, we compare our approach to COSMIC  (Ghosal et al., 2020) , RoBERTa and RoBERTa DialogueRNN, implemented by the authors of COSMIC, and the Psychological model  (Li et al., 2021) , all models described in Section 2. Concerning the performance on the EmoWOZ dataset, we compare out approach to COSMIC, BERT and BERT DialogueRNN, tested by the authors of EmoWOZ  (Feng et al., 2022) , since for this dataset the authors obtained a more suitable DailyDialog EmoWOZ macro-F1 macro-F1 RoBERTa  (Ghosal et al., 2020)  / BERT  (Feng et al., 2022)  48.20 55.80 RoBERTa  (Ghosal et al., 2020)  / BERT  (Feng et al., 2022 ) + DlgRNN 49.65 57.10 ContextBERT  (Feng et al., 2022)  -59.70 COSMIC  (Ghosal et al., 2020)  /  (Feng et al., 2022)  51.05 61.12 Psychological  (Li et al., 2021)  51.95 -CD-ERC (Ours) 51.23 65.33\n\nuterrance representation using BERT instead of RoBERTa. Results are displayed on table 6 and are an average of 5 runs.\n\nRegarding performance on the DailyDailog dataset, our approach outperforms not only the simple RoBERTa/BERT, but also RoBERTa/BERT in a more elaborate gated neural network model such as DialogueRNN and COSMIC. The Psychological model has a slightly higher performance than ours. It may be due to the fact that it leverages a large commonsense knowledge base and an elaborate classifier architecture, while we opted for a minimalistic classification module. Concerning performance on the EmoWOZ dataset, our approach outperforms all baselines by a wide margin, setting a new state of the art for task-oriented emotion datasets.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Case Studies",
      "text": "On Table  7  we can compare the performance of our contextual classifier when considering the ideal 3 context turns on both datasets versus not considering any context at all.\n\nIn the first example, from the DailyDialog dataset, A offers B assistance, so B asks A to view the apartment, to which A sadly apologizes informing B that B will not be able to view it. The classifier that does not consider context classifies this last apology as neutral. However, given the context of the conversation, A should not be neutral since A is unable to assist B which was A's initial purpose. The contextual classifier is able to consider this, thus correctly classifying A's utterance with the emotion Sadness.\n\nIn the second example, also from the DailyDialog dataset, A gives B a good idea to which B happily reacts and thanks A. A happily reacts to B's acknowledgments, especially since B mentioned A's was a \"wonderful idea\". The classifier that does not consider context classifies A's final reac-tion to B as neutral, since A's utterance is a merely \"No problem. Good luck\", not being able to recognize A's positive reaction to B's acknowledgements. The contextual classifier, however, having this utterances into account, correctly classifies A's final reaction with the emotion Happiness.\n\nIn the last example, from the EmoWOZ dataset, B is merely answering A's question of what day B would like to travel. The classifier that does not consider context takes into account the words \"please\" and \"vacation\" which bias the classification towards the emotion Excited. The contextual classifier might grasp that \"please\" is used as a polite expression and \"vacation\" is just the object of the phrase, thus correctly classifying the utterance as neutral.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Conclusions And Future Work",
      "text": "In this work we have leveraged context-dependent embedding utterrance representations for Emotion Recognition in Conversations. Our approach of producing context-dependent representations of each utterance contrasted with the usual approach of producing context independent representations of each utterance and subsequently performing contextual modeling of these. It consisted in feeding a variable number of previous conversational turns appended to the utterance to be classified as input to the state-of-the-art pre-trained-language model RoBERTa, to which we appended a simple classification module. We further investigated how the number of introduced conversational turns influenced our model performance. We concluded that the introduction of an adequate number of context turns directly as the language model input significantly improves model performance.\n\nFurthermore, we attained state-of-the-art results on the widely used DailyDialog dataset and established a new state-of-the-art by a wide margin on the EmoWOZ dataset, which are usually yielded by",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: A dialogue in which context is key to infer the",
      "page": 1
    },
    {
      "caption": "Figure 2: Model architecture. Two utterances are given",
      "page": 3
    },
    {
      "caption": "Figure 3: , in which the label nomenclature and order is the",
      "page": 5
    },
    {
      "caption": "Figure 4: , in which the label nomenclature and order is the",
      "page": 5
    },
    {
      "caption": "Figure 3: Confusion Matrix for the DailyDialog dataset",
      "page": 6
    },
    {
      "caption": "Figure 4: Confusion Matrix for the EmoWOZ dataset",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "5670\n5670\n93%\n93%": "8\n8\n0%\n0%",
          "5\n5\n29%\n29%": "6\n6\n35%\n35%",
          "105\n105\n21%\n21%": "2\n2\n0%\n0%",
          "15\n15\n20%\n20%": "0\n0\n0%\n0%",
          "1\n1\n6%\n6%": "0\n0\n0%\n0%",
          "34\n34\n38%\n38%": "0\n0\n0%\n0%",
          "180\n180\n10%\n10%": "0\n0\n0%\n0%"
        },
        {
          "5670\n5670\n93%\n93%": "203\n203\n3%\n3%",
          "5\n5\n29%\n29%": "1\n1\n6%\n6%",
          "105\n105\n21%\n21%": "377\n377\n75%\n75%",
          "15\n15\n20%\n20%": "3\n3\n4%\n4%",
          "1\n1\n6%\n6%": "5\n5\n32%\n32%",
          "34\n34\n38%\n38%": "1\n1\n1%\n1%",
          "180\n180\n10%\n10%": "12\n12\n1%\n1%"
        },
        {
          "5670\n5670\n93%\n93%": "13\n13\n0%\n0%",
          "5\n5\n29%\n29%": "0\n0\n0%\n0%",
          "105\n105\n21%\n21%": "3\n3\n1%\n1%",
          "15\n15\n20%\n20%": "53\n53\n71%\n71%",
          "1\n1\n6%\n6%": "1\n1\n6%\n6%",
          "34\n34\n38%\n38%": "0\n0\n0%\n0%",
          "180\n180\n10%\n10%": "1\n1\n0%\n0%"
        },
        {
          "5670\n5670\n93%\n93%": "1\n1\n0%\n0%",
          "5\n5\n29%\n29%": "1\n1\n6%\n6%",
          "105\n105\n21%\n21%": "6\n6\n1%\n1%",
          "15\n15\n20%\n20%": "0\n0\n0%\n0%",
          "1\n1\n6%\n6%": "7\n7\n44%\n44%",
          "34\n34\n38%\n38%": "0\n0\n0%\n0%",
          "180\n180\n10%\n10%": "0\n0\n0%\n0%"
        },
        {
          "5670\n5670\n93%\n93%": "31\n31\n1%\n1%",
          "5\n5\n29%\n29%": "1\n1\n6%\n6%",
          "105\n105\n21%\n21%": "2\n2\n0%\n0%",
          "15\n15\n20%\n20%": "0\n0\n0%\n0%",
          "1\n1\n6%\n6%": "0\n0\n0%\n0%",
          "34\n34\n38%\n38%": "42\n42\n48%\n48%",
          "180\n180\n10%\n10%": "13\n13\n1%\n1%"
        },
        {
          "5670\n5670\n93%\n93%": "147\n147\nSat\n2%\n2%",
          "5\n5\n29%\n29%": "0\n0\n0%\n0%",
          "105\n105\n21%\n21%": "8\n8\n2%\n2%",
          "15\n15\n20%\n20%": "2\n2\n3%\n3%",
          "1\n1\n6%\n6%": "0\n0\n0%\n0%",
          "34\n34\n38%\n38%": "9\n9\n10%\n10%",
          "180\n180\n10%\n10%": "1648\n1648\n89%\n89%"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "5823\n5823\n91%\n91%": "57\n57\n1%\n1%",
          "42\n42\n42%\n42%": "47\n47\n47%\n47%",
          "7\n7\n29%\n29%": "3\n3\n13%\n13%",
          "6\n6\n42%\n42%": "0\n0\n0%\n0%",
          "340\n340\n36%\n36%": "1\n1\n0%\n0%",
          "48\n48\n54%\n54%": "2\n2\n2%\n2%",
          "52\n52\n38%\n38%": "5\n5\n4%\n4%"
        },
        {
          "5823\n5823\n91%\n91%": "28\n28\n0%\n0%",
          "42\n42\n42%\n42%": "5\n5\n5%\n5%",
          "7\n7\n29%\n29%": "11\n11\n46%\n46%",
          "6\n6\n42%\n42%": "0\n0\n0%\n0%",
          "340\n340\n36%\n36%": "0\n0\n0%\n0%",
          "48\n48\n54%\n54%": "0\n0\n0%\n0%",
          "52\n52\n38%\n38%": "0\n0\n0%\n0%"
        },
        {
          "5823\n5823\n91%\n91%": "10\n10\n0%\n0%",
          "42\n42\n42%\n42%": "0\n0\n0%\n0%",
          "7\n7\n29%\n29%": "0\n0\n0%\n0%",
          "6\n6\n42%\n42%": "6\n6\n42%\n42%",
          "340\n340\n36%\n36%": "0\n0\n0%\n0%",
          "48\n48\n54%\n54%": "0\n0\n0%\n0%",
          "52\n52\n38%\n38%": "0\n0\n0%\n0%"
        },
        {
          "5823\n5823\n91%\n91%": "399\n399\n6%\n6%",
          "42\n42\n42%\n42%": "1\n1\n1%\n1%",
          "7\n7\n29%\n29%": "0\n0\n0%\n0%",
          "6\n6\n42%\n42%": "0\n0\n0%\n0%",
          "340\n340\n36%\n36%": "602\n602\n63%\n63%",
          "48\n48\n54%\n54%": "1\n1\n1%\n1%",
          "52\n52\n38%\n38%": "12\n12\n9%\n9%"
        },
        {
          "5823\n5823\n91%\n91%": "62\n62\n1%\n1%",
          "42\n42\n42%\n42%": "1\n1\n1%\n1%",
          "7\n7\n29%\n29%": "0\n0\n0%\n0%",
          "6\n6\n42%\n42%": "0\n0\n0%\n0%",
          "340\n340\n36%\n36%": "0\n0\n0%\n0%",
          "48\n48\n54%\n54%": "36\n36\n40%\n40%",
          "52\n52\n38%\n38%": "0\n0\n0%\n0%"
        },
        {
          "5823\n5823\n91%\n91%": "39\n39\n1%\n1%",
          "42\n42\n42%\n42%": "1\n1\n1%\n1%",
          "7\n7\n29%\n29%": "0\n0\n0%\n0%",
          "6\n6\n42%\n42%": "0\n0\n0%\n0%",
          "340\n340\n36%\n36%": "10\n10\n1%\n1%",
          "48\n48\n54%\n54%": "0\n0\n0%\n0%",
          "52\n52\n38%\n38%": "64\n64\n47%\n47%"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Comet: Commonsense transformers for automatic knowledge graph construction",
      "authors": [
        "Antoine Bosselut",
        "Hannah Rashkin",
        "Maarten Sap",
        "Chaitanya Malaviya",
        "Asli Celikyilmaz",
        "Yejin Choi"
      ],
      "year": "2019",
      "venue": "Comet: Commonsense transformers for automatic knowledge graph construction",
      "arxiv": "arXiv:1906.05317"
    },
    {
      "citation_id": "2",
      "title": "MultiWOZ -a largescale multi-domain Wizard-of-Oz dataset for taskoriented dialogue modelling",
      "authors": [
        "Paweł Budzianowski",
        "Tsung-Hsien Wen",
        "Bo-Hsiang Tseng",
        "Iñigo Casanueva",
        "Stefan Ultes",
        "Milica Osman Ramadan",
        "Gašić"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/D18-1547"
    },
    {
      "citation_id": "3",
      "title": "Semeval-2019 task 3: Emocontext contextual emotion detection in text",
      "authors": [
        "Ankush Chatterjee",
        "Kedhar Nath Narahari",
        "Meghana Joshi",
        "Puneet Agrawal"
      ],
      "year": "2019",
      "venue": "Proceedings of the 13th international workshop on semantic evaluation"
    },
    {
      "citation_id": "4",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2018",
      "venue": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "5",
      "title": "Basic emotions. Handbook of cognition and emotion",
      "authors": [
        "Paul Ekman"
      ],
      "year": "1999",
      "venue": "Basic emotions. Handbook of cognition and emotion"
    },
    {
      "citation_id": "6",
      "title": "Distributed representations, simple recurrent networks, and grammatical structure",
      "authors": [
        "Jeffrey L Elman"
      ],
      "year": "1991",
      "venue": "Machine learning"
    },
    {
      "citation_id": "7",
      "title": "Emowoz: A large-scale corpus and labelling scheme for emotion recognition in task-oriented dialogue systems",
      "authors": [
        "Shutong Feng",
        "Nurul Lubis",
        "Christian Geishauser",
        "Hsien-Chin Lin",
        "Michael Heck",
        "Carel Van Niekerk",
        "Milica Gasic"
      ],
      "year": "2022",
      "venue": "Proceedings of the Thirteenth Language Resources and Evaluation Conference"
    },
    {
      "citation_id": "8",
      "title": "COSMIC: COmmonSense knowledge for eMotion identification in conversations",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder"
      ],
      "year": "2020",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020",
      "doi": "10.18653/v1/2020.findings-emnlp.224"
    },
    {
      "citation_id": "9",
      "title": "Bidirectional lstm networks for improved phoneme classification and recognition",
      "authors": [
        "Alex Graves",
        "Santiago Fernández",
        "Jürgen Schmidhuber"
      ],
      "year": "2005",
      "venue": "ternational conference on artificial neural networks"
    },
    {
      "citation_id": "10",
      "title": "Long short-term memory",
      "authors": [
        "Sepp Hochreiter",
        "Jürgen Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural computation"
    },
    {
      "citation_id": "11",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "12",
      "title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
      "authors": [
        "John Lafferty",
        "Andrew Mccallum",
        "Fernando Cn Pereira"
      ],
      "year": "2001",
      "venue": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data"
    },
    {
      "citation_id": "13",
      "title": "Past, present, and future: Conversational emotion recognition through structural modeling of psychological knowledge",
      "authors": [
        "Jiangnan Li",
        "Zheng Lin",
        "Peng Fu",
        "Weiping Wang"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021",
      "doi": "10.18653/v1/2021.findings-emnlp.104"
    },
    {
      "citation_id": "14",
      "title": "Dailydialog: A manually labelled multi-turn dialogue dataset",
      "authors": [
        "Yanran Li",
        "Hui Su",
        "Xiaoyu Shen",
        "Wenjie Li",
        "Ziqiang Cao",
        "Shuzi Niu"
      ],
      "year": "2017",
      "venue": "Dailydialog: A manually labelled multi-turn dialogue dataset",
      "arxiv": "arXiv:1710.03957"
    },
    {
      "citation_id": "15",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "16",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "Navonil Majumder",
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Rada Mihalcea",
        "Alexander Gelbukh",
        "Erik Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "17",
      "title": "Ethics sheet for automatic emotion recognition and sentiment analysis",
      "authors": [
        "M Saif",
        "Mohammad"
      ],
      "year": "2022",
      "venue": "Computational Linguistics"
    },
    {
      "citation_id": "18",
      "title": "Deep emotion recognition in textual conversations: A survey",
      "authors": [
        "Patrícia Pereira",
        "Helena Moniz",
        "Joao Carvalho"
      ],
      "year": "2022",
      "venue": "Deep emotion recognition in textual conversations: A survey",
      "arxiv": "arXiv:2211.09172"
    },
    {
      "citation_id": "19",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P17-1081"
    },
    {
      "citation_id": "20",
      "title": "Emotion recognition in conversation: Research challenges, datasets, and recent advances",
      "authors": [
        "Soujanya Poria",
        "Navonil Majumder",
        "Rada Mihalcea",
        "Eduard Hovy"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "21",
      "title": "Dialogxl: All-in-one xlnet for multiparty conversation emotion recognition",
      "authors": [
        "Weizhou Shen",
        "Junqing Chen",
        "Xiaojun Quan",
        "Zhixian Xie"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "22",
      "title": "2021b. Directed acyclic graph network for conversational emotion recognition",
      "authors": [
        "Weizhou Shen",
        "Siyue Wu",
        "Yunyi Yang",
        "Xiaojun Quan"
      ],
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
      "doi": "10.18653/v1/2021.acl-long.123"
    },
    {
      "citation_id": "23",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "24",
      "title": "Transformers: State-of-the-art natural language processing",
      "authors": [
        "Thomas Wolf",
        "Lysandre Debut",
        "Victor Sanh",
        "Julien Chaumond",
        "Clement Delangue",
        "Anthony Moi",
        "Pierric Cistac",
        "Tim Rault",
        "Remi Louf",
        "Morgan Funtowicz",
        "Joe Davison",
        "Sam Shleifer",
        "Clara Patrick Von Platen",
        "Yacine Ma",
        "Julien Jernite",
        "Canwen Plu",
        "Teven Xu",
        "Sylvain Le Scao",
        "Mariama Gugger",
        "Quentin Drame",
        "Alexander Lhoest",
        "Rush"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
      "doi": "10.18653/v1/2020.emnlp-demos.6"
    },
    {
      "citation_id": "25",
      "title": "Knowledge-enriched transformer for emotion detection in textual conversations",
      "authors": [
        "Peixiang Zhong",
        "Di Wang",
        "Chunyan Miao"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
      "doi": "10.18653/v1/D19-1016"
    }
  ]
}