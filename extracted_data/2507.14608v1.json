{
  "paper_id": "2507.14608v1",
  "title": "Exp-Graph: How Connections Learn Facial Attributes In Graph-Based Expression Recognition",
  "published": "2025-07-19T13:10:21Z",
  "authors": [
    "Nandani Sharma",
    "Dinesh Singh"
  ],
  "keywords": [
    "Facial expression recognition",
    "graph convolutional networks",
    "and vision transformer"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Facial expression recognition is crucial for humancomputer interaction applications such as face animation, video surveillance, affective computing, medical analysis, etc. Since the structure of facial attributes varies with facial expressions, incorporating structural information into facial attributes is essential for facial expression recognition. In this paper, we propose Exp-Graph, a novel framework designed to represent the structural relationships among facial attributes using graphbased modeling for facial expression recognition. For facial attributes graph representation, facial landmarks are used as the graph's vertices. At the same time, the edges are determined based on the proximity of the facial landmark and the similarity of the local appearance of the facial attributes encoded using the vision transformer. Additionally, graph convolutional networks are utilized to capture and integrate these structural dependencies into the encoding of facial attributes, thereby enhancing the accuracy of expression recognition. Thus, Exp-Graph learns from the facial attribute graphs highly expressive semantic representations. On the other hand, the vision transformer and graph convolutional blocks help the framework exploit the local and global dependencies among the facial attributes that are essential for the recognition of facial expressions. We conducted comprehensive evaluations of the proposed Exp-Graph model on three benchmark datasets: Oulu-CASIA, eNTERFACE05, and AFEW. The model achieved recognition accuracies of 98.09%, 79.01%, and 56.39%, respectively. These results indicate that Exp-Graph maintains strong generalization capabilities across both controlled laboratory settings and real-world, unconstrained environments, underscoring its effectiveness for practical facial expression recognition applications.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "I. Introduction",
      "text": "F ACIAL expression recognition (FER) has garnered signif- icant attention in computer vision research over the last few decades because of its critical role in enabling computers to comprehend human emotions and engage in human-tohuman communication as shown in Fig.  1 . However, their success lies in learning robust and discriminative representations of the expressions ( such as AN: anger, DI: disgust, FE: fear, HA: happy, SA: sad, SU: surprise, NE: neutral) from the facial images that are invariant to variations in the angle of viewpoint, lighting conditions, and head postures. Thus, devising a suitable feature representation is the primary objective of facial expression recognition. In the early research, successful hand-crafted features, such as Gabor wavelets  [1] , local binary pattern (LBP)  [2] ,  [3] , histogram of oriented gradients (HoG)  [4] -  [7] , etc., were used to represent the facial expressions. However, these methods fall short of adequate recognition in more complicated real-world situations because of their low semantic correlation with facial expressions.\n\nSince deep learning techniques have rapidly advanced in the past decade, numerous attempts have been made to investigate discriminative representations for various recognition tasks. Deep models for visual representation, especially convolutional neural networks (CNNs) and vision transformers (ViTs), have shown promising results in various real-world applications because they can effectively learn discriminatory feature representations from visual observations  [8] -  [11] . Also, several studies have used CNNs to improve semantic representations of facial expressions and demonstrated good results in identifying human emotions  [12] -  [16] . However, they mostly depend on the appearance only and cannot exploit the deep structure for the problems where the training data is limited  [17] -  [19] .\n\nVision transformers  [20]   with large-scale datasets than CNNs, which concentrate on local patterns only. Additionally, ViTs are more generalizable because of their excellent transfer learning capabilities and less inductive bias. Vision transformers are increasingly favoured for visual feature extraction over traditional CNNs due to their ability to capture global context via self-attention, offering better scalability and input flexibility. Unlike CNNs, which focus on local patterns and are sensitive to variations like lighting and occlusions, ViTs can generalize better and enhance transfer learning. This research explores using ViTs for facial expression recognition to address the limitations of CNNs, aiming to improve accuracy and reliability in recognizing complex facial expressions  [21] ,  [22] . Attention methods like graph attention (GAT)  [23]  and ViT  [24]  at both geometry and appearance levels are used to improve FER performance. Vision transformers excel in capturing global features for image recognition, including FER, but they struggle with local feature extraction and require large datasets  [20] ,  [25] . Graph convolutional networks (GCNs) address these limitations by enhancing local feature detection and improving data efficiency, making ViTs more effective for FER by capturing subtle facial details  [9] ,  [26] ,  [27] . Integrating GCNs with ViTs offers a balanced solution, combining global and local feature learning for improved performance. Since facial expression highly depends on the relative change in the structure of facial attributes, incorporating graph structures of the facial attributes can significantly improve the facial expression representation, mainly when relying on transfer learning for visual encoding. However, traditional DNNs also struggle with non-Euclidean data, such as graphs, where relationships between data points are complex and irregular. Also, extensive feature engineering is required to capture complex relationships between data points. Scalability is another challenge for DNNs when dealing with large, highly connected datasets, and they need a large amount of labeled data to perform well. The lack of inductive bias in DNNs for relational data further hampers their ability to generalize across tasks where relational information is critical.\n\nIn contrast, GCNs are designed to handle graph-structured data, allowing them to represent and process such information naturally. Graph convolutional networks effectively learn and represent node connections, requiring fewer labeled samples and improving their generalization capabilities because of their built-in structure and natural inductive bias for graphs  [28] . Therefore, GCNs offer distinct advantages over DNNs in handling graph-structured data and learning complex relationships. However, facial expression recognition using GCNs facial challenges, such as constructing complex graphs, dealing with high-dimensional and irregular data, learning robust features, managing lighting, occlusions, head poses, scalability, and real-time performance. Integrating GCNs with ViTs in facial expression recognition can address challenges by combining GCNs' ability to capture structural relationships between facial landmarks with ViT's strength in modeling both local and global features. This hybrid approach requires advancement in GCNs' architecture and innovative training strategies to effectively merge the strengths of both models, leading to improved accuracy in recognizing complex facial expressions. Few works explore geometric knowledge of facial attributes for facial expression recognition  [29] -  [31] . Geometric information, such as relative location and selfdeformation, can accurately describe emotional states based on facial observations  [32] . Geometric face descriptions are more resistant to appearance changes, making them ideal for real-world facial expression recognition applications  [9] ,  [32] ,  [33] . In the GCNs methods  [9] ,  [32] ,  [32] -  [46] , landmarks, AUs (or nodes) and the connections (edges) between them are often predefined or fixed, meaning that the graph's structure remains constant during the learning process. In contrast, our model introduces a more flexible or dynamic approach. Instead of using a fixed graph structure, we are allowing the model to learn the connections between nodes while using the threshold (τ ) hyperparameter, potentially evaluating the graph structure during training. Our approach could enable the model to learn more meaningful or relevant relationships between nodes rather than relying on static or predefined connections.\n\nOur research presents an Exp-Graph framework as shown in Fig.  2  that uses GCNs  [28] ,  [47] ,  [48]  to learn geometric descriptions from facial landmarks. The system architecture seeks to increase emotional reasoning from facial images, as geometric information alone cannot distinguish geometrically identical expressions such as disgust and sadness, as shown in Fig 3 . Consequently, GCNs offer a valuable substitute for incorporating the geometric information derived from facial landmarks into emotional representations. Local appearance representations are extracted from landmark positions and aggregated with geometric representations during graph learning. The following briefly describes the primary contributions of this paper: Exp-Graph approach on publicly available datasets, including Oulu-CASIA  [49] , eNTERFACE05  [50] , and AFEW  [51]  are publicly available datasets. The rest of the paper is organized as follows: Section II reviews related work in facial expression recognition. Section III provides a comprehensive overview of the proposed Exp-Graph framework, detailing its design and methodology. Section IV describes the experimental setup, including dataset specifications and evaluation metrics, and presents a thorough analysis of the results. Finally, it concludes the paper and outlines potential directions in §V.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Ii. Related Work",
      "text": "Since structural information is crucial for facial expression recognition, as emphasized in the previous section, some works have incorporated geometric feature extraction  [26] ,  [31] ,  [52]  for facial expression recognition. Also, GCNs  [28] ,  [53] ,  [54]  and vision transformers  [20]  have already been explored in different works. Here, we present their limitations and the key differences with our proposed work.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Geometry-Based Fer",
      "text": "Due to the high association between geometric knowledge and expression representations, several researchers propose using landmark geometries for face expression manipulation  [52] ,  [55] ,  [56] . Furthermore, several studies were conducted that suggested using landmark placements as a guide to identify noteworthy local characteristics for representation learning  [57] -  [59] . Kotsia et al.  [31]  employed geometric information to identify informative frames from facial expression sequences, whereas Zhang et al.  [60]  incorporated fiducial points on face images to characterize emotions. Gaining momentum from the explosive growth of deep learning technology over the last decade, FER is paying more and more attention to the learning of geometry-associated representations of features. However, most of these techniques adopt multi-task learning approaches instead of directly learning from the geometric data. Devries et al.  [61]  introduced simultaneously learning facial landmark localization and facial expression recognition to enhance the geometric understanding of emotion-related features. Additionally, a multi-domain multi-task network with landmark detection for FER was presented by Gerard and Masip  [62] . Zhang et al.  [26]  used generative adversarial networks in conjunction with face landmarks to train the pose-invariant features for FER. Investigates using landmark locations as feature descriptions for FER in geometric facial landmarks. However, in practical applications, it is challenging to generate discriminative features due to the poor semantic correlation of these locations. A multimodal auto-encoder was presented by Zhang et al.  [63]  to learn a combined representation from both geometric and visual modalities. Only landmark information is considered in the above research on graph-based representations for extracting geometric information from face images. However, this paper aims to look at a graph-based learning strategy proper for feature representations to provide reliable geometric knowledge.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Transformer Mechanism For Fer",
      "text": "Attention mechanisms are explored in the highlight of the regions of interest (RoI) in the domain of facial expression recognition  [64] ,  [65] . Our studies investigate integrating attention mechanisms into GCNs to enable attentive graph-based representation learning. While transformers, initially proposed for natural language processing (NLP), have become a popular method for handling sequential data, their application in graphbased vision tasks has not been extensively explored. There has been significant research into using ViTs for image interpretation  [25] ,  [66] ,  [67] . Dosovitskiy et al.  [20]  introduced a vision transformer for image classification, where an image is divided into patches that serve as tokens to learn non-linear mappings based on dense correlations among all tokens. Yuan et al.  [68]  further refined the ViT  [20]  approach by developing a more generalized transformer design. Our study aims to extend the vision transformer into the domain of graph-based learning to establish longer-range dependencies among vertices in a series of facial graphs. By doing so, it seeks to enhance the effectiveness of facial expression recognition.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Gcns For Analyzing Facial Images",
      "text": "Graph convolutional networks effectively evaluate structured data across various domains, including skeleton-based action detection, NLP, and semi-supervised learning  [28] ,  [69] ,  [70] . However, while GCNs have been extensively applied to text datasets, their application to image data presents unique challenges that previous studies have not fully addressed. In the domain of facial expression recognition, Zhou et al.  [71]  introduced a FER framework, which uses end-toend feature learning based on facial topological structures to automatically learn patterns over time and space. However, the method  [71]  relies on pre-established facial landmarks identified by HOG as nodes, limiting the flexibility of the graph learning approach. To address the method  [71] , Zhou et al.  [72]  later introduced an improved method, but it still faced similar limitations. Zhao et al.  [32]  proposed a geometry-aware FER framework that combines appearance and geometric data using GCNs. This method extensively evaluates the structural information of facial attributes across various expressions and uses CNNs to extract general expression characteristics.\n\nRecently, Qu et al.  [40]  combined a spatio-temporal graph convolutional model with a self-attention mechanism, automatically adjusting attention distribution across peak frame images. Luo et al.  [27]  proposed NFER-Former, a hypergraphguided feature embedding approach designed to model significant facial actions and capture their complex interrelationships, supported by the introduction of a large NIR-VIS facial expression dataset for validation. Dong et al.  [37]  developed an attention-based visual GCNs for FER, addressing data processing inflexibility by incorporating pixel-level composition strategies. Jin et al.  [34]  presented a region-of-interest (ROI) -based method, constructing facial graphs from cropped ROIs of action units (AUs) using a deep auto-encoder. Similarly, Chen et al.  [34]  proposed a node classification approach for FER based on dual subspace manifold learning. Despite these advancements, existing methods cannot dynamically allocate edges and nodes during GCNs training.\n\nIn summary, conventional facial feature extraction methods-such as HOG, LBP, and CNNs-struggle to capture subtle variations and interdependencies in facial expressions effectively. More advanced approaches, including hypergraphbased embeddings and vision transformers, are better equipped to handle these complexities, particularly when applied to large and diverse datasets. Vision transformers, in particular, excel at modeling global context and capturing nuanced feature interactions, making them highly suitable for facial expression recognition. Recent studies have focused on combining vision transformers with graph convolutional networks to improve FER performance. These efforts integrate global visual cues with structural facial information, leverage localized convolutional branches for detailed appearance features, and apply attention mechanisms within graph-based learning frameworks to better understand facial dynamics.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iii. Exp-Graph Framework",
      "text": "The proposed Exp-Graph framework integrates face detection, feature encoding using pre-trained ViT, as shown in Fig.  4 , and recognition via GCNs, as demonstrated in Fig.  5 . The main steps of the framework are as follows: (A) encoding facial attributes through graph-based representation, (B) facial expression recognition through graph convolutional networks. Additionally, we detail the encoding of landmark geometry, the extraction and alignment of local visual features with these geometric representations, and the network architectures involved.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Encoding Facial Attributes Through Graph-Based Representation",
      "text": "The process begins with image preprocessing to normalize dimensions and improve visual quality. Facial landmarks are detected using the Dlib  [73] , and the patch around the landmark points is encoded using a pre-trained ViT  [20] . The facial attribute graph is built by generating an adjacency matrix A, which captures relationships between facial landmarks based on their spatial proximity and feature similarity. This approach effectively captures the appearance of facial attributes and relates them to various expressions. The procedure starts by applying L 2 normalization to each feature vector, followed by computing a similarity measure K(x i , x j ) for pairs of feature vectors x i and x j . Simultaneously, a distance matrix is computed based on the squared Euclidean distances between the spatial coordinates of the landmarks. The initial adjacency matrix A ij is derived by normalizing the similarity measure using an exponential function of the Euclidean distance, as illustrated in Eq. (  1 ). This method effectively integrates both feature and spatial information into the graph structure.\n\nA thresholding step (T s = µ K + τ • σ K ) is applied to refine the adjacency matrix further. Specifically, a threshold parameter τ filters out weak connections, retaining only significant relationships between landmarks and µ K and σ K are the mean and standard deviation of the A ij . As defined in Eq. (  2 ), if A ij surpasses the threshold T s , it is assigned a value of 1; otherwise, it is set to 0.\n\nThis formulation ensures that landmarks are close in space and similar features have stronger connections in the matrix. The facial landmarks are nodes, and both feature similarity and spatial proximity weight the edges (connections) between them. The final step applies a threshold to filter out weak connections, leaving only the most significant relationships between landmarks. This refined adjacency matrix can then be used for facial expression recognition for graph construction as shown in the Algorithm 1.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Algorithm 1 Graph Representation",
      "text": "Input: Image I Output: Graph G = (P , X, A) Face ← Dlib(I) {Detect face in the image} P ← ExtractLandmarks(Face) {Landmark coordi-nates} for each landmark p i in P do\n\n{Compute feature vector} Append x i to X end for Compute the adjacency matrix A using Eq. (  1 ) & (2). return G = (P , X, A)",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Facial Expression Recognition Through Graph Convolutional Networks",
      "text": "The matrix A defines the spatial relationships between facial landmarks, contributing to constructing a graph. We further learn feature embedding for the facial expression recognition using GCNs. The Algorithm 1 generates the set of facial landmarks P , the set of graph-based features X.\n\nEach layer performs two primary operations:\n\n1) Node Feature Intergration: The node features H (l) are integrated based on the graph structure encoded in Â as in Eq. (3).\n\nwhere D is the degree matrix of A + I. 2) Node Latent Feature Projection: The integrated features are then projected through a learnable weight matrix W (l) and followed by a non-linear activation function σ(•) which essential for capturing complex patterns and interactions in the data, allowing the network to learn more expressive and discriminative features in Eq. (  4 ).\n\nwhere W (l) is the weight matrix for layer l.\n\nwhere W (l) is the weight matrix for layer l. end for return H (L)   The Exp-Graph is trained using the cross-entropy loss:\n\nwhere L(Y, Ŷ ) denotes the loss function measuring the dissimilarity between the true labels Y and the predicted probabilities Ŷ . N is the total number of instances (samples) in the dataset.\n\nC is the number of classes. Y = [y j,i ] ∈ {0, 1} N ×C is the true label matrix, where y j,i = 1 if the j th instance belongs to class i, and y j,i = 0 otherwise. Ŷ = [ŷ j,i ] ∈ [0, 1] N ×C is the predicted probability matrix, where ŷj,i is the predicted probability that instance j belongs to class i. These probabilities are typically obtained via a softmax output layer in multi-class classification.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Iv. Experimental Evaluation",
      "text": "This section presents the experimental setup used to evaluate the effectiveness of the proposed approach and compares its performance against existing state-of-the-art (SOTA) methods. We compared our Exp-Graph method with recent and SOTA methods such as DTAGN(Joint)  [74] , PPDN  [75] , GCNet  [76] , FN2EN  [77] , DeRL  [78] , SASE-FE  [79] , ArcFace + lmrk  [22] , ATT  [45] , STGCN+AM+PF  [40] , AT-ViG  [37]  on the Oulu-CASIA  [49]  dataset. For eNTERFACE05  [50]  dataset, we compared with methods such as Mansoorizadehet al.  [80] , Zhalehpour et al.  [81] , Vnet  [82] . For the AFEW  [51]  dataset, we compared with methods such as HoloNet  [83] , Emotiw2018  [51] , DSAN-VGG  [84] , DGNN  [46] . To explore the generalizability of our method, we conducted extensive experiments with the standard benchmark datasets used to evaluate FER using the Exp-Graph method. The datasets and the details used in our experiments are as follows Oulu-CASIA, eNTERFACE05 and AFEW.\n\nFace detection was performed using Dlib  [73] , while the deep learning models were implemented using PyTorch 2.1.2 with CUDA 12.8 support. All experiments were conducted on an NVIDIA RTX A6000 GPU equipped with 48 GB of memory. In our implementation, the hyperparameters are configured as follows: input images are resized to 224×224 pixels. The training begins with an initial learning rate of 0.001, which is gradually reduced to a minimum of 1e-4. A weight decay of 5e-4 is applied to prevent overfitting. The Adam optimizer is employed for optimization, and various activation functions-ReLU, GeLU, and ELU-are explored. The model architecture includes a hidden layer with 256 units and a dropout rate of 0.2 to enhance generalization. To ensure the reproducibility of results, a fixed random seed of 1000 is used. Our study used datasets summarized in Table  I . The Oulu-CASIA dataset  [49] , with a resolution of 320x240 pixels and a frame rate of 25 frames per second, captures expressions under three lighting conditions: normal, weak, and dark. The eNTERFACE05 dataset  [50]  is significantly larger, containing over 1,200 video sequences from 44 subjects, and is widely used for multi-modality (visual-audio) facial expression recognition and video-based technique evaluation, with each sequence lasting about four seconds and consisting of approximately 120 frames. The AFEW dataset  [51] , used in the EmotiW challenge, is a popular video-based FER dataset in the wild, sourced from various television shows and films, presenting challenges such as varying head poses, lighting, and occlusions.   [74]  81.46 GA+GC PPDN  [75]  84.59 GA GCNet  [76]  86.11 GA FN2EN  [77]  87.71 GA DeRL  [78]  88.0 GA SASE-FE  [79]  89.6 GA+LA ArcFace + lmrk  [22]  90.28 GC+LA ATT  [45]  89.03 GC STGCN+AM+PF  [40]  90.05 GC AT-ViG  [37]  92 Table  II , III, IV compare the accuracy of various SOTA methods on Oulu-CASIA, eNTERFACE05 and AFEW datasets, respectively. In particular, our proposed Exp-Graph achieves the highest accuracy of 98.09%, 79.01%, and 56.39% on the Oulu-CASIA, eNTERFACE05, and AFEW datasets, respectively. A comparison of the Exp-Graph with the ensuing methods for facial expression recognition for accuracy, UAR (Unweighted Average Recall), WAR (Weighted Average Recall), cross-entropy loss, and F1-score are presented as metrics for the evaluation. Also, we present results using different thresholds (Th = 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50, 0.70, 0.90) and patch-size ( 10x10, 20x20, 30x30, 50x50, 70x70, 90x90). Also use the ResNet18  [85]  and EfficientNetB0  [23]  base model combined with the GAT  [86]  and GCNs for further exploration in our research with Exp-Graph.   6a, 6b,  and 6c . The study evaluates the ViT+GCNs model across three benchmark datasets: Oulu-CASIA, eNTERFACE05, and AFEW. The results show that a threshold of 0.50 consistently yields the best overall performance, particularly in the Oulu-CASIA dataset, where the model achieves an F1 score (84%), a WAR (84%), and a UAR (92%). Performance degrades significantly at higher threshold values, with τ = 0.90 resulting in the poorest results in all metrics on the Oulu-CASIA dataset. A similar trend is observed in the eNTERFACE05 and AFEW datasets, although the threshold of 0.30 yields the worst results in these cases. Notably, the threshold of 0.90 is consistently suboptimal across all datasets, indicating its tendency to introduce excessive sparsity in the relational graphs, which negatively impacts model learning and prediction stability. This configuration benefits from the synergistic use of global appearance (GA) and local appearance (LA) features, outperforming models that rely solely on geometric or individual feature types. The enhanced results can be attributed to the proposed model's ability to construct and utilize expressive graph representations that capture nuanced structural relationships between facial landmarks. This inverse relationship between threshold value and performance can be attributed to the increased sparsity and potential noise in the constructed graph at higher thresholds, which results in less informative node connections. Lower thresholds preserve more connections, which are beneficial for learning discriminative patterns, particularly in imbalanced datasets. The accompanying figures-6a, 6b, and 6c-visually reinforce these findings, showing a clear peak in accuracy and F1 Score around τ = 0.50, followed by a noticeable decline as the threshold increases. While WAR and UAR display slightly more stable trends, they also show reduced performance at higher thresholds. These consistent patterns across all datasets highlight the ViT+GCNs model's robustness and adaptability, particularly its capacity to handle class imbalance effectively when configured with an appropriately tuned threshold. Fig.  7  and Table VI present the results comparing the performance of various model architectures on the Oulu-CASIA dataset. Among the evaluated models, the proposed Exp-Graph (ViT+GCNs) configuration consistently achieves the highest performance across the evaluation metrics. These results indicate the model's superior capacity for accurate classification and its robustness in addressing class imbalance.\n\nIn contrast, models such as EfficientNet+GCNs, ResNet18+GAT, and EfficientNet+GAT demonstrate moderate performance, achieving results that are relatively close to each other but noticeably lower than those of ViT+GCNs. The ViT+GAT configuration yields the weakest performance across all metrics, suggesting that the combination of graph attention mechanisms with vision transformers may not be well-suited to this dataset or task without further architectural refinements. The consistently strong performance of ViT+GCNs highlights the effectiveness of integrating graph convolutional networks with vision transformers, leveraging both spatial relational structure and high-capacity feature extraction. This comprehensive approach to feature extraction and graph-based modeling significantly improves recognition performance, demonstrating the model's robustness and generalizability on complex facial expressions recognition tasks.   Among these, thresholds of 0.20 and 0.30 consistently produced the most distinct and well-separated clusters, indicating effective discrimination between different facial expression classes. Interestingly, the threshold of 0.30 offered good separation, it also resulted in overly dispersed clusters, potentially obscuring the underlying relational structure among expressions. The most balanced and interpretable visualizations were obtained at a threshold of 0.50, where the t-SNE plots showed both clear inter-class separation and coherent intra-class clustering. This suggests that the threshold of 0.20 in the Oulu-CASIA effectively preserves both local and global topological structures within the learned graph representations. The clarity of the clustering at this threshold, particularly when using features extracted from the final layer of the GCNs, underscores its ability to capture complex patterns in facial expression data, thus supporting more accurate and meaningful interpretation of the learned embeddings.\n\nFig.  9  illustrates the Visualization of the learned graph for the Oulu-CASIA dataset sample images at a threshold value of τ = 0.30. In this visualization, the first column displays samples of different facial expressions (e.g., anger, disgust, fear), while the second column overlays the corresponding connection graph on the original image, highlighting the relational structure among key facial landmarks. The performance metrics of the proposed model across varying threshold values        (98.09%), F1-score (98.09%), WAR (98.09%), and UAR (99.06%). A noticeable performance improvement begins at the 30×30 patch size, continuing to rise steadily with larger patches, and peaking at 70×70. In contrast, smaller patches such as 10×10 and 20×20 result in significantly lower performance across all metrics, with 20×20 yielding the weakest results, suggesting that smaller patches may fail to capture sufficient contextual and spatial information.\n\nFor the eNTERFACE05 dataset, results reported in Table XI and visualized in Fig.  10b , the optimal performance is observed at a patch size of 30×30, where the model achieves its highest values for accuracy (79.06%), F1-score (60.93%), WAR (79.06%), and UAR (91.59%). Performance declines for both smaller and larger patch sizes. While 20×20 still performs reasonably well, 10×10 and 50×50 show a marked drop in effectiveness, and the 70×70 patch-which was optimal for the Oulu dataset-fails to deliver comparable results on eNTERFACE05. These differences highlight the datasetspecific sensitivity to patch size and the importance of tailoring patch-based feature extraction to the characteristics of each dataset. Additionally, 10b presents t-SNE visualizations of the test samples using the optimal patch sizes for each dataset. These plots demonstrate clear and distinct class separations, further validating the effectiveness of the selected patch sizes in preserving discriminative features and supporting accurate classification.\n\nOverall, these results highlight that the optimal patch size varies by dataset, with larger patches proving more effective on Oulu, while a mid-sized patch (30×30) yields the best performance on eNTERFACE05. We also demonstrate how varying the threshold impacts the model's training and validation performance, recall metrics, and feature detection capabilities. Lower thresholds generally lead to better accuracy, reduced loss, and improved recall, while enabling more comprehensive facial feature analysis.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "V. Conclusion",
      "text": "In conclusion, this work introduced Exp-Graph, a novel framework for facial expression recognition that utilizes a graph-based representation of facial attribute structures. By modeling facial landmarks as graph nodes and defining edges through spatial and appearance-based relationships, Exp-Graph captures intricate dependencies among facial features. Selecting an appropriate patch size and threshold (τ ) is crucial for the optimal performance of the Exp-Graph. An appropriate patch size with a suitable threshold can reduce information loss and result in a more relevant graph representation. In contrast, too large or too small patch sizes and thresholds may result in either similar graph structures or disconnected graphs due to the loss of important information. Therefore, determining the optimal threshold and patch size is essential for effectively preserving facial features and ensuring robust performance. However, the optimal size may vary depending on the characteristics of the dataset. The integration of vision transformers and graph convolutional networks enables the framework to effectively encode global context and structural information. The experimental results confirm the robustness and strong generalization of the method across the datasets.",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: However, their",
      "page": 1
    },
    {
      "caption": "Figure 1: System architecture of facial expression recognition using Exp-Graph",
      "page": 1
    },
    {
      "caption": "Figure 2: General pipeline of our system.",
      "page": 2
    },
    {
      "caption": "Figure 3: Geometry alone is insufficient. [Best shown in color]",
      "page": 2
    },
    {
      "caption": "Figure 2: that uses GCNs [28], [47], [48] to learn geometric",
      "page": 3
    },
    {
      "caption": "Figure 3: Consequently, GCNs offer a valuable substitute for",
      "page": 3
    },
    {
      "caption": "Figure 4: , and recognition via GCNs, as demonstrated in Fig. 5.",
      "page": 4
    },
    {
      "caption": "Figure 4: An illustration of feature extraction of facial units using vision",
      "page": 4
    },
    {
      "caption": "Figure 5: An outline of the proposed framework for detecting and recognizing facial expressions. The exp-Graph framework is composed of two primary steps:",
      "page": 5
    },
    {
      "caption": "Figure 6: Test results across thresholds (Th = τ) for three datasets. [Best shown",
      "page": 7
    },
    {
      "caption": "Figure 7: and Table VI present the results comparing the",
      "page": 8
    },
    {
      "caption": "Figure 7: Test result on the Oulu-CASIA dataset across models. [Best shown",
      "page": 8
    },
    {
      "caption": "Figure 9: illustrates the Visualization of the learned graph for",
      "page": 8
    },
    {
      "caption": "Figure 8: t-SNE test results on two FER datasets. [Best shown in color]",
      "page": 9
    },
    {
      "caption": "Figure 9: Visualization of learned graphs for the Oulu-CASIA dataset sample",
      "page": 9
    },
    {
      "caption": "Figure 10: Test results across patch sizes for two FER datasets at a threshold",
      "page": 10
    },
    {
      "caption": "Figure 10: a, the performance on",
      "page": 10
    },
    {
      "caption": "Figure 11: t-SNE test results on the best patch size for two datasets. [Best",
      "page": 10
    },
    {
      "caption": "Figure 10: b, the optimal performance is ob-",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Node Indexing &  Edge Indexing \nFace Detection & Keypoints \nFeature Extraction\nPatches Extraction\nLocalization": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "DTAGN(Joint)\n[74]\nPPDN [75]\nGCNet\n[76]\nFN2EN [77]\nDeRL [78]\nSASE-FE [79]\nArcFace + lmrk [22]\nATT [45]\nSTGCN+AM+PF [40]\nAT-ViG [37]",
          "Accuracy (%)": "81.46\n84.59\n86.11\n87.71\n88.0\n89.6\n90.28\n89.03\n90.05\n92.03",
          "Info.": "GA+GC\nGA\nGA\nGA\nGA\nGA+LA\nGC+LA\nGC\nGC\nGC"
        },
        {
          "Method": "Exp-Graph (GeLU)\nExp-Graph (ELU)\nExp-Graph (ReLU)",
          "Accuracy (%)": "98.09\n98.09\n98.09",
          "Info.": "GC+LA\nGC+LA\nGC+LA"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "Mansoorizadehet al.\n[80]\nZhalehpour et al.\n[81]\nVnet\n[82]",
          "Accuracy (%)": "37.00\n42.12\n54.35",
          "Info.": "GC\nGA\nGA"
        },
        {
          "Method": "Exp-Graph (ReLU)\nExp-Graph (GeLU)\nExp-Graph (ELU)",
          "Accuracy (%)": "79.01\n79.01\n79.01",
          "Info.": "GC+LA\nGC+LA\nGC+LA"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "HoloNet\n[83]\nEmotiw2018 [51]\nDSAN-VGG [84]\nDGNN [46]",
          "Accuracy (%)": "38.81\n38.81\n52.74\n32.64",
          "Info.": "GA\nGA\nGA\nGA+LA"
        },
        {
          "Method": "Exp-Graph (ReLU)\nExp-Graph (GeLU)\nExp-Graph (ELU)",
          "Accuracy (%)": "56.39\n56.39\n56.39",
          "Info.": "GC+LA\nGC+LA\nGC+LA"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Metrics": "Loss\nF1 (%)\nWAR (%)\nUAR (%)",
          "Th0.30": "1.34\n68.11\n69.88\n85.51",
          "Th0.50": "1.21\n84.00\n84.00\n92.00",
          "Th0.70": "1.65\n11.58\n36.91\n86.95",
          "Th0.90": "1.74\n07.34\n27.60\n87.33",
          "Dataset": "Oulu\nOulu\nOulu\nOulu"
        },
        {
          "Metrics": "Loss\nF1 (%)\nWAR (%)\nUAR (%)",
          "Th0.30": "1.34\n68.11\n69.88\n85.51",
          "Th0.50": "1.21\n84.00\n84.00\n92.00",
          "Th0.70": "1.65\n11.58\n36.91\n86.95",
          "Th0.90": "1.74\n07.34\n27.60\n87.33",
          "Dataset": "eNTER\neNTER\neNTER\neNTER"
        },
        {
          "Metrics": "Loss\nF1 (%)\nWAR (%)\nUAR (%)",
          "Th0.30": "1.92\n59.27\n62.67\n82.50",
          "Th0.50": "1.50\n26.95\n53.54\n86.00",
          "Th0.70": "1.74\n9.03\n25.71\n82.73",
          "Th0.90": "1.77\n11.06\n21.08\n74.74",
          "Dataset": "AFEW\nAFEW\nAFEW\nAFEW"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "ResNet18+GCNs\nEfficientNet+GCNs\nViT+GCNs\nResNet18+GAT\nEfficientNet+GAT\nViT+GAT",
          "Acc.": "59.93\n67.03\n91.09\n67.00\n50.30\n66.18",
          "F1-Score": "34.00\n36.79\n91.00\n26.77\n19.29\n25.51",
          "WAR": "60\n67.03\n91.09\n67.89\n50.30\n66.18",
          "UAR": "87.06\n89.25\n95.55\n92.77\n86.28\n91.17"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Th = τ": "0.20\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\n0.70\n0.90",
          "Acc": "85.38\n85.54\n69.88\n68.64\n72.24\n70.7\n91.09\n36.91\n27.6",
          "F1-Score": "85.09\n85.06\n68.11\n60.4\n55.28\n41.82\n91.1\n11.58\n7.34",
          "WAR": "85.4\n85.54\n68.88\n68.64\n72.25\n70.7\n91.1\n36.9\n27.6",
          "UAR": "92.69\n93.02\n85.51\n85.96\n89.44\n90.51\n95.55\n86.95\n87.33"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Th = τ": "0.20\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\n0.70\n0.90",
          "Acc": "57.65\n59.77\n62.67\n60.37\n60.6\n58.98\n53.54\n25.71\n21.08",
          "F1-Score": "57.2\n58.5\n59.27\n53.47\n47.07\n38.05\n26.95\n9.03\n11.06",
          "WAR": "57.62\n59.77\n62.7\n60.37\n60.6\n58.98\n53.5\n25.7\n21.08",
          "UAR": "79.01\n80.4\n82.5\n82.41\n84.2\n85.62\n86\n82.73\n74.74"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "HxW": "10x10\n20x20\n30x30\n50x50\n70x70",
          "Acc": "69.88\n63.30\n75.52\n95.58\n98.09",
          "F1-Score": "68.11\n51.81\n70.27\n95.07\n98.09",
          "WAR": "68.88\n63.30\n75.52\n95.58\n98.09",
          "UAR": "85.51\n86.33\n89.13\n97.99\n99.06"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Th = τ": "0.20\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\n0.70\n0.90",
          "Acc": "27.04\n29.81\n62.67\n30.37\n32.69\n33.05\n56.39\n25.71\n21.08",
          "F1-Score": "26.9\n29.56\n59.27\n29.92\n31.38\n30.72\n56.39\n9.03\n11.06",
          "WAR": "27.04\n29.81\n62.67\n30.37\n32.69\n33.05\n56.39\n25.71\n21.08",
          "UAR": "63.64\n65.08\n82.5\n65.65\n67.6\n69.05\n86\n82.73\n74.74"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "HxW": "10x10\n20x20\n30x30\n50x50\n70x70",
          "Acc": "62.67\n76.12\n79.06\n68.63\n69.60",
          "F1-Score": "59.27\n62.25\n60.93\n58.50\n68.59",
          "WAR": "62.7\n76.12\n79.06\n68.63\n69.60",
          "UAR": "82.5\n90.19\n91.59\n86.50\n87.50"
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Gabor-based kernel pca with doubly nonlinear mapping for face recognition with a single face image",
      "authors": [
        "X Xie",
        "K.-M Lam"
      ],
      "year": "2006",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "2",
      "title": "Dynamic texture recognition using local binary patterns with an application to facial expressions",
      "authors": [
        "G Zhao",
        "M Pietikainen"
      ],
      "year": "2007",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "3",
      "title": "Facial expression recognition based on local binary patterns: A comprehensive study",
      "authors": [
        "C Shan",
        "S Gong",
        "P Mcowan"
      ],
      "year": "2009",
      "venue": "Image and vision Computing"
    },
    {
      "citation_id": "4",
      "title": "Emotion recognition using phog and lpq features",
      "authors": [
        "A Dhall",
        "A Asthana",
        "R Goecke",
        "T Gedeon"
      ],
      "year": "2011",
      "venue": "2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG)"
    },
    {
      "citation_id": "5",
      "title": "Facial expression recognition and analysis: a comparison study of feature descriptors",
      "authors": [
        "C Liew",
        "T Yairi"
      ],
      "year": "2015",
      "venue": "IPSJ transactions on computer vision and applications"
    },
    {
      "citation_id": "6",
      "title": "Facial expression recognition in video with multiple feature fusion",
      "authors": [
        "J Chen",
        "Z Chen",
        "Z Chi",
        "H Fu"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "7",
      "title": "Facial expression recognition using iterative fusion of mo-hog and deep features",
      "authors": [
        "H Wang",
        "S Wei",
        "B Fang"
      ],
      "year": "2020",
      "venue": "The Journal of Supercomputing"
    },
    {
      "citation_id": "8",
      "title": "Context-aware emotion recognition networks",
      "authors": [
        "J Lee",
        "S Kim",
        "S Kim",
        "J Park",
        "K Sohn"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "9",
      "title": "Spatialtemporal graphs plus transformers for geometry-guided facial expression recognition",
      "authors": [
        "R Zhao",
        "T Liu",
        "Z Huang",
        "D Lun",
        "K.-M Lam"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "10",
      "title": "Emoticon: Context-aware multimodal emotion recognition using frege's principle",
      "authors": [
        "T Mittal",
        "P Guhan",
        "U Bhattacharya",
        "R Chandra",
        "A Bera",
        "D Manocha"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "11",
      "title": "Deep-emotion: Facial expression recognition using attentional convolutional network",
      "authors": [
        "S Minaee",
        "M Minaei",
        "A Abdolrashidi"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "12",
      "title": "A discriminative feature learning approach for deep face recognition",
      "authors": [
        "Y Wen",
        "K Zhang",
        "Z Li",
        "Y Qiao"
      ],
      "year": "2016",
      "venue": "Computer vision-ECCV 2016: 14th European conference"
    },
    {
      "citation_id": "13",
      "title": "Reliable crowdsourcing and deep localitypreserving learning for expression recognition in the wild",
      "authors": [
        "S Li",
        "W Deng",
        "J Du"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "14",
      "title": "Island loss for learning discriminative features in facial expression recognition",
      "authors": [
        "J Cai",
        "Z Meng",
        "A Khan",
        "Z Li",
        "J O'reilly",
        "Y Tong"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "15",
      "title": "Facial expression recognition in the wild using multi-level features and attention mechanisms",
      "authors": [
        "Y Li",
        "G Lu",
        "J Li",
        "Z Zhang",
        "D Zhang"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "16",
      "title": "Deep multitask learning for facial expression recognition and synthesis based on selective feature sharing",
      "authors": [
        "R Zhao",
        "T Liu",
        "J Xiao",
        "D Lun",
        "K.-M Lam"
      ],
      "year": "2021",
      "venue": "Proceedings of the International Conference on Pattern Recognition (ICPR)"
    },
    {
      "citation_id": "17",
      "title": "Graph formulation of video activities for abnormal activity recognition",
      "authors": [
        "D Singh",
        "C Mohan"
      ],
      "year": "2017",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "18",
      "title": "Graphlime: Local interpretable model explanations for graph neural networks",
      "authors": [
        "Q Huang",
        "M Yamada",
        "Y Tian",
        "D Singh",
        "Y Chang"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Knowledge and Data Engineering"
    },
    {
      "citation_id": "19",
      "title": "Graph representation for weakly-supervised spatio-temporal action detection",
      "authors": [
        "D Singh"
      ],
      "year": "2023",
      "venue": "Proceedings of the International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "20",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly"
      ],
      "year": "2020",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "arxiv": "arXiv:2010.11929"
    },
    {
      "citation_id": "21",
      "title": "Facial expression recognition from occluded images using deep convolution neural network with vision transformer",
      "authors": [
        "M Li",
        "S Tu",
        "S Rehman"
      ],
      "year": "2023",
      "venue": "International Conference on Image and Graphics"
    },
    {
      "citation_id": "22",
      "title": "Facial expression recognition on the high aggregation subgraphs",
      "authors": [
        "T Liu",
        "J Li",
        "J Wu",
        "B Du",
        "J Chang",
        "Y Liu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "23",
      "title": "Graph attention networks",
      "authors": [
        "P Veličković",
        "G Cucurull",
        "A Casanova",
        "A Romero",
        "P Lio",
        "Y Bengio"
      ],
      "year": "2017",
      "venue": "Graph attention networks",
      "arxiv": "arXiv:1710.10903"
    },
    {
      "citation_id": "24",
      "title": "Visual transformers: Tokenbased image representation and processing for computer vision",
      "authors": [
        "B Wu",
        "C Xu",
        "X Dai",
        "A Wan",
        "P Zhang",
        "Z Yan",
        "M Tomizuka",
        "J Gonzalez",
        "K Keutzer",
        "P Vajda"
      ],
      "year": "2020",
      "venue": "Visual transformers: Tokenbased image representation and processing for computer vision"
    },
    {
      "citation_id": "25",
      "title": "Facial expression recognition with visual transformers and attentional selective fusion",
      "authors": [
        "F Ma",
        "B Sun",
        "S Li"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "26",
      "title": "Geometry guided poseinvariant facial expression recognition",
      "authors": [
        "F Zhang",
        "T Zhang",
        "Q Mao",
        "C Xu"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "27",
      "title": "Hypergraphguided disentangled spectrum transformer networks for near-infrared facial expression recognition",
      "authors": [
        "B Luo",
        "H Wang",
        "J Wang",
        "J Zhu",
        "X Zhao",
        "Y Gao"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "28",
      "title": "Semi-supervised classification with graph convolutional networks",
      "authors": [
        "T Kipf",
        "M Welling"
      ],
      "year": "2016",
      "venue": "Semi-supervised classification with graph convolutional networks",
      "arxiv": "arXiv:1609.02907"
    },
    {
      "citation_id": "29",
      "title": "Recognizing action units for facial expression analysis",
      "authors": [
        "Y.-I Tian",
        "T Kanade",
        "J Cohn"
      ],
      "year": "2001",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "30",
      "title": "Automatic facial expression recognition using features of salient facial patches",
      "authors": [
        "S Happy",
        "A Routray"
      ],
      "year": "2014",
      "venue": "IEEE transactions on Affective Computing"
    },
    {
      "citation_id": "31",
      "title": "Facial expression recognition in image sequences using geometric deformation features and support vector machines",
      "authors": [
        "I Kotsia",
        "I Pitas"
      ],
      "year": "2006",
      "venue": "IEEE transactions on image processing"
    },
    {
      "citation_id": "32",
      "title": "Geometryaware facial expression recognition via attentive graph convolutional networks",
      "authors": [
        "R Zhao",
        "T Liu",
        "Z Huang",
        "D Lun",
        "K.-M Lam"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "33",
      "title": "Facial landmark detection by deep multi-task learning",
      "authors": [
        "Z Zhang",
        "P Luo",
        "C Loy",
        "X Tang"
      ],
      "year": "2014",
      "venue": "Computer Vision-ECCV 2014: 13th European Conference"
    },
    {
      "citation_id": "34",
      "title": "Transformer embedded spectralbased graph network for facial expression recognition",
      "authors": [
        "X Jin",
        "X Song",
        "X Wu",
        "W Yan"
      ],
      "year": "2024",
      "venue": "International Journal of Machine Learning and Cybernetics"
    },
    {
      "citation_id": "35",
      "title": "A joint hierarchical cross-attention graph convolutional network for multi-modal facial expression recognition",
      "authors": [
        "C Xu",
        "Y Du",
        "J Wang",
        "W Zheng",
        "T Li",
        "Z Yuan"
      ],
      "year": "2024",
      "venue": "Computational Intelligence"
    },
    {
      "citation_id": "36",
      "title": "A descriptive human visual cognitive strategy using graph neural network for facial expression recognition",
      "authors": [
        "S Liu",
        "S Huang",
        "W Fu",
        "J Lin"
      ],
      "year": "2024",
      "venue": "International Journal of Machine Learning and Cybernetics"
    },
    {
      "citation_id": "37",
      "title": "Attentional visual graph neural network based facial expression recognition method",
      "authors": [
        "W Dong",
        "X Zheng",
        "L Zhang",
        "Y Zhang"
      ],
      "year": "2024",
      "venue": "Signal, Image and Video Processing"
    },
    {
      "citation_id": "38",
      "title": "Ter-ca-wgnn: trimodel emotion recognition using cumulative attribute-weighted graph neural network",
      "authors": [
        "H Al-Saadawi",
        "R Das"
      ],
      "year": "2024",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "39",
      "title": "Facial action units as a joint dataset training bridge for facial expression recognition",
      "authors": [
        "S Mao",
        "X Li",
        "F Zhang",
        "X Peng",
        "Y Yang"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "40",
      "title": "Design and research of facial expression recognition system based on key point extraction",
      "authors": [
        "Y Qu",
        "Y Liu"
      ],
      "year": "2025",
      "venue": "KSII Transactions on Internet & Information Systems"
    },
    {
      "citation_id": "41",
      "title": "Modeling fine-grained relations in dynamic space-time graphs for video-based facial expression recognition",
      "authors": [
        "C Huang",
        "F Jiang",
        "Z Han",
        "X Huang",
        "S Wang",
        "Y Zhu",
        "Y Jiang",
        "B Hu"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "42",
      "title": "Facial expression recognition using graph-based features and artificial neural networks",
      "authors": [
        "C Tanchotsrinon",
        "S Phimoltares",
        "S Maneeroj"
      ],
      "year": "2011",
      "venue": "2011 IEEE International Conference on Imaging Systems and Techniques"
    },
    {
      "citation_id": "43",
      "title": "Gcf: Graph convolutional networks for facial expression recognition",
      "authors": [
        "H Kassab",
        "M Bahaa",
        "A Hamdi"
      ],
      "year": "2024",
      "venue": "2024 Intelligent Methods, Systems, and Applications (IMSA)"
    },
    {
      "citation_id": "44",
      "title": "Facial expression recognition based on graph neural network",
      "authors": [
        "X Xu",
        "Z Ruan",
        "L Yang"
      ],
      "year": "2020",
      "venue": "2020 IEEE 5th International Conference on Image, Vision and Computing (ICIVC)"
    },
    {
      "citation_id": "45",
      "title": "Dual subspace manifold learning based on gcn for intensity-invariant facial expression recognition",
      "authors": [
        "J Chen",
        "J Shi",
        "R Xu"
      ],
      "year": "2024",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "46",
      "title": "Facial landmark-based emotion recognition via directed graph neural network",
      "authors": [
        "Q Ngoc",
        "S Lee",
        "B Song"
      ],
      "year": "2020",
      "venue": "Electronics"
    },
    {
      "citation_id": "47",
      "title": "Modeling relational data with graph convolutional networks",
      "authors": [
        "M Schlichtkrull",
        "T Kipf",
        "P Bloem",
        "R Van Den",
        "I Berg",
        "M Titov",
        "Welling"
      ],
      "year": "2018",
      "venue": "The semantic web: 15th international conference"
    },
    {
      "citation_id": "48",
      "title": "Graph convolutional networks for text classification",
      "authors": [
        "L Yao",
        "C Mao",
        "Y Luo"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "49",
      "title": "Facial expression recognition from near-infrared videos",
      "authors": [
        "G Zhao",
        "X Huang",
        "M Taini",
        "S Li",
        "M Pietikäinen"
      ],
      "year": "2011",
      "venue": "Image and vision computing"
    },
    {
      "citation_id": "50",
      "title": "The enterface'05 audiovisual emotion database",
      "authors": [
        "O Martin",
        "I Kotsia",
        "B Macq",
        "I Pitas"
      ],
      "year": "2006",
      "venue": "Proceedings of the 22nd IEEE International Conference on Data Engineering Workshops (ICDEW)"
    },
    {
      "citation_id": "51",
      "title": "Emotiw 2018: Audiovideo, student engagement and group-level affect prediction",
      "authors": [
        "A Dhall",
        "A Kaur",
        "R Goecke",
        "T Gedeon"
      ],
      "year": "2018",
      "venue": "Proceedings of the 20th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "52",
      "title": "Geometry guided adversarial facial expression synthesis",
      "authors": [
        "L Song",
        "Z Lu",
        "R He",
        "Z Sun",
        "T Tan"
      ],
      "year": "2018",
      "venue": "Proceedings of the 26th ACM international conference on Multimedia"
    },
    {
      "citation_id": "53",
      "title": "Mer-gcn: Microexpression recognition based on relation modeling with graph convolutional networks",
      "authors": [
        "L Lo",
        "H.-X Xie",
        "H.-H Shuai",
        "W.-H Cheng"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE International Conference on Multimedia Information Processing and Retrieval (MIPR)"
    },
    {
      "citation_id": "54",
      "title": "Facial expression recognition via deep action units graph network based on psychological mechanism",
      "authors": [
        "Y Liu",
        "X Zhang",
        "Y Lin",
        "H Wang"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "55",
      "title": "Geometrycontrastive gan for facial expression transfer",
      "authors": [
        "F Qiao",
        "N Yao",
        "Z Jiao",
        "Z Li",
        "H Chen",
        "H Wang"
      ],
      "year": "2018",
      "venue": "Geometrycontrastive gan for facial expression transfer"
    },
    {
      "citation_id": "56",
      "title": "3d dense geometry-guided facial expression synthesis by adversarial learning",
      "authors": [
        "R Bodur",
        "B Bhattarai",
        "T.-K Kim"
      ],
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Winter, 2021"
    },
    {
      "citation_id": "57",
      "title": "Emotion recognition with facial landmark heatmaps",
      "authors": [
        "S Mo",
        "W Yang",
        "G Wang",
        "Q Liao"
      ],
      "year": "2020",
      "venue": "MultiMedia Modeling: 26th International Conference, MMM 2020"
    },
    {
      "citation_id": "58",
      "title": "Facial expression recognition using enhanced deep 3d convolutional neural networks",
      "authors": [
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE CVPRW"
    },
    {
      "citation_id": "59",
      "title": "Region attention networks for pose and occlusion robust facial expression recognition",
      "authors": [
        "K Wang",
        "X Peng",
        "J Yang",
        "D Meng",
        "Y Qiao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "60",
      "title": "Comparison between geometry-based and gabor-wavelets-based facial expression recognition using multi-layer perceptron",
      "authors": [
        "Z Zhang",
        "M Lyons",
        "M Schuster",
        "S Akamatsu"
      ],
      "year": "1998",
      "venue": "Proceedings Third IEEE International Conference on Automatic face and gesture recognition"
    },
    {
      "citation_id": "61",
      "title": "Multi-task learning of facial landmarks and expression",
      "authors": [
        "T Devries",
        "K Biswaranjan",
        "G Taylor"
      ],
      "year": "2014",
      "venue": "2014 Canadian conference on computer and robot vision"
    },
    {
      "citation_id": "62",
      "title": "Multi-task, multi-label and multi-domain learning with residual convolutional networks for emotion recognition",
      "authors": [
        "G Pons",
        "D Masip"
      ],
      "year": "2018",
      "venue": "Multi-task, multi-label and multi-domain learning with residual convolutional networks for emotion recognition",
      "arxiv": "arXiv:1802.06664"
    },
    {
      "citation_id": "63",
      "title": "Multimodal learning for facial expression recognition",
      "authors": [
        "W Zhang",
        "Y Zhang",
        "L Ma",
        "J Guan",
        "S Gong"
      ],
      "year": "2015",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "64",
      "title": "A visual attention based roi detection method for facial expression recognition",
      "authors": [
        "W Sun",
        "H Zhao",
        "Z Jin"
      ],
      "year": "2018",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "65",
      "title": "Patch-gated cnn for occlusionaware facial expression recognition",
      "authors": [
        "Y Li",
        "J Zeng",
        "S Shan",
        "X Chen"
      ],
      "year": "2018",
      "venue": "2018 24th Proceedings of the International Conference on Pattern Recognition (ICPR)"
    },
    {
      "citation_id": "66",
      "title": "Poster: A pyramid cross-fusion transformer network for facial expression recognition",
      "authors": [
        "C Zheng",
        "M Mendieta",
        "C Chen"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "67",
      "title": "Transformer-based multimodal emotional perception for dynamic facial expression recognition in the wild",
      "authors": [
        "X Zhang",
        "M Li",
        "S Lin",
        "H Xu",
        "G Xiao"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "68",
      "title": "Tokens-to-token vit: Training vision transformers from scratch on imagenet",
      "authors": [
        "L Yuan",
        "Y Chen",
        "T Wang",
        "W Yu",
        "Y Shi",
        "Z.-H Jiang",
        "F Tay",
        "J Feng",
        "S Yan"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "69",
      "title": "Deeper insights into graph convolutional networks for semi-supervised learning",
      "authors": [
        "Q Li",
        "Z Han",
        "X.-M Wu"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "70",
      "title": "Dual graph convolutional networks for graphbased semi-supervised classification",
      "authors": [
        "C Zhuang",
        "Q Ma"
      ],
      "year": "2018",
      "venue": "Proceedings of the ACM Web Conference"
    },
    {
      "citation_id": "71",
      "title": "Facial expression recognition using spatial-temporal semantic graph network",
      "authors": [
        "J Zhou",
        "X Zhang",
        "Y Liu",
        "X Lan"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE International Conference on Image Processing"
    },
    {
      "citation_id": "72",
      "title": "Learning the connectivity: Situational graph convolution network for facial expression recognition",
      "authors": [
        "J Zhou",
        "X Zhang",
        "Y Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE International Conference on Visual Communications and Image Processing"
    },
    {
      "citation_id": "73",
      "title": "dlib c++ library",
      "authors": [
        "D King"
      ],
      "venue": "dlib c++ library"
    },
    {
      "citation_id": "74",
      "title": "Joint fine-tuning in deep neural networks for facial expression recognition",
      "authors": [
        "H Jung",
        "S Lee",
        "J Yim",
        "S Park",
        "J Kim"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "75",
      "title": "Peak-piloted deep network for facial expression recognition",
      "authors": [
        "X Zhao",
        "X Liang",
        "L Liu",
        "T Li",
        "Y Han",
        "N Vasconcelos",
        "S Yan"
      ],
      "year": "2016",
      "venue": "Computer Vision-ECCV 2016: European Conference"
    },
    {
      "citation_id": "76",
      "title": "Deep generativecontrastive networks for facial expression recognition",
      "authors": [
        "Y Kim",
        "B Yoo",
        "Y Kwak",
        "C Choi",
        "J Kim"
      ],
      "year": "2017",
      "venue": "Deep generativecontrastive networks for facial expression recognition",
      "arxiv": "arXiv:1703.07140"
    },
    {
      "citation_id": "77",
      "title": "Facenet2expnet: Regularizing a deep face recognition net for expression recognition",
      "authors": [
        "H Ding",
        "S Zhou",
        "R Chellappa"
      ],
      "year": "2017",
      "venue": "2017 12th IEEE international conference on automatic face & gesture recognition"
    },
    {
      "citation_id": "78",
      "title": "Facial expression recognition by deexpression residue learning",
      "authors": [
        "H Yang",
        "U Ciftci",
        "L Yin"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on CVPR"
    },
    {
      "citation_id": "79",
      "title": "Automatic recognition of facial displays of unfelt emotions",
      "authors": [
        "K Kulkarni",
        "C Corneanu",
        "I Ofodile",
        "S Escalera",
        "X Baro",
        "S Hyniewska",
        "J Allik",
        "G Anbarjafari"
      ],
      "year": "2018",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "80",
      "title": "Multimodal information fusion application to human emotion recognition from face and speech",
      "authors": [
        "M Mansoorizadeh",
        "N Charkari"
      ],
      "year": "2010",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "81",
      "title": "Baum-1: A spontaneous audio-visual face database of affective and mental states",
      "authors": [
        "S Zhalehpour",
        "O Onder",
        "Z Akhtar",
        "C Erdem"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "82",
      "title": "Learning affective features with a hybrid deep model for audio-visual emotion recognition",
      "authors": [
        "S Zhang",
        "S Zhang",
        "T Huang",
        "W Gao",
        "Q Tian"
      ],
      "year": "2017",
      "venue": "IEEE transactions on circuits and systems for video technology"
    },
    {
      "citation_id": "83",
      "title": "Holonet: towards robust emotion recognition in the wild",
      "authors": [
        "A Yao",
        "D Cai",
        "P Hu",
        "S Wang",
        "L Sha",
        "Y Chen"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM international conference on multimodal interaction"
    },
    {
      "citation_id": "84",
      "title": "Facial expression recognition with deeply-supervised attention network",
      "authors": [
        "Y Fan",
        "V Li",
        "J Lam"
      ],
      "year": "2020",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "85",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "86",
      "title": "Efficientnet: Rethinking model scaling for convolutional neural networks",
      "authors": [
        "M Tan",
        "Q Le"
      ],
      "year": "2019",
      "venue": "International conference on machine learning"
    }
  ]
}