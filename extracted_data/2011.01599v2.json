{
  "paper_id": "2011.01599v2",
  "title": "Experiencers, Stimuli, Or Targets: Which Semantic Roles Enable Machine Learning To Infer The Emotions?",
  "published": "2020-11-03T10:01:44Z",
  "authors": [
    "Laura Oberländer",
    "Kevin Reich",
    "Roman Klinger"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition is predominantly formulated as text classification in which textual units are assigned to an emotion from a predefined inventory (e.g., fear, joy, anger, disgust, sadness, surprise, trust, anticipation). More recently, semantic role labeling approaches have been developed to extract structures from the text to answer questions like: \"who is described to feel the emotion?\" (experiencer), \"what causes this emotion?\" (stimulus), and at which entity is it directed?\" (target). Though it has been shown that jointly modeling stimulus and emotion category prediction is beneficial for both subtasks, it remains unclear which of these semantic roles enables a classifier to infer the emotion. Is it the experiencer, because the identity of a person is biased towards a particular emotion (X is always happy)? Is it a particular target (everybody loves X) or a stimulus (doing X makes everybody sad)? We answer these questions by training emotion classification models on five available datasets annotated with at least one semantic role by masking the fillers of these roles in the text in a controlled manner and find that across multiple corpora, stimuli and targets carry emotion information, while the experiencer might be considered a confounder. Further, we analyze if informing the model about the position of the role improves the classification decision. Particularly on literature corpora we find that the role information improves the emotion classification.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion analysis is now an established research area which finds application in a variety of different fields, including social media analysis  (Purver and Battersby, 2012; Wang et al., 2012; Mohammad and Bravo-Marquez, 2017; Ying et al., 2019, i.a.) , opinion mining  (Choi et al., 2006, i.a.) , and computational literary studies  (Alm et al., 2005; Kim and Klinger, 2019a; Haider et al., 2020; Zehe et al., 2020, i.a.) . The most prominent task in emotion analysis is emotion categorization, where text receives assignments from a predefined emotion inventory, such as the fundamental emotions of fear, anger, joy, anticipation, trust, surprise, disgust, and sadness which follow theories by  Ekman (1999)  or  Plutchik (2001) . Other tasks include the recognition of affect values, namely valence or arousal  (Posner et al., 2005)  or analyses of event appraisal  (Hofmann et al., 2020; Scherer, 2005) .\n\nMore recently, categorization (or regression) tasks have been complemented by more fine-grained analyses, namely emotion stimulus detection and role labeling, to detect which words denote the experiencer of an emotion, the emotion cue description, or the target of an emotion. These efforts lead to computational approaches of detecting stimulus clauses  (Xia and Ding, 2019; Wei et al., 2020; Gao et al., 2017)  and emotion role labeling and sequence labeling  (Mohammad et al., 2014; Bostan et al., 2020; Kim and Klinger, 2018; Ghazi et al., 2015; Zehe et al., 2020) , with different advantages and disadvantages we discuss in  Oberländer and Klinger (2020) .\n\nFurther, this work led to a rich set of corpora with annotations of different subsets of roles. An example of a sentence annotated with semantic role labels for emotion is \" John .\" A number of English-language resources are available:  Ghazi et al. (2015)   manually construct a dataset following FrameNet's emotion predicate and annotate the stimulus as its core argument.  Mohammad et al. (2014)  annotate Tweets for emotion cue phrases, emotion targets, and the emotion stimulus. In our previous work  (Bostan et al., 2020)  we publish news headlines annotated with the roles of emotion experiencer, cue, target, and stimulus.  Kim and Klinger (2018)  annotate sentence triples taken from literature for the same roles. A popular benchmark for emotion stimulus detection is the Mandarin corpus by  Gui et al. (2016) .  Gao et al. (2017)  annotate English and Mandarin texts in a comparable way on the clause level (Emotion Cause Analysis, ECA).\n\nIn this paper, we utilize role annotations to understand their influence on emotion classification. We evaluate which of the roles' contents enable an emotion classifier to infer the emotions. It is reasonable to assume that the roles' content carries different kinds of information regarding the emotion: One particular experiencer present in a corpus might always feel the same emotion; hence, be prone to a bias the model could pick up on. The target or stimulus might be independent of the experiencer and be sufficient to infer the emotion. The presence of a target might limit the set of emotions that can be triggered. Finally, as some of the corpora contain cue annotations, we assume that these are the most helpful to decide on the expressed emotion, as they typically have explicit references towards concrete emotion names.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Experimental Setting",
      "text": "In the following, we describe our experiments to understand which of the datasets' annotated roles contribute to the emotion classification performance.\n\nDatasets. We base our experiments on five available datasets that are annotated for at least one of the roles of an experiencer, stimulus, target, or cue. The dataset by  Ghazi et al. (2015)  is one of the earliest we are aware of that contains stimulus annotations. They annotate based on FrameNet's emotion-directed frames that have a stimulus argument in the data (we refer to their corpus as Emotion-Stimulus, ES). Similarly early work is the Twitter corpus by  Mohammad et al. (2014) (ElectoralTweets, ET) . They also follow the emotion frame semantics definition but use data concerning the 2012 U.S. election. Therefore, their resource may be considered more diverse in language but more consistent in its domain than ES. More recently,  Bostan et al. (2020)  published an annotation of news headlines (GoodNewsEveryone, GNE). While they do not limit their corpus on a domain, they use a comparably narrow time window to retrieve the data and sample according to the inclusion of emotion words and popularity on social media.  Kim and Klinger (2018, REMAN)  and  Gao et al. (2017, Emotion  Cause Analysis, ECA) use literature data, which might be considered the most challenging for emotion analysis (for ECA, we use the English subset only).\n\nAs Table  1  shows, the literature data (REMAN, ECA) has the longest instances and also the longest stimulus annotations. The other resources have less than one third of their length in tokens, with GNE being the shortest. However, the overall annotation length does not differ dramatically. Cue, target, and experiencer annotations are only available in three out of five corpora (ET, REMAN, and GNE) 1  .\n\nModel Configuration. Our goal is to analyze the importance of different roles for the emotion classification. We use two different models, namely a bidirectional long short-term memory network  (Hochreiter and Schmidhuber, 1997)  with pretrained 300-dimensional GloVe embeddings 2  and a transformer-based model, RoBERTa  (Liu et al., 2019) . Both models take as input the text sequence and output the emotion class, where the concrete set of emotion labels depends on the dataset.\n\nThe models have different advantages and disadvantages in our experimental setting. The bi-LSTM with non-contextualized word embeddings might be more appropriate to be used in our setting in which we manipulate the input token sequence (see below). The transformer might benefit from the rich contextualized pretraining, which is particularly relevant given that the annotated corpora are of comparably limited size (in the context of deep learning) 3  .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Setting Model Input",
      "text": "As-Is John hates cars because they pollute the environment Only\n\nX hates cars because they pollute the environment Without Tar.\n\nJohn hates X because they pollute the environment Pos. Stim.\n\nJohn hates cars because they pollute the environment Pos. Exp.\n\nJohn hates cars because they pollute the environment Pos. Tar.\n\nJohn hates cars because they pollute the environment Table  2 : Illustration of the experimental settings. X, , denote special tokens added to the input according to each setting.\n\nSetting and Hypotheses. We apply these models in several settings (illustrated in Table  2 ), which differ in the availability of information from the roles, namely (1), As-Is: This is the standard setting: The classifier has access to the whole text.\n\n(2), Without the text of the particular roles.\n\n(3), Only with the text of a particular role, masking the text that does not belong to it. Finally, (4), we keep the information available as is, but besides inform the model about the Position of the role. The latter is realized by adding positional indicators, inspired by  Kim and Klinger (2019b)  who showed the use of positional indicators for emotion relation classification  4  . For roles that carry information relevant for emotion classification, we expect the Without setting to show a drop in performance compared to the As-Is setting. In such cases, the Only setting might show comparable performance, and the Position setting would show further improvements. When the role is a confounder, the performance in the Without setting is expected to be increased over the As-Is setting.\n\nThe label set depends on each of the datasets. For ES, we use the emotion labels anger, disgust, fear, joy, no emotion, sadness, and surprise; for ECA, we use anger, sadness, disgust, joy, fear, surprise, and no emotion. For GNE and ET, we merge the categories according to the rules described for ET by  Bostan and Klinger (2018)  and keep the primary emotions described in Plutchik's wheel. For REMAN, we group similarly and keep anger, disgust, fear, joy, anticipation, surprise, sadness, trust, and no emotion. ECA has a low number of instances annotated with multiple labels, which we ignore to keep all tasks as single-label classification. REMAN has emotion annotations only for the middle sentence in each triple. Thus we include only these middle segments in our experiments.\n\nThe results are based on a random split of each dataset into train, validation, and test (0.8, 0.1, 0.1). We report macro-averages across 10 runs for the bi-LSTM and 5 runs for RoBERTa.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results",
      "text": "In the following, we discuss the results of the bi-LSTM model in detail and then point to differences to those of the transformer-based approach.  based model. Intuitively, we would expect the As-Is setting to outperform both the Without and Only settings because there is more information available to the model. Conversely, because information is added in Position, we expect it to outperform the As-Is setting. As we see in column As-Is, the scores for the emotion classification task differ substantially, even when all available information is shown to the model. In the Without setting, we see that removing information can sometimes help a model improve its decision. For instance, when we mask the labels of the respective role, we observe a performance increase for the experiencer role in GNE, which could potentially point to an unwanted bias for particular experiencers in this corpus. This is also the case for the stimulus role in ECA and the target role in ET.\n\nAs expected, an important role for emotion classification is the cue. In REMAN, the performance drops the most when the classifier does not see the cue span and gains the most when only the cue is available. For all other corpora, the cue role is not as important, but performance still shows a drop when it is not available (Without). Similarly, for all datasets except ECA, the performance drops when the stimulus is not shown. On the other hand, the stimulus alone is insufficient to infer the emotion with competitive performance. Noteworthy here is the corpus ES, in which the performance drop is particularly high.\n\nThese results show that the information contained in different roles is of varying importance and depends on the data's source and domain. In the setting Position, we leave all information accessible to the model but add positional indicators for the investigated role to the input for emotion classification. We see improvements in most cases, except REMAN, for which adding the positional information hurts the classification for all roles. This result could be because REMAN has very long annotation spans. Both ECA and ES show an improvement for their annotated role (stimulus). For ET, an increase in performance is shown when additional knowledge about the stimulus position is given, and for GNE, a slight improvement is shown when the model is given the experiencer's position information.\n\nTable  4  shows the results of the transformer-based model evaluated in the same settings. As expected, the model shows performance improvements across all datasets in comparison to the bi-LSTM model. In the As-Is setting, we see a substantial increase in performance for REMAN. This result can be explained by the fact that the pretrained large language model has seen more literary English than the embeddings used as pretrained input to the bi-LSTM. GNE and ET scores are also improved across the roles. In the Without setting, we do not see the same patterns as for the bi-LSTM based model; the scores when hiding the stimulus for ECA, the target for ET, and experiencer for GNE do not increase over the scores of the As-Is setting.\n\nThis might have two reasons: On one hand, it is less likely to improve upon already high values when changing the model configuration. On the other hand, and more interestingly, it might be that the contextualized embeddings compensate for missing information. Interestingly for the Position setting, the results are improving on all datasets, and REMAN gains from the cue's positional indicators. The dataset that stands out in this setting is ET, for which we see a slight decrease in performance across all roles available. The Only setting shows that the stimulus captures most of the emotion information for GNE and ET. The result for GNE is due to the particularly lengthy stimuli spans that sometimes stretch over the whole instance.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Conclusion And Future Work",
      "text": "Our experiments show that the importance of semantic roles for emotion classification differs between datasets and roles: The stimulus and cue are critical for classification, which correspond to the direct report of a feeling and the description that triggered an emotion. This result is shown in the drop in performance when removing these roles. This information is not redundantly available outside of these arguments.\n\nIt is particularly beneficial for the model's performance to have access to the position of cues and stimuli. This suggests that the classifier learns to tackle the problem differently when this information is available, especially so for ECA and ES -the cases in which literature has been annotated and the instances are comparably long.\n\nThe bi-LSTM model indicates that the experiencer role is a confounder in GNE. The performance can be increased when the model does not have access to its content. Similar results are observed for ET, in which the target role is a confounder. However, these results should be taken with a grain of salt given that they are not confirmed while switching to the transformer-based model. The differences in results between the bi-LSTM and the transformer also motivate further research, as they suggest that the contextualized representation might compensate for missing information, and is, therefore, more robust.\n\nFinally, our results across both models and multiple datasets indicate that emotion classification approaches indeed benefit from semantic roles' information by adding the positional information. Similarly to targeted and aspect-based sentiment analysis, this motivates future work, in which emotion classification and role labeling should be modelled jointly.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Analysis Of Content Of Roles",
      "text": "Table  7  shows the most frequent tokens marked as stimulus, experiencer or target over each dataset. They differ substantially per dataset and reflect well the respective source. The counts suggest a Zipfian distribution for ElectoralTweets (stimulus and target) and GoodNewsEveryone (experiencer, stimulus). This could explain the results obtained in the Without setting by the bi-LSTM-based model. The most common tokens annotated with the target role in ElectoralTweets also show the polarized nature of those who tweeted about the election.\n\nFigure  1  shows the distribution of the most frequent tokens (across all roles) for the most frequent emotions of ET and GNE. The plots marked with \"overall\" show the prior distribution of emotions in the respective dataset. We see that for the emotion admiration, \"president\" stands out. Further we note that \"Romney\" is associated with dislike in this corpus.\n\nFor GNE we observe that the most frequent tokens are occurring less in instances annotated with positive surprise than overall, and more in instances annotated with anger (except for \"Biden\") showing that these tokens could be biased towards more negative emotions. This shows a bias of the dataset towards negative emotion when it comes to the most prominent tokens.   7 ), wish (6) Stim. little (10), another (8), face (8), got (7), lord (7), left (7), great (7), wife (7), men (6), life (6) Exp. man (23), woman (12), boy (7), old (7), isabel (6), people (6), god (5), father (5), heart (5), henry (5) Target man (22), little (9), things (8), woman (8), see (8), old (7), god (6), wife (6), another (6), true (  5",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: shows the distribution of the most frequent tokens (across all roles) for the most frequent",
      "page": 10
    },
    {
      "caption": "Figure 1: Emotion distribution of instances containing the respective tokens (% for the top-5 most frequent",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Datasets with annotations of roles. # refers to the number of total instances. ∅len shows the",
      "page": 2
    },
    {
      "caption": "Table 1: shows, the literature data (REMAN, ECA) has the longest instances and also the longest",
      "page": 2
    },
    {
      "caption": "Table 2: Illustration of the experimental settings. X, ⌊, ⌉denote",
      "page": 3
    },
    {
      "caption": "Table 2: ), which differ in the",
      "page": 3
    },
    {
      "caption": "Table 3: shows the results of our experiments for the bi-LSTM-",
      "page": 3
    },
    {
      "caption": "Table 3: Results of our bi-LSTM based model for emotion classiﬁcation, with access to all tokens (As-Is),",
      "page": 4
    },
    {
      "caption": "Table 4: shows the results of the transformer-based model evaluated in the same settings. As expected,",
      "page": 4
    },
    {
      "caption": "Table 4: Results of our transformer based model (RoBERTa) for emotion classiﬁcation.",
      "page": 5
    },
    {
      "caption": "Table 5: We observe in instances correctly classiﬁed in the",
      "page": 8
    },
    {
      "caption": "Table 5: Examples in which the prediction is incorrect when the model is applied on the whole instance,",
      "page": 8
    },
    {
      "caption": "Table 6: (only for the bi-LSTM model).",
      "page": 9
    },
    {
      "caption": "Table 6: Results per emotion for ECA and ES with and without positional stimuli information. Bold",
      "page": 9
    },
    {
      "caption": "Table 7: shows the most frequent tokens marked as cue, stimulus, experiencer or target over each dataset.",
      "page": 10
    },
    {
      "caption": "Table 7: Most frequent 10 tokens with frequencies for each role and dataset.",
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotions from text: Machine learning for textbased emotion prediction",
      "authors": [
        "Cecilia Ovesdotter Alm",
        "Dan Roth"
      ],
      "year": "2005",
      "venue": "Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "2",
      "title": "An analysis of annotated corpora for emotion classification in text",
      "authors": [
        "Laura-Ana-Maria Bostan",
        "Roman Klinger"
      ],
      "year": "2018",
      "venue": "Proceedings of the 27th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "3",
      "title": "GoodNewsEveryone: A corpus of news headlines annotated with emotions, semantic roles, and reader perception",
      "authors": [
        "Laura Ana",
        "Maria Bostan",
        "Evgeny Kim",
        "Roman Klinger"
      ],
      "year": "2020",
      "venue": "Proceedings of the 12th International Conference on Language Resources and Evaluation (LREC'20)"
    },
    {
      "citation_id": "4",
      "title": "Joint extraction of entities and relations for opinion recognition",
      "authors": [
        "Yejin Choi",
        "Eric Breck",
        "Claire Cardie"
      ],
      "year": "2006",
      "venue": "Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "5",
      "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "6",
      "title": "Basic emotions",
      "authors": [
        "Paul Ekman"
      ],
      "year": "1999",
      "venue": "Handbook of Cognition and Emotion"
    },
    {
      "citation_id": "7",
      "title": "Overview of NTCIR-13 ECA task",
      "authors": [
        "Qinghong Gao",
        "Jiannan Hu",
        "Ruifeng Xu",
        "Gui Lin",
        "Yulan He",
        "Qin Lu",
        "Kam-Fai Wong"
      ],
      "year": "2017",
      "venue": "Proceedings of the 13th NTCIR Conference on Evaluation of Information Access Technologies"
    },
    {
      "citation_id": "8",
      "title": "AllenNLP: A deep semantic natural language processing platform",
      "authors": [
        "Matt Gardner",
        "Joel Grus",
        "Mark Neumann",
        "Oyvind Tafjord",
        "Pradeep Dasigi",
        "Nelson Liu",
        "Matthew Peters",
        "Michael Schmitz",
        "Luke Zettlemoyer"
      ],
      "year": "2018",
      "venue": "Proceedings of Workshop for NLP Open Source Software (NLP-OSS)"
    },
    {
      "citation_id": "9",
      "title": "Detecting emotion stimuli in emotion-bearing sentences",
      "authors": [
        "Diman Ghazi",
        "Diana Inkpen",
        "Stan Szpakowicz"
      ],
      "year": "2015",
      "venue": "International Conference on Intelligent Text Processing and Computational Linguistics"
    },
    {
      "citation_id": "10",
      "title": "Event-driven emotion cause extraction with corpus construction",
      "authors": [
        "Lin Gui",
        "Dongyin Wu",
        "Ruifeng Xu",
        "Qin Lu",
        "Yu Zhou"
      ],
      "year": "2016",
      "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "11",
      "title": "PO-EMO: Conceptualization, annotation, and modeling of aesthetic emotions in German and English poetry",
      "authors": [
        "Thomas Haider",
        "Steffen Eger",
        "Evgeny Kim",
        "Roman Klinger",
        "Winfried Menninghaus"
      ],
      "year": "2020",
      "venue": "Proceedings of the 12th Language Resources and Evaluation Conference"
    },
    {
      "citation_id": "12",
      "title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "13",
      "title": "Long short-term memory",
      "authors": [
        "Sepp Hochreiter",
        "Jürgen Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural Comput"
    },
    {
      "citation_id": "14",
      "title": "Appraisal theories for emotion classification in text",
      "authors": [
        "Jan Hofmann",
        "Enrica Troiano",
        "Kai Sassenberg",
        "Roman Klinger"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "15",
      "title": "Who feels what and why? annotation of a literature corpus with semantic roles of emotions",
      "authors": [
        "Evgeny Kim",
        "Roman Klinger"
      ],
      "year": "2018",
      "venue": "Proceedings of the 27th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "16",
      "title": "2019a. An analysis of emotion communication channels in fan-fiction: Towards emotional storytelling",
      "authors": [
        "Evgeny Kim",
        "Roman Klinger"
      ],
      "venue": "Proceedings of the Second Workshop on Storytelling"
    },
    {
      "citation_id": "17",
      "title": "Frowning wincing Leia, and a seriously great friendship: Learning to classify emotional relationships of fictional characters",
      "authors": [
        "Evgeny Kim",
        "Roman Klinger"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "18",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2015",
      "venue": "3rd International Conference on Learning Representations, ICLR 2015"
    },
    {
      "citation_id": "19",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "20",
      "title": "WASSA-2017 shared task on emotion intensity",
      "authors": [
        "Saif Mohammad",
        "Felipe Bravo-Marquez"
      ],
      "year": "2017",
      "venue": "Proceedings of the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis"
    },
    {
      "citation_id": "21",
      "title": "Semantic role labeling of emotions in tweets",
      "authors": [
        "Saif Mohammad",
        "Xiaodan Zhu",
        "Joel Martin"
      ],
      "year": "2014",
      "venue": "Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis"
    },
    {
      "citation_id": "22",
      "title": "Token sequence labeling vs. clause classification for english emotion stimulus detection",
      "authors": [
        "Laura Oberländer",
        "Roman Klinger"
      ],
      "year": "2020",
      "venue": "Proceedings of the 9th Joint Conference on Lexical and Computational Semantics"
    },
    {
      "citation_id": "23",
      "title": "Glove: Global vectors for word representation",
      "authors": [
        "Jeffrey Pennington",
        "Richard Socher",
        "Christopher Manning"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)"
    },
    {
      "citation_id": "24",
      "title": "The nature of emotions human emotions have deep evolutionary roots, a fact that may explain their complexity and provide tools for clinical practice",
      "authors": [
        "Robert Plutchik"
      ],
      "year": "2001",
      "venue": "American Scientist"
    },
    {
      "citation_id": "25",
      "title": "The circumplex model of affect: an integrative approach to affective neuroscience, cognitive development, and psychopathology",
      "authors": [
        "Jonathan Posner",
        "James Russell",
        "Bradley Peterson"
      ],
      "year": "2005",
      "venue": "Development and Psychopathology"
    },
    {
      "citation_id": "26",
      "title": "Experimenting with distant supervision for emotion classification",
      "authors": [
        "Matthew Purver",
        "Stuart Battersby"
      ],
      "year": "2012",
      "venue": "Proceedings of the 13th Conference of the European Chapter"
    },
    {
      "citation_id": "27",
      "title": "What are emotions? And how can they be measured?",
      "authors": [
        "Klaus Scherer"
      ],
      "year": "2005",
      "venue": "Social Science Information"
    },
    {
      "citation_id": "28",
      "title": "Harnessing twitter \"big data\" for automatic emotion identification",
      "authors": [
        "Wenbo Wang",
        "Lu Chen",
        "Krishnaprasad Thirunarayan",
        "Amit Sheth"
      ],
      "year": "2012",
      "venue": "SocialCom/PASSAT"
    },
    {
      "citation_id": "29",
      "title": "Effective inter-clause modeling for end-to-end emotion-cause pair extraction",
      "authors": [
        "Penghui Wei",
        "Jiahao Zhao",
        "Wenji Mao"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "30",
      "title": "Huggingface's transformers: State-of-the-art natural language processing",
      "authors": [
        "Thomas Wolf",
        "Lysandre Debut",
        "Victor Sanh",
        "Julien Chaumond",
        "Clement Delangue",
        "Anthony Moi",
        "Pierric Cistac",
        "Tim Rault",
        "Rémi Louf",
        "Morgan Funtowicz",
        "Joe Davison",
        "Sam Shleifer",
        "Clara Patrick Von Platen",
        "Yacine Ma",
        "Julien Jernite",
        "Canwen Plu",
        "Teven Xu",
        "Sylvain Le Scao",
        "Mariama Gugger",
        "Quentin Drame",
        "Alexander Lhoest",
        "Rush"
      ],
      "year": "2019",
      "venue": "Huggingface's transformers: State-of-the-art natural language processing"
    },
    {
      "citation_id": "31",
      "title": "Emotion-cause pair extraction: A new task to emotion analysis in texts",
      "authors": [
        "Rui Xia",
        "Zixiang Ding"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "32",
      "title": "Improving multi-label emotion classification by integrating both general and domain-specific knowledge",
      "authors": [
        "Wenhao Ying",
        "Rong Xiang",
        "Qin Lu"
      ],
      "year": "2019",
      "venue": "Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)"
    },
    {
      "citation_id": "33",
      "title": "Harrymotions -classifying relationships in harry potter based on emotion analysis",
      "authors": [
        "Albin Zehe",
        "Julia Arns",
        "Lena Hettinger",
        "Andreas Hotho"
      ],
      "year": "2020",
      "venue": "th SwissText & 16th KONVENS Joint Conference"
    }
  ]
}