{
  "paper_id": "2309.11014v1",
  "title": "Ensembling Multilingual Pre-Trained Models For Predicting Multi-Label Regression Emotion Share From Speech",
  "published": "2023-09-20T02:28:00Z",
  "authors": [
    "Bagus Tris Atmaja",
    "Akira Sasou"
  ],
  "keywords": [
    "speech emotion recognition",
    "ensemble learning",
    "multilingual speech processing",
    "pre-trained model",
    "emotion share"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech emotion recognition has evolved from research to practical applications. Previous studies of emotion recognition from speech have focused on developing models on certain datasets like IEMOCAP. The lack of data in the domain of emotion modeling emerges as a challenge to evaluate models in the other dataset, as well as to evaluate speech emotion recognition models that work in a multilingual setting. This paper proposes an ensemble learning to fuse results of pre-trained models for emotion share recognition from speech. The models were chosen to accommodate multilingual data from English and Spanish. The results show that ensemble learning can improve the performance of the baseline model with a single model and the previous best model from the late fusion. The performance is measured using the Spearman rank correlation coefficient since the task is a regression problem with ranking values. A Spearman rank correlation coefficient of 0.537 is reported for the test set, while for the development set, the score is 0.524. These scores are higher than the previous study of a fusion method from monolingual data, which achieved scores of 0.476 for the test and 0.470 for the development.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Research on speech emotion recognition (SER) has been focused mainly on recognizing categorial emotions like happy, angry, sad, or neutral. There are attempts to predict dimensional emotions like valence, arousal, and dominance; however, their application is still limited. The reason is that the dimensional emotion is not as straightforward as the categorical emotion. Nevertheless, the previous studies on categorial emotion recognition are designed to choose a single class of emotions in multiclass settings. In contrast to categorial emotions, the emotions, particularly conveyed by vocal bursts, are bridged by smooth gradients (instead of clear separation between discreet emotions) with continuously varying meanings  [1] . In that study, the authors found 24 categorial emotions, which may not have been limited to vocal bursts only but also to general emotion perception or speech perception (since vocal bursts are commonly found within speech).\n\nAn attempt to analyze emotion fine-grained emotion (i.e., as a regression task) instead of coarse-grained emotion (i.e., by classification task) has been conducted from a linguistic point of view  [2] . The authors mapped the degrees of valence and arousal given the sentence into six basic categorical emotions via several lexicons and corpus. The top model (using WKB lexicon) obtained correlation scores of 0.65 for dimensional emotions (valence, arousal, dominance) and 0.416 for categorial emotions. Besides only using linguistic information, the evaluation was conducted on English data only.\n\nRecently, the multilingual approach to tackle the limitation of speech technology has been proposed in the parent tasks of speech processing, automatic speech recognition, speech synthesis, and speech translation. Models like Whisper  [3]  and MMS  [4]  can precisely recognize speech in multiple languages. Stirred by this direction, research on speech emotion recognition has been directed to tackle its limitation under multilingual settings. Assuming that emotion is a universal language (i.e., the pattern of angry speech is the same across languages or cultures), the multilingual approach is expected to work also on speech-emotion recognition with diverse language samples.\n\nMultilingual SER attempts to recognize emotion regardless of the language. One can build a single model (including a model trained on multilingual data) for each language with language detection as pre-processing; another can build a single model for all languages. This study is aimed at the latter approach. The previous studies utilized transfer learning for cross-lingual SER  [5] , distribution adaption  [6] , and implicit distribution alignment  [7] . The last two studies were developed for cross-corpus SER with different languages.\n\nThe use of multiple models for SER has shown to be effective for monolingual data. The technique of ensemble was usually adopted to fuse several models from different modalities and/or features. Examples are acoustic and linguistic for dimensional SER  [8] ,  [9] , acoustic and facial expression  [10] , and multi-feature fusion  [11] . In the previous study  [12] , ensemble learning was also utilized to tackle the limitation of skewed training data in monolingual SER by maximizing, averaging, and log averaging of the output probabilities of the component classifiers.\n\nThis paper contributes to the evaluation of ensemble learning, combining several models via late fusion methods for predicting emotion share from speech in multilingual settings. The fusion was performed by averaging predictions of different models. We evaluated eight multilingual pre-trained models since the dataset contains multilingual data from English and Spanish. The pre-trained models are chosen from those languages. We showed that by combining the models via ensemble learning with the late fusion method, we could improve the performance of the baseline model with a single model. The performance is measured using the Spearman rank correlation coefficient since the task is a regression problem with ranking values.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Dataset And Task",
      "text": "The dataset used in this study is obtained from the dataset for The ACM Multimedia 2023 Computational Paralinguistic Challenge (ComParE) Emotion Request Sub-Challange  [13] , which is provided by Hume AI  [14] . There are nine categorical emotions in the dataset: 'Anger', 'Boredom', 'Calmness', 'Concentration', 'Determination', 'Excitement', 'Interest', 'Sadness', and 'Tiredness'. The choice of these nine emotions is based on their more balanced distribution in the valence-arousal space. The basis for the dataset itself is more than 5,000 'seed' samples.\n\nSeeds consist of various emotional expressions, which were gathered from openly available datasets, including MELD  [15]  and VENEC  [16] . The seed samples were mimicked by speakers recruited via Amazon Mechanical Turk by the provider of the dataset  [14] . The dataset consists of 51,881 'mimic' samples (total of 41:48:55 h of data, mean 2.9 s, range 1.2 -7.98 s) from 1,004 speakers aged from 20 to 66 years old. The data were collected from 3 countries with broadly differing cultures: the United States (English/EN), South Africa (English/EN), and Venezuela (Spanish/SP). Each seed sample was rated by the individual who imitated it using a 'select-all-that-apply' method. Seeds were assigned a mean of 2.03 emotions per rater (max: 7.11, min: 1.00), with a standard deviation of 1.33 emotions. The proportion of times a given seed sample was rated with each emotion was then applied to all mimics of that seed sample. This method results in the share per emotion assigned by the speakers. The label then is normalized by dividing all (floating point) scores with their highest score per sample; hence, each sample has a maximum score of 1.0 for one emotion category and other scores less than 1.0 for other emotion categories. The baseline results for this dataset is a Spearman score of 0.500 for the development set and 0.514 for the test set (using wav2vec finetuned of affective speech dataset with SVM method). The labels of the test set are not open to the public; hence, only performance on the development/validation data can be calculated directly.\n\nTable  I  shows the characteristics of the dataset. The ratio for training/test split follows the practical machine/deep learning practice, i.e., 80/20. About 18.3% of the samples are allocated for the test (without labels), while the rest are for training (including validation/development). The ratio of female/male is about 63.5/36.5, with the number of female samples being higher than male. The splitting of training/test follows speakerindependent criteria with a total of 1004 speakers; 20% of them  is allocated for the test set while the rest is for training and development.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iii. Pre-Trained Models",
      "text": "We evaluated nine pre-trained models as acoustic embedding feature extractors and fused the results of these models for ensemble learning. Ensemble learning leverages the advantages of multiple models to achieve a better performance score. In this study, we evaluated a single late fusion method by averaging the emotion share predictions of different models. The baseline model is the robust version of on wav2vec 2.0  [17]  finetuned the affective dataset  [18] . For multilingual pretrained models, we employed XLS-R 53  [19]  and its variants, XLSR variants  [20]  (XLSR-300M, XLSR-1B, and XLSR-2B). For the XLS-R 53 and XLSR-1B, we also evaluated the finetuned version of this model on the English (EN) and Spanish (SP) datasets. Finetuning for these languages is not available on other XLSR variants. The complete pre-trained models are listed in Table  II .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iv. Classifier And Hyperparameter Search Space",
      "text": "We employed a support vector machine (SVM) classifier for regression (SVR, support vector regression). The type of kernel for SVR is linear (LinearSVR in the scikit-learn toolkit). The optimal parameters for this SVR were searched using the Grid Search algorithm, i.e., the regularization parameter C and the algorithm to solve either the 'dual' or 'primal' optimization problem. Other parameters like scoring were also searched. The scoring is either using negative mean absolute error (NMAE) or negative mean squared error (NMSE). The maximum iteration is fixed at 5000. The value range for these parameters is shown in Table  III .\n\nFor the fusion of all nine models, we employed the average of the predicted values (arithmetic mean). The predicted values are the continuous values in either development or test for each of the nine emotion categories. The ordinal values are then used to calculate performance metrics. The final score is an average of nine emotional categories.\n\nThe performance for evaluating the models is measured as the Spearman correlation coefficient (ρ)  [21] . This Spearman's ρ metric is similar to the Pearson correlation coefficient but for ranked data. This metric is chosen since the annotation of the dataset is ordinal (based on rank) and for consistency with the baseline model  [13] . Spearman correlation coefficient is calculated as follows:\n\nwhere cov(R(X), R(Y )) denotes the covariance of the ranked data R(X) and R(Y ), and σ R(X) and σ R(Y ) denote the standard deviation of the ranked data R(X) and R(Y ), respectively. A repository is created to ensure the reproducibility of the experiments 1 . The repository contains the experiment settings, including the hyperparameter search space, and the results of the experiments. The requirements to run the experiments are also provided in the repository with the exact version of the libraries used at the time of the experiment.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "V. Experiment Results And Discussions",
      "text": "We divided our results into validation or development (Table  IV ) and test results (Table  V ). In the development stage, we ensure that the result of ensemble learning (Fusion all in Table  IV ) is higher than baseline results, either from a single model or late fusion. We submitted our prediction of the test result to the provider of the dataset  [13]  to obtain the performance on the test set.\n\nTable  IV  shows our result in the development stage using nine pre-trained models and a fusion of all these models. On a single model evaluation, it is shown that using pre-trained models (such as XLS-R 53 and XLSR variants) achieves higher accuracies than conventional acoustic feature extractors like auDeep and ComParE. For instance, XLS-R 53 (the smallest multilingual model) gained a Spearman score of 0.4328, while ComParE achieved 0.359. For the fusion models, our fusion of all nine pre-trained models also overcomes the previous late fusion of wav2vec2, auDeep, DeepSpectrum, and ComParE.\n\nHaving good results in the development stage, we evaluated the same model on the test set. Table  V  shows the result of the test set. The result shows that the fusion of all models achieves the highest Spearman score of 0.537 (compared to the 0.476 of the late fusion from the previous study  [13] ).  This result is also higher than the previous best result of 0.500 from wav2vec2  [18] . We assume ensemble learning works by leveraging information across different languages and pooling maximum information for multilingual SER. Similar trends between test and development sets are observed where the performance of the test set is slightly higher than the development set. An exception only applies to the DeepSpectrum result where the test result is 0.004 lower than the development set. This trend indicates the generalization of the evaluated method (including the baseline methods that use a similar SVM classifier) to unseen data and shows that, perhaps, the distribution of the test set is similar to the development set.\n\nFinally, Table VI breaks down the average Spearman's ρ in the test set (ρ = 0.537) into each emotion category. The result is from 'Fusion all' of nine pre-trained models on the test set. The result shows that the model performs best in the 'Calmness' emotion category (0.6061) and worst in the 'Interest' emotion category (0.4238). The result is similar to the previous study  [13]  where the 'Calmness' emotion category is the highest (0.559), and the 'Anger' emotion category is the lowest (0.428). The previous study's result was achieved with wav2vec 2.0 but with a different setup from this study (particularly on the scoring method).",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Vi. Conclusions",
      "text": "In this study, we evaluated nine pre-trained speech models and a fusion of all these nine models for emotion share recognition from speech. The idea was to collect multilingual models from different languages and apply the fusion of these models to multilingual SER data. A dataset from three countries with English and Spanish languages was selected. First, the fusion of nine pre-trained models defeats the previously reported best result of a single model (in which the performance of this single model is higher than the late fusion of four methods). Second, the results of any single model in this study generally are also higher than the single models/methods in the previous study. Third, there is a trend in which the performance of the test set is slightly higher than the development set. Future studies could be directed to explore more about the dataset and improve the performance using more advanced methods with more recent speech embeddings.",
      "page_start": 3,
      "page_end": 3
    }
  ],
  "figures": [],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "E-mail: b-atmaja@aist.go.jp, a-sasou@aist.go.jp": "of view [2]. The authors mapped the degrees of valence and"
        },
        {
          "E-mail: b-atmaja@aist.go.jp, a-sasou@aist.go.jp": ""
        },
        {
          "E-mail: b-atmaja@aist.go.jp, a-sasou@aist.go.jp": "arousal given the sentence into six basic categorical emotions"
        },
        {
          "E-mail: b-atmaja@aist.go.jp, a-sasou@aist.go.jp": ""
        },
        {
          "E-mail: b-atmaja@aist.go.jp, a-sasou@aist.go.jp": "via several\nlexicons and corpus. The top model\n(using WKB"
        },
        {
          "E-mail: b-atmaja@aist.go.jp, a-sasou@aist.go.jp": ""
        },
        {
          "E-mail: b-atmaja@aist.go.jp, a-sasou@aist.go.jp": "lexicon) obtained correlation scores of 0.65 for dimensional"
        },
        {
          "E-mail: b-atmaja@aist.go.jp, a-sasou@aist.go.jp": ""
        },
        {
          "E-mail: b-atmaja@aist.go.jp, a-sasou@aist.go.jp": "emotions (valence, arousal, dominance) and 0.416 for catego-"
        },
        {
          "E-mail: b-atmaja@aist.go.jp, a-sasou@aist.go.jp": "rial\nemotions. Besides only using linguistic information,\nthe"
        },
        {
          "E-mail: b-atmaja@aist.go.jp, a-sasou@aist.go.jp": ""
        },
        {
          "E-mail: b-atmaja@aist.go.jp, a-sasou@aist.go.jp": "evaluation was conducted on English data only."
        },
        {
          "E-mail: b-atmaja@aist.go.jp, a-sasou@aist.go.jp": ""
        },
        {
          "E-mail: b-atmaja@aist.go.jp, a-sasou@aist.go.jp": "Recently,\nthe multilingual approach to tackle the limitation"
        },
        {
          "E-mail: b-atmaja@aist.go.jp, a-sasou@aist.go.jp": ""
        },
        {
          "E-mail: b-atmaja@aist.go.jp, a-sasou@aist.go.jp": "of\nspeech technology has been proposed in the parent\ntasks"
        },
        {
          "E-mail: b-atmaja@aist.go.jp, a-sasou@aist.go.jp": ""
        },
        {
          "E-mail: b-atmaja@aist.go.jp, a-sasou@aist.go.jp": "of\nspeech processing,\nautomatic\nspeech recognition,\nspeech"
        },
        {
          "E-mail: b-atmaja@aist.go.jp, a-sasou@aist.go.jp": "synthesis, and speech translation. Models like Whisper [3] and"
        },
        {
          "E-mail: b-atmaja@aist.go.jp, a-sasou@aist.go.jp": ""
        },
        {
          "E-mail: b-atmaja@aist.go.jp, a-sasou@aist.go.jp": "MMS [4] can precisely recognize speech in multiple languages."
        },
        {
          "E-mail: b-atmaja@aist.go.jp, a-sasou@aist.go.jp": ""
        },
        {
          "E-mail: b-atmaja@aist.go.jp, a-sasou@aist.go.jp": "Stirred by this direction, research on speech emotion recogni-"
        },
        {
          "E-mail: b-atmaja@aist.go.jp, a-sasou@aist.go.jp": ""
        },
        {
          "E-mail: b-atmaja@aist.go.jp, a-sasou@aist.go.jp": "tion has been directed to tackle its limitation under multilingual"
        },
        {
          "E-mail: b-atmaja@aist.go.jp, a-sasou@aist.go.jp": ""
        },
        {
          "E-mail: b-atmaja@aist.go.jp, a-sasou@aist.go.jp": "settings. Assuming that emotion is a universal\nlanguage (i.e.,"
        },
        {
          "E-mail: b-atmaja@aist.go.jp, a-sasou@aist.go.jp": "the pattern of angry speech is\nthe same across\nlanguages or"
        },
        {
          "E-mail: b-atmaja@aist.go.jp, a-sasou@aist.go.jp": ""
        },
        {
          "E-mail: b-atmaja@aist.go.jp, a-sasou@aist.go.jp": "cultures),\nthe multilingual approach is expected to work also"
        },
        {
          "E-mail: b-atmaja@aist.go.jp, a-sasou@aist.go.jp": ""
        },
        {
          "E-mail: b-atmaja@aist.go.jp, a-sasou@aist.go.jp": "on speech-emotion recognition with diverse language samples."
        },
        {
          "E-mail: b-atmaja@aist.go.jp, a-sasou@aist.go.jp": ""
        },
        {
          "E-mail: b-atmaja@aist.go.jp, a-sasou@aist.go.jp": "Multilingual SER attempts to recognize emotion regardless"
        },
        {
          "E-mail: b-atmaja@aist.go.jp, a-sasou@aist.go.jp": "of\nthe language. One can build a single model\n(including a"
        },
        {
          "E-mail: b-atmaja@aist.go.jp, a-sasou@aist.go.jp": "model\ntrained on multilingual data)\nfor\neach language with"
        },
        {
          "E-mail: b-atmaja@aist.go.jp, a-sasou@aist.go.jp": ""
        },
        {
          "E-mail: b-atmaja@aist.go.jp, a-sasou@aist.go.jp": "language\ndetection\nas\npre-processing;\nanother\ncan\nbuild\na"
        },
        {
          "E-mail: b-atmaja@aist.go.jp, a-sasou@aist.go.jp": "single model for all languages. This study is aimed at the latter"
        },
        {
          "E-mail: b-atmaja@aist.go.jp, a-sasou@aist.go.jp": "approach. The previous\nstudies utilized transfer\nlearning for"
        },
        {
          "E-mail: b-atmaja@aist.go.jp, a-sasou@aist.go.jp": "cross-lingual SER [5], distribution adaption [6], and implicit"
        },
        {
          "E-mail: b-atmaja@aist.go.jp, a-sasou@aist.go.jp": "distribution alignment [7]. The last two studies were developed"
        },
        {
          "E-mail: b-atmaja@aist.go.jp, a-sasou@aist.go.jp": "for cross-corpus SER with different\nlanguages."
        },
        {
          "E-mail: b-atmaja@aist.go.jp, a-sasou@aist.go.jp": "The\nuse\nof multiple models\nfor\nSER has\nshown\nto\nbe"
        },
        {
          "E-mail: b-atmaja@aist.go.jp, a-sasou@aist.go.jp": "effective\nfor monolingual data. The\ntechnique of\nensemble"
        },
        {
          "E-mail: b-atmaja@aist.go.jp, a-sasou@aist.go.jp": "was\nusually\nadopted\nto\nfuse\nseveral models\nfrom different"
        },
        {
          "E-mail: b-atmaja@aist.go.jp, a-sasou@aist.go.jp": "modalities and/or features. Examples are acoustic and linguis-"
        },
        {
          "E-mail: b-atmaja@aist.go.jp, a-sasou@aist.go.jp": "tic for dimensional SER [8], [9], acoustic and facial expression"
        },
        {
          "E-mail: b-atmaja@aist.go.jp, a-sasou@aist.go.jp": "[10], and multi-feature fusion [11]. In the previous study [12],"
        },
        {
          "E-mail: b-atmaja@aist.go.jp, a-sasou@aist.go.jp": "ensemble\nlearning was\nalso utilized to tackle\nthe\nlimitation"
        },
        {
          "E-mail: b-atmaja@aist.go.jp, a-sasou@aist.go.jp": "of skewed training data in monolingual SER by maximizing,"
        },
        {
          "E-mail: b-atmaja@aist.go.jp, a-sasou@aist.go.jp": "averaging, and log averaging of the output probabilities of the"
        },
        {
          "E-mail: b-atmaja@aist.go.jp, a-sasou@aist.go.jp": "component classiﬁers."
        },
        {
          "E-mail: b-atmaja@aist.go.jp, a-sasou@aist.go.jp": "This paper contributes to the evaluation of ensemble learn-"
        },
        {
          "E-mail: b-atmaja@aist.go.jp, a-sasou@aist.go.jp": "ing,\ncombining several models via\nlate\nfusion methods\nfor"
        },
        {
          "E-mail: b-atmaja@aist.go.jp, a-sasou@aist.go.jp": "predicting emotion share from speech in multilingual settings."
        },
        {
          "E-mail: b-atmaja@aist.go.jp, a-sasou@aist.go.jp": "The fusion was performed by averaging predictions of different"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE I": "SUMMARY OF THE DATASET FOR EMOTION SHARE RECOGNITION,"
        },
        {
          "TABLE I": "DERIVED FROM [13]"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "Train\nDev"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "30,133\n12,241"
        },
        {
          "TABLE I": "58.1\n23.6"
        },
        {
          "TABLE I": "600\n202"
        },
        {
          "TABLE I": "379 : 221\n117 : 85"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "62.9 : 37.1\n57.9 : 42.1"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "TABLE II"
        },
        {
          "TABLE I": "PRE-TRAINED MODELS USED IN THIS STUDY"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "model. The performance is measured using the Spearman rank": ""
        },
        {
          "model. The performance is measured using the Spearman rank": "correlation coefﬁcient"
        },
        {
          "model. The performance is measured using the Spearman rank": "with ranking values."
        },
        {
          "model. The performance is measured using the Spearman rank": ""
        },
        {
          "model. The performance is measured using the Spearman rank": ""
        },
        {
          "model. The performance is measured using the Spearman rank": ""
        },
        {
          "model. The performance is measured using the Spearman rank": ""
        },
        {
          "model. The performance is measured using the Spearman rank": ""
        },
        {
          "model. The performance is measured using the Spearman rank": ""
        },
        {
          "model. The performance is measured using the Spearman rank": "for The ACM Multimedia 2023 Computational Paralinguistic"
        },
        {
          "model. The performance is measured using the Spearman rank": ""
        },
        {
          "model. The performance is measured using the Spearman rank": "Challenge (ComParE) Emotion Request Sub-Challange [13],"
        },
        {
          "model. The performance is measured using the Spearman rank": ""
        },
        {
          "model. The performance is measured using the Spearman rank": "which is\nprovided by Hume AI"
        },
        {
          "model. The performance is measured using the Spearman rank": ""
        },
        {
          "model. The performance is measured using the Spearman rank": "gorical emotions"
        },
        {
          "model. The performance is measured using the Spearman rank": ""
        },
        {
          "model. The performance is measured using the Spearman rank": "ness’,\n‘Concentration’,"
        },
        {
          "model. The performance is measured using the Spearman rank": ""
        },
        {
          "model. The performance is measured using the Spearman rank": "est’,\n‘Sadness’,\nand"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "gorical emotions\nin the dataset:\n‘Anger’,\n‘Boredom’,\n‘Calm-": "",
          "XLSR-1B EN\njonatasgrosman/wav2vec2-xls-r-1b-english": "XLSR-1B SP\njonatasgrosman/wav2vec2-xls-r-1b-spanish"
        },
        {
          "gorical emotions\nin the dataset:\n‘Anger’,\n‘Boredom’,\n‘Calm-": "ness’,\n‘Concentration’,\n‘Determination’,\n‘Excitement’,\n‘Inter-",
          "XLSR-1B EN\njonatasgrosman/wav2vec2-xls-r-1b-english": ""
        },
        {
          "gorical emotions\nin the dataset:\n‘Anger’,\n‘Boredom’,\n‘Calm-": "",
          "XLSR-1B EN\njonatasgrosman/wav2vec2-xls-r-1b-english": "XLSR-2B\nfacebook/wav2vec2-xls-r-2b"
        },
        {
          "gorical emotions\nin the dataset:\n‘Anger’,\n‘Boredom’,\n‘Calm-": "est’,\n‘Sadness’,\nand\n‘Tiredness’. The\nchoice\nof\nthese\nnine",
          "XLSR-1B EN\njonatasgrosman/wav2vec2-xls-r-1b-english": ""
        },
        {
          "gorical emotions\nin the dataset:\n‘Anger’,\n‘Boredom’,\n‘Calm-": "emotions is based on their more balanced distribution in the",
          "XLSR-1B EN\njonatasgrosman/wav2vec2-xls-r-1b-english": ""
        },
        {
          "gorical emotions\nin the dataset:\n‘Anger’,\n‘Boredom’,\n‘Calm-": "valence-arousal space. The basis for\nthe dataset\nitself\nis more",
          "XLSR-1B EN\njonatasgrosman/wav2vec2-xls-r-1b-english": ""
        },
        {
          "gorical emotions\nin the dataset:\n‘Anger’,\n‘Boredom’,\n‘Calm-": "",
          "XLSR-1B EN\njonatasgrosman/wav2vec2-xls-r-1b-english": "is allocated for\nthe test\nset while the rest\nis\nfor\ntraining and"
        },
        {
          "gorical emotions\nin the dataset:\n‘Anger’,\n‘Boredom’,\n‘Calm-": "than 5,000 ‘seed’ samples.",
          "XLSR-1B EN\njonatasgrosman/wav2vec2-xls-r-1b-english": ""
        },
        {
          "gorical emotions\nin the dataset:\n‘Anger’,\n‘Boredom’,\n‘Calm-": "",
          "XLSR-1B EN\njonatasgrosman/wav2vec2-xls-r-1b-english": "development."
        },
        {
          "gorical emotions\nin the dataset:\n‘Anger’,\n‘Boredom’,\n‘Calm-": "Seeds consist of various emotional expressions, which were",
          "XLSR-1B EN\njonatasgrosman/wav2vec2-xls-r-1b-english": ""
        },
        {
          "gorical emotions\nin the dataset:\n‘Anger’,\n‘Boredom’,\n‘Calm-": "gathered\nfrom openly\navailable\ndatasets,\nincluding MELD",
          "XLSR-1B EN\njonatasgrosman/wav2vec2-xls-r-1b-english": "III. PRE-TRAINED MODELS"
        },
        {
          "gorical emotions\nin the dataset:\n‘Anger’,\n‘Boredom’,\n‘Calm-": "[15]\nand VENEC [16]. The\nseed\nsamples were mimicked",
          "XLSR-1B EN\njonatasgrosman/wav2vec2-xls-r-1b-english": ""
        },
        {
          "gorical emotions\nin the dataset:\n‘Anger’,\n‘Boredom’,\n‘Calm-": "",
          "XLSR-1B EN\njonatasgrosman/wav2vec2-xls-r-1b-english": "We evaluated nine pre-trained models as acoustic embedding"
        },
        {
          "gorical emotions\nin the dataset:\n‘Anger’,\n‘Boredom’,\n‘Calm-": "by speakers\nrecruited via Amazon Mechanical Turk by the",
          "XLSR-1B EN\njonatasgrosman/wav2vec2-xls-r-1b-english": ""
        },
        {
          "gorical emotions\nin the dataset:\n‘Anger’,\n‘Boredom’,\n‘Calm-": "",
          "XLSR-1B EN\njonatasgrosman/wav2vec2-xls-r-1b-english": "feature\nextractors and fused the\nresults of\nthese models\nfor"
        },
        {
          "gorical emotions\nin the dataset:\n‘Anger’,\n‘Boredom’,\n‘Calm-": "provider of\nthe dataset\n[14]. The dataset\nconsists of 51,881",
          "XLSR-1B EN\njonatasgrosman/wav2vec2-xls-r-1b-english": ""
        },
        {
          "gorical emotions\nin the dataset:\n‘Anger’,\n‘Boredom’,\n‘Calm-": "",
          "XLSR-1B EN\njonatasgrosman/wav2vec2-xls-r-1b-english": "ensemble learning. Ensemble learning leverages the advantages"
        },
        {
          "gorical emotions\nin the dataset:\n‘Anger’,\n‘Boredom’,\n‘Calm-": "‘mimic’\nsamples\n(total of\n41:48:55 h of\ndata, mean 2.9 s,",
          "XLSR-1B EN\njonatasgrosman/wav2vec2-xls-r-1b-english": ""
        },
        {
          "gorical emotions\nin the dataset:\n‘Anger’,\n‘Boredom’,\n‘Calm-": "",
          "XLSR-1B EN\njonatasgrosman/wav2vec2-xls-r-1b-english": "of multiple models\nto\nachieve\na\nbetter\nperformance\nscore."
        },
        {
          "gorical emotions\nin the dataset:\n‘Anger’,\n‘Boredom’,\n‘Calm-": "range\n1.2\n-\n7.98\ns)\nfrom 1,004\nspeakers\naged\nfrom 20\nto",
          "XLSR-1B EN\njonatasgrosman/wav2vec2-xls-r-1b-english": ""
        },
        {
          "gorical emotions\nin the dataset:\n‘Anger’,\n‘Boredom’,\n‘Calm-": "",
          "XLSR-1B EN\njonatasgrosman/wav2vec2-xls-r-1b-english": "In this\nstudy, we\nevaluated a\nsingle\nlate\nfusion method by"
        },
        {
          "gorical emotions\nin the dataset:\n‘Anger’,\n‘Boredom’,\n‘Calm-": "66 years old. The data were collected from 3 countries with",
          "XLSR-1B EN\njonatasgrosman/wav2vec2-xls-r-1b-english": ""
        },
        {
          "gorical emotions\nin the dataset:\n‘Anger’,\n‘Boredom’,\n‘Calm-": "",
          "XLSR-1B EN\njonatasgrosman/wav2vec2-xls-r-1b-english": "averaging the emotion share predictions of different models."
        },
        {
          "gorical emotions\nin the dataset:\n‘Anger’,\n‘Boredom’,\n‘Calm-": "broadly\ndiffering\ncultures:\nthe United\nStates\n(English/EN),",
          "XLSR-1B EN\njonatasgrosman/wav2vec2-xls-r-1b-english": ""
        },
        {
          "gorical emotions\nin the dataset:\n‘Anger’,\n‘Boredom’,\n‘Calm-": "",
          "XLSR-1B EN\njonatasgrosman/wav2vec2-xls-r-1b-english": "The baseline model\nis\nthe robust version of on wav2vec 2.0"
        },
        {
          "gorical emotions\nin the dataset:\n‘Anger’,\n‘Boredom’,\n‘Calm-": "South Africa (English/EN), and Venezuela (Spanish/SP). Each",
          "XLSR-1B EN\njonatasgrosman/wav2vec2-xls-r-1b-english": ""
        },
        {
          "gorical emotions\nin the dataset:\n‘Anger’,\n‘Boredom’,\n‘Calm-": "",
          "XLSR-1B EN\njonatasgrosman/wav2vec2-xls-r-1b-english": "[17] ﬁnetuned the affective dataset\n[18]. For multilingual pre-"
        },
        {
          "gorical emotions\nin the dataset:\n‘Anger’,\n‘Boredom’,\n‘Calm-": "seed sample was rated by the individual who imitated it using a",
          "XLSR-1B EN\njonatasgrosman/wav2vec2-xls-r-1b-english": ""
        },
        {
          "gorical emotions\nin the dataset:\n‘Anger’,\n‘Boredom’,\n‘Calm-": "",
          "XLSR-1B EN\njonatasgrosman/wav2vec2-xls-r-1b-english": "trained models, we employed XLS-R 53 [19] and its variants,"
        },
        {
          "gorical emotions\nin the dataset:\n‘Anger’,\n‘Boredom’,\n‘Calm-": "‘select-all-that-apply’ method. Seeds were assigned a mean of",
          "XLSR-1B EN\njonatasgrosman/wav2vec2-xls-r-1b-english": ""
        },
        {
          "gorical emotions\nin the dataset:\n‘Anger’,\n‘Boredom’,\n‘Calm-": "",
          "XLSR-1B EN\njonatasgrosman/wav2vec2-xls-r-1b-english": "XLSR variants\n[20]\n(XLSR-300M, XLSR-1B,\nand XLSR-"
        },
        {
          "gorical emotions\nin the dataset:\n‘Anger’,\n‘Boredom’,\n‘Calm-": "2.03 emotions per rater (max: 7.11, min: 1.00), with a standard",
          "XLSR-1B EN\njonatasgrosman/wav2vec2-xls-r-1b-english": ""
        },
        {
          "gorical emotions\nin the dataset:\n‘Anger’,\n‘Boredom’,\n‘Calm-": "",
          "XLSR-1B EN\njonatasgrosman/wav2vec2-xls-r-1b-english": "2B). For\nthe XLS-R 53\nand XLSR-1B, we\nalso\nevaluated"
        },
        {
          "gorical emotions\nin the dataset:\n‘Anger’,\n‘Boredom’,\n‘Calm-": "deviation of 1.33 emotions. The proportion of\ntimes a given",
          "XLSR-1B EN\njonatasgrosman/wav2vec2-xls-r-1b-english": ""
        },
        {
          "gorical emotions\nin the dataset:\n‘Anger’,\n‘Boredom’,\n‘Calm-": "",
          "XLSR-1B EN\njonatasgrosman/wav2vec2-xls-r-1b-english": "the ﬁnetuned version of\nthis model on the English (EN) and"
        },
        {
          "gorical emotions\nin the dataset:\n‘Anger’,\n‘Boredom’,\n‘Calm-": "seed sample was\nrated with each emotion was\nthen applied",
          "XLSR-1B EN\njonatasgrosman/wav2vec2-xls-r-1b-english": ""
        },
        {
          "gorical emotions\nin the dataset:\n‘Anger’,\n‘Boredom’,\n‘Calm-": "",
          "XLSR-1B EN\njonatasgrosman/wav2vec2-xls-r-1b-english": "Spanish (SP) datasets. Finetuning for\nthese languages\nis not"
        },
        {
          "gorical emotions\nin the dataset:\n‘Anger’,\n‘Boredom’,\n‘Calm-": "to all mimics of\nthat seed sample. This method results in the",
          "XLSR-1B EN\njonatasgrosman/wav2vec2-xls-r-1b-english": ""
        },
        {
          "gorical emotions\nin the dataset:\n‘Anger’,\n‘Boredom’,\n‘Calm-": "",
          "XLSR-1B EN\njonatasgrosman/wav2vec2-xls-r-1b-english": "available on other XLSR variants. The complete pre-trained"
        },
        {
          "gorical emotions\nin the dataset:\n‘Anger’,\n‘Boredom’,\n‘Calm-": "share per emotion assigned by the speakers. The label\nthen is",
          "XLSR-1B EN\njonatasgrosman/wav2vec2-xls-r-1b-english": ""
        },
        {
          "gorical emotions\nin the dataset:\n‘Anger’,\n‘Boredom’,\n‘Calm-": "",
          "XLSR-1B EN\njonatasgrosman/wav2vec2-xls-r-1b-english": "models are listed in Table II."
        },
        {
          "gorical emotions\nin the dataset:\n‘Anger’,\n‘Boredom’,\n‘Calm-": "normalized by dividing all\n(ﬂoating point)\nscores with their",
          "XLSR-1B EN\njonatasgrosman/wav2vec2-xls-r-1b-english": ""
        },
        {
          "gorical emotions\nin the dataset:\n‘Anger’,\n‘Boredom’,\n‘Calm-": "",
          "XLSR-1B EN\njonatasgrosman/wav2vec2-xls-r-1b-english": "IV. CLASSIFIER AND HYPERPARAMETER SEARCH SPACE"
        },
        {
          "gorical emotions\nin the dataset:\n‘Anger’,\n‘Boredom’,\n‘Calm-": "highest score per sample; hence, each sample has a maximum",
          "XLSR-1B EN\njonatasgrosman/wav2vec2-xls-r-1b-english": ""
        },
        {
          "gorical emotions\nin the dataset:\n‘Anger’,\n‘Boredom’,\n‘Calm-": "score of 1.0 for one emotion category and other scores less than",
          "XLSR-1B EN\njonatasgrosman/wav2vec2-xls-r-1b-english": "We\nemployed a\nsupport vector machine\n(SVM)\nclassiﬁer"
        },
        {
          "gorical emotions\nin the dataset:\n‘Anger’,\n‘Boredom’,\n‘Calm-": "1.0 for other emotion categories. The baseline results for\nthis",
          "XLSR-1B EN\njonatasgrosman/wav2vec2-xls-r-1b-english": "for\nregression (SVR,\nsupport vector\nregression). The type of"
        },
        {
          "gorical emotions\nin the dataset:\n‘Anger’,\n‘Boredom’,\n‘Calm-": "dataset\nis a Spearman score of 0.500 for\nthe development set",
          "XLSR-1B EN\njonatasgrosman/wav2vec2-xls-r-1b-english": "kernel for SVR is linear (LinearSVR in the scikit-learn toolkit)."
        },
        {
          "gorical emotions\nin the dataset:\n‘Anger’,\n‘Boredom’,\n‘Calm-": "and 0.514 for the test set (using wav2vec ﬁnetuned of affective",
          "XLSR-1B EN\njonatasgrosman/wav2vec2-xls-r-1b-english": "The\noptimal\nparameters\nfor\nthis SVR were\nsearched\nusing"
        },
        {
          "gorical emotions\nin the dataset:\n‘Anger’,\n‘Boredom’,\n‘Calm-": "speech dataset with SVM method). The labels of\nthe test set",
          "XLSR-1B EN\njonatasgrosman/wav2vec2-xls-r-1b-english": "the Grid Search algorithm,\ni.e.,\nthe\nregularization parameter"
        },
        {
          "gorical emotions\nin the dataset:\n‘Anger’,\n‘Boredom’,\n‘Calm-": "are not open to the public; hence, only performance on the",
          "XLSR-1B EN\njonatasgrosman/wav2vec2-xls-r-1b-english": "C and\nthe\nalgorithm to\nsolve\neither\nthe\n’dual’\nor\n’primal’"
        },
        {
          "gorical emotions\nin the dataset:\n‘Anger’,\n‘Boredom’,\n‘Calm-": "development/validation data can be calculated directly.",
          "XLSR-1B EN\njonatasgrosman/wav2vec2-xls-r-1b-english": "optimization problem. Other parameters like scoring were also"
        },
        {
          "gorical emotions\nin the dataset:\n‘Anger’,\n‘Boredom’,\n‘Calm-": "Table I shows the characteristics of the dataset. The ratio for",
          "XLSR-1B EN\njonatasgrosman/wav2vec2-xls-r-1b-english": "searched. The scoring is either using negative mean absolute"
        },
        {
          "gorical emotions\nin the dataset:\n‘Anger’,\n‘Boredom’,\n‘Calm-": "training/test split\nfollows the practical machine/deep learning",
          "XLSR-1B EN\njonatasgrosman/wav2vec2-xls-r-1b-english": "error\n(NMAE) or negative mean squared error\n(NMSE). The"
        },
        {
          "gorical emotions\nin the dataset:\n‘Anger’,\n‘Boredom’,\n‘Calm-": "practice,\ni.e., 80/20. About 18.3% of the samples are allocated",
          "XLSR-1B EN\njonatasgrosman/wav2vec2-xls-r-1b-english": "maximum iteration is ﬁxed at 5000. The value range for these"
        },
        {
          "gorical emotions\nin the dataset:\n‘Anger’,\n‘Boredom’,\n‘Calm-": "for\nthe\ntest\n(without\nlabels), while\nthe\nrest\nare\nfor\ntraining",
          "XLSR-1B EN\njonatasgrosman/wav2vec2-xls-r-1b-english": "parameters is shown in Table III."
        },
        {
          "gorical emotions\nin the dataset:\n‘Anger’,\n‘Boredom’,\n‘Calm-": "(including validation/development). The ratio of\nfemale/male",
          "XLSR-1B EN\njonatasgrosman/wav2vec2-xls-r-1b-english": "For the fusion of all nine models, we employed the average"
        },
        {
          "gorical emotions\nin the dataset:\n‘Anger’,\n‘Boredom’,\n‘Calm-": "is about 63.5/36.5, with the number of\nfemale samples being",
          "XLSR-1B EN\njonatasgrosman/wav2vec2-xls-r-1b-english": "of the predicted values (arithmetic mean). The predicted values"
        },
        {
          "gorical emotions\nin the dataset:\n‘Anger’,\n‘Boredom’,\n‘Calm-": "higher than male. The splitting of training/test follows speaker-",
          "XLSR-1B EN\njonatasgrosman/wav2vec2-xls-r-1b-english": "are the continuous values in either development or test for each"
        },
        {
          "gorical emotions\nin the dataset:\n‘Anger’,\n‘Boredom’,\n‘Calm-": "independent criteria with a total of 1004 speakers; 20% of them",
          "XLSR-1B EN\njonatasgrosman/wav2vec2-xls-r-1b-english": "of\nthe nine emotion categories. The ordinal values are\nthen"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE III": "",
          "TABLE IV": ""
        },
        {
          "TABLE III": "",
          "TABLE IV": "OF THE EVALUATED PRE-TRAINED MODELS"
        },
        {
          "TABLE III": "Value range",
          "TABLE IV": ""
        },
        {
          "TABLE III": "[NMAE, NMSE]",
          "TABLE IV": "SSL model"
        },
        {
          "TABLE III": "",
          "TABLE IV": "wav2vec 2.0"
        },
        {
          "TABLE III": "[10−2 − 10−6]",
          "TABLE IV": "XLS-R 53"
        },
        {
          "TABLE III": "[True, False]",
          "TABLE IV": "XLS-R 53 EN"
        },
        {
          "TABLE III": "[50000]",
          "TABLE IV": "XLS-R 53 SP"
        },
        {
          "TABLE III": "",
          "TABLE IV": "XLSR-300M"
        },
        {
          "TABLE III": "",
          "TABLE IV": "XLSR-1B"
        },
        {
          "TABLE III": "",
          "TABLE IV": "XLSR-1B EN"
        },
        {
          "TABLE III": "",
          "TABLE IV": "XLSR-1B SP"
        },
        {
          "TABLE III": "",
          "TABLE IV": "XLSR-2B"
        },
        {
          "TABLE III": "",
          "TABLE IV": ""
        },
        {
          "TABLE III": "",
          "TABLE IV": "Fusion all\n(of 9)"
        },
        {
          "TABLE III": "",
          "TABLE IV": ""
        },
        {
          "TABLE III": "",
          "TABLE IV": ""
        },
        {
          "TABLE III": "",
          "TABLE IV": "TABLE V"
        },
        {
          "TABLE III": "",
          "TABLE IV": ""
        },
        {
          "TABLE III": "",
          "TABLE IV": ""
        },
        {
          "TABLE III": "",
          "TABLE IV": ""
        },
        {
          "TABLE III": "",
          "TABLE IV": "ROWS) AND THE PROPOSED FUSION METHOD (THE LAST ROW)"
        },
        {
          "TABLE III": "(based on rank)",
          "TABLE IV": ""
        },
        {
          "TABLE III": "",
          "TABLE IV": ""
        },
        {
          "TABLE III": "",
          "TABLE IV": "Model"
        },
        {
          "TABLE III": "",
          "TABLE IV": "wav2vec2 [18]"
        },
        {
          "TABLE III": "",
          "TABLE IV": "auDeep [22]"
        },
        {
          "TABLE III": "",
          "TABLE IV": "DeepSpectrum [23]"
        },
        {
          "TABLE III": "ρ =",
          "TABLE IV": "ComParE [24],\n[25]"
        },
        {
          "TABLE III": "σR(X)σR(Y )",
          "TABLE IV": "Late Fusion (of 4)"
        },
        {
          "TABLE III": "",
          "TABLE IV": "Fusion all\n(of 9)"
        },
        {
          "TABLE III": "",
          "TABLE IV": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "σR(X)σR(Y )": "",
          "Late Fusion (of 4)\n[13]\n0.470\n0.476": "Fusion all\n(of 9)\n0.524\n0.537"
        },
        {
          "σR(X)σR(Y )": "where cov(R(X), R(Y )) denotes the covariance of the ranked",
          "Late Fusion (of 4)\n[13]\n0.470\n0.476": ""
        },
        {
          "σR(X)σR(Y )": "data R(X) and R(Y ), and σR(X) and σR(Y ) denote the standard",
          "Late Fusion (of 4)\n[13]\n0.470\n0.476": ""
        },
        {
          "σR(X)σR(Y )": "deviation of\nrespectively.\nthe ranked data R(X) and R(Y ),",
          "Late Fusion (of 4)\n[13]\n0.470\n0.476": ""
        },
        {
          "σR(X)σR(Y )": "",
          "Late Fusion (of 4)\n[13]\n0.470\n0.476": "This result\nis also higher than the previous best result of 0.500"
        },
        {
          "σR(X)σR(Y )": "A repository is created to ensure the reproducibility of\nthe",
          "Late Fusion (of 4)\n[13]\n0.470\n0.476": ""
        },
        {
          "σR(X)σR(Y )": "",
          "Late Fusion (of 4)\n[13]\n0.470\n0.476": "from wav2vec2 [18]. We assume ensemble learning works by"
        },
        {
          "σR(X)σR(Y )": "experiments 1. The repository contains the experiment settings,",
          "Late Fusion (of 4)\n[13]\n0.470\n0.476": ""
        },
        {
          "σR(X)σR(Y )": "",
          "Late Fusion (of 4)\n[13]\n0.470\n0.476": "leveraging information across different\nlanguages and pooling"
        },
        {
          "σR(X)σR(Y )": "including the hyperparameter search space, and the results of",
          "Late Fusion (of 4)\n[13]\n0.470\n0.476": ""
        },
        {
          "σR(X)σR(Y )": "",
          "Late Fusion (of 4)\n[13]\n0.470\n0.476": "maximum information for multilingual SER."
        },
        {
          "σR(X)σR(Y )": "the experiments. The requirements to run the experiments are",
          "Late Fusion (of 4)\n[13]\n0.470\n0.476": ""
        },
        {
          "σR(X)σR(Y )": "",
          "Late Fusion (of 4)\n[13]\n0.470\n0.476": "Similar\ntrends between test\nand development\nsets are ob-"
        },
        {
          "σR(X)σR(Y )": "also provided in the repository with the exact version of\nthe",
          "Late Fusion (of 4)\n[13]\n0.470\n0.476": ""
        },
        {
          "σR(X)σR(Y )": "",
          "Late Fusion (of 4)\n[13]\n0.470\n0.476": "served where the performance of the test set\nis slightly higher"
        },
        {
          "σR(X)σR(Y )": "libraries used at\nthe time of\nthe experiment.",
          "Late Fusion (of 4)\n[13]\n0.470\n0.476": ""
        },
        {
          "σR(X)σR(Y )": "",
          "Late Fusion (of 4)\n[13]\n0.470\n0.476": "than the development\nset. An exception only applies\nto the"
        },
        {
          "σR(X)σR(Y )": "V. EXPERIMENT RESULTS AND DISCUSSIONS",
          "Late Fusion (of 4)\n[13]\n0.470\n0.476": "DeepSpectrum result where the test\nresult\nis 0.004 lower than"
        },
        {
          "σR(X)σR(Y )": "",
          "Late Fusion (of 4)\n[13]\n0.470\n0.476": "the development\nset. This\ntrend indicates\nthe generalization"
        },
        {
          "σR(X)σR(Y )": "We divided our results into validation or development (Table",
          "Late Fusion (of 4)\n[13]\n0.470\n0.476": ""
        },
        {
          "σR(X)σR(Y )": "",
          "Late Fusion (of 4)\n[13]\n0.470\n0.476": "of\nthe evaluated method (including the baseline methods that"
        },
        {
          "σR(X)σR(Y )": "IV) and test\nresults\n(Table V).\nIn the development stage, we",
          "Late Fusion (of 4)\n[13]\n0.470\n0.476": ""
        },
        {
          "σR(X)σR(Y )": "",
          "Late Fusion (of 4)\n[13]\n0.470\n0.476": "use a similar SVM classiﬁer)\nto unseen data and shows that,"
        },
        {
          "σR(X)σR(Y )": "ensure that the result of ensemble learning (Fusion all\nin Table",
          "Late Fusion (of 4)\n[13]\n0.470\n0.476": ""
        },
        {
          "σR(X)σR(Y )": "",
          "Late Fusion (of 4)\n[13]\n0.470\n0.476": "perhaps,\nthe\ndistribution\nof\nthe\ntest\nset\nis\nsimilar\nto\nthe"
        },
        {
          "σR(X)σR(Y )": "IV) is higher than baseline results, either from a single model",
          "Late Fusion (of 4)\n[13]\n0.470\n0.476": ""
        },
        {
          "σR(X)σR(Y )": "",
          "Late Fusion (of 4)\n[13]\n0.470\n0.476": "development set."
        },
        {
          "σR(X)σR(Y )": "or\nlate fusion. We submitted our prediction of\nthe test\nresult",
          "Late Fusion (of 4)\n[13]\n0.470\n0.476": ""
        },
        {
          "σR(X)σR(Y )": "to the provider of\nthe dataset\n[13]\nto obtain the performance",
          "Late Fusion (of 4)\n[13]\n0.470\n0.476": "Finally, Table VI breaks down the\naverage Spearman’s ρ"
        },
        {
          "σR(X)σR(Y )": "on the test set.",
          "Late Fusion (of 4)\n[13]\n0.470\n0.476": "in the test\nset\ninto each emotion category. The\n(ρ = 0.537)"
        },
        {
          "σR(X)σR(Y )": "Table IV shows our\nresult\nin the development stage using",
          "Late Fusion (of 4)\n[13]\n0.470\n0.476": "result\nis\nfrom ‘Fusion all’ of nine pre-trained models on the"
        },
        {
          "σR(X)σR(Y )": "nine pre-trained models and a fusion of all\nthese models. On",
          "Late Fusion (of 4)\n[13]\n0.470\n0.476": "test\nset. The\nresult\nshows\nthat\nthe model performs best\nin"
        },
        {
          "σR(X)σR(Y )": "a single model evaluation,\nit\nis\nshown that using pre-trained",
          "Late Fusion (of 4)\n[13]\n0.470\n0.476": "the\n’Calmness’\nemotion category (0.6061) and worst\nin the"
        },
        {
          "σR(X)σR(Y )": "models (such as XLS-R 53 and XLSR variants) achieves higher",
          "Late Fusion (of 4)\n[13]\n0.470\n0.476": "‘Interest’ emotion category (0.4238). The result\nis\nsimilar\nto"
        },
        {
          "σR(X)σR(Y )": "accuracies\nthan conventional acoustic\nfeature\nextractors\nlike",
          "Late Fusion (of 4)\n[13]\n0.470\n0.476": "the previous study [13] where the ‘Calmness’ emotion category"
        },
        {
          "σR(X)σR(Y )": "auDeep and ComParE. For\ninstance, XLS-R 53 (the smallest",
          "Late Fusion (of 4)\n[13]\n0.470\n0.476": "is\nthe highest\n(0.559), and the\n‘Anger’\nemotion category is"
        },
        {
          "σR(X)σR(Y )": "multilingual model) gained a Spearman score of 0.4328, while",
          "Late Fusion (of 4)\n[13]\n0.470\n0.476": "the lowest\n(0.428). The previous study’s\nresult was achieved"
        },
        {
          "σR(X)σR(Y )": "ComParE achieved 0.359. For the fusion models, our fusion of",
          "Late Fusion (of 4)\n[13]\n0.470\n0.476": "with wav2vec 2.0 but with a different setup from this\nstudy"
        },
        {
          "σR(X)σR(Y )": "all nine pre-trained models also overcomes the previous late",
          "Late Fusion (of 4)\n[13]\n0.470\n0.476": "(particularly on the scoring method)."
        },
        {
          "σR(X)σR(Y )": "fusion of wav2vec2, auDeep, DeepSpectrum, and ComParE.",
          "Late Fusion (of 4)\n[13]\n0.470\n0.476": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Ragolta, M. Pateraki, H. Coppock,\nI. Kiskin, M. Sinka, and S. Roberts,": ""
        },
        {
          "Ragolta, M. Pateraki, H. Coppock,\nI. Kiskin, M. Sinka, and S. Roberts,": "“The ACM Multimedia 2023 Computational Paralinguistics Challenge:"
        },
        {
          "Ragolta, M. Pateraki, H. Coppock,\nI. Kiskin, M. Sinka, and S. Roberts,": ""
        },
        {
          "Ragolta, M. Pateraki, H. Coppock,\nI. Kiskin, M. Sinka, and S. Roberts,": "Emotion Share and Requests,” in Proc. 30th ACM Int. Conf. Multimed."
        },
        {
          "Ragolta, M. Pateraki, H. Coppock,\nI. Kiskin, M. Sinka, and S. Roberts,": "New York, NY, USA: ACM,\noct\n2023,\npp.\n7120–7124.\n[Online]."
        },
        {
          "Ragolta, M. Pateraki, H. Coppock,\nI. Kiskin, M. Sinka, and S. Roberts,": "Available: https://dl.acm.org/doi/10.1145/3503161.3551591"
        },
        {
          "Ragolta, M. Pateraki, H. Coppock,\nI. Kiskin, M. Sinka, and S. Roberts,": "S.\nCowen,\nP.\nLaukka,\nH.\nA.\nElfenbein,\nR.\nLiu,\nand"
        },
        {
          "Ragolta, M. Pateraki, H. Coppock,\nI. Kiskin, M. Sinka, and S. Roberts,": "D.\nKeltner,\n“The\nprimacy\nof\ncategories\nin\nthe\nrecognition\nof"
        },
        {
          "Ragolta, M. Pateraki, H. Coppock,\nI. Kiskin, M. Sinka, and S. Roberts,": ""
        },
        {
          "Ragolta, M. Pateraki, H. Coppock,\nI. Kiskin, M. Sinka, and S. Roberts,": "Nat. Hum.\n12\nemotions\nin\nspeech\nprosody\nacross\ntwo\ncultures,”"
        },
        {
          "Ragolta, M. Pateraki, H. Coppock,\nI. Kiskin, M. Sinka, and S. Roberts,": "Behav.,\nvol.\n3,\nno.\n4,\npp.\n369–382,\napr\n2019.\n[Online]. Available:"
        },
        {
          "Ragolta, M. Pateraki, H. Coppock,\nI. Kiskin, M. Sinka, and S. Roberts,": "http://www.nature.com/articles/s41562-019-0533-6"
        },
        {
          "Ragolta, M. Pateraki, H. Coppock,\nI. Kiskin, M. Sinka, and S. Roberts,": "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and R. Mi-"
        },
        {
          "Ragolta, M. Pateraki, H. Coppock,\nI. Kiskin, M. Sinka, and S. Roberts,": ""
        },
        {
          "Ragolta, M. Pateraki, H. Coppock,\nI. Kiskin, M. Sinka, and S. Roberts,": "halcea, “MELD: A Multimodal Multi-Party Dataset for Emotion Recog-"
        },
        {
          "Ragolta, M. Pateraki, H. Coppock,\nI. Kiskin, M. Sinka, and S. Roberts,": "57th Annu. Meet. Assoc. Comput.\nnition\nin Conversations,”\nin Proc."
        },
        {
          "Ragolta, M. Pateraki, H. Coppock,\nI. Kiskin, M. Sinka, and S. Roberts,": "Linguist., 2019, pp. 527–536."
        },
        {
          "Ragolta, M. Pateraki, H. Coppock,\nI. Kiskin, M. Sinka, and S. Roberts,": "P. Laukka, H. A. Elfenbein, W. Chui, N. S. Thingujam, F. K.\nIraki,"
        },
        {
          "Ragolta, M. Pateraki, H. Coppock,\nI. Kiskin, M. Sinka, and S. Roberts,": ""
        },
        {
          "Ragolta, M. Pateraki, H. Coppock,\nI. Kiskin, M. Sinka, and S. Roberts,": "T. Rockstuhl, and J. Althoff, “Presenting the VENEC Corpus: Develop-"
        },
        {
          "Ragolta, M. Pateraki, H. Coppock,\nI. Kiskin, M. Sinka, and S. Roberts,": "ment of a Cross-Cultural Corpus of Vocal Emotion Expressions and a"
        },
        {
          "Ragolta, M. Pateraki, H. Coppock,\nI. Kiskin, M. Sinka, and S. Roberts,": "Novel Method of Annotating Emotion Appraisals,” Lr. 2010 - Seventh"
        },
        {
          "Ragolta, M. Pateraki, H. Coppock,\nI. Kiskin, M. Sinka, and S. Roberts,": "Int. Conf. Lang. Resour. Eval., vol. 7, pp. 53–57, 2010."
        },
        {
          "Ragolta, M. Pateraki, H. Coppock,\nI. Kiskin, M. Sinka, and S. Roberts,": "and M. Auli,\n“wav2vec\n2.0: A"
        },
        {
          "Ragolta, M. Pateraki, H. Coppock,\nI. Kiskin, M. Sinka, and S. Roberts,": "framework for self-supervised learning of speech representations,” Adv."
        },
        {
          "Ragolta, M. Pateraki, H. Coppock,\nI. Kiskin, M. Sinka, and S. Roberts,": "Neural\nInf. Process. Syst., 2020."
        },
        {
          "Ragolta, M. Pateraki, H. Coppock,\nI. Kiskin, M. Sinka, and S. Roberts,": "J. Wagner, A. Triantafyllopoulos, H. Wierstorf, M. Schmitt, F. Burkhardt,"
        },
        {
          "Ragolta, M. Pateraki, H. Coppock,\nI. Kiskin, M. Sinka, and S. Roberts,": "F. Eyben, and B. W. Schuller, “Dawn of the Transformer Era in Speech"
        },
        {
          "Ragolta, M. Pateraki, H. Coppock,\nI. Kiskin, M. Sinka, and S. Roberts,": ""
        },
        {
          "Ragolta, M. Pateraki, H. Coppock,\nI. Kiskin, M. Sinka, and S. Roberts,": "Emotion Recognition: Closing the Valence Gap,” IEEE Trans. Pattern"
        },
        {
          "Ragolta, M. Pateraki, H. Coppock,\nI. Kiskin, M. Sinka, and S. Roberts,": "Anal. Mach.\nIntell., pp. 1–13, mar 2023."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE VI": ""
        },
        {
          "TABLE VI": "SPEARMAN CORRELATION COEFFICIENT (ρ) ON EACH EMOTION"
        },
        {
          "TABLE VI": ""
        },
        {
          "TABLE VI": "CATEGORIES FOR THE TEST SET"
        },
        {
          "TABLE VI": ""
        },
        {
          "TABLE VI": ""
        },
        {
          "TABLE VI": "Emotion\nSpearman’s ρ"
        },
        {
          "TABLE VI": ""
        },
        {
          "TABLE VI": "Anger\n0.4894"
        },
        {
          "TABLE VI": ""
        },
        {
          "TABLE VI": "Boredom\n0.6032"
        },
        {
          "TABLE VI": ""
        },
        {
          "TABLE VI": "Calmness\n0.6061"
        },
        {
          "TABLE VI": ""
        },
        {
          "TABLE VI": "Concentration\n0.5855"
        },
        {
          "TABLE VI": ""
        },
        {
          "TABLE VI": "Determination\n0.5589"
        },
        {
          "TABLE VI": ""
        },
        {
          "TABLE VI": "Excitement\n0.4669"
        },
        {
          "TABLE VI": ""
        },
        {
          "TABLE VI": "Interest\n0.4238"
        },
        {
          "TABLE VI": ""
        },
        {
          "TABLE VI": "Sadness\n0.5123"
        },
        {
          "TABLE VI": ""
        },
        {
          "TABLE VI": "Tiredness\n0.5867"
        },
        {
          "TABLE VI": ""
        },
        {
          "TABLE VI": ""
        },
        {
          "TABLE VI": ""
        },
        {
          "TABLE VI": "of nine pre-trained models defeats the previously reported best"
        },
        {
          "TABLE VI": ""
        },
        {
          "TABLE VI": "result of\na\nsingle model\n(in which the performance of\nthis"
        },
        {
          "TABLE VI": ""
        },
        {
          "TABLE VI": "single model\nis higher\nthan the late fusion of\nfour methods)."
        },
        {
          "TABLE VI": "Second,\nthe results of any single model\nin this study generally"
        },
        {
          "TABLE VI": ""
        },
        {
          "TABLE VI": "are also higher than the single models/methods in the previous"
        },
        {
          "TABLE VI": ""
        },
        {
          "TABLE VI": "study. Third,\nthere\nis\na\ntrend in which the performance of"
        },
        {
          "TABLE VI": "the test set\nis slightly higher than the development set. Future"
        },
        {
          "TABLE VI": ""
        },
        {
          "TABLE VI": "studies could be directed to explore more about the dataset and"
        },
        {
          "TABLE VI": ""
        },
        {
          "TABLE VI": "improve the performance using more advanced methods with"
        },
        {
          "TABLE VI": "more recent speech embeddings."
        },
        {
          "TABLE VI": ""
        },
        {
          "TABLE VI": "ACKNOWLEDGMENT"
        },
        {
          "TABLE VI": ""
        },
        {
          "TABLE VI": "This paper is partly based on results obtained from a project,"
        },
        {
          "TABLE VI": "JPNP20006, commissioned by the New Energy and Industrial"
        },
        {
          "TABLE VI": ""
        },
        {
          "TABLE VI": "Technology Development Organization (NEDO), Japan."
        },
        {
          "TABLE VI": ""
        },
        {
          "TABLE VI": ""
        },
        {
          "TABLE VI": "REFERENCES"
        },
        {
          "TABLE VI": ""
        },
        {
          "TABLE VI": "[1] A. S. Cowen, H. A. Elfenbein, P. Laukka, and D. Keltner, “Mapping 24"
        },
        {
          "TABLE VI": "emotions conveyed by brief human vocalization.” Am. Psychol., vol. 74,"
        },
        {
          "TABLE VI": "no. 6, pp. 698–712, sep 2019."
        },
        {
          "TABLE VI": "[2]\nS. Buechel\nand U. Hahn, “Emotion analysis\nas a regression problem-"
        },
        {
          "TABLE VI": "dimensional models\nand their\nimplications\non Emotion representation"
        },
        {
          "TABLE VI": "and metrical evaluation,” Front. Artif.\nIntell. Appl., vol. 285, pp. 1114–"
        },
        {
          "TABLE VI": "1122, 2016."
        },
        {
          "TABLE VI": "[3] A. Radford, J. Wook, K. Tao, X. Greg, B. Christine, and M. Ilya, “Robust"
        },
        {
          "TABLE VI": "Speech Recognition\nvia Large-Scale Weak Supervision,”\nopenai.com,"
        },
        {
          "TABLE VI": "2021."
        },
        {
          "TABLE VI": "[4] V. Pratap, M. Auli, and M. Ai, “Scaling Speech Technology to 1 , 000"
        },
        {
          "TABLE VI": "+ Languages.”"
        },
        {
          "TABLE VI": "[5]\nS. G. Upadhyay, L. Martinez-Lucas, B.-h. Su, W.-c. Lin, W.-s. Chien,"
        },
        {
          "TABLE VI": "Y\n.-t. Wu, W. Katz, C. Busso, and C.-c. Lee, “Phonetic Anchor-Based"
        },
        {
          "TABLE VI": "Transfer\nLearning\nto\nFacilitate Unsupervised\nCross-Lingual\nSpeech"
        },
        {
          "TABLE VI": "Emotion Recognition,” in ICASSP 2023 - 2023 IEEE Int. Conf. Acoust."
        },
        {
          "TABLE VI": "Speech Signal Process.\nIEEE,\njun 2023, pp. 1–5.\n[Online]. Available:"
        },
        {
          "TABLE VI": "https://ieeexplore.ieee.org/document/10095250/"
        },
        {
          "TABLE VI": "[6]\nS.\nLi,\nP.\nSong,\nL.\nJi,\nY\n.\nJin,\nand W.\nZheng,\n“A\nGeneralized"
        },
        {
          "TABLE VI": "Subspace Distribution Adaptation Framework for Cross-Corpus Speech"
        },
        {
          "TABLE VI": "Emotion Recognition,” in ICASSP 2023 - 2023 IEEE Int. Conf. Acoust."
        },
        {
          "TABLE VI": "Speech Signal Process.\nIEEE,\njun 2023, pp. 1–5.\n[Online]. Available:"
        },
        {
          "TABLE VI": "https://ieeexplore.ieee.org/document/10097258/"
        },
        {
          "TABLE VI": "[7] Y. Zhao, J. Wang, Y. Zong, W. Zheng, H. Lian, and L. Zhao, “DEEP IM-"
        },
        {
          "TABLE VI": "PLICIT DISTRIBUTION ALIGNMENT NETWORKS FOR CROSS-"
        },
        {
          "TABLE VI": "CORPUS SPEECH EMOTION RECOGNITION,” ICASSP 2023 - 2023"
        },
        {
          "TABLE VI": "IEEE Int. Conf. Acoust. Speech Signal Process."
        },
        {
          "TABLE VI": "[8] B. T. Atmaja, Y. Hamada,\nand M. Akagi,\n“Predicting Valence\nand"
        },
        {
          "TABLE VI": "Arousal by Aggregating Acoustic Features\nfor Acoustic-Linguistic\nIn-"
        },
        {
          "TABLE VI": "formation Fusion,” in 2020 IEEE Reg. 10 Conf.\nIEEE, nov 2020, pp."
        },
        {
          "TABLE VI": "1081–1085."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JPNP20006, commissioned by the New Energy and Industrial": "",
          "[18]\nJ. Wagner, A. Triantafyllopoulos, H. Wierstorf, M. Schmitt, F. Burkhardt,": "F. Eyben, and B. W. Schuller, “Dawn of the Transformer Era in Speech"
        },
        {
          "JPNP20006, commissioned by the New Energy and Industrial": "Technology Development Organization (NEDO), Japan.",
          "[18]\nJ. Wagner, A. Triantafyllopoulos, H. Wierstorf, M. Schmitt, F. Burkhardt,": ""
        },
        {
          "JPNP20006, commissioned by the New Energy and Industrial": "",
          "[18]\nJ. Wagner, A. Triantafyllopoulos, H. Wierstorf, M. Schmitt, F. Burkhardt,": "Emotion Recognition: Closing the Valence Gap,” IEEE Trans. Pattern"
        },
        {
          "JPNP20006, commissioned by the New Energy and Industrial": "",
          "[18]\nJ. Wagner, A. Triantafyllopoulos, H. Wierstorf, M. Schmitt, F. Burkhardt,": "Anal. Mach.\nIntell., pp. 1–13, mar 2023."
        },
        {
          "JPNP20006, commissioned by the New Energy and Industrial": "REFERENCES",
          "[18]\nJ. Wagner, A. Triantafyllopoulos, H. Wierstorf, M. Schmitt, F. Burkhardt,": ""
        },
        {
          "JPNP20006, commissioned by the New Energy and Industrial": "",
          "[18]\nJ. Wagner, A. Triantafyllopoulos, H. Wierstorf, M. Schmitt, F. Burkhardt,": "[19] X. Cai, Z. Wu, K. Zhong, B. Su, D. Dai, and H. Meng, “Unsupervised"
        },
        {
          "JPNP20006, commissioned by the New Energy and Industrial": "[1] A. S. Cowen, H. A. Elfenbein, P. Laukka, and D. Keltner, “Mapping 24",
          "[18]\nJ. Wagner, A. Triantafyllopoulos, H. Wierstorf, M. Schmitt, F. Burkhardt,": "Cross-Lingual Speech Emotion Recognition Using Domain Adversarial"
        },
        {
          "JPNP20006, commissioned by the New Energy and Industrial": "emotions conveyed by brief human vocalization.” Am. Psychol., vol. 74,",
          "[18]\nJ. Wagner, A. Triantafyllopoulos, H. Wierstorf, M. Schmitt, F. Burkhardt,": "Neural Network,” in 2021 12th Int. Symp. Chinese Spok. Lang. Process."
        },
        {
          "JPNP20006, commissioned by the New Energy and Industrial": "no. 6, pp. 698–712, sep 2019.",
          "[18]\nJ. Wagner, A. Triantafyllopoulos, H. Wierstorf, M. Schmitt, F. Burkhardt,": "ISCSLP 2021, 2021, pp. 3–7."
        },
        {
          "JPNP20006, commissioned by the New Energy and Industrial": "[2]\nS. Buechel\nand U. Hahn, “Emotion analysis\nas a regression problem-",
          "[18]\nJ. Wagner, A. Triantafyllopoulos, H. Wierstorf, M. Schmitt, F. Burkhardt,": "[20] A.\nBabu,\nC. Wang, A.\nTjandra, K.\nLakhotia, Q. Xu, N. Goyal,"
        },
        {
          "JPNP20006, commissioned by the New Energy and Industrial": "dimensional models\nand their\nimplications\non Emotion representation",
          "[18]\nJ. Wagner, A. Triantafyllopoulos, H. Wierstorf, M. Schmitt, F. Burkhardt,": "K. Singh, P. von Platen, Y. Saraf,\nJ. Pino, A. Baevski, A. Conneau,"
        },
        {
          "JPNP20006, commissioned by the New Energy and Industrial": "and metrical evaluation,” Front. Artif.\nIntell. Appl., vol. 285, pp. 1114–",
          "[18]\nJ. Wagner, A. Triantafyllopoulos, H. Wierstorf, M. Schmitt, F. Burkhardt,": "and\nM.\nAuli,\n“XLS-R:\nSelf-supervised\nCross-lingual\nSpeech"
        },
        {
          "JPNP20006, commissioned by the New Energy and Industrial": "1122, 2016.",
          "[18]\nJ. Wagner, A. Triantafyllopoulos, H. Wierstorf, M. Schmitt, F. Burkhardt,": "Interspeech\nRepresentation\nLearning\nat\nScale,”\nin\n2022,\nvol.\n2022-"
        },
        {
          "JPNP20006, commissioned by the New Energy and Industrial": "[3] A. Radford, J. Wook, K. Tao, X. Greg, B. Christine, and M. Ilya, “Robust",
          "[18]\nJ. Wagner, A. Triantafyllopoulos, H. Wierstorf, M. Schmitt, F. Burkhardt,": "Septe.\nISCA:\nISCA,\nsep 2022,\npp. 2278–2282.\n[Online]. Available:"
        },
        {
          "JPNP20006, commissioned by the New Energy and Industrial": "Speech Recognition\nvia Large-Scale Weak Supervision,”\nopenai.com,",
          "[18]\nJ. Wagner, A. Triantafyllopoulos, H. Wierstorf, M. Schmitt, F. Burkhardt,": "}2022/babu22{\n}interspeech.html\nhttps://www.isca-speech.org/archive/interspeech{"
        },
        {
          "JPNP20006, commissioned by the New Energy and Industrial": "2021.",
          "[18]\nJ. Wagner, A. Triantafyllopoulos, H. Wierstorf, M. Schmitt, F. Burkhardt,": "[21] C. Spearman,\n“The Proof\nand Measurement\nof Association\nbetween"
        },
        {
          "JPNP20006, commissioned by the New Energy and Industrial": "[4] V. Pratap, M. Auli, and M. Ai, “Scaling Speech Technology to 1 , 000",
          "[18]\nJ. Wagner, A. Triantafyllopoulos, H. Wierstorf, M. Schmitt, F. Burkhardt,": "Two Things,” Am. J. Psychol., vol. 100, no. 3/4, p. 441, 1987.\n[Online]."
        },
        {
          "JPNP20006, commissioned by the New Energy and Industrial": "+ Languages.”",
          "[18]\nJ. Wagner, A. Triantafyllopoulos, H. Wierstorf, M. Schmitt, F. Burkhardt,": "Available: https://www.jstor.org/stable/1422689?origin=crossref"
        },
        {
          "JPNP20006, commissioned by the New Energy and Industrial": "[5]\nS. G. Upadhyay, L. Martinez-Lucas, B.-h. Su, W.-c. Lin, W.-s. Chien,",
          "[18]\nJ. Wagner, A. Triantafyllopoulos, H. Wierstorf, M. Schmitt, F. Burkhardt,": "[22] M.\nFreitag,\nS.\nAmiriparian,\nS.\nPugachevskiy,\nN.\nCummins,\nand"
        },
        {
          "JPNP20006, commissioned by the New Energy and Industrial": "Y\n.-t. Wu, W. Katz, C. Busso, and C.-c. Lee, “Phonetic Anchor-Based",
          "[18]\nJ. Wagner, A. Triantafyllopoulos, H. Wierstorf, M. Schmitt, F. Burkhardt,": "B. Schuller,\n“auDeep: Unsupervised\nlearning\nof\nrepresentations\nfrom"
        },
        {
          "JPNP20006, commissioned by the New Energy and Industrial": "Transfer\nLearning\nto\nFacilitate Unsupervised\nCross-Lingual\nSpeech",
          "[18]\nJ. Wagner, A. Triantafyllopoulos, H. Wierstorf, M. Schmitt, F. Burkhardt,": "audio with deep recurrent neural networks,” J. Mach. Learn. Res., vol. 18,"
        },
        {
          "JPNP20006, commissioned by the New Energy and Industrial": "Emotion Recognition,” in ICASSP 2023 - 2023 IEEE Int. Conf. Acoust.",
          "[18]\nJ. Wagner, A. Triantafyllopoulos, H. Wierstorf, M. Schmitt, F. Burkhardt,": "pp. 1–5, 2018."
        },
        {
          "JPNP20006, commissioned by the New Energy and Industrial": "Speech Signal Process.\nIEEE,\njun 2023, pp. 1–5.\n[Online]. Available:",
          "[18]\nJ. Wagner, A. Triantafyllopoulos, H. Wierstorf, M. Schmitt, F. Burkhardt,": "[23]\nS. Amiriparian, M. Gerczuk, S. Ottl, N. Cummins, M. Freitag, S. Pu-"
        },
        {
          "JPNP20006, commissioned by the New Energy and Industrial": "https://ieeexplore.ieee.org/document/10095250/",
          "[18]\nJ. Wagner, A. Triantafyllopoulos, H. Wierstorf, M. Schmitt, F. Burkhardt,": "gachevskiy, A. Baird, and B. Schuller, “Snore Sound Classiﬁcation Using"
        },
        {
          "JPNP20006, commissioned by the New Energy and Industrial": "[6]\nS.\nLi,\nP.\nSong,\nL.\nJi,\nY\n.\nJin,\nand W.\nZheng,\n“A\nGeneralized",
          "[18]\nJ. Wagner, A. Triantafyllopoulos, H. Wierstorf, M. Schmitt, F. Burkhardt,": "Image-Based Deep Spectrum Features,”\nin Interspeech 2017.\nISCA:"
        },
        {
          "JPNP20006, commissioned by the New Energy and Industrial": "Subspace Distribution Adaptation Framework for Cross-Corpus Speech",
          "[18]\nJ. Wagner, A. Triantafyllopoulos, H. Wierstorf, M. Schmitt, F. Burkhardt,": "ISCA, aug 2017, pp. 3512–3516."
        },
        {
          "JPNP20006, commissioned by the New Energy and Industrial": "Emotion Recognition,” in ICASSP 2023 - 2023 IEEE Int. Conf. Acoust.",
          "[18]\nJ. Wagner, A. Triantafyllopoulos, H. Wierstorf, M. Schmitt, F. Burkhardt,": "[24]\nF. Eyben, F. Weninger, F. Gross, and B. Schuller, “Recent developments"
        },
        {
          "JPNP20006, commissioned by the New Energy and Industrial": "Speech Signal Process.\nIEEE,\njun 2023, pp. 1–5.\n[Online]. Available:",
          "[18]\nJ. Wagner, A. Triantafyllopoulos, H. Wierstorf, M. Schmitt, F. Burkhardt,": "in openSMILE,\nthe munich open-source multimedia feature extractor,”"
        },
        {
          "JPNP20006, commissioned by the New Energy and Industrial": "https://ieeexplore.ieee.org/document/10097258/",
          "[18]\nJ. Wagner, A. Triantafyllopoulos, H. Wierstorf, M. Schmitt, F. Burkhardt,": "in Proc. 21st ACM Int. Conf. Multimed.\n- MM ’13.\nNew York, New"
        },
        {
          "JPNP20006, commissioned by the New Energy and Industrial": "[7] Y. Zhao, J. Wang, Y. Zong, W. Zheng, H. Lian, and L. Zhao, “DEEP IM-",
          "[18]\nJ. Wagner, A. Triantafyllopoulos, H. Wierstorf, M. Schmitt, F. Burkhardt,": "York, USA: ACM Press, 2013, pp. 835–838."
        },
        {
          "JPNP20006, commissioned by the New Energy and Industrial": "PLICIT DISTRIBUTION ALIGNMENT NETWORKS FOR CROSS-",
          "[18]\nJ. Wagner, A. Triantafyllopoulos, H. Wierstorf, M. Schmitt, F. Burkhardt,": "[25] B. Schuller, S. Steidl, A. Batliner, A. Vinciarelli, K. Scherer, F. Ringeval,"
        },
        {
          "JPNP20006, commissioned by the New Energy and Industrial": "CORPUS SPEECH EMOTION RECOGNITION,” ICASSP 2023 - 2023",
          "[18]\nJ. Wagner, A. Triantafyllopoulos, H. Wierstorf, M. Schmitt, F. Burkhardt,": "M.\nChetouani,\nF. Weninger,\nF.\nEyben,\nE. Marchi, M. Mortillaro,"
        },
        {
          "JPNP20006, commissioned by the New Energy and Industrial": "IEEE Int. Conf. Acoust. Speech Signal Process.",
          "[18]\nJ. Wagner, A. Triantafyllopoulos, H. Wierstorf, M. Schmitt, F. Burkhardt,": "H. Salamin, A. Polychroniou, F. Valente,\nand S. Kim,\n“The\nINTER-"
        },
        {
          "JPNP20006, commissioned by the New Energy and Industrial": "[8] B. T. Atmaja, Y. Hamada,\nand M. Akagi,\n“Predicting Valence\nand",
          "[18]\nJ. Wagner, A. Triantafyllopoulos, H. Wierstorf, M. Schmitt, F. Burkhardt,": "SPEECH 2013 computational paralinguistics\nchallenge:\nsocial\nsignals,"
        },
        {
          "JPNP20006, commissioned by the New Energy and Industrial": "Arousal by Aggregating Acoustic Features\nfor Acoustic-Linguistic\nIn-",
          "[18]\nJ. Wagner, A. Triantafyllopoulos, H. Wierstorf, M. Schmitt, F. Burkhardt,": "conﬂict,\nemotion,\nautism,”\nin Interspeech\n2013, no. August.\nISCA:"
        },
        {
          "JPNP20006, commissioned by the New Energy and Industrial": "formation Fusion,” in 2020 IEEE Reg. 10 Conf.\nIEEE, nov 2020, pp.",
          "[18]\nJ. Wagner, A. Triantafyllopoulos, H. Wierstorf, M. Schmitt, F. Burkhardt,": "ISCA, aug 2013, pp. 148–152."
        },
        {
          "JPNP20006, commissioned by the New Energy and Industrial": "1081–1085.",
          "[18]\nJ. Wagner, A. Triantafyllopoulos, H. Wierstorf, M. Schmitt, F. Burkhardt,": ""
        },
        {
          "JPNP20006, commissioned by the New Energy and Industrial": "[9] B. T. Atmaja and M. Akagi, “Improving Valence Prediction in Dimen-",
          "[18]\nJ. Wagner, A. Triantafyllopoulos, H. Wierstorf, M. Schmitt, F. Burkhardt,": ""
        },
        {
          "JPNP20006, commissioned by the New Energy and Industrial": "sional Speech Emotion Recognition Using Linguistic\nInformation,”\nin",
          "[18]\nJ. Wagner, A. Triantafyllopoulos, H. Wierstorf, M. Schmitt, F. Burkhardt,": ""
        },
        {
          "JPNP20006, commissioned by the New Energy and Industrial": "Proc. 2020 23rd Conf. Orient. COCOSDA Int. Comm. Co-ord. Stand.",
          "[18]\nJ. Wagner, A. Triantafyllopoulos, H. Wierstorf, M. Schmitt, F. Burkhardt,": ""
        },
        {
          "JPNP20006, commissioned by the New Energy and Industrial": "Speech Databases Assess. Tech. O-COCOSDA 2020.\nIEEE, nov 2020,",
          "[18]\nJ. Wagner, A. Triantafyllopoulos, H. Wierstorf, M. Schmitt, F. Burkhardt,": ""
        },
        {
          "JPNP20006, commissioned by the New Energy and Industrial": "pp. 166–171.",
          "[18]\nJ. Wagner, A. Triantafyllopoulos, H. Wierstorf, M. Schmitt, F. Burkhardt,": ""
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Mapping 24 emotions conveyed by brief human vocalization",
      "authors": [
        "A Cowen",
        "H Elfenbein",
        "P Laukka",
        "D Keltner"
      ],
      "year": "2019",
      "venue": "Am. Psychol"
    },
    {
      "citation_id": "2",
      "title": "Emotion analysis as a regression problemdimensional models and their implications on Emotion representation and metrical evaluation",
      "authors": [
        "S Buechel",
        "U Hahn"
      ],
      "year": "2016",
      "venue": "Front. Artif. Intell. Appl"
    },
    {
      "citation_id": "3",
      "title": "Robust Speech Recognition via Large-Scale Weak Supervision",
      "authors": [
        "A Radford",
        "J Wook",
        "K Tao",
        "X Greg",
        "B Christine",
        "M Ilya"
      ],
      "venue": "Robust Speech Recognition via Large-Scale Weak Supervision"
    },
    {
      "citation_id": "4",
      "title": "Scaling Speech Technology to 1 , 000 + Languages",
      "authors": [
        "V Pratap",
        "M Auli",
        "M Ai"
      ],
      "venue": "Scaling Speech Technology to 1 , 000 + Languages"
    },
    {
      "citation_id": "5",
      "title": "Phonetic Anchor-Based Transfer Learning to Facilitate Unsupervised Cross-Lingual Speech Emotion Recognition",
      "authors": [
        "S Upadhyay",
        "L Martinez-Lucas",
        "B.-H Su",
        "W -C. Lin",
        "W -S. Chien",
        "Y -T. Wu",
        "W Katz",
        "C Busso",
        "C.-C Lee"
      ],
      "year": "2023",
      "venue": "ICASSP 2023 -2023 IEEE Int. Conf. Acoust. Speech Signal Process"
    },
    {
      "citation_id": "6",
      "title": "A Generalized Subspace Distribution Adaptation Framework for Cross-Corpus Speech Emotion Recognition",
      "authors": [
        "S Li",
        "P Song",
        "L Ji",
        "Y Jin",
        "W Zheng"
      ],
      "year": "2023",
      "venue": "ICASSP 2023 -2023 IEEE Int. Conf. Acoust. Speech Signal Process"
    },
    {
      "citation_id": "7",
      "title": "DEEP IM-PLICIT DISTRIBUTION ALIGNMENT NETWORKS FOR CROSS-CORPUS SPEECH EMOTION RECOGNITION",
      "authors": [
        "Y Zhao",
        "J Wang",
        "Y Zong",
        "W Zheng",
        "H Lian",
        "L Zhao"
      ],
      "venue": "ICASSP 2023 -2023 IEEE Int. Conf. Acoust. Speech Signal Process"
    },
    {
      "citation_id": "8",
      "title": "Predicting Valence and Arousal by Aggregating Acoustic Features for Acoustic-Linguistic Information Fusion",
      "authors": [
        "B Atmaja",
        "Y Hamada",
        "M Akagi"
      ],
      "year": "2020",
      "venue": "2020 IEEE Reg. 10 Conf. IEEE"
    },
    {
      "citation_id": "9",
      "title": "Improving Valence Prediction in Dimensional Speech Emotion Recognition Using Linguistic Information",
      "authors": [
        "B Atmaja",
        "M Akagi"
      ],
      "year": "2020",
      "venue": "Proc. 2020 23rd Conf. Orient. COCOSDA Int. Comm. Co-ord. Stand. Speech Databases Assess"
    },
    {
      "citation_id": "10",
      "title": "Multi-modal Multi-cultural Dimensional Continues Emotion Recognition in Dyadic Interactions",
      "authors": [
        "J Zhao",
        "S Chen"
      ],
      "year": "2018",
      "venue": "Sub-challenge AVEC"
    },
    {
      "citation_id": "11",
      "title": "The Effect of Silence Feature in Dimensional Speech Emotion Recognition",
      "authors": [
        "B Atmaja",
        "M Akagi"
      ],
      "year": "2020",
      "venue": "10th Int. Conf. Speech Prosody"
    },
    {
      "citation_id": "12",
      "title": "Speech Emotion Recognition With Ensemble Learning Methods",
      "authors": [
        "P.-Y Shih",
        "C.-P Chen",
        "C.-H Wu"
      ],
      "year": "2017",
      "venue": "IEEE Int. Conf. Acoust. Speech, Signal Process"
    },
    {
      "citation_id": "13",
      "title": "The ACM Multimedia 2023 Computational Paralinguistics Challenge: Emotion Share and Requests",
      "authors": [
        "B Schuller",
        "A Batliner",
        "S Amiriparian",
        "C Bergler",
        "M Gerczuk",
        "N Holz",
        "P Larrouy-Maestri",
        "S Bayerl",
        "K Riedhammer",
        "A Mallol-Ragolta",
        "M Pateraki",
        "H Coppock",
        "I Kiskin",
        "M Sinka",
        "S Roberts"
      ],
      "year": "2023",
      "venue": "Proc. 30th ACM Int. Conf. Multimed",
      "doi": "10.1145/3503161.3551591"
    },
    {
      "citation_id": "14",
      "title": "The primacy of categories in the recognition of 12 emotions in speech prosody across two cultures",
      "authors": [
        "A Cowen",
        "P Laukka",
        "H Elfenbein",
        "R Liu",
        "D Keltner"
      ],
      "year": "2019",
      "venue": "Nat. Hum. Behav"
    },
    {
      "citation_id": "15",
      "title": "MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "Proc. 57th Annu"
    },
    {
      "citation_id": "16",
      "title": "Presenting the VENEC Corpus: Development of a Cross-Cultural Corpus of Vocal Emotion Expressions and a Novel Method of Annotating Emotion Appraisals",
      "authors": [
        "P Laukka",
        "H Elfenbein",
        "W Chui",
        "N Thingujam",
        "F Iraki",
        "T Rockstuhl",
        "J Althoff"
      ],
      "year": "2010",
      "venue": "Lr. 2010 -Seventh Int. Conf. Lang. Resour. Eval."
    },
    {
      "citation_id": "17",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "H Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Adv. Neural Inf. Process. Syst"
    },
    {
      "citation_id": "18",
      "title": "Dawn of the Transformer Era in Speech Emotion Recognition: Closing the Valence Gap",
      "authors": [
        "J Wagner",
        "A Triantafyllopoulos",
        "H Wierstorf",
        "M Schmitt",
        "F Burkhardt",
        "F Eyben",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell"
    },
    {
      "citation_id": "19",
      "title": "Unsupervised Cross-Lingual Speech Emotion Recognition Using Domain Adversarial Neural Network",
      "authors": [
        "X Cai",
        "Z Wu",
        "K Zhong",
        "B Su",
        "D Dai",
        "H Meng"
      ],
      "year": "2021",
      "venue": "2021 12th Int. Symp. Chinese Spok. Lang. Process. ISCSLP 2021"
    },
    {
      "citation_id": "20",
      "title": "XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale",
      "authors": [
        "A Babu",
        "C Wang",
        "A Tjandra",
        "K Lakhotia",
        "Q Xu",
        "N Goyal",
        "K Singh",
        "P Von Platen",
        "Y Saraf",
        "J Pino",
        "A Baevski",
        "A Conneau",
        "M Auli"
      ],
      "year": "2022",
      "venue": "Septe. ISCA: ISCA"
    },
    {
      "citation_id": "21",
      "title": "The Proof and Measurement of Association between Two Things",
      "authors": [
        "C Spearman"
      ],
      "year": "1987",
      "venue": "Am. J. Psychol"
    },
    {
      "citation_id": "22",
      "title": "auDeep: Unsupervised learning of representations from audio with deep recurrent neural networks",
      "authors": [
        "M Freitag",
        "S Amiriparian",
        "S Pugachevskiy",
        "N Cummins",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "J. Mach. Learn. Res"
    },
    {
      "citation_id": "23",
      "title": "Snore Sound Classification Using Image-Based Deep Spectrum Features",
      "authors": [
        "S Amiriparian",
        "M Gerczuk",
        "S Ottl",
        "N Cummins",
        "M Freitag",
        "S Pugachevskiy",
        "A Baird",
        "B Schuller"
      ],
      "year": "2017",
      "venue": "Interspeech 2017. ISCA: ISCA"
    },
    {
      "citation_id": "24",
      "title": "Recent developments in openSMILE, the munich open-source multimedia feature extractor",
      "authors": [
        "F Eyben",
        "F Weninger",
        "F Gross",
        "B Schuller"
      ],
      "year": "2013",
      "venue": "Proc. 21st ACM Int. Conf. Multimed. -MM '13"
    },
    {
      "citation_id": "25",
      "title": "The INTER-SPEECH 2013 computational paralinguistics challenge: social signals, conflict, emotion, autism",
      "authors": [
        "B Schuller",
        "S Steidl",
        "A Batliner",
        "A Vinciarelli",
        "K Scherer",
        "F Ringeval",
        "M Chetouani",
        "F Weninger",
        "F Eyben",
        "E Marchi",
        "M Mortillaro",
        "H Salamin",
        "A Polychroniou",
        "F Valente",
        "S Kim"
      ],
      "year": "2013",
      "venue": "Interspeech 2013, no. August. ISCA: ISCA"
    }
  ]
}