{
  "paper_id": "2307.10982v2",
  "title": "Masr: Multi-Label Aware Speech Representation",
  "published": "2023-07-20T16:09:57Z",
  "authors": [
    "Anjali Raj",
    "Shikhar Bharadwaj",
    "Sriram Ganapathy",
    "Min Ma",
    "Shikhar Vashishth"
  ],
  "keywords": [
    "speech representation learning",
    "supervision and self-supervision",
    "language identification"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In the recent years, speech representation learning is constructed primarily as a self-supervised learning (SSL) task, using the raw audio signal alone, while ignoring the side-information that is often available for a given speech recording. In this paper, we propose MASR, a Multi-label Aware Speech Representation learning framework, which addresses the aforementioned limitations. MASR enables the inclusion of multiple external knowledge sources to enhance the utilization of meta-data information. The external knowledge sources are incorporated in the form of sample-level pair-wise similarity matrices that are useful in a hard-mining loss. A key advantage of the MASR framework is that it can be combined with any choice of SSL method. Using MASR representations, we perform evaluations on several downstream tasks such as language identification, speech recognition and other non-semantic tasks such as speaker and emotion recognition. In these experiments, we illustrate significant performance improvements for the MASR over other established benchmarks. We perform a detailed analysis on the language identification task to provide insights on how the proposed loss function enables the representations to separate closely related languages.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "In recent years, representation learning of speech signals has seen a paradigm shift from knowledge-driven features such as melfrequency cepstral coefficients (MFCC)  [1]  to data driven learning approaches like wav2vec  [2] . Representation learning of audio signals, from the raw data, has closely mirrored the advances made in self-supervised learning (SSL) of other domains such as natural language processing (NLP)  [3]  and computer vision  [4] . The initial approaches for SSL of speech data explored the modeling of the continuous latent representations of speech. The popular examples include constrastive predictive coding (CPC)  [5] , wav2vec modeling  [2]  and auto-regressive predictive coding (APC)  [6] . In the subsequent modeling frameworks, a discretized representation of audio is found to be more beneficial than the continuous space  [7] . The recent success in SSL speech modeling relies on three major steps -i) converting speech to a discrete token sequence, ii) masking portions of the speech signal and, iii) predicting the token sequence corresponding to the masked parts of the audio. Examples of such approaches include wav2vec 2.0  [8] , hidden unit bidirectional encoder representation from transformers (HuBERT)  [9]  and best random quantizer (BestRQ)  [10] . The downstream tasks for these models were initially centered on low resource automatic speech recognition (ASR)  [11]  and multilingual ASR  [10] . However, these approaches have been extended to various other downstream tasks such as emotion recognition  [12] , speaker recognition  [13] , language identification  [14]  and other audio tasks through the SUPERB  [15]  and NOSS  [16]  challenges.\n\nTypically, speech data resources possess supplementary metadata alongside the audio signals, manifesting in various forms such as speaker, language, emotion, style, and transcripts. Often, during speech data collection or data mining from the web, additional information is recorded along the semantic and non-semantic aspects of the data. The current representation learning paradigms completely disregard these additional streams of information.\n\nThe prior works  [17, 18]  have explored the joint optimization of SSL loss function with speech recognition loss functions. Further, the previous work by Vashisth et al.  [19]  investigated an approach for combining SSL loss functions with a language label based triplet mining loss. Nevertheless, previous studies have not investigated the potential of employing multiple meta-labels for the purpose of representation learning. In this paper, we propose MASR, a Multi-label Aware Speech Representation learning framework, which attempts to address some of these limitations by leveraging the existence of multiple meta-information sources and external knowledge sources. Our contributions in this work can be summarized as follows.\n\n• We propose MASR, a novel framework for incorporating multiple meta-labels in any self-supervised learning (SSL) framework.\n\n• MASR enables inclusion of external knowledge sources to enhance the utilization of meta-data information.\n\n• Through extensive experiments on several downstream tasks, we demonstrate the effectiveness of our proposed approach.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Works",
      "text": "Speech Self-Supervised Learning: The initial works in speech representation learning used a series of convolutional and long-short term memory (LSTM) layers  [5]  with a contrastive loss that predicts the future frames of the audio signal  [6] . The first in the series of wav2vec models  [2]  started with similar contrastive prediction losses, while the subsequent version  [20]  also involved a discretization of the audio using a vector quantization module. The recent version in this series, wav2vec 2.0  [8]  also included transformer based encoding layers. The application of MLM loss with wav2vec model, called w2v-bert  [21] , combined the discretization of speech with the masked modeling. Another notable effort, titled HuBERT  [9] , performed iterative clustering/discretization and masked language modeling. A simplification of k-means clustering algorithm on the hidden layer representations, allowed the direct encoding of the input mel-spectrogram with a random quantizer  [10] . All these methods perform frame-level encoding with the benefits illustrated primarily for speech recognition tasks  [22]  or zero-resource language modeling task  [23] .  1 , z i 2 , ..., z i t . These are fed to a pooling function (g) to derive utterance-level embedding hi. For the jth metadata, the metadata label ϕ j i is encoded to real-valued vectors, e j i , while the utterance-embedding hi is transformed to q j i through a projection layer. The MASR loss is then computed on the transformed representations. combination of utterance level objective functions with frame-level MLM loss functions.\n\nSpeech Supervised Learning: In the direction of utterance level embedding extractors, the supervised embedding extraction using neural networks were explored for speaker recognition  [24]  and language recognition  [25] . The architectures explored for this supervised setting include time-delay neural network (TDNN)  [26] , residual networks  [27]  and TDNN with enhanced channel attention (ECAPA-TDNN)  [28] . In contrast to the supervised embedding extraction frameworks, this work explores the combination of selfsupervision and meta-data based hard-mining losses.\n\nNon-semantic Speech Representation: TRILL  [16]  leverages temporal proximity as a supervision signal for learning non-semantic representation. The TRILL and subsequent variants such as FRILL  [29]  and TRILLsson  [30]  have shown encouraging results on NOSS (non-semantic speech) tasks  [16] . The effort discussed in COLA  [31]  changes the negative sampling strategy for audio tasks. While these works are designed for contrastive learning, our work explores a combination of meta-data based weak supervision losses that can be coupled with any SSL approach.\n\nJoint Learning: For joint modeling of self-supervised and supervised objectives in speech processing, the combined loss of CTC (connectionist temporal classification) and CPC losses for ASR have been explored  [17] . The work referred to as UniSpeech  [32]  combined CTC loss with a phonetic loss, while JOIST (joint learning of supervised and unsupervised losses)  [18]  used the mixture of MLM and speech recognition loss functions. A recent work exploring the meta-data information of language, called label aware speech representation learning (LASR)  [19] , showed the benefits of combining meta-data of language labels with the SSL framework. However, these methods are restricted to a particular type of meta-information, while this work proposes a general framework which enables utilization of multiple streams of meta-data.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Proposed Approach",
      "text": "A detailed illustration of the complete framework is provided in Figure  1 . Our framework is a multi-label extension of the LASR  [19]  method, that enabled the inclusion of language meta-information during self-supervised learning. The LASR approach is not directly generalizable to the presence of multiple meta-information streams. Further, the LASR setting does not allow the incorporation of metainformation in the form of soft-labels. MASR addresses these limitations by introducing two additional modules in the LASR framework: (i) Metadata Encoder, which allows inclusion of external knowledge sources, and (ii) Projection Layer, which allows the utilization of multiple meta-information streams jointly.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Notations",
      "text": "We denote a speech pre-training dataset with meta-information as D = {(X 1, Φ1), (X 2, Φ2), ..., (X N , ΦN )} where X i denotes the waveform and Φi = {ϕ 1 i , ϕ 2 i , ..., ϕ M i } represents the metainformation associated with the i-th sample. A self-supervised speech-representation learning model, such as w2v-BERT  [2]  [33] or BEST-RQ  [10] , is defined as an encoder, f : X → Z, which transforms a waveform X to its frame-level representation Z = [z1, z2, ..., zT ]. We utilize an aggregation model g : Z → h to obtain an utterance level embedding. In this work, we define g as an average pooling operation, i.e., h = g(Z) = 1 T t zt.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Metadata Encoder (Me)",
      "text": "Most of the existing methods utilize metadata information directly in their raw form. In this work, we introduce a metadata encoder module that leverages external knowledge resources to generate a representation for each type of metadata. Specifically, each type of meta-data is mapped to a real-valued d-dimensional space. Let the mapping be π j : ϕ j → e j , where j denotes the metadata type, and e j denotes the encoding of metadata. Thus, for each metadata, the encoder module gives a metadata specific representation e j , which is utilized for computing the final objective function. In this work, metadata encoder π j is pre-defined and not learnable.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Projection Layer",
      "text": "In order to integrate multiple types of metadata information into MASR, we introduce a metadata specific projection applied to the utterance-level representation h. We note that using h directly, makes the overall speech encoder specific to a particular type of metadata. Moreover, it may lead to conflicting objectives when using multiple types of metadata. To overcome this, we introduce a metadata-specific transformation function, δ j : h → q j , where j denotes the metadata type, and q j ∈ R d denotes the projection. Finally, the metadata-specific aggregated representation p j is obtained by concatenating the metadata encoder and the projection layer outputs, i.e.,\n\nwhere αj is used to scale the importance of e j with respect to q j . Here, M denotes the number of meta-data streams. In this study, we employ δ j as a fully connected layer for each metadata.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Training Loss",
      "text": "In a batch of size B, for i th speech utterance and j th metadata type, we define a positive and negative sample set ρ + ij and ρ - ij such that,\n\n} We utilize the hard triplet mining loss  [34, 35] , where the farthest (cosine distance) positive sample (from ρ + ij ) and the nearest negative sample (from ρ - ij ) are used for computing the triplet loss, defined as,\n\nwhere,\n\nwith k + denoting the farthest positive sample (based on representations, p j ) and k -denotes the nearest negative sample. The final loss function is given by,\n\nHere, LSSL is the loss corresponding to the self-supervised speech encoding method f , and λj decides the trade-off between the SSL objective and hard-triplet objective for the j-th meta-data stream.\n\nIn our work, we have primarily explored meta-data of language labels encoded in various ways defined in URIEL  [36]  database. Further, we also perform ablation studies on NOSS evaluations using text transcripts and speaker's geographic location information.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experimental Setup",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Pre-Training Data",
      "text": "In our experimental setup, we utilize a substantial collection of publicly available speech data for pre-training, amounting to approximately 429k hours of audio. This encompasses various datasets, including 372k hours of speech data spanning 23 languages from the VoxPopuli  [37] , 50k hours of speech in 25 languages from the Common Voice  [38] , 50k hours of read speech in 8 European languages from the Multilingual LibriSpeech (MLS)  [39] , and 1k hours of telephone conversation data encompassing 17 African and Asian languages from the BABEL dataset  [40] . In total, our combined dataset comprises speech utterances from 75 different languages. The language metadata is available for the entire pre-training data.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Evaluation Data",
      "text": "Language Identification We report language identification performance on two datasets -FLEURS  [41]  and Dhwani  [42] . FLEURS is a n-way parallel speech dataset with uniform coverage of 102 languages. On average, each language contains about 12 hours of audio from multiple speakers. The dataset contains sentences sourced from Wikipedia  [43] . For fine-tuning the pre-trained models, we use the standard FLEURS train set and report performance on the test set. In order to understand the robustness of the model for language recognition, we also perform experiments using the Dhwani dataset  [42] . The Dhwani dataset contains raw audio data acquired from social media, video-sharing and news platforms like YouTube and News On Air bulletins. This corpus has around 12.6k hours of publicly downloadable speech data from 22 Indian languages.\n\nAutomatic Speech Recognition: For the ASR task, we have utilized the Multilingual LibriSpeech (MLS) corpus, which comprises of 50k hours of read speech from LibriVox corpus across 8 European languages. We use the standard train (full) for fine-tuning the models, and report the results on the test splits of MLS.\n\nNon-semantic Speech Benchmark: The third set of evaluation uses 6 tasks defined as part of the NOSS benchmark  [16] . This captures emotion recognition, speaker recognition, language identification, environment sound classification and masked speech classification. Unlike the LangID and ASR tasks defined above, the pre-trained models are not fine-tuned for the downstream tasks in the NOSS benchmark. A randomly initialized soft-max layer is alone trained for each downstream task in the NOSS suite.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Implementation Details",
      "text": "For most of our ablation experiments, we use the SSL approach of BEST-RQ  [10]  as the baseline choice. The baseline system is trained for 1.25M steps on the pre-trained data. The MASR models are initialized with the BEST-RQ model trained for 1M steps, followed by 250k steps of additional pre-training using the proposed objective functions. The hyper-parameter λ = 16 has been adopted for the language related metadata.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Language Meta-Data Encoder",
      "text": "The encoding of the language labels is obtained from the lang2vec models extracted from the URIEL  [36]  typological database. The dataset encompasses a range of metrics that capture the language features derived from the syntactic structures of the language, as well as features derived from the World Atlas of Language Structures  [44] . The set comprises of a vast number of languages, totaling 8070. In our case, we use the lang2vec related to the 75 pre-training languages. The distances between languages is the cosine similarity between the respective feature vectors. The downward arrow for a label indicates lower values are better. Refer to Section 5 for details.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Fleurs",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Language Identification Performance",
      "text": "The language identification results are reported in Table  1 . The baselines used for comparison include w2v-BERT  [33]  and BEST-RQ  [10] . The metrics include accuracy over the entire test set, along with the macro-F1 score and the EER (equal error rate). Using the MASR framework, the performance is shown to improve on both the datasets, i.e., Dhwani and FLEURS.\n\nTo summarize, following are the observations from the results:\n\n• The MASR approach improves the BEST-RQ model relatively by 11.2%, 11.9%, and 66.6% in terms of accuracy, F1 and EER metrics, respectively. The corresponding improvements for the MASR framework over the LASR model are by 2.6% , 2.3% and 40.0% in terms of accuracy, F1 and EER respectively. Similar improvement in performance is also observed for Dhwani dataset as well. The MASR framework improves the relative accuracy over the BEST-RQ baseline by 9.0%, F1 by 11.1% and EER by 8.3%.\n\n• This increase in performance is also prominent in the SSL baseline with w2v-BERT framework. In addition, the overlapped (languages that are common between pre-training and fine-tuning) and the Non-overlapped (languages that are different from those in the pre-training dataset) class accuracy also show a similar trend across the two datasets.\n\n• The improvement in Non-Overlap accuracy is observed to be more for MASR as compared to that in the Overlap accuracy, which reflects that the model is able to generalize the meta-data based objective functions to unseen languages.\n\n• From our analysis, we find that MASR influences the selection of negative samples by more than 80%, i.e., nearly 12 out of 16 samples in a batch undergo a change in their negative neighbour selection compared to the LASR training.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Evaluation On Non-Semantic Tasks",
      "text": "The evaluation on NOSS benchmark was carried out on 6 tasks which include the Voxforge, Vox-filtered, Mask Challenge, Iemocap, ESC-human, ESC-cough and ASVSpoof-2019. VoxForge is a language classification dataset that consists of user-submitted audio clips in six languages: English, Spanish, French, German, Russian, and Italian. ESC50-human is a benchmark dataset of 2k  For each representation-task pair, we explore different downstream models, representation aggregation techniques, and normalization methods. In Table  2 , we report results for various ablations of MASR as well as the LASR method  [19] . We observe that using LASR objective along with the SSL objective on BEST-RQ leads to a deterioration in performance on 3 out of 6 NOSS tasks. Adding the metadata encoder with the projection layer leads to an increase in performance over the BEST-RQ model for 4 out of 6 tasks. The description of the last two rows, GeoMASR and TextMASR, is given in Section 7.1. The addition of Geological metadata as loss computation head massively improves the MASR performance on Voxforge which is a language identification task. It is interesting to observe that incorporating extra head reflecting Indic geography is able to improve Language identification task for European languages. Overall, MASR variants see more improvements in the audio classification tasks.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Impact On Automatic Speech Recognition",
      "text": "We evaluate the baseline system and the MASR approach for the semantic ASR tasks defined on the MLS dataset. The word error rate (WER) results are reported in Table  3 . All models are finetuned for 10k steps. We observe that the MASR setting, even with the use of non-semantic language based meta-data, does not cause a degradation on the average WER. Table  2 : Non Semantic Speech Tasks accuracy (%) for various tasks. The table shows the average performance of the different embeddings on a number of downstream tasks with fixed conventional splits. We find that methods trained with the MASR objective achieves better performance in terms of accuracy in 3 out of 6 tasks when compared to BEST-RQ, and BEST-RQ + LASR. Refer to Section 5 for details.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Method",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Analysis Of Language Identification",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Choice Of Meta-Data Encoder",
      "text": "For MASR, we explored a variety of ways to encode the language meta-data using various Lang2vec representations  [36] . In particular, we experiment with syntactic, geographic, phonetic, featural, genetic and inventory based embeddings as defined by  Littell et al. (2017) . As shown in Figure  2 , MASR relies significantly on the choice of Lang2vec embeddings, with the syntactic encoding providing the best accuracy. Thus, we use the syntactic distance for all the other experiments in this paper.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Locale Level Improvements On Fleurs Dataset",
      "text": "Overall our single best langID model achieved 83.7% accuracy over 102 FLEURS locales, which is 11% better than the baseline BEST-RQ accuracy of 75.4%. Locale wisely, 88 locales have been improved, while 13 locales observed regressions in accuracy. Figure  3  illustrates the details the significant changes in prediction for the MASR over the BEST-RQ baseline. An interesting improvement is that after incorporating the syntactic Lang2vec embeddings, our langID model performs significantly better in disentangling three Serbo-Croatian languages: Bosnian, Serbian and Croatian. As shown in confusion matrix of model predictions on test data (Figure  4 ), the baseline model generates substantial confusions between these languages. However, leveraging the syntactic information of the languages, through the Lang2vec model, we see an improvement in accuracy for all languages in this group.\n\nSimilarly, substantial improvements were observed in Japanese and Korean, where the baseline system predicted 58% test utterances of Japanese incorrectly as Korean, while with MASR pretraining, the error rate dramatically dropped to 3%. Aside, 25% test examples of Icelandic samples had their predictions changed from Norwegian. We also see performance gains in Indic languages such as Kannada and Malayalam.\n\nThe largest performance gains were observed for Southeastern Asian languages, Sub-Saharan African languages, and Southern Asian languages. For example, 49% of Filipino utterances were mis-predicted as Indonesian, using syntactic distance reduced such mis-classification to 9%. The metadata encoder also helped the prediction of tail languages. It is known that Galician and Asturian are closely related, as Galician and Asturian are both Romance languages, and spoken in adjacent regions of Spain. Syntactic distance helped correct 16% of Galician test utterances from incorrect predictions of Asturian, which improved the accuracy by 45% relative.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Method",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Locale Level Improvements On Dhwani Dataset",
      "text": "The language identification for Southern Asian languages has been known to be challenging  [41] , the accuracy baselines of langID on Dhwani dataset are thus relative low. The LASR improved the language recognition accuracy by 3% on Dhwani. In this work, we proved that, with the inclusion of syntactic similarities, accuracy can be further improved by 2%. Our MASR approach made largest improvements in the prediction of Kashmiri language, as most predictions that were previously classified as Urdu or Marathi by LASR system. Santali language had a degradation, mainly due to the increased mis-classifications as Oriya.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Influence Of Pre-Training Data",
      "text": "Overlapped Languages: We have 75 languages whose speech data are included for pre-training, with 48 of them being common with languages in the FLEURS dataset, and 5 being common with those in the Dhwani dataset. We call these languages \"overlapped\" languages. Experimental results show that injecting syntactic similarities improved the language recognition accuracy for most of the overlapped languages, even when they already have sufficient pretraining data. For example, there are about 26k hours of speech for German language, but in the baseline BEST-RQ system, German has a relatively low recall of 21% with the major confusion with Luxembourgish. The MASR, with syntactic similarities, successfully corrected most of the false negatives, thus enhanced the accuracy for German by 16%. Another typical example is Swedish, which has 16k hours of speech involved in the pre-training. The baseline system had 45% of the Swedish test examples predicted incorrectly as Norwegian. MASR reduced such errors to 13%, and improved the accuracy of Swedish by 41% relative. More interestingly, even for overlapped languages whose speech pre-training data is limited, the MASR with syntactic similarities enabled to classify them better. For example, the Indian languages whose pre-training speech is less than 50 hours, obtained significant relative improvements in accuracy: Oriya (+37%), Telugu (+24%), Tamil (+16%), Pashto (+15%), Hindi (+10%). Only Kazakh and Georgian showed performance degradation while the confusions between Kazakh and Uzbek, Georgian and Armenian, got worsen.\n\nNon-overlapped Languages: The average relative improvements in accuracy over non-overlapped languages is 14.4%, larger than the average improvements of 6.7% on overlapped languages for FLEURS, indicating that MASR framework can effectively leverage the external knowledge base of syntactic similarities, even for languages which do not have any speech data presented in the pretraining, as shown in Figure  3 . This finding is encouraging, as it may pave the way to scale up language identification to more unseen languages, without extra efforts to add significant data resources.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Discussion And Conclusion",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Incorporating Diverse Meta-Data",
      "text": "In all the experiments reported thus far, the MASR setting used only the language label information. As stated in Section 1, one of the goals of formulating the MASR framework is to facilitate the use of multiple streams of label information. In line with this goal, we explore the incorporation of additional meta-labels in the form of i) geographic location of the speaker and ii) textual content of the audio data.\n\nFor including the meta-data based on the geographic location of the speaker, we added speech recordings from the Vaani dataset 1 . This dataset contained audio recordings, of size 4k hours belonging to 36 languages, collected from 80 districts of India. The pre-trained MASR model with language meta-data was further trained (using the MASR objective (Equation  4 )) with language and geographic location based labels. The meta-data encoder for the language was similar to the one used in previous experiments, while the metadata for the geographic location used a Haversine distance between the 2-D location vectors containing the latitude and longitude of the speaker's geographic identity.\n\nFor including the meta-data based on textual content, we used the textual transcripts of the audio recordings (when available) in the pre-training dataset. The text content at character level was encoded  using an embedding matrix and the average embedding for a given utterance was chosen as the text embedding. Similar to the previous setting using geographic location, the text based MASR included the encoded text embeddings at the utterance level along with the language labels. The NOSS results (Table  2 ) using the geographic location based MASR and the text content based MASR show that the audio classification is improved using addition of these metalabels. However, the speaker and emotion tasks see a degradation as these meta-labels may not be beneficial for those downstream tasks.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we have proposed MASR, a novel approach for incorporating multiple meta-data information sources associated with a speech signal. The objective function of hard mining is modified for the inclusion of these information streams. The final objective function is a combination of the meta-label based objectives with the self-supervision based loss functions. Experiments are performed using speech data along with the meta-data such as language labels, speaker's geographic location and text transcripts. Several downstream evaluations are performed to illustrate the performance improvements achieved using the proposed framework.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview of MASR framework. For a speech sample (xi) in a batch of speech samples, MASR utilizes a speech encoder (f) to obtain",
      "page": 2
    },
    {
      "caption": "Figure 2: Comparison of different distance vectors from Lang2vec",
      "page": 4
    },
    {
      "caption": "Figure 2: , MASR relies significantly on the",
      "page": 5
    },
    {
      "caption": "Figure 3: illustrates the details the significant changes in prediction for the",
      "page": 5
    },
    {
      "caption": "Figure 4: ), the baseline model",
      "page": 5
    },
    {
      "caption": "Figure 3: Relative changes in accuracy from the best MASR language identification model over the BEST-RQ baselines. Languages whose",
      "page": 6
    },
    {
      "caption": "Figure 3: This finding is encouraging, as it",
      "page": 6
    },
    {
      "caption": "Figure 4: Confusion Matrices for Serbo-Croatian languages- Bosnian",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          ".73": ".60",
          ".01": ".12",
          ".26": ".28"
        },
        {
          ".73": ".12",
          ".01": ".25",
          ".26": ".63"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          ".22": ".72",
          "0": ".11",
          ".77": ".17"
        },
        {
          ".22": ".29",
          "0": ".33",
          ".77": ".38"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences",
      "authors": [
        "Steven Davis",
        "Paul Mermelstein"
      ],
      "year": "1980",
      "venue": "IEEE transactions on acoustics, speech, and signal processing"
    },
    {
      "citation_id": "3",
      "title": "wav2vec: Unsupervised Pre-Training for Speech Recognition",
      "authors": [
        "Steffen Schneider",
        "Alexei Baevski",
        "Ronan Collobert",
        "Michael Auli"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "4",
      "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of 2019 NAACL-HLT"
    },
    {
      "citation_id": "5",
      "title": "Unsupervised learning of visual representations by solving jigsaw puzzles",
      "authors": [
        "Mehdi Noroozi",
        "Paolo Favaro"
      ],
      "year": "2016",
      "venue": "Computer Vision -ECCV"
    },
    {
      "citation_id": "6",
      "title": "Representation learning with contrastive predictive coding",
      "authors": [
        "Aaron Van Den Oord",
        "Yazhe Li",
        "Oriol Vinyals"
      ],
      "year": "2018",
      "venue": "Representation learning with contrastive predictive coding",
      "arxiv": "arXiv:1807.03748"
    },
    {
      "citation_id": "7",
      "title": "Generative pre-training for speech with autoregressive predictive coding",
      "authors": [
        "Yu-An Chung",
        "James Glass"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "8",
      "title": "Are discrete units necessary for spoken language modeling?",
      "authors": [
        "Anh Tu",
        "Benoit Nguyen",
        "Emmanuel Sagot",
        "Dupoux"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "9",
      "title": "Wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Henry Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Proceedings of the 34th NeurIPS Conference"
    },
    {
      "citation_id": "10",
      "title": "Hubert: Self-supervised speech representation learning masked prediction of hidden units",
      "authors": [
        "Wei-Ning Hsu",
        "Benjamin Bolte",
        "Hubert Yao-Hung",
        "Kushal Tsai",
        "Ruslan Lakhotia",
        "Abdelrahman Salakhutdinov",
        "Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Trans. Audio, Speech and Lang. Proc"
    },
    {
      "citation_id": "11",
      "title": "Self-supervised learning with randomprojection quantizer for speech recognition",
      "authors": [
        "Chung-Cheng Chiu",
        "James Qin",
        "Yu Zhang",
        "Jiahui Yu",
        "Yonghui Wu"
      ],
      "year": "2022",
      "venue": "Proceedings of Machine Learning Research"
    },
    {
      "citation_id": "12",
      "title": "wav2vec: Unsupervised pre-training for speech recognition",
      "authors": [
        "Steffen Schneider",
        "Alexei Baevski",
        "Ronan Collobert",
        "Michael Auli"
      ],
      "year": "2019",
      "venue": "wav2vec: Unsupervised pre-training for speech recognition",
      "arxiv": "arXiv:1904.05862"
    },
    {
      "citation_id": "13",
      "title": "Emotion recognition from speech: a review",
      "authors": [
        "G Shashidhar",
        "K Koolagudi",
        "Rao"
      ],
      "year": "2012",
      "venue": "International Journal of Speech Technology"
    },
    {
      "citation_id": "14",
      "title": "Speaker recognition based on deep learning: An overview",
      "authors": [
        "Zhongxin Bai",
        "Xiao-Lei Zhang"
      ],
      "year": "2020",
      "venue": "Neural networks : the official journal of the International Neural Network Society"
    },
    {
      "citation_id": "15",
      "title": "Self-supervised speech representation learning: A review",
      "authors": [
        "Abdelrahman Mohamed",
        "Hung-Yi Lee",
        "Lasse Borgholt",
        "Jakob Havtorn",
        "Joakim Edin",
        "Christian Igel",
        "Katrin Kirchhoff",
        "Shang-Wen",
        "Karen Li",
        "Lars Livescu",
        "Tara Maaløe",
        "Shinji Sainath",
        "Watanabe"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "16",
      "title": "Superb: Speech processing universal performance benchmark",
      "authors": [
        "Shu-Wen Yang",
        "Po-Han Chi",
        "Yung-Sung Chuang",
        "Cheng-I Jeff Lai",
        "Kushal Lakhotia",
        "Andy Yist Y Lin",
        "Jiatong Liu",
        "Xuankai Shi",
        "Guan-Ting Chang",
        "Lin"
      ],
      "year": "2021",
      "venue": "Superb: Speech processing universal performance benchmark",
      "arxiv": "arXiv:2105.01051"
    },
    {
      "citation_id": "17",
      "title": "Towards Learning a Universal Non-Semantic Representation of Speech",
      "authors": [
        "Joel Shor",
        "Aren Jansen",
        "Ronnie Maor",
        "Oran Lang",
        "Omry Tuval",
        "Félix De Chaumont",
        "Marco Quitry",
        "Ira Tagliasacchi",
        "Dotan Shavitt",
        "Yinnon Emanuel",
        "Haviv"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech 2020"
    },
    {
      "citation_id": "18",
      "title": "Joint masked cpc and ctc training for asr",
      "authors": [
        "Chaitanya Talnikar",
        "Tatiana Likhomanenko",
        "Ronan Collobert",
        "Gabriel Synnaeve"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021"
    },
    {
      "citation_id": "19",
      "title": "Joint unsupervised and supervised training for multilingual asr",
      "authors": [
        "Junwen Bai",
        "Bo Li",
        "Yu Zhang",
        "Ankur Bapna",
        "Nikhil Siddhartha",
        "Khe Sim",
        "Tara Sainath"
      ],
      "year": "2022",
      "venue": "Joint unsupervised and supervised training for multilingual asr"
    },
    {
      "citation_id": "20",
      "title": "Label aware speech representation learning for language identification",
      "authors": [
        "Shikhar Vashishth",
        "Shikhar Bharadwaj",
        "Sriram Ganapathy",
        "Ankur Bapna",
        "Min Ma",
        "Wei Han",
        "Vera Axelrod",
        "Partha Talukdar"
      ],
      "year": "2023",
      "venue": "ArXiv"
    },
    {
      "citation_id": "21",
      "title": "wav2vec-C: A Self-Supervised Model for Speech Representation Learning",
      "authors": [
        "Samik Sadhu",
        "Di He",
        "Che-Wei Huang",
        "Sri Harish Mallidi",
        "Minhua Wu",
        "Ariya Rastrow",
        "Andreas Stolcke",
        "Jasha Droppo",
        "Roland Maas"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech 2021"
    },
    {
      "citation_id": "22",
      "title": "W2v-bert: Combining contrastive learning and masked language modeling for self-supervised speech pre-training",
      "authors": [
        "Yu-An Chung",
        "Yu Zhang",
        "Wei Han",
        "Chung-Cheng Chiu",
        "James Qin",
        "Ruoming Pang",
        "Yonghui Wu"
      ],
      "year": "2021",
      "venue": "2021 IEEE Automatic Speech Recognition and Understanding Workshop"
    },
    {
      "citation_id": "23",
      "title": "Unsupervised speech recognition",
      "authors": [
        "Alexei Baevski",
        "Wei-Ning Hsu",
        "Alexis Conneau",
        "Michael Auli"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "24",
      "title": "An exploration of hubert with large number of cluster units and model assessment using bayesian information criterion",
      "authors": [
        "Takashi Maekaku",
        "Xuankai Chang",
        "Yuya Fujita",
        "Shinji Watanabe"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022"
    },
    {
      "citation_id": "25",
      "title": "End-to-end text-dependent speaker verification",
      "authors": [
        "Georg Heigold",
        "Ignacio Moreno",
        "Samy Bengio",
        "Noam Shazeer"
      ],
      "year": "2016",
      "venue": "IEEE"
    },
    {
      "citation_id": "26",
      "title": "Spoken language recognition using x-vectors.,\" in Odyssey",
      "authors": [
        "David Snyder",
        "Daniel Garcia-Romero",
        "Alan Mccree",
        "Gregory Sell",
        "Daniel Povey",
        "Sanjeev Khudanpur"
      ],
      "year": "2018",
      "venue": "Spoken language recognition using x-vectors.,\" in Odyssey"
    },
    {
      "citation_id": "27",
      "title": "X-vectors: Robust dnn embeddings for speaker recognition",
      "authors": [
        "David Snyder",
        "Daniel Garcia-Romero",
        "Gregory Sell",
        "Daniel Povey",
        "Sanjeev Khudanpur"
      ],
      "year": "2018",
      "venue": "2018 IEEE ICASSP"
    },
    {
      "citation_id": "28",
      "title": "Deep speaker: an end-to-end neural speaker embedding system",
      "authors": [
        "Chao Li",
        "Xiaokong Ma",
        "Bing Jiang",
        "Xiangang Li",
        "Xuewei Zhang",
        "Xiao Liu",
        "Ying Cao",
        "Ajay Kannan",
        "Zhenyao Zhu"
      ],
      "year": "2017",
      "venue": "Deep speaker: an end-to-end neural speaker embedding system",
      "arxiv": "arXiv:1705.02304"
    },
    {
      "citation_id": "29",
      "title": "Ecapa-tdnn: Emphasized channel attention, propagation and aggregation in tdnn based speaker verification",
      "authors": [
        "Brecht Desplanques",
        "Jenthe Thienpondt",
        "Kris Demuynck"
      ],
      "year": "2020",
      "venue": "Ecapa-tdnn: Emphasized channel attention, propagation and aggregation in tdnn based speaker verification",
      "arxiv": "arXiv:2005.07143"
    },
    {
      "citation_id": "30",
      "title": "Frill: A non-semantic speech embedding for mobile devices",
      "authors": [
        "Jacob Peplinski",
        "Joel Shor",
        "P Sachin",
        "Jake Joglekar",
        "Shwetak Garrison",
        "Patel"
      ],
      "year": "2020",
      "venue": "Frill: A non-semantic speech embedding for mobile devices"
    },
    {
      "citation_id": "31",
      "title": "TRILLsson: Distilled Universal Paralinguistic Speech Representations",
      "authors": [
        "Joel Shor",
        "Subhashini Venugopalan"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech 2022"
    },
    {
      "citation_id": "32",
      "title": "Contrastive learning of general-purpose audio representations",
      "authors": [
        "Aaqib Saeed",
        "David Grangier",
        "Neil Zeghidour"
      ],
      "year": "2021",
      "venue": "ICASSP 2021 -2021 IEEE"
    },
    {
      "citation_id": "33",
      "title": "Unispeech: Unified speech representation learning with labeled and unlabeled data",
      "authors": [
        "Chengyi Wang",
        "Yu Wu",
        "Yao Qian",
        "Kenichi Kumatani",
        "Shujie Liu",
        "Furu Wei",
        "Michael Zeng",
        "Xuedong Huang"
      ],
      "year": "2021",
      "venue": "ICML"
    },
    {
      "citation_id": "34",
      "title": "w2v-bert: Combining contrastive learning and masked language modeling for selfsupervised speech pre-training",
      "authors": [
        "Yu-An Chung",
        "Yu Zhang",
        "Wei Han",
        "Chung-Cheng Chiu",
        "James Qin",
        "Ruoming Pang",
        "Yonghui Wu"
      ],
      "year": "2021",
      "venue": "2021 IEEE ASRU Workshop"
    },
    {
      "citation_id": "35",
      "title": "In defense of the classification loss for person re-identification",
      "authors": [
        "Yao Zhai",
        "Xun Guo",
        "Yan Lu",
        "Houqiang Li"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF CVPR Workshops"
    },
    {
      "citation_id": "36",
      "title": "Language Recognition Using Triplet Neural Networks",
      "authors": [
        "Victoria Mingote",
        "Diego Castan",
        "Mitchell Mclaren",
        "Mahesh Kumar Nandwana",
        "Alfonso Ortega",
        "Eduardo Lleida",
        "Antonio Miguel"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "37",
      "title": "Uriel and lang2vec: Representing languages as typological, geographical, and phylogenetic vectors",
      "authors": [
        "Patrick Littell",
        "Ke David R Mortensen",
        "Katherine Lin",
        "Carlisle Kairis",
        "Lori Turner",
        "Levin"
      ],
      "year": "2017",
      "venue": "Proceedings of the 15th Conference of the European Chapter"
    },
    {
      "citation_id": "38",
      "title": "Voxpopuli: A large-scale multilingual speech corpus for representation learning, semisupervised learning and interpretation",
      "authors": [
        "Changhan Wang",
        "Morgane Rivière",
        "Ann Lee",
        "Anne Wu",
        "Chaitanya Talnikar",
        "Daniel Haziza",
        "Mary Williamson",
        "Juan Pino",
        "Emmanuel Dupoux"
      ],
      "year": "2021",
      "venue": "ACL"
    },
    {
      "citation_id": "39",
      "title": "Common voice: A massively-multilingual speech corpus",
      "authors": [
        "Rosana Ardila",
        "Megan Branson",
        "Kelly Davis",
        "Michael Henretty",
        "Michael Kohler",
        "Josh Meyer",
        "Reuben Morais",
        "Lindsay Saunders",
        "Francis Tyers",
        "Gregor Weber"
      ],
      "year": "2019",
      "venue": "LREC"
    },
    {
      "citation_id": "40",
      "title": "MLS: A Large-Scale Multilingual Dataset for Speech Research",
      "authors": [
        "Qiantong Vineel Pratap",
        "Anuroop Xu",
        "Gabriel Sriram",
        "Ronan Synnaeve",
        "Collobert"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "41",
      "title": "Speech recognition and keyword spotting for low-resource languages: Babel project research at cued",
      "authors": [
        "Mark John",
        "Francis Gales",
        "Kate Knill",
        "Anton Ragni",
        "Shakti Prasad"
      ],
      "year": "2014",
      "venue": "Workshop on Spoken Language Technologies for Underresourced Languages"
    },
    {
      "citation_id": "42",
      "title": "Fleurs: Few-shot learning evaluation of universal representations of speech",
      "authors": [
        "Alexis Conneau",
        "Min Ma",
        "Simran Khanuja",
        "Yu Zhang",
        "Vera Axelrod",
        "Siddharth Dalmia",
        "Jason Riesa",
        "Clara Rivera",
        "Ankur Bapna"
      ],
      "year": "2022",
      "venue": "2022 IEEE Spoken Language Technology Workshop"
    },
    {
      "citation_id": "43",
      "title": "Towards building asr systems for the next billion users",
      "authors": [
        "Tahir Javed",
        "Sumanth Doddapaneni",
        "Abhigyan Raman",
        "Kaushal Santosh Bhogale",
        "Gowtham Ramesh",
        "Anoop Kunchukuttan",
        "Pratyush Kumar",
        "Mitesh Khapra"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "44",
      "title": "The flores-101 evaluation benchmark for low-resource and multilingual machine translation",
      "authors": [
        "Naman Goyal",
        "Cynthia Gao",
        "Vishrav Chaudhary",
        "Peng-Jen Chen",
        "Guillaume Wenzek",
        "Da Ju",
        "Sanjan Krishnan",
        "Marc'aurelio Ranzato",
        "Francisco Guzmán",
        "Angela Fan"
      ],
      "year": "2021",
      "venue": "Transactions of the Association for Computational Linguistics"
    }
  ]
}