{
  "paper_id": "2102.03229v1",
  "title": "Multi-Task Self-Supervised Pre-Training For Music Classification",
  "published": "2021-02-05T15:19:58Z",
  "authors": [
    "Ho-Hsiang Wu",
    "Chieh-Chi Kao",
    "Qingming Tang",
    "Ming Sun",
    "Brian McFee",
    "Juan Pablo Bello",
    "Chao Wang"
  ],
  "keywords": [
    "Self-supervised learning",
    "multi-task learning",
    "music classification"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Deep learning is very data hungry, and supervised learning especially requires massive labeled data to work well. Machine listening research often suffers from limited labeled data problem, as human annotations are costly to acquire, and annotations for audio are time consuming and less intuitive. Besides, models learned from labeled dataset often embed biases specific to that particular dataset. Therefore, unsupervised learning techniques become popular approaches in solving machine listening problems. Particularly, a self-supervised learning technique utilizing reconstructions of multiple hand-crafted audio features has shown promising results when it is applied to speech domain such as emotion recognition and automatic speech recognition (ASR). In this paper, we apply selfsupervised and multi-task learning methods for pre-training music encoders, and explore various design choices including encoder architectures, weighting mechanisms to combine losses from multiple tasks, and worker selections of pretext tasks. We investigate how these design choices interact with various downstream music classification tasks. We find that using various music specific workers altogether with weighting mechanisms to balance the losses during pre-training helps improve and generalize to the downstream tasks.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Deep learning has shown great successes with end-to-end learned representations replacing hand-crafted features in various machine perception fields, including computer vision, natural language processing and machine listening, especially in supervised learning paradigm. However, unlike ImageNet for computer vision, which contains millions of labeled images, human annotated datasets for machine listening are usually small  [1] . Therefore, learning from limited labeled data  [2]  is especially important. There are existing methods such as transfer learning  [3]  and domain adaptation, where models learned from different tasks with larger datasets are transferred and fine-tuned to another task/domain, and unsupervised learning  [4, 5, 6] , such as generative models  [7, 8] , where data distribution is often learned through reconstruction of the signal.\n\nSelf-supervised learning  [9, 10, 11, 12] , as one sub-field of unsupervised learning, exploits the structure of the input data to provide supervision signals. It has become more popular in recent years, showing good improvement in multiple fields. For selfsupervised learning, raw signals are transformed, and models are Work done at Amazon This work is partially supported by the National Science Foundation award #1544753 optimized with reconstruction or contrastive losses against original signals, where preserving of temporal or spatial data consistency is assumed for learning meaningful representations. These representations are proven useful to generalize and solve downstream tasks. On the other hand, multi-task learning  [13]  improves generality by solving multiple tasks altogether during training, while weighting mechanisms among the losses from each task are crucial  [14, 15] . Self-supervised and multi-task learning techniques are combined and applied to the speech domain, and they have shown success in  [16, 17] , where reconstruction of various hand-crafted features are used for pre-training, and further learned representations are evaluated with downstream emotion recognition and automatic speech recognition (ASR) tasks.\n\nSimilar to speech, music is also a highly structured audio signal. There are many hand-crafted features designed specifically for music to solve various music information retrieval (MIR) tasks. In this paper, we are interested in applying self-supervised and multitask learning methods for pre-training music encoders. We explore various design choices including encoder architectures, weighting mechanisms to combine losses from pretext tasks, and worker selections to reconstruct various music specific hand-crafted features, such as Mel-frequency cepstral coefficients (MFCCs) for timbre  [18] , Chroma for harmonic  [19] , and Tempogram  [20]  for rhythmic attributes. Our main contributions are 1. provide suggestions on best design choice among all the variations from our experiments, and 2. investigate how different selections of pretext tasks interact with the performance of downstream music classification tasks, including instrument, rhythm and genre.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Method",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Encoder",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Waveform",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Mfcc Chroma",
      "text": "Regression Pre-training Encoder with AudioSet Downstream Tasks",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Music Classification",
      "text": "Violin, Guitar, Chacha, Tango, Jazz, Rock, ...",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Datasets Openmic Extended Ballroom",
      "text": "FMA Genre LPS Proso Tempo A two-stage approach involving unsupervised or self-supervised pre-training and supervised learning for training to evaluate on downstream tasks is commonly adopted  [9, 10, 16, 17]  in recent literature, especially in the context of limited labeled data, where representation learning is key. In order to evaluate the effectiveness of the pre-training, simple linear or multi-layer perceptron (MLP) classifiers are usually used where the pre-trained encoders are required to capture meaningful representations to perform well on linear separation evaluation tasks.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Multi-Task Self-Supervised Pre-Training",
      "text": "As shown in Figure  1 , we combine self-supervised and multi-task learning ideas for pre-training. Raw audio inputs are passed through multiple encoding layers, and outputs are two dimensional representations with temporal information. These encoded representations are then used for solving pretext tasks via workers including waveform reconstruction, and prediction of various popular hand-crafted features used in MIR to guide the learning jointly.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Downstream Task Training Scenarios",
      "text": "After pre-training, we remove the workers, and feed the encoder outputs to MLP classifiers for downstream tasks. We adopt three training scenarios proposed in  [16] : 1. Supervised: Initialize the encoder weights randomly and train from scratch on the downstream datasets directly. 2. Frozen: Treat the pre-trained encoder as feature extractor with frozen weights, concatenate the feature extractor with trainable MLP classifiers and only optimize the classifier weights. 3. Fine-tuned: Initialize the encoder with pre-trained weights and fine-tune the encoder with downstream tasks altogether.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Experimental Design",
      "text": "We experiment with various design choices during pre-training including 1. Encoder architectures, 2. Pretext tasks for worker selections, 3. Weighting mechanisms for losses from pretext tasks. We provide more details on the downstream evaluations and data usage for both pre-training and downstream tasks in section 3.4 and 3.5.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Encoder Architectures",
      "text": "We compare two encoder architectures proposed in two relevant studies in speech domain which inspire our work. We refer the two encoder architectures as PASE  [16]  and PASE+  [17] , respectively.\n\n1. PASE: We use the same encoder architecture as the original PASE work  [16]  with source code implementation  1  . The first layer is based on SincNet  [21] , where the raw input waveform is convolved with a set of parameterized Sinc functions implementing rectangular band-pass filters. The authors claim that SincNet has fewer parameters and provides better interpretability. SincNet layer is followed by 7 one-dimensional convolutional blocks, batch normalization  [22] , and multi-parametric rectified linear unit activation  [23] . We use the same model parameters as provided in the original work including kernel widths, number of filters, and strides. The set of parameters for convolutional layers emulates a 10ms sliding window.\n\n2. PASE+: PASE+  [17]  improves upon PASE  [16]  by adding skip connections and Quasi-Recurrent Neural Network (QRNN)  [24]  layers to capture longer-term contextual information. QRNN layers consist of interleaved convolutional layers with RNN layers to speed up training with parallel optimization, while maintaining compatible performance.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Pretext Tasks Worker Selections",
      "text": "Inspired by the original PASE  [16]  work, we select waveform reconstruction, log power spectrum (LPS) and prosody features as baseline workers. We then choose three popular hand-crafted features in MIR field including MFCC, Chroma, and Tempogram as mixed-in workers. For waveform reconstruction, encoder layers are applied in reverse order to decode embeddings and optimized with mean absolute error (MAE) loss. For all the other workers, we use MLP with convolutional layers, and mean squared error (MSE) loss.\n\nWaveform, LPS, and MFCC are commonly used in machine listening. Chroma is inspired from western 12-tone theory which frequencies are folded into 12 bins as one octave. Tempogram  [20]  takes local auto-correlation of the onset strength envelope. As used in  [16] , prosody features include zero crossing rate (ZCR), energy, voice/unvoice probability and fundamental frequency (F0) estimation, resulting in 4 features concatenated along with temporal dimension. For LPS, MFCC, Chroma, Tempogram and prosody, we use librosa 2  implementations with hop length = 160, n fft = 2048, sr = 16000 in order to align each hop as 10ms to match encoder parameters, with other default parameters.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Weighting Mechanisms",
      "text": "We explore two weighting mechanisms to combine losses from each worker during pre-training. 1. Equal weighted by simply sum up losses from different workers for backpropagation. 2. Re-weighted by taking the validation losses per worker of the first 10 epochs from equal weighted training, averaging the loss per worker, taking the reciprocal as the new weights and applying those to retrain from scratch. The intuition is that the losses from each worker will then contribute more equally during backpropagation optimization.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Downstream Evaluation",
      "text": "After pre-training, we remove the workers for pretext tasks and concatenate the output of the encoder with a simple MLP classifier. The input layer of the MLP is to take mean pooling across temporal dimension, resulting in one 512 dimension embedding, followed by 1 fully connected layer to adapt to output dimensions corresponding to the number of classes of each downstream dataset. We train with three scenarios discussed in section 2.2, including supervised, frozen and fine-tuned, all with the same hyper-parameters, Adam optimizer  [25]  with initial learning rate as 0.001 and early stopping criteria with patience value of 10 on validation loss. We run 10 trials for each experiment in this paper to get statistically meaningful results.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Data",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Audioset For Pre-Training",
      "text": "We use clips in AudioSet  [26]  with \"Music\" label for pre-training. We are able to acquire ˜2M (97% of the original AudioSet data) clips, within which there are ˜980k clips labeled with \"Music\". We randomly select 100k for pre-training, resulting in ˜83 hours of data.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Datasets For Downstream Evaluation",
      "text": "OpenMIC  [27] , Extended Ballroom  [28]  and FMA Small (FMA)  [29] , three publicly available classification datasets are used for downstream evaluation as representative samples of well-known MIR tasks. These datasets range from different number of clips, clip duration, and number of classes. For all three datasets, we report macro F1 scores as shown in the figures.\n\n1. OpenMIC  [27] : OpenMIC is a multi-label instrument classification dataset containing 15k samples total with provided train/valid/test splits as well as masks for strong positive and negative examples for each class. We follow similar setup as the official baseline 3  by training 20 binary classifiers.\n\n2. Extended Ballroom  [28] : Extended Ballroom (4k samples) is a multi-class dance genre classification dataset. We follow the same setup as  [30]  by removing 4 categories due to dataset imbalance, resulting in only using 9 categories.\n\n3. FMA Small  [29] : FMA Small (8k samples) is a multi-class music genre classification dataset with 8 genre categories.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results And Discussions",
      "text": "We first show results of encoder choices and whether pre-training helps. All workers (waveform (W), LPS (L), prosody (P), MFCC (M), Chroma (C) and Tempogram (T), where WLP are also referred to as baseline) and frozen scenario are used. We then dive deeper into the effects of different weighting mechanisms, and ablation study of worker selections, for which we also report results in frozen scenario. Finally, we investigate whether fine-tuning further improves performance. From Figure  2 , we observe that for all three downstream tasks, PASE+ outperforms PASE. This is not surprising as PASE+ is a more powerful encoder with ˜8M parameters, skip-connection and QRNN layer, and PASE has only ˜6M parameters and basic convolutional layers. This confirms with the findings from original PASE+  [17]  work applied to speech data.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Encoder Architectures",
      "text": "The dotted lines are trained supervisedly (scenario 1) from scratch directly on the downstream tasks with random weights initialization. It shows that pre-training in general helps to initialize the encoder weights better, resulting in better performance on downstream tasks. One exception is PASE for OpenMIC, we hypothesize that it is because OpenMIC already contains enough data to train PASE encoder (with less capacity) from scratch well, which is not the case for PASE+. This shows that pre-training for encoders with larger capacities is especially helpful when evaluating on downstream tasks with limited labeled data. We conducted experiments using PASE+ through out the remaining paper as it's a better encoder for our tasks. In Figure  3 , we show results comparing equal weighted and reweighted mechanisms with different worker selections during pretraining. We see that re-weighted mechanism (filled color) helps to boost the influences from various workers to the performance of downstream tasks in general. For Extended Ballroom on the right especially, we see clearly that results with workers containing Tempogram are improved by a large margin. We further examine losses per worker during pre-training as shown in Figure  4 . We can see that with equal weighted on the left, LPS (L) almost dominates all losses and Tempogram (T) worker loss contributes the least with two orders of magnitude smaller, but for re-weighted on the right, each worker contributes more equally.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Weighting Mechanisms",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Pretext Tasks Worker Selections",
      "text": "Figure  5  shows the relative difference in accuracy by including different workers over the WLP baseline. We observe that different worker selections affect variously to different downstream tasks. Tempogram helps the most across all different combinations especially for Extended Ballroom. MFCC is usually important for most of the downstream tasks as it captures the low-level attributes differentiating instrument and genre. Chroma is however at a disadvantage, especially for OpenMIC, since Chroma is designed to normalize for timbre, which is important for instrumentation. MFCC only hurts slightly on Extended Ballroom as it brings together different dance genres with similar timbre, and separates music from same dance genre that changes in timbre. These variations can be further compensated to show improvement across all tasks by using all workers as shown on the right most of each subplot in Figure  5 . We observe relative improvement adding all workers compared to WLP baseline by 1.9%, 4.5% and 14% on OpenMIC, FMA and Extended Ballroom datasets respectively. This indicates that workers complement each other, and the encoders are able to use signals from diversified workers to generalize better to various downstream tasks.  We then show confusion matrices of Extended Ballroom and FMA in Figure  6  and 7 . In Figure  6 , we show the difference between WLP + T and WLP, and observe that adding Tempogram helps differentiate Chacha with Jive and Samba, which differ in rhythm and tempo, as well as Foxtrot with Quickstep, and Viennesewaltz with Waltz, as the two pairs of dance genres originate from similar music playing in different speed. Adding MFCC and Chroma further helps differentiate Foxtrot with Rumba and Viennesewaltz as additional timbre cues are provided.\n\nIn Figure  7 , we observe that even adding MFCC (WLP+M -WLP) helps in general as hypothesized, however, it misclassifies Electronic with Hip-Hop and International, and Pop with Hip-Hop and Rock, as there might be similar instruments used in these genres, resulting in similar timbre. Adding Tempogram (WLP+T -WLP+M) corrects the mistakes made on Electronic and Pop genres, but misclassifying International with Folk and Instrumental. Finally, adding both workers (WLP+MT -WLP+T) provides further improvements upon MFCC and Tempogram only. In general we observe improvements with positive values (red) in diagonal and negative (blue) in off-diagonal. In Figure  8 , we plot frozen (filled) versus fine-tuned (no filled) with re-weighted mechanisms and all workers used during pretraining. By using all available training examples, both Extended Ballroom (2.8k) and OpenMIC (11k) show further improvement with fine-tuning, while FMA does not. We hypothesize that this is because each downstream task requires different number of samples for fine-tuned to work well. For FMA, we just do not have enough training samples. We further reduce number of samples used for training OpenMIC and Extended Ballroom as shown in Figure  8 , where we see clear reverting behavior around 8k (OpenMIC) and 1k (Extended Ballroom) that fine-tuning stops to outperform frozen.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we explore different design choices for pre-training music encoders with multi-task and self-supervised learning techniques, and show that this method, when combined with different encoder architectures, generally benefits for downstream tasks. The improvement is clearer and more stable when (# unlabeled data / # labeled data) is larger. We also show that each type of pretext task provides different and complementary information, re-weighted mechanism helps the encoder to better learn different cues provided from each task, and fine-tuning can further improve performance.\n\nFor future work, we are interested in applying this pre-training technique to various encoders, adding more audio specific features, and explore other unsupervised and self-supervised learning ideas such as wav2vec  [5]  as pretext tasks. We are also interested in including more diverse downstream tasks such as music tagging, and chord recognition (Chroma should be more effective in this task) for evaluation. We think that this pre-training technique can be applied to a large varieties of music encoders and generalize to different downstream music tasks, especially those with limited labeled data.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Diagram of multi-task self-supervised encoder pre-training",
      "page": 1
    },
    {
      "caption": "Figure 1: , we combine self-supervised and multi-task",
      "page": 2
    },
    {
      "caption": "Figure 2: Comparisons of encoder architectures (PASE vs PASE+).",
      "page": 3
    },
    {
      "caption": "Figure 2: , we observe that for all three downstream tasks,",
      "page": 3
    },
    {
      "caption": "Figure 3: Comparisons of equal weighted vs re-weighted for different",
      "page": 3
    },
    {
      "caption": "Figure 3: , we show results comparing equal weighted and re-",
      "page": 3
    },
    {
      "caption": "Figure 4: Log loss per worker for ﬁrst 20 epochs. X-axis is number of",
      "page": 3
    },
    {
      "caption": "Figure 4: We can see that with equal weighted on the left,",
      "page": 3
    },
    {
      "caption": "Figure 5: shows the relative difference in accuracy by including dif-",
      "page": 3
    },
    {
      "caption": "Figure 5: Relative improvement (%) of different additional music spe-",
      "page": 4
    },
    {
      "caption": "Figure 5: We observe relative improvement adding",
      "page": 4
    },
    {
      "caption": "Figure 6: Confusion matrices of Extended Ballroom. On the left is",
      "page": 4
    },
    {
      "caption": "Figure 7: Confusion matrices of FMA. On the left is WLP baseline.",
      "page": 4
    },
    {
      "caption": "Figure 6: and 7. In Figure 6, we show the difference between",
      "page": 4
    },
    {
      "caption": "Figure 7: , we observe that even adding MFCC (WLP+M -",
      "page": 4
    },
    {
      "caption": "Figure 8: Comparisons of frozen and ﬁne-tuned on # of training sam-",
      "page": 4
    },
    {
      "caption": "Figure 8: , we plot frozen (ﬁlled) versus ﬁne-tuned (no ﬁlled)",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2 Alexa Speech, Amazon": "ABSTRACT"
        },
        {
          "2 Alexa Speech, Amazon": ""
        },
        {
          "2 Alexa Speech, Amazon": "Deep learning is very data hungry, and supervised learning es-"
        },
        {
          "2 Alexa Speech, Amazon": ""
        },
        {
          "2 Alexa Speech, Amazon": "pecially requires massive labeled data to work well. Machine lis-"
        },
        {
          "2 Alexa Speech, Amazon": ""
        },
        {
          "2 Alexa Speech, Amazon": "tening research often suffers from limited labeled data problem, as"
        },
        {
          "2 Alexa Speech, Amazon": ""
        },
        {
          "2 Alexa Speech, Amazon": "human annotations are costly to acquire,\nand annotations\nfor au-"
        },
        {
          "2 Alexa Speech, Amazon": ""
        },
        {
          "2 Alexa Speech, Amazon": "dio are time consuming and less intuitive. Besides, models learned"
        },
        {
          "2 Alexa Speech, Amazon": ""
        },
        {
          "2 Alexa Speech, Amazon": "from labeled dataset often embed biases speciﬁc to that particular"
        },
        {
          "2 Alexa Speech, Amazon": ""
        },
        {
          "2 Alexa Speech, Amazon": "dataset. Therefore, unsupervised learning techniques become pop-"
        },
        {
          "2 Alexa Speech, Amazon": ""
        },
        {
          "2 Alexa Speech, Amazon": "ular approaches\nin solving machine listening problems.\nParticu-"
        },
        {
          "2 Alexa Speech, Amazon": ""
        },
        {
          "2 Alexa Speech, Amazon": "larly, a self-supervised learning technique utilizing reconstructions"
        },
        {
          "2 Alexa Speech, Amazon": ""
        },
        {
          "2 Alexa Speech, Amazon": "of multiple hand-crafted audio features has shown promising results"
        },
        {
          "2 Alexa Speech, Amazon": ""
        },
        {
          "2 Alexa Speech, Amazon": "when it is applied to speech domain such as emotion recognition and"
        },
        {
          "2 Alexa Speech, Amazon": ""
        },
        {
          "2 Alexa Speech, Amazon": "automatic speech recognition (ASR).\nIn this paper, we apply self-"
        },
        {
          "2 Alexa Speech, Amazon": ""
        },
        {
          "2 Alexa Speech, Amazon": "supervised and multi-task learning methods for pre-training music"
        },
        {
          "2 Alexa Speech, Amazon": ""
        },
        {
          "2 Alexa Speech, Amazon": "encoders, and explore various design choices including encoder ar-"
        },
        {
          "2 Alexa Speech, Amazon": ""
        },
        {
          "2 Alexa Speech, Amazon": "chitectures, weighting mechanisms to combine losses from multiple"
        },
        {
          "2 Alexa Speech, Amazon": ""
        },
        {
          "2 Alexa Speech, Amazon": "tasks, and worker selections of pretext\ntasks. We investigate how"
        },
        {
          "2 Alexa Speech, Amazon": ""
        },
        {
          "2 Alexa Speech, Amazon": "these design choices interact with various downstream music clas-"
        },
        {
          "2 Alexa Speech, Amazon": ""
        },
        {
          "2 Alexa Speech, Amazon": "siﬁcation tasks. We ﬁnd that using various music speciﬁc workers"
        },
        {
          "2 Alexa Speech, Amazon": ""
        },
        {
          "2 Alexa Speech, Amazon": "altogether with weighting mechanisms to balance the losses during"
        },
        {
          "2 Alexa Speech, Amazon": ""
        },
        {
          "2 Alexa Speech, Amazon": "pre-training helps improve and generalize to the downstream tasks."
        },
        {
          "2 Alexa Speech, Amazon": ""
        },
        {
          "2 Alexa Speech, Amazon": "Index Terms— Self-supervised learning, multi-task learning,"
        },
        {
          "2 Alexa Speech, Amazon": "music classiﬁcation"
        },
        {
          "2 Alexa Speech, Amazon": ""
        },
        {
          "2 Alexa Speech, Amazon": ""
        },
        {
          "2 Alexa Speech, Amazon": "1.\nINTRODUCTION"
        },
        {
          "2 Alexa Speech, Amazon": ""
        },
        {
          "2 Alexa Speech, Amazon": ""
        },
        {
          "2 Alexa Speech, Amazon": "Deep learning has shown great successes with end-to-end learned"
        },
        {
          "2 Alexa Speech, Amazon": "representations replacing hand-crafted features in various machine"
        },
        {
          "2 Alexa Speech, Amazon": "perception ﬁelds,\nincluding computer vision, natural\nlanguage pro-"
        },
        {
          "2 Alexa Speech, Amazon": ""
        },
        {
          "2 Alexa Speech, Amazon": "cessing and machine\nlistening,\nespecially in supervised learning"
        },
        {
          "2 Alexa Speech, Amazon": "paradigm. However, unlike ImageNet\nfor computer vision, which"
        },
        {
          "2 Alexa Speech, Amazon": "contains millions of\nlabeled images, human annotated datasets for"
        },
        {
          "2 Alexa Speech, Amazon": "machine listening are usually small\n[1].\nTherefore,\nlearning from"
        },
        {
          "2 Alexa Speech, Amazon": "limited labeled data [2]\nis especially important.\nThere are exist-"
        },
        {
          "2 Alexa Speech, Amazon": ""
        },
        {
          "2 Alexa Speech, Amazon": "ing methods such as\ntransfer\nlearning [3] and domain adaptation,"
        },
        {
          "2 Alexa Speech, Amazon": "where models learned from different\ntasks with larger datasets are"
        },
        {
          "2 Alexa Speech, Amazon": ""
        },
        {
          "2 Alexa Speech, Amazon": "transferred and ﬁne-tuned to another task/domain, and unsupervised"
        },
        {
          "2 Alexa Speech, Amazon": "learning [4, 5, 6],\nsuch as generative models\n[7, 8], where data"
        },
        {
          "2 Alexa Speech, Amazon": "distribution is often learned through reconstruction of the signal."
        },
        {
          "2 Alexa Speech, Amazon": ""
        },
        {
          "2 Alexa Speech, Amazon": "Self-supervised learning [9, 10, 11, 12],\nas one\nsub-ﬁeld of"
        },
        {
          "2 Alexa Speech, Amazon": ""
        },
        {
          "2 Alexa Speech, Amazon": "unsupervised learning,\nexploits\nthe structure of\nthe input data to"
        },
        {
          "2 Alexa Speech, Amazon": ""
        },
        {
          "2 Alexa Speech, Amazon": "provide supervision signals.\nIt has become more popular in recent"
        },
        {
          "2 Alexa Speech, Amazon": ""
        },
        {
          "2 Alexa Speech, Amazon": "years,\nshowing good improvement\nin multiple ﬁelds.\nFor\nself-"
        },
        {
          "2 Alexa Speech, Amazon": ""
        },
        {
          "2 Alexa Speech, Amazon": "supervised learning,\nraw signals are transformed, and models are"
        },
        {
          "2 Alexa Speech, Amazon": ""
        },
        {
          "2 Alexa Speech, Amazon": "(cid:63) Work done at Amazon"
        },
        {
          "2 Alexa Speech, Amazon": "This work is partially supported by the National Science Foundation"
        },
        {
          "2 Alexa Speech, Amazon": "award #1544753"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "A two-stage approach involving unsupervised or self-supervised": "pre-training\nand\nsupervised\nlearning\nfor\ntraining\nto\nevaluate\non",
          "to speed up training with parallel optimization, while maintaining": "compatible performance."
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "downstream tasks\nis commonly adopted [9, 10, 16, 17]\nin recent",
          "to speed up training with parallel optimization, while maintaining": ""
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "literature, especially in the context of\nlimited labeled data, where",
          "to speed up training with parallel optimization, while maintaining": ""
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "",
          "to speed up training with parallel optimization, while maintaining": "3.2. Pretext tasks worker selections"
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "representation learning is key.\nIn order to evaluate the effectiveness",
          "to speed up training with parallel optimization, while maintaining": ""
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "of\nthe pre-training,\nsimple linear or multi-layer perceptron (MLP)",
          "to speed up training with parallel optimization, while maintaining": ""
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "",
          "to speed up training with parallel optimization, while maintaining": "Inspired by the original PASE [16] work, we select waveform recon-"
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "classiﬁers are usually used where the pre-trained encoders are re-",
          "to speed up training with parallel optimization, while maintaining": ""
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "",
          "to speed up training with parallel optimization, while maintaining": "struction,\nlog power spectrum (LPS) and prosody features as base-"
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "quired to capture meaningful\nrepresentations\nto perform well on",
          "to speed up training with parallel optimization, while maintaining": ""
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "",
          "to speed up training with parallel optimization, while maintaining": "line workers. We then choose three popular hand-crafted features in"
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "linear separation evaluation tasks.",
          "to speed up training with parallel optimization, while maintaining": ""
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "",
          "to speed up training with parallel optimization, while maintaining": "MIR ﬁeld including MFCC, Chroma, and Tempogram as mixed-in"
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "",
          "to speed up training with parallel optimization, while maintaining": "workers. For waveform reconstruction, encoder layers are applied in"
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "2.1. Multi-task self-supervised pre-training",
          "to speed up training with parallel optimization, while maintaining": ""
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "",
          "to speed up training with parallel optimization, while maintaining": "reverse order to decode embeddings and optimized with mean abso-"
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "",
          "to speed up training with parallel optimization, while maintaining": "lute error (MAE) loss. For all\nthe other workers, we use MLP with"
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "As shown in Figure 1, we combine self-supervised and multi-task",
          "to speed up training with parallel optimization, while maintaining": ""
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "",
          "to speed up training with parallel optimization, while maintaining": "convolutional layers, and mean squared error (MSE) loss."
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "learning ideas for pre-training. Raw audio inputs are passed through",
          "to speed up training with parallel optimization, while maintaining": ""
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "",
          "to speed up training with parallel optimization, while maintaining": "Waveform, LPS, and MFCC are commonly used in machine lis-"
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "multiple encoding layers, and outputs are two dimensional represen-",
          "to speed up training with parallel optimization, while maintaining": ""
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "",
          "to speed up training with parallel optimization, while maintaining": "tening. Chroma is inspired from western 12-tone theory which fre-"
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "tations with temporal\ninformation.\nThese encoded representations",
          "to speed up training with parallel optimization, while maintaining": ""
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "",
          "to speed up training with parallel optimization, while maintaining": "quencies are folded into 12 bins as one octave.\nTempogram [20]"
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "are then used for solving pretext\ntasks via workers including wave-",
          "to speed up training with parallel optimization, while maintaining": ""
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "",
          "to speed up training with parallel optimization, while maintaining": "takes local auto-correlation of the onset strength envelope. As used"
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "form reconstruction, and prediction of various popular hand-crafted",
          "to speed up training with parallel optimization, while maintaining": ""
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "",
          "to speed up training with parallel optimization, while maintaining": "in [16], prosody features include zero crossing rate (ZCR), energy,"
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "features used in MIR to guide the learning jointly.",
          "to speed up training with parallel optimization, while maintaining": ""
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "",
          "to speed up training with parallel optimization, while maintaining": "voice/unvoice probability and fundamental\nfrequency (F0) estima-"
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "",
          "to speed up training with parallel optimization, while maintaining": "tion,\nresulting in 4 features concatenated along with temporal di-"
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "2.2. Downstream task training scenarios",
          "to speed up training with parallel optimization, while maintaining": ""
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "",
          "to speed up training with parallel optimization, while maintaining": "mension. For LPS, MFCC, Chroma, Tempogram and prosody, we"
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "",
          "to speed up training with parallel optimization, while maintaining": "use librosa2\nimplementations with hop length = 160, n fft = 2048,"
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "After pre-training, we remove the workers, and feed the encoder out-",
          "to speed up training with parallel optimization, while maintaining": ""
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "",
          "to speed up training with parallel optimization, while maintaining": "sr = 16000 in order\nto align each hop as 10ms to match encoder"
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "puts to MLP classiﬁers for downstream tasks. We adopt three train-",
          "to speed up training with parallel optimization, while maintaining": ""
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "",
          "to speed up training with parallel optimization, while maintaining": "parameters, with other default parameters."
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "ing scenarios proposed in [16]:\n1.\nSupervised:\nInitialize the en-",
          "to speed up training with parallel optimization, while maintaining": ""
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "coder weights randomly and train from scratch on the downstream",
          "to speed up training with parallel optimization, while maintaining": ""
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "datasets directly. 2. Frozen: Treat the pre-trained encoder as feature",
          "to speed up training with parallel optimization, while maintaining": ""
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "",
          "to speed up training with parallel optimization, while maintaining": "3.3. Weighting mechanisms"
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "extractor with frozen weights, concatenate the feature extractor with",
          "to speed up training with parallel optimization, while maintaining": ""
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "trainable MLP classiﬁers and only optimize the classiﬁer weights.",
          "to speed up training with parallel optimization, while maintaining": ""
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "",
          "to speed up training with parallel optimization, while maintaining": "We explore two weighting mechanisms to combine losses from each"
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "3. Fine-tuned:\nInitialize the encoder with pre-trained weights and",
          "to speed up training with parallel optimization, while maintaining": ""
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "",
          "to speed up training with parallel optimization, while maintaining": "worker during pre-training.\n1. Equal weighted by simply sum up"
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "ﬁne-tune the encoder with downstream tasks altogether.",
          "to speed up training with parallel optimization, while maintaining": ""
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "",
          "to speed up training with parallel optimization, while maintaining": "losses from different workers for backpropagation. 2. Re-weighted"
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "",
          "to speed up training with parallel optimization, while maintaining": "by taking the validation losses per worker of the ﬁrst 10 epochs from"
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "",
          "to speed up training with parallel optimization, while maintaining": "equal weighted training, averaging the loss per worker,\ntaking the"
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "3. EXPERIMENTAL DESIGN",
          "to speed up training with parallel optimization, while maintaining": ""
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "",
          "to speed up training with parallel optimization, while maintaining": "reciprocal as\nthe new weights and applying those to retrain from"
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "",
          "to speed up training with parallel optimization, while maintaining": "scratch. The intuition is that\nthe losses from each worker will\nthen"
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "We experiment with various design choices during pre-training in-",
          "to speed up training with parallel optimization, while maintaining": ""
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "",
          "to speed up training with parallel optimization, while maintaining": "contribute more equally during backpropagation optimization."
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "cluding 1. Encoder architectures, 2. Pretext\ntasks for worker selec-",
          "to speed up training with parallel optimization, while maintaining": ""
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "tions, 3. Weighting mechanisms for losses from pretext\ntasks. We",
          "to speed up training with parallel optimization, while maintaining": ""
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "provide more details on the downstream evaluations and data usage",
          "to speed up training with parallel optimization, while maintaining": ""
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "",
          "to speed up training with parallel optimization, while maintaining": "3.4. Downstream evaluation"
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "for both pre-training and downstream tasks in section 3.4 and 3.5.",
          "to speed up training with parallel optimization, while maintaining": ""
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "",
          "to speed up training with parallel optimization, while maintaining": "After pre-training, we remove the workers for pretext tasks and con-"
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "3.1. Encoder architectures",
          "to speed up training with parallel optimization, while maintaining": "catenate the output of the encoder with a simple MLP classiﬁer. The"
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "",
          "to speed up training with parallel optimization, while maintaining": "input\nlayer of the MLP is to take mean pooling across temporal di-"
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "We\ncompare\ntwo encoder\narchitectures proposed in two relevant",
          "to speed up training with parallel optimization, while maintaining": ""
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "",
          "to speed up training with parallel optimization, while maintaining": "mension, resulting in one 512 dimension embedding, followed by 1"
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "studies in speech domain which inspire our work. We refer the two",
          "to speed up training with parallel optimization, while maintaining": ""
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "",
          "to speed up training with parallel optimization, while maintaining": "fully connected layer\nto adapt\nto output dimensions corresponding"
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "encoder architectures as PASE [16] and PASE+ [17], respectively.",
          "to speed up training with parallel optimization, while maintaining": ""
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "",
          "to speed up training with parallel optimization, while maintaining": "to the number of classes of each downstream dataset. We train with"
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "1. PASE: We use the same encoder architecture as the original",
          "to speed up training with parallel optimization, while maintaining": ""
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "",
          "to speed up training with parallel optimization, while maintaining": "three scenarios discussed in section 2.2, including supervised, frozen"
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "PASE work [16] with source code implementation1. The ﬁrst layer is",
          "to speed up training with parallel optimization, while maintaining": ""
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "",
          "to speed up training with parallel optimization, while maintaining": "and ﬁne-tuned, all with the same hyper-parameters, Adam optimizer"
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "based on SincNet [21], where the raw input waveform is convolved",
          "to speed up training with parallel optimization, while maintaining": ""
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "",
          "to speed up training with parallel optimization, while maintaining": "[25] with initial\nlearning rate as 0.001 and early stopping criteria"
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "with a set of parameterized Sinc functions implementing rectangular",
          "to speed up training with parallel optimization, while maintaining": ""
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "",
          "to speed up training with parallel optimization, while maintaining": "with patience value of 10 on validation loss. We run 10 trials for"
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "band-pass ﬁlters. The authors claim that SincNet has fewer parame-",
          "to speed up training with parallel optimization, while maintaining": ""
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "",
          "to speed up training with parallel optimization, while maintaining": "each experiment in this paper to get statistically meaningful results."
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "ters and provides better interpretability. SincNet layer is followed by",
          "to speed up training with parallel optimization, while maintaining": ""
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "7 one-dimensional convolutional blocks, batch normalization [22],",
          "to speed up training with parallel optimization, while maintaining": ""
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "and multi-parametric rectiﬁed linear unit activation [23]. We use the",
          "to speed up training with parallel optimization, while maintaining": "3.5. Data"
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "same model parameters as provided in the original work including",
          "to speed up training with parallel optimization, while maintaining": ""
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "kernel widths, number of ﬁlters, and strides. The set of parameters",
          "to speed up training with parallel optimization, while maintaining": "3.5.1. AudioSet for pre-training"
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "for convolutional layers emulates a 10ms sliding window.",
          "to speed up training with parallel optimization, while maintaining": ""
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "",
          "to speed up training with parallel optimization, while maintaining": "We use clips in AudioSet [26] with ”Music” label for pre-training."
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "2. PASE+: PASE+ [17]\nimproves upon PASE [16] by adding",
          "to speed up training with parallel optimization, while maintaining": ""
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "",
          "to speed up training with parallel optimization, while maintaining": "We are able to acquire ˜2M (97% of the original AudioSet data) clips,"
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "skip connections\nand Quasi-Recurrent Neural Network (QRNN)",
          "to speed up training with parallel optimization, while maintaining": ""
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "",
          "to speed up training with parallel optimization, while maintaining": "within which there are ˜980k clips labeled with ”Music”. We ran-"
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "[24]\nlayers to capture longer-term contextual\ninformation. QRNN",
          "to speed up training with parallel optimization, while maintaining": ""
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "",
          "to speed up training with parallel optimization, while maintaining": "domly select 100k for pre-training, resulting in ˜83 hours of data."
        },
        {
          "A two-stage approach involving unsupervised or self-supervised": "layers consist of\ninterleaved convolutional\nlayers with RNN layers",
          "to speed up training with parallel optimization, while maintaining": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3.5.2. Datasets for downstream evaluation": "",
          "the case for PASE+. This shows that pre-training for encoders with": "larger\ncapacities\nis\nespecially helpful when evaluating on down-"
        },
        {
          "3.5.2. Datasets for downstream evaluation": "OpenMIC [27], Extended Ballroom [28] and FMA Small\n(FMA)",
          "the case for PASE+. This shows that pre-training for encoders with": ""
        },
        {
          "3.5.2. Datasets for downstream evaluation": "",
          "the case for PASE+. This shows that pre-training for encoders with": "stream tasks with limited labeled data. We conducted experiments"
        },
        {
          "3.5.2. Datasets for downstream evaluation": "[29],\nthree publicly available\nclassiﬁcation datasets\nare used for",
          "the case for PASE+. This shows that pre-training for encoders with": ""
        },
        {
          "3.5.2. Datasets for downstream evaluation": "",
          "the case for PASE+. This shows that pre-training for encoders with": "using PASE+ through out the remaining paper as it’s a better encoder"
        },
        {
          "3.5.2. Datasets for downstream evaluation": "downstream evaluation as\nrepresentative\nsamples of well-known",
          "the case for PASE+. This shows that pre-training for encoders with": ""
        },
        {
          "3.5.2. Datasets for downstream evaluation": "",
          "the case for PASE+. This shows that pre-training for encoders with": "for our tasks."
        },
        {
          "3.5.2. Datasets for downstream evaluation": "MIR tasks. These datasets range from different number of clips, clip",
          "the case for PASE+. This shows that pre-training for encoders with": ""
        },
        {
          "3.5.2. Datasets for downstream evaluation": "duration, and number of classes.\nFor all\nthree datasets, we report",
          "the case for PASE+. This shows that pre-training for encoders with": ""
        },
        {
          "3.5.2. Datasets for downstream evaluation": "",
          "the case for PASE+. This shows that pre-training for encoders with": "4.2. Weighting mechanisms"
        },
        {
          "3.5.2. Datasets for downstream evaluation": "macro F1 scores as shown in the ﬁgures.",
          "the case for PASE+. This shows that pre-training for encoders with": ""
        },
        {
          "3.5.2. Datasets for downstream evaluation": "1. OpenMIC [27]: OpenMIC is a multi-label\ninstrument clas-",
          "the case for PASE+. This shows that pre-training for encoders with": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MIR tasks. These datasets range from different number of clips, clip": "duration, and number of classes.\nFor all\nthree datasets, we report"
        },
        {
          "MIR tasks. These datasets range from different number of clips, clip": ""
        },
        {
          "MIR tasks. These datasets range from different number of clips, clip": "macro F1 scores as shown in the ﬁgures."
        },
        {
          "MIR tasks. These datasets range from different number of clips, clip": "1. OpenMIC [27]: OpenMIC is a multi-label\ninstrument clas-"
        },
        {
          "MIR tasks. These datasets range from different number of clips, clip": "siﬁcation dataset containing 15k samples total with provided"
        },
        {
          "MIR tasks. These datasets range from different number of clips, clip": "train/valid/test splits as well as masks for strong positive and"
        },
        {
          "MIR tasks. These datasets range from different number of clips, clip": "negative examples for each class. We follow similar setup as"
        },
        {
          "MIR tasks. These datasets range from different number of clips, clip": "the ofﬁcial baseline3 by training 20 binary classiﬁers."
        },
        {
          "MIR tasks. These datasets range from different number of clips, clip": "2. Extended Ballroom [28]: Extended Ballroom (4k samples)"
        },
        {
          "MIR tasks. These datasets range from different number of clips, clip": "is a multi-class dance genre classiﬁcation dataset. We follow"
        },
        {
          "MIR tasks. These datasets range from different number of clips, clip": "the same setup as [30] by removing 4 categories due to dataset"
        },
        {
          "MIR tasks. These datasets range from different number of clips, clip": "imbalance, resulting in only using 9 categories."
        },
        {
          "MIR tasks. These datasets range from different number of clips, clip": "3.\nFMA Small [29]: FMA Small (8k samples) is a multi-class"
        },
        {
          "MIR tasks. These datasets range from different number of clips, clip": "music genre classiﬁcation dataset with 8 genre categories."
        },
        {
          "MIR tasks. These datasets range from different number of clips, clip": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "normalize for timbre, which is important for instrumentation. MFCC": "only hurts slightly on Extended Ballroom as it brings together dif-",
          "playing in different speed. Adding MFCC and Chroma further helps": "differentiate Foxtrot with Rumba and Viennesewaltz as additional"
        },
        {
          "normalize for timbre, which is important for instrumentation. MFCC": "ferent dance genres with similar\ntimbre, and separates music from",
          "playing in different speed. Adding MFCC and Chroma further helps": "timbre cues are provided."
        },
        {
          "normalize for timbre, which is important for instrumentation. MFCC": "same dance genre that changes in timbre.",
          "playing in different speed. Adding MFCC and Chroma further helps": "In Figure 7, we observe that even adding MFCC (WLP+M -"
        },
        {
          "normalize for timbre, which is important for instrumentation. MFCC": "",
          "playing in different speed. Adding MFCC and Chroma further helps": "WLP) helps\nin general as hypothesized, however,\nit misclassiﬁes"
        },
        {
          "normalize for timbre, which is important for instrumentation. MFCC": "",
          "playing in different speed. Adding MFCC and Chroma further helps": "Electronic with Hip-Hop and International, and Pop with Hip-Hop"
        },
        {
          "normalize for timbre, which is important for instrumentation. MFCC": "",
          "playing in different speed. Adding MFCC and Chroma further helps": "and Rock, as there might be similar instruments used in these gen-"
        },
        {
          "normalize for timbre, which is important for instrumentation. MFCC": "",
          "playing in different speed. Adding MFCC and Chroma further helps": "res,\nresulting in similar\ntimbre.\nAdding Tempogram (WLP+T -"
        },
        {
          "normalize for timbre, which is important for instrumentation. MFCC": "",
          "playing in different speed. Adding MFCC and Chroma further helps": "WLP+M) corrects the mistakes made on Electronic and Pop genres,"
        },
        {
          "normalize for timbre, which is important for instrumentation. MFCC": "",
          "playing in different speed. Adding MFCC and Chroma further helps": "but misclassifying International with Folk and Instrumental.\nFi-"
        },
        {
          "normalize for timbre, which is important for instrumentation. MFCC": "",
          "playing in different speed. Adding MFCC and Chroma further helps": "nally, adding both workers (WLP+MT - WLP+T) provides further"
        },
        {
          "normalize for timbre, which is important for instrumentation. MFCC": "",
          "playing in different speed. Adding MFCC and Chroma further helps": "improvements upon MFCC and Tempogram only.\nIn general we"
        },
        {
          "normalize for timbre, which is important for instrumentation. MFCC": "",
          "playing in different speed. Adding MFCC and Chroma further helps": "observe improvements with positive values\n(red)\nin diagonal and"
        },
        {
          "normalize for timbre, which is important for instrumentation. MFCC": "",
          "playing in different speed. Adding MFCC and Chroma further helps": "negative (blue) in off-diagonal."
        },
        {
          "normalize for timbre, which is important for instrumentation. MFCC": "Fig. 5. Relative improvement (%) of different additional music spe-",
          "playing in different speed. Adding MFCC and Chroma further helps": ""
        },
        {
          "normalize for timbre, which is important for instrumentation. MFCC": "ciﬁc workers included during pre-training compared to WLP on dif-",
          "playing in different speed. Adding MFCC and Chroma further helps": ""
        },
        {
          "normalize for timbre, which is important for instrumentation. MFCC": "",
          "playing in different speed. Adding MFCC and Chroma further helps": "4.4. Frozen versus ﬁne-tuned"
        },
        {
          "normalize for timbre, which is important for instrumentation. MFCC": "ferent downstream tasks.",
          "playing in different speed. Adding MFCC and Chroma further helps": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6. REFERENCES": "",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": "tala, Suchismita Padhy, Anthony Ndirango, Gokce Keskin, and"
        },
        {
          "6. REFERENCES": "[1] Keunwoo Choi, George Fazekas, and Mark Sandler,\n“Auto-",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": "Oguz H Elibol, “A comparison of loss weighting strategies for"
        },
        {
          "6. REFERENCES": "IS-\nmatic tagging using deep convolutional neural networks,”",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": "multi\ntask learning in deep neural networks,”\nIEEE Access,"
        },
        {
          "6. REFERENCES": "MIR 2016, 2016.",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": "vol. 7, pp. 141627–141632, 2019."
        },
        {
          "6. REFERENCES": "[2]\nJaehun Kim, Juli´an Urbano, Cynthia CS Liem, and Alan Han-",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": "[16]\nSantiago Pascual, Mirco Ravanelli, Joan Serr`a, Antonio Bona-"
        },
        {
          "6. REFERENCES": "jalic,\n“One deep music representation to rule them all?\na",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": "fonte, and Yoshua Bengio, “Learning problem-agnostic speech"
        },
        {
          "6. REFERENCES": "comparative analysis of different representation learning strate-",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": "INTER-\nrepresentations from multiple self-supervised tasks,”"
        },
        {
          "6. REFERENCES": "gies,” Neural Computing and Applications, vol. 32, no. 4, pp.",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": "SPEECH, 2019."
        },
        {
          "6. REFERENCES": "1067–1093, 2020.",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": "[17] Mirco Ravanelli,\nJianyuan Zhong, Santiago Pascual, Pawel"
        },
        {
          "6. REFERENCES": "[3] Keunwoo\nChoi,\nGy¨orgy\nFazekas,\nMark\nSandler,\nand",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": "Swietojanski, Joao Monteiro, Jan Trmal, and Yoshua Bengio,"
        },
        {
          "6. REFERENCES": "Kyunghyun Cho,\n“Transfer\nlearning for music classiﬁcation",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": "“Multi-task self-supervised learning for robust speech recogni-"
        },
        {
          "6. REFERENCES": "and regression tasks,” in ISMIR 2017. International Society for",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": "tion,” in ICASSP 2020. IEEE, 2020, pp. 6989–6993."
        },
        {
          "6. REFERENCES": "Music Information Retrieval, 2017, pp. 141–149.",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": "[18]\nFranz De Leon and Kirk Martinez,\n“Enhancing timbre model"
        },
        {
          "6. REFERENCES": "[4]\nJan W¨ulﬁng and Martin A Riedmiller, “Unsupervised learning",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": "using mfcc and its time derivatives for music similarity estima-"
        },
        {
          "6. REFERENCES": "of local features for music classiﬁcation.,” in ISMIR, 2012, pp.",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": "the 20th European Signal Pro-\ntion,”\nin 2012 Proceedings of"
        },
        {
          "6. REFERENCES": "139–144.",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": "cessing Conference (EUSIPCO). IEEE, 2012, pp. 2005–2009."
        },
        {
          "6. REFERENCES": "",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": "[19] Daniel PW Ellis,\n“Classifying music audio with timbral and"
        },
        {
          "6. REFERENCES": "[5]\nSteffen\nSchneider, Alexei Baevski, Ronan Collobert,\nand",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": "chroma features,” 2007."
        },
        {
          "6. REFERENCES": "Michael Auli, “wav2vec: Unsupervised pre-training for speech",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": ""
        },
        {
          "6. REFERENCES": "recognition,” arXiv preprint arXiv:1904.05862, 2019.",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": "[20]\nPeter Grosche, Meinard M¨uller, and Frank Kurth, “Cyclic tem-"
        },
        {
          "6. REFERENCES": "",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": "pogram—a mid-level\ntempo representation for musicsignals,”"
        },
        {
          "6. REFERENCES": "[6] Alexei Baevski, Steffen Schneider, and Michael Auli,\n“vq-",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": "in 2010 IEEE International Conference on Acoustics, Speech"
        },
        {
          "6. REFERENCES": "wav2vec: Self-supervised learning of discrete speech represen-",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": "and Signal Processing. IEEE, 2010, pp. 5522–5525."
        },
        {
          "6. REFERENCES": "tations,” arXiv preprint arXiv:1910.05453, 2019.",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": "[21] Mirco Ravanelli and Yoshua Bengio,\n“Speaker\nrecognition"
        },
        {
          "6. REFERENCES": "[7] Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Si-",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": "from raw waveform with sincnet,” in 2018 IEEE Spoken Lan-"
        },
        {
          "6. REFERENCES": "monyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, An-",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": "guage Technology Workshop (SLT).\nIEEE, 2018, pp. 1021–"
        },
        {
          "6. REFERENCES": "drew Senior, and Koray Kavukcuoglu, “Wavenet: A generative",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": "1028."
        },
        {
          "6. REFERENCES": "model for raw audio,” arXiv preprint arXiv:1609.03499, 2016.",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": "[22]\nSergey Ioffe and Christian Szegedy, “Batch normalization: Ac-"
        },
        {
          "6. REFERENCES": "[8] Kundan Kumar, Rithesh Kumar, Thibault de Boissiere, Lucas",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": "celerating deep network training by reducing internal covariate"
        },
        {
          "6. REFERENCES": "Gestin, Wei Zhen Teoh, Jose Sotelo, Alexandre de Br´ebisson,",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": "shift,” Proc. of ICML, 2015."
        },
        {
          "6. REFERENCES": "Yoshua Bengio, and Aaron C Courville, “Melgan: Generative",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": ""
        },
        {
          "6. REFERENCES": "adversarial networks for conditional waveform synthesis,”\nin",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": "[23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun,"
        },
        {
          "6. REFERENCES": "Advances in Neural Information Processing Systems, 2019, pp.",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": "“Delving deep into rectiﬁers: Surpassing human-level perfor-"
        },
        {
          "6. REFERENCES": "14910–14921.",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": "mance on imagenet classiﬁcation,” in Proceedings of the IEEE"
        },
        {
          "6. REFERENCES": "",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": "international conference on computer vision, 2015, pp. 1026–"
        },
        {
          "6. REFERENCES": "[9]\nJason Cramer, Ho-Hsiang Wu, Justin Salamon, and Juan Pablo",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": "1034."
        },
        {
          "6. REFERENCES": "Bello, “Look, listen, and learn more: Design choices for deep",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": "[24]\nJames Bradbury, Stephen Merity, Caiming Xiong, and Richard"
        },
        {
          "6. REFERENCES": "audio embeddings,”\nin ICASSP 2019. IEEE, 2019, pp. 3852–",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": "Proc. of\nSocher,\n“Quasi-recurrent neural networks,”\nICLR,"
        },
        {
          "6. REFERENCES": "3856.",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": "2017."
        },
        {
          "6. REFERENCES": "[10] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": "[25] Diederik P Kingma and Jimmy Ba,\n“Adam: A method for"
        },
        {
          "6. REFERENCES": "offrey Hinton,\n“A simple framework for contrastive learning",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": "arXiv\npreprint\nstochastic\noptimization,”\narXiv:1412.6980,"
        },
        {
          "6. REFERENCES": "of visual\nrepresentations,”\narXiv preprint arXiv:2002.05709,",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": "2014."
        },
        {
          "6. REFERENCES": "2020.",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": "[26]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren"
        },
        {
          "6. REFERENCES": "[11] Ting Chen,\nSimon Kornblith, Kevin Swersky, Mohammad",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": "Jansen, Wade Lawrence, R Channing Moore, Manoj Plakal,"
        },
        {
          "6. REFERENCES": "Norouzi,\nand Geoffrey Hinton,\n“Big self-supervised mod-",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": "and Marvin Ritter, “Audio set: An ontology and human-labeled"
        },
        {
          "6. REFERENCES": "arXiv\npreprint\nels\nare\nstrong\nsemi-supervised\nlearners,”",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": "dataset\nfor audio events,”\nin ICASSP 2017.\nIEEE, 2017, pp."
        },
        {
          "6. REFERENCES": "arXiv:2006.10029, 2020.",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": "776–780."
        },
        {
          "6. REFERENCES": "[12] Beat Gfeller, Christian Frank, Dominik Roblek, Matt Shariﬁ,",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": "[27] Eric Humphrey, Simon Durand, and Brian McFee, “Openmic-"
        },
        {
          "6. REFERENCES": "Marco Tagliasacchi, and Mihajlo Velimirovi´c,\n“Spice: Self-",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": "2018: An open data-set for multiple instrument recognition.,”"
        },
        {
          "6. REFERENCES": "supervised pitch estimation,” IEEE/ACM Transactions on Au-",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": "in ISMIR, 2018."
        },
        {
          "6. REFERENCES": "dio, Speech, and Language Processing, vol. 28, pp. 1118–",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": "[28] Ugo Marchand and Geoffroy Peeters, “The extended ballroom"
        },
        {
          "6. REFERENCES": "1128, 2020.",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": "dataset,” ISMIR Late-breaking Session, 2016."
        },
        {
          "6. REFERENCES": "[13] Yun-Ning Hung, Yi-An Chen, and Yi-Hsuan Yang, “Multitask",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": "[29] Micha¨el Defferrard, Kirell Benzi, Pierre Vandergheynst, and"
        },
        {
          "6. REFERENCES": "learning for\nframe-level\ninstrument\nrecognition,”\nin ICASSP",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": "Xavier Bresson, “Fma: A dataset for music analysis,” in 18th"
        },
        {
          "6. REFERENCES": "2019. IEEE, 2019, pp. 381–385.",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": "International Society for Music Information Retrieval Confer-"
        },
        {
          "6. REFERENCES": "[14] Alex Kendall, Yarin Gal, and Roberto Cipolla,\n“Multi-task",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": "ence, 2017."
        },
        {
          "6. REFERENCES": "learning using uncertainty to weigh losses for scene geome-",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": "[30] Yeonwoo Jeong, Keunwoo Choi, and Hosan Jeong,\n“Dlr: To-"
        },
        {
          "6. REFERENCES": "the IEEE conference\ntry and semantics,”\nin Proceedings of",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": "ward a deep learned rhythmic representation for music content"
        },
        {
          "6. REFERENCES": "on computer vision and pattern recognition, 2018, pp. 7482–",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": ""
        },
        {
          "6. REFERENCES": "",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": "analysis,” arXiv preprint arXiv:1712.05119, 2017."
        },
        {
          "6. REFERENCES": "7491.",
          "[15] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchin-": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Automatic tagging using deep convolutional neural networks",
      "authors": [
        "Keunwoo Choi",
        "George Fazekas",
        "Mark Sandler"
      ],
      "year": "2016",
      "venue": "Automatic tagging using deep convolutional neural networks"
    },
    {
      "citation_id": "3",
      "title": "One deep music representation to rule them all? a comparative analysis of different representation learning strategies",
      "authors": [
        "Jaehun Kim",
        "Julián Urbano",
        "Cynthia Liem",
        "Alan Hanjalic"
      ],
      "year": "2020",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "4",
      "title": "Transfer learning for music classification and regression tasks",
      "authors": [
        "Keunwoo Choi",
        "György Fazekas",
        "Mark Sandler",
        "Kyunghyun Cho"
      ],
      "year": "2017",
      "venue": "ISMIR 2017. International Society for Music Information Retrieval"
    },
    {
      "citation_id": "5",
      "title": "Unsupervised learning of local features for music classification",
      "authors": [
        "Jan Wülfing",
        "Martin Riedmiller"
      ],
      "year": "2012",
      "venue": "ISMIR"
    },
    {
      "citation_id": "6",
      "title": "wav2vec: Unsupervised pre-training for speech recognition",
      "authors": [
        "Steffen Schneider",
        "Alexei Baevski",
        "Ronan Collobert",
        "Michael Auli"
      ],
      "year": "2019",
      "venue": "wav2vec: Unsupervised pre-training for speech recognition",
      "arxiv": "arXiv:1904.05862"
    },
    {
      "citation_id": "7",
      "title": "vqwav2vec: Self-supervised learning of discrete speech representations",
      "authors": [
        "Alexei Baevski",
        "Steffen Schneider",
        "Michael Auli"
      ],
      "year": "2019",
      "venue": "vqwav2vec: Self-supervised learning of discrete speech representations",
      "arxiv": "arXiv:1910.05453"
    },
    {
      "citation_id": "8",
      "title": "Wavenet: A generative model for raw audio",
      "authors": [
        "Aaron Van Den Oord",
        "Sander Dieleman",
        "Heiga Zen",
        "Karen Simonyan",
        "Oriol Vinyals",
        "Alex Graves",
        "Nal Kalchbrenner",
        "Andrew Senior",
        "Koray Kavukcuoglu"
      ],
      "year": "2016",
      "venue": "Wavenet: A generative model for raw audio",
      "arxiv": "arXiv:1609.03499"
    },
    {
      "citation_id": "9",
      "title": "Melgan: Generative adversarial networks for conditional waveform synthesis",
      "authors": [
        "Kundan Kumar",
        "Rithesh Kumar",
        "Lucas Thibault De Boissiere",
        "Gestin",
        "Jose Wei Zhen Teoh",
        "Alexandre Sotelo",
        "Yoshua De Brébisson",
        "Aaron Bengio",
        "Courville"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "10",
      "title": "Look, listen, and learn more: Design choices for deep audio embeddings",
      "authors": [
        "Jason Cramer",
        "Ho-Hsiang Wu",
        "Justin Salamon",
        "Juan Bello"
      ],
      "year": "2019",
      "venue": "Look, listen, and learn more: Design choices for deep audio embeddings"
    },
    {
      "citation_id": "11",
      "title": "A simple framework for contrastive learning of visual representations",
      "authors": [
        "Ting Chen",
        "Simon Kornblith",
        "Mohammad Norouzi",
        "Geoffrey Hinton"
      ],
      "year": "2020",
      "venue": "A simple framework for contrastive learning of visual representations",
      "arxiv": "arXiv:2002.05709"
    },
    {
      "citation_id": "12",
      "title": "Big self-supervised models are strong semi-supervised learners",
      "authors": [
        "Ting Chen",
        "Simon Kornblith",
        "Kevin Swersky",
        "Mohammad Norouzi",
        "Geoffrey Hinton"
      ],
      "year": "2020",
      "venue": "Big self-supervised models are strong semi-supervised learners",
      "arxiv": "arXiv:2006.10029"
    },
    {
      "citation_id": "13",
      "title": "Spice: Selfsupervised pitch estimation",
      "authors": [
        "Beat Gfeller",
        "Christian Frank",
        "Dominik Roblek",
        "Matt Sharifi",
        "Marco Tagliasacchi",
        "Mihajlo Velimirović"
      ],
      "year": "2020",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "14",
      "title": "Multitask learning for frame-level instrument recognition",
      "authors": [
        "Yun-Ning Hung",
        "Yi-An Chen",
        "Yi-Hsuan Yang"
      ],
      "year": "2019",
      "venue": "Multitask learning for frame-level instrument recognition"
    },
    {
      "citation_id": "15",
      "title": "Multi-task learning using uncertainty to weigh losses for scene geometry and semantics",
      "authors": [
        "Alex Kendall",
        "Yarin Gal",
        "Roberto Cipolla"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "16",
      "title": "A comparison of loss weighting strategies for multi task learning in deep neural networks",
      "authors": [
        "Ting Gong",
        "Tyler Lee",
        "Cory Stephenson",
        "Venkata Renduchintala",
        "Suchismita Padhy",
        "Anthony Ndirango",
        "Gokce Keskin",
        "H Oguz",
        "Elibol"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "17",
      "title": "Learning problem-agnostic speech representations from multiple self-supervised tasks",
      "authors": [
        "Santiago Pascual",
        "Mirco Ravanelli",
        "Joan Serrà",
        "Antonio Bonafonte",
        "Yoshua Bengio"
      ],
      "year": "2019",
      "venue": "INTER-SPEECH"
    },
    {
      "citation_id": "18",
      "title": "Multi-task self-supervised learning for robust speech recognition",
      "authors": [
        "Mirco Ravanelli",
        "Jianyuan Zhong",
        "Santiago Pascual",
        "Pawel Swietojanski",
        "Joao Monteiro",
        "Jan Trmal",
        "Yoshua Bengio"
      ],
      "venue": "Multi-task self-supervised learning for robust speech recognition"
    },
    {
      "citation_id": "19",
      "title": "Enhancing timbre model using mfcc and its time derivatives for music similarity estimation",
      "authors": [
        "Franz De",
        "Kirk Martinez"
      ],
      "year": "2012",
      "venue": "2012 Proceedings of the 20th European Signal Processing Conference"
    },
    {
      "citation_id": "20",
      "title": "Classifying music audio with timbral and chroma features",
      "authors": [
        "Ellis Daniel"
      ],
      "year": "2007",
      "venue": "Classifying music audio with timbral and chroma features"
    },
    {
      "citation_id": "21",
      "title": "Cyclic tempogram-a mid-level tempo representation for musicsignals",
      "authors": [
        "Peter Grosche",
        "Meinard Müller",
        "Frank Kurth"
      ],
      "year": "2010",
      "venue": "2010 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "22",
      "title": "Speaker recognition from raw waveform with sincnet",
      "authors": [
        "Mirco Ravanelli",
        "Yoshua Bengio"
      ],
      "year": "2018",
      "venue": "2018 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "23",
      "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "authors": [
        "Sergey Ioffe",
        "Christian Szegedy"
      ],
      "year": "2015",
      "venue": "Proc. of ICML"
    },
    {
      "citation_id": "24",
      "title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE international conference on computer vision"
    },
    {
      "citation_id": "25",
      "title": "Quasi-recurrent neural networks",
      "authors": [
        "James Bradbury",
        "Stephen Merity",
        "Caiming Xiong",
        "Richard Socher"
      ],
      "year": "2017",
      "venue": "Proc. of ICLR"
    },
    {
      "citation_id": "26",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "27",
      "title": "Audio set: An ontology and human-labeled dataset for audio events",
      "authors": [
        "Jort F Gemmeke",
        "P Daniel",
        "Dylan Ellis",
        "Aren Freedman",
        "Wade Jansen",
        "R Channing Lawrence",
        "Manoj Moore",
        "Marvin Plakal",
        "Ritter"
      ],
      "year": "2017",
      "venue": "Audio set: An ontology and human-labeled dataset for audio events"
    },
    {
      "citation_id": "28",
      "title": "Openmic-2018: An open data-set for multiple instrument recognition",
      "authors": [
        "Eric Humphrey",
        "Simon Durand",
        "Brian Mcfee"
      ],
      "year": "2018",
      "venue": "ISMIR"
    },
    {
      "citation_id": "29",
      "title": "The extended ballroom dataset",
      "authors": [
        "Ugo Marchand",
        "Geoffroy Peeters"
      ],
      "year": "2016",
      "venue": "ISMIR Late-breaking Session"
    },
    {
      "citation_id": "30",
      "title": "Fma: A dataset for music analysis",
      "authors": [
        "Michaël Defferrard",
        "Kirell Benzi",
        "Pierre Vandergheynst",
        "Xavier Bresson"
      ],
      "year": "2017",
      "venue": "18th International Society for Music Information Retrieval Conference"
    },
    {
      "citation_id": "31",
      "title": "Dlr: Toward a deep learned rhythmic representation for music content analysis",
      "authors": [
        "Yeonwoo Jeong",
        "Keunwoo Choi",
        "Hosan Jeong"
      ],
      "year": "2017",
      "venue": "Dlr: Toward a deep learned rhythmic representation for music content analysis",
      "arxiv": "arXiv:1712.05119"
    }
  ]
}