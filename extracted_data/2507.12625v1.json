{
  "paper_id": "2507.12625v1",
  "title": "Mapping Emotions In The Brain: A Bi-Hemispheric Neural Model With Explainable Deep Learning",
  "published": "2025-07-16T20:39:58Z",
  "authors": [
    "David Freire-Obregón",
    "Agnieszka Dubiel",
    "Prasoon Kumar Vinodkumar",
    "Gholamreza Anbarjafari",
    "Dorota Kamińska",
    "Modesto Castrillón-Santana"
  ],
  "keywords": [
    "EEG",
    "Emotion Recognition",
    "Explainable AI",
    "LIME",
    "Hemispheric Specialization"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Recent advances have shown promise in emotion recognition from electroencephalogram (EEG) signals by employing bi-hemispheric neural architectures that incorporate neuroscientific priors into deep learning models. However, interpretability remains a significant limitation for their application in sensitive fields such as affective computing and cognitive modeling. In this work, we introduce a post-hoc interpretability framework tailored to dual-stream EEG classifiers, extending the Local Interpretable Model-Agnostic Explanations (LIME) approach to accommodate structured, bi-hemispheric inputs. Our method adapts LIME to handle structured two-branch inputs corresponding to left and righthemisphere EEG channel groups. It decomposes prediction relevance into per-channel contributions across hemispheres and emotional classes. We apply this framework to a previously validated dual-branch recurrent neural network trained on EmoNeuroDB, a dataset of EEG recordings captured during a VR-based emotion elicitation task. The resulting explanations reveal emotion-specific hemispheric activation patterns consistent with known neurophysiological phenomena, such as frontal lateralization in joy and posterior asymmetry in sadness. Furthermore, we aggregate local explanations across samples to derive global channel importance profiles, enabling a neurophysiologically grounded interpretation of the model's decisions. Correlation analysis between symmetric electrodes further highlights the model's emotion-dependent lateralization behavior, supporting the functional asymmetries reported in affective neuroscience.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Emotion plays a central role in human behavior, influencing decision-making, communication, and social interaction  [11, 17] . Accurately recognizing emotional states is essential for the development of affective computing systems, braincomputer interfaces, and emotion-aware agents  [1] . Among the various modalities for emotion recognition, electroencephalography (EEG) provides a non-invasive and direct measure of brain activity with high temporal resolution, making it a promising source of information for decoding internal affective states.\n\nRecent work has demonstrated that neural architectures inspired by brain organization, particularly hemispheric specialization, can enhance EEG-based emotion recognition. In this context, bi-hemispheric models, which process signals from the left and right hemispheres independently, have shown improved performance by capturing asymmetrical neural activation patterns associated with emotional processing  [13] . A dual-stream recurrent neural network architecture  [8] , designed to reflect hemispheric specialization, was proposed and evaluated in the FG2024 Brain Responses to Emotional Avatars Challenge  [4] . This model demonstrated competitive performance on the EmoNeuroDB dataset, which comprises EEG recordings collected during immersive virtual reality scenarios involving emotionally expressive avatars.\n\nDespite these advances, a critical barrier remains: the lack of interpretability in deep learning models applied to neurophysiological data. Most high-performing models operate as black boxes, providing limited insight into the decision-making process and hindering their adoption in clinical, cognitive, and ethically sensitive applications. Understanding why a model associates specific EEG patterns with emotions, as well as the brain regions and temporal segments involved in that inference, is essential for aligning machine learning decisions with neuroscientific knowledge.\n\nTo address this gap, we propose an explainability framework that utilizes Local Interpretable Model-Agnostic Explanations (LIME) in conjunction with a bi-hemispheric EEG model  [16] . The method adapts LIME to structured dual-input architectures, enabling the quantification of channel-wise importance across hemispheres and emotional categories. Through this approach, model predictions are aligned with neurophysiological plausibility, offering interpretable, localized, and emotion-specific insights into how deep models process affective brain signals. The objective of this work is three-fold: (i) to identify which EEG features and channels contribute most significantly to emotion classification, (ii) to visualize these relevance patterns by mapping them onto brain cortical maps for each emotional category, and (iii) to analyze cross-emotion relevance profiles, revealing potential shared or divergent neural substrates among emotions. interaction (HCI)  [2] . Despite this promise, emotion classification from EEG remains difficult due to the signals' nonlinear and high-dimensional nature. Emotion modeling often follows either a discrete (e.g., joy, anger) or dimensional (e.g., valence, arousal) approach, with the latter being more prevalent in EEG studies  [14] . Standard pipelines include preprocessing, feature extraction (e.g., spectral power, connectivity measures), and classification. Classifiers range from topology-invariant methods, such as Support Vector Machines (SVMs), to topology-aware models, including Convolutional Neural Networks (CNNs) and Graph Neural Networks (GNNs)  [5] . However, many struggle with long-range dependencies, which recent dual-stream RNN architectures aim to address by separately processing hemispheric signals with spatially-informed designs  [18] .\n\nHemispheric Specialization in Affective Neuroscience. The integration of deep learning methods such as Recurrent Neural Networks (RNNs), CNNs, and GCNs with classical signal processing techniques has significantly improved classification performance in emotion recognition tasks  [6, 7] . Moreover, the adoption of transfer learning and domain adaptation has enhanced the generalizability of models across different individuals and experimental setups  [9] .\n\nMultimodal approaches have also emerged as a promising direction. By combining EEG with complementary signals, such as electrocardiogram (ECG) or facial expressions, studies have improved the robustness and ecological validity of emotion classification  [10, 12] . For instance, the DREAMER dataset integrates EEG and ECG using affordable consumer-grade equipment to support emotion detection in real-world settings. However, it introduces trade-offs in data quality and generalization  [12] .\n\nExplainable AI for EEG. Finally, several surveys have systematized the methodological landscape of EEG-based affective computing. For example, some studies highlight the diversity of approaches in stimuli presentation, emotion elicitation, and classification strategies, advocating for multimodal integration and deeper neurophysiological interpretability  [15] . These overviews underscore the field's shift from purely performance-driven goals toward models that are also explainable and aligned with neuroscientific knowledge.\n\nIn this context, our work builds upon the emerging trend of incorporating domain priors, such as hemispheric specialization, into the model design and complements it with post-hoc interpretability techniques. Specifically, we adapt the LIME framework to dual-branch RNN architectures, aiming to bridge the gap between high-performing black-box models and the need for interpretable, neurophysiologically grounded emotion decoding.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Methodology",
      "text": "Our methodological pipeline consists of two main components: a dual-stream neural architecture designed to exploit hemispheric asymmetries in EEG data and a post-hoc interpretability framework based on LIME adapted to such structured input. We first describe the bi-hemispheric classifier used for emotion  recognition, which serves as the backbone for our interpretability analysis. Then, we detail how LIME was extended to operate on this architecture, allowing us to extract channel-level relevance scores for each emotional category.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Bi-Hemispheric Emotion Classifier",
      "text": "To evaluate our interpretability framework, we apply LIME to the bi-hemispheric neural model for EEG-based emotion recognition  [8] . According to the authors, this model was designed to leverage hemispheric specialization by processing leftand right-brain signals through parallel but independent branches (see Figure  1 ). Each branch receives frequency-domain EEG features extracted from one hemisphere, allowing the network to capture lateralized patterns of emotional processing.\n\nInput Representation. The EEG signals are first re-referenced to mastoid electrodes, band-pass filtered, and corrected for filter-induced delay. Then, the signals are transformed into the frequency domain using the Fast Fourier Transform (FFT), resulting in magnitude-based spectral representations. Electrodes are grouped into left and right hemispheres based on standard EEG montages, forming two structurally distinct inputs to the model.\n\nDual-Branch Architecture. As depicted in Figure  1 , the architecture consists of two symmetrical yet non-weight-sharing streams, each responsible for processing one hemisphere's input. Within each stream, a 1D convolutional layer captures local spectral patterns, followed by max pooling and dropout for regularization. The resulting features are reshaped and passed through a Long Short-Term Memory (LSTM) layer, enabling the model to learn temporal dependencies in the EEG signal dynamics. Another dropout layer is applied after the LSTM to mitigate overfitting.\n\nFusion and Classification. Outputs from both hemispheric branches are flattened and concatenated, producing a joint representation that is then passed through a dense layer with ReLU activation and L2 regularization. A softmax output layer maps the representation to six emotion classes (e.g., joy, sadness, fear, anger, surprise, and disgust) optimized using categorical cross-entropy loss and the Adam optimizer.\n\nChallenge Performance. The bi-hemispheric model achieved an average validation accuracy of 22.78% on the competition leaderboard, securing the second-highest score among all submitted solutions. Notably, it outperformed all provided baselines, including SVM (17.78%), LightGBM (19.44%), and Random Forests (19.44%). In class-wise performance, this model demonstrated strong recognition of emotions such as joy (43.33%) and anger (33.33%), while maintaining balanced performance across other categories. These results underscore the effectiveness of incorporating hemispheric specialization into EEG-based emotion recognition, providing a strong foundation for our subsequent interpretability analysis. In this work, rather than modifying the model architecture or training process, we focus on augmenting it with post-hoc interpretability through LIME in order to understand the specific contributions of channels, frequencies, and hemispheres to the final emotion predictions.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Adapting Lime To Dual-Stream Eeg Models",
      "text": "LIME is a model-agnostic technique that explains individual predictions by learning a local surrogate model, typically a sparse linear regressor, around each instance of interest. It perturbs the input and observes output changes to infer feature importances. The relevance of each input component is estimated based on its contribution to the local decision boundary  [16] .\n\nTo interpret the predictions of the bi-hemispheric model, we extend the LIME framework to support structured, dual-input neural architectures. The model under study receives EEG signals separately from the left and right hemispheres, which are then processed through independent branches. Since standard LIME is designed for flat, tabular input, we adapt it to handle this two-stream input format while preserving the spatial structure of the EEG signals.\n\nLet Φ ∈ R C×F be the frequency-domain EEG representation obtained after preprocessing and FFT, where C denotes the number of channels, and F is the number of frequency bins. The spectral data is split into two hemispheres:\n\nEach hemisphere-specific input is processed through a sequence of operations: a 1D convolution, followed by max-pooling, dropout, and a recurrent (LSTM) layer. Let f conv , f pool , f drop , f lstm represent these operations. The latent representations from each branch are:\n\nThese vectors are concatenated as h = [h L ; h R ] ∈ R 2H and passed through a dense layer and softmax to predict one of K = 6 emotion classes.\n\nFlattening and Concatenating Inputs. Each EEG sample consists of two matrices, one for each hemisphere. To make the input compatible with LIME, we reshape both matrices into 1D vectors and concatenate them, producing a single flattened vector x ∈ R C•F that includes all channels and frequency features.\n\nCustom Prediction Function. To ensure that LIME can still query the original model correctly, we define a custom prediction function that reverses the process of flattening. Let x L ∈ R C L •F and x R ∈ R C R •F represent the flattened left and right hemisphere features. The full input is x = [x L ; x R ]. We define:\n\nwhere reshape L and reshape R restore the original 2D format required by the model. This step is essential for maintaining architectural consistency during perturbation-based explanation.\n\nGenerating Explanations. For each sample, we apply LIME using the custom predictor and generate feature importance scores specific to the predicted emotion class. Perturbations are sampled around the original input, and their impact on the model's output is used to fit a local, interpretable model.\n\nMapping Importances Back to EEG Channels. Once local feature importances are computed for the flattened input, we re-map these values to their original EEG channels. We calculate the average importance score across frequency bins to obtain a single relevance score per channel. This is done separately for the left and right hemispheres, yielding a per-channel relevance profile that aligns with the bi-hemispheric structure.\n\nFinally, we apply this procedure across all validation samples and aggregate the results. For each instance, we record the predicted class, ground truth label, class probabilities, and the corresponding LIME explanation. This adaptation enables LIME to provide interpretable, channel-level explanations for dualstream EEG models, preserving the spatial organization of brain signals while exposing the internal reasoning of the classifier.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Experiments And Results",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experimental Setup",
      "text": "Dataset. We conduct our experiments using the EmoNeuroDB dataset  [4] , a multimodal corpus designed to support EEG-based emotion recognition in immersive virtual reality environments. The dataset comprises recordings from 40 participants (balanced by gender) who engaged in a VR-based emotional mimicry task. Each subject was exposed to an avatar displaying one of six fundamental emotions (joy, sadness, anger, fear, disgust, and surprise) and instructed to imitate the expression while wearing an EEG headset. Recordings were made with the DSI-24 wireless EEG system, using 21 dry electrodes arranged according to the 10-20 International System. Each emotion was repeated three times per subject, with each trial lasting approximately 15 seconds at a sampling rate Parameters and Evaluation Metrics. The bi-hemispheric model is trained using the Adam optimizer and categorical cross-entropy loss. Input data are fed as frequency-domain features into two parallel branches (left and right hemispheres). Following the training setup described by the model's original authors, we use a batch size of 64, train for up to 200 epochs, and apply early stopping with a learning rate schedule. For LIME-based analysis, 5K perturbed samples are used per explanation, and channel-level relevance scores are aggregated across validation samples for group-level interpretation. Model performance is primarily assessed using classification accuracy on the validation set. To facilitate a deeper analysis, we also compute confusion matrices and class-wise performance metrics to examine the model's ability to distinguish between emotions. These metrics provide insight into potential asymmetries or misclassifications relevant to emotion decoding.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Local Explanations Per Hemisphere",
      "text": "To gain insight into the spatial relevance of the EEG signals per emotional class, we analyzed the average influence of each electrode on the left and right hemispheres separately. Figures  2  and 3   Fig.  3 . Influence of the right hemisphere electrodes on each emotion.\n\nOn the left hemisphere (Figure  2 ), certain frontal and central electrodes show consistently high influence. Notably, Fp1, F3, and Fz exhibit high relevance for multiple emotions. For sadness, electrodes Fp1, O1, F3, T5, and Fz stand out with influence values above 0.8. In contrast, joy shows a more distributed pattern, with peaks at P3, T3, Cz, and A1. Fear and disgust display moderate influence in temporal and parietal regions such as T3, P3, and T5. Interestingly, Fz remains influential across most emotional states, suggesting its central role in emotion decoding from the left side.\n\nOn the right hemisphere (Figure  3 ), the influence pattern varies more strongly across emotions. For example, during joy, electrodes F8, F4, and T4 show dominant influence (above 0.9), suggesting right-lateralized activation in this emotional state. In contrast, anger is characterized by maximal influence in posterior and temporal electrodes like T6, P4, and O2. The central electrodes Cz and Fz also contribute significantly across emotions, although with more variability compared to their left counterparts. Fp2 and F8 are especially important for surprise and fear.\n\nOverall, these local explanation maps reveal how different brain regions contribute to emotion classification and highlight notable asymmetries between hemispheres. For instance, emotional states such as joy and anger activate distinct electrode patterns across the two hemispheres, underscoring the importance of spatial decoding in EEG-based emotion recognition.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Topographic Relevance Maps",
      "text": "To further interpret the spatial dynamics of emotional processing in EEG signals, we generated topographic relevance maps based on local explanation scores per electrode. These maps visualize the most influential brain regions across both hemispheres for each emotion, using averaged local surrogate explanations over the validation set.\n\nThe relevance maps, shown in Figure  4 , clearly highlight distinct spatial activation patterns associated with different emotional states. Each electrode on the topographic maps is color-coded based on its average influence score: red indicates very high relevance (≥ 0.75), orange indicates high relevance (0.50-0.74), green indicates moderate relevance (0.25-0.49), and blue represents low relevance (< 0.25). For example, sadness and fear exhibit strong influence in frontal and occipital regions of the left hemisphere, notably around electrodes Fp1, F7, and O1. In contrast, anger and joy show stronger activations on the right side, particularly in temporal and parietal areas such as T6, P4, and F8.\n\nInterestingly, the central electrode Fz demonstrates consistent relevance across most emotions, suggesting its role as a bilateral integrator in emotional processing. The lateral asymmetry observed, with emotions like disgust showing more balanced activation, while surprise is left-dominant, reinforces the importance of hemispheric specialization in EEG-based emotion decoding. These topographic insights offer a valuable perspective on how different brain regions contribute to emotion classification and validate the spatial patterns observed in the left and right hemisphere bar plots.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Symmetric Electrode Correlations Across Emotions",
      "text": "To explore the degree of functional symmetry between brain hemispheres under different emotional states, we computed the Pearson correlation coefficients between pairs of symmetric electrodes (e.g., Fp1-Fp2, F3-F4 ) across all instances labeled with the same emotion. These correlations were based on binarized importance values derived from the LIME explanations, where activations were thresholded and encoded as binary indicators of relevance.\n\nFigure  5  shows a heatmap depicting the average correlation values for each emotion across the 11 predefined symmetric electrode pairs. Higher correlation values indicate stronger bilateral engagement, while lower or negative values may suggest hemispheric asymmetries in how the model attends to features during classification. The color gradient in the heatmap spans from blue (negative correlation) to red (strong positive correlation), highlighting emotion-specific patterns of inter-hemispheric activation. Notably, emotions such as joy and surprise exhibited higher bilateral symmetry in frontal and parietal regions, while fear and disgust revealed more lateralized activation profiles.\n\nThe correlation analysis between symmetric electrode pairs provides insight into the lateralization of brain activity patterns during emotion classification. Overall, the results indicate variable degrees of hemispheric symmetry depending on the emotional category, suggesting neural activation strategies adopted by the model across affective states.\n\nEmotions such joy and surprise consistently showed high positive correlations across multiple symmetric pairs, particularly in frontal (Fp1-Fp2, F3-F4 ) and parietal (P3-P4 ) regions, implying that these affective states may elicit more balanced bilateral processing. This aligns with existing neurophysiological evidence suggesting that positive emotions tend to involve more symmetric or bilateral engagement of cortical areas, especially in non-pathological populations. contrast, emotions like fear and disgust demonstrated lower or even negative correlations in several symmetric pairs, indicating more asymmetric activation patterns. For instance, fear displayed weak correlations in posterior and temporal regions, suggesting a potential lateralization effect, possibly reflecting right-hemisphere dominance often reported in affective neuroscience literature for threat-related stimuli.\n\nBearing in mind that, according to Figure  1 , both hemispheres include the central region, sadness exhibited a mixed profile, with moderate symmetry in central regions (e.g., Cz-Cz, Fz-Fz ) but reduced correlation in occipital areas, which may suggest more localized or unbalanced visual processing during the classification of this emotion.\n\nThese findings reflect that the model does not rely on uniform patterns of bilateral activation across emotions. Instead, its decision-making process appears to be emotion-specific, recruiting lateralized or symmetric cortical features depending on the affective state being classified. This behavior aligns with known functional asymmetries in the brain's emotional circuitry, reinforcing the importance of considering hemispheric dynamics in affective computing systems.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Conclusions",
      "text": "This work introduces an explainable AI framework for interpreting bi-hemispheric EEG-based emotion recognition models by extending LIME to structured dualinput networks. The approach yields per-channel relevance maps that reveal how affective brain signals are processed across hemispheres.\n\nOur analysis uncovers consistent emotion-specific hemispheric patterns: left frontal and central regions are more influential for sadness and fear, while righthemispheric activity dominates for joy, anger, and disgust. Relevance maps offer clear visualizations of these dynamics. Correlation analyses between symmetric electrode pairs show higher inter-hemispheric synchrony for positive emotions (e.g., joy, surprise) and more lateralized patterns for negative ones (e.g., fear, disgust), aligning with prior neuroscience findings  [3] .\n\nThese findings demonstrate that deep learning models trained on EEG signals can capture meaningful spatial and lateralized patterns associated with emotional processing. The proposed framework bridges the gap between model accuracy and neuroscientific insight by introducing a tailored LIME extension for dual-input networks, enabling spatially grounded, emotion-specific explanations that are statistically robust and generalizable across subjects.",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Diagram illustrating the three-module stages for bi-hemispheric emotion recog-",
      "page": 4
    },
    {
      "caption": "Figure 1: ). Each branch receives frequency-domain EEG features extracted from one",
      "page": 4
    },
    {
      "caption": "Figure 1: , the architecture con-",
      "page": 4
    },
    {
      "caption": "Figure 2: Influence of the left hemisphere electrodes on each emotion.",
      "page": 7
    },
    {
      "caption": "Figure 3: Influence of the right hemisphere electrodes on each emotion.",
      "page": 8
    },
    {
      "caption": "Figure 2: ), certain frontal and central electrodes",
      "page": 8
    },
    {
      "caption": "Figure 3: ), the influence pattern varies more",
      "page": 8
    },
    {
      "caption": "Figure 4: Topographic relevance maps. Red indicates very high relevance (≥0.75), orange",
      "page": 9
    },
    {
      "caption": "Figure 4: , clearly highlight distinct spatial acti-",
      "page": 9
    },
    {
      "caption": "Figure 5: Average correlation of binarized relevance scores between symmetric electrode",
      "page": 10
    },
    {
      "caption": "Figure 5: shows a heatmap depicting the average correlation values for each",
      "page": 10
    },
    {
      "caption": "Figure 1: , both hemispheres include the",
      "page": 11
    }
  ],
  "tables": [],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotions recognition using EEG signals: A survey",
      "authors": [
        "S Alarcao",
        "M Fonseca"
      ],
      "year": "2017",
      "venue": "IEEE Trans. on Affective Computing"
    },
    {
      "citation_id": "2",
      "title": "Neural correlates of social and nonsocial emotions: An fMRI study",
      "authors": [
        "J Britton",
        "K Phan",
        "S Taylor",
        "R Welsh",
        "K Berridge",
        "I Liberzon"
      ],
      "year": "2006",
      "venue": "NeuroImage"
    },
    {
      "citation_id": "3",
      "title": "Anterior cerebral asymmetry and the nature of emotion",
      "authors": [
        "R Davidson"
      ],
      "year": "1992",
      "venue": "Brain and Cognition"
    },
    {
      "citation_id": "4",
      "title": "Brain responses to emotional avatars challenge: Dataset and results",
      "authors": [
        "A Dubiel",
        "D Kamińska",
        "G Zwoliński",
        "A Jafari",
        "P Vinodkumar",
        "E Avots",
        "J Jacques",
        "S Escalera",
        "G Anbajafari"
      ],
      "year": "2024",
      "venue": "Int. Conf. on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "5",
      "title": "Emotion recognition from physiological signal analysis: A review",
      "authors": [
        "M Egger",
        "M Ley",
        "S Hanke"
      ],
      "year": "2019",
      "venue": "Electronic Notes in Theoretical Computer Science"
    },
    {
      "citation_id": "6",
      "title": "Smile detection using local binary patterns and support vector machines",
      "authors": [
        "D Freire-Obregón",
        "M Castrillón-Santana",
        "O Déniz-Suárez"
      ],
      "year": "2009",
      "venue": "Int. Conf. on Computer Vision Theory and Applications"
    },
    {
      "citation_id": "7",
      "title": "Towards facial expression robustness in multi-scale wild environments",
      "authors": [
        "D Freire-Obregón",
        "D Hernández-Sosa",
        "O Santana",
        "J Lorenzo-Navarro",
        "M Castrillón-Santana"
      ],
      "year": "2023",
      "venue": "Image Analysis and Processing -ICIAP 2023"
    },
    {
      "citation_id": "8",
      "title": "Towards Bi-Hemispheric Emotion Mapping Through EEG: A Dual-Stream Neural Network Approach",
      "authors": [
        "D Freire-Obregón",
        "D Hernández-Sosa",
        "O Santana",
        "J Lorenzo-Navarro",
        "M Castrillón-Santana"
      ],
      "year": "2024",
      "venue": "Int. Conf. on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "9",
      "title": "Node-Wise Domain Adaptation Based on Transferable Attention for Recognizing Road Rage via EEG",
      "authors": [
        "X Gao",
        "C Xu",
        "Y Song",
        "J Hu",
        "J Xiao",
        "Z Meng"
      ],
      "year": "2023",
      "venue": "Int. Conf. on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "10",
      "title": "Feature Extraction and Selection for Emotion Recognition from EEG",
      "authors": [
        "R Jenke",
        "A Peer",
        "M Buss"
      ],
      "year": "2014",
      "venue": "IEEE Trans. on Affective Computing"
    },
    {
      "citation_id": "11",
      "title": "The subjective experience of emotion: a fearful view",
      "authors": [
        "J Ledoux",
        "S Hofmann"
      ],
      "year": "2018",
      "venue": "Current Opinion in Behavioral Sciences"
    },
    {
      "citation_id": "12",
      "title": "Hierarchical convolutional neural networks for eeg-based emotion recognition",
      "authors": [
        "J Li",
        "Z Zhang",
        "H He"
      ],
      "year": "2017",
      "venue": "Cognitive Computation"
    },
    {
      "citation_id": "13",
      "title": "A novel bi-hemispheric discrepancy model for eeg emotion recognition",
      "authors": [
        "Y Li",
        "L Wang",
        "W Zheng",
        "Y Zong",
        "L Qi",
        "Z Cui",
        "T Zhang",
        "T Song"
      ],
      "year": "2021",
      "venue": "IEEE Trans. on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "14",
      "title": "Emotions detection using facial expressions recognition and eeg",
      "authors": [
        "T Matlovic",
        "P Gaspar",
        "R Moro",
        "J Simko",
        "M Bielikova"
      ],
      "year": "2016",
      "venue": "Int. Workshop on Semantic and Social Media Adaptation and Personalization"
    },
    {
      "citation_id": "15",
      "title": "Toward an affect-sensitive multimodal humancomputer interaction",
      "authors": [
        "M Pantic",
        "L Rothkrantz"
      ],
      "year": "2003",
      "venue": "Proc. IEEE"
    },
    {
      "citation_id": "16",
      "title": "Why Should I Trust You?\": Explaining the Predictions of Any Classifier",
      "authors": [
        "M Ribeiro",
        "S Singh",
        "C Guestrin"
      ],
      "year": "2016",
      "venue": "ACM SIGKDD Int. Conf. on Knowledge Discovery and Data Mining"
    },
    {
      "citation_id": "17",
      "title": "Multimodal emotion recognition based on a fusion of audiovisual information with temporal dynamics",
      "authors": [
        "J Salas-Cáceres",
        "J Lorenzo-Navarro",
        "D Freire-Obregón",
        "M Castrillón-Santana"
      ],
      "year": "2024",
      "venue": "Multimodal emotion recognition based on a fusion of audiovisual information with temporal dynamics"
    },
    {
      "citation_id": "18",
      "title": "Facial expression recognition via learning deep sparse autoencoders",
      "authors": [
        "N Zeng",
        "H Zhang",
        "B Song",
        "W Liu",
        "Y Li",
        "A Dobaie"
      ],
      "year": "2018",
      "venue": "Neurocomputing"
    }
  ]
}