{
  "paper_id": "2407.11300v1",
  "title": "Large Vision-Language Models As Emotion Recognizers In Context Awareness",
  "published": "2024-07-16T01:28:06Z",
  "authors": [
    "Yuxuan Lei",
    "Dingkang Yang",
    "Zhaoyu Chen",
    "Jiawei Chen",
    "Peng Zhai",
    "Lihua Zhang"
  ],
  "keywords": [
    "Emotion Recognition",
    "Context understanding",
    "Large Vision-Language Models",
    "In-Context Learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Context-aware emotion recognition (CAER) is a complex and significant task that requires perceiving emotions from various contextual cues. Previous approaches primarily focus on designing sophisticated architectures to extract emotional cues from images. However, their knowledge is confined to specific training datasets and may reflect the subjective emotional biases of the annotators. Furthermore, acquiring large amounts of labeled data is often challenging in real-world applications. In this paper, we systematically explore the potential of leveraging Large Vision-Language Models (LVLMs) to empower the CAER task from three paradigms: 1) We fine-tune LVLMs on two CAER datasets, which is the most common way to transfer large models to downstream tasks. 2) We design zero-shot and few-shot patterns to evaluate the performance of LVLMs in scenarios with limited data or even completely unseen. In this case, a training-free framework is proposed to fully exploit the In-Context Learning (ICL) capabilities of LVLMs. Specifically, we develop an image similarity-based ranking algorithm to retrieve examples; subsequently, the instructions, retrieved examples, and the test example are combined to feed LVLMs to obtain the corresponding sentiment judgment. 3) To leverage the rich knowledge base of LVLMs, we incorporate Chain-of-Thought (CoT) into our framework to enhance the model's reasoning ability and provide interpretable results. Extensive experiments and analyses demonstrate that LVLMs achieve competitive performance in the CAER task across different paradigms. Notably, the superior performance in few-shot settings indicates the feasibility of LVLMs for accomplishing specific tasks without extensive training.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "As a crucial factor in mutual understanding, friendly communication, and maintaining longterm relationships, emotions play a vital role in our daily lives. In recent years, emotion recognition tasks have garnered increasing attention. Researchers are striving to identify and analyze the complex emotions expressed by humans from various modalities such as vision, language, and speech. So far, this technology has been applied in numerous fields  (Pepa et al. (2021) ;  Yang et al. (2023b) ). In the past, research on visual emotion analysis has primarily concentrated on facial expressions  Jiang et al. (2020) , with most facial recognition datasets providing cropped facial images. However, this does not fully align with the way humans perceive emotions through visual cues. Typically, in natural scenes, we do not only observe a person's facial expressions but also their body language (such as gestures and postures), the environment, and their interactions with others. These elements collectively contribute to our assessment of emotional states.  Kosti et al. (2019)  recognized this aspect and introduced the task of Context-Aware Emotion Recognition (CAER). They combined contextual and environmental factors in static images to construct and release the EMOTIC dataset, making a significant contribution to the field of CAER. Due to the complexity and subjectivity of human emotions, fine-grained emotion recognition is challenging even for human experts. Different individuals may interpret the emotional context of the same image differently. Therefore, it can be said that there is no exact correct answer in emotion recognition. Traditional CAER models  (Kosti et al. (2017) ;  Lee et al. (2019) ;  Zhang et al. (2019) ) are typically trained and tested on fixed datasets. It raises issues of generalization, as the knowledge capacity of traditionally trained models is limited to the specific datasets they are trained on. Additionally, CAER models trained on particular datasets may learn the annotators' subjective emotional bias biases, which could cause confusion for others. Second, in real-world scenarios, it is not always feasible to obtain large amounts of labeled data. When a model is transferred to an unseen sentiment domain, collecting substantial data and retraining the model incur significant costs.\n\nLarge Language Models (LLMs)  (Touvron et al. (2023) ;  Achiam et al. (2023) ) have learned human thought processes from billions of data points. Their massive parameter size translates to a vast knowledge capacity, enabling them to generalize well to different downstream tasks even in few-shot or zero-shot scenarios. Large Vision-Language Models (LVLMs)  (Liu et al. (2024a) ;  Lin et al. (2024) ) combine the capabilities of LLMs with visual understanding, allowing the powerful abilities of these models to be transferred to the visual domain. By learning the correlation between vision and language from extensive image-text data, LVLMs have achieved remarkable results in tasks such as image classification  (Radford et al. (2021) ), object segmentation  (Zhang et al. (2024) ), and visual question answering (VQA)  (Alayrac et al. (2022) ). However, as far as we know, the exploration of LVLMs in the CAER task remains relatively underdeveloped.\n\nThis paper aims to explore the potential of leveraging various paradigms of LVLMs to enhance the CAER task. Specifically, we first perform supervised fine-tuning (SFT) on the LVLMs, which is the most common way for generalizing large models to downstream tasks. However, in real-world scenarios, it is often challenging to obtain large amounts of labeled data, and the knowledge distribution of fine-tuned models may be disrupted. Therefore, we extend our investigation to include zero-shot and few-shot paradigms. For the few-shot setting, we aim to leverage the In-Context Learning (ICL)  (Brown et al. (2020) ) capability of LVLMs to achieve emotion recognition with small samples. This paradigm allows LVLMs to improve their emotion recognition performance by learning from a few examples within the context, without updating the model parameters. During the few-shot inference, LVLMs do not rely on learning the distribution of the dataset to make predictions. Instead, it generates decisions by learning from relevant tasks and leveraging its own extensive knowledge, which avoids the impact of annotation biases inherent in the dataset. It has been provided that downstream performance is highly sensitive to the choice of in-context demonstrations by  Zhang et al. (2024) , and a good in-context demonstration should be semantically similar to the query. To achieve this, we propose a training-free framework to fully exploit the ICL capability of LVLMs. Within our framework, we design a demonstration retrieval module based on different contexts to retrieve the top-k examples most similar to the test sample. This allows us to fully consider various aspects of the images when selecting example samples. Then, we construct a prompt template that packages the top-k selected examples ranked by similarity into the template. Finally, we concatenate the instructions, demonstrations, and target test sample to serve as input to the LVLMs and obtain the corresponding sentiment judgment.\n\nFurthermore, to fully leverage the internal knowledge of large models, we extend the proposed framework to a few-shot Chain-of-Thought (CoT) paradigm. We utilize GPT-4 Vision's  (Achiam et al. (2023) ) powerful instruction-following capability to generate CoT rationales for each demonstration. These rationales are concatenated with the corresponding example images and provided as input to guide the model in using its rich knowledge for reasoning and simultaneously providing interpretable results.\n\nOur primary contributions are summarized below.\n\n(1) This paper synthetically explores the potential and promise of various paradigms of LVLMs applied to the field of traditional visual emotion recognition. (2) We design a training-free framework to effectively apply the ICL paradigm to the CAER task. The proposed demonstration retrieval module retrieves demonstrations with the highest semantic similarity to the current test image from both the person context and scene context. This approach fully harnesses the ICL capability of LVLMs. (3) We conduct extensive experiments and perform a comprehensive analysis of the results to elucidate the impact of different paradigms on the emotion analysis performance of LVLMs. Our findings show that large models, even without parameter updates, can achieve competitive results in the CAER task, even surpassing traditional methods.",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Context-Aware Emotion Recognition",
      "text": "The expression of emotions is often multifaceted. In visual emotion recognition tasks, facial expressions are commonly regarded as the most expressive visual information for conveying emotions, and much of the existing work has focused on facial expression analysis  (Jiang et al. (2020) ). However, in uncontrolled natural scenes, human emotions typically need to be inferred from a combination of visual information, including facial expressions, body movements, interactions, and the surrounding context. Recently, there have been numerous attempts to address context-aware emotion recognition tasks.  Kosti et al. (2019)  introduced the EMOTIC dataset to encourage and support the Context-Aware Emotion Recognition (CAER) task, proposing a dual-stream convolutional architecture to separately extract information from human bodies and the entire scene. Similarly,  Lee et al. (2019)  proposed a dual-stream architecture, but in contrast to EMOTIC, one branch is used to extract facial features while the other branch processes the entire image with the facial information masked as context.  Yang et al. (2022)  released the HECO dataset for the CAER task and presented a novel multi-stream emotion recognition framework that incorporates four context information.  Bose et al. (2023)  utilizes pre-trained vision-language models (VLMs) to extract descriptions of foreground context from images and proposes a multimodal context fusion module that combines foreground cues with visual scenes and human-centered contextual information for emotion prediction.  Mittal et al. (2020)  considered multiple signals such as facial expressions, postures, backgrounds, and depth to predict specific human emotions.  Yang et al. (2023a Yang et al. ( , 2024) )  introduced the causal modeling patterns to address spurious correlations between context and emotions caused by harmful context biases.\n\nAlthough the above works have shown good performance on the CAER task, their capabilities are ultimately limited to the training datasets, resulting in poor generalization in real-world scenarios. Unlike these traditional training-based methods, we aim to explore the performance of various paradigms of LVLMs in the CAER task, seeking better approaches to overcome the limitations of traditional methods.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Large Models In Affective Analysis",
      "text": "As the scale of pre-trained language models continues to expand, the comprehension and generation capabilities of LLMs are rapidly advancing at an astonishing pace. Recently, several works have focused on exploring the emotion analysis capabilities of these large models.  Lei et al. (2023)  designed a retrieval template module and two emotional alignment tasks, utilizing fine-tuning to reform emotion recognition in conversation (ERC). Similarly,  Zhang et al. (2023)  had contributed to the development of large models in the ERC task. They fine-tuned the models using conversational context and emotional knowledge and incorporated multimodal information by converting videos into text for the fine-tuning process.  Liu et al. (2024b)  proposed a series of annotation tools for comprehensive affective analysis based on fine-tuning various LLMs with instruction data.\n\nThe aforementioned works are based on LLMs and primarily focus on emotion analysis in the field of NLP.  Zhao and Patras (2023)  proposed DFER-CLIP, which is based on the CLIP  (Radford et al. (2021) ) model and specifically designed for dynamic facial expression recognition (DFER) in the wild.  Cheng et al. (2024)  proposed Emotion-LLaMA, which integrates audio, visual, and text inputs for multimodal emotional recognition and reasoning.  Lian et al. (2023)  comprehensively explored the performance of GPT-4 Vison  (Achiam et al. (2023) ) on generalized emotion recognition tasks.  Xenos et al. (2024)  utilized LLaVA  (Liu et al. (2024a) ) to generate textual descriptions of images as auxiliary information for training a multimodal architecture for the CAER task, but it did not fully explore the emotion recognition capabilities of LVLMs. Different with  Xenos et al. (2024) , we are not using LVLMs as auxiliary tools, but rather exploring their inherent emotion analysis capabilities in the CAER task.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Proposed Framework For Few-Shot Setting",
      "text": "Figure  1  illustrates the overall architecture of our framework for the CAER task on the 2-shot setting. It is mainly composed of three parts: the Demonstration Retrieval Module, the Prompt Template Designing Module, and the Inference Module.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Demonstration Retrieval Module",
      "text": "Previous works  (Zhang et al. (2024) ) have emphasized the importance of demonstration selection in ICL and shown that examples with semantic similarity to the test sample can effectively enhance the performance of LVLMs. Traditional visual demonstration retrieval methods typically rely on image similarity. However, for the CAER task, considering only the overall image similarity is insufficient. On one hand, a single image may contain multiple annotated individuals, and each of them may express different emotions. Consequently, the images retrieved for different individuals should be distinct. On the other hand, the similarity between person context and scene context should be considered separately to prevent either context from being overlooked. For instance, when the proportion of the person context in an image is small, scene context may dominate the consideration, leading to the loss of important information. Therefore, we propose a demonstration retrieval module that jointly considers the similarity of person context and visual scene context. This approach ensures that both contexts are equally valued, facilitating the selection of the most appropriate demonstrations for different individuals within the same image.\n\n(1)\n\nScene Context. In addition to the explicit context of the individual, the surrounding scene semantics are essential for understanding emotions. To account for the influence of scene context, we designate the remaining part of the image I after cropping out the labeled individual as the visual scene context I scene ,\n\n(2)\n\nImage Similarity Rank. Given the test example I q and a set of candidate demonstration images D = {I 1 , I 2 , ..., I N }, where I i denotes the i-th candidate image. First, we crop each test image and candidate image into person and scene components, then encode these components to obtain their representations F q = (f person q , f scene q ) and\n\nThen we separately calculate the similarity of F q and F D ,\n\nand then record the rank R q ∈ R N of each candidate image in D,\n\nCompared to ranking based on overall image similarity scores, the method of ranking based on the similarity scores of two distinct contexts better accounts for the influence of different contexts on the emotional semantics of images, thereby providing optimal demonstrations for the ICL learning of LVLMs. Finally, we take the top-k images with the highest similarity ranking as selected demonstrations.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Prompt Template Designing Module",
      "text": "To better transfer the capabilities of LVLMs to the CAER task, we reframe the CAER task as a generative task. We construct a prompt template to bridge the gap when applying LVLMs to a subtask. As shown in Figure  1 , for the CAER task, each input consists of three parts: Instruction, Demonstration, and Test Sample.\n\nInstruction. At the beginning of the input, we first provide LVLM with a clear task instruction and a label statement as follows: \"Given the list of emotion labels: {class names}, please choose which emotion is more suitable for describing how the person in the red box feels\", where {class names} are substituted for the list of class names available in each dataset. Specifically, for multi-label classification tasks, we replace 'emotion is' with 'emotions are' to prevent LVLMs from producing a single-label output.\n\nDemonstration. Each of our demonstrations is an image-answering pair {I, A}. We select the top-k demonstrations obtained by the proposed demonstration retrieval module and concatenate them to form the overall demonstration.\n\nwhere σ are the indices of top-k similarity ranking, and σ = {σ 1 , ..., σ k }. Specifically, for the few-shot CoT paradigm, our demonstrations include not only the images and labels but also the rationales that explain why the image corresponds to the given emotion labels,\n\nThe rationale is generated by GPT-4 Vision (Achiam et al. (  2023 )) due to its powerful instruction-following capabilities. Figure  2  shows an example of generated rationale. The final demonstration is expressed as follows:\n\n1. **Scene**:\n\n-The person is behind a counter with food items, likely in a food service or retail environment. This setting might indicate a context where the person is about to serve or present food to customers, which aligns with anticipation and engagement in their role.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "**Interactions Between People**:",
      "text": "-There are other individuals present, both behind and in front of the counter. The person in the red box might be interacting with colleagues or customers. This social interaction can foster a sense of engagement and anticipation. -The person appears to be in the midst of an action, possibly communicating or positioning themselves to handle an item or address someone. His body posture and positioning suggest attentiveness and readiness, reflecting confidence and engagement in his task. This poised and active stance is characteristic of someone prepared and focused on their immediate activity.\n\nThese combined factors create a scene where the individual in the red box is likely experiencing anticipation towards serving or interacting with customers, confidence in their role or task, and overall engagement in the current activities.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Gt: Anticipation, Confidence, Engagement",
      "text": "Given an image and the emotion label of the person in the red box: Anticipation, Confidence, Engagement, analyze why this person is expressing the emotion from the following possible aspects: the scene, interactions between people, the actions and expressions of the person in the red box, etc. Please only select the aspects that you believe convey emotional information for your analysis. Keep your response concise and clear.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "User Gpt-4 Vision",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Inference Module",
      "text": "Following the instruction and demonstration, we append the test sample at the end of the prompt as input to the LVLM and select the most likely generated sequence as the output,\n\nOutput q = LV LM (Input q , θ).\n\nFinally, we post-process the output sequence to obtain the list of predicted labels. Since the LVLM treats the CAER task as a text generation process, its output consists of textual emotion labels rather than probabilities. Consequently, we cannot use mAP (mean Average Precision) as an evaluation metric, as was done in previous works  (Kosti et al. (2019) ;  Mittal et al. (2020) ). In this work, we report Precision, Recall, F1 Score, Hamming Loss, and multi-label Accuracy on the EMOTIC (both micro average and macro average). For the HECO, we use the Precision, Recall, F1 Score on macro average, and the standard classification accuracy for evaluation. Note that a smaller Hamming Loss and larger Accuracy, Precision, Recall, and F1 Score indicate better classification quality.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Experimental Setup",
      "text": "Extensive experiments are conducted on two LVLMs, including LLAVA-7B  (Liu et al. (2024a) ) and VILA-8B  (Lin et al. (2024) ). We find that LLAVA struggles with handling multiple image inputs, and its instruction-following capability significantly decreases as the number of input images increases. Therefore, for the few-shot setting, we only report the 2-shot results for LLAVA. For the EMOTIC and HECO datasets, We randomly select 200 examples from the training set as the candidate demonstrations set and chose k examples from the set for the k-shot ICL task. Specifically, due to the difference in the average number of labels between the EMOTIC training set and validation set, we additionally randomly sample 200 examples from the EMOTIC validation set as a candidate set to assess the impact of the number of demonstration labels on few-shot prediction results. To facilitate subsequent processing of the generated text, we specify \"reply with label(s) only\" for zero-shot prompts, while for few-shot tasks, the output format is demonstrated in the examples. All experiments are conducted on four Nvidia A800-80G GPUs. For the fine-tuning approach, we set the learning rate to 1e-5 and the batch size to 8.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Results And Analysis",
      "text": "The results of zero-shot setting, supervised fine-tuning (SFT) setting, few-shot setting, and few-shot CoT setting are shown in Table  1  and Table 2 . Due to the differing task characteristics and complexities of the EMOTIC and HECO (i.e., EMOTIC being a multilabel classification task and HECO a single-label classification task), we analyze the results of the two datasets separately.\n\nFor the EMOTIC, the fine-tuned LLAVA achieves the highest precision. VILA performs few-shot learning by retrieving demonstrations from the validation subset, obtaining the highest recall and F1 score. Compared to the trained EMOT-Net  (Kosti et al. (2017) ), the fine-tuned LVLMs exhibit significantly higher precision but considerably lower recall. This suggests that the fine-tuned LVLMs tend to output the most accurate emotions rather than the most comprehensive ones because the average number of labels in the EMOTIC training set is much smaller than that in the test set. Although EMOT-Net is also trained on the training set, it outputs probability values, unlike LVLMs which generate direct labels. This allows for dynamic threshold adjustment to balance precision and recall, preventing the imbalance seen in LVLMs.\n\nAnother notable observation is the significant gap between the micro average and macro average scores for both the trained EMOT-Net and the fine-tuned LVLMs. The lower macro average scores indicate poor performance on certain emotion categories, while the higher micro average scores suggest that the well-performing emotion categories are more prevalent in the dataset. This discrepancy highlights the issue of imbalanced label distribution in the dataset, consistent with previous findings  (Yang et al. (2024) ). Whether through training or fine-tuning, models are profoundly influenced by the distribution of the training set. For subjective annotations like emotions, this is not ideal. Focusing on fitting the training set's label distribution causes models to learn the annotators' emotional biases, and the imbalanced label distribution leads to insufficient learning of less prevalent emotions. Furthermore, we observe the performance of LVLMs in the few-shot CoT paradigm. We find that the addition of rationale examples increases precision but decreases recall. That's because without CoT, the model likely recognizes simple patterns, but with CoT examples, it needs to learn and focus on more detailed information. This makes the model more stringent and cautious in emotion classification, leading to the omission of some true labels. How to streamline the CoT process, allowing LVLMs to balance resources between analysis and decision-making during inference, will be the focus of our future research. In the zero-shot setting, we find that unguided or non-finetuned LVLMs perform poorly on the CAER task. However, we also notice that the gap between the micro average score and the macro average score is not significant, indicating that untrained LVLMs do not exhibit a clear bias toward specific emotions. In the few-shot paradigm, the demonstrations provided to LVLMs significantly enhance their emotion recognition capabilities. VILA achieves performance comparable to EMOT-Net in the few-shot setting where the retrieval set is derived from the training set. It also shows a better balance between precision and recall, with the gap between micro and macro average scores narrowing considerably. Due to LLAVA's relatively poor ICL capability, its performance in the few-shot setting is subpar. This suggests that our work will be limited by the inherent capabilities of the LVLMs. However, the good results of VILA indicate that LVLMs with good ICL ability can achieve relatively good and balanced emotion analysis performance with just few-shot prompts. When the retrieval set is derived from the validation set, VILA's performance in the F1 Score significantly surpasses the EMOT-Net. We believe this is because the average number of labels in the validation set is closer to that of the test set, resulting in a much higher recall. This also shows that untuned LVLMs still rely on the quality of samples in the retrieval set. However, compared to the training dataset, which contains tens of thousands of samples, the retrieval set is much smaller. Maintaining a high-quality retrieval set is far less costly than maintaining a good training set. More importantly, the few-shot paradigm requires no training, meaning the cost of transferring LVLMs to different scenarios is minimal. This demonstrates the potential of LVLMs in the CAER task, paving the way for promising future developments in this field.\n\ndicates that separately considering person context and scene context is beneficial for the CAER task. Furthermore, considering the similarity of the entire image is more effective than only considering the person context or scene context, demonstrating that each context contributes to retrieving the optimal demonstrations. Figure  3  presents the top 2 demonstrations obtained by our retrieval method and the \"Overall\" retrieval method, along with the prediction results based on these demonstrations. We can see that images retrieved by our method are semantically closer to the test samples (i.e., both expressing negative emotions), whereas the images retrieved based on overall image similarity all express positive emotions. The prediction results indicate that the samples retrieved by our method help the model predict more correct labels.\n\nImpact of Demonstration Number. To investigate the impact of the number of demonstrations on performance, we conduct experiments with varying numbers of examples, as shown in Table  4 . We observe that, on both datasets, most metrics generally improve as the number of examples increases. Our framework performs best on average with 10shot demonstrations. Beyond 10-shot, the improvements stabilize and some metrics even decline. This decline can be attributed to the fact that longer inputs make it difficult for the model to focus on key points, and more demonstrations may include semantically dissimilar examples, which can have a negative effect. Therefore, selecting an appropriate number of demonstrations is crucial for maximizing the potential of LVLMs.",
      "page_start": 9,
      "page_end": 12
    },
    {
      "section_name": "Case Study",
      "text": "Figure  4  presents examples of the CoT results under different settings on the EMOTIC. We find that in the zero-shot setting, while the model's decisions are not poor, the generated rationale does not explain in detail why the person feels this emotion, and in some cases (e.g., Example 3), it is merely a redundant description of the decision. With few demonstrations, the model's CoT improves significantly by learning from the demonstrated CoT to generate rationale from different perspectives. And it often leads to more accurate decisions. CoT with images from the validation subset tends to predict more labels, whereas CoT with images from the training set yields more concise results. We can achieve our desired outcomes by constructing retrieval sets with different characteristics for the model.\n\nAdditionally, we find that although sometimes the model predictions are not always entirely correct, its analysis is subjectively reasonable. For instance, in the first image, the model thinks the person should feel \"Excitement\" in a sports setting, which seems understandable even though this category is not among the ground truth labels of the image. In contrast, the \"Esteem\" label in the ground truth is not clearly perceivable, which we believe reflects the annotator's emotional bias. For the third image, the model provided the label \"Discomfort\", which is not one of the 26 emotion categories specified in the EMOTIC dataset. However, emotions are inherently complex and cannot be fully encapsulated by a limited set of categories. Models trained on specific datasets might ignore or consider \"Discomfort\" as an incorrect decision, but as humans, we recognize it as a right label. This underscores the necessity of our research: emotion analysis should be comprehensive rather than limited. Traditional methods cannot achieve this, whereas large models hold great potential in this regard.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Conclusion",
      "text": "This paper explores the potential and performance of various LVLM paradigms in the CAER task. Our findings indicate that LVLMs can outperform traditional models without additional training through our proposed framework, demonstrating their significant potential in understanding contextual emotions. The powerful reasoning and generalization capabilities of large models provide a promising outlook: we envision that emotion analysis should not be constrained by the limitations of specific datasets but should advance toward a broader and more fine-grained emotional landscape. Future Work. We plan to improve the performance of LVLMs in the Few-shot CoT paradigm by constructing higher-quality CoT demonstrations, guiding the model to generate more logical and comprehensive rationales. -The proximity and physical contact suggest a level of comfort and mutual affection, leading to feelings of affection, esteem, and sympathy. 2. **Expressions**:\n\n-The person is smiling, a clear sign of happiness and confidence.\n\n-The smile and relaxed body language also suggest engagement and anticipation, possibly looking forward to what is happening or enjoying the moment. 3. **Scene**:\n\n-The indoor setting and casual attire imply a relaxed, informal context which can contribute to overall positive emotions like happiness and engagement. Based on these aspects, the person in the red box is likely expressing emotions related to positive social interactions and a pleasant environment.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Prediction: Affection, Confidence, Engagement, Excitement, Happiness, Sympathy",
      "text": "Based on the facial expressions, body language, and interaction, the person in the red box appears to be feeling 'Confidence', 'Excitement', and 'Pleasure'. These emotions are suitable for describing the person's feelings because they are actively engaged in playing tennis, and their body language and facial expressions suggest that they are enjoying the game and feeling confident in their abilities. Based on the facial expressions, body language, and interaction, the person in the red box appears to be feeling 'Happiness', 'Excitement', and 'Engagement'. The person is smiling and has their arm around another person, which suggests a positive and joyful interaction. Additionally, the person's body language, such as their raised fist, conveys a sense of excitement and enthusiasm. Therefore, the most suitable emotions to describe how the person in the red box feels are 'Happiness', 'Excitement', and 'Engagement'.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Prediction: Happiness, Excitement, Engagement",
      "text": "The person in the red box is likely expressing emotions of excitement, happiness, and pleasure due to the following aspects: 1. **Scene**: The background appears to show a professional or social environment, which often promotes positive interactions and feelings of achievement or connection. The pink text highlights the emotions mentioned in the rationale, the red labels indicate correctly predicted labels, and the blue text denotes labels that are not within the GT range.",
      "page_start": 14,
      "page_end": 14
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: illustrates the overall architecture of our framework for the CAER task on the",
      "page": 4
    },
    {
      "caption": "Figure 1: The architecture of our framework on the 2-shot setting. Although we present the",
      "page": 5
    },
    {
      "caption": "Figure 1: , for the CAER task, each input consists of three",
      "page": 6
    },
    {
      "caption": "Figure 2: shows an example of generated rationale. The",
      "page": 7
    },
    {
      "caption": "Figure 2: An example of emotional rationale generated for the EMOTIC dataset using",
      "page": 7
    },
    {
      "caption": "Figure 3: An example of the results obtained using different retrieval methods. The labels",
      "page": 11
    },
    {
      "caption": "Figure 3: presents the top 2 demonstrations obtained by our retrieval method and the",
      "page": 12
    },
    {
      "caption": "Figure 4: presents examples of the CoT results under different settings on the EMOTIC. We",
      "page": 12
    },
    {
      "caption": "Figure 4: Examples of the CoT results on the EMOTIC. Each example from top to bottom",
      "page": 14
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Comparison results on the EMOTIC dataset. The left and right of “/” denote the",
      "page": 9
    },
    {
      "caption": "Table 1: and Table 2. Due to the differing task",
      "page": 9
    },
    {
      "caption": "Table 2: Comparison results on the HECO dataset.",
      "page": 10
    },
    {
      "caption": "Table 2: , the supervised fine-tuned LVLMs outperform traditional models and all",
      "page": 11
    },
    {
      "caption": "Table 3: In this case, “Overall” refers to rank-",
      "page": 11
    },
    {
      "caption": "Table 3: Comparison results of different retrieval methods on EMOTIC and HECO.",
      "page": 12
    },
    {
      "caption": "Table 4: We observe that, on both datasets, most metrics generally improve",
      "page": 12
    },
    {
      "caption": "Table 4: Comparison results of different shot settings on EMOTIC and HECO.",
      "page": 13
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Shyamal Anadkat, et al. Gpt-4 technical report",
      "authors": [
        "Josh Achiam",
        "Steven Adler",
        "Sandhini Agarwal",
        "Lama Ahmad",
        "Ilge Akkaya",
        "Florencia Leoni Aleman",
        "Diogo Almeida",
        "Janko Altenschmidt",
        "Sam Altman"
      ],
      "year": "2023",
      "venue": "Shyamal Anadkat, et al. Gpt-4 technical report",
      "arxiv": "arXiv:2303.08774"
    },
    {
      "citation_id": "2",
      "title": "Flamingo: a visual language model for few-shot learning",
      "authors": [
        "Jean-Baptiste Alayrac",
        "Jeff Donahue",
        "Pauline Luc",
        "Antoine Miech",
        "Iain Barr",
        "Yana Hasson",
        "Karel Lenc",
        "Arthur Mensch",
        "Katherine Millican",
        "Malcolm Reynolds"
      ],
      "year": "2022",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "3",
      "title": "Contextually-rich human affect perception using multimodal scene information",
      "authors": [
        "Digbalay Bose",
        "Rajat Hebbar",
        "Krishna Somandepalli",
        "Shrikanth Narayanan"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "4",
      "title": "Language models are few-shot learners",
      "authors": [
        "Tom Brown",
        "Benjamin Mann",
        "Nick Ryder",
        "Melanie Subbiah",
        "Jared Kaplan",
        "Prafulla Dhariwal",
        "Arvind Neelakantan",
        "Pranav Shyam",
        "Girish Sastry",
        "Amanda Askell"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "5",
      "title": "Learning to detect human-object interactions",
      "authors": [
        "Yu-Wei Chao",
        "Yunfan Liu",
        "Xieyang Liu",
        "Huayi Zeng",
        "Jia Deng"
      ],
      "year": "2018",
      "venue": "2018 ieee winter conference on applications of computer vision (wacv)"
    },
    {
      "citation_id": "6",
      "title": "Emotion-llama: Multimodal emotion recognition and reasoning with instruction tuning",
      "authors": [
        "Zebang Cheng",
        "Zhi-Qi Cheng",
        "Jun-Yan He",
        "Jingdong Sun",
        "Kai Wang",
        "Yuxiang Lin",
        "Zheng Lian",
        "Xiaojiang Peng",
        "Alexander Hauptmann"
      ],
      "year": "2024",
      "venue": "Emotion-llama: Multimodal emotion recognition and reasoning with instruction tuning",
      "arxiv": "arXiv:2406.11161"
    },
    {
      "citation_id": "7",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "Alexey Dosovitskiy",
        "Lucas Beyer",
        "Alexander Kolesnikov",
        "Dirk Weissenborn",
        "Xiaohua Zhai",
        "Thomas Unterthiner",
        "Mostafa Dehghani",
        "Matthias Minderer",
        "Georg Heigold",
        "Sylvain Gelly"
      ],
      "year": "2020",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "arxiv": "arXiv:2010.11929"
    },
    {
      "citation_id": "8",
      "title": "Dfew: A large-scale database for recognizing dynamic facial expressions in the wild",
      "authors": [
        "Xingxun Jiang",
        "Yuan Zong",
        "Wenming Zheng",
        "Chuangao Tang",
        "Wanchuang Xia",
        "Cheng Lu",
        "Jiateng Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM international conference on multimedia"
    },
    {
      "citation_id": "9",
      "title": "Emotion recognition in context",
      "authors": [
        "Ronak Kosti",
        "Jose Alvarez",
        "Adria Recasens",
        "Agata Lapedriza"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "10",
      "title": "Context based emotion recognition using emotic dataset",
      "authors": [
        "Ronak Kosti",
        "Jose Alvarez",
        "Adria Recasens",
        "Agata Lapedriza"
      ],
      "year": "2019",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "11",
      "title": "Contextaware emotion recognition networks",
      "authors": [
        "Jiyoung Lee",
        "Seungryong Kim",
        "Sunok Kim",
        "Jungin Park",
        "Kwanghoon Sohn"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF international conference on computer vision"
    },
    {
      "citation_id": "12",
      "title": "Instructerc: Reforming emotion recognition in conversation with a retrieval multi-task llms framework",
      "authors": [
        "Shanglin Lei",
        "Guanting Dong",
        "Xiaoping Wang",
        "Keheng Wang",
        "Sirui Wang"
      ],
      "year": "2023",
      "venue": "Instructerc: Reforming emotion recognition in conversation with a retrieval multi-task llms framework",
      "arxiv": "arXiv:2309.11911"
    },
    {
      "citation_id": "13",
      "title": "Gpt-4v with emotion: a zero-shot benchmark for multimodal emotion understanding",
      "authors": [
        "Zheng Lian",
        "Licai Sun",
        "Haiyang Sun",
        "Kang Chen",
        "Zhuofan Wen",
        "Hao Gu",
        "Shun Chen",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2023",
      "venue": "Gpt-4v with emotion: a zero-shot benchmark for multimodal emotion understanding",
      "arxiv": "arXiv:2312.04293"
    },
    {
      "citation_id": "14",
      "title": "On pre-training for visual language models",
      "authors": [
        "Ji Lin",
        "Hongxu Yin",
        "Wei Ping",
        "Pavlo Molchanov",
        "Mohammad Shoeybi",
        "Song Han",
        "Vila"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "15",
      "title": "Visual instruction tuning",
      "authors": [
        "Haotian Liu",
        "Chunyuan Li",
        "Qingyang Wu",
        "Yong Jae Lee"
      ],
      "year": "2024",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "16",
      "title": "Emollms: A series of emotional large language models and annotation tools for comprehensive affective analysis",
      "authors": [
        "Zhiwei Liu",
        "Kailai Yang",
        "Tianlin Zhang",
        "Qianqian Xie",
        "Zeping Yu",
        "Sophia Ananiadou"
      ],
      "year": "2024",
      "venue": "Emollms: A series of emotional large language models and annotation tools for comprehensive affective analysis",
      "arxiv": "arXiv:2401.08508"
    },
    {
      "citation_id": "17",
      "title": "Emoticon: Context-aware multimodal emotion recognition using frege's principle",
      "authors": [
        "Trisha Mittal",
        "Pooja Guhan",
        "Uttaran Bhattacharya",
        "Rohan Chandra",
        "Aniket Bera",
        "Dinesh Manocha"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "18",
      "title": "Automatic emotion recognition in clinical scenario: a systematic review of methods",
      "authors": [
        "Lucia Pepa",
        "Luca Spalazzi",
        "Marianna Capecci",
        "Maria Gabriella"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "19",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Chris Hallacy",
        "Aditya Ramesh",
        "Gabriel Goh",
        "Sandhini Agarwal",
        "Girish Sastry",
        "Amanda Askell",
        "Pamela Mishkin",
        "Jack Clark"
      ],
      "year": "2021",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "20",
      "title": "Open and efficient foundation language models",
      "authors": [
        "Hugo Touvron",
        "Thibaut Lavril",
        "Gautier Izacard",
        "Xavier Martinet",
        "Marie-Anne Lachaux",
        "Timothée Lacroix",
        "Baptiste Rozière",
        "Naman Goyal",
        "Eric Hambro",
        "Faisal Azhar"
      ],
      "year": "2023",
      "venue": "Open and efficient foundation language models",
      "arxiv": "arXiv:2302.13971"
    },
    {
      "citation_id": "21",
      "title": "Vllms provide better context for emotion understanding through common sense reasoning",
      "authors": [
        "Alexandros Xenos",
        "Niki Foteinopoulou"
      ],
      "year": "2024",
      "venue": "Ioanna Ntinou, Ioannis Patras, and Georgios Tzimiropoulos",
      "arxiv": "arXiv:2404.07078"
    },
    {
      "citation_id": "22",
      "title": "Emotion recognition for multiple context awareness",
      "authors": [
        "Dingkang Yang",
        "Shuai Huang",
        "Shunli Wang",
        "Yang Liu",
        "Peng Zhai",
        "Liuzhen Su",
        "Mingcheng Li",
        "Lihua Zhang"
      ],
      "year": "2022",
      "venue": "European conference on computer vision"
    },
    {
      "citation_id": "23",
      "title": "Context de-confounded emotion recognition",
      "authors": [
        "Dingkang Yang",
        "Zhaoyu Chen",
        "Yuzheng Wang",
        "Shunli Wang",
        "Mingcheng Li",
        "Siao Liu",
        "Xiao Zhao",
        "Shuai Huang",
        "Zhiyan Dong",
        "Peng Zhai"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "24",
      "title": "Aide: A vision-driven multi-view, multi-modal, multi-tasking dataset for assistive driving perception",
      "authors": [
        "Dingkang Yang",
        "Shuai Huang",
        "Zhi Xu",
        "Zhenpeng Li",
        "Shunli Wang",
        "Mingcheng Li",
        "Yuzheng Wang",
        "Yang Liu",
        "Kun Yang",
        "Zhaoyu Chen"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "25",
      "title": "Robust emotion recognition in context debiasing",
      "authors": [
        "Dingkang Yang",
        "Kun Yang",
        "Mingcheng Li",
        "Shunli Wang",
        "Shuaibing Wang",
        "Lihua Zhang"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "26",
      "title": "Context-aware affective graph reasoning for emotion recognition",
      "authors": [
        "Minghui Zhang",
        "Yumeng Liang",
        "Huadong Ma"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "27",
      "title": "Dialoguellm: Context and emotion knowledge-tuned llama models for emotion recognition in conversations",
      "authors": [
        "Yazhou Zhang",
        "Mengyao Wang",
        "Prayag Tiwari",
        "Qiuchi Li",
        "Benyou Wang",
        "Jing Qin"
      ],
      "year": "2023",
      "venue": "Dialoguellm: Context and emotion knowledge-tuned llama models for emotion recognition in conversations",
      "arxiv": "arXiv:2310.11374"
    },
    {
      "citation_id": "28",
      "title": "What makes good examples for visual in-context learning?",
      "authors": [
        "Yuanhan Zhang",
        "Kaiyang Zhou",
        "Ziwei Liu"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "29",
      "title": "Prompting visual-language models for dynamic facial expression recognition",
      "authors": [
        "Zengqun Zhao",
        "Ioannis Patras"
      ],
      "year": "2023",
      "venue": "Prompting visual-language models for dynamic facial expression recognition",
      "arxiv": "arXiv:2308.13382"
    }
  ]
}