{
  "paper_id": "2409.10762v4",
  "title": "Stimulus Modality Matters: Impact Of Perceptual Evaluations From Different Modalities On Speech Emotion Recognition System Performance",
  "published": "2024-09-16T22:32:22Z",
  "authors": [
    "Huang-Cheng Chou",
    "Haibin Wu",
    "Hung-yi Lee",
    "Chi-Chun Lee"
  ],
  "keywords": [
    "speech emotion recognition",
    "the effects of stimulus modality",
    "the ambiguity of emotions"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech Emotion Recognition (SER) systems rely on speech input and emotional labels annotated by humans. However, various emotion databases collect perceptional evaluations in different ways. For instance, the IEMOCAP dataset uses video clips with sounds for annotators to provide their emotional perceptions. However, the most significant English emotion dataset, the MSP-PODCAST, only provides speech for raters to choose the emotional ratings. Nevertheless, using speech as input is the standard approach to training SER systems. Therefore, the open question is the emotional labels elicited by which scenarios are the most effective for training SER systems. We comprehensively compare the effectiveness of SER systems trained with labels elicited by different modality stimuli and evaluate the SER systems on various testing conditions. Also, we introduce an all-inclusive label that combines all labels elicited by various modalities. We show that using labels elicited by voiceonly stimuli for training yields better performance on the test set, whereas labels elicited by voice-only stimuli.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Prior studies trained Speech Emotion Recognition (SER) systems using labels elicited by the different types of stimuli. There are two main ways to obtain labels. One is giving raters audio-only emotional stimuli and letting them assign labels. For instance, the MSP-PODCAST emotion dataset  [1]  uses this scenario. The other is giving raters audio-visual emotional stimuli and letting them provide labels, and the IEMOCAP  [2]  emotion dataset is in the condition. While some papers have utilized labels derived from audio-visual stimuli  [3]  to train SER systems, others have focused on training SER models with labels elicited by audio-only stimuli  [4] . This variation in methodology raises an intriguing open question: are the emotional labels elicited by multi-modal emotional stimuli different from those used in training SER systems with audio-only inputs?\n\nThe emergence of speech self-supervised learning models (SSLMs) has significantly propelled advancements across a wide array of speech-related tasks  [5] , including SER. The current stateof-the-art frameworks in SER are primarily built upon these SSLMs  [6] . To thoroughly investigate the research question concerning the advantage of utilizing labels elicited by multi-modal or singlemodal emotional stimuli for training SER models, we conducted extensive experiments using SSLMs. In our approach, we trained SER systems using labels derived from various modalities, including audio-only, facial-only, and audio-visual inputs. This training utilized the S3PRL toolkit  [5] , which encompasses 14 self-supervised learning models (SSLMs). Our findings highlight significant differences in the performance of SER systems trained with labels elicited from these modalities when tested under three distinct conditions: audio-only, facial-only, and audio-visual label elicitation.\n\nWe initiated a cross-testing experiment to discern the most productive approach to training SER systems. For different labeling processes using various stimuli, we use one type of label for training and all label types for testing. For instance, we trained SER systems using labels elicited by audio-only stimuli. Then, we evaluated these models using test sets labeled with audio-only, facial-only, and audiovisual stimuli. Furthermore, we introduced an innovative all-inclusive label set that combines labels elicited by audio-only, facial-only, and audio-visual stimuli to train the SER systems. The SER systems trained with this all-inclusive label set outperformed those trained with labels elicited by uni-modal or multi-modal emotional stimuli on the facial-only and audio-visual conditions. The SER systems trained with the voice-only label set achieved the best performance on the voice-only testing condition.\n\nIn conclusion, our work makes three contributions as follows. The source code is available 1 .\n\n• We presented an exhaustive comparative analysis of SER systems trained with labels elicited by various annotation conditions (e.g., audio-visual stimuli) and evaluated on test sets with labels elicited by different modalities (e.g., voice-only stimuli). This analysis provides valuable insights into the performance of SER systems under different training and testing annotation conditions. • We introduce a novel all-inclusive label for training SER systems. Our results demonstrate that this label set shows promise for improving the performance of SER systems on the test set whose labels elicited by face-only and audio-visual modalities scenarios. This finding highlights the potential of leveraging information from multiple annotation conditions to enhance the accuracy of SER systems. • We are the first to reveal that training SER systems using labels elicited by audio-only stimuli is better than using labels elicited by audio-visual stimuli based on our extensive experimental results. Our findings indicate that focusing on audio cues alone during labeling is more effective for training SER in audioonly contexts, and the findings draw a connection to the fact that recent benchmark databases (such as MSP-PODCAST) use audio-only stimuli for labels. Emotion perception is inherently multifaceted, influenced by various sensory inputs such as auditory signals, facial expressions, and a combination of audio-visual cues  [7] . Consequently, the field of emotion recognition has evolved to include a diverse array of systems, each focusing on different modalities: facial emotion recognition  [8] , text emotion recognition  [9] , speech emotion recognition  [10] ,  [11] , and multi-modal (e.g., audio-visual  [12] ,  [13]  or speech and text  [14] ) emotion recognition. This study primarily seeks to advance SER systems that rely solely on speech as the input modality.\n\nMuch research has traditionally favored training SER models using labels derived from multi-modal stimuli, particularly audio-visual inputs. The IEMOCAP corpus  [2] , one of the most influential datasets in SER research, exemplifies this approach by collecting labels through audio and visual stimuli. Recent trends, however, have shown an increasing shift toward using audio-only stimuli for collecting emotional labels in SER tasks. The MSP-PODCAST database  [1]  is the most extensive annotated emotional corpus in English and relies exclusively on audio stimuli for label collection. This shift marks an emerging need to evaluate the efficacy of labels derived from varying stimuli types for training SER models. Investigating this research question could provide valuable insights into optimizing SER models for more accurate and efficient emotion detection.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Iii. Methodology",
      "text": "Most prior SER researches have mainly relied on annotations elicited by audio-visual stimulus as their learning objective  [3] , or the prior study did not specify labels elicited by which modalities they used  [15] . However, the relationship between the modalities from which labels are elicited and the resulting performance improvements has not yet been thoroughly investigated. To answer the question, we aim to compare the performances of SER systems trained with labels elicited by different modalities (e.g., face-only or audio-only) across various testing conditions according to modality stimulus.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Labels Elicited By Multi-Modal Emotional Stimulus",
      "text": "The annotation process in the CREMA-D corpus  [16]  introduced in Section IV-A encompassed three scenarios: voice-only, face-only, and audio-visual settings. In the voice-only scenario, annotators were presented solely with the audio component of the clips for their reference to label the data. Conversely, annotators were limited to observing actors' facial expressions without accompanying audio when assigning labels in the face-only setting. On the other hand, the audio-visual setting provided annotators with a complete experience, enabling them to see the actors' faces and hear their voices.   emotional cues available from audio, visual, and audio-visual sources, we aim to harness the synergistic effect of multi-modal input to improve the performance of SER systems. This strategy is directly inspired by the inherent human capability to more accurately perceive and interpret emotions when multiple modal cues are available.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Proposed All-Inclusive Label Set And Rationales",
      "text": "In the example of Table  I , we summarize how to convert the raw annotations into the training/testing labels generated by the different modalities. The all-inclusive label (All) considers all labels elicited by voice-only, facial-only, and audio-visual stimuli. We consider a six-class emotion task, including anger (A), disgust (D), fear (F), happiness (H), sadness (S), and a neutral state (N). Importantly, the labels used during the training stage are distributional and are converted into binary vectors when their values surpass the defined threshold outlined in Section IV-C, which is 1/C, where C is the number of emotions. In this work, we have six emotions in total (threshold = 1/6), so the testing label of the audiovisual scenario (1,1,0,0,0,0) is different from others (1,1,0,0,0,1) after applying the threshold method introduced in  [10] . We follow  [17]  to allow the samples to have more than one emotion to reflect the nature of emotion perception that could involve mixed emotions from the psychology perspective  [18] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Sslms-Based Ser Framework",
      "text": "We adopt SER models using 14 SSLMs as the backbone models following the EMO-SUPERB settings 2  [19]  to train SER systems. We use SSLMs as they achieve SOTA results in SER. We leverage two mainstream categories of SSLMs, pre-trained using generative loss, DeCoAR 2  [20] , Autoregressive Predictive Coding (APC)  [21] , VQ-APC  [22] , Non-autoregressive Predictive Coding (NPC)  [23] , TERA  [24] , and Mockingjay  [25] ), and discriminative loss (XLS-R-1B)  [26] , WavLM Large  [27] , Data2Vec-A  [28] , Hubert Large [29], wav2vec 2.0 Large (W2V2)  [30] , VQ wav2vec (VQ-W2V)  [31] , wav2vec (W2V)  [32] , wav2vec 2.0 Robustness (W2V2 R)  [33]  and Contrastive Predictive Coding (CPC) (M CPC)  [34] ). Additionally, we include the log mel filterbank (FBANK).\n\nprofessional actors. This rich dataset comprises 7,442 clips in English, and every clip received annotations from at least six raters, who were allowed to select only one of the six emotions, including anger, disgust, fear, happiness, sadness, and a neutral state. The annotation process of the database contains three scenarios: voice-only, faceonly, and audio-visual settings, as shown in Fig.  1 , mentioned in section III. The database does not provide a standard partition  3  , so we use the defined partition provided by EMO-SUPERB  [19] . In total, there were five sessions, and we reported average results.\n\nHowever, there is a lack of clarity in existing literature regarding the specific labels used to train SER models, as noted in several prior SER studies  [15] . This highlights the need for further investigation into the optimal stimuli for training SER models, potentially unlocking new insights into more effective SER methodologies.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Class-Balanced Objective Function",
      "text": "We follow the EMO-SUPERB  [19]  to employ the Class-Balanced Cross-Entropy Loss (BCE) strategy as a loss function, initially proposed by Cui et al.  [35] . The BCE method incorporates a weighting factor into the loss function, designed to recalibrate the loss based on the inverse frequencies of each class within the training dataset. This approach ensures that each class is given appropriate consideration during training, regardless of its frequency, thereby mitigating the challenges posed by uneven annotation distributions and leading to more robust and equitable SER system performance.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Evaluation Metrics And Confidence Intervals",
      "text": "In our evaluation framework, we follow the EMO-SUPERB  [19]  and recent SER challenge  [36]  to utilize the macro-F1 score and F1 score, metrics that simultaneously assess recall and precision rates to provide a balanced measure of our SER systems' performance  [37] . This evaluation method is executed using the Scikit-learn library  [38] . Our evaluation process adopts a threshold-based approach  [10]  for scenarios involving multi-label classifications to accurately identify the target classes from the ground truth data.\n\nSpecifically, a prediction for a particular class is deemed correct if its proportional representation among all predictions exceeds the threshold of (1/C), where C is the total number of emotional courses under consideration. This strategic choice of threshold ensures that predictions are classified based on a fair representation criterion, aligning with methodologies previously described in the literature  [10] ,  [39] . Notice that we collect the predictions from each partition defined in the study  [19]  and then measure the performance in macro-F1 score with the average and lower and upper bound of the confidence interval (CI) between 2.75% and 97.5% using the toolkit  [40] . All results are single-run with a fixed random seed number.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "D. Models Training And Choice",
      "text": "We employ the AdamW optimizer  [41]  with a learning rate of 0.0001. The batch size is set to 32, and the models are trained for 50 epochs. The best-performing models are selected based on the lowest loss value on the development set. All experiments use two Nvidia Tesla V100 GPUs with 32 GB of memory, requiring approximately 84 GPU hours. Our work is built upon the S3PRL [5] 4  , which is implemented using PyTorch  [42]  and HuggingFace library  [43] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "V. Results And Analysis",
      "text": "The CREMA-D database provides annotations for various stimulus modalities, including face-only (Face), voice-only (Voice), and audiovisual (AV). We propose the all-inclusive label set (All), a combination of all modalities. To investigate the impact of these modalities on emotion perception, we calculate the multi-label distribution labels for six emotions, as described in Section III, using the annotations elicited by different modalities to answer some research questions as below. What is a correlation between labels elicited by various modalities? We employ the concordance correlation coefficient (CCC)  [44]  to assess the correlation between the averaged labels under different conditions, as presented in Table  II . Interestingly, the CCC between Voice and AV modalities is only 0.573, while the CCC between Face and AV is considerably higher at 0.805. Furthermore, the CCC between Voice and All (0.745) is lower than the CCC between Face and All (0.875), as well as the CCC between AV and All (0.913). The All modality exhibits overall higher correlations with other modalities, providing an additional justification for our proposal of the all-inclusive label set (All), which considers all labels from all modalities. This approach ensures a comprehensive consideration of the information available when analyzing emotion perception.\n\nWhat is the effect of the stimulus modality on the performance of the SER systems? The results in Table III reveal intriguing differences in the performance of the SER model when trained on labels elicited from different modalities. For the evaluation annotations, Voice utilizes voice-only annotations, Face uses face-only annotations, and AV uses audio-visual annotations. The crucial insight in Table  III  is the performance variation when different modalities are elicited. The best overall macro-F1 score in 9 out of 15 experiments was achieved by models trained with the Voice, and 5 out of 15 experiments was achieved by models trained with the AV. These findings highlight the significant impact that the chosen annotation modality can have on the capability of SER systems to recognize emotions from speech accurately. Consequently, when developing SER models, it is crucial to carefully consider the modality used for annotating emotional labels, as this factor can substantially influence the model's ability to capture the nuances of emotions in speech.\n\nIs there a difference between SER systems trained with the labels elicited by different conditions based on the layerwise  weights analysis? We conduct a layerwise analysis to understand the of different layers in the WavLM Large-based SER models trained with the labels elicited by various modalities. We extract the layer weights from the best checkpoint of each model and normalize them using the softmax function to ensure values between 0 and 1. We average the layerwise weights across multiple partitions of the CREMA-D dataset. Fig.  2  plots the layer weights across all models trained with emotion labels elicited by various modalities (voice-only, face-only, and audio-visual). The models tend to assign higher weights to the 10th to 15th layers, suggesting that these middle layers encode more emotional information than the earlier or later layers. Interestingly, the model trained with voice-only labels exhibits more balanced weights across layers than those taught with labels elicited by other modalities.\n\nWhich labels elicited by various modalities are the most effective for SER systems? We choose the best model (WavLM Large) in Table  III  as the backbone model for the experiments. Table  IV  summarizes the performance of SER systems trained with labels elicited by various modalities and evaluated on test sets defined by labels elicited by different modalities. The \"Train Set\" column shows the models trained by which label type (Voice, Face, AV, or All), and the \"Test Set\" column shows the testing label type that decides the ground truth of the test sets. The testing conditions are of three types: Voice, Face, and AV. In the Overall column of Table  IV , we report the macro-F1 scores along with the lower and upper bounds of the confidence interval between 2.75% and 97.5% for each result. Additionally, we present the sample-based F1 scores for the recognition performance of each emotion.\n\nInterestingly, when tested on the voice-only condition (Voice), the SER system trained with voice-only labels achieved promising results compared to models trained with other modalities. This finding suggests that voice-only labels are suitable for training SER systems, as the input is speech-only. This finding connects to recent benchmark databases (such as MSP-PODCAST  [1] ) that use audio-only stimuli for labels. Furthermore, we observed that the model trained with the proposed all-inclusive label set (All) performed better on sad and disgusted emotions than the one trained with voice-only labels (Voice). Regarding the testing conditions using face-only (Face) and audiovisual (AV), our proposed label type (All) results in the best performance, demonstrating the effectiveness of the proposed label sets. The models trained with the proposed label set perform best for sad, disgust, and happy emotions on the three testing conditions.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Vi. Discussion And Limitations",
      "text": "While the study by Paulmann et al.  [7]  suggests that humans have enhanced emotion recognition with multimodal stimuli compared to single modalities, our research found that audio-only labels are the most effective for training SER systems when only speech input is available. Systems were evaluated using audio-only, visual-only, audio-visual, and all-inclusive labels, with the audio-only approach proving to be the most optimal. Multimodal emotion systems are not used since current SER systems predominantly rely exclusively on audio input. Moreover, to ensure that emotion predictions closely resemble human perceptions, speech-only emotion recognition evaluations must use labels derived from voice-only contexts. Additionally, the findings from the mentioned study might not apply to other emotion recognition systems, such as those based on facial expressions, text, or audio-visual inputs. Besides, our experiments have one limitation: they were conducted on a single emotion database, as no other publicly available databases provide emotional annotations across various stimuli.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Vii. Conclusion And Future Work",
      "text": "This work compares SER systems (based on 14 SSLMs) trained with labels elicited from various emotional stimuli (multi-modal and uni-modal). The results show that the different modalities of emotional stimuli can significantly impact the performance of SER systems. Also, we propose an all-inclusive label set that combines labels elicited by multi-modal and uni-modal emotional stimuli. The SER systems trained on the proposed all-inclusive label set achieved the best performance on test sets for facial-only and audio-visual scenarios. Moreover, the SER systems trained solely on labels elicited by the voice-only stimuli provided promising results on the voice-only test condition, suggesting that voice-only training is preferable for speech-only applications. The findings connect to recent benchmark databases (such as MSP-PODCAST) that use audio-only stimuli for labels and align with how humans perceive emotions through voice alone. Also, the findings suggest that speech-only SER systems find it challenging to interpret emotional ratings derived from audio-visual or face-only modalities, as these lack the inherent emotional signals in voice. In future work, we plan to incorporate systems that can take different modalities, such as audio and video inputs.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: illustrates our proposed innovative approach for training",
      "page": 2
    },
    {
      "caption": "Figure 1: The figure shows the multi-modal emotional stimulus in the CREMA-",
      "page": 2
    },
    {
      "caption": "Figure 1: , mentioned in",
      "page": 3
    },
    {
      "caption": "Figure 2: plots the layer weights across all",
      "page": 4
    },
    {
      "caption": "Figure 2: The layerwise weights of WavLM-based SER systems trained with",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "set, whereas labels elicited by voice-only stimuli.": ""
        },
        {
          "set, whereas labels elicited by voice-only stimuli.": "Index Terms—speech emotion recognition,\nthe effects of stim-"
        },
        {
          "set, whereas labels elicited by voice-only stimuli.": ""
        },
        {
          "set, whereas labels elicited by voice-only stimuli.": "ulus modality,\nthe ambiguity of emotions"
        },
        {
          "set, whereas labels elicited by voice-only stimuli.": ""
        },
        {
          "set, whereas labels elicited by voice-only stimuli.": "I.\nINTRODUCTION"
        },
        {
          "set, whereas labels elicited by voice-only stimuli.": ""
        },
        {
          "set, whereas labels elicited by voice-only stimuli.": "Prior studies trained Speech Emotion Recognition (SER) systems"
        },
        {
          "set, whereas labels elicited by voice-only stimuli.": ""
        },
        {
          "set, whereas labels elicited by voice-only stimuli.": "using\nlabels\nelicited\nby\nthe\ndifferent\ntypes\nof\nstimuli. There\nare"
        },
        {
          "set, whereas labels elicited by voice-only stimuli.": ""
        },
        {
          "set, whereas labels elicited by voice-only stimuli.": "two main ways\nto\nobtain\nlabels. One\nis\ngiving\nraters\naudio-only"
        },
        {
          "set, whereas labels elicited by voice-only stimuli.": ""
        },
        {
          "set, whereas labels elicited by voice-only stimuli.": "emotional\nstimuli\nand letting them assign labels. For\ninstance,\nthe"
        },
        {
          "set, whereas labels elicited by voice-only stimuli.": ""
        },
        {
          "set, whereas labels elicited by voice-only stimuli.": "MSP-PODCAST emotion dataset [1] uses this scenario. The other is"
        },
        {
          "set, whereas labels elicited by voice-only stimuli.": ""
        },
        {
          "set, whereas labels elicited by voice-only stimuli.": "giving raters audio-visual emotional stimuli and letting them provide"
        },
        {
          "set, whereas labels elicited by voice-only stimuli.": ""
        },
        {
          "set, whereas labels elicited by voice-only stimuli.": "labels, and the IEMOCAP [2] emotion dataset\nis\nin the condition."
        },
        {
          "set, whereas labels elicited by voice-only stimuli.": ""
        },
        {
          "set, whereas labels elicited by voice-only stimuli.": "While\nsome papers have utilized labels derived from audio-visual"
        },
        {
          "set, whereas labels elicited by voice-only stimuli.": ""
        },
        {
          "set, whereas labels elicited by voice-only stimuli.": "stimuli [3] to train SER systems, others have focused on training SER"
        },
        {
          "set, whereas labels elicited by voice-only stimuli.": ""
        },
        {
          "set, whereas labels elicited by voice-only stimuli.": "models with labels elicited by audio-only stimuli\n[4]. This variation"
        },
        {
          "set, whereas labels elicited by voice-only stimuli.": ""
        },
        {
          "set, whereas labels elicited by voice-only stimuli.": "in methodology raises an intriguing open question: are the emotional"
        },
        {
          "set, whereas labels elicited by voice-only stimuli.": ""
        },
        {
          "set, whereas labels elicited by voice-only stimuli.": "labels\nelicited by multi-modal\nemotional\nstimuli different\nfrom"
        },
        {
          "set, whereas labels elicited by voice-only stimuli.": ""
        },
        {
          "set, whereas labels elicited by voice-only stimuli.": "those used in training SER systems with audio-only inputs?"
        },
        {
          "set, whereas labels elicited by voice-only stimuli.": ""
        },
        {
          "set, whereas labels elicited by voice-only stimuli.": "The\nemergence\nof\nspeech\nself-supervised\nlearning\nmodels"
        },
        {
          "set, whereas labels elicited by voice-only stimuli.": ""
        },
        {
          "set, whereas labels elicited by voice-only stimuli.": "(SSLMs)\nhas\nsignificantly\npropelled\nadvancements\nacross\na wide"
        },
        {
          "set, whereas labels elicited by voice-only stimuli.": ""
        },
        {
          "set, whereas labels elicited by voice-only stimuli.": "array of\nspeech-related tasks\n[5],\nincluding SER. The current\nstate-"
        },
        {
          "set, whereas labels elicited by voice-only stimuli.": ""
        },
        {
          "set, whereas labels elicited by voice-only stimuli.": "of-the-art\nframeworks in SER are primarily built upon these SSLMs"
        },
        {
          "set, whereas labels elicited by voice-only stimuli.": ""
        },
        {
          "set, whereas labels elicited by voice-only stimuli.": "[6]. To thoroughly investigate the research question concerning the"
        },
        {
          "set, whereas labels elicited by voice-only stimuli.": ""
        },
        {
          "set, whereas labels elicited by voice-only stimuli.": "advantage\nof\nutilizing\nlabels\nelicited\nby multi-modal\nor\nsingle-"
        },
        {
          "set, whereas labels elicited by voice-only stimuli.": ""
        },
        {
          "set, whereas labels elicited by voice-only stimuli.": "modal\nemotional\nstimuli\nfor\ntraining SER models, we\nconducted"
        },
        {
          "set, whereas labels elicited by voice-only stimuli.": ""
        },
        {
          "set, whereas labels elicited by voice-only stimuli.": "extensive\nexperiments using SSLMs.\nIn our\napproach, we\ntrained"
        },
        {
          "set, whereas labels elicited by voice-only stimuli.": "SER systems using labels derived from various modalities,\nincluding"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "on\nspeech\ninput\nand\nemotional\nlabels\nannotated\nby\nhumans.",
          "models (SSLMs). Our findings highlight significant differences in the": "performance of SER systems trained with labels elicited from these"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "However, various emotion databases collect perceptional evalu-",
          "models (SSLMs). Our findings highlight significant differences in the": "modalities when tested under\nthree distinct conditions: audio-only,"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "ations\nin different ways. For\ninstance,\nthe\nIEMOCAP dataset",
          "models (SSLMs). Our findings highlight significant differences in the": "facial-only, and audio-visual\nlabel elicitation."
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "uses\nvideo\nclips with\nsounds\nfor\nannotators\nto\nprovide\ntheir",
          "models (SSLMs). Our findings highlight significant differences in the": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "",
          "models (SSLMs). Our findings highlight significant differences in the": "We initiated a cross-testing experiment\nto discern the most pro-"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "emotional\nperceptions. However,\nthe most\nsignificant English",
          "models (SSLMs). Our findings highlight significant differences in the": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "",
          "models (SSLMs). Our findings highlight significant differences in the": "ductive\napproach\nto\ntraining SER systems. For\ndifferent\nlabeling"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "emotion dataset,\nthe MSP-PODCAST, only provides\nspeech for",
          "models (SSLMs). Our findings highlight significant differences in the": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "",
          "models (SSLMs). Our findings highlight significant differences in the": "processes using various stimuli, we use one type of label for training"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "raters to choose the emotional ratings. Nevertheless, using speech",
          "models (SSLMs). Our findings highlight significant differences in the": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "",
          "models (SSLMs). Our findings highlight significant differences in the": "and all\nlabel\ntypes for testing. For instance, we trained SER systems"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "as\ninput\nis\nthe\nstandard\napproach\nto\ntraining\nSER systems.",
          "models (SSLMs). Our findings highlight significant differences in the": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "",
          "models (SSLMs). Our findings highlight significant differences in the": "using labels elicited by audio-only stimuli. Then, we evaluated these"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "Therefore,\nthe open question is\nthe emotional\nlabels elicited by",
          "models (SSLMs). Our findings highlight significant differences in the": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "",
          "models (SSLMs). Our findings highlight significant differences in the": "models using test sets labeled with audio-only, facial-only, and audio-"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "which scenarios are the most effective for training SER systems.",
          "models (SSLMs). Our findings highlight significant differences in the": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "",
          "models (SSLMs). Our findings highlight significant differences in the": "visual stimuli. Furthermore, we introduced an innovative all-inclusive"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "We comprehensively compare the effectiveness of SER systems",
          "models (SSLMs). Our findings highlight significant differences in the": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "",
          "models (SSLMs). Our findings highlight significant differences in the": "label set\nthat combines labels elicited by audio-only, facial-only, and"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "trained with labels\nelicited by different modality\nstimuli\nand",
          "models (SSLMs). Our findings highlight significant differences in the": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "",
          "models (SSLMs). Our findings highlight significant differences in the": "audio-visual\nstimuli\nto\ntrain\nthe SER systems. The SER systems"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "evaluate the SER systems on various testing conditions. Also, we",
          "models (SSLMs). Our findings highlight significant differences in the": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "",
          "models (SSLMs). Our findings highlight significant differences in the": "trained with this all-inclusive\nlabel\nset outperformed those\ntrained"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "introduce an all-inclusive label that combines all labels elicited by",
          "models (SSLMs). Our findings highlight significant differences in the": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "",
          "models (SSLMs). Our findings highlight significant differences in the": "with labels elicited by uni-modal or multi-modal emotional stimuli on"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "various modalities. We show that using labels elicited by voice-",
          "models (SSLMs). Our findings highlight significant differences in the": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "",
          "models (SSLMs). Our findings highlight significant differences in the": "the facial-only and audio-visual conditions. The SER systems trained"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "only stimuli\nfor\ntraining yields better performance on the\ntest",
          "models (SSLMs). Our findings highlight significant differences in the": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "",
          "models (SSLMs). Our findings highlight significant differences in the": "with the voice-only label\nset achieved the best performance on the"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "set, whereas labels elicited by voice-only stimuli.",
          "models (SSLMs). Our findings highlight significant differences in the": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "",
          "models (SSLMs). Our findings highlight significant differences in the": "voice-only testing condition."
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "Index Terms—speech emotion recognition,\nthe effects of stim-",
          "models (SSLMs). Our findings highlight significant differences in the": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "",
          "models (SSLMs). Our findings highlight significant differences in the": "In conclusion, our work makes three contributions as follows. The"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "ulus modality,\nthe ambiguity of emotions",
          "models (SSLMs). Our findings highlight significant differences in the": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "",
          "models (SSLMs). Our findings highlight significant differences in the": "source code is available1."
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "I.\nINTRODUCTION",
          "models (SSLMs). Our findings highlight significant differences in the": "• We presented an exhaustive comparative analysis of SER sys-"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "",
          "models (SSLMs). Our findings highlight significant differences in the": "tems\ntrained with labels elicited by various annotation condi-"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "Prior studies trained Speech Emotion Recognition (SER) systems",
          "models (SSLMs). Our findings highlight significant differences in the": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "",
          "models (SSLMs). Our findings highlight significant differences in the": "tions (e.g., audio-visual stimuli) and evaluated on test sets with"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "using\nlabels\nelicited\nby\nthe\ndifferent\ntypes\nof\nstimuli. There\nare",
          "models (SSLMs). Our findings highlight significant differences in the": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "",
          "models (SSLMs). Our findings highlight significant differences in the": "labels elicited by different modalities (e.g., voice-only stimuli)."
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "two main ways\nto\nobtain\nlabels. One\nis\ngiving\nraters\naudio-only",
          "models (SSLMs). Our findings highlight significant differences in the": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "",
          "models (SSLMs). Our findings highlight significant differences in the": "This analysis provides valuable insights\ninto the performance"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "emotional\nstimuli\nand letting them assign labels. For\ninstance,\nthe",
          "models (SSLMs). Our findings highlight significant differences in the": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "",
          "models (SSLMs). Our findings highlight significant differences in the": "of SER systems under different\ntraining and testing annotation"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "MSP-PODCAST emotion dataset [1] uses this scenario. The other is",
          "models (SSLMs). Our findings highlight significant differences in the": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "",
          "models (SSLMs). Our findings highlight significant differences in the": "conditions."
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "giving raters audio-visual emotional stimuli and letting them provide",
          "models (SSLMs). Our findings highlight significant differences in the": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "",
          "models (SSLMs). Our findings highlight significant differences in the": "• We introduce a novel all-inclusive label\nfor\ntraining SER sys-"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "labels, and the IEMOCAP [2] emotion dataset\nis\nin the condition.",
          "models (SSLMs). Our findings highlight significant differences in the": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "",
          "models (SSLMs). Our findings highlight significant differences in the": "tems. Our results demonstrate that\nthis label set shows promise"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "While\nsome papers have utilized labels derived from audio-visual",
          "models (SSLMs). Our findings highlight significant differences in the": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "",
          "models (SSLMs). Our findings highlight significant differences in the": "for\nimproving the performance of SER systems on the test set"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "stimuli [3] to train SER systems, others have focused on training SER",
          "models (SSLMs). Our findings highlight significant differences in the": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "",
          "models (SSLMs). Our findings highlight significant differences in the": "whose labels elicited by face-only and audio-visual modalities"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "models with labels elicited by audio-only stimuli\n[4]. This variation",
          "models (SSLMs). Our findings highlight significant differences in the": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "",
          "models (SSLMs). Our findings highlight significant differences in the": "scenarios. This finding highlights\nthe potential of\nleveraging"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "in methodology raises an intriguing open question: are the emotional",
          "models (SSLMs). Our findings highlight significant differences in the": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "",
          "models (SSLMs). Our findings highlight significant differences in the": "information from multiple annotation conditions to enhance the"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "labels\nelicited by multi-modal\nemotional\nstimuli different\nfrom",
          "models (SSLMs). Our findings highlight significant differences in the": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "",
          "models (SSLMs). Our findings highlight significant differences in the": "accuracy of SER systems."
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "those used in training SER systems with audio-only inputs?",
          "models (SSLMs). Our findings highlight significant differences in the": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "",
          "models (SSLMs). Our findings highlight significant differences in the": "• We are the first\nto reveal\nthat\ntraining SER systems using labels"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "The\nemergence\nof\nspeech\nself-supervised\nlearning\nmodels",
          "models (SSLMs). Our findings highlight significant differences in the": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "",
          "models (SSLMs). Our findings highlight significant differences in the": "elicited by audio-only stimuli\nis better than using labels elicited"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "(SSLMs)\nhas\nsignificantly\npropelled\nadvancements\nacross\na wide",
          "models (SSLMs). Our findings highlight significant differences in the": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "",
          "models (SSLMs). Our findings highlight significant differences in the": "by\naudio-visual\nstimuli\nbased\non\nour\nextensive\nexperimental"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "array of\nspeech-related tasks\n[5],\nincluding SER. The current\nstate-",
          "models (SSLMs). Our findings highlight significant differences in the": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "",
          "models (SSLMs). Our findings highlight significant differences in the": "results. Our findings indicate that focusing on audio cues alone"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "of-the-art\nframeworks in SER are primarily built upon these SSLMs",
          "models (SSLMs). Our findings highlight significant differences in the": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "",
          "models (SSLMs). Our findings highlight significant differences in the": "during labeling is more\neffective\nfor\ntraining SER in audio-"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "[6]. To thoroughly investigate the research question concerning the",
          "models (SSLMs). Our findings highlight significant differences in the": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "",
          "models (SSLMs). Our findings highlight significant differences in the": "only contexts, and the findings draw a connection to the fact"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "advantage\nof\nutilizing\nlabels\nelicited\nby multi-modal\nor\nsingle-",
          "models (SSLMs). Our findings highlight significant differences in the": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "",
          "models (SSLMs). Our findings highlight significant differences in the": "that recent benchmark databases (such as MSP-PODCAST) use"
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "modal\nemotional\nstimuli\nfor\ntraining SER models, we\nconducted",
          "models (SSLMs). Our findings highlight significant differences in the": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "",
          "models (SSLMs). Our findings highlight significant differences in the": "audio-only stimuli\nfor\nlabels."
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "extensive\nexperiments using SSLMs.\nIn our\napproach, we\ntrained",
          "models (SSLMs). Our findings highlight significant differences in the": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "SER systems using labels derived from various modalities,\nincluding",
          "models (SSLMs). Our findings highlight significant differences in the": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "audio-only, facial-only, and audio-visual\ninputs. This training utilized",
          "models (SSLMs). Our findings highlight significant differences in the": ""
        },
        {
          "Abstract—Speech Emotion Recognition\n(SER)\nsystems\nrely": "the S3PRL toolkit [5], which encompasses 14 self-supervised learning",
          "models (SSLMs). Our findings highlight significant differences in the": "1https://github.com/EMOsuperb/Stimulus-Modality-Matters"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "II. BACKGROUND AND RELATED WORK": "",
          "TABLE I": "OVERVIEW OF ONE SAMPLE IN THE CREMA-D. THE A, S, AND N, ARE"
        },
        {
          "II. BACKGROUND AND RELATED WORK": "Emotion perception is inherently multifaceted,\ninfluenced by var-",
          "TABLE I": ""
        },
        {
          "II. BACKGROUND AND RELATED WORK": "",
          "TABLE I": "ANGER, SADNESS, AND NEUTRAL EMOTIONS, RESPECTIVELY. THE"
        },
        {
          "II. BACKGROUND AND RELATED WORK": "ious sensory inputs such as auditory signals,\nfacial expressions, and",
          "TABLE I": ""
        },
        {
          "II. BACKGROUND AND RELATED WORK": "",
          "TABLE I": "NUMBER MEANS THE COUNT OF EMOTIONS. FOR INSTANCE, A*2, S*2,"
        },
        {
          "II. BACKGROUND AND RELATED WORK": "a\ncombination of\naudio-visual\ncues\n[7]. Consequently,\nthe field of",
          "TABLE I": ""
        },
        {
          "II. BACKGROUND AND RELATED WORK": "",
          "TABLE I": "AND N*4 MEANS A, A, S, S, N, N, N, N"
        },
        {
          "II. BACKGROUND AND RELATED WORK": "emotion recognition has evolved to include a diverse array of systems,",
          "TABLE I": ""
        },
        {
          "II. BACKGROUND AND RELATED WORK": "",
          "TABLE I": "Raw Annotation\nA*2, S*2, N*4"
        },
        {
          "II. BACKGROUND AND RELATED WORK": "each focusing on different modalities: facial emotion recognition [8],",
          "TABLE I": "Voice-only"
        },
        {
          "II. BACKGROUND AND RELATED WORK": "",
          "TABLE I": "Label\nfor Training Stage\n(0.25,0.25,0.0,0.0,0.0,0.5)"
        },
        {
          "II. BACKGROUND AND RELATED WORK": "text emotion recognition [9], speech emotion recognition [10],\n[11],",
          "TABLE I": ""
        },
        {
          "II. BACKGROUND AND RELATED WORK": "",
          "TABLE I": "Label\nfor Testing Stage\n(1,1,0,0,0,1)"
        },
        {
          "II. BACKGROUND AND RELATED WORK": "and multi-modal (e.g., audio-visual [12], [13] or speech and text [14])",
          "TABLE I": ""
        },
        {
          "II. BACKGROUND AND RELATED WORK": "study primarily seeks\nto advance SER\nemotion recognition. This",
          "TABLE I": "Raw Annotation\nA*4, S*2, N*2"
        },
        {
          "II. BACKGROUND AND RELATED WORK": "systems that rely solely on speech as the input modality.",
          "TABLE I": "Facial-only"
        },
        {
          "II. BACKGROUND AND RELATED WORK": "",
          "TABLE I": "Label\nfor Training Stage\n(0.5,0.25,0.0,0.0,0.0,0.25)"
        },
        {
          "II. BACKGROUND AND RELATED WORK": "Much research has traditionally favored training SER models using",
          "TABLE I": ""
        },
        {
          "II. BACKGROUND AND RELATED WORK": "",
          "TABLE I": "Label\nfor Testing Stage\n(1,1,0,0,0,1)"
        },
        {
          "II. BACKGROUND AND RELATED WORK": "labels\nderived\nfrom multi-modal\nstimuli,\nparticularly\naudio-visual",
          "TABLE I": ""
        },
        {
          "II. BACKGROUND AND RELATED WORK": "",
          "TABLE I": "Raw Annotation\nA*6, S*2"
        },
        {
          "II. BACKGROUND AND RELATED WORK": "inputs. The IEMOCAP corpus [2], one of the most influential datasets",
          "TABLE I": ""
        },
        {
          "II. BACKGROUND AND RELATED WORK": "",
          "TABLE I": "Audio-Visual"
        },
        {
          "II. BACKGROUND AND RELATED WORK": "in\nSER research,\nexemplifies\nthis\napproach\nby\ncollecting\nlabels",
          "TABLE I": "Label\nfor Training Stage\n(0.75,0.25,0.0,0.0,0.0,0.0)"
        },
        {
          "II. BACKGROUND AND RELATED WORK": "through audio and visual stimuli. Recent trends, however, have shown",
          "TABLE I": "Label\nfor Testing Stage\n(1,1,0,0,0,0)"
        },
        {
          "II. BACKGROUND AND RELATED WORK": "an\nincreasing\nshift\ntoward\nusing\naudio-only\nstimuli\nfor\ncollecting",
          "TABLE I": ""
        },
        {
          "II. BACKGROUND AND RELATED WORK": "",
          "TABLE I": "Raw Annotation\nA*12, S*6, N*6"
        },
        {
          "II. BACKGROUND AND RELATED WORK": "emotional\nlabels in SER tasks. The MSP-PODCAST database [1]\nis",
          "TABLE I": ""
        },
        {
          "II. BACKGROUND AND RELATED WORK": "",
          "TABLE I": "All-inclusive"
        },
        {
          "II. BACKGROUND AND RELATED WORK": "the most extensive annotated emotional corpus in English and relies",
          "TABLE I": "Label\nfor Training Stage\n(0.5,0.25,0.0,0.0,0.0,0.25)"
        },
        {
          "II. BACKGROUND AND RELATED WORK": "",
          "TABLE I": "Label\nfor Testing Stage\n(1,1,0,0,0,1)"
        },
        {
          "II. BACKGROUND AND RELATED WORK": "exclusively on audio stimuli for label collection. This shift marks an",
          "TABLE I": ""
        },
        {
          "II. BACKGROUND AND RELATED WORK": "emerging need to evaluate the efficacy of labels derived from varying",
          "TABLE I": ""
        },
        {
          "II. BACKGROUND AND RELATED WORK": "",
          "TABLE I": "emotional cues available from audio, visual, and audio-visual sources,"
        },
        {
          "II. BACKGROUND AND RELATED WORK": "stimuli\ntypes\nfor\ntraining SER models.\nInvestigating this\nresearch",
          "TABLE I": ""
        },
        {
          "II. BACKGROUND AND RELATED WORK": "",
          "TABLE I": "we\naim to\nharness\nthe\nsynergistic\neffect\nof multi-modal\ninput\nto"
        },
        {
          "II. BACKGROUND AND RELATED WORK": "question could provide valuable insights into optimizing SER models",
          "TABLE I": ""
        },
        {
          "II. BACKGROUND AND RELATED WORK": "",
          "TABLE I": "improve the performance of SER systems. This\nstrategy is directly"
        },
        {
          "II. BACKGROUND AND RELATED WORK": "for more accurate and efficient emotion detection.",
          "TABLE I": ""
        },
        {
          "II. BACKGROUND AND RELATED WORK": "",
          "TABLE I": "inspired by the inherent human capability to more accurately perceive"
        },
        {
          "II. BACKGROUND AND RELATED WORK": "",
          "TABLE I": "and interpret emotions when multiple modal cues are available."
        },
        {
          "II. BACKGROUND AND RELATED WORK": "III. METHODOLOGY",
          "TABLE I": ""
        },
        {
          "II. BACKGROUND AND RELATED WORK": "",
          "TABLE I": "In the example of Table I, we summarize how to convert\nthe raw"
        },
        {
          "II. BACKGROUND AND RELATED WORK": "Most\nprior SER researches\nhave mainly\nrelied\non\nannotations",
          "TABLE I": ""
        },
        {
          "II. BACKGROUND AND RELATED WORK": "",
          "TABLE I": "annotations into the training/testing labels generated by the different"
        },
        {
          "II. BACKGROUND AND RELATED WORK": "elicited by audio-visual\nstimulus as\ntheir\nlearning objective [3], or",
          "TABLE I": ""
        },
        {
          "II. BACKGROUND AND RELATED WORK": "",
          "TABLE I": "modalities. The all-inclusive label\n(All) considers all\nlabels elicited"
        },
        {
          "II. BACKGROUND AND RELATED WORK": "the prior\nstudy did not\nspecify labels elicited by which modalities",
          "TABLE I": ""
        },
        {
          "II. BACKGROUND AND RELATED WORK": "",
          "TABLE I": "by voice-only,\nfacial-only, and audio-visual\nstimuli. We consider a"
        },
        {
          "II. BACKGROUND AND RELATED WORK": "they used [15]. However, the relationship between the modalities from",
          "TABLE I": ""
        },
        {
          "II. BACKGROUND AND RELATED WORK": "",
          "TABLE I": "six-class\nemotion task,\nincluding anger\n(A), disgust\n(D),\nfear\n(F),"
        },
        {
          "II. BACKGROUND AND RELATED WORK": "which labels are elicited and the resulting performance improvements",
          "TABLE I": ""
        },
        {
          "II. BACKGROUND AND RELATED WORK": "",
          "TABLE I": "happiness\n(H),\nsadness\n(S), and a neutral\nstate (N). For voice-only"
        },
        {
          "II. BACKGROUND AND RELATED WORK": "has not yet been thoroughly investigated. To answer the question, we",
          "TABLE I": ""
        },
        {
          "II. BACKGROUND AND RELATED WORK": "",
          "TABLE I": "elicitation,\nthe\nannotations\nare\n{A*2, S*2, N*4};\nfor\nfacial-only"
        },
        {
          "II. BACKGROUND AND RELATED WORK": "aim to compare the performances of SER systems trained with labels",
          "TABLE I": ""
        },
        {
          "II. BACKGROUND AND RELATED WORK": "",
          "TABLE I": "elicitation,\nthe\nannotations\nare\n{A*4, S*2, N*2};\nand\nfor\naudio-"
        },
        {
          "II. BACKGROUND AND RELATED WORK": "elicited by different modalities (e.g.,\nface-only or audio-only) across",
          "TABLE I": ""
        },
        {
          "II. BACKGROUND AND RELATED WORK": "",
          "TABLE I": "visual\nelicitation,\nthe\nannotations\nare\n{A*6, S*2}. The\nproposed"
        },
        {
          "II. BACKGROUND AND RELATED WORK": "various testing conditions according to modality stimulus.",
          "TABLE I": ""
        },
        {
          "II. BACKGROUND AND RELATED WORK": "",
          "TABLE I": "all-inclusive label\nintegrates all\nratings,\nresulting in annotations of"
        },
        {
          "II. BACKGROUND AND RELATED WORK": "",
          "TABLE I": "{A*12, S*6, N*6}."
        },
        {
          "II. BACKGROUND AND RELATED WORK": "A. Labels Elicited by Multi-modal Emotional Stimulus",
          "TABLE I": ""
        },
        {
          "II. BACKGROUND AND RELATED WORK": "",
          "TABLE I": "Importantly,\nthe labels used during the training stage are distri-"
        },
        {
          "II. BACKGROUND AND RELATED WORK": "The annotation process in the CREMA-D corpus [16]\nintroduced",
          "TABLE I": ""
        },
        {
          "II. BACKGROUND AND RELATED WORK": "",
          "TABLE I": "butional\nand\nare\nconverted\ninto\nbinary\nvectors when\ntheir\nvalues"
        },
        {
          "II. BACKGROUND AND RELATED WORK": "face-only,\nin Section IV-A encompassed three scenarios: voice-only,",
          "TABLE I": ""
        },
        {
          "II. BACKGROUND AND RELATED WORK": "",
          "TABLE I": "surpass\nthe\ndefined\nthreshold\noutlined\nin Section\nIV-C, which\nis"
        },
        {
          "II. BACKGROUND AND RELATED WORK": "and audio-visual\nsettings.\nIn\nthe\nvoice-only\nscenario,\nannotators",
          "TABLE I": ""
        },
        {
          "II. BACKGROUND AND RELATED WORK": "",
          "TABLE I": "1/C, where C is the number of emotions.\nIn this work, we have six"
        },
        {
          "II. BACKGROUND AND RELATED WORK": "were\npresented\nsolely with\nthe\naudio\ncomponent\nof\nthe\nclips\nfor",
          "TABLE I": ""
        },
        {
          "II. BACKGROUND AND RELATED WORK": "",
          "TABLE I": "emotions in total (threshold = 1/6), so the testing label of the audio-"
        },
        {
          "II. BACKGROUND AND RELATED WORK": "their reference to label\nthe data. Conversely, annotators were limited",
          "TABLE I": ""
        },
        {
          "II. BACKGROUND AND RELATED WORK": "",
          "TABLE I": "visual scenario (1,1,0,0,0,0) is different from others (1,1,0,0,0,1) after"
        },
        {
          "II. BACKGROUND AND RELATED WORK": "to observing actors’\nfacial expressions without accompanying audio",
          "TABLE I": ""
        },
        {
          "II. BACKGROUND AND RELATED WORK": "",
          "TABLE I": "applying the threshold method introduced in [10]. We follow [17] to"
        },
        {
          "II. BACKGROUND AND RELATED WORK": "when assigning labels in the face-only setting. On the other hand,\nthe",
          "TABLE I": ""
        },
        {
          "II. BACKGROUND AND RELATED WORK": "",
          "TABLE I": "allow the samples to have more than one emotion to reflect the nature"
        },
        {
          "II. BACKGROUND AND RELATED WORK": "audio-visual setting provided annotators with a complete experience,",
          "TABLE I": ""
        },
        {
          "II. BACKGROUND AND RELATED WORK": "",
          "TABLE I": "of emotion perception that could involve mixed emotions\nfrom the"
        },
        {
          "II. BACKGROUND AND RELATED WORK": "enabling them to see the actors’\nfaces and hear\ntheir voices.",
          "TABLE I": ""
        },
        {
          "II. BACKGROUND AND RELATED WORK": "",
          "TABLE I": "psychology perspective [18]."
        },
        {
          "II. BACKGROUND AND RELATED WORK": "B. Proposed All-inclusive Label Set and Rationales",
          "TABLE I": "C.\nSSLMs-based SER Framework"
        },
        {
          "II. BACKGROUND AND RELATED WORK": "Fig. 1 illustrates our proposed innovative\napproach for\ntraining",
          "TABLE I": "We adopt SER models using 14 SSLMs as the backbone models"
        },
        {
          "II. BACKGROUND AND RELATED WORK": "all-inclusive\nSER systems, which\ninvolves\ncreating\nan\nlabel\nset",
          "TABLE I": "following the EMO-SUPERB settings2 [19] to train SER systems. We"
        },
        {
          "II. BACKGROUND AND RELATED WORK": "(All)\nthat\nintegrates labels derived from uni-modal and multi-modal",
          "TABLE I": "use SSLMs as they achieve SOTA results in SER. We leverage two"
        },
        {
          "II. BACKGROUND AND RELATED WORK": "emotional\nstimuli. By\nleveraging\nthe\ncomprehensive\nspectrum of",
          "TABLE I": "mainstream categories of SSLMs, pre-trained using generative loss,"
        },
        {
          "II. BACKGROUND AND RELATED WORK": "",
          "TABLE I": "DeCoAR 2 [20], Autoregressive Predictive Coding (APC) [21], VQ-"
        },
        {
          "II. BACKGROUND AND RELATED WORK": "",
          "TABLE I": "APC [22], Non-autoregressive Predictive Coding (NPC) [23], TERA"
        },
        {
          "II. BACKGROUND AND RELATED WORK": "",
          "TABLE I": "[24],\nand Mockingjay [25]),\nand discriminative\nloss\n(XLS-R-1B)"
        },
        {
          "II. BACKGROUND AND RELATED WORK": "",
          "TABLE I": "[26], WavLM Large [27], Data2Vec-A [28], Hubert Large [29],"
        },
        {
          "II. BACKGROUND AND RELATED WORK": "",
          "TABLE I": "wav2vec 2.0 Large\n(W2V2)\n[30], VQ wav2vec\n(VQ-W2V)\n[31],"
        },
        {
          "II. BACKGROUND AND RELATED WORK": "",
          "TABLE I": "wav2vec (W2V)\n[32], wav2vec 2.0 Robustness (W2V2 R)\n[33] and"
        },
        {
          "II. BACKGROUND AND RELATED WORK": "",
          "TABLE I": "Contrastive Predictive Coding (CPC)\n(M CPC)\n[34]). Additionally,"
        },
        {
          "II. BACKGROUND AND RELATED WORK": "",
          "TABLE I": "we include the log mel filterbank (FBANK)."
        },
        {
          "II. BACKGROUND AND RELATED WORK": "",
          "TABLE I": "IV. EXPERIMENTAL SETTINGS"
        },
        {
          "II. BACKGROUND AND RELATED WORK": "",
          "TABLE I": "A. The CREAMA-D"
        },
        {
          "II. BACKGROUND AND RELATED WORK": "",
          "TABLE I": "The CREMA-D dataset, as introduced by Cao et al.\n[16], encom-"
        },
        {
          "II. BACKGROUND AND RELATED WORK": "",
          "TABLE I": "passes high-quality audio-visual clips featuring performances by 91"
        },
        {
          "II. BACKGROUND AND RELATED WORK": "Fig. 1. The figure shows the multi-modal emotional stimulus in the CREMA-",
          "TABLE I": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "However,\nthere is a lack of clarity in existing literature regarding": "the specific labels used to train SER models, as noted in several prior",
          "AV\n0.913\n0.805\n0.573\n1.000": "All\n0.875\n0.745\n0.913\n1.000"
        },
        {
          "However,\nthere is a lack of clarity in existing literature regarding": "SER studies\n[15]. This highlights\nthe need for\nfurther\ninvestigation",
          "AV\n0.913\n0.805\n0.573\n1.000": ""
        },
        {
          "However,\nthere is a lack of clarity in existing literature regarding": "into the optimal stimuli for training SER models, potentially unlock-",
          "AV\n0.913\n0.805\n0.573\n1.000": ""
        },
        {
          "However,\nthere is a lack of clarity in existing literature regarding": "",
          "AV\n0.913\n0.805\n0.573\n1.000": "What is a correlation between labels elicited by various modal-"
        },
        {
          "However,\nthere is a lack of clarity in existing literature regarding": "ing new insights into more effective SER methodologies.",
          "AV\n0.913\n0.805\n0.573\n1.000": ""
        },
        {
          "However,\nthere is a lack of clarity in existing literature regarding": "",
          "AV\n0.913\n0.805\n0.573\n1.000": "ities? We employ the concordance correlation coefficient (CCC) [44]"
        },
        {
          "However,\nthere is a lack of clarity in existing literature regarding": "B. Class-balanced Objective Function",
          "AV\n0.913\n0.805\n0.573\n1.000": "to assess the correlation between the averaged labels under different"
        },
        {
          "However,\nthere is a lack of clarity in existing literature regarding": "",
          "AV\n0.913\n0.805\n0.573\n1.000": "conditions, as presented in Table II.\nInterestingly,\nthe CCC between"
        },
        {
          "However,\nthere is a lack of clarity in existing literature regarding": "We follow the EMO-SUPERB [19] to employ the Class-Balanced",
          "AV\n0.913\n0.805\n0.573\n1.000": ""
        },
        {
          "However,\nthere is a lack of clarity in existing literature regarding": "",
          "AV\n0.913\n0.805\n0.573\n1.000": "Voice\nand AV modalities\nis\nonly\n0.573, while\nthe CCC between"
        },
        {
          "However,\nthere is a lack of clarity in existing literature regarding": "Cross-Entropy Loss (BCE) strategy as a loss function,\ninitially pro-",
          "AV\n0.913\n0.805\n0.573\n1.000": ""
        },
        {
          "However,\nthere is a lack of clarity in existing literature regarding": "",
          "AV\n0.913\n0.805\n0.573\n1.000": "Face and AV is considerably higher at 0.805. Furthermore,\nthe CCC"
        },
        {
          "However,\nthere is a lack of clarity in existing literature regarding": "posed by Cui et al.\n[35]. The BCE method incorporates a weighting",
          "AV\n0.913\n0.805\n0.573\n1.000": ""
        },
        {
          "However,\nthere is a lack of clarity in existing literature regarding": "",
          "AV\n0.913\n0.805\n0.573\n1.000": "between Voice\nand All\n(0.745)\nis\nlower\nthan\nthe CCC between"
        },
        {
          "However,\nthere is a lack of clarity in existing literature regarding": "factor into the loss function, designed to recalibrate the loss based on",
          "AV\n0.913\n0.805\n0.573\n1.000": ""
        },
        {
          "However,\nthere is a lack of clarity in existing literature regarding": "",
          "AV\n0.913\n0.805\n0.573\n1.000": "Face\nand All\n(0.875),\nas well\nas\nthe CCC between AV and All"
        },
        {
          "However,\nthere is a lack of clarity in existing literature regarding": "the inverse frequencies of each class within the training dataset. This",
          "AV\n0.913\n0.805\n0.573\n1.000": ""
        },
        {
          "However,\nthere is a lack of clarity in existing literature regarding": "",
          "AV\n0.913\n0.805\n0.573\n1.000": "(0.913). The All modality exhibits overall higher correlations with"
        },
        {
          "However,\nthere is a lack of clarity in existing literature regarding": "approach ensures\nthat each class\nis given appropriate consideration",
          "AV\n0.913\n0.805\n0.573\n1.000": ""
        },
        {
          "However,\nthere is a lack of clarity in existing literature regarding": "",
          "AV\n0.913\n0.805\n0.573\n1.000": "other modalities, providing an additional justification for our proposal"
        },
        {
          "However,\nthere is a lack of clarity in existing literature regarding": "during training,\nregardless of\nits\nfrequency,\nthereby mitigating the",
          "AV\n0.913\n0.805\n0.573\n1.000": ""
        },
        {
          "However,\nthere is a lack of clarity in existing literature regarding": "",
          "AV\n0.913\n0.805\n0.573\n1.000": "of the all-inclusive label set (All), which considers all\nlabels from all"
        },
        {
          "However,\nthere is a lack of clarity in existing literature regarding": "challenges posed by uneven annotation distributions and leading to",
          "AV\n0.913\n0.805\n0.573\n1.000": ""
        },
        {
          "However,\nthere is a lack of clarity in existing literature regarding": "",
          "AV\n0.913\n0.805\n0.573\n1.000": "modalities. This approach ensures a comprehensive consideration of"
        },
        {
          "However,\nthere is a lack of clarity in existing literature regarding": "more robust and equitable SER system performance.",
          "AV\n0.913\n0.805\n0.573\n1.000": ""
        },
        {
          "However,\nthere is a lack of clarity in existing literature regarding": "",
          "AV\n0.913\n0.805\n0.573\n1.000": "the information available when analyzing emotion perception."
        },
        {
          "However,\nthere is a lack of clarity in existing literature regarding": "C. Evaluation Metrics and Confidence Intervals",
          "AV\n0.913\n0.805\n0.573\n1.000": ""
        },
        {
          "However,\nthere is a lack of clarity in existing literature regarding": "",
          "AV\n0.913\n0.805\n0.573\n1.000": "What is the effect of the stimulus modality on the performance"
        },
        {
          "However,\nthere is a lack of clarity in existing literature regarding": "In our evaluation framework, we follow the EMO-SUPERB [19]",
          "AV\n0.913\n0.805\n0.573\n1.000": "of\nthe\nSER systems? The\nresults\nin Table\nIII\nreveal\nintriguing"
        },
        {
          "However,\nthere is a lack of clarity in existing literature regarding": "and recent SER challenge [36]\nto utilize the macro-F1 score and F1",
          "AV\n0.913\n0.805\n0.573\n1.000": "differences\nin the performance of\nthe SER model when trained on"
        },
        {
          "However,\nthere is a lack of clarity in existing literature regarding": "score, metrics that simultaneously assess recall and precision rates to",
          "AV\n0.913\n0.805\n0.573\n1.000": "labels\nelicited from different modalities. For\nthe\nevaluation anno-"
        },
        {
          "However,\nthere is a lack of clarity in existing literature regarding": "provide a balanced measure of our SER systems’ performance [37].",
          "AV\n0.913\n0.805\n0.573\n1.000": "tations, Voice utilizes voice-only annotations, Face uses\nface-only"
        },
        {
          "However,\nthere is a lack of clarity in existing literature regarding": "This evaluation method is executed using the Scikit-learn library [38].",
          "AV\n0.913\n0.805\n0.573\n1.000": "annotations, and AV uses audio-visual annotations. The crucial insight"
        },
        {
          "However,\nthere is a lack of clarity in existing literature regarding": "Our evaluation process adopts a threshold-based approach [10]\nfor",
          "AV\n0.913\n0.805\n0.573\n1.000": "in Table III is the performance variation when different modalities are"
        },
        {
          "However,\nthere is a lack of clarity in existing literature regarding": "scenarios\ninvolving multi-label classifications\nto accurately identify",
          "AV\n0.913\n0.805\n0.573\n1.000": "elicited. The best overall macro-F1 score in 9 out of 15 experiments"
        },
        {
          "However,\nthere is a lack of clarity in existing literature regarding": "the target classes from the ground truth data.",
          "AV\n0.913\n0.805\n0.573\n1.000": "was\nachieved by models\ntrained with the Voice,\nand 5 out of 15"
        },
        {
          "However,\nthere is a lack of clarity in existing literature regarding": "Specifically, a prediction for a particular class\nis deemed correct",
          "AV\n0.913\n0.805\n0.573\n1.000": "experiments was\nachieved by models\ntrained with the AV. These"
        },
        {
          "However,\nthere is a lack of clarity in existing literature regarding": "if\nits proportional\nrepresentation among all predictions exceeds\nthe",
          "AV\n0.913\n0.805\n0.573\n1.000": "findings highlight\nthe significant\nimpact\nthat\nthe chosen annotation"
        },
        {
          "However,\nthere is a lack of clarity in existing literature regarding": "threshold of (1/C), where C is the total number of emotional courses",
          "AV\n0.913\n0.805\n0.573\n1.000": "modality can have on the\ncapability of SER systems\nto recognize"
        },
        {
          "However,\nthere is a lack of clarity in existing literature regarding": "under consideration. This\nstrategic choice of\nthreshold ensures\nthat",
          "AV\n0.913\n0.805\n0.573\n1.000": "emotions\nfrom speech\naccurately. Consequently, when\ndeveloping"
        },
        {
          "However,\nthere is a lack of clarity in existing literature regarding": "predictions\nare\nclassified\nbased\non\na\nfair\nrepresentation\ncriterion,",
          "AV\n0.913\n0.805\n0.573\n1.000": "SER models,\nit\nis crucial\nto carefully consider the modality used for"
        },
        {
          "However,\nthere is a lack of clarity in existing literature regarding": "aligning with methodologies previously described in the\nliterature",
          "AV\n0.913\n0.805\n0.573\n1.000": "annotating emotional\nlabels, as this factor can substantially influence"
        },
        {
          "However,\nthere is a lack of clarity in existing literature regarding": "[10],\n[39]. Notice that we collect\nthe predictions from each partition",
          "AV\n0.913\n0.805\n0.573\n1.000": "the model’s ability to capture the nuances of emotions in speech."
        },
        {
          "However,\nthere is a lack of clarity in existing literature regarding": "defined\nin\nthe\nstudy\n[19]\nand\nthen measure\nthe\nperformance\nin",
          "AV\n0.913\n0.805\n0.573\n1.000": "Is\nthere a difference between SER systems\ntrained with the"
        },
        {
          "However,\nthere is a lack of clarity in existing literature regarding": "macro-F1 score with the average and lower and upper bound of\nthe",
          "AV\n0.913\n0.805\n0.573\n1.000": "labels\nelicited\nby\ndifferent\nconditions\nbased\non\nthe\nlayerwise"
        },
        {
          "However,\nthere is a lack of clarity in existing literature regarding": "confidence interval\n(CI) between 2.75% and 97.5% using the toolkit",
          "AV\n0.913\n0.805\n0.573\n1.000": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "and\nevery\nclip\nreceived\nannotations\nfrom at\nleast\nsix\nraters, who",
          "TABLE II": "THE TABLE SUMMARIZES THE CONCORDANCE CORRELATION"
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "",
          "TABLE II": "COEFFICIENT (CCC) BETWEEN LABELS OF VARIOUS MODALITIES. THE"
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "were allowed to select only one of the six emotions,\nincluding anger,",
          "TABLE II": ""
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "",
          "TABLE II": "FACE, VOICE, AV, AND ALL REPRESENT FACE-ONLY, VOICE-ONLY,"
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "disgust,\nfear, happiness, sadness, and a neutral state. The annotation",
          "TABLE II": ""
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "",
          "TABLE II": "AUDIO-VISUAL, AND A COMBINATION OF ALL MODALITIES,"
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "process of\nthe database\ncontains\nthree\nscenarios: voice-only,\nface-",
          "TABLE II": ""
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "",
          "TABLE II": "RESPECTIVELY."
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "only,\nand audio-visual\nsettings,\nas\nshown in Fig. 1, mentioned in",
          "TABLE II": ""
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "section III. The database does not provide a standard partition3,\nso",
          "TABLE II": "Stimulus Modality\nFace\nVoice\nAV\nAll"
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "we use\nthe defined partition provided by EMO-SUPERB [19].\nIn",
          "TABLE II": "Face\n0.875\n1.000\n0.459\n0.805"
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "total,\nthere were five sessions, and we reported average results.",
          "TABLE II": "Voice\n0.745\n0.459\n1.000\n0.573"
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "However,\nthere is a lack of clarity in existing literature regarding",
          "TABLE II": "AV\n0.913\n0.805\n0.573\n1.000"
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "the specific labels used to train SER models, as noted in several prior",
          "TABLE II": "All\n0.875\n0.745\n0.913\n1.000"
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "SER studies\n[15]. This highlights\nthe need for\nfurther\ninvestigation",
          "TABLE II": ""
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "into the optimal stimuli for training SER models, potentially unlock-",
          "TABLE II": ""
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "",
          "TABLE II": "What is a correlation between labels elicited by various modal-"
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "ing new insights into more effective SER methodologies.",
          "TABLE II": ""
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "",
          "TABLE II": "ities? We employ the concordance correlation coefficient (CCC) [44]"
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "B. Class-balanced Objective Function",
          "TABLE II": "to assess the correlation between the averaged labels under different"
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "",
          "TABLE II": "conditions, as presented in Table II.\nInterestingly,\nthe CCC between"
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "We follow the EMO-SUPERB [19] to employ the Class-Balanced",
          "TABLE II": ""
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "",
          "TABLE II": "Voice\nand AV modalities\nis\nonly\n0.573, while\nthe CCC between"
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "Cross-Entropy Loss (BCE) strategy as a loss function,\ninitially pro-",
          "TABLE II": ""
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "",
          "TABLE II": "Face and AV is considerably higher at 0.805. Furthermore,\nthe CCC"
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "posed by Cui et al.\n[35]. The BCE method incorporates a weighting",
          "TABLE II": ""
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "",
          "TABLE II": "between Voice\nand All\n(0.745)\nis\nlower\nthan\nthe CCC between"
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "factor into the loss function, designed to recalibrate the loss based on",
          "TABLE II": ""
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "",
          "TABLE II": "Face\nand All\n(0.875),\nas well\nas\nthe CCC between AV and All"
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "the inverse frequencies of each class within the training dataset. This",
          "TABLE II": ""
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "",
          "TABLE II": "(0.913). The All modality exhibits overall higher correlations with"
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "approach ensures\nthat each class\nis given appropriate consideration",
          "TABLE II": ""
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "",
          "TABLE II": "other modalities, providing an additional justification for our proposal"
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "during training,\nregardless of\nits\nfrequency,\nthereby mitigating the",
          "TABLE II": ""
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "",
          "TABLE II": "of the all-inclusive label set (All), which considers all\nlabels from all"
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "challenges posed by uneven annotation distributions and leading to",
          "TABLE II": ""
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "",
          "TABLE II": "modalities. This approach ensures a comprehensive consideration of"
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "more robust and equitable SER system performance.",
          "TABLE II": ""
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "",
          "TABLE II": "the information available when analyzing emotion perception."
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "C. Evaluation Metrics and Confidence Intervals",
          "TABLE II": ""
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "",
          "TABLE II": "What is the effect of the stimulus modality on the performance"
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "In our evaluation framework, we follow the EMO-SUPERB [19]",
          "TABLE II": "of\nthe\nSER systems? The\nresults\nin Table\nIII\nreveal\nintriguing"
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "and recent SER challenge [36]\nto utilize the macro-F1 score and F1",
          "TABLE II": "differences\nin the performance of\nthe SER model when trained on"
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "score, metrics that simultaneously assess recall and precision rates to",
          "TABLE II": "labels\nelicited from different modalities. For\nthe\nevaluation anno-"
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "provide a balanced measure of our SER systems’ performance [37].",
          "TABLE II": "tations, Voice utilizes voice-only annotations, Face uses\nface-only"
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "This evaluation method is executed using the Scikit-learn library [38].",
          "TABLE II": "annotations, and AV uses audio-visual annotations. The crucial insight"
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "Our evaluation process adopts a threshold-based approach [10]\nfor",
          "TABLE II": "in Table III is the performance variation when different modalities are"
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "scenarios\ninvolving multi-label classifications\nto accurately identify",
          "TABLE II": "elicited. The best overall macro-F1 score in 9 out of 15 experiments"
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "the target classes from the ground truth data.",
          "TABLE II": "was\nachieved by models\ntrained with the Voice,\nand 5 out of 15"
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "Specifically, a prediction for a particular class\nis deemed correct",
          "TABLE II": "experiments was\nachieved by models\ntrained with the AV. These"
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "if\nits proportional\nrepresentation among all predictions exceeds\nthe",
          "TABLE II": "findings highlight\nthe significant\nimpact\nthat\nthe chosen annotation"
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "threshold of (1/C), where C is the total number of emotional courses",
          "TABLE II": "modality can have on the\ncapability of SER systems\nto recognize"
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "under consideration. This\nstrategic choice of\nthreshold ensures\nthat",
          "TABLE II": "emotions\nfrom speech\naccurately. Consequently, when\ndeveloping"
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "predictions\nare\nclassified\nbased\non\na\nfair\nrepresentation\ncriterion,",
          "TABLE II": "SER models,\nit\nis crucial\nto carefully consider the modality used for"
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "aligning with methodologies previously described in the\nliterature",
          "TABLE II": "annotating emotional\nlabels, as this factor can substantially influence"
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "[10],\n[39]. Notice that we collect\nthe predictions from each partition",
          "TABLE II": "the model’s ability to capture the nuances of emotions in speech."
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "defined\nin\nthe\nstudy\n[19]\nand\nthen measure\nthe\nperformance\nin",
          "TABLE II": "Is\nthere a difference between SER systems\ntrained with the"
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "macro-F1 score with the average and lower and upper bound of\nthe",
          "TABLE II": "labels\nelicited\nby\ndifferent\nconditions\nbased\non\nthe\nlayerwise"
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "confidence interval\n(CI) between 2.75% and 97.5% using the toolkit",
          "TABLE II": ""
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "",
          "TABLE II": "TABLE III"
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "[40]. All\nresults are single-run with a fixed random seed number.",
          "TABLE II": ""
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "",
          "TABLE II": "THE TABLE SUMMARIZES PERFORMANCES OF THE VARIOUS"
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "D. Models Training and Choice",
          "TABLE II": "SSLMS-BASED SER SYSTEMS TRAINED WITH THE LABELS ELICITED BY"
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "",
          "TABLE II": "DIFFERENT MODALITIES. THE FACE, VOICE, AND AV REPRESENT"
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "We\nemploy the AdamW optimizer\n[41] with a\nlearning rate of",
          "TABLE II": ""
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "",
          "TABLE II": "FACE-ONLY, VOICE-ONLY, AND AUDIO-VISUAL, RESPECTIVELY. WE USE"
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "0.0001. The batch size is set\nto 32, and the models are trained for 50",
          "TABLE II": ""
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "",
          "TABLE II": "BOLD TO REPRESENT THE BEST PERFORMANCE ACCORDING TO EACH"
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "epochs. The best-performing models are selected based on the lowest",
          "TABLE II": ""
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "",
          "TABLE II": "UPSTREAM MODEL. ALL VALUES ARE IN MACRO-F1 SCORES."
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "loss value on the development\nset. All experiments use two Nvidia",
          "TABLE II": ""
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "",
          "TABLE II": "Upstream\n#Pars.\n(M)\nVoice\nFace\nAV"
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "Tesla V100 GPUs with 32 GB of memory,\nrequiring approximately",
          "TABLE II": ""
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "84 GPU hours. Our work is built upon the S3PRL [5]4, which is",
          "TABLE II": "WavLM Large\n0.7117\n317\n0.6366\n0.7076"
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "",
          "TABLE II": "0.6960\nXLS-R-1B\n965\n0.6764\n0.6251"
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "implemented using PyTorch [42] and HuggingFace library [43].",
          "TABLE II": ""
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "",
          "TABLE II": "Hubert Large\n0.6823\n317\n0.6746\n0.6148"
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "V. RESULTS AND ANALYSIS",
          "TABLE II": "W2V2 Large\n0.6687\n317\n0.5957\n0.6555"
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "",
          "TABLE II": "0.6587\nData2Vec-A\n313\n0.5926\n0.6557"
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "The CREMA-D database provides annotations for various stimulus",
          "TABLE II": ""
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "",
          "TABLE II": "DeCoAR 2\n0.6462\n90\n0.5830\n0.6433"
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "modalities, including face-only (Face), voice-only (Voice), and audio-",
          "TABLE II": ""
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "",
          "TABLE II": "W2V2 R\n0.6470\n317\n0.5598\n0.6132"
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "visual (AV). We propose the all-inclusive label set (All), a combina-",
          "TABLE II": ""
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "",
          "TABLE II": "0.6118\nW2V\n33\n0.5385\n0.6045"
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "tion of all modalities. To investigate the impact of\nthese modalities",
          "TABLE II": "APC\n0.6079\n4\n0.5433\n0.6040"
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "on emotion perception, we calculate the multi-label distribution labels",
          "TABLE II": "VQ-APC\n0.6030\n5\n0.5380\n0.6021"
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "for\nsix emotions, as described in Section III, using the annotations",
          "TABLE II": "TERA\n0.6043\n21\n0.5964\n0.5416"
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "elicited by different modalities to answer some research questions as",
          "TABLE II": "Mockingjay\n0.5783\n85\n0.5704\n0.5344"
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "",
          "TABLE II": "NPC\n0.5822\n19\n0.5701\n0.5267"
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "below.",
          "TABLE II": ""
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "",
          "TABLE II": "M CPC\n0.5272\n2\n0.4764\n0.5262"
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "3https://emosuperb.github.io/standardization.html",
          "TABLE II": ""
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "",
          "TABLE II": "FBANK\n0.1580\n0\n0.1442\n0.1528"
        },
        {
          "professional actors. This rich dataset comprises 7,442 clips in English,": "4https://github.com/s3prl/s3prl",
          "TABLE II": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "FACE-ONLY, VOICE-ONLY, AND AUDIO-VISUAL, RESPECTIVELY. WE USE": ""
        },
        {
          "FACE-ONLY, VOICE-ONLY, AND AUDIO-VISUAL, RESPECTIVELY. WE USE": "BOLD TO REPRESENT THE BEST PERFORMANCE ACCORDING TO EACH"
        },
        {
          "FACE-ONLY, VOICE-ONLY, AND AUDIO-VISUAL, RESPECTIVELY. WE USE": ""
        },
        {
          "FACE-ONLY, VOICE-ONLY, AND AUDIO-VISUAL, RESPECTIVELY. WE USE": ""
        },
        {
          "FACE-ONLY, VOICE-ONLY, AND AUDIO-VISUAL, RESPECTIVELY. WE USE": ""
        },
        {
          "FACE-ONLY, VOICE-ONLY, AND AUDIO-VISUAL, RESPECTIVELY. WE USE": "Upstream"
        },
        {
          "FACE-ONLY, VOICE-ONLY, AND AUDIO-VISUAL, RESPECTIVELY. WE USE": ""
        },
        {
          "FACE-ONLY, VOICE-ONLY, AND AUDIO-VISUAL, RESPECTIVELY. WE USE": "WavLM Large"
        },
        {
          "FACE-ONLY, VOICE-ONLY, AND AUDIO-VISUAL, RESPECTIVELY. WE USE": "XLS-R-1B"
        },
        {
          "FACE-ONLY, VOICE-ONLY, AND AUDIO-VISUAL, RESPECTIVELY. WE USE": ""
        },
        {
          "FACE-ONLY, VOICE-ONLY, AND AUDIO-VISUAL, RESPECTIVELY. WE USE": "Hubert Large"
        },
        {
          "FACE-ONLY, VOICE-ONLY, AND AUDIO-VISUAL, RESPECTIVELY. WE USE": "W2V2 Large"
        },
        {
          "FACE-ONLY, VOICE-ONLY, AND AUDIO-VISUAL, RESPECTIVELY. WE USE": "Data2Vec-A"
        },
        {
          "FACE-ONLY, VOICE-ONLY, AND AUDIO-VISUAL, RESPECTIVELY. WE USE": ""
        },
        {
          "FACE-ONLY, VOICE-ONLY, AND AUDIO-VISUAL, RESPECTIVELY. WE USE": "DeCoAR 2"
        },
        {
          "FACE-ONLY, VOICE-ONLY, AND AUDIO-VISUAL, RESPECTIVELY. WE USE": ""
        },
        {
          "FACE-ONLY, VOICE-ONLY, AND AUDIO-VISUAL, RESPECTIVELY. WE USE": "W2V2 R"
        },
        {
          "FACE-ONLY, VOICE-ONLY, AND AUDIO-VISUAL, RESPECTIVELY. WE USE": ""
        },
        {
          "FACE-ONLY, VOICE-ONLY, AND AUDIO-VISUAL, RESPECTIVELY. WE USE": "W2V"
        },
        {
          "FACE-ONLY, VOICE-ONLY, AND AUDIO-VISUAL, RESPECTIVELY. WE USE": "APC"
        },
        {
          "FACE-ONLY, VOICE-ONLY, AND AUDIO-VISUAL, RESPECTIVELY. WE USE": "VQ-APC"
        },
        {
          "FACE-ONLY, VOICE-ONLY, AND AUDIO-VISUAL, RESPECTIVELY. WE USE": "TERA"
        },
        {
          "FACE-ONLY, VOICE-ONLY, AND AUDIO-VISUAL, RESPECTIVELY. WE USE": "Mockingjay"
        },
        {
          "FACE-ONLY, VOICE-ONLY, AND AUDIO-VISUAL, RESPECTIVELY. WE USE": "NPC"
        },
        {
          "FACE-ONLY, VOICE-ONLY, AND AUDIO-VISUAL, RESPECTIVELY. WE USE": ""
        },
        {
          "FACE-ONLY, VOICE-ONLY, AND AUDIO-VISUAL, RESPECTIVELY. WE USE": "M CPC"
        },
        {
          "FACE-ONLY, VOICE-ONLY, AND AUDIO-VISUAL, RESPECTIVELY. WE USE": ""
        },
        {
          "FACE-ONLY, VOICE-ONLY, AND AUDIO-VISUAL, RESPECTIVELY. WE USE": "FBANK"
        },
        {
          "FACE-ONLY, VOICE-ONLY, AND AUDIO-VISUAL, RESPECTIVELY. WE USE": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": "Angry"
        },
        {
          "TABLE IV": "0.7738"
        },
        {
          "TABLE IV": "0.6979"
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": "0.7394"
        },
        {
          "TABLE IV": "0.7419"
        },
        {
          "TABLE IV": "0.6139"
        },
        {
          "TABLE IV": "0.6597"
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": "0.6559"
        },
        {
          "TABLE IV": "0.6548"
        },
        {
          "TABLE IV": "0.7164"
        },
        {
          "TABLE IV": "0.7484"
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": "0.7650"
        },
        {
          "TABLE IV": "0.7539"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Face\n0.6750 (0.6691,0.6812)": "AV",
          "0.7484\n0.5593\n0.6507\n0.6264\n0.7628\n0.7021": ""
        },
        {
          "Face\n0.6750 (0.6691,0.6812)": "0.7076 (0.7016,0.7138)\nAV",
          "0.7484\n0.5593\n0.6507\n0.6264\n0.7628\n0.7021": "0.7650\n0.6679\n0.7742\n0.6246\n0.6705\n0.7436"
        },
        {
          "Face\n0.6750 (0.6691,0.6812)": "0.7085 (0.7025,0.7142)\nAll",
          "0.7484\n0.5593\n0.6507\n0.6264\n0.7628\n0.7021": "0.6398\n0.6936\n0.7662\n0.7539\n0.6442\n0.7534"
        },
        {
          "Face\n0.6750 (0.6691,0.6812)": "weights\nanalysis? We\nconduct\na\nlayerwise\nanalysis\nto understand",
          "0.7484\n0.5593\n0.6507\n0.6264\n0.7628\n0.7021": "Regarding the testing conditions using face-only (Face) and audio-"
        },
        {
          "Face\n0.6750 (0.6691,0.6812)": "the importance of different\nlayers in the WavLM Large-based SER",
          "0.7484\n0.5593\n0.6507\n0.6264\n0.7628\n0.7021": "visual\n(AV), our proposed label\ntype (All)\nresults in the best perfor-"
        },
        {
          "Face\n0.6750 (0.6691,0.6812)": "models\ntrained with the\nlabels\nelicited by various modalities. We",
          "0.7484\n0.5593\n0.6507\n0.6264\n0.7628\n0.7021": "mance, demonstrating the\neffectiveness of\nthe proposed label\nsets."
        },
        {
          "Face\n0.6750 (0.6691,0.6812)": "extract\nthe layer weights from the best checkpoint of each model and",
          "0.7484\n0.5593\n0.6507\n0.6264\n0.7628\n0.7021": "The models trained with the proposed label set perform best for sad,"
        },
        {
          "Face\n0.6750 (0.6691,0.6812)": "normalize them using the softmax function to ensure values between",
          "0.7484\n0.5593\n0.6507\n0.6264\n0.7628\n0.7021": "disgust, and happy emotions on the three testing conditions."
        },
        {
          "Face\n0.6750 (0.6691,0.6812)": "0 and 1. We average the layerwise weights across multiple partitions",
          "0.7484\n0.5593\n0.6507\n0.6264\n0.7628\n0.7021": ""
        },
        {
          "Face\n0.6750 (0.6691,0.6812)": "of\nthe CREMA-D dataset. Fig. 2 plots\nthe layer weights across all",
          "0.7484\n0.5593\n0.6507\n0.6264\n0.7628\n0.7021": "VI. DISCUSSION AND LIMITATIONS"
        },
        {
          "Face\n0.6750 (0.6691,0.6812)": "models\ntrained with emotion labels\nelicited by various modalities",
          "0.7484\n0.5593\n0.6507\n0.6264\n0.7628\n0.7021": "While the study by Paulmann et al. [7] suggests that humans have"
        },
        {
          "Face\n0.6750 (0.6691,0.6812)": "(voice-only,\nface-only, and audio-visual). The models tend to assign",
          "0.7484\n0.5593\n0.6507\n0.6264\n0.7628\n0.7021": "enhanced emotion recognition with multimodal stimuli compared to"
        },
        {
          "Face\n0.6750 (0.6691,0.6812)": "higher weights to the 10th to 15th layers, suggesting that these middle",
          "0.7484\n0.5593\n0.6507\n0.6264\n0.7628\n0.7021": "single modalities, our\nresearch found that audio-only labels are the"
        },
        {
          "Face\n0.6750 (0.6691,0.6812)": "layers encode more emotional\ninformation than the earlier or\nlater",
          "0.7484\n0.5593\n0.6507\n0.6264\n0.7628\n0.7021": "most\neffective\nfor\ntraining SER systems when\nonly\nspeech\ninput"
        },
        {
          "Face\n0.6750 (0.6691,0.6812)": "layers. Interestingly, the model trained with voice-only labels exhibits",
          "0.7484\n0.5593\n0.6507\n0.6264\n0.7628\n0.7021": "is available. Systems were evaluated using audio-only, visual-only,"
        },
        {
          "Face\n0.6750 (0.6691,0.6812)": "more balanced weights across\nlayers\nthan those taught with labels",
          "0.7484\n0.5593\n0.6507\n0.6264\n0.7628\n0.7021": "audio-visual, and all-inclusive labels, with the audio-only approach"
        },
        {
          "Face\n0.6750 (0.6691,0.6812)": "elicited by other modalities.",
          "0.7484\n0.5593\n0.6507\n0.6264\n0.7628\n0.7021": "proving to be\nthe most optimal. Multimodal\nemotion systems\nare"
        },
        {
          "Face\n0.6750 (0.6691,0.6812)": "Which\nlabels\nelicited\nby\nvarious modalities\nare\nthe most",
          "0.7484\n0.5593\n0.6507\n0.6264\n0.7628\n0.7021": "not used since current SER systems predominantly rely exclusively"
        },
        {
          "Face\n0.6750 (0.6691,0.6812)": "effective\nfor SER systems? We\nchoose\nthe best model\n(WavLM",
          "0.7484\n0.5593\n0.6507\n0.6264\n0.7628\n0.7021": "on audio input. Moreover,\nto ensure that emotion predictions closely"
        },
        {
          "Face\n0.6750 (0.6691,0.6812)": "Large)\nin Table\nIII\nas\nthe\nbackbone model\nfor\nthe\nexperiments.",
          "0.7484\n0.5593\n0.6507\n0.6264\n0.7628\n0.7021": "resemble human perceptions, speech-only emotion recognition evalu-"
        },
        {
          "Face\n0.6750 (0.6691,0.6812)": "Table IV summarizes the performance of SER systems trained with",
          "0.7484\n0.5593\n0.6507\n0.6264\n0.7628\n0.7021": "ations must use labels derived from voice-only contexts. Additionally,"
        },
        {
          "Face\n0.6750 (0.6691,0.6812)": "labels elicited by various modalities and evaluated on test sets defined",
          "0.7484\n0.5593\n0.6507\n0.6264\n0.7628\n0.7021": "the findings\nfrom the mentioned\nstudy might\nnot\napply\nto\nother"
        },
        {
          "Face\n0.6750 (0.6691,0.6812)": "by labels\nelicited by different modalities. The\n“Train Set”\ncolumn",
          "0.7484\n0.5593\n0.6507\n0.6264\n0.7628\n0.7021": "emotion recognition systems,\nsuch as\nthose based on facial expres-"
        },
        {
          "Face\n0.6750 (0.6691,0.6812)": "shows\nthe models\ntrained by which label\ntype\n(Voice, Face, AV,",
          "0.7484\n0.5593\n0.6507\n0.6264\n0.7628\n0.7021": "sions,\ntext, or audio-visual\ninputs. Besides, our experiments have one"
        },
        {
          "Face\n0.6750 (0.6691,0.6812)": "or All),\nand\nthe\n“Test Set”\ncolumn\nshows\nthe\ntesting\nlabel\ntype",
          "0.7484\n0.5593\n0.6507\n0.6264\n0.7628\n0.7021": "limitation:\nthey were\nconducted on a\nsingle\nemotion database,\nas"
        },
        {
          "Face\n0.6750 (0.6691,0.6812)": "that decides the ground truth of\nthe test sets. The testing conditions",
          "0.7484\n0.5593\n0.6507\n0.6264\n0.7628\n0.7021": "no other publicly available databases provide emotional annotations"
        },
        {
          "Face\n0.6750 (0.6691,0.6812)": "are of\nthree types: Voice, Face, and AV.\nIn the Overall column of",
          "0.7484\n0.5593\n0.6507\n0.6264\n0.7628\n0.7021": "across various stimuli."
        },
        {
          "Face\n0.6750 (0.6691,0.6812)": "Table IV, we report\nthe macro-F1 scores along with the lower and",
          "0.7484\n0.5593\n0.6507\n0.6264\n0.7628\n0.7021": ""
        },
        {
          "Face\n0.6750 (0.6691,0.6812)": "upper bounds of\nthe confidence interval between 2.75% and 97.5%",
          "0.7484\n0.5593\n0.6507\n0.6264\n0.7628\n0.7021": "VII. CONCLUSION AND FUTURE WORK"
        },
        {
          "Face\n0.6750 (0.6691,0.6812)": "for each result. Additionally, we present\nthe sample-based F1 scores",
          "0.7484\n0.5593\n0.6507\n0.6264\n0.7628\n0.7021": "This work compares SER systems\n(based on 14 SSLMs)\ntrained"
        },
        {
          "Face\n0.6750 (0.6691,0.6812)": "for\nthe recognition performance of each emotion.",
          "0.7484\n0.5593\n0.6507\n0.6264\n0.7628\n0.7021": "with\nlabels\nelicited\nfrom various\nemotional\nstimuli\n(multi-modal"
        },
        {
          "Face\n0.6750 (0.6691,0.6812)": "Interestingly, when\ntested\non\nthe\nvoice-only\ncondition\n(Voice),",
          "0.7484\n0.5593\n0.6507\n0.6264\n0.7628\n0.7021": "and uni-modal). The\nresults\nshow that\nthe different modalities of"
        },
        {
          "Face\n0.6750 (0.6691,0.6812)": "the SER system trained with voice-only labels achieved promising",
          "0.7484\n0.5593\n0.6507\n0.6264\n0.7628\n0.7021": "emotional\nstimuli can significantly impact\nthe performance of SER"
        },
        {
          "Face\n0.6750 (0.6691,0.6812)": "results compared to models trained with other modalities. This finding",
          "0.7484\n0.5593\n0.6507\n0.6264\n0.7628\n0.7021": "systems. Also, we propose an all-inclusive label\nset\nthat combines"
        },
        {
          "Face\n0.6750 (0.6691,0.6812)": "suggests that voice-only labels are suitable for training SER systems,",
          "0.7484\n0.5593\n0.6507\n0.6264\n0.7628\n0.7021": "labels elicited by multi-modal and uni-modal emotional stimuli. The"
        },
        {
          "Face\n0.6750 (0.6691,0.6812)": "as the input is speech-only. This finding connects to recent benchmark",
          "0.7484\n0.5593\n0.6507\n0.6264\n0.7628\n0.7021": "SER systems trained on the proposed all-inclusive label set achieved"
        },
        {
          "Face\n0.6750 (0.6691,0.6812)": "databases (such as MSP-PODCAST [1])\nthat use audio-only stimuli",
          "0.7484\n0.5593\n0.6507\n0.6264\n0.7628\n0.7021": "the best performance on test sets for facial-only and audio-visual sce-"
        },
        {
          "Face\n0.6750 (0.6691,0.6812)": "for\nlabels. Furthermore, we observed that\nthe model\ntrained with",
          "0.7484\n0.5593\n0.6507\n0.6264\n0.7628\n0.7021": "narios. Moreover, the SER systems trained solely on labels elicited by"
        },
        {
          "Face\n0.6750 (0.6691,0.6812)": "the proposed all-inclusive\nlabel\nset\n(All) performed better on sad",
          "0.7484\n0.5593\n0.6507\n0.6264\n0.7628\n0.7021": "the voice-only stimuli provided promising results on the voice-only"
        },
        {
          "Face\n0.6750 (0.6691,0.6812)": "and disgusted emotions\nthan the one trained with voice-only labels",
          "0.7484\n0.5593\n0.6507\n0.6264\n0.7628\n0.7021": "test\ncondition,\nsuggesting that voice-only training is preferable\nfor"
        },
        {
          "Face\n0.6750 (0.6691,0.6812)": "(Voice).",
          "0.7484\n0.5593\n0.6507\n0.6264\n0.7628\n0.7021": "speech-only applications. The findings connect\nto recent benchmark"
        },
        {
          "Face\n0.6750 (0.6691,0.6812)": "",
          "0.7484\n0.5593\n0.6507\n0.6264\n0.7628\n0.7021": "databases (such as MSP-PODCAST)\nthat use audio-only stimuli\nfor"
        },
        {
          "Face\n0.6750 (0.6691,0.6812)": "Voice\nFace\nAV",
          "0.7484\n0.5593\n0.6507\n0.6264\n0.7628\n0.7021": "labels and align with how humans perceive emotions through voice"
        },
        {
          "Face\n0.6750 (0.6691,0.6812)": "0.0450",
          "0.7484\n0.5593\n0.6507\n0.6264\n0.7628\n0.7021": ""
        },
        {
          "Face\n0.6750 (0.6691,0.6812)": "",
          "0.7484\n0.5593\n0.6507\n0.6264\n0.7628\n0.7021": "alone. Also, the findings suggest that speech-only SER systems find it"
        },
        {
          "Face\n0.6750 (0.6691,0.6812)": "0.0425",
          "0.7484\n0.5593\n0.6507\n0.6264\n0.7628\n0.7021": "challenging to interpret emotional\nratings derived from audio-visual"
        },
        {
          "Face\n0.6750 (0.6691,0.6812)": "Weights",
          "0.7484\n0.5593\n0.6507\n0.6264\n0.7628\n0.7021": "or\nface-only modalities, as these lack the inherent emotional signals"
        },
        {
          "Face\n0.6750 (0.6691,0.6812)": "0.0400",
          "0.7484\n0.5593\n0.6507\n0.6264\n0.7628\n0.7021": ""
        },
        {
          "Face\n0.6750 (0.6691,0.6812)": "",
          "0.7484\n0.5593\n0.6507\n0.6264\n0.7628\n0.7021": "in voice. In future work, we plan to incorporate systems that can take"
        },
        {
          "Face\n0.6750 (0.6691,0.6812)": "0.0375",
          "0.7484\n0.5593\n0.6507\n0.6264\n0.7628\n0.7021": "different modalities, such as audio and video inputs."
        },
        {
          "Face\n0.6750 (0.6691,0.6812)": "1\n2\n3\n4\n5\n6\n7\n8\n9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25",
          "0.7484\n0.5593\n0.6507\n0.6264\n0.7628\n0.7021": ""
        },
        {
          "Face\n0.6750 (0.6691,0.6812)": "Layer",
          "0.7484\n0.5593\n0.6507\n0.6264\n0.7628\n0.7021": ""
        },
        {
          "Face\n0.6750 (0.6691,0.6812)": "",
          "0.7484\n0.5593\n0.6507\n0.6264\n0.7628\n0.7021": "ACKNOWLEDGMENT"
        },
        {
          "Face\n0.6750 (0.6691,0.6812)": "Fig. 2.\nThe layerwise weights of WavLM-based SER systems\ntrained with",
          "0.7484\n0.5593\n0.6507\n0.6264\n0.7628\n0.7021": ""
        },
        {
          "Face\n0.6750 (0.6691,0.6812)": "the labels elicited by various modalities. The Face, Voice, and AV represent",
          "0.7484\n0.5593\n0.6507\n0.6264\n0.7628\n0.7021": "This work was supported by the NSTC under Grant 113-2634-F-"
        },
        {
          "Face\n0.6750 (0.6691,0.6812)": "face-only, voice-only, audio-visual, and all-inclusive,\nrespectively.",
          "0.7484\n0.5593\n0.6507\n0.6264\n0.7628\n0.7021": "002-003. We thank Professor Hung-yi Lee for his valuable comments."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "sentations with vector quantization,” arXiv preprint arXiv:2012.06659,"
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "2020."
        },
        {
          "REFERENCES": "[1] R. Lotfian and C. Busso, “Building Naturalistic Emotionally Balanced",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "[21] Y.-A. Chung et al., “An unsupervised autoregressive model\nfor speech"
        },
        {
          "REFERENCES": "Speech Corpus by Retrieving Emotional Speech From Existing Podcast",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "representation learning,” arXiv preprint arXiv:1904.03240, 2019."
        },
        {
          "REFERENCES": "Recordings,” IEEE Transactions on Affective Computing, vol. 10, no. 4,",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "arXiv\n[22] ——,\n“Vector-quantized\nautoregressive\npredictive\ncoding,”"
        },
        {
          "REFERENCES": "pp. 471–483, October-December 2019.",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "preprint arXiv:2005.08392, 2020."
        },
        {
          "REFERENCES": "et\n[2] C. Busso\nal.,\n“IEMOCAP:\nInteractive\nemotional\ndyadic motion",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "et\n[23] A. H. Liu\nal.,\n“Non-autoregressive\npredictive\ncoding\nfor\nlearn-"
        },
        {
          "REFERENCES": "Journal\nof\nLanguage Resources\ncapture\ndatabase,”\nand Evaluation,",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "arXiv\npreprint\ning\nspeech\nrepresentations\nfrom local\ndependencies,”"
        },
        {
          "REFERENCES": "vol. 42, no. 4, pp. 335–359, December 2008.",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "arXiv:2011.00406, 2020."
        },
        {
          "REFERENCES": "[3]\nL. Goncalves\nand C. Busso,\n“Improving Speech Emotion Recogni-",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "[24] A. T. Liu et al., “Tera: Self-supervised learning of\ntransformer encoder"
        },
        {
          "REFERENCES": "tion Using Self-Supervised Learning with Domain-Specific Audiovisual",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "representation for speech,” IEEE/ACM Transactions on Audio, Speech,"
        },
        {
          "REFERENCES": "Tasks,” in Proc.\nInterspeech 2022, 2022, pp. 1168–1172.",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "and Language Processing, vol. 29, pp. 2351–2366, 2021."
        },
        {
          "REFERENCES": "[4] H.-C. Chou et al., “The Importance of Calibration: Rethinking Confi-",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "[25] ——, “Mockingjay: Unsupervised speech representation learning with"
        },
        {
          "REFERENCES": "dence and Performance of Speech Multi-label Emotion Classifiers,” in",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "deep bidirectional\ntransformer encoders,” in ICASSP 2020-2020 IEEE"
        },
        {
          "REFERENCES": "Proc.\nINTERSPEECH 2023, 2023, pp. 641–645.",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "International Conference on Acoustics, Speech and Signal Processing"
        },
        {
          "REFERENCES": "[5]\nS. wen Yang et al., “SUPERB: Speech Processing Universal PERfor-",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "(ICASSP).\nIEEE, 2020, pp. 6419–6423."
        },
        {
          "REFERENCES": "mance Benchmark,” in Proc.\nInterspeech 2021, 2021, pp. 1194–1198.",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "[26] A. Babu et al., “XLS-R: Self-supervised cross-lingual speech represen-"
        },
        {
          "REFERENCES": "[6]\nJ. Wagner\net al.,\n“Dawn of\nthe Transformer Era\nin Speech Emotion",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "tation learning at scale,” arXiv preprint arXiv:2111.09296, 2021."
        },
        {
          "REFERENCES": "Recognition: Closing the Valence Gap,” IEEE Transactions on Pattern",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "[27]\nS. Chen et al., “Wavlm: Large-scale self-supervised pre-training for full"
        },
        {
          "REFERENCES": "Analysis and Machine Intelligence, vol. 45, no. 9, pp. 10 745–10 759,",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "IEEE Journal of Selected Topics\nin Signal\nstack speech processing,”"
        },
        {
          "REFERENCES": "2023.",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "Processing, vol. 16, no. 6, pp. 1505–1518, 2022."
        },
        {
          "REFERENCES": "[7]\nS. Paulmann and M. D. Pell,\n“Is\nthere\nan advantage\nfor\nrecognizing",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "[28] A. Baevski et al., “data2vec: A General Framework for Self-supervised"
        },
        {
          "REFERENCES": "multi-modal emotional stimuli?” Motivation and Emotion, vol. 35, pp.",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "of\nthe\nLearning\nin\nSpeech, Vision\nand Language,”\nin Proceedings"
        },
        {
          "REFERENCES": "192–201, 2011.",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "39th International Conference on Machine Learning,\nser. Proceedings"
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "of Machine Learning Research, K. Chaudhuri, S.\nJegelka, L. Song,"
        },
        {
          "REFERENCES": "[8]\nE. Kim et al., “Age Bias in Emotion Detection: An Analysis of Facial",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "C. Szepesvari, G. Niu, and S. Sabato, Eds., vol. 162.\nPMLR, 17–23"
        },
        {
          "REFERENCES": "Emotion Recognition Performance on Young, Middle-Aged, and Older",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "Jul 2022, pp. 1298–1312."
        },
        {
          "REFERENCES": "of\nthe\n2021 AAAI/ACM Conference\non AI,\nAdults,”\nin Proceedings",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "[29] W.-N. Hsu et al., “Hubert: Self-supervised speech representation learn-"
        },
        {
          "REFERENCES": "Ethics, and Society, ser. AIES ’21.\nNew York, NY, USA: Association",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "ing by masked prediction of hidden units,” IEEE/ACM Transactions on"
        },
        {
          "REFERENCES": "for Computing Machinery,\n2021,\np.\n638–644.\n[Online]. Available:",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "Audio, Speech, and Language Processing, vol. 29, pp. 3451–3460, 2021."
        },
        {
          "REFERENCES": "https://doi.org/10.1145/3461702.3462609",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "et\n[30] A. Baevski\nal.,\n“wav2vec\n2.0: A framework\nfor\nself-supervised"
        },
        {
          "REFERENCES": "[9]\nP. Kumar\nand B. Raman,\n“A BERT based\ndual-channel\nexplainable",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "in\nneural\ninformation\nlearning\nof\nspeech\nrepresentations,” Advances"
        },
        {
          "REFERENCES": "text\nemotion\nrecognition\nsystem,” Neural Networks,\nvol.\n150,\npp.",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "processing systems, vol. 33, pp. 12 449–12 460, 2020."
        },
        {
          "REFERENCES": "392–407,\n2022.\n[Online].\nAvailable:\nhttps://www.sciencedirect.com/",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "[31] ——, “vq-wav2vec: Self-supervised learning of discrete speech repre-"
        },
        {
          "REFERENCES": "science/article/pii/S0893608022000958",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "sentations,” arXiv preprint arXiv:1910.05453, 2019."
        },
        {
          "REFERENCES": "[10]\nP. Riera\net al.,\n“No Sample Left Behind: Towards\na Comprehensive",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "et\n[32]\nS. Schneider\nal.,\n“wav2vec: Unsupervised\npre-training\nfor\nspeech"
        },
        {
          "REFERENCES": "Evaluation of Speech Emotion Recognition Systems,” in Proc. SMM19,",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "recognition,” arXiv preprint arXiv:1904.05862, 2019."
        },
        {
          "REFERENCES": "Workshop on Speech, Music and Mind 2019, Graz, Austria, September",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "[33] W.-N. Hsu et al., “Robust wav2vec 2.0: Analyzing Domain Shift in Self-"
        },
        {
          "REFERENCES": "2019, pp. 11–15.",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "Supervised Pre-Training,” in Proc. Interspeech 2021, 2021, pp. 721–725."
        },
        {
          "REFERENCES": "[11] M. Abdelwahab and C. Busso,\n“Study of dense network approaches",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "[34] A. v. d. Oord et al., “Representation learning with contrastive predictive"
        },
        {
          "REFERENCES": "for speech emotion recognition,” in IEEE International Conference on",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "coding,” arXiv preprint arXiv:1807.03748, 2018."
        },
        {
          "REFERENCES": "Acoustics, Speech and Signal Processing (ICASSP 2018).\nCalgary, AB,",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "et\n[35] Y. Cui\nal.,\n“Class-Balanced Loss Based\non Effective Number\nof"
        },
        {
          "REFERENCES": "Canada:\nIEEE, April 2018, pp. 5084–5088.",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "2019\nIEEE/CVF Conference\non Computer Vision\nand\nSamples,”\nin"
        },
        {
          "REFERENCES": "[12]\nF. Ma, S. L. Huang, and L. Zhang, “An efficient approach for audio-",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "Pattern Recognition (CVPR), California, USA, June 2019."
        },
        {
          "REFERENCES": "visual emotion recognition with missing labels and missing modalities,”",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "[36]\nL. G. others, “Odyssey 2024 - Speech Emotion Recognition Challenge:"
        },
        {
          "REFERENCES": "IEEE International Conference\non Multimedia\nand Expo\n(ICME\nin",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "Dataset, Baseline Framework, and Results,” in The Speaker and Lan-"
        },
        {
          "REFERENCES": "2021), Shenzhen, China, July 2021, pp. 1–6.",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "guage Recognition Workshop (Odyssey 2024), Quebec, Canada,\nJune"
        },
        {
          "REFERENCES": "[13] Y. Lei and H. Cao, “Audio-Visual Emotion Recognition With Preference",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "2024."
        },
        {
          "REFERENCES": "Learning Based on Intended and Multi-Modal Perceived Labels,” IEEE",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "arXiv\npreprint\n[37]\nJ. Opitz\nand\nS. Burst,\n“Macro\nf1\nand macro\nf1,”"
        },
        {
          "REFERENCES": "Transactions on Affective Computing, vol. 14, no. 4, pp. 2954–2969,",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "arXiv:1911.03347, 2019."
        },
        {
          "REFERENCES": "2023.",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "[38]\nF. Pedregosa et al., “Scikit-learn: Machine Learning in Python,” Journal"
        },
        {
          "REFERENCES": "[14] W.-C. Lin et al.,\n“Enhancing resilience\nto missing data\nin audio-text",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "of Machine Learning Research, vol. 12, pp. 2825–2830, 2011."
        },
        {
          "REFERENCES": "emotion\nrecognition with multi-scale\nchunk\nregularization,”\nin ACM",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "[39] H.-C. Chou, L. Goncalves, S.-G. Leem, A. N. Salman, C.-C. Lee,"
        },
        {
          "REFERENCES": "International Conference on Multimodal\nInteraction (ICMI 2023), vol.",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "and C. Busso,\n“Minority Views Matter: Evaluating Speech Emotion"
        },
        {
          "REFERENCES": "To appear, Paris, France, October 2023.",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "Classifiers with Human\nSubjective Annotations\nby\nan All-Inclusive"
        },
        {
          "REFERENCES": "[15] W. Chen et al.,\n“Vesper: A Compact\nand Effective Pretrained Model",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "Aggregation Rule,” IEEE Transactions on Affective Computing, pp. 1–"
        },
        {
          "REFERENCES": "IEEE\nTransactions\non\nAffective\nfor\nSpeech\nEmotion Recognition,”",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "15, 2024."
        },
        {
          "REFERENCES": "Computing, pp. 1–14, 2024.",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "[40]\nL.\nFerrer\nand\nP.\nRiera,\n“Confidence\nIntervals\nfor\nevaluation\nin"
        },
        {
          "REFERENCES": "et\n[16] H. Cao\nal.,\n“CREMA-D: Crowd-Sourced Emotional Multimodal",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "machine\nlearning,”\nComputer\nsoftware,\n2024.\n[Online]. Available:"
        },
        {
          "REFERENCES": "IEEE Transactions\nActors Dataset,”\non Affective Computing,\nvol.\n5,",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "https://github.com/luferrer/ConfidenceIntervals"
        },
        {
          "REFERENCES": "no. 4, pp. 377–390, 2014.",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "[41]\nI. Loshchilov and F. Hutter, “Decoupled Weight Decay Regularization,”"
        },
        {
          "REFERENCES": "[17] H.-C. Chou et al., “Exploiting Annotators’ Typed Description of Emo-",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "in International Conference on Learning Representations, 2019."
        },
        {
          "REFERENCES": "tion Perception to Maximize Utilization of Ratings for Speech Emotion",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "[42] A. Paszke\net al.,\n“Automatic differentiation in PyTorch,”\nin NIPS-W,"
        },
        {
          "REFERENCES": "Recognition,” in IEEE International Conference on Acoustics, Speech",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "2017."
        },
        {
          "REFERENCES": "and Signal Processing (ICASSP 2022), Singapore, May 2022, pp. 7717–",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "[43]\nT. Wolf et al., “Transformers: State-of-the-Art Natural Language Pro-"
        },
        {
          "REFERENCES": "7721.",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "the 2020 Conference on Empirical Methods\ncessing,” in Proceedings of"
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "in Natural Language Processing: System Demonstrations, Q. Liu and"
        },
        {
          "REFERENCES": "[18] A. S. Cowen and D. Keltner, “Semantic Space Theory: A Computational",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "D. Schlangen, Eds.\nOnline: Association for Computational Linguistics,"
        },
        {
          "REFERENCES": "Approach to Emotion,” Trends in Cognitive Sciences, vol. 25, no. 2, pp.",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "Oct. 2020, pp. 38–45."
        },
        {
          "REFERENCES": "124–136, 2021.",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "[44]\nI. Lawrence\nand K. Lin,\n“A concordance\ncorrelation\ncoefficient\nto"
        },
        {
          "REFERENCES": "[19] H. Wu, H.-C. Chou, K.-W. Chang, L. Goncalves, J. Du, J.-S. R. Jang,",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "evaluate reproducibility,” Biometrics, pp. 255–268, 1989."
        },
        {
          "REFERENCES": "C.-C. Lee,\nand H.-Y. Lee,\n“Open-Emotion: A Reproducible EMO-",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "2024\nIEEE\nSUPERB for Speech Emotion Recognition Systems,”\nin",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "sentations with vector quantization,” arXiv preprint arXiv:2012.06659,"
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "2020."
        },
        {
          "REFERENCES": "[1] R. Lotfian and C. Busso, “Building Naturalistic Emotionally Balanced",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "[21] Y.-A. Chung et al., “An unsupervised autoregressive model\nfor speech"
        },
        {
          "REFERENCES": "Speech Corpus by Retrieving Emotional Speech From Existing Podcast",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "representation learning,” arXiv preprint arXiv:1904.03240, 2019."
        },
        {
          "REFERENCES": "Recordings,” IEEE Transactions on Affective Computing, vol. 10, no. 4,",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "arXiv\n[22] ——,\n“Vector-quantized\nautoregressive\npredictive\ncoding,”"
        },
        {
          "REFERENCES": "pp. 471–483, October-December 2019.",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "preprint arXiv:2005.08392, 2020."
        },
        {
          "REFERENCES": "et\n[2] C. Busso\nal.,\n“IEMOCAP:\nInteractive\nemotional\ndyadic motion",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "et\n[23] A. H. Liu\nal.,\n“Non-autoregressive\npredictive\ncoding\nfor\nlearn-"
        },
        {
          "REFERENCES": "Journal\nof\nLanguage Resources\ncapture\ndatabase,”\nand Evaluation,",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "arXiv\npreprint\ning\nspeech\nrepresentations\nfrom local\ndependencies,”"
        },
        {
          "REFERENCES": "vol. 42, no. 4, pp. 335–359, December 2008.",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "arXiv:2011.00406, 2020."
        },
        {
          "REFERENCES": "[3]\nL. Goncalves\nand C. Busso,\n“Improving Speech Emotion Recogni-",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "[24] A. T. Liu et al., “Tera: Self-supervised learning of\ntransformer encoder"
        },
        {
          "REFERENCES": "tion Using Self-Supervised Learning with Domain-Specific Audiovisual",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "representation for speech,” IEEE/ACM Transactions on Audio, Speech,"
        },
        {
          "REFERENCES": "Tasks,” in Proc.\nInterspeech 2022, 2022, pp. 1168–1172.",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "and Language Processing, vol. 29, pp. 2351–2366, 2021."
        },
        {
          "REFERENCES": "[4] H.-C. Chou et al., “The Importance of Calibration: Rethinking Confi-",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "[25] ——, “Mockingjay: Unsupervised speech representation learning with"
        },
        {
          "REFERENCES": "dence and Performance of Speech Multi-label Emotion Classifiers,” in",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "deep bidirectional\ntransformer encoders,” in ICASSP 2020-2020 IEEE"
        },
        {
          "REFERENCES": "Proc.\nINTERSPEECH 2023, 2023, pp. 641–645.",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "International Conference on Acoustics, Speech and Signal Processing"
        },
        {
          "REFERENCES": "[5]\nS. wen Yang et al., “SUPERB: Speech Processing Universal PERfor-",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "(ICASSP).\nIEEE, 2020, pp. 6419–6423."
        },
        {
          "REFERENCES": "mance Benchmark,” in Proc.\nInterspeech 2021, 2021, pp. 1194–1198.",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "[26] A. Babu et al., “XLS-R: Self-supervised cross-lingual speech represen-"
        },
        {
          "REFERENCES": "[6]\nJ. Wagner\net al.,\n“Dawn of\nthe Transformer Era\nin Speech Emotion",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "tation learning at scale,” arXiv preprint arXiv:2111.09296, 2021."
        },
        {
          "REFERENCES": "Recognition: Closing the Valence Gap,” IEEE Transactions on Pattern",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "[27]\nS. Chen et al., “Wavlm: Large-scale self-supervised pre-training for full"
        },
        {
          "REFERENCES": "Analysis and Machine Intelligence, vol. 45, no. 9, pp. 10 745–10 759,",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "IEEE Journal of Selected Topics\nin Signal\nstack speech processing,”"
        },
        {
          "REFERENCES": "2023.",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "Processing, vol. 16, no. 6, pp. 1505–1518, 2022."
        },
        {
          "REFERENCES": "[7]\nS. Paulmann and M. D. Pell,\n“Is\nthere\nan advantage\nfor\nrecognizing",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "[28] A. Baevski et al., “data2vec: A General Framework for Self-supervised"
        },
        {
          "REFERENCES": "multi-modal emotional stimuli?” Motivation and Emotion, vol. 35, pp.",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "of\nthe\nLearning\nin\nSpeech, Vision\nand Language,”\nin Proceedings"
        },
        {
          "REFERENCES": "192–201, 2011.",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "39th International Conference on Machine Learning,\nser. Proceedings"
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "of Machine Learning Research, K. Chaudhuri, S.\nJegelka, L. Song,"
        },
        {
          "REFERENCES": "[8]\nE. Kim et al., “Age Bias in Emotion Detection: An Analysis of Facial",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "C. Szepesvari, G. Niu, and S. Sabato, Eds., vol. 162.\nPMLR, 17–23"
        },
        {
          "REFERENCES": "Emotion Recognition Performance on Young, Middle-Aged, and Older",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "Jul 2022, pp. 1298–1312."
        },
        {
          "REFERENCES": "of\nthe\n2021 AAAI/ACM Conference\non AI,\nAdults,”\nin Proceedings",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "[29] W.-N. Hsu et al., “Hubert: Self-supervised speech representation learn-"
        },
        {
          "REFERENCES": "Ethics, and Society, ser. AIES ’21.\nNew York, NY, USA: Association",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "ing by masked prediction of hidden units,” IEEE/ACM Transactions on"
        },
        {
          "REFERENCES": "for Computing Machinery,\n2021,\np.\n638–644.\n[Online]. Available:",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "Audio, Speech, and Language Processing, vol. 29, pp. 3451–3460, 2021."
        },
        {
          "REFERENCES": "https://doi.org/10.1145/3461702.3462609",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "et\n[30] A. Baevski\nal.,\n“wav2vec\n2.0: A framework\nfor\nself-supervised"
        },
        {
          "REFERENCES": "[9]\nP. Kumar\nand B. Raman,\n“A BERT based\ndual-channel\nexplainable",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "in\nneural\ninformation\nlearning\nof\nspeech\nrepresentations,” Advances"
        },
        {
          "REFERENCES": "text\nemotion\nrecognition\nsystem,” Neural Networks,\nvol.\n150,\npp.",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "processing systems, vol. 33, pp. 12 449–12 460, 2020."
        },
        {
          "REFERENCES": "392–407,\n2022.\n[Online].\nAvailable:\nhttps://www.sciencedirect.com/",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "[31] ——, “vq-wav2vec: Self-supervised learning of discrete speech repre-"
        },
        {
          "REFERENCES": "science/article/pii/S0893608022000958",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "sentations,” arXiv preprint arXiv:1910.05453, 2019."
        },
        {
          "REFERENCES": "[10]\nP. Riera\net al.,\n“No Sample Left Behind: Towards\na Comprehensive",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "et\n[32]\nS. Schneider\nal.,\n“wav2vec: Unsupervised\npre-training\nfor\nspeech"
        },
        {
          "REFERENCES": "Evaluation of Speech Emotion Recognition Systems,” in Proc. SMM19,",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "recognition,” arXiv preprint arXiv:1904.05862, 2019."
        },
        {
          "REFERENCES": "Workshop on Speech, Music and Mind 2019, Graz, Austria, September",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "[33] W.-N. Hsu et al., “Robust wav2vec 2.0: Analyzing Domain Shift in Self-"
        },
        {
          "REFERENCES": "2019, pp. 11–15.",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "Supervised Pre-Training,” in Proc. Interspeech 2021, 2021, pp. 721–725."
        },
        {
          "REFERENCES": "[11] M. Abdelwahab and C. Busso,\n“Study of dense network approaches",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "[34] A. v. d. Oord et al., “Representation learning with contrastive predictive"
        },
        {
          "REFERENCES": "for speech emotion recognition,” in IEEE International Conference on",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "coding,” arXiv preprint arXiv:1807.03748, 2018."
        },
        {
          "REFERENCES": "Acoustics, Speech and Signal Processing (ICASSP 2018).\nCalgary, AB,",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "et\n[35] Y. Cui\nal.,\n“Class-Balanced Loss Based\non Effective Number\nof"
        },
        {
          "REFERENCES": "Canada:\nIEEE, April 2018, pp. 5084–5088.",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "2019\nIEEE/CVF Conference\non Computer Vision\nand\nSamples,”\nin"
        },
        {
          "REFERENCES": "[12]\nF. Ma, S. L. Huang, and L. Zhang, “An efficient approach for audio-",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "Pattern Recognition (CVPR), California, USA, June 2019."
        },
        {
          "REFERENCES": "visual emotion recognition with missing labels and missing modalities,”",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "[36]\nL. G. others, “Odyssey 2024 - Speech Emotion Recognition Challenge:"
        },
        {
          "REFERENCES": "IEEE International Conference\non Multimedia\nand Expo\n(ICME\nin",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "Dataset, Baseline Framework, and Results,” in The Speaker and Lan-"
        },
        {
          "REFERENCES": "2021), Shenzhen, China, July 2021, pp. 1–6.",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "guage Recognition Workshop (Odyssey 2024), Quebec, Canada,\nJune"
        },
        {
          "REFERENCES": "[13] Y. Lei and H. Cao, “Audio-Visual Emotion Recognition With Preference",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "2024."
        },
        {
          "REFERENCES": "Learning Based on Intended and Multi-Modal Perceived Labels,” IEEE",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "arXiv\npreprint\n[37]\nJ. Opitz\nand\nS. Burst,\n“Macro\nf1\nand macro\nf1,”"
        },
        {
          "REFERENCES": "Transactions on Affective Computing, vol. 14, no. 4, pp. 2954–2969,",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "arXiv:1911.03347, 2019."
        },
        {
          "REFERENCES": "2023.",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "[38]\nF. Pedregosa et al., “Scikit-learn: Machine Learning in Python,” Journal"
        },
        {
          "REFERENCES": "[14] W.-C. Lin et al.,\n“Enhancing resilience\nto missing data\nin audio-text",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "of Machine Learning Research, vol. 12, pp. 2825–2830, 2011."
        },
        {
          "REFERENCES": "emotion\nrecognition with multi-scale\nchunk\nregularization,”\nin ACM",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "[39] H.-C. Chou, L. Goncalves, S.-G. Leem, A. N. Salman, C.-C. Lee,"
        },
        {
          "REFERENCES": "International Conference on Multimodal\nInteraction (ICMI 2023), vol.",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "and C. Busso,\n“Minority Views Matter: Evaluating Speech Emotion"
        },
        {
          "REFERENCES": "To appear, Paris, France, October 2023.",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "Classifiers with Human\nSubjective Annotations\nby\nan All-Inclusive"
        },
        {
          "REFERENCES": "[15] W. Chen et al.,\n“Vesper: A Compact\nand Effective Pretrained Model",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "Aggregation Rule,” IEEE Transactions on Affective Computing, pp. 1–"
        },
        {
          "REFERENCES": "IEEE\nTransactions\non\nAffective\nfor\nSpeech\nEmotion Recognition,”",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "15, 2024."
        },
        {
          "REFERENCES": "Computing, pp. 1–14, 2024.",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "[40]\nL.\nFerrer\nand\nP.\nRiera,\n“Confidence\nIntervals\nfor\nevaluation\nin"
        },
        {
          "REFERENCES": "et\n[16] H. Cao\nal.,\n“CREMA-D: Crowd-Sourced Emotional Multimodal",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "machine\nlearning,”\nComputer\nsoftware,\n2024.\n[Online]. Available:"
        },
        {
          "REFERENCES": "IEEE Transactions\nActors Dataset,”\non Affective Computing,\nvol.\n5,",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "https://github.com/luferrer/ConfidenceIntervals"
        },
        {
          "REFERENCES": "no. 4, pp. 377–390, 2014.",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "[41]\nI. Loshchilov and F. Hutter, “Decoupled Weight Decay Regularization,”"
        },
        {
          "REFERENCES": "[17] H.-C. Chou et al., “Exploiting Annotators’ Typed Description of Emo-",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "in International Conference on Learning Representations, 2019."
        },
        {
          "REFERENCES": "tion Perception to Maximize Utilization of Ratings for Speech Emotion",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "[42] A. Paszke\net al.,\n“Automatic differentiation in PyTorch,”\nin NIPS-W,"
        },
        {
          "REFERENCES": "Recognition,” in IEEE International Conference on Acoustics, Speech",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "2017."
        },
        {
          "REFERENCES": "and Signal Processing (ICASSP 2022), Singapore, May 2022, pp. 7717–",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "[43]\nT. Wolf et al., “Transformers: State-of-the-Art Natural Language Pro-"
        },
        {
          "REFERENCES": "7721.",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "the 2020 Conference on Empirical Methods\ncessing,” in Proceedings of"
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "in Natural Language Processing: System Demonstrations, Q. Liu and"
        },
        {
          "REFERENCES": "[18] A. S. Cowen and D. Keltner, “Semantic Space Theory: A Computational",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "D. Schlangen, Eds.\nOnline: Association for Computational Linguistics,"
        },
        {
          "REFERENCES": "Approach to Emotion,” Trends in Cognitive Sciences, vol. 25, no. 2, pp.",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "Oct. 2020, pp. 38–45."
        },
        {
          "REFERENCES": "124–136, 2021.",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "[44]\nI. Lawrence\nand K. Lin,\n“A concordance\ncorrelation\ncoefficient\nto"
        },
        {
          "REFERENCES": "[19] H. Wu, H.-C. Chou, K.-W. Chang, L. Goncalves, J. Du, J.-S. R. Jang,",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": "evaluate reproducibility,” Biometrics, pp. 255–268, 1989."
        },
        {
          "REFERENCES": "C.-C. Lee,\nand H.-Y. Lee,\n“Open-Emotion: A Reproducible EMO-",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "2024\nIEEE\nSUPERB for Speech Emotion Recognition Systems,”\nin",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        },
        {
          "REFERENCES": "Spoken Language Technology Workshop (SLT), 2024.",
          "[20]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Building Naturalistic Emotionally Balanced Speech Corpus by Retrieving Emotional Speech From Existing Podcast Recordings",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "2",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso"
      ],
      "year": "2008",
      "venue": "Journal of Language Resources and Evaluation"
    },
    {
      "citation_id": "3",
      "title": "Improving Speech Emotion Recognition Using Self-Supervised Learning with Domain-Specific Audiovisual Tasks",
      "authors": [
        "L Goncalves",
        "C Busso"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "4",
      "title": "The Importance of Calibration: Rethinking Confidence and Performance of Speech Multi-label Emotion Classifiers",
      "authors": [
        "H.-C Chou"
      ],
      "year": "2023",
      "venue": "Proc. INTERSPEECH 2023"
    },
    {
      "citation_id": "5",
      "title": "SUPERB: Speech Processing Universal PERformance Benchmark",
      "authors": [
        "S Yang"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech 2021"
    },
    {
      "citation_id": "6",
      "title": "Dawn of the Transformer Era in Speech Emotion Recognition: Closing the Valence Gap",
      "authors": [
        "J Wagner"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "7",
      "title": "Is there an advantage for recognizing emotional stimuli?",
      "authors": [
        "S Paulmann",
        "M Pell"
      ],
      "year": "2011",
      "venue": "Motivation and Emotion"
    },
    {
      "citation_id": "8",
      "title": "Age Bias in Emotion Detection: An Analysis of Facial Emotion Recognition Performance on Young, Middle-Aged, and Older Adults",
      "authors": [
        "E Kim"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society, ser. AIES '21",
      "doi": "10.1145/3461702.3462609"
    },
    {
      "citation_id": "9",
      "title": "A BERT based dual-channel explainable text emotion recognition system",
      "authors": [
        "P Kumar",
        "B Raman"
      ],
      "year": "2022",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "10",
      "title": "No Sample Left Behind: Towards a Comprehensive Evaluation of Speech Emotion Recognition Systems",
      "authors": [
        "P Riera"
      ],
      "year": "2019",
      "venue": "Proc. SMM19, Workshop on Speech, Music and Mind"
    },
    {
      "citation_id": "11",
      "title": "Study of dense network approaches for speech emotion recognition",
      "authors": [
        "M Abdelwahab",
        "C Busso"
      ],
      "year": "2018",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "12",
      "title": "An efficient approach for audiovisual emotion recognition with missing labels and missing modalities",
      "authors": [
        "F Ma",
        "S Huang",
        "L Zhang"
      ],
      "year": "2021",
      "venue": "IEEE International Conference on Multimedia and Expo (ICME 2021)"
    },
    {
      "citation_id": "13",
      "title": "Audio-Visual Emotion Recognition With Preference Learning Based on Intended and Multi-Modal Perceived Labels",
      "authors": [
        "Y Lei",
        "H Cao"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "14",
      "title": "Enhancing resilience to missing data in audio-text emotion recognition with multi-scale chunk regularization",
      "authors": [
        "W.-C Lin"
      ],
      "year": "2023",
      "venue": "ACM International Conference on Multimodal Interaction (ICMI 2023)"
    },
    {
      "citation_id": "15",
      "title": "Vesper: A Compact and Effective Pretrained Model for Speech Emotion Recognition",
      "authors": [
        "W Chen"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "16",
      "title": "CREMA-D: Crowd-Sourced Emotional Multimodal Actors Dataset",
      "authors": [
        "H Cao"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "17",
      "title": "Exploiting Annotators' Typed Description of Emotion Perception to Maximize Utilization of Ratings for Speech Emotion Recognition",
      "authors": [
        "H.-C Chou"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2022)"
    },
    {
      "citation_id": "18",
      "title": "Semantic Space Theory: A Computational Approach to Emotion",
      "authors": [
        "A Cowen",
        "D Keltner"
      ],
      "year": "2021",
      "venue": "Trends in Cognitive Sciences"
    },
    {
      "citation_id": "19",
      "title": "Open-Emotion: A Reproducible EMO-SUPERB for Speech Emotion Recognition Systems",
      "authors": [
        "H Wu",
        "H.-C Chou",
        "K.-W Chang",
        "L Goncalves",
        "J Du",
        "J.-S Jang",
        "C.-C Lee",
        "H.-Y Lee"
      ],
      "venue": "2024 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "20",
      "title": "Decoar 2.0: Deep contextualized acoustic representations with vector quantization",
      "authors": [
        "S Ling",
        "Y Liu"
      ],
      "year": "2020",
      "venue": "Decoar 2.0: Deep contextualized acoustic representations with vector quantization",
      "arxiv": "arXiv:2012.06659"
    },
    {
      "citation_id": "21",
      "title": "An unsupervised autoregressive model for speech representation learning",
      "authors": [
        "Y.-A Chung"
      ],
      "year": "2019",
      "venue": "An unsupervised autoregressive model for speech representation learning",
      "arxiv": "arXiv:1904.03240"
    },
    {
      "citation_id": "22",
      "title": "Vector-quantized autoregressive predictive coding",
      "year": "2020",
      "venue": "Vector-quantized autoregressive predictive coding",
      "arxiv": "arXiv:2005.08392"
    },
    {
      "citation_id": "23",
      "title": "Non-autoregressive predictive coding for learning speech representations from local dependencies",
      "authors": [
        "A Liu"
      ],
      "year": "2020",
      "venue": "Non-autoregressive predictive coding for learning speech representations from local dependencies",
      "arxiv": "arXiv:2011.00406"
    },
    {
      "citation_id": "24",
      "title": "Tera: Self-supervised learning of transformer encoder representation for speech",
      "authors": [
        "A Liu"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "25",
      "title": "Mockingjay: Unsupervised speech representation learning with deep bidirectional transformer encoders",
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "26",
      "title": "XLS-R: Self-supervised cross-lingual speech representation learning at scale",
      "authors": [
        "A Babu"
      ],
      "year": "2021",
      "venue": "XLS-R: Self-supervised cross-lingual speech representation learning at scale",
      "arxiv": "arXiv:2111.09296"
    },
    {
      "citation_id": "27",
      "title": "Wavlm: Large-scale self-supervised pre-training for full stack speech processing",
      "authors": [
        "S Chen"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "28",
      "title": "data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language",
      "authors": [
        "A Baevski"
      ],
      "year": "2022",
      "venue": "Proceedings of the 39th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research"
    },
    {
      "citation_id": "29",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "30",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "31",
      "title": "vq-wav2vec: Self-supervised learning of discrete speech representations",
      "year": "2019",
      "venue": "vq-wav2vec: Self-supervised learning of discrete speech representations",
      "arxiv": "arXiv:1910.05453"
    },
    {
      "citation_id": "32",
      "title": "wav2vec: Unsupervised pre-training for speech recognition",
      "authors": [
        "S Schneider"
      ],
      "year": "2019",
      "venue": "wav2vec: Unsupervised pre-training for speech recognition",
      "arxiv": "arXiv:1904.05862"
    },
    {
      "citation_id": "33",
      "title": "Robust wav2vec 2.0: Analyzing Domain Shift in Self-Supervised Pre-Training",
      "authors": [
        "W.-N Hsu"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "34",
      "title": "Representation learning with contrastive predictive coding",
      "authors": [
        "A Oord"
      ],
      "year": "2018",
      "venue": "Representation learning with contrastive predictive coding",
      "arxiv": "arXiv:1807.03748"
    },
    {
      "citation_id": "35",
      "title": "Class-Balanced Loss Based on Effective Number of Samples",
      "authors": [
        "Y Cui"
      ],
      "year": "2019",
      "venue": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "36",
      "title": "Odyssey 2024 -Speech Emotion Recognition Challenge: Dataset, Baseline Framework, and Results",
      "authors": [
        "L Others"
      ],
      "year": "2024",
      "venue": "The Speaker and Language Recognition Workshop"
    },
    {
      "citation_id": "37",
      "title": "Macro f1 and macro f1",
      "authors": [
        "J Opitz",
        "S Burst"
      ],
      "year": "2019",
      "venue": "Macro f1 and macro f1",
      "arxiv": "arXiv:1911.03347"
    },
    {
      "citation_id": "38",
      "title": "Scikit-learn: Machine Learning in Python",
      "authors": [
        "F Pedregosa"
      ],
      "year": "2011",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "39",
      "title": "Minority Views Matter: Evaluating Speech Emotion Classifiers with Human Subjective Annotations by an All-Inclusive Aggregation Rule",
      "authors": [
        "H.-C Chou",
        "L Goncalves",
        "S.-G Leem",
        "A Salman",
        "C.-C Lee",
        "C Busso"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "40",
      "title": "Confidence Intervals for evaluation in machine learning",
      "authors": [
        "L Ferrer",
        "P Riera"
      ],
      "year": "2024",
      "venue": "Computer software"
    },
    {
      "citation_id": "41",
      "title": "Decoupled Weight Decay Regularization",
      "authors": [
        "I Loshchilov",
        "F Hutter"
      ],
      "year": "2019",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "42",
      "title": "Automatic differentiation in PyTorch",
      "authors": [
        "A Paszke"
      ],
      "year": "2017",
      "venue": "NIPS-W"
    },
    {
      "citation_id": "43",
      "title": "Transformers: State-of-the-Art Natural Language Processing",
      "authors": [
        "T Wolf"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations"
    },
    {
      "citation_id": "44",
      "title": "A concordance correlation coefficient to evaluate reproducibility",
      "authors": [
        "I Lawrence",
        "K Lin"
      ],
      "year": "1989",
      "venue": "Biometrics"
    }
  ]
}