{
  "paper_id": "2405.16016v3",
  "title": "Comface: Facial Representation Learning With Synthetic Data For Comparing Faces",
  "published": "2024-05-25T02:44:07Z",
  "authors": [
    "Yusuke Akamatsu",
    "Terumi Umematsu",
    "Hitoshi Imaoka",
    "Shizuko Gomi",
    "Hideo Tsurushima"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Daily monitoring of intra-personal facial changes associated with health and emotional conditions has great potential to be useful for medical, healthcare, and emotion recognition fields. However, the approach for capturing intra-personal facial changes is relatively unexplored due to the difficulty of collecting temporally changing face images. In this paper, we propose a facial representation learning method using synthetic images for comparing faces, called ComFace, which is designed to capture intrapersonal facial changes. For effective representation learning, ComFace aims to acquire two feature representations, i.e., inter-personal facial differences and intra-personal facial changes. The key point of our method is the use of synthetic face images to overcome the limitations of collecting real intra-personal face images. Facial representations learned by ComFace are transferred to three extensive downstream tasks for comparing faces: estimating facial expression changes, weight changes, and age changes from two face images of the same individual. Our Com-Face, trained using only synthetic data, achieves comparable to or better transfer performance than general pretraining and state-of-the-art representation learning methods trained using real images.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Human faces contain a variety of information, including identities, health conditions, and emotions. Face recognition has long been studied for personal identification  [60, 69] . Besides face recognition, biological information such as age  [15, 67] , facial expression  [21, 37] , body weight  [2, 12] , and body mass index (BMI)  [12, 53]  have also been estimated from face images. Estimation of biological information from faces has great potential to be applied to medical, healthcare, and emotion recognition fields. In these fields, it is important to capture daily intra-personal changes associated with health and emotional conditions  [19, 24, 57, 68] . Specifically, monitoring daily weight  [2]  and facial expression changes  [34]  from the face helps to understand a person's health and emotional state.\n\nTo estimate biological information, most previous methods have performed classification or regression analysis from a single face image. For example, a face image is classified into a facial expression class  [37] , or BMI is estimated from a face image  [53] . However, these methods do not focus on capturing subtle intra-personal changes in health and emotions. Since it is useful to understand the subtle changes, capturing temporal changes in the face within an individual is important. One of the reasons estimating intrapersonal changes from face images has remained relatively unexplored is that it is difficult to collect a large number of temporally changing face images.\n\nMany existing methods for face analysis have used deep neural networks (DNNs) with supervised learning. To overcome the limitations of training data annotated with supervised labels, the mainstream approach utilizes DNNs pretrained on large-scale data (e.g., ImageNet  [13]  and VG-GFace2  [7] ) and then performs transfer learning with annotated face data. Also, facial representation learning (FRL) has been recently attracting attention  [6, 40, 64, 70] , where a large amount of unlabeled face images  [6, 40, 64]  and face image-text pairs  [70]  are used to obtain prior knowledge about facial representation and improve the performance of downstream face tasks. Nevertheless, existing FRL methods mainly learn representations of facial differences between individuals and neglect those of facial changes within an individual. Furthermore, visual representation learning using synthetic images has recently emerged in the computer vision domain  [55, 56] , surpassing the performance of representations learned using real images. Even in the face domain, representation learning using synthetic images for temporally changing faces, which are very scarce in real images, has the potential to boost the estimation performance of intra-personal facial changes.\n\nIn this paper, we propose an FRL method using synthetic data for comparing faces, called ComFace, which is designed to capture intra-personal facial changes. Fig- ure 1 is an overview of ComFace framework. ComFace aims to learn two feature representations using synthetic data: inter-personal facial differences and intra-personal facial changes. This makes it feasible to acquire facial representations relating not only to facial differences between individuals, but also to subtle facial changes within an individual. To address the limitation in the number of real intra-personal face images, ComFace utilizes synthetic face images generated by StyleGANs  [27] [28] [29] [30] . We generate synthetic face images for a large number of individuals and then generate intra-personal face images that vary according to many facial attributes. Benefiting from synthetic data, ComFace can use an unlimited number of individuals and intra-personal face images, overcoming the problem of existing real image datasets with a small number of temporally changing face images. The first FRL in ComFace is self-supervised contrastive learning  [9]  to identify facial differences between individuals. With self-supervised learning, ComFace learns feature representations of inter-personal facial differences without supervised labels. In the second FRL, ComFace learns feature representations of intra-personal facial changes. Specifically, how much the intra-personal face has changed is learned from two face images of the same individual. Furthermore, curriculum learning  [5]  is introduced to gradually increase the difficulty level of estimating intra-personal facial changes during training, resulting in successful representation learning related to subtle facial changes.\n\nWe transfer DNNs pre-trained by ComFace to three downstream tasks for comparing faces: estimating facial expression changes, weight changes, and age changes from two face images of the same individual. In these downstream tasks, we aim to estimate the direction and degree of facial expression change (e.g., +2 and -3 intensity), weight change (e.g., +2.6 kg and -1.3 kg), and age change (e.g., +10 and -5 years) from two face images. For facial expression change, we estimate the change in intensity of facial expressions for action unit 6 (AU6) and AU12. For weight change, we estimate the weight change associated with the degree of facial edema in dialysis patients, as shown in a recent study  [2] . These downstream tasks consist of various periods of temporal changes in the face: short (i.e., facial expression change), medium (i.e., weight change), and long (i.e., age change) periods.\n\nThe main contributions of this paper are summarized as follows:\n\n• We propose ComFace, the first FRL method using synthetic face images for comparing faces. ComFace learns feature representations regarding intra-personal facial changes as well as inter-personal facial differences. • Facial representations learned by ComFace are transferred to three extensive downstream tasks for comparing faces. ComFace achieves comparable to or superior transfer performance to general pre-training and state-of-the-art (SoTA) representation learning methods trained using real images. • Our approach of comparing two face images within an individual generalizes well to new patients and environmental conditions not used for training data.\n\nOur weight change estimation model, trained without patient-specific data, outperforms the previous method  [2]  that estimates weight from a single face image, trained with patient-specific data.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Face Analysis: Various biological information has been estimated from a single face image  [2, 43, 53, 67] . Akamatsu et al.  [2]  focused on edema, a symptom of kidney disease, and estimated weight from the degree of facial edema in dialysis patients. Weight reflects the fluid volume in a dialysis patient's body, so it is helpful to be able to easily monitor daily weight from a face image. The weakness of their method is that it requires training data from a patient who uses the system in order to construct a patientspecific model. This is because the method  [2]  estimates weight from a single face image and does not generalize well to new patients not used for training data. In contrast, our approach of comparing two intra-personal face images generalizes well to new patients and does not require patient-specific data. Our motivation to capture daily intra-personal facial changes is similar to Ref.  [34] , which estimates changes in the degree of smiling from two face images. While the method  [34]  performs transfer learning using a pre-trained model for face recognition, we use a pre-trained model specialized to capture intra-personal facial changes on the basis of synthetic images and achieve better transfer performance. Facial Representation Learning: Transfer learning in face analysis tasks commonly depends on pre-training using ImageNet  [2, 15, 38, 45]  and large face recognition datasets  [33, 47, 51, 53, 54] . Recently, facial representation learning (FRL) has been studied as a pre-training method for face analysis tasks  [6, 40, 70] . Bulat et al.  [6]  investigated pre-training strategies and datasets for several face analysis tasks. Zheng et al.  [70]  proposed a weakly-supervised method called FaRL using face image-text pairs. Liu et al.  [40]  proposed a pose-disentangled contrastive learning (PCL) method for general self-supervised facial representation. Different from the FRL methods described above, our ComFace focuses on FRL regarding intra-personal facial changes. Inter-and intra-personal influences on human interactions have been studied previously  [31, 63] . These methods simply handle inter-and intra-personal multimodal signals (e.g., video  [31] , audio  [31, 63] , facial gestures  [63] ) to consider human-human interactions for specific applications such as modeling affect dynamics and adapted behavior synthesis. Our work differs from previous work  [31, 63]  in that our work is a comprehensive representation learning study for downstream tasks that capture intra-personal facial changes. Synthetic Data: Synthetic data for human analysis has been widely explored  [26] , e.g., face recognition  [35, 36, 50, 58, 66] , crowd counting  [61, 62] , and fingerprint recognition  [18, 25] . In particular, with the recent success of generative adversarial networks (GANs)  [20] , the quality of face synthesis has improved rapidly. Qiu et al.  [50]  proposed a face recognition method called SynFace using synthetic face images generated by DiscoFaceGAN  [14] . They explored the performance gap between face recognition models trained with synthetic and real face images and then designed SynFace to suppress the domain gap between synthetic and real face images. Most recently, representation learning using synthetic data has emerged  [16, 55, 56] . Ref.  [16]  is the first study to consider introducing synthetic images into FRL and compares the transfer performance with that of real images. Ref.  [16]  aims to learn general facial representation and differs from our FRL that focuses on intra-personal facial changes. Although synthetic data have been introduced into the face domain, their performance in Refs.  [16, 50]  is still lower than when using real images. Our motivation differs from that of Refs.  [16, 50] ,\n\ni.e., while Refs.  [16, 50]  explores the potential of synthetic images instead of real images, we rather leverage synthetic images since we have very few real images for temporally changing faces. We make the most of the advantages of synthetic images over real images, resulting in our FRL using synthetic images achieving better transfer performance than other methods using real images.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Method",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Synthetic Face Images",
      "text": "Our method utilizes synthetic face images generated by StyleGANs  [27] [28] [29] [30]  for representation learning. Style-GAN is one of the most popular generative models for face synthesis, demonstrating impressive performance in image generation, inversion, and manipulation  [4] . StyleGAN generators synthesize exceedingly realistic images and enable editing  [1, 3, 11, 39, 48, 52] . InterFaceGAN  [52]  is a major framework for face editing. It semantically edits faces by interpreting latent semantics learned by StyleGAN. Specifically, InterFaceGAN can easily edit latent code w in high dimensional space W to manipulate the attributes (e.g., weight, age, and gender) of a synthesized image as w edit = w + α • n m , where w edit is the edited latent code, n m is a normal vector to manipulate for attribute m, and α is a parameter that controls the intensity of the manipulation. It will make the synthesis look more positive for each attribute when α > 0 (e.g., get fat when the attribute m is weight), and α < 0 will make the synthesis look more negative (e.g., slim down when the attribute m is weight). As attributes to manipulate synthetic faces, we use weight  [49] , age  [52] , smile  [52] , and the 40 attributes included in the CelebA dataset  [41] . Examples of synthetic images manipulated when the attribute m is weight are shown on the left side of Fig.  1 . We synthesize a new person's face by randomly calculating the latent code w, and generate images of intra-personal face changes by setting multiple α.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Frl With Synthetic Face Images",
      "text": "Our ComFace learns two feature representations, i.e., inter-personal facial differences and intra-personal facial changes. Since the inter-personal represents broad differences in faces and the intra-personal represents subtle changes in faces, both of these two facial representations are essential to downstream tasks for comparing faces. Figure  2  shows the training scheme of ComFace. The goal of the training is representation learning of the backbone f (•) using synthetic face images. As the backbone f (•), arbitrary DNNs such as ResNet  [23]  and Vision Transformer (ViT)  [17]  are used. During the training, suppose that x i is the i-th synthetic image in the mini-batch (i = {1, • • • , N }, N is the mini-batch size) and the parameter α corresponding to x i is denoted as α xi (see Section 3.1). Also, y i is a synthetic image edited from the face of the same person in x i , and the parameter α corresponding to y i is denoted\n\nwhere z p xj is a vector that forms a positive pair with z xj (i.e., z xi and z xi+N ), 1l k̸ =j ∈ {0, 1} is a function that returns 1 when k ̸ = j and 0 when k = j, and τ is a tem-perature parameter. With the above optimization, feature vectors of the same face are brought closer and those of different faces are kept apart. Intra-personal Learning: We acquire feature representations of intra-personal facial changes. The synthetic image y i is converted into the image ỹi by data augmentation. Then, ỹi is input to f (•), which outputs the vector h ỹi = f ( ỹi ). We next calculate the difference vector h ỹi -h xi between h xi and h ỹi to capture intra-personal facial changes. Furthermore, the difference vector is input to ReLU and a linear layer l(•), which outputs the distance of intra-personal facial changes d i . ComFace learns how much the intrapersonal face has changed on the basis of the mean squared error (MSE) loss: 2 , where |α yi -α xi | is the ground truth (GT) for the distance of intra-personal facial changes. Since the direction of facial change depends on the attribute, we use |α yi -α xi | instead of α yi -α xi as the intra-personal GT.\n\nAs described above, ComFace acquires two feature representations, inter-personal facial differences and intrapersonal facial changes, using the sum of those loss functions: L = L inter + L intra .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Curriculum Learning Of Intra-Personal Facial Changes",
      "text": "In intra-personal learning, the difficulty level of learning tasks depends on α. Specifically, when α = 0.0 and 5.0, there are significant differences in the facial changes, so it is easy to identify them (see the left side of Fig.  1 ). On the other hand, when α = 0.0 and 2.0, the facial changes are small, so it is difficult to identify them (see the left side of Fig.  1 ). ComFace provides effective representation learning by increasing the difficulty level of learning tasks on the basis of curriculum learning. Curriculum learning  [5]  mimics the human learning behavior of starting with simple tasks and gradually learning more complex concepts. ComFace gradually decreases the distance between intra-personal facial changes |α yi -α xi | during training to acquire more effective feature representations. Specifically, suppose that S is the range of the distance of facial changes |α yi -α xi | when sampling the pairs of x i and y i used for training, and we gradually decrease S according to the number of epochs as follows:\n\n, S e = 1 (e ≤ e 1 ) t (e t-1 < e ≤ e t ),\n\nwhere S max is the maximum range of the distance of facial changes in the dataset, and e is the number of epochs. In Eq. (  2 ), we perform curriculum learning by increasing the difficulty level of learning tasks with the progress of the training (i.e., with the increase in the number of epochs).",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Transfer Learning Toward Downstream Tasks",
      "text": "We transfer DNNs pre-trained by ComFace to downstream tasks for comparing faces. For downstream tasks, we utilize DNNs in intra-personal learning (see the red box in Fig.  2 ). Since the linear layer l(•) trained by intra-personal learning is useful for comparing faces, we transfer both backbone f (•) and linear layer l(•). In transfer learning, we perform downstream tasks for comparing faces using two face images, x task i and y task i )) 2 . In each downstream task, we estimate not only the distance but also the direction of facial changes (e.g., increase/decrease of weight). Therefore, unlike the distance of the facial change in the intra-personal GT (see Section 3.2), we use α y task i -α x task i as GT for downstream tasks.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Setup For Frl",
      "text": "Synthetic Face Images: We utilize synthetic face images generated by StyleGAN  [29]  and StyleGAN3  [28] . The reason for using two different StyleGANs is to employ the attributes for face manipulation provided in each Style-GAN. In StyleGAN, we use the attributes that vary weight provided by Ref.  [49]  and the attributes that vary age and smile provided by InterFaceGAN  [52] . In StyleGAN3, we employ the 40 attributes included in the CelebA dataset  [41]  provided by Ref.  [4] . The 40 attributes contain various facerelated factors such as Big Nose, Bags Under Eyes, and Pale Skin. With these 43 attributes, we use synthetic face images that change according to a wide range of face attributes. We generate synthetic face images with 250,000 identities using StyleGANs. For each identity, synthetic images for each attribute with α = {-5.0, -4.9, • • • , 0.0, • • • , 4.9, 5.0} are generated (see Section 3.1). As a result, the total number of synthetic face images used for FRL is 35 million (M). For details on the composition of synthetic images, please refer to Supplement A.1. See Supplement C.1 for a quality assessment of the synthetic images and an evaluation of the transfer performance with respect to the quality. Details for FRL: For FRL with synthetic images, we use 90% of all identities as training data and 10% as validation data. As a backbone f (•), we use ResNet50  [23] , which is commonly used for general pre-training  [7, 13]  and representation learning  [6, 9, 22] . Since ResNet50 is better than ViT  [17]  for ComFace (see Supplement C.2 for the evaluation), we employ ResNet50. Synthetic images are resized to 224×224 and the temperature parameter τ is set to 0.1. Our model is trained from scratch with randomly initialized weights. We run the training for 12 epochs with batch size 1024 on 32 NVIDIA A100 GPUs (∼22 hours training). As parameters for curriculum learning, we set S max = 10, t ∈ {2, 3, 4}, e 1 = 3, e 2 = 6, e 3 = 9, e 4 = 12. For each epoch, we randomly sample x i and y i pairs according to the range S in curriculum learning and construct mini-batches. Adam  [32]  optimizer is used, and the learning rate is initialized as 4e-4 and halved in 10 epochs. See Supplement B.1 for other setups.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Setup For Downstream Tasks",
      "text": "We use the following facial expression change, weight change, and age change datasets in our downstream tasks for comparing faces. The three downstream tasks correspond to short (∼1 day), medium (1 day ∼ 3 months), and long (1 year∼) time periods of temporal changes in the face, respectively. Facial Expression Change Dataset: We use the public dataset DISFA  [42, 43] , which contains facial videos of 27 subjects (12 women and 15 men) while they watch a 4-minute video intended to elicit a range of facial expressions. For each subject, 4845 video frames were recorded, and the action unit (AU) intensity was annotated for each frame with six levels from 0 (not present) to 5 (maximum intensity) for several AUs. We use AU6 (Cheek Raiser) and AU12 (Lip Corner Puller), which contain frames with a high AU intensity, and we extract frames with an intensity from 1 to 5. We estimate the intensity changes of AU6 and AU12 as facial expression changes. See Supplement C.3 for an evaluation for other major AUs. Weight Change Dataset: We use the dataset collected in Ref.  [2]  (Edema-A) and our newly collected dataset (Edema-B). These datasets contain face images and weight data obtained from dialysis patients before and after dialysis. Edema-A and Edema-B were collected from different hospitals and different patients. Dialysis removes fluid from the body, which generally results in 2 to 3 kg weight change, with edema appearing on the face before dialysis and edema being alleviated after dialysis  [2] . In Edema-A, the number of patients is 38, the total number of acquired data is 392 (including pre-and post-dialysis), and the total number of images is 39200 (using 100 images per data). In Edema-B, the number of patients is 19, the total number of acquired data is 320, and the total number of images is 32000. Age Change Dataset: We use the public dataset FG-NET  [46] , which contains 1002 face images from 82 subjects. The age ranges from 0 to 69 years, and the number of images per subject is 12 on average. See Supplement A.2 for more information on the above datasets.\n\nDetails for Downstream Tasks: For each dataset, we perform a four-fold cross-validation that splits subjects between the training and test data. Hence, we evaluate the generalization performance for new subjects not included in the training data. We use 10% of the subjects in the training data as validation data. For both training and testing, the estimation of intra-personal facial changes is performed using two randomly sampled face images within an individual (i.e., approximately the same number of samples in the direction of increasing or decreasing changes). Note that the sampled pairs are identical across methods. For transfer learning, we use two types of evaluation, linear evaluation (i.e., backbone f (•) is frozen and linear layer l(•) is trained from scratch 1  ) and fine-tuning (i.e., all layers in f (•) and l(•) are trained from pre-trained weights 2  ), as in the previous representation learning studies  [6, 9, 22] . See Supplement B.2 for other setups.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Comparative Methods And Evaluation Metrics",
      "text": "We are curious about the following questions: Does ComFace successfully acquire facial representations that capture intra-personal facial changes? How is the transfer performance of representation learning using synthetic images against real images? To answer these questions, we compare ComFace with four types of methods: scratch (random initialization), general pre-training, visual representation learning, and facial representation learning. We use a supervised method using ImageNet  [13]  and a face recognition method using VGGFace2  [7]  for general pre-training. We also use SimCLR  [9] , MoCo v2  [10, 22] , SwAV  [8] , and Barlow Twins  [65]  for visual representation learning. In the above methods, we use ResNet50, which is the same backbone as ours for fair comparison. For facial representation learning, we use Bulat et al.  [6] , FaRL  [70] , and PCL  [40] . Note that all comparative methods use real images to learn feature representations. In Supplement B.3, we summarize The downstream tasks for comparing faces are regression analyses that estimate facial expression, weight, and age changes from two face images. As evaluation metrics, we use the mean absolute error (MAE) and Pearson correlation coefficient (Corr.). In weight change estimation, we also use accuracy (Acc.), which represents the prediction performance in the direction of weight gain or loss, since weight changes before and after dialysis.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Main Results",
      "text": "Facial Expression Change: Table  1  shows the results of estimating facial expression change in AU6 and AU12. First, in linear evaluation  3  , ComFace outperforms all other methods on both AUs by a large margin. This result indicates the advantage of ComFace, which focuses on intrapersonal facial changes, over other methods that do not focus on such changes. Thus, we find that ComFace successfully acquires representations that capture intra-personal facial changes by using synthetic face images. Second, in fine-tuning, ComFace is slightly worse for AU6 and slightly better for AU12 than Bulat et al.  [6] . Our model, trained using only synthetic images, has comparable transfer performance to the SoTA FRL method trained using real images, suggesting the potential of representation learning using synthetic images. Furthermore, ComFace performs better than general pre-training and visual representation learning methods, so our synthetic image-based model outperforms previous baseline methods.\n\nWeight Change: In facial expression change estimation, fine-tuning is superior to linear evaluation for ComFace. Since our ultimate goal is achieving high performance, we  perform an evaluation in fine-tuning for the rest of our experiments. Table  2  shows the results of weight change estimation for an Edema-A, Edema-B, and Edema-A→B cross-dataset evaluation. In the cross-dataset evaluation, models trained on Edema-A are tested directly on Edema-B. This is done to evaluate the robustness of the models against differences in lighting/environmental conditions and patient groups across hospitals. From table 2, we confirm that ComFace outperforms all other methods in most metrics. Specifically, our model has an accuracy improvement of 3.8%, 2.9%, and 4.3% over the best model for general pre-training, visual representation learning, and FRL for Edema-A and 4.4%, 3.5%, and 1.7% for Edema-B, respectively. Furthermore, ComFace has better performance than the other methods in the cross-dataset evaluation. The performance of Edema-A→B in ComFace is close to that of Edema-B, indicating its high robustness. These results suggest that the transfer performance of our representation learning using synthetic images is better than that of other methods using real images. We also confirm that ComFace outperforms a visual representation learning method using synthetic images  [56] . See Supplement C.6 for details. Furthermore, we compare ComFace with the previous method that estimates weight from a single face image  [2] . The previous method performs pre-training on multiple patient data and then builds patient-specific models via transfer learning on per-patient data. As in the original paper  [2] , the patient-specific model uses 24 patients for pre-training   [13]  7.863 0.614 SwAV  [8]  6.368 0.780 FaRL  [70]  5.249 0.851 ComFace (Ours) 4.914 0.870 Table  5 . Ablation study on ComFace. In \"Learning\" column, \"Both\" denotes both inter-and intra-personal learning. Line indicated in gray is our final setting. and 15 patients for transfer learning and testing, and Com-Face uses the same 15 patients for testing. Since the performance of patient-specific models depends on the number of dialysis days on per-patient data for transfer learning, we evaluate the performance when using per-patient data from 1 to 3 days. Table  3  shows the results of weight change estimation with the patient-specific model and our patientgeneric model. Our patient-generic model performs better than the patient-specific model transferred on 1 or 2 days data and is comparable to the patient-specific model on 3 days data. Since our patient-generic model is built without patient-specific data (i.e., 0 days), we confirm the advantage of our two image-based method over the previous single image-based method  [2] . The results suggest that our weight change estimation from two face images within an individual generalizes well to new patients.\n\nAge Change: Table  4  shows the results of estimating age change in fine-tuning. The best models from the four types of comparative methods are listed (see Supplement C.7 for full versions). The table indicates that ComFace has a higher transfer performance for age change than all other methods. We find that our method can be successfully adapted to the task related to long time periods (1 year∼) of temporal changes in the face.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Ablation Study",
      "text": "We ablate the components of ComFace to verify the effectiveness of each factor. In the ablation study, we evaluate transfer performance for three downstream tasks in finetuning. Key observations are described as follows: Inter-and Intra-personal Learning: Table  5  (a,b,c) compares learning strategies. It shows that intra-personal learning is more beneficial than inter-personal learning. We find that representation learning that focuses on subtle facial changes is more essential than broad facial differences. Furthermore, the employment of both inter-and intra-personal",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Visualization",
      "text": "We calculate saliency maps to provide the interpretability of models. The saliency maps are obtained from the final block of the pre-trained backbones via Eigen-CAM  [44] , which visualizes the principle components of learned representations without relying on class relevance scores. Figure  4  illustrates the saliency maps of four models for a face image from DISFA  [42, 43] . Models using ImageNet and SimCLR focus on regions other than the face (e.g., clothing and hair), which is expected due to the fact that the models are trained on general images (i.e., ImageNet). The model using VGGFace2 focuses on the entire face since it is trained for face recognition. Our ComFace focuses on the center of the face, which includes AU6, AU12, and eyelid and nose shapes that are affected by edema  [2]  (see the original image in Fig.  4 ). We believe that ComFace captures the center of the face, where intra-personal facial changes often appear, resulting in high transfer performance of downstream tasks for comparing faces.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusion And Societal Impact",
      "text": "This paper introduces facial representation learning (FRL) using synthetic images for comparing faces. We obtain the following answers from the experimental results: (i) ComFace successfully acquires representations that capture intra-personal facial change. (ii) Our transfer performance in representation learning using synthetic images is comparable to or better than SoTA representation learning methods using real images. (iii) Our approach of comparing two face images generalizes well to new patients and environmental conditions. In future work, we plan to explore more downstream tasks related to facial changes other than facial expression, weight, and age. Potential Negative Societal Impact: Machine learning models using face images may include biases such as ethnicity, age, and gender. Leveraging synthetic images has the potential to reduce those biases by manipulating the generative model. However, it is important to acknowledge that our method relies on generative models trained on images crawled from a large-scale website (Flicker). The generative model inherits the biases of Flicker, and the synthetic images may contain social biases and errors.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview of ComFace framework. ComFace performs facial representation learning using synthetic data relating to inter-personal",
      "page": 2
    },
    {
      "caption": "Figure 1: We synthesize a new person’s face by ran-",
      "page": 3
    },
    {
      "caption": "Figure 2: Training scheme of ComFace. Learning strategy consists of two components, i.e., inter-personal learning and intra-personal",
      "page": 4
    },
    {
      "caption": "Figure 1: ). ComFace provides effective representation learning",
      "page": 4
    },
    {
      "caption": "Figure 2: ). Since the linear layer l(·) trained by intra-personal",
      "page": 5
    },
    {
      "caption": "Figure 3: Transfer performance for facial expression change (AU12), weight change (Edema-A), and age change on different training",
      "page": 8
    },
    {
      "caption": "Figure 4: Saliency maps for a face image in four pre-trained backbones. Original images show positions of AU6 and AU12 and eyelid and",
      "page": 8
    },
    {
      "caption": "Figure 3: represents the transfer performance",
      "page": 8
    },
    {
      "caption": "Figure 4: ). We believe that ComFace captures the",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1NEC Corporation, Japan": "",
          "2University of Tsukuba, Japan": "†yusuke-akamatsu@nec.com"
        },
        {
          "1NEC Corporation, Japan": "Abstract",
          "2University of Tsukuba, Japan": "conditions [19, 24, 57, 68].\nSpecifically, monitoring daily"
        },
        {
          "1NEC Corporation, Japan": "",
          "2University of Tsukuba, Japan": "weight [2] and facial expression changes [34] from the face"
        },
        {
          "1NEC Corporation, Japan": "Daily monitoring of\nintra-personal\nfacial changes as-",
          "2University of Tsukuba, Japan": "helps to understand a person’s health and emotional state."
        },
        {
          "1NEC Corporation, Japan": "sociated with health and emotional conditions has great",
          "2University of Tsukuba, Japan": "To estimate biological information, most previous meth-"
        },
        {
          "1NEC Corporation, Japan": "potential\nto be useful\nfor medical, healthcare, and emo-",
          "2University of Tsukuba, Japan": "ods have performed classification or\nregression analysis"
        },
        {
          "1NEC Corporation, Japan": "tion recognition fields. However,\nthe approach for captur-",
          "2University of Tsukuba, Japan": "from a single face image. For example, a face image is clas-"
        },
        {
          "1NEC Corporation, Japan": "ing intra-personal\nfacial changes is relatively unexplored",
          "2University of Tsukuba, Japan": "sified into a facial expression class [37], or BMI is estimated"
        },
        {
          "1NEC Corporation, Japan": "due to the difficulty of collecting temporally changing face",
          "2University of Tsukuba, Japan": "from a face image [53]. However,\nthese methods do not"
        },
        {
          "1NEC Corporation, Japan": "images.\nIn this paper, we propose a facial\nrepresenta-",
          "2University of Tsukuba, Japan": "focus on capturing subtle intra-personal changes in health"
        },
        {
          "1NEC Corporation, Japan": "tion learning method using synthetic images for comparing",
          "2University of Tsukuba, Japan": "and emotions.\nSince it\nis useful\nto understand the subtle"
        },
        {
          "1NEC Corporation, Japan": "faces, called ComFace, which is designed to capture intra-",
          "2University of Tsukuba, Japan": "changes, capturing temporal changes in the face within an"
        },
        {
          "1NEC Corporation, Japan": "personal facial changes. For effective representation learn-",
          "2University of Tsukuba, Japan": "individual is important. One of the reasons estimating intra-"
        },
        {
          "1NEC Corporation, Japan": "ing, ComFace aims to acquire two feature representations,",
          "2University of Tsukuba, Japan": "personal changes from face images has remained relatively"
        },
        {
          "1NEC Corporation, Japan": "i.e., inter-personal facial differences and intra-personal fa-",
          "2University of Tsukuba, Japan": "unexplored is that it is difficult to collect a large number of"
        },
        {
          "1NEC Corporation, Japan": "cial changes.\nThe key point of our method is\nthe use of",
          "2University of Tsukuba, Japan": "temporally changing face images."
        },
        {
          "1NEC Corporation, Japan": "synthetic face images\nto overcome the limitations of col-",
          "2University of Tsukuba, Japan": "Many existing methods for face analysis have used deep"
        },
        {
          "1NEC Corporation, Japan": "lecting real\nintra-personal\nface images.\nFacial represen-",
          "2University of Tsukuba, Japan": "neural networks (DNNs) with supervised learning. To over-"
        },
        {
          "1NEC Corporation, Japan": "tations learned by ComFace are transferred to three exten-",
          "2University of Tsukuba, Japan": "come the limitations of training data annotated with super-"
        },
        {
          "1NEC Corporation, Japan": "sive downstream tasks for comparing faces: estimating fa-",
          "2University of Tsukuba, Japan": "vised labels,\nthe mainstream approach utilizes DNNs pre-"
        },
        {
          "1NEC Corporation, Japan": "cial expression changes, weight changes, and age changes",
          "2University of Tsukuba, Japan": "trained on large-scale data (e.g.,\nImageNet\n[13] and VG-"
        },
        {
          "1NEC Corporation, Japan": "from two face images of\nthe same individual. Our Com-",
          "2University of Tsukuba, Japan": "GFace2 [7]) and then performs transfer learning with anno-"
        },
        {
          "1NEC Corporation, Japan": "Face,\ntrained using only synthetic data, achieves compa-",
          "2University of Tsukuba, Japan": "tated face data. Also, facial representation learning (FRL)"
        },
        {
          "1NEC Corporation, Japan": "rable to or better transfer performance than general pre-",
          "2University of Tsukuba, Japan": "has been recently attracting attention [6, 40, 64, 70], where a"
        },
        {
          "1NEC Corporation, Japan": "training and state-of-the-art representation learning meth-",
          "2University of Tsukuba, Japan": "large amount of unlabeled face images [6, 40, 64] and face"
        },
        {
          "1NEC Corporation, Japan": "ods trained using real images.",
          "2University of Tsukuba, Japan": "image-text pairs\n[70] are used to obtain prior knowledge"
        },
        {
          "1NEC Corporation, Japan": "",
          "2University of Tsukuba, Japan": "about facial representation and improve the performance of"
        },
        {
          "1NEC Corporation, Japan": "",
          "2University of Tsukuba, Japan": "downstream face tasks. Nevertheless, existing FRL meth-"
        },
        {
          "1NEC Corporation, Japan": "",
          "2University of Tsukuba, Japan": "ods mainly learn representations of\nfacial differences be-"
        },
        {
          "1NEC Corporation, Japan": "1. Introduction",
          "2University of Tsukuba, Japan": ""
        },
        {
          "1NEC Corporation, Japan": "",
          "2University of Tsukuba, Japan": "tween individuals and neglect those of facial changes within"
        },
        {
          "1NEC Corporation, Japan": "Human faces contain a variety of\ninformation,\ninclud-",
          "2University of Tsukuba, Japan": "an individual.\nFurthermore, visual representation learning"
        },
        {
          "1NEC Corporation, Japan": "ing\nidentities,\nhealth\nconditions,\nand\nemotions.\nFace",
          "2University of Tsukuba, Japan": "using synthetic images has recently emerged in the com-"
        },
        {
          "1NEC Corporation, Japan": "recognition has long been studied for personal\nidentifica-",
          "2University of Tsukuba, Japan": "puter vision domain [55,56], surpassing the performance of"
        },
        {
          "1NEC Corporation, Japan": "tion [60, 69].\nBesides\nface recognition, biological\ninfor-",
          "2University of Tsukuba, Japan": "representations learned using real images. Even in the face"
        },
        {
          "1NEC Corporation, Japan": "mation such as\nage\n[15, 67],\nfacial\nexpression [21, 37],",
          "2University of Tsukuba, Japan": "domain, representation learning using synthetic images for"
        },
        {
          "1NEC Corporation, Japan": "body weight\n[2, 12], and body mass index (BMI)\n[12, 53]",
          "2University of Tsukuba, Japan": "temporally changing faces, which are very scarce in real im-"
        },
        {
          "1NEC Corporation, Japan": "have also been estimated from face images.\nEstimation",
          "2University of Tsukuba, Japan": "ages, has the potential\nto boost\nthe estimation performance"
        },
        {
          "1NEC Corporation, Japan": "of biological\ninformation from faces has great potential\nto",
          "2University of Tsukuba, Japan": "of intra-personal facial changes."
        },
        {
          "1NEC Corporation, Japan": "be\napplied to medical, healthcare,\nand emotion recogni-",
          "2University of Tsukuba, Japan": "In this paper, we propose an FRL method using syn-"
        },
        {
          "1NEC Corporation, Japan": "tion fields.\nIn these fields,\nit\nis important\nto capture daily",
          "2University of Tsukuba, Japan": "thetic data for comparing faces,\ncalled ComFace, which"
        },
        {
          "1NEC Corporation, Japan": "intra-personal changes associated with health and emotional",
          "2University of Tsukuba, Japan": "is designed to capture intra-personal\nfacial changes.\nFig-"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "cial changes. This makes it\nfeasible to acquire facial\nrep-": "resentations relating not only to facial differences between",
          "(i.e., age change) periods.": "The main contributions of this paper are summarized as"
        },
        {
          "cial changes. This makes it\nfeasible to acquire facial\nrep-": "individuals, but also to subtle facial changes within an in-",
          "(i.e., age change) periods.": "follows:"
        },
        {
          "cial changes. This makes it\nfeasible to acquire facial\nrep-": "dividual.\nTo address the limitation in the number of\nreal",
          "(i.e., age change) periods.": ""
        },
        {
          "cial changes. This makes it\nfeasible to acquire facial\nrep-": "",
          "(i.e., age change) periods.": "• We propose ComFace,\nthe first FRL method using"
        },
        {
          "cial changes. This makes it\nfeasible to acquire facial\nrep-": "intra-personal face images, ComFace utilizes synthetic face",
          "(i.e., age change) periods.": ""
        },
        {
          "cial changes. This makes it\nfeasible to acquire facial\nrep-": "",
          "(i.e., age change) periods.": "synthetic face images for comparing faces. ComFace"
        },
        {
          "cial changes. This makes it\nfeasible to acquire facial\nrep-": "images generated by StyleGANs [27–30]. We generate syn-",
          "(i.e., age change) periods.": ""
        },
        {
          "cial changes. This makes it\nfeasible to acquire facial\nrep-": "",
          "(i.e., age change) periods.": "learns feature representations regarding intra-personal"
        },
        {
          "cial changes. This makes it\nfeasible to acquire facial\nrep-": "thetic face images\nfor a large number of\nindividuals and",
          "(i.e., age change) periods.": ""
        },
        {
          "cial changes. This makes it\nfeasible to acquire facial\nrep-": "",
          "(i.e., age change) periods.": "facial changes as well as inter-personal\nfacial differ-"
        },
        {
          "cial changes. This makes it\nfeasible to acquire facial\nrep-": "then generate intra-personal face images that vary accord-",
          "(i.e., age change) periods.": ""
        },
        {
          "cial changes. This makes it\nfeasible to acquire facial\nrep-": "",
          "(i.e., age change) periods.": "ences."
        },
        {
          "cial changes. This makes it\nfeasible to acquire facial\nrep-": "ing to many facial attributes. Benefiting from synthetic data,",
          "(i.e., age change) periods.": ""
        },
        {
          "cial changes. This makes it\nfeasible to acquire facial\nrep-": "",
          "(i.e., age change) periods.": "• Facial representations learned by ComFace are trans-"
        },
        {
          "cial changes. This makes it\nfeasible to acquire facial\nrep-": "ComFace can use an unlimited number of individuals and",
          "(i.e., age change) periods.": ""
        },
        {
          "cial changes. This makes it\nfeasible to acquire facial\nrep-": "",
          "(i.e., age change) periods.": "ferred to three extensive downstream tasks\nfor com-"
        },
        {
          "cial changes. This makes it\nfeasible to acquire facial\nrep-": "intra-personal face images, overcoming the problem of ex-",
          "(i.e., age change) periods.": ""
        },
        {
          "cial changes. This makes it\nfeasible to acquire facial\nrep-": "",
          "(i.e., age change) periods.": "paring faces. ComFace achieves comparable to or su-"
        },
        {
          "cial changes. This makes it\nfeasible to acquire facial\nrep-": "isting real image datasets with a small number of temporally",
          "(i.e., age change) periods.": ""
        },
        {
          "cial changes. This makes it\nfeasible to acquire facial\nrep-": "",
          "(i.e., age change) periods.": "perior transfer performance to general pre-training and"
        },
        {
          "cial changes. This makes it\nfeasible to acquire facial\nrep-": "changing face images.",
          "(i.e., age change) periods.": ""
        },
        {
          "cial changes. This makes it\nfeasible to acquire facial\nrep-": "",
          "(i.e., age change) periods.": "state-of-the-art\n(SoTA)\nrepresentation learning meth-"
        },
        {
          "cial changes. This makes it\nfeasible to acquire facial\nrep-": "The first FRL in ComFace is self-supervised contrastive",
          "(i.e., age change) periods.": ""
        },
        {
          "cial changes. This makes it\nfeasible to acquire facial\nrep-": "",
          "(i.e., age change) periods.": "ods trained using real images."
        },
        {
          "cial changes. This makes it\nfeasible to acquire facial\nrep-": "learning [9] to identify facial differences between individu-",
          "(i.e., age change) periods.": ""
        },
        {
          "cial changes. This makes it\nfeasible to acquire facial\nrep-": "",
          "(i.e., age change) periods.": "• Our approach of comparing two face images within"
        },
        {
          "cial changes. This makes it\nfeasible to acquire facial\nrep-": "als. With self-supervised learning, ComFace learns feature",
          "(i.e., age change) periods.": ""
        },
        {
          "cial changes. This makes it\nfeasible to acquire facial\nrep-": "",
          "(i.e., age change) periods.": "an\nindividual\ngeneralizes well\nto\nnew patients\nand"
        },
        {
          "cial changes. This makes it\nfeasible to acquire facial\nrep-": "representations of inter-personal facial differences without",
          "(i.e., age change) periods.": ""
        },
        {
          "cial changes. This makes it\nfeasible to acquire facial\nrep-": "",
          "(i.e., age change) periods.": "environmental conditions not used for\ntraining data."
        },
        {
          "cial changes. This makes it\nfeasible to acquire facial\nrep-": "supervised labels.\nIn the\nsecond FRL, ComFace\nlearns",
          "(i.e., age change) periods.": ""
        },
        {
          "cial changes. This makes it\nfeasible to acquire facial\nrep-": "",
          "(i.e., age change) periods.": "Our weight\nchange\nestimation model,\ntrained with-"
        },
        {
          "cial changes. This makes it\nfeasible to acquire facial\nrep-": "feature\nrepresentations\nof\nintra-personal\nfacial\nchanges.",
          "(i.e., age change) periods.": ""
        },
        {
          "cial changes. This makes it\nfeasible to acquire facial\nrep-": "",
          "(i.e., age change) periods.": "out\npatient-specific\ndata,\noutperforms\nthe\nprevious"
        },
        {
          "cial changes. This makes it\nfeasible to acquire facial\nrep-": "Specifically, how much the intra-personal face has changed",
          "(i.e., age change) periods.": ""
        },
        {
          "cial changes. This makes it\nfeasible to acquire facial\nrep-": "",
          "(i.e., age change) periods.": "method [2]\nthat estimates weight\nfrom a single face"
        },
        {
          "cial changes. This makes it\nfeasible to acquire facial\nrep-": "is learned from two face images of the same individual. Fur-",
          "(i.e., age change) periods.": ""
        },
        {
          "cial changes. This makes it\nfeasible to acquire facial\nrep-": "",
          "(i.e., age change) periods.": "image, trained with patient-specific data."
        },
        {
          "cial changes. This makes it\nfeasible to acquire facial\nrep-": "thermore, curriculum learning [5] is introduced to gradually",
          "(i.e., age change) periods.": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Age": "+3 years"
        },
        {
          "Age": ": Intensity of"
        },
        {
          "Age": "Intra-personal changes in"
        },
        {
          "Age": "manipulation"
        },
        {
          "Age": "health and emotions"
        },
        {
          "Age": ""
        },
        {
          "Age": ""
        },
        {
          "Age": ""
        },
        {
          "Age": "recent study [2]. These downstream tasks consist of various"
        },
        {
          "Age": "periods of\ntemporal changes in the face:\nshort\n(i.e.,\nfacial"
        },
        {
          "Age": "expression change), medium (i.e., weight change), and long"
        },
        {
          "Age": "(i.e., age change) periods."
        },
        {
          "Age": "The main contributions of this paper are summarized as"
        },
        {
          "Age": "follows:"
        },
        {
          "Age": ""
        },
        {
          "Age": "• We propose ComFace,\nthe first FRL method using"
        },
        {
          "Age": ""
        },
        {
          "Age": "synthetic face images for comparing faces. ComFace"
        },
        {
          "Age": ""
        },
        {
          "Age": "learns feature representations regarding intra-personal"
        },
        {
          "Age": ""
        },
        {
          "Age": "facial changes as well as inter-personal\nfacial differ-"
        },
        {
          "Age": ""
        },
        {
          "Age": "ences."
        },
        {
          "Age": ""
        },
        {
          "Age": "• Facial representations learned by ComFace are trans-"
        },
        {
          "Age": ""
        },
        {
          "Age": "ferred to three extensive downstream tasks\nfor com-"
        },
        {
          "Age": ""
        },
        {
          "Age": "paring faces. ComFace achieves comparable to or su-"
        },
        {
          "Age": ""
        },
        {
          "Age": "perior transfer performance to general pre-training and"
        },
        {
          "Age": ""
        },
        {
          "Age": "state-of-the-art\n(SoTA)\nrepresentation learning meth-"
        },
        {
          "Age": ""
        },
        {
          "Age": "ods trained using real images."
        },
        {
          "Age": ""
        },
        {
          "Age": "• Our approach of comparing two face images within"
        },
        {
          "Age": ""
        },
        {
          "Age": "an\nindividual\ngeneralizes well\nto\nnew patients\nand"
        },
        {
          "Age": ""
        },
        {
          "Age": "environmental conditions not used for\ntraining data."
        },
        {
          "Age": ""
        },
        {
          "Age": "Our weight\nchange\nestimation model,\ntrained with-"
        },
        {
          "Age": ""
        },
        {
          "Age": "out\npatient-specific\ndata,\noutperforms\nthe\nprevious"
        },
        {
          "Age": ""
        },
        {
          "Age": "method [2]\nthat estimates weight\nfrom a single face"
        },
        {
          "Age": ""
        },
        {
          "Age": "image, trained with patient-specific data."
        },
        {
          "Age": ""
        },
        {
          "Age": ""
        },
        {
          "Age": "2. Related Work"
        },
        {
          "Age": ""
        },
        {
          "Age": "Face Analysis: Various biological information has been"
        },
        {
          "Age": "estimated from a single face image [2, 43, 53, 67].\nAka-"
        },
        {
          "Age": "matsu et al. [2] focused on edema, a symptom of kidney dis-"
        },
        {
          "Age": "ease, and estimated weight from the degree of facial edema"
        },
        {
          "Age": "in dialysis patients. Weight\nreflects the fluid volume in a"
        },
        {
          "Age": "dialysis patient’s body,\nso it\nis helpful\nto be able to eas-"
        },
        {
          "Age": "ily monitor daily weight from a face image. The weakness"
        },
        {
          "Age": "of\ntheir method is that\nit\nrequires training data from a pa-"
        },
        {
          "Age": "tient who uses the system in order\nto construct a patient-"
        },
        {
          "Age": "specific model.\nThis is because the method [2] estimates"
        },
        {
          "Age": "weight\nfrom a single face image and does not generalize"
        },
        {
          "Age": "well\nto new patients not used for\ntraining data.\nIn con-"
        },
        {
          "Age": "trast, our approach of comparing two intra-personal\nface"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "images generalizes well\nto new patients and does not\nre-": "quire patient-specific data. Our motivation to capture daily",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": "images instead of real images, we rather leverage synthetic"
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "intra-personal facial changes is similar to Ref. [34], which",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": "images since we have very few real\nimages for temporally"
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "estimates changes in the degree of smiling from two face",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": "changing faces. We make the most of the advantages of syn-"
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "images. While the method [34] performs transfer learning",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": "thetic images over real\nimages, resulting in our FRL using"
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "using a pre-trained model\nfor\nface recognition, we use a",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": "synthetic images achieving better transfer performance than"
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "pre-trained model specialized to capture intra-personal\nfa-",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": "other methods using real images."
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "cial changes on the basis of synthetic images and achieve",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": ""
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": "3. Method"
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "better transfer performance.",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": ""
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": "3.1. Synthetic Face Images"
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "Facial Representation Learning:\nTransfer\nlearning\nin",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": ""
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": "Our method\nutilizes\nsynthetic\nface\nimages\ngenerated"
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "face analysis tasks commonly depends on pre-training us-",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": ""
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": "by StyleGANs [27–30] for representation learning. Style-"
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "ing\nImageNet\n[2, 15, 38, 45]\nand\nlarge\nface\nrecognition",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": ""
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": "GAN is one of the most popular generative models for face"
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "datasets [33, 47, 51, 53, 54]. Recently, facial representation",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": ""
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": "synthesis, demonstrating impressive performance in image"
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "learning (FRL) has been studied as a pre-training method",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": ""
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": "generation,\ninversion,\nand manipulation [4].\nStyleGAN"
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "for face analysis tasks [6,40,70]. Bulat et al. [6] investigated",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": ""
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": "generators synthesize exceedingly realistic images and en-"
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "pre-training strategies and datasets for several\nface analy-",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": ""
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": "able editing [1, 3, 11, 39, 48, 52].\nInterFaceGAN [52]\nis"
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "sis tasks. Zheng et al. [70] proposed a weakly-supervised",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": ""
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": "a major\nframework for\nface editing.\nIt semantically edits"
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "method called FaRL using face image-text pairs.\nLiu et",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": ""
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": "faces by interpreting latent semantics learned by StyleGAN."
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "al.\n[40] proposed a pose-disentangled contrastive learning",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": ""
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": "Specifically,\nInterFaceGAN can easily edit\nlatent code w"
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "(PCL) method for general self-supervised facial\nrepresen-",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": ""
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": "in high dimensional space W to manipulate the attributes"
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "tation. Different\nfrom the FRL methods described above,",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": ""
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": "(e.g., weight, age, and gender) of a synthesized image as"
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "our ComFace focuses on FRL regarding intra-personal fa-",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": ""
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": "wedit = w + α · nm, where wedit is the edited latent code,"
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "cial changes. Inter- and intra-personal influences on human",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": ""
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": "nm is a normal vector to manipulate for attribute m, and α"
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "interactions have been studied previously [31, 63].\nThese",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": ""
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": "is a parameter\nthat controls the intensity of\nthe manipula-"
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "methods simply handle inter- and intra-personal multimodal",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": ""
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": "tion.\nIt will make the synthesis look more positive for each"
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "signals (e.g., video [31], audio [31,63], facial gestures [63])",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": ""
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": "attribute when α > 0 (e.g., get fat when the attribute m is"
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "to consider human-human interactions for specific applica-",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": ""
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": "weight), and α < 0 will make the synthesis look more neg-"
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "tions such as modeling affect dynamics and adapted behav-",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": ""
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": "ative (e.g., slim down when the attribute m is weight). As"
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "ior synthesis. Our work differs from previous work [31, 63]",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": ""
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": "attributes to manipulate synthetic faces, we use weight [49],"
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "in that our work is a comprehensive representation learning",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": ""
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": "age [52], smile [52], and the 40 attributes included in the"
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "study for downstream tasks that capture intra-personal\nfa-",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": ""
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": "CelebA dataset [41]. Examples of synthetic images manip-"
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "cial changes.",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": ""
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": "ulated when the attribute m is weight are shown on the left"
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "Synthetic Data:\nSynthetic data\nfor human analysis has",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": ""
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": "side of Fig. 1. We synthesize a new person’s face by ran-"
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "been widely explored [26], e.g.,\nface recognition [35, 36,",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": ""
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": "domly calculating the latent code w, and generate images"
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "50, 58, 66], crowd counting [61, 62], and fingerprint recog-",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": ""
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": "of intra-personal face changes by setting multiple α."
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "nition [18, 25].\nIn particular, with the recent\nsuccess of",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": ""
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "generative adversarial networks\n(GANs)\n[20],\nthe quality",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": ""
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": "3.2. FRL with Synthetic Face Images"
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "of\nface\nsynthesis has\nimproved rapidly.\nQiu et al.\n[50]",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": ""
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": "Our ComFace\nlearns\ntwo feature\nrepresentations,\ni.e.,"
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "proposed a face recognition method called SynFace using",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": ""
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": "inter-personal\nfacial differences\nand intra-personal\nfacial"
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "synthetic face images generated by DiscoFaceGAN [14].",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": ""
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": "changes.\nSince\nthe\ninter-personal\nrepresents broad dif-"
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "They explored the performance gap between face recogni-",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": ""
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": "ferences\nin faces and the intra-personal\nrepresents\nsubtle"
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "tion models trained with synthetic and real face images and",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": ""
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": "changes in faces, both of\nthese two facial\nrepresentations"
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "then designed SynFace to suppress the domain gap between",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": ""
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": "are essential to downstream tasks for comparing faces. Fig-"
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "synthetic and real face images. Most recently,\nrepresenta-",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": ""
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": "ure 2 shows the training scheme of ComFace. The goal of"
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "tion learning using synthetic data has emerged [16, 55, 56].",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": ""
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": "the training is representation learning of the backbone f (·)"
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "Ref. [16] is the first study to consider introducing synthetic",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": ""
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": "using synthetic face images. As the backbone f (·), arbi-"
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "images\ninto FRL and compares\nthe transfer performance",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": ""
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": "trary DNNs such as ResNet\n[23] and Vision Transformer"
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "with that of\nreal\nimages.\nRef.\n[16] aims to learn general",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": ""
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": "is\n(ViT) [17] are used. During the training, suppose that xi"
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "facial representation and differs from our FRL that focuses",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": ""
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": "the i-th synthetic image in the mini-batch (i = {1, · · ·\n, N },"
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "on intra-personal\nfacial changes. Although synthetic data",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": ""
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": "N is the mini-batch size) and the parameter α correspond-"
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "have been introduced into the face domain,\ntheir perfor-",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": ""
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": "is\ning to xi\n(see Section 3.1). Also, yi\nis denoted as αxi"
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "mance in Refs. [16, 50] is still\nlower than when using real",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": ""
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": "a synthetic image edited from the face of the same person"
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "images. Our motivation differs from that of Refs. [16, 50],",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": ""
        },
        {
          "images generalizes well\nto new patients and does not\nre-": "",
          "i.e., while Refs. [16, 50] explores the potential of synthetic": "is denoted\nin xi, and the parameter α corresponding to yi"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Mini-batch": "",
          "MSE loss": "(GT)"
        },
        {
          "Mini-batch": "",
          "MSE loss": "Intra-personal learning"
        },
        {
          "Mini-batch": "Inter-personal",
          "MSE loss": ""
        },
        {
          "Mini-batch": "facial differences",
          "MSE loss": ""
        },
        {
          "Mini-batch": "Figure 2. Training scheme of ComFace. Learning strategy consists of",
          "MSE loss": "two components,\ni.e.,\ninter-personal\nlearning and intra-personal"
        },
        {
          "Mini-batch": "learning. Inter-personal learning acquires feature representations of facial differences between individuals. Intra-personal learning acquires",
          "MSE loss": ""
        },
        {
          "Mini-batch": "feature representations of facial changes within individuals.",
          "MSE loss": ""
        },
        {
          "Mini-batch": "as αyi. Since xi and yi are synthetic images of the same",
          "MSE loss": "perature parameter. With the above optimization,\nfeature"
        },
        {
          "Mini-batch": "person whose face has been edited,\nthe distance between",
          "MSE loss": "vectors of the same face are brought closer and those of dif-"
        },
        {
          "Mini-batch": "intra-personal face changes is represented by |αyi − αxi|.",
          "MSE loss": "ferent faces are kept apart."
        },
        {
          "Mini-batch": "FRL in ComFace consists of\nthe following inter-personal",
          "MSE loss": "Intra-personal Learning: We acquire feature representa-"
        },
        {
          "Mini-batch": "learning and intra-personal learning.",
          "MSE loss": "tions of\nintra-personal\nfacial changes.\nThe synthetic im-"
        },
        {
          "Mini-batch": "Inter-personal Learning: We use contrastive learning to",
          "MSE loss": ""
        },
        {
          "Mini-batch": "",
          "MSE loss": "age yi\nis converted into the image ˜yi by data augmentation."
        },
        {
          "Mini-batch": "acquire\nfeature\nrepresentations\nof\nfacial\ndifferences\nbe-",
          "MSE loss": ""
        },
        {
          "Mini-batch": "",
          "MSE loss": "is input\nThen, ˜yi\nto f (·), which outputs the vector h ˜yi ="
        },
        {
          "Mini-batch": "tween individuals.\nContrastive learning is a kind of self-",
          "MSE loss": ""
        },
        {
          "Mini-batch": "",
          "MSE loss": "f ( ˜yi). We next calculate the difference vector h ˜yi −h ˜xi be-"
        },
        {
          "Mini-batch": "supervised learning that performs\nrepresentation learning",
          "MSE loss": ""
        },
        {
          "Mini-batch": "",
          "MSE loss": "to capture intra-personal facial changes.\ntween h ˜xi and h ˜yi"
        },
        {
          "Mini-batch": "without supervised labels [9, 22], attracting positive pairs",
          "MSE loss": "Furthermore,\nthe difference vector is input\nto ReLU and a"
        },
        {
          "Mini-batch": "(e.g.,\nthe same images with different data augmentation)",
          "MSE loss": "linear layer l(·), which outputs the distance of intra-personal"
        },
        {
          "Mini-batch": "and pulling negative pairs\n(e.g.,\ndifferent\nimages)\naway.",
          "MSE loss": ""
        },
        {
          "Mini-batch": "",
          "MSE loss": "ComFace learns how much the intra-\nfacial changes di."
        },
        {
          "Mini-batch": "ComFace\nperforms\ncontrastive\nlearning\nbased\non\nSim-",
          "MSE loss": "personal face has changed on the basis of the mean squared"
        },
        {
          "Mini-batch": "",
          "MSE loss": "(cid:80)N"
        },
        {
          "Mini-batch": "CLR [9]. First, the synthetic image xi is converted into two",
          "MSE loss": "error (MSE) loss: Lintra = 1"
        },
        {
          "Mini-batch": "",
          "MSE loss": "i=1(di − |αyi − αxi|)2,\nN"
        },
        {
          "Mini-batch": "images\nxi and\nxi+N by data augmentation consisting of",
          "MSE loss": ""
        },
        {
          "Mini-batch": "",
          "MSE loss": "where |αyi − αxi| is the ground truth (GT) for the distance"
        },
        {
          "Mini-batch": "horizontal flip, color\njitter, grayscale conversion, and ran-",
          "MSE loss": "of intra-personal facial changes. Since the direction of fa-"
        },
        {
          "Mini-batch": "dom crop. Since ˜xi and ˜xi+N are derived from the same",
          "MSE loss": ""
        },
        {
          "Mini-batch": "",
          "MSE loss": "cial change depends on the attribute, we use |αyi − αxi|"
        },
        {
          "Mini-batch": "image,\nis\nthey are referred to as a positive pair. Then, ˜xi",
          "MSE loss": ""
        },
        {
          "Mini-batch": "",
          "MSE loss": "instead of αyi − αxi as the intra-personal GT."
        },
        {
          "Mini-batch": "input\nto f (·), which outputs the vector h ˜xi = f ( ˜xi). A",
          "MSE loss": ""
        },
        {
          "Mini-batch": "",
          "MSE loss": "As described above, ComFace acquires two feature rep-"
        },
        {
          "Mini-batch": "projection head g(·) consisting of a two-layer multi-layer",
          "MSE loss": ""
        },
        {
          "Mini-batch": "",
          "MSE loss": "resentations,\ninter-personal\nfacial\ndifferences\nand\nintra-"
        },
        {
          "Mini-batch": "perceptron (MLP) follows after f (·), and we input h ˜xi and",
          "MSE loss": "personal facial changes, using the sum of those loss func-"
        },
        {
          "Mini-batch": "output vector z ˜xi = g(h ˜xi). A mini-batch consists of N",
          "MSE loss": "tions: L = Linter + Lintra."
        },
        {
          "Mini-batch": "samples, and data augmentation creates pairs, resulting in a",
          "MSE loss": ""
        },
        {
          "Mini-batch": "total of 2N samples.\nIn contrastive learning, one positive",
          "MSE loss": "3.3. Curriculum Learning of Intra-personal Facial"
        },
        {
          "Mini-batch": "pair [z ˜xi, z ˜xi+N ] and the other 2(N − 1) negative pairs are",
          "MSE loss": "Changes"
        },
        {
          "Mini-batch": "consisted, and the positive pairs are attracted and the neg-",
          "MSE loss": ""
        },
        {
          "Mini-batch": "",
          "MSE loss": "In intra-personal learning, the difficulty level of learning"
        },
        {
          "Mini-batch": "ative ones are pulled away from each other.\nSpecifically,",
          "MSE loss": ""
        },
        {
          "Mini-batch": "",
          "MSE loss": "tasks depends on α.\nSpecifically, when α = 0.0 and 5.0,"
        },
        {
          "Mini-batch": "we perform contrastive learning based on the following In-",
          "MSE loss": ""
        },
        {
          "Mini-batch": "",
          "MSE loss": "there are significant differences in the facial changes, so it"
        },
        {
          "Mini-batch": "foNCE loss [59]:",
          "MSE loss": ""
        },
        {
          "Mini-batch": "",
          "MSE loss": "is easy to identify them (see the left side of Fig. 1). On the"
        },
        {
          "Mini-batch": "2N\n· zp\n/τ )\nexp(z ˜xj",
          "MSE loss": ""
        },
        {
          "Mini-batch": "1",
          "MSE loss": ""
        },
        {
          "Mini-batch": "xj",
          "MSE loss": "other hand, when α = 0.0 and 2.0,\nthe facial changes are"
        },
        {
          "Mini-batch": "(cid:88) j\n,\nlog\nLinter = −",
          "MSE loss": ""
        },
        {
          "Mini-batch": "(cid:80)2N\n2N",
          "MSE loss": ""
        },
        {
          "Mini-batch": "",
          "MSE loss": "small, so it\nis difficult\nto identify them (see the left side of"
        },
        {
          "Mini-batch": "· z ˜xk /τ )\nk=1 1lk̸=j exp(z ˜xj\n=1",
          "MSE loss": ""
        },
        {
          "Mini-batch": "",
          "MSE loss": "Fig. 1). ComFace provides effective representation learning"
        },
        {
          "Mini-batch": "(1)",
          "MSE loss": ""
        },
        {
          "Mini-batch": "",
          "MSE loss": "by increasing the difficulty level of learning tasks on the ba-"
        },
        {
          "Mini-batch": "where zp\nis a vector\nthat\nforms a positive pair with z ˜xj",
          "MSE loss": "sis of curriculum learning. Curriculum learning [5] mimics"
        },
        {
          "Mini-batch": "xj",
          "MSE loss": ""
        },
        {
          "Mini-batch": "(i.e., z ˜xi and z ˜xi+N ), 1lk̸=j ∈ {0, 1} is a function that re-",
          "MSE loss": "the human learning behavior of starting with simple tasks"
        },
        {
          "Mini-batch": "turns 1 when k ̸= j and 0 when k = j, and τ\nis a tem-",
          "MSE loss": "and gradually learning more complex concepts. ComFace"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "gradually decreases the distance between intra-personal fa-": "cial changes |αyi − αxi | during training to acquire more",
          "StyleGANs. For each identity, synthetic images for each at-": ", 0.0, · · ·\ntribute with α = {−5.0, −4.9, · · ·\n, 4.9, 5.0} are"
        },
        {
          "gradually decreases the distance between intra-personal fa-": "effective feature representations. Specifically, suppose that",
          "StyleGANs. For each identity, synthetic images for each at-": "generated (see Section 3.1). As a result,\nthe total number"
        },
        {
          "gradually decreases the distance between intra-personal fa-": "S is the range of the distance of facial changes |αyi − αxi |",
          "StyleGANs. For each identity, synthetic images for each at-": "of synthetic face images used for FRL is 35 million (M)."
        },
        {
          "gradually decreases the distance between intra-personal fa-": "when sampling the pairs of xi and yi used for training, and",
          "StyleGANs. For each identity, synthetic images for each at-": "For details on the composition of synthetic images, please"
        },
        {
          "gradually decreases the distance between intra-personal fa-": "we gradually decrease S according to the number of epochs",
          "StyleGANs. For each identity, synthetic images for each at-": "refer to Supplement A.1. See Supplement C.1 for a quality"
        },
        {
          "gradually decreases the distance between intra-personal fa-": "as follows:",
          "StyleGANs. For each identity, synthetic images for each at-": "assessment of the synthetic images and an evaluation of the"
        },
        {
          "gradually decreases the distance between intra-personal fa-": "(cid:40)",
          "StyleGANs. For each identity, synthetic images for each at-": "transfer performance with respect to the quality."
        },
        {
          "gradually decreases the distance between intra-personal fa-": "1 (e ≤ e1)\nSmax",
          "StyleGANs. For each identity, synthetic images for each at-": ""
        },
        {
          "gradually decreases the distance between intra-personal fa-": "S =\n,\n(2)\nSe =",
          "StyleGANs. For each identity, synthetic images for each at-": ""
        },
        {
          "gradually decreases the distance between intra-personal fa-": "",
          "StyleGANs. For each identity, synthetic images for each at-": "Details for FRL: For FRL with synthetic images, we use"
        },
        {
          "gradually decreases the distance between intra-personal fa-": "Se\nt (et−1 < e ≤ et),",
          "StyleGANs. For each identity, synthetic images for each at-": ""
        },
        {
          "gradually decreases the distance between intra-personal fa-": "",
          "StyleGANs. For each identity, synthetic images for each at-": "90% of all identities as training data and 10% as validation"
        },
        {
          "gradually decreases the distance between intra-personal fa-": "the distance of\nfa-\nwhere Smax is the maximum range of",
          "StyleGANs. For each identity, synthetic images for each at-": "data. As a backbone f (·), we use ResNet50 [23], which is"
        },
        {
          "gradually decreases the distance between intra-personal fa-": "cial changes in the dataset, and e is the number of epochs.",
          "StyleGANs. For each identity, synthetic images for each at-": "commonly used for general pre-training [7, 13] and repre-"
        },
        {
          "gradually decreases the distance between intra-personal fa-": "In Eq.\n(2), we perform curriculum learning by increasing",
          "StyleGANs. For each identity, synthetic images for each at-": "sentation learning [6, 9, 22]. Since ResNet50 is better than"
        },
        {
          "gradually decreases the distance between intra-personal fa-": "the difficulty level of learning tasks with the progress of the",
          "StyleGANs. For each identity, synthetic images for each at-": "ViT [17] for ComFace (see Supplement C.2 for the evalu-"
        },
        {
          "gradually decreases the distance between intra-personal fa-": "training (i.e., with the increase in the number of epochs).",
          "StyleGANs. For each identity, synthetic images for each at-": "ation), we employ ResNet50. Synthetic images are resized"
        },
        {
          "gradually decreases the distance between intra-personal fa-": "",
          "StyleGANs. For each identity, synthetic images for each at-": "to 224×224 and the temperature parameter τ is set\nto 0.1."
        },
        {
          "gradually decreases the distance between intra-personal fa-": "3.4. Transfer Learning toward Downstream Tasks",
          "StyleGANs. For each identity, synthetic images for each at-": ""
        },
        {
          "gradually decreases the distance between intra-personal fa-": "",
          "StyleGANs. For each identity, synthetic images for each at-": "Our model is trained from scratch with randomly initialized"
        },
        {
          "gradually decreases the distance between intra-personal fa-": "We transfer DNNs pre-trained by ComFace to down-",
          "StyleGANs. For each identity, synthetic images for each at-": "weights. We run the training for 12 epochs with batch size"
        },
        {
          "gradually decreases the distance between intra-personal fa-": "stream tasks for comparing faces. For downstream tasks, we",
          "StyleGANs. For each identity, synthetic images for each at-": "1024 on 32 NVIDIA A100 GPUs (∼22 hours training). As"
        },
        {
          "gradually decreases the distance between intra-personal fa-": "utilize DNNs in intra-personal\nlearning (see the red box in",
          "StyleGANs. For each identity, synthetic images for each at-": "t ∈\nparameters for curriculum learning, we set Smax = 10,"
        },
        {
          "gradually decreases the distance between intra-personal fa-": "Fig. 2). Since the linear layer l(·) trained by intra-personal",
          "StyleGANs. For each identity, synthetic images for each at-": "{2, 3, 4},\ne1 = 3,\ne2 = 6,\ne3 = 9,\ne4 = 12. For each"
        },
        {
          "gradually decreases the distance between intra-personal fa-": "learning is useful\nfor comparing faces, we transfer both",
          "StyleGANs. For each identity, synthetic images for each at-": "epoch, we randomly sample xi and yi pairs according to the"
        },
        {
          "gradually decreases the distance between intra-personal fa-": "backbone f (·) and linear layer l(·). In transfer learning, we",
          "StyleGANs. For each identity, synthetic images for each at-": "range S in curriculum learning and construct mini-batches."
        },
        {
          "gradually decreases the distance between intra-personal fa-": "perform downstream tasks for comparing faces using two",
          "StyleGANs. For each identity, synthetic images for each at-": "Adam [32] optimizer is used, and the learning rate is initial-"
        },
        {
          "gradually decreases the distance between intra-personal fa-": "face images, xtask\nand ytask\n(i = {1, · · ·\n, N task}, N task",
          "StyleGANs. For each identity, synthetic images for each at-": "ized as 4e-4 and halved in 10 epochs. See Supplement B.1"
        },
        {
          "gradually decreases the distance between intra-personal fa-": "i\ni",
          "StyleGANs. For each identity, synthetic images for each at-": ""
        },
        {
          "gradually decreases the distance between intra-personal fa-": "",
          "StyleGANs. For each identity, synthetic images for each at-": "for other setups."
        },
        {
          "gradually decreases the distance between intra-personal fa-": "is the number of images), and corresponding labels αxtask\ni",
          "StyleGANs. For each identity, synthetic images for each at-": ""
        },
        {
          "gradually decreases the distance between intra-personal fa-": "(e.g., facial expression intensity and weight) to\nand αytask",
          "StyleGANs. For each identity, synthetic images for each at-": ""
        },
        {
          "gradually decreases the distance between intra-personal fa-": "i",
          "StyleGANs. For each identity, synthetic images for each at-": "4.2. Setup for Downstream Tasks"
        },
        {
          "gradually decreases the distance between intra-personal fa-": "estimate intra-personal facial changes. Let dtask\nbe the pre-\ni",
          "StyleGANs. For each identity, synthetic images for each at-": ""
        },
        {
          "gradually decreases the distance between intra-personal fa-": "",
          "StyleGANs. For each identity, synthetic images for each at-": "We use the following facial expression change, weight"
        },
        {
          "gradually decreases the distance between intra-personal fa-": "dicted facial change;\nthe DNN model\nis transferred on the",
          "StyleGANs. For each identity, synthetic images for each at-": ""
        },
        {
          "gradually decreases the distance between intra-personal fa-": "",
          "StyleGANs. For each identity, synthetic images for each at-": "change, and age change datasets in our downstream tasks"
        },
        {
          "gradually decreases the distance between intra-personal fa-": "1\n(cid:80)N task",
          "StyleGANs. For each identity, synthetic images for each at-": ""
        },
        {
          "gradually decreases the distance between intra-personal fa-": "(dtask\n−\nbasis of\nthe MSE loss: Ltask =",
          "StyleGANs. For each identity, synthetic images for each at-": ""
        },
        {
          "gradually decreases the distance between intra-personal fa-": "i\ni=1\nN task",
          "StyleGANs. For each identity, synthetic images for each at-": ""
        },
        {
          "gradually decreases the distance between intra-personal fa-": "",
          "StyleGANs. For each identity, synthetic images for each at-": "for comparing faces.\nThe three downstream tasks corre-"
        },
        {
          "gradually decreases the distance between intra-personal fa-": "))2. In each downstream task, we estimate\n(αytask\n− αxtask",
          "StyleGANs. For each identity, synthetic images for each at-": ""
        },
        {
          "gradually decreases the distance between intra-personal fa-": "i\ni",
          "StyleGANs. For each identity, synthetic images for each at-": "spond to short (∼1 day), medium (1 day ∼ 3 months), and"
        },
        {
          "gradually decreases the distance between intra-personal fa-": "not only the distance but also the direction of facial changes",
          "StyleGANs. For each identity, synthetic images for each at-": ""
        },
        {
          "gradually decreases the distance between intra-personal fa-": "",
          "StyleGANs. For each identity, synthetic images for each at-": "long (1 year∼) time periods of temporal changes in the face,"
        },
        {
          "gradually decreases the distance between intra-personal fa-": "(e.g.,\nincrease/decrease of weight).\nTherefore, unlike the",
          "StyleGANs. For each identity, synthetic images for each at-": ""
        },
        {
          "gradually decreases the distance between intra-personal fa-": "",
          "StyleGANs. For each identity, synthetic images for each at-": "respectively."
        },
        {
          "gradually decreases the distance between intra-personal fa-": "distance of the facial change in the intra-personal GT (see",
          "StyleGANs. For each identity, synthetic images for each at-": ""
        },
        {
          "gradually decreases the distance between intra-personal fa-": "",
          "StyleGANs. For each identity, synthetic images for each at-": "Facial Expression Change Dataset: We use the public"
        },
        {
          "gradually decreases the distance between intra-personal fa-": "as GT for downstream\nSection 3.2), we use αytask\n− αxtask",
          "StyleGANs. For each identity, synthetic images for each at-": ""
        },
        {
          "gradually decreases the distance between intra-personal fa-": "i\ni",
          "StyleGANs. For each identity, synthetic images for each at-": "dataset DISFA [42, 43], which contains\nfacial videos of"
        },
        {
          "gradually decreases the distance between intra-personal fa-": "tasks.",
          "StyleGANs. For each identity, synthetic images for each at-": ""
        },
        {
          "gradually decreases the distance between intra-personal fa-": "",
          "StyleGANs. For each identity, synthetic images for each at-": "27 subjects\n(12 women and 15 men) while they watch a"
        },
        {
          "gradually decreases the distance between intra-personal fa-": "",
          "StyleGANs. For each identity, synthetic images for each at-": "4-minute video intended to elicit a range of\nfacial expres-"
        },
        {
          "gradually decreases the distance between intra-personal fa-": "4. Experiments",
          "StyleGANs. For each identity, synthetic images for each at-": ""
        },
        {
          "gradually decreases the distance between intra-personal fa-": "",
          "StyleGANs. For each identity, synthetic images for each at-": "sions. For each subject, 4845 video frames were recorded,"
        },
        {
          "gradually decreases the distance between intra-personal fa-": "4.1. Setup for FRL",
          "StyleGANs. For each identity, synthetic images for each at-": ""
        },
        {
          "gradually decreases the distance between intra-personal fa-": "",
          "StyleGANs. For each identity, synthetic images for each at-": "and the action unit\n(AU)\nintensity was annotated for each"
        },
        {
          "gradually decreases the distance between intra-personal fa-": "Synthetic Face Images: We utilize synthetic face im-",
          "StyleGANs. For each identity, synthetic images for each at-": "frame with six levels from 0 (not present) to 5 (maximum"
        },
        {
          "gradually decreases the distance between intra-personal fa-": "ages generated by StyleGAN [29] and StyleGAN3 [28].",
          "StyleGANs. For each identity, synthetic images for each at-": "intensity) for several AUs. We use AU6 (Cheek Raiser) and"
        },
        {
          "gradually decreases the distance between intra-personal fa-": "The reason for using two different StyleGANs is to employ",
          "StyleGANs. For each identity, synthetic images for each at-": "AU12 (Lip Corner Puller), which contain frames with a high"
        },
        {
          "gradually decreases the distance between intra-personal fa-": "the attributes for face manipulation provided in each Style-",
          "StyleGANs. For each identity, synthetic images for each at-": "AU intensity, and we extract frames with an intensity from"
        },
        {
          "gradually decreases the distance between intra-personal fa-": "GAN. In StyleGAN, we use the attributes that vary weight",
          "StyleGANs. For each identity, synthetic images for each at-": "1 to 5. We estimate the intensity changes of AU6 and AU12"
        },
        {
          "gradually decreases the distance between intra-personal fa-": "provided by Ref.\n[49] and the attributes that vary age and",
          "StyleGANs. For each identity, synthetic images for each at-": "as facial expression changes.\nSee Supplement C.3 for an"
        },
        {
          "gradually decreases the distance between intra-personal fa-": "smile provided by InterFaceGAN [52].\nIn StyleGAN3, we",
          "StyleGANs. For each identity, synthetic images for each at-": "evaluation for other major AUs."
        },
        {
          "gradually decreases the distance between intra-personal fa-": "employ the 40 attributes included in the CelebA dataset [41]",
          "StyleGANs. For each identity, synthetic images for each at-": "Weight Change Dataset: We use\nthe dataset\ncollected"
        },
        {
          "gradually decreases the distance between intra-personal fa-": "provided by Ref. [4]. The 40 attributes contain various face-",
          "StyleGANs. For each identity, synthetic images for each at-": "in Ref.\n[2]\n(Edema-A)\nand\nour newly collected\ndataset"
        },
        {
          "gradually decreases the distance between intra-personal fa-": "related factors such as Big Nose, Bags Under Eyes, and Pale",
          "StyleGANs. For each identity, synthetic images for each at-": "(Edema-B). These datasets contain face images and weight"
        },
        {
          "gradually decreases the distance between intra-personal fa-": "Skin. With these 43 attributes, we use synthetic face images",
          "StyleGANs. For each identity, synthetic images for each at-": "data obtained from dialysis patients before and after dialy-"
        },
        {
          "gradually decreases the distance between intra-personal fa-": "that change according to a wide range of face attributes. We",
          "StyleGANs. For each identity, synthetic images for each at-": "sis. Edema-A and Edema-B were collected from different"
        },
        {
          "gradually decreases the distance between intra-personal fa-": "generate synthetic face images with 250,000 identities using",
          "StyleGANs. For each identity, synthetic images for each at-": "hospitals and different patients. Dialysis removes fluid from"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 1: shows the results",
      "data": [
        {
          "Table 1. Results of estimating facial expression change for AU6": ""
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": "and AU12.\nResults are evaluated in linear evaluation and fine-"
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": ""
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": "tuning."
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": ""
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": "AU6\nAU12"
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": "Linear\nFine-tuning\nLinear\nFine-tuning"
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": ""
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": "Method\nMAE↓\nCorr.↑ MAE↓\nCorr.↑ MAE↓\nCorr.↑ MAE↓\nCorr.↑"
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": ""
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": "Scratch\n-\n-\n0.745\n0.277\n-\n-\n0.975\n0.326"
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": ""
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": "General Pre-training:"
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": "ImageNet [13]\n0.752\n0.339\n0.650\n0.613\n0.973\n0.457\n0.634\n0.801"
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": "VGGFace2 [7]\n0.730\n0.461\n0.660\n0.578\n0.796\n0.670\n0.644\n0.795"
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": ""
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": "Visual Representation Learning:"
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": "SimCLR [9]\n0.742\n0.469\n0.662\n0.595\n0.959\n0.576\n0.664\n0.793"
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": "MoCo v2 [10, 22]\n0.749\n0.294\n0.621\n0.639\n0.985\n0.484\n0.606\n0.815"
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": "SwAV [8]\n0.745\n0.464\n0.626\n0.655\n0.964\n0.645\n0.656\n0.796"
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": "Barlow Twins [65]\n0.745\n0.472\n0.662\n0.607\n0.958\n0.698\n0.659\n0.793"
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": "Facial Representation Learning:"
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": "0.604\n0.669\nBulat et al. [6]\n0.745\n0.412\n0.964\n0.567\n0.599\n0.829"
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": ""
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": "FaRL [70]\n0.727\n0.476\n0.645\n0.627\n0.809\n0.748\n0.617\n0.820"
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": "PCL [40]\n0.735\n0.415\n0.684\n0.552\n0.914\n0.635\n0.636\n0.800"
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": "ComFace (Ours)\n0.639\n0.648\n0.663\n0.786\n0.598\n0.831\n0.629\n0.663"
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": ""
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": ""
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": "the training datasets,\ntraining scales,\ntraining sources, and"
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": ""
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": "backbones for all comparative and proposed methods."
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": ""
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": "The downstream tasks for comparing faces are regres-"
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": ""
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": "sion analyses that estimate facial expression, weight, and"
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": ""
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": "age changes from two face images. As evaluation metrics,"
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": ""
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": "we use the mean absolute error (MAE) and Pearson corre-"
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": ""
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": "lation coefficient (Corr.).\nIn weight change estimation, we"
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": ""
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": "also use accuracy (Acc.), which represents the prediction"
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": ""
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": "performance in the direction of weight gain or\nloss, since"
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": ""
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": "weight changes before and after dialysis."
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": ""
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": "4.4. Main Results"
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": ""
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": "Facial Expression Change: Table 1 shows the results"
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": ""
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": "of estimating facial expression change in AU6 and AU12."
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": ""
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": "First, in linear evaluation 3, ComFace outperforms all other"
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": "methods on both AUs by a large margin. This result\nindi-"
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": ""
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": "cates the advantage of ComFace, which focuses on intra-"
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": "personal facial changes, over other methods that do not fo-"
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": "cus on such changes. Thus, we find that ComFace success-"
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": "fully acquires representations that capture intra-personal fa-"
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": "cial changes by using synthetic face images.\nSecond,\nin"
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": "fine-tuning, ComFace is slightly worse for AU6 and slightly"
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": "better\nfor AU12 than Bulat et al.\n[6]. Our model,\ntrained"
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": "using only synthetic images, has comparable transfer per-"
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": "formance to the SoTA FRL method trained using real\nim-"
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": "ages, suggesting the potential of representation learning us-"
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": "ing synthetic images. Furthermore, ComFace performs bet-"
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": "ter than general pre-training and visual representation learn-"
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": "ing methods, so our synthetic image-based model outper-"
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": "forms previous baseline methods."
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": ""
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": "Weight Change:\nIn facial expression change estimation,"
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": ""
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": "fine-tuning is\nsuperior\nto linear evaluation for ComFace."
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": ""
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": "Since our ultimate goal\nis achieving high performance, we"
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": ""
        },
        {
          "Table 1. Results of estimating facial expression change for AU6": "3To evaluate the learned representations, the backbone is frozen and the"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 5: Ablation study on ComFace. In “Learning” column,",
      "data": [
        {
          "Table 2. Results of weight change estimation in Edema-A, Edema-": "B, and Edema-A→B cross-dataset evaluation. Results are evalu-",
          "Table 4. Results of estimating age change. Results are evaluated": "in fine-tuning."
        },
        {
          "Table 2. Results of weight change estimation in Edema-A, Edema-": "ated in fine-tuning.",
          "Table 4. Results of estimating age change. Results are evaluated": "Method\nMAE↓\nCorr.↑"
        },
        {
          "Table 2. Results of weight change estimation in Edema-A, Edema-": "Edema-A",
          "Table 4. Results of estimating age change. Results are evaluated": "Scratch\n8.980\n0.514"
        },
        {
          "Table 2. Results of weight change estimation in Edema-A, Edema-": "",
          "Table 4. Results of estimating age change. Results are evaluated": "ImageNet [13]\n7.863\n0.614"
        },
        {
          "Table 2. Results of weight change estimation in Edema-A, Edema-": "Method\nMAE↓\nCorr.↑\nAcc.↑ MAE↓",
          "Table 4. Results of estimating age change. Results are evaluated": ""
        },
        {
          "Table 2. Results of weight change estimation in Edema-A, Edema-": "",
          "Table 4. Results of estimating age change. Results are evaluated": "SwAV [8]\n6.368\n0.780"
        },
        {
          "Table 2. Results of weight change estimation in Edema-A, Edema-": "Scratch\n1.768\n0.416\n68.5\n1.682",
          "Table 4. Results of estimating age change. Results are evaluated": ""
        },
        {
          "Table 2. Results of weight change estimation in Edema-A, Edema-": "",
          "Table 4. Results of estimating age change. Results are evaluated": "FaRL [70]\n5.249\n0.851"
        },
        {
          "Table 2. Results of weight change estimation in Edema-A, Edema-": "General Pre-training:",
          "Table 4. Results of estimating age change. Results are evaluated": ""
        },
        {
          "Table 2. Results of weight change estimation in Edema-A, Edema-": "",
          "Table 4. Results of estimating age change. Results are evaluated": "ComFace (Ours)\n4.914\n0.870"
        },
        {
          "Table 2. Results of weight change estimation in Edema-A, Edema-": "ImageNet [13]\n1.486\n0.655\n83.6\n1.665",
          "Table 4. Results of estimating age change. Results are evaluated": ""
        },
        {
          "Table 2. Results of weight change estimation in Edema-A, Edema-": "VGGFace2 [7]\n1.482\n0.695\n84.8\n1.593",
          "Table 4. Results of estimating age change. Results are evaluated": ""
        },
        {
          "Table 2. Results of weight change estimation in Edema-A, Edema-": "Visual Representation Learning:",
          "Table 4. Results of estimating age change. Results are evaluated": "Table 5.\nAblation study on ComFace.\nIn “Learning” column,"
        },
        {
          "Table 2. Results of weight change estimation in Edema-A, Edema-": "SimCLR [9]\n1.535\n0.671\n82.2\n1.504",
          "Table 4. Results of estimating age change. Results are evaluated": "“Both” denotes both inter- and intra-personal\nlearning.\nLine in-"
        },
        {
          "Table 2. Results of weight change estimation in Edema-A, Edema-": "MoCo v2 [10, 22]\n1.451\n0.702\n84.8\n1.488",
          "Table 4. Results of estimating age change. Results are evaluated": ""
        },
        {
          "Table 2. Results of weight change estimation in Edema-A, Edema-": "1.448\nSwAV [8]\n1.483\n0.718\n85.7",
          "Table 4. Results of estimating age change. Results are evaluated": "dicated in gray is our final setting."
        },
        {
          "Table 2. Results of weight change estimation in Edema-A, Edema-": "Barlow Twins [65]\n1.552\n0.634\n80.4\n1.575",
          "Table 4. Results of estimating age change. Results are evaluated": ""
        },
        {
          "Table 2. Results of weight change estimation in Edema-A, Edema-": "",
          "Table 4. Results of estimating age change. Results are evaluated": "AU6\nAU12\nEdema-A\nEdema-B\nAge"
        },
        {
          "Table 2. Results of weight change estimation in Edema-A, Edema-": "Facial Representation Learning:",
          "Table 4. Results of estimating age change. Results are evaluated": "Learning\nIntra-personal GT\nCurriculum"
        },
        {
          "Table 2. Results of weight change estimation in Edema-A, Edema-": "",
          "Table 4. Results of estimating age change. Results are evaluated": "Corr.↑\nCorr.↑\nAcc.↑\nAcc.↑\nCorr.↑"
        },
        {
          "Table 2. Results of weight change estimation in Edema-A, Edema-": "Bulat et al. [6]\n1.462\n0.697\n84.3\n1.479",
          "Table 4. Results of estimating age change. Results are evaluated": ""
        },
        {
          "Table 2. Results of weight change estimation in Edema-A, Edema-": "FaRL [70]\n1.517\n0.654\n82.8\n1.454",
          "Table 4. Results of estimating age change. Results are evaluated": "✓"
        },
        {
          "Table 2. Results of weight change estimation in Edema-A, Edema-": "",
          "Table 4. Results of estimating age change. Results are evaluated": "(a)\nInter-\n0.593\n0.829\n79.5\n89.0\n0.782\n|αyi − αxi |"
        },
        {
          "Table 2. Results of weight change estimation in Edema-A, Edema-": "PCL [40]\n1.484\n0.662\n82.0\n1.510",
          "Table 4. Results of estimating age change. Results are evaluated": "✓"
        },
        {
          "Table 2. Results of weight change estimation in Edema-A, Edema-": "",
          "Table 4. Results of estimating age change. Results are evaluated": "(b)\nIntra-\n0.660\n0.833\n87.0\n93.1\n0.861\n|αyi − αxi |"
        },
        {
          "Table 2. Results of weight change estimation in Edema-A, Edema-": "ComFace (Ours)\n1.394\n0.750\n88.6\n1.523",
          "Table 4. Results of estimating age change. Results are evaluated": "✓"
        },
        {
          "Table 2. Results of weight change estimation in Edema-A, Edema-": "",
          "Table 4. Results of estimating age change. Results are evaluated": "(c)\nBoth\n0.663\n0.831\n88.6\n96.3\n0.870\n|αyi − αxi |"
        },
        {
          "Table 2. Results of weight change estimation in Edema-A, Edema-": "",
          "Table 4. Results of estimating age change. Results are evaluated": "✓"
        },
        {
          "Table 2. Results of weight change estimation in Edema-A, Edema-": "",
          "Table 4. Results of estimating age change. Results are evaluated": "(d)\nBoth\n0.662\n0.818\n82.8\n91.0\n0.830\nαyi − αxi"
        },
        {
          "Table 2. Results of weight change estimation in Edema-A, Edema-": "",
          "Table 4. Results of estimating age change. Results are evaluated": "(e)\nBoth\n0.659\n0.821\n88.0\n94.9\n0.856\n|αyi − αxi |"
        },
        {
          "Table 2. Results of weight change estimation in Edema-A, Edema-": "Table 3. Results of estimating weight change with patient-specific",
          "Table 4. Results of estimating age change. Results are evaluated": ""
        },
        {
          "Table 2. Results of weight change estimation in Edema-A, Edema-": "model and our patient-generic model.",
          "Table 4. Results of estimating age change. Results are evaluated": ""
        },
        {
          "Table 2. Results of weight change estimation in Edema-A, Edema-": "",
          "Table 4. Results of estimating age change. Results are evaluated": "and 15 patients for transfer learning and testing, and Com-"
        },
        {
          "Table 2. Results of weight change estimation in Edema-A, Edema-": "transferred on 1 to 3 days of per-patient data.",
          "Table 4. Results of estimating age change. Results are evaluated": ""
        },
        {
          "Table 2. Results of weight change estimation in Edema-A, Edema-": "",
          "Table 4. Results of estimating age change. Results are evaluated": "Face uses the same 15 patients for\ntesting.\nSince the per-"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 5: Ablation study on ComFace. In “Learning” column,",
      "data": [
        {
          "model and our patient-generic model.": "",
          "Patient-specific model": "",
          "is": ""
        },
        {
          "model and our patient-generic model.": "transferred on 1 to 3 days of per-patient data.",
          "Patient-specific model": "",
          "is": ""
        },
        {
          "model and our patient-generic model.": "",
          "Patient-specific model": "",
          "is": "Face uses the same 15 patients for"
        },
        {
          "model and our patient-generic model.": "",
          "Patient-specific model": "Edema-A",
          "is": ""
        },
        {
          "model and our patient-generic model.": "",
          "Patient-specific model": "",
          "is": ""
        },
        {
          "model and our patient-generic model.": "Method",
          "Patient-specific model": "Corr.↑",
          "is": ""
        },
        {
          "model and our patient-generic model.": "",
          "Patient-specific model": "",
          "is": ""
        },
        {
          "model and our patient-generic model.": "Patient-specific (1-day) [2]",
          "Patient-specific model": "0.473",
          "is": ""
        },
        {
          "model and our patient-generic model.": "Patient-specific (2-day) [2]",
          "Patient-specific model": "0.617",
          "is": "1 to 3 days."
        },
        {
          "model and our patient-generic model.": "Patient-specific (3-day) [2]",
          "Patient-specific model": "0.717",
          "is": ""
        },
        {
          "model and our patient-generic model.": "Patient-generic (Ours)",
          "Patient-specific model": "0.720",
          "is": ""
        },
        {
          "model and our patient-generic model.": "",
          "Patient-specific model": "",
          "is": ""
        },
        {
          "model and our patient-generic model.": "",
          "Patient-specific model": "",
          "is": "than the patient-specific model"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 5: (c,e) demonstrates the ef- center of the face, where intra-personal facial changes of-",
      "data": [
        {
          "0.831": "",
          "88.6": "",
          "0.870": ""
        },
        {
          "0.831": "",
          "88.6": "",
          "0.870": ""
        },
        {
          "0.831": "0.816",
          "88.6": "",
          "0.870": ""
        },
        {
          "0.831": "",
          "88.6": "",
          "0.870": ""
        },
        {
          "0.831": "",
          "88.6": "",
          "0.870": ""
        },
        {
          "0.831": "",
          "88.6": "",
          "0.870": ""
        },
        {
          "0.831": "Edema-A  Fine-tuning Acc. (%)",
          "88.6": "",
          "0.870": ""
        },
        {
          "0.831": "",
          "88.6": "",
          "0.870": ""
        },
        {
          "0.831": "ComFace",
          "88.6": "",
          "0.870": ""
        },
        {
          "0.831": "ImageNet",
          "88.6": "",
          "0.870": "ImageNet"
        },
        {
          "0.831": "",
          "88.6": "",
          "0.870": ""
        },
        {
          "0.831": "MoCo v2",
          "88.6": "",
          "0.870": ""
        },
        {
          "0.831": "",
          "88.6": "",
          "0.870": ""
        },
        {
          "0.831": "Bulat et al.",
          "88.6": "",
          "0.870": ""
        },
        {
          "0.831": "",
          "88.6": "",
          "0.870": ""
        },
        {
          "0.831": "",
          "88.6": "",
          "0.870": ""
        },
        {
          "0.831": "20\n35",
          "88.6": "35",
          "0.870": "35"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 5: (c,e) demonstrates the ef- center of the face, where intra-personal facial changes of-",
      "data": [
        {
          "Figure 4. Saliency maps for a face image in four pre-trained backbones. Original images show positions of AU6 and AU12 and eyelid and": "nose shape where edema appears."
        },
        {
          "Figure 4. Saliency maps for a face image in four pre-trained backbones. Original images show positions of AU6 and AU12 and eyelid and": "learning tends to improve the performance."
        },
        {
          "Figure 4. Saliency maps for a face image in four pre-trained backbones. Original images show positions of AU6 and AU12 and eyelid and": "Intra-personal GT: For FRL, we use |αyi − αxi| as intra-"
        },
        {
          "Figure 4. Saliency maps for a face image in four pre-trained backbones. Original images show positions of AU6 and AU12 and eyelid and": "personal GT (see Section 3.2) instead of αyi − αxi, which"
        },
        {
          "Figure 4. Saliency maps for a face image in four pre-trained backbones. Original images show positions of AU6 and AU12 and eyelid and": "is used for transfer learning. Table 5 (c,d) compares the two"
        },
        {
          "Figure 4. Saliency maps for a face image in four pre-trained backbones. Original images show positions of AU6 and AU12 and eyelid and": "intra-personal GTs and shows that the use of |αyi − αxi| is"
        },
        {
          "Figure 4. Saliency maps for a face image in four pre-trained backbones. Original images show positions of AU6 and AU12 and eyelid and": "better than that of αyi −αxi. We expect the use of αyi −αxi"
        },
        {
          "Figure 4. Saliency maps for a face image in four pre-trained backbones. Original images show positions of AU6 and AU12 and eyelid and": "harms FRL since the direction of facial change varies by at-"
        },
        {
          "Figure 4. Saliency maps for a face image in four pre-trained backbones. Original images show positions of AU6 and AU12 and eyelid and": "tribute (e.g., gender and age)."
        },
        {
          "Figure 4. Saliency maps for a face image in four pre-trained backbones. Original images show positions of AU6 and AU12 and eyelid and": "Curriculum Learning: Table 5 (c,e) demonstrates the ef-"
        },
        {
          "Figure 4. Saliency maps for a face image in four pre-trained backbones. Original images show positions of AU6 and AU12 and eyelid and": "fectiveness of curriculum learning. We can see that grad-"
        },
        {
          "Figure 4. Saliency maps for a face image in four pre-trained backbones. Original images show positions of AU6 and AU12 and eyelid and": "ually increasing the difficulty level of\nidentifying intra-"
        },
        {
          "Figure 4. Saliency maps for a face image in four pre-trained backbones. Original images show positions of AU6 and AU12 and eyelid and": "personal\nfacial changes via curriculum learning is reason-"
        },
        {
          "Figure 4. Saliency maps for a face image in four pre-trained backbones. Original images show positions of AU6 and AU12 and eyelid and": ""
        },
        {
          "Figure 4. Saliency maps for a face image in four pre-trained backbones. Original images show positions of AU6 and AU12 and eyelid and": "able and leads to effective representation learning."
        },
        {
          "Figure 4. Saliency maps for a face image in four pre-trained backbones. Original images show positions of AU6 and AU12 and eyelid and": "Training Scale: We investigate the performance of Com-"
        },
        {
          "Figure 4. Saliency maps for a face image in four pre-trained backbones. Original images show positions of AU6 and AU12 and eyelid and": ""
        },
        {
          "Figure 4. Saliency maps for a face image in four pre-trained backbones. Original images show positions of AU6 and AU12 and eyelid and": "Face with respect to training scales of synthetic images. We"
        },
        {
          "Figure 4. Saliency maps for a face image in four pre-trained backbones. Original images show positions of AU6 and AU12 and eyelid and": ""
        },
        {
          "Figure 4. Saliency maps for a face image in four pre-trained backbones. Original images show positions of AU6 and AU12 and eyelid and": "vary the number of identities for synthetic faces and set the"
        },
        {
          "Figure 4. Saliency maps for a face image in four pre-trained backbones. Original images show positions of AU6 and AU12 and eyelid and": ""
        },
        {
          "Figure 4. Saliency maps for a face image in four pre-trained backbones. Original images show positions of AU6 and AU12 and eyelid and": "following training scales: 0.1M, 5M, 20M, and 35M (our"
        },
        {
          "Figure 4. Saliency maps for a face image in four pre-trained backbones. Original images show positions of AU6 and AU12 and eyelid and": ""
        },
        {
          "Figure 4. Saliency maps for a face image in four pre-trained backbones. Original images show positions of AU6 and AU12 and eyelid and": "final setting). Figure 3 represents the transfer performance"
        },
        {
          "Figure 4. Saliency maps for a face image in four pre-trained backbones. Original images show positions of AU6 and AU12 and eyelid and": ""
        },
        {
          "Figure 4. Saliency maps for a face image in four pre-trained backbones. Original images show positions of AU6 and AU12 and eyelid and": "for three downstream tasks versus the training scales. Com-"
        },
        {
          "Figure 4. Saliency maps for a face image in four pre-trained backbones. Original images show positions of AU6 and AU12 and eyelid and": ""
        },
        {
          "Figure 4. Saliency maps for a face image in four pre-trained backbones. Original images show positions of AU6 and AU12 and eyelid and": "Face is compared with the best methods\nfor general pre-"
        },
        {
          "Figure 4. Saliency maps for a face image in four pre-trained backbones. Original images show positions of AU6 and AU12 and eyelid and": ""
        },
        {
          "Figure 4. Saliency maps for a face image in four pre-trained backbones. Original images show positions of AU6 and AU12 and eyelid and": "training, visual\nrepresentation learning, and FRL,\nrespec-"
        },
        {
          "Figure 4. Saliency maps for a face image in four pre-trained backbones. Original images show positions of AU6 and AU12 and eyelid and": ""
        },
        {
          "Figure 4. Saliency maps for a face image in four pre-trained backbones. Original images show positions of AU6 and AU12 and eyelid and": "tively. The performance of ComFace improves as the train-"
        },
        {
          "Figure 4. Saliency maps for a face image in four pre-trained backbones. Original images show positions of AU6 and AU12 and eyelid and": ""
        },
        {
          "Figure 4. Saliency maps for a face image in four pre-trained backbones. Original images show positions of AU6 and AU12 and eyelid and": "ing scale increases. ComFace outperforms the comparative"
        },
        {
          "Figure 4. Saliency maps for a face image in four pre-trained backbones. Original images show positions of AU6 and AU12 and eyelid and": ""
        },
        {
          "Figure 4. Saliency maps for a face image in four pre-trained backbones. Original images show positions of AU6 and AU12 and eyelid and": "method on the full training scale, showing the advantage of"
        },
        {
          "Figure 4. Saliency maps for a face image in four pre-trained backbones. Original images show positions of AU6 and AU12 and eyelid and": ""
        },
        {
          "Figure 4. Saliency maps for a face image in four pre-trained backbones. Original images show positions of AU6 and AU12 and eyelid and": "synthetic images not being limited in the number of images."
        },
        {
          "Figure 4. Saliency maps for a face image in four pre-trained backbones. Original images show positions of AU6 and AU12 and eyelid and": ""
        },
        {
          "Figure 4. Saliency maps for a face image in four pre-trained backbones. Original images show positions of AU6 and AU12 and eyelid and": ""
        },
        {
          "Figure 4. Saliency maps for a face image in four pre-trained backbones. Original images show positions of AU6 and AU12 and eyelid and": "4.6. Visualization"
        },
        {
          "Figure 4. Saliency maps for a face image in four pre-trained backbones. Original images show positions of AU6 and AU12 and eyelid and": ""
        },
        {
          "Figure 4. Saliency maps for a face image in four pre-trained backbones. Original images show positions of AU6 and AU12 and eyelid and": "We calculate saliency maps to provide the interpretabil-"
        },
        {
          "Figure 4. Saliency maps for a face image in four pre-trained backbones. Original images show positions of AU6 and AU12 and eyelid and": "ity of models. The saliency maps are obtained from the final"
        },
        {
          "Figure 4. Saliency maps for a face image in four pre-trained backbones. Original images show positions of AU6 and AU12 and eyelid and": "block of\nthe pre-trained backbones via Eigen-CAM [44],"
        },
        {
          "Figure 4. Saliency maps for a face image in four pre-trained backbones. Original images show positions of AU6 and AU12 and eyelid and": "which visualizes the principle components of\nlearned rep-"
        },
        {
          "Figure 4. Saliency maps for a face image in four pre-trained backbones. Original images show positions of AU6 and AU12 and eyelid and": "resentations without relying on class relevance scores. Fig-"
        },
        {
          "Figure 4. Saliency maps for a face image in four pre-trained backbones. Original images show positions of AU6 and AU12 and eyelid and": "ure 4 illustrates the saliency maps of four models for a face"
        },
        {
          "Figure 4. Saliency maps for a face image in four pre-trained backbones. Original images show positions of AU6 and AU12 and eyelid and": "image from DISFA [42, 43]. Models using ImageNet and"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "References": "",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": "2020. 3"
        },
        {
          "References": "[1] R. Abdal, P. Zhu, N. J. Mitra, and P. Wonka.\nStyleFlow:",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": ""
        },
        {
          "References": "",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": "[15] Z. Deng, H. Liu, Y. Wang, C. Wang, et al.\nPML: Progres-"
        },
        {
          "References": "Attribute-conditioned\nexploration\nof StyleGAN-generated",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": ""
        },
        {
          "References": "",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": "sive margin loss for long-tailed age classification.\nIn Proc."
        },
        {
          "References": "images\nusing\nconditional\ncontinuous\nnormalizing\nflows.",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": ""
        },
        {
          "References": "",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": "IEEE/CVF Conf. Computer Vision and Pattern Recognition"
        },
        {
          "References": "ACM Transactions on Graphics (ToG), 40(3):1–21, 2021. 3",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": ""
        },
        {
          "References": "",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": "(CVPR), pages 10503–10512, 2021. 1, 3"
        },
        {
          "References": "[2] Y. Akamatsu, Y. Onishi, H.\nImaoka,\nJ. Kameyama,\net al.",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": ""
        },
        {
          "References": "",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": "[16] X. Di, Y. Zheng, X. Liu, and Y. Cheng. Pros: Facial omni-"
        },
        {
          "References": "Edema estimation from facial\nimages taken before and af-",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": ""
        },
        {
          "References": "",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": "representation learning via prototype-based self-distillation."
        },
        {
          "References": "IEEE\nter dialysis via contrastive multi-patient pre-training.",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": ""
        },
        {
          "References": "",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": "In Proc. IEEE/CVF Winter Conf. Applications of Computer"
        },
        {
          "References": "Journal of Biomedical and Health Informatics, 27(3):1419–",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": ""
        },
        {
          "References": "",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": "Vision (WACV), pages 6087–6098, 2024. 3"
        },
        {
          "References": "1430, 2023. 1, 2, 3, 5, 6, 7, 8",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": ""
        },
        {
          "References": "",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": "[17] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn,"
        },
        {
          "References": "[3] Y. Alaluf, O. Patashnik, and D. Cohen-Or.\nOnly a matter",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": ""
        },
        {
          "References": "",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": "et al.\nAn image is worth 16x16 words: Transformers for"
        },
        {
          "References": "of style: Age transformation using a style-based regression",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": ""
        },
        {
          "References": "",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": "image recognition at scale. In Proc. Int. Conf. Learning Rep-"
        },
        {
          "References": "model. ACM Transactions on Graphics (TOG), 40(4):1–12,",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": ""
        },
        {
          "References": "",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": "resentations (ICLR), 2020. 3, 5"
        },
        {
          "References": "2021. 3",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": ""
        },
        {
          "References": "",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": "[18]\nJ. J. Engelsma, S. Grosz, and A. K. Jain. Printsgan: Synthetic"
        },
        {
          "References": "[4] Y. Alaluf, O. Patashnik, Z. Wu, A. Zamir, et al. Third time’s",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": ""
        },
        {
          "References": "",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": "fingerprint generator. IEEE Transactions on Pattern Analysis"
        },
        {
          "References": "the charm?\nimage and video editing with StyleGAN3.\nIn",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": ""
        },
        {
          "References": "",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": "and Machine Intelligence, 45(5):6111–6124, 2022. 3"
        },
        {
          "References": "Proc. European Conf. Computer Vision Workshops, pages",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": ""
        },
        {
          "References": "",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": "Iranian Journal\n[19] D. D. Farhud.\nImpact of lifestyle on health."
        },
        {
          "References": "204–220. Springer, 2022. 3, 5",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": ""
        },
        {
          "References": "",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": "of Public Health, 44(11):1442, 2015. 1"
        },
        {
          "References": "[5] Y. Bengio, J. Louradour, R. Collobert, and J. Weston. Cur-",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": ""
        },
        {
          "References": "",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": "[20]\nI. Goodfellow,\nJ. Pouget-Abadie, M. Mirza, B. Xu,\net al."
        },
        {
          "References": "Int. Conf. Machine Learning\nriculum learning.\nIn Proc.",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": ""
        },
        {
          "References": "",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": "Generative adversarial nets.\nIn Proc. Advances in Neural In-"
        },
        {
          "References": "(ICML), pages 41–48, 2009. 2, 4",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": ""
        },
        {
          "References": "",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": "formation Processing Systems (NeurIPS), volume 27, 2014."
        },
        {
          "References": "[6] A. Bulat, S. Cheng, J. Yang, A. Garbett, et al.\nPre-training",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": ""
        },
        {
          "References": "",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": "3"
        },
        {
          "References": "strategies and datasets for facial representation learning.\nIn",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": ""
        },
        {
          "References": "",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": "[21]\nI. J. Goodfellow, D. Erhan, P. L. Carrier, A. Courville, et al."
        },
        {
          "References": "Proc. European Conf. Computer Vision (ECCV), pages 107–",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": ""
        },
        {
          "References": "",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": "Challenges in representation learning: A report on three ma-"
        },
        {
          "References": "125. Springer, 2022. 1, 3, 5, 6, 7",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": ""
        },
        {
          "References": "",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": "chine learning contests.\nIn Proc. Int. Conf. Neural Informa-"
        },
        {
          "References": "[7] Q. Cao, L. Shen, W. Xie, O. M. Parkhi, et al. Vggface2: A",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": "tion Processing (ICONIP), pages 117–124, 2013. 1"
        },
        {
          "References": "dataset for recognising faces across pose and age.\nIn Proc.",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": ""
        },
        {
          "References": "",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": "[22] K. He, H. Fan, Y. Wu, S. Xie, et al. Momentum contrast"
        },
        {
          "References": "Int. Conf. Automatic Face & Gesture Recognition\n(FG),",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": ""
        },
        {
          "References": "",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": "for unsupervised visual\nrepresentation learning.\nIn Proc."
        },
        {
          "References": "pages 67–74, 2018. 1, 5, 6, 7",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": ""
        },
        {
          "References": "",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": "IEEE/CVF Conf. Computer Vision and Pattern Recognition"
        },
        {
          "References": "[8] M. Caron,\nI. Misra,\nJ. Mairal, P. Goyal,\net al.\nUnsuper-",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": "(CVPR), pages 9729–9738, 2020. 4, 5, 6, 7"
        },
        {
          "References": "vised learning of visual\nfeatures by contrasting cluster as-",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": ""
        },
        {
          "References": "",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": "[23] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual\nlearn-"
        },
        {
          "References": "Information Pro-\nsignments.\nIn Proc. Advances in Neural",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": ""
        },
        {
          "References": "",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": "ing for image recognition.\nIn Proc. IEEE/CVF Conf. Com-"
        },
        {
          "References": "cessing Systems\n(NeurIPS), volume 33, pages 9912–9924,",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": ""
        },
        {
          "References": "",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": "puter Vision and Pattern Recognition (CVPR), pages 770–"
        },
        {
          "References": "2020. 6, 7",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": ""
        },
        {
          "References": "",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": "778, 2016. 3, 5"
        },
        {
          "References": "[9] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton. A simple",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": ""
        },
        {
          "References": "",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": "[24]\nS. J. Henly, J. F. Wyman, and M. J. Findorff. Health and ill-"
        },
        {
          "References": "framework for contrastive learning of visual representations.",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": ""
        },
        {
          "References": "",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": "ness over time: The trajectory perspective in nursing science."
        },
        {
          "References": "In Proc. Int. Conf. Machine Learning (ICML), pages 1597–",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": ""
        },
        {
          "References": "",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": "Nursing Research, 60(3 Suppl):S5, 2011. 1"
        },
        {
          "References": "1607, 2020. 2, 4, 5, 6, 7",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": ""
        },
        {
          "References": "",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": "[25]\nP. Irtem, E. Irtem, and N. Erdo˘gmus¸.\nImpact of variations in"
        },
        {
          "References": "[10] X. Chen, H. Fan, R. Girshick, and K. He.\nImproved base-",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": "synthetic training data on fingerprint classification.\nIn Proc."
        },
        {
          "References": "arXiv preprint\nlines with momentum contrastive learning.",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": "Int. Conf. Biometrics Special Interest Group (BIOSIG), 2019."
        },
        {
          "References": "arXiv:2003.04297, 2020. 6, 7",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": "3"
        },
        {
          "References": "[11] E. Collins, R. Bala, B. Price,\nand S. Susstrunk.\nEditing",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": "[26]\nI. Joshi, M. Grimmer, C. Rathgeb, C. Busch, et al. Synthetic"
        },
        {
          "References": "in style: Uncovering the local semantics of gans.\nIn Proc.",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": "IEEE Transactions on\ndata in human analysis: A survey."
        },
        {
          "References": "IEEE/CVF Conf. Computer Vision and Pattern Recognition",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": "Pattern Analysis and Machine Intelligence, 2024. 3"
        },
        {
          "References": "(CVPR), pages 5771–5780, 2020. 3",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": ""
        },
        {
          "References": "",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": "[27] T. Karras, M. Aittala,\nJ. Hellsten, S. Laine, et al.\nTrain-"
        },
        {
          "References": "[12] A. Dantcheva, F. Bremond, and P. Bilinski.\nShow me your",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": "ing generative adversarial networks with limited data.\nIn"
        },
        {
          "References": "face and I will\ntell you your height, weight and body mass",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": "Proc. Advances in Neural\nInformation Processing Systems"
        },
        {
          "References": "index. In Proc. Int. Conf. Pattern Recognition (ICPR), pages",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": "(NeurIPS), volume 33, pages 12104–12114, 2020. 2, 3"
        },
        {
          "References": "3555–3560. IEEE, 2018. 1",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": "[28] T. Karras, M. Aittala, S. Laine, E. H¨ark¨onen, et al. Alias-free"
        },
        {
          "References": "[13]\nJ. Deng, W. Dong, R. Socher, L. Li, et al. Imagenet: A large-",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": "generative adversarial networks.\nIn Proc. Advances in Neu-"
        },
        {
          "References": "scale hierarchical image database.\nIn Proc. IEEE/CVF Conf.",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": "ral\nInformation Processing Systems (NeurIPS), volume 34,"
        },
        {
          "References": "Computer Vision and Pattern Recognition (CVPR), pages",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": "pages 852–863, 2021. 2, 3, 5"
        },
        {
          "References": "248–255, 2009. 1, 5, 6, 7",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": "[29] T. Karras, S. Laine, and T. Aila.\nA style-based generator"
        },
        {
          "References": "[14] Y. Deng,\nJ. Yang, D. Chen, F. Wen,\net al.\nDisentangled",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": "architecture for generative adversarial networks.\nIn Proc."
        },
        {
          "References": "and\ncontrollable\nface\nimage\ngeneration\nvia\n3d\nimitative-",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": "IEEE/CVF Conf. Computer Vision and Pattern Recognition"
        },
        {
          "References": "IEEE/CVF Conf. Computer\ncontrastive learning.\nIn Proc.",
          "Vision and Pattern Recognition (CVPR), pages 5154–5163,": "(CVPR), pages 4401–4410, 2019. 2, 3, 5"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "ing and improving the image quality of StyleGAN.\nIn Proc.",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": "Interaction, pages 443–449, 2015. 3"
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "IEEE/CVF Conf. Computer Vision and Pattern Recognition",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": "[46] G. Panis, A. Lanitis, N. Tsapatsoulis,\nand T. F. Cootes."
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "(CVPR), pages 8110–8119, 2020. 2, 3",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": "Overview of\nresearch on facial ageing using the FG-NET"
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "[31] Y. Kim, D. W. Lee, P. P. Liang, and S. Alghowinem. HI-",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": "ageing database.\nIET Biometrics, 5(2):37–46, 2016. 6"
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "INT: Historical, intra-and inter-personal dynamics modeling",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": "[47] A. Parkin and O. Grinchuk. Recognizing multi-modal face"
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "with cross-person memory transformer.\nIn Proc. Int. Conf.",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": "spoofing with face recognition networks. In Proc. IEEE/CVF"
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "Multimodal Interaction, pages 314–325, 2023. 3",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": "Conf. Computer Vision and Pattern Recognition Workshops,"
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "[32] D. P. Kingma and J. Ba.\nAdam: A method for stochastic",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": "2019. 3"
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "optimization. arXiv preprint arXiv:1412.6980, 2014. 5",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": "[48] O. Patashnik, Z. Wu, E. Shechtman, D. Cohen-Or,\net al."
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": "StyleCLIP: Text-driven manipulation of StyleGAN imagery."
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "[33] B. Knyazev, R. Shvetsov, N. Efremova, and A. Kuharenko.",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": ""
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": "IEEE/CVF Int. Conf. Computer Vision\nIn Proc.\n(ICCV),"
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "Convolutional\nneural\nnetworks\npretrained\non\nlarge\nface",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": ""
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": "pages 2085–2094, 2021. 3"
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "recognition datasets for emotion classification from video.",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": ""
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "arXiv preprint arXiv:1711.04598, 2017. 3",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": "[49] V. Pinnimty, M. Zhao, P. Achananuparp, and E. Lim. Trans-"
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": "forming facial weight of real\nimages by editing latent space"
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "[34] K. Kondo,\nT. Nakamura, Y. Nakamura,\nand\nS.\nSatoh.",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": ""
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": "of StyleGAN. arXiv preprint arXiv:2011.02606, 2020. 3, 5"
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "Siamese-structure deep neural network recognizing changes",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": ""
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": "[50] H. Qiu, B. Yu, D. Gong, Z. Li, et al. SynFace: Face recog-"
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "in facial expression according to the degree of smiling.\nIn",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": ""
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": "IEEE/CVF Int. Conf.\nnition with synthetic data.\nIn Proc."
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "Proc.\nInt. Conf. Pattern Recognition (ICPR), pages 4605–",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": ""
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": "Computer Vision (ICCV), pages 10880–10890, 2021. 3"
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "4612, 2021. 1, 3",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": ""
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": "[51] R. Ranjan, S. Sankaranarayanan, C. D. Castillo, and R. Chel-"
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "[35] A. Kortylewski, B. Egger, A. Schneider, T. Gerig, et al. Ana-",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": ""
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": "lappa. An all-in-one convolutional neural network for face"
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "lyzing and reducing the damage of dataset bias to face recog-",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": ""
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": "Int. Conf. Automatic Face & Gesture\nanalysis.\nIn Proc."
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "nition with synthetic data.\nIn Proc. IEEE/CVF Conf. Com-",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": ""
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": "Recognition (FG), pages 17–24. IEEE, 2017. 3"
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "puter Vision and Pattern Recognition Workshops, 2019. 3",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": ""
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": "[52] Y. Shen,\nJ. Gu, X. Tang,\nand B. Zhou.\nInterpreting the"
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "[36] A. Kortylewski, A. Schneider, T. Gerig, B. Egger,\net\nal.",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": ""
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": "latent space of GANs\nfor semantic face editing.\nIn Proc."
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "Training deep face recognition systems with synthetic data.",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": ""
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": "IEEE/CVF Conf. Computer Vision and Pattern Recognition"
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "arXiv preprint arXiv:1802.05891, 2018. 3",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": ""
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": "(CVPR), pages 9243–9252, 2020. 3, 5"
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "[37]\nS. Li and W. Deng.\nDeep facial expression recognition:",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": ""
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": "[53]\nJ. Sidhpura, R. Veerkhare, P. Shah, and S. Dholay. Face To"
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "IEEE Transactions\nA survey.\non Affective Computing,",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": ""
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": "BMI: A deep learning based approach for computing bmi"
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "13(3):1195–1215, 2020. 1",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": ""
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": "Int. Conf.\nInnovative Trends in Infor-\nfrom face.\nIn Proc."
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "[38]\nS. Lin, Z. Li, B. Fu, S. Chen, et al. Feasibility of using deep",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": ""
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": "mation Technology (ICITIIT), pages 1–6. IEEE, 2022.\n1, 2,"
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "learning to detect coronary artery disease based on facial",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": ""
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": "3"
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "photo.\nEuropean Heart Journal, 41(46):4400–4411, 2020.",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": ""
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": "[54]\nP. T. D. Thinh, H. M. Hung, H. Yang, S. Kim, et al. Emotion"
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "3",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": ""
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": "recognition with sequential multi-task learning technique."
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "[39] H. Ling, K. Kreis, D. Li, S. W. Kim, et al. EditGAN: High-",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": ""
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": "In Proc. IEEE/CVF Int. Conf. Computer Vision Workshops,"
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "precision semantic image editing. In Proc. Advances in Neu-",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": ""
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": "pages 3593–3596, 2021. 3"
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "ral\nInformation Processing Systems (NeurIPS), volume 34,",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": ""
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": "[55] Y. Tian, L. Fan, K. Chen, D. Katabi, et al. Learning vision"
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "pages 16331–16345, 2021. 3",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": ""
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": "from models rivals learning vision from data. arXiv preprint"
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "[40] Y. Liu, W. Wang, Y. Zhan, S. Feng, et al. Pose-disentangled",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": ""
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": "arXiv:2312.17742, 2023. 1, 3"
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "contrastive\nlearning\nfor\nself-supervised\nfacial\nrepresenta-",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": ""
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": "[56] Y. Tian, L. Fan, P. Isola, H. Chang, and D. Krishnan.\nSta-"
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "tion. In Proc. IEEE/CVF Conf. Computer Vision and Pattern",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": ""
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": "bleRep: Synthetic images from text-to-image models make"
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "Recognition (CVPR), pages 9717–9728, 2023. 1, 3, 6, 7",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": ""
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": "strong visual representation learners.\nIn Proc. Advances in"
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "[41] Z. Liu, P. Luo, X. Wang, and X. Tang. Deep learning face at-",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": ""
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": "Neural Information Processing Systems (NeurIPS), 2023. 1,"
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "tributes in the wild.\nIn Proc. IEEE/CVF Int. Conf. Computer",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": ""
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": "3, 7"
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "Vision (ICCV), 2015. 3, 5",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": ""
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": "[57] D. Trampe, J. Quoidbach, and M. Taquet. Emotions in ev-"
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "[42]\nS. M. Mavadati, M. H. Mahoor, K. Bartlett, and P. Trinh. Au-",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": "eryday life. PloS one, 10(12):e0145450, 2015. 1"
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "tomatic detection of non-posed facial action units.\nIn Proc.",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": ""
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": "[58] D. Trigueros, L. Meng, and M. Hartnett. Generating photo-"
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "Int. Conf. Image Processing (ICIP), pages 1817–1820, 2012.",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": ""
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": "realistic training data to improve face recognition accuracy."
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "5, 8",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": ""
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": "Neural Networks, 134:86–94, 2021. 3"
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "[43]\nS. M. Mavadati, M. H. Mahoor, K. Bartlett, P. Trinh, et al.",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": "[59] A. Van den Oord, Y. Li, and O. Vinyals.\nRepresentation"
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "IEEE\nDisfa: A spontaneous facial action intensity database.",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": "arXiv preprint\nlearning with contrastive predictive coding."
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "Transactions on Affective Computing, 4(2):151–160, 2013.",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": "arXiv:1807.03748, 2018. 4"
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "2, 5, 8",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": "[60] M. Wang and W. Deng. Deep face recognition: A survey."
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "[44] M. B. Muhammad and M. Yeasin. Eigen-CAM: Class acti-",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": "Neurocomputing, 429:215–244, 2021. 1"
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "vation map using principal components.\nIn Proc. Int. Joint",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": "[61] Q. Wang,\nJ. Gao, W. Lin,\nand Y. Yuan.\nLearning from"
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "Conf. Neural Networks (IJCNN), pages 1–7, 2020. 8",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": "synthetic data\nfor\ncrowd counting in the wild.\nIn Proc."
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "[45] H. Ng, V. D. Nguyen, V. Vonikakis, and S. Winkler. Deep",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": "IEEE/CVF Conf. Computer Vision and Pattern Recognition"
        },
        {
          "[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, et al. Analyz-": "learning for\nemotion recognition on small datasets using",
          "transfer\nlearning.\nIn Proc. ACM on Int. Conf. Multimodal": "(CVPR), pages 8198–8207, 2019. 3"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[62] Q. Wang, J. Gao, W. Lin, and Y. Yuan.\nPixel-wise crowd": "International Journal of\nunderstanding via synthetic data."
        },
        {
          "[62] Q. Wang, J. Gao, W. Lin, and Y. Yuan.\nPixel-wise crowd": "Computer Vision, 129(1):225–245, 2021. 3"
        },
        {
          "[62] Q. Wang, J. Gao, W. Lin, and Y. Yuan.\nPixel-wise crowd": "[63]\nJ. Woo, M. Fares, C. Pelachaud,\nand C. Achard.\nAMII:"
        },
        {
          "[62] Q. Wang, J. Gao, W. Lin, and Y. Yuan.\nPixel-wise crowd": "Adaptive\nmultimodal\ninter-personal\nand\nintra-personal"
        },
        {
          "[62] Q. Wang, J. Gao, W. Lin, and Y. Yuan.\nPixel-wise crowd": "arXiv\npreprint\nmodel\nfor\nadapted\nbehavior\nsynthesis."
        },
        {
          "[62] Q. Wang, J. Gao, W. Lin, and Y. Yuan.\nPixel-wise crowd": "arXiv:2305.11310, 2023. 3"
        },
        {
          "[62] Q. Wang, J. Gao, W. Lin, and Y. Yuan.\nPixel-wise crowd": "[64]\nF. Xue, Y. Sun, and Y. Yang. Unsupervised facial expression"
        },
        {
          "[62] Q. Wang, J. Gao, W. Lin, and Y. Yuan.\nPixel-wise crowd": "representation learning with contrastive local warping. arXiv"
        },
        {
          "[62] Q. Wang, J. Gao, W. Lin, and Y. Yuan.\nPixel-wise crowd": "preprint arXiv:2303.09034, 2023. 1"
        },
        {
          "[62] Q. Wang, J. Gao, W. Lin, and Y. Yuan.\nPixel-wise crowd": "[65]\nJ. Zbontar, L. Jing, I. Misra, Y. LeCun, et al. Barlow Twins:"
        },
        {
          "[62] Q. Wang, J. Gao, W. Lin, and Y. Yuan.\nPixel-wise crowd": "Self-supervised learning via redundancy reduction.\nIn Proc."
        },
        {
          "[62] Q. Wang, J. Gao, W. Lin, and Y. Yuan.\nPixel-wise crowd": "Int. Conf. Machine Learning (ICML), pages 12310–12320,"
        },
        {
          "[62] Q. Wang, J. Gao, W. Lin, and Y. Yuan.\nPixel-wise crowd": "2021. 6, 7"
        },
        {
          "[62] Q. Wang, J. Gao, W. Lin, and Y. Yuan.\nPixel-wise crowd": "[66] Z. Zhai, P. Yang, X. Zhang, M. Huang, et al. Demodalizing"
        },
        {
          "[62] Q. Wang, J. Gao, W. Lin, and Y. Yuan.\nPixel-wise crowd": "face recognition with synthetic samples. In Proc. AAAI Conf."
        },
        {
          "[62] Q. Wang, J. Gao, W. Lin, and Y. Yuan.\nPixel-wise crowd": "Artificial Intelligence, volume 35, pages 3278–3286, 2021. 3"
        },
        {
          "[62] Q. Wang, J. Gao, W. Lin, and Y. Yuan.\nPixel-wise crowd": "[67] C. Zhang, S. Liu, X. Xu, and C. Zhu.\nC3AE: Exploring"
        },
        {
          "[62] Q. Wang, J. Gao, W. Lin, and Y. Yuan.\nPixel-wise crowd": "the limits of compact model\nfor age estimation.\nIn Proc."
        },
        {
          "[62] Q. Wang, J. Gao, W. Lin, and Y. Yuan.\nPixel-wise crowd": "IEEE/CVF Conf. Computer Vision and Pattern Recognition"
        },
        {
          "[62] Q. Wang, J. Gao, W. Lin, and Y. Yuan.\nPixel-wise crowd": "(CVPR), pages 12587–12596, 2019. 1, 2"
        },
        {
          "[62] Q. Wang, J. Gao, W. Lin, and Y. Yuan.\nPixel-wise crowd": "[68]\nS. Zhang, L. Baams, D. van de Bongardt, and J. S. Dubas."
        },
        {
          "[62] Q. Wang, J. Gao, W. Lin, and Y. Yuan.\nPixel-wise crowd": "Intra-and inter-individual differences in adolescent depres-"
        },
        {
          "[62] Q. Wang, J. Gao, W. Lin, and Y. Yuan.\nPixel-wise crowd": "sive mood: The role of relationships with parents and friends."
        },
        {
          "[62] Q. Wang, J. Gao, W. Lin, and Y. Yuan.\nPixel-wise crowd": "Journal of Abnormal Child Psychology, 46:811–824, 2018."
        },
        {
          "[62] Q. Wang, J. Gao, W. Lin, and Y. Yuan.\nPixel-wise crowd": "1"
        },
        {
          "[62] Q. Wang, J. Gao, W. Lin, and Y. Yuan.\nPixel-wise crowd": "[69] W. Zhao, R. Chellappa, P. J. Phillips, and A. Rosenfeld. Face"
        },
        {
          "[62] Q. Wang, J. Gao, W. Lin, and Y. Yuan.\nPixel-wise crowd": "ACM Computing Surveys\nrecognition: A literature survey."
        },
        {
          "[62] Q. Wang, J. Gao, W. Lin, and Y. Yuan.\nPixel-wise crowd": "(CSUR), 35(4):399–458, 2003. 1"
        },
        {
          "[62] Q. Wang, J. Gao, W. Lin, and Y. Yuan.\nPixel-wise crowd": "[70] Y. Zheng, H. Yang, T. Zhang, J. Bao, et al. General facial rep-"
        },
        {
          "[62] Q. Wang, J. Gao, W. Lin, and Y. Yuan.\nPixel-wise crowd": "resentation learning in a visual-linguistic manner.\nIn Proc."
        },
        {
          "[62] Q. Wang, J. Gao, W. Lin, and Y. Yuan.\nPixel-wise crowd": "IEEE/CVF Conf. Computer Vision and Pattern Recognition"
        },
        {
          "[62] Q. Wang, J. Gao, W. Lin, and Y. Yuan.\nPixel-wise crowd": "(CVPR), pages 18697–18709, 2022. 1, 3, 6, 7"
        }
      ],
      "page": 11
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "StyleFlow: Attribute-conditioned exploration of StyleGAN-generated images using conditional continuous normalizing flows",
      "authors": [
        "R Abdal",
        "P Zhu",
        "N Mitra",
        "P Wonka"
      ],
      "year": "2021",
      "venue": "ACM Transactions on Graphics (ToG)"
    },
    {
      "citation_id": "2",
      "title": "Edema estimation from facial images taken before and after dialysis via contrastive multi-patient pre-training",
      "authors": [
        "Y Akamatsu",
        "Y Onishi",
        "H Imaoka",
        "J Kameyama"
      ],
      "year": "2008",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "3",
      "title": "Only a matter of style: Age transformation using a style-based regression model",
      "authors": [
        "Y Alaluf",
        "O Patashnik",
        "D Cohen-Or"
      ],
      "year": "2021",
      "venue": "ACM Transactions on Graphics (TOG)"
    },
    {
      "citation_id": "4",
      "title": "Third time's the charm? image and video editing with StyleGAN3",
      "authors": [
        "Y Alaluf",
        "O Patashnik",
        "Z Wu",
        "A Zamir"
      ],
      "year": "2022",
      "venue": "Proc. European Conf. Computer Vision Workshops"
    },
    {
      "citation_id": "5",
      "title": "Curriculum learning",
      "authors": [
        "Y Bengio",
        "J Louradour",
        "R Collobert",
        "J Weston"
      ],
      "year": "2009",
      "venue": "Proc. Int. Conf. Machine Learning (ICML)"
    },
    {
      "citation_id": "6",
      "title": "Pre-training strategies and datasets for facial representation learning",
      "authors": [
        "A Bulat",
        "S Cheng",
        "J Yang",
        "A Garbett"
      ],
      "year": "2007",
      "venue": "Proc. European Conf. Computer Vision (ECCV)"
    },
    {
      "citation_id": "7",
      "title": "Vggface2: A dataset for recognising faces across pose and age",
      "authors": [
        "Q Cao",
        "L Shen",
        "W Xie",
        "O Parkhi"
      ],
      "year": "2007",
      "venue": "Proc. Int. Conf. Automatic Face & Gesture Recognition (FG)"
    },
    {
      "citation_id": "8",
      "title": "Unsupervised learning of visual features by contrasting cluster assignments",
      "authors": [
        "M Caron",
        "I Misra",
        "J Mairal",
        "P Goyal"
      ],
      "year": "2020",
      "venue": "Proc. Advances in Neural Information Processing Systems (NeurIPS)"
    },
    {
      "citation_id": "9",
      "title": "A simple framework for contrastive learning of visual representations",
      "authors": [
        "T Chen",
        "S Kornblith",
        "M Norouzi",
        "G Hinton"
      ],
      "year": "2007",
      "venue": "Proc. Int. Conf. Machine Learning (ICML)"
    },
    {
      "citation_id": "10",
      "title": "Improved baselines with momentum contrastive learning",
      "authors": [
        "X Chen",
        "H Fan",
        "R Girshick",
        "K He"
      ],
      "year": "2020",
      "venue": "Improved baselines with momentum contrastive learning",
      "arxiv": "arXiv:2003.04297"
    },
    {
      "citation_id": "11",
      "title": "Editing in style: Uncovering the local semantics of gans",
      "authors": [
        "E Collins",
        "R Bala",
        "B Price",
        "S Susstrunk"
      ],
      "year": "2020",
      "venue": "Proc. IEEE/CVF Conf. Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "12",
      "title": "Show me your face and I will tell you your height, weight and body mass index",
      "authors": [
        "A Dantcheva",
        "F Bremond",
        "P Bilinski"
      ],
      "year": "2018",
      "venue": "Proc. Int. Conf. Pattern Recognition (ICPR)"
    },
    {
      "citation_id": "13",
      "title": "Imagenet: A largescale hierarchical image database",
      "authors": [
        "J Deng",
        "W Dong",
        "R Socher",
        "L Li"
      ],
      "year": "2007",
      "venue": "Proc. IEEE/CVF Conf. Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "14",
      "title": "Disentangled and controllable face image generation via 3d imitativecontrastive learning",
      "authors": [
        "Y Deng",
        "J Yang",
        "D Chen",
        "F Wen"
      ],
      "year": "2020",
      "venue": "Proc. IEEE/CVF Conf. Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "15",
      "title": "PML: Progressive margin loss for long-tailed age classification",
      "authors": [
        "Z Deng",
        "H Liu",
        "Y Wang",
        "C Wang"
      ],
      "year": "2021",
      "venue": "Proc. IEEE/CVF Conf. Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "16",
      "title": "Pros: Facial omnirepresentation learning via prototype-based self-distillation",
      "authors": [
        "X Di",
        "Y Zheng",
        "X Liu",
        "Y Cheng"
      ],
      "venue": "Pros: Facial omnirepresentation learning via prototype-based self-distillation"
    },
    {
      "citation_id": "17",
      "title": "Proc. IEEE/CVF Winter Conf. Applications of Computer Vision (WACV)",
      "year": "2024",
      "venue": "Proc. IEEE/CVF Winter Conf. Applications of Computer Vision (WACV)"
    },
    {
      "citation_id": "18",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn"
      ],
      "venue": "Proc. Int. Conf. Learning Representations"
    },
    {
      "citation_id": "19",
      "title": "Printsgan: Synthetic fingerprint generator",
      "authors": [
        "J Engelsma",
        "S Grosz",
        "A Jain"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "20",
      "title": "Impact of lifestyle on health",
      "authors": [
        "D Farhud"
      ],
      "year": "2015",
      "venue": "Iranian Journal of Public Health"
    },
    {
      "citation_id": "21",
      "title": "Generative adversarial nets",
      "authors": [
        "I Goodfellow",
        "J Pouget-Abadie",
        "M Mirza",
        "B Xu"
      ],
      "year": "2014",
      "venue": "Proc. Advances in Neural Information Processing Systems (NeurIPS)"
    },
    {
      "citation_id": "22",
      "title": "Challenges in representation learning: A report on three machine learning contests",
      "authors": [
        "I Goodfellow",
        "D Erhan",
        "P Carrier",
        "A Courville"
      ],
      "year": "2013",
      "venue": "Proc. Int. Conf. Neural Information Processing (ICONIP)"
    },
    {
      "citation_id": "23",
      "title": "Momentum contrast for unsupervised visual representation learning",
      "authors": [
        "K He",
        "H Fan",
        "Y Wu",
        "S Xie"
      ],
      "year": "2007",
      "venue": "Proc. IEEE/CVF Conf. Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "24",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proc. IEEE/CVF Conf. Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "25",
      "title": "Health and illness over time: The trajectory perspective in nursing science",
      "authors": [
        "S Henly",
        "J Wyman",
        "M Findorff"
      ],
      "year": "2011",
      "venue": "Nursing Research"
    },
    {
      "citation_id": "26",
      "title": "Impact of variations in synthetic training data on fingerprint classification",
      "authors": [
        "P Irtem",
        "E Irtem",
        "N Erdogmus"
      ],
      "year": "2019",
      "venue": "Proc. Int. Conf. Biometrics Special Interest Group (BIOSIG)"
    },
    {
      "citation_id": "27",
      "title": "Synthetic data in human analysis: A survey",
      "authors": [
        "I Joshi",
        "M Grimmer",
        "C Rathgeb",
        "C Busch"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "28",
      "title": "Training generative adversarial networks with limited data",
      "authors": [
        "T Karras",
        "M Aittala",
        "J Hellsten",
        "S Laine"
      ],
      "year": "2020",
      "venue": "Proc. Advances in Neural Information Processing Systems (NeurIPS)"
    },
    {
      "citation_id": "29",
      "title": "Alias-free generative adversarial networks",
      "authors": [
        "T Karras",
        "M Aittala",
        "S Laine",
        "E Härkönen"
      ],
      "year": "2021",
      "venue": "Proc. Advances in Neural Information Processing Systems (NeurIPS)"
    },
    {
      "citation_id": "30",
      "title": "A style-based generator architecture for generative adversarial networks",
      "authors": [
        "T Karras",
        "S Laine",
        "T Aila"
      ],
      "year": "2019",
      "venue": "Proc. IEEE/CVF Conf. Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "31",
      "title": "Analyzing and improving the image quality of StyleGAN",
      "authors": [
        "T Karras",
        "S Laine",
        "M Aittala",
        "J Hellsten"
      ],
      "year": "2020",
      "venue": "Proc. IEEE/CVF Conf. Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "32",
      "title": "HI-INT: Historical, intra-and inter-personal dynamics modeling with cross-person memory transformer",
      "authors": [
        "Y Kim",
        "D Lee",
        "P Liang",
        "S Alghowinem"
      ],
      "year": "2023",
      "venue": "Proc. Int. Conf. Multimodal Interaction"
    },
    {
      "citation_id": "33",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "34",
      "title": "Convolutional neural networks pretrained on large face recognition datasets for emotion classification from video",
      "authors": [
        "B Knyazev",
        "R Shvetsov",
        "N Efremova",
        "A Kuharenko"
      ],
      "year": "2017",
      "venue": "Convolutional neural networks pretrained on large face recognition datasets for emotion classification from video",
      "arxiv": "arXiv:1711.04598"
    },
    {
      "citation_id": "35",
      "title": "Siamese-structure deep neural network recognizing changes in facial expression according to the degree of smiling",
      "authors": [
        "K Kondo",
        "T Nakamura",
        "Y Nakamura",
        "S Satoh"
      ],
      "year": "2021",
      "venue": "Proc. Int. Conf. Pattern Recognition (ICPR)"
    },
    {
      "citation_id": "36",
      "title": "Analyzing and reducing the damage of dataset bias to face recognition with synthetic data",
      "authors": [
        "A Kortylewski",
        "B Egger",
        "A Schneider",
        "T Gerig"
      ],
      "year": "2019",
      "venue": "Proc. IEEE/CVF Conf. Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "37",
      "title": "Training deep face recognition systems with synthetic data",
      "authors": [
        "A Kortylewski",
        "A Schneider",
        "T Gerig",
        "B Egger"
      ],
      "year": "2018",
      "venue": "Training deep face recognition systems with synthetic data",
      "arxiv": "arXiv:1802.05891"
    },
    {
      "citation_id": "38",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "39",
      "title": "Feasibility of using deep learning to detect coronary artery disease based on facial photo",
      "authors": [
        "S Lin",
        "Z Li",
        "B Fu",
        "S Chen"
      ],
      "year": "2020",
      "venue": "European Heart Journal"
    },
    {
      "citation_id": "40",
      "title": "EditGAN: Highprecision semantic image editing",
      "authors": [
        "H Ling",
        "K Kreis",
        "D Li",
        "S Kim"
      ],
      "year": "2021",
      "venue": "Proc. Advances in Neural Information Processing Systems (NeurIPS)"
    },
    {
      "citation_id": "41",
      "title": "Pose-disentangled contrastive learning for self-supervised facial representation",
      "authors": [
        "Y Liu",
        "W Wang",
        "Y Zhan",
        "S Feng"
      ],
      "year": "2007",
      "venue": "Proc. IEEE/CVF Conf. Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "42",
      "title": "Deep learning face attributes in the wild",
      "authors": [
        "Z Liu",
        "P Luo",
        "X Wang",
        "X Tang"
      ],
      "year": "2005",
      "venue": "Proc. IEEE/CVF Int. Conf. Computer Vision (ICCV)"
    },
    {
      "citation_id": "43",
      "title": "Automatic detection of non-posed facial action units",
      "authors": [
        "S Mavadati",
        "M Mahoor",
        "K Bartlett",
        "P Trinh"
      ],
      "year": "2012",
      "venue": "Proc. Int. Conf. Image Processing (ICIP)"
    },
    {
      "citation_id": "44",
      "title": "Disfa: A spontaneous facial action intensity database",
      "authors": [
        "S Mavadati",
        "M Mahoor",
        "K Bartlett",
        "P Trinh"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "45",
      "title": "Eigen-CAM: Class activation map using principal components",
      "authors": [
        "M Muhammad",
        "M Yeasin"
      ],
      "year": "2020",
      "venue": "Proc. Int. Joint Conf. Neural Networks (IJCNN)"
    },
    {
      "citation_id": "46",
      "title": "Deep learning for emotion recognition on small datasets using transfer learning",
      "authors": [
        "H Ng",
        "V Nguyen",
        "V Vonikakis",
        "S Winkler"
      ],
      "year": "2015",
      "venue": "Proc. ACM on Int. Conf. Multimodal Interaction"
    },
    {
      "citation_id": "47",
      "title": "Overview of research on facial ageing using the FG-NET ageing database",
      "authors": [
        "G Panis",
        "A Lanitis",
        "N Tsapatsoulis",
        "T Cootes"
      ],
      "year": "2016",
      "venue": "IET Biometrics"
    },
    {
      "citation_id": "48",
      "title": "Recognizing multi-modal face spoofing with face recognition networks",
      "authors": [
        "A Parkin",
        "O Grinchuk"
      ],
      "year": "2019",
      "venue": "Proc. IEEE/CVF Conf. Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "49",
      "title": "Text-driven manipulation of StyleGAN imagery",
      "authors": [
        "O Patashnik",
        "Z Wu",
        "E Shechtman",
        "D Cohen-Or"
      ],
      "year": "2021",
      "venue": "Proc. IEEE/CVF Int. Conf. Computer Vision (ICCV)"
    },
    {
      "citation_id": "50",
      "title": "Transforming facial weight of real images by editing latent space of StyleGAN",
      "authors": [
        "V Pinnimty",
        "M Zhao",
        "P Achananuparp",
        "E Lim"
      ],
      "year": "2020",
      "venue": "Transforming facial weight of real images by editing latent space of StyleGAN",
      "arxiv": "arXiv:2011.02606"
    },
    {
      "citation_id": "51",
      "title": "SynFace: Face recognition with synthetic data",
      "authors": [
        "H Qiu",
        "B Yu",
        "D Gong",
        "Z Li"
      ],
      "year": "2021",
      "venue": "Proc. IEEE/CVF Int. Conf. Computer Vision (ICCV)"
    },
    {
      "citation_id": "52",
      "title": "An all-in-one convolutional neural network for face analysis",
      "authors": [
        "R Ranjan",
        "S Sankaranarayanan",
        "C Castillo",
        "R Chellappa"
      ],
      "year": "2017",
      "venue": "Proc. Int. Conf. Automatic Face & Gesture Recognition (FG)"
    },
    {
      "citation_id": "53",
      "title": "Interpreting the latent space of GANs for semantic face editing",
      "authors": [
        "Y Shen",
        "J Gu",
        "X Tang",
        "B Zhou"
      ],
      "year": "2020",
      "venue": "Proc. IEEE/CVF Conf. Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "54",
      "title": "Face To BMI: A deep learning based approach for computing bmi from face",
      "authors": [
        "J Sidhpura",
        "R Veerkhare",
        "P Shah",
        "S Dholay"
      ],
      "year": "2022",
      "venue": "Proc. Int. Conf. Innovative Trends in Information Technology (ICITIIT)"
    },
    {
      "citation_id": "55",
      "title": "Emotion recognition with sequential multi-task learning technique",
      "authors": [
        "P Thinh",
        "H Hung",
        "H Yang",
        "S Kim"
      ],
      "year": "2021",
      "venue": "Proc. IEEE/CVF Int. Conf. Computer Vision Workshops"
    },
    {
      "citation_id": "56",
      "title": "Learning vision from models rivals learning vision from data",
      "authors": [
        "Y Tian",
        "L Fan",
        "K Chen",
        "D Katabi"
      ],
      "year": "2023",
      "venue": "Learning vision from models rivals learning vision from data",
      "arxiv": "arXiv:2312.17742"
    },
    {
      "citation_id": "57",
      "title": "Sta-bleRep: Synthetic images from text-to-image models make strong visual representation learners",
      "authors": [
        "Y Tian",
        "L Fan",
        "P Isola",
        "H Chang",
        "D Krishnan"
      ],
      "year": "2023",
      "venue": "Proc. Advances in Neural Information Processing Systems (NeurIPS)"
    },
    {
      "citation_id": "58",
      "title": "Emotions in everyday life",
      "authors": [
        "D Trampe",
        "J Quoidbach",
        "M Taquet"
      ],
      "year": "2015",
      "venue": "PloS one"
    },
    {
      "citation_id": "59",
      "title": "Generating photorealistic training data to improve face recognition accuracy",
      "authors": [
        "D Trigueros",
        "L Meng",
        "M Hartnett"
      ],
      "year": "2021",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "60",
      "title": "Representation learning with contrastive predictive coding",
      "authors": [
        "A Van Den Oord",
        "Y Li",
        "O Vinyals"
      ],
      "year": "2018",
      "venue": "Representation learning with contrastive predictive coding",
      "arxiv": "arXiv:1807.03748"
    },
    {
      "citation_id": "61",
      "title": "Deep face recognition: A survey",
      "authors": [
        "M Wang",
        "W Deng"
      ],
      "year": "2021",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "62",
      "title": "Learning from synthetic data for crowd counting in the wild",
      "authors": [
        "Q Wang",
        "J Gao",
        "W Lin",
        "Y Yuan"
      ],
      "year": "2019",
      "venue": "Proc. IEEE/CVF Conf. Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "63",
      "title": "Pixel-wise crowd understanding via synthetic data",
      "authors": [
        "Q Wang",
        "J Gao",
        "W Lin",
        "Y Yuan"
      ],
      "year": "2021",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "64",
      "title": "AMII: Adaptive multimodal inter-personal and intra-personal model for adapted behavior synthesis",
      "authors": [
        "J Woo",
        "M Fares",
        "C Pelachaud",
        "C Achard"
      ],
      "year": "2023",
      "venue": "AMII: Adaptive multimodal inter-personal and intra-personal model for adapted behavior synthesis",
      "arxiv": "arXiv:2305.11310"
    },
    {
      "citation_id": "65",
      "title": "Unsupervised facial expression representation learning with contrastive local warping",
      "authors": [
        "F Xue",
        "Y Sun",
        "Y Yang"
      ],
      "year": "2023",
      "venue": "Unsupervised facial expression representation learning with contrastive local warping",
      "arxiv": "arXiv:2303.09034"
    },
    {
      "citation_id": "66",
      "title": "Barlow Twins: Self-supervised learning via redundancy reduction",
      "authors": [
        "J Zbontar",
        "L Jing",
        "I Misra",
        "Y Lecun"
      ],
      "year": "2021",
      "venue": "Proc. Int. Conf. Machine Learning (ICML)"
    },
    {
      "citation_id": "67",
      "title": "Demodalizing face recognition with synthetic samples",
      "authors": [
        "Z Zhai",
        "P Yang",
        "X Zhang",
        "M Huang"
      ],
      "year": "2021",
      "venue": "Proc. AAAI Conf. Artificial Intelligence"
    },
    {
      "citation_id": "68",
      "title": "C3AE: Exploring the limits of compact model for age estimation",
      "authors": [
        "C Zhang",
        "S Liu",
        "X Xu",
        "C Zhu"
      ],
      "year": "2019",
      "venue": "Proc. IEEE/CVF Conf. Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "69",
      "title": "Intra-and inter-individual differences in adolescent depressive mood: The role of relationships with parents and friends",
      "authors": [
        "S Zhang",
        "L Baams",
        "D Van De Bongardt",
        "J Dubas"
      ],
      "year": "2018",
      "venue": "Journal of Abnormal Child Psychology"
    },
    {
      "citation_id": "70",
      "title": "Face recognition: A literature survey",
      "authors": [
        "W Zhao",
        "R Chellappa",
        "P Phillips",
        "A Rosenfeld"
      ],
      "year": "2003",
      "venue": "ACM Computing Surveys (CSUR)"
    },
    {
      "citation_id": "71",
      "title": "General facial representation learning in a visual-linguistic manner",
      "authors": [
        "Y Zheng",
        "H Yang",
        "T Zhang",
        "J Bao"
      ],
      "year": "2007",
      "venue": "Proc. IEEE/CVF Conf. Computer Vision and Pattern Recognition (CVPR)"
    }
  ]
}