{
  "paper_id": "2212.03102v1",
  "title": "A Comparative Study Of Emotion Recognition Methods Using Facial Expressions",
  "published": "2022-12-05T10:34:35Z",
  "authors": [
    "Rim EL Cheikh",
    "Hélène Tran",
    "Issam Falih",
    "Engelbert Mephu Nguifo"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Understanding the facial expressions of our interlocutor is important to enrich the communication and to give it a depth that goes beyond the explicitly expressed. In fact, studying one's facial expression gives insight into their hidden emotion state. However, even as humans, and despite our empathy and familiarity with the human emotional experience, we are only able to guess what the other might be feeling. In the fields of artificial intelligence and computer vision, Facial Emotion Recognition (FER) is a topic that is still in full growth mostly with the advancement of deep learning approaches and the improvement of data collection. The main purpose of this paper is to compare the performance of three state-of-the-art networks, each having their own approach to improve on FER tasks, on three FER datasets. The first and second sections respectively describe the three datasets and the three studied network architectures designed for an FER task. The experimental protocol, the results and their interpretation are outlined in the remaining sections.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Automatic Emotion Recognition is an area of research that has been active for many decades. Researchers are not only trying to mimic human logical-mathematical intelligence but have been for many years now trying to explore emotional functions. However, despite many impressive results in terms of predictive ability, automatic methods still have a long way to go when it comes to offering a satisfying model that can capture the complexity, heterogeneity, and subjectivity of the emotional experience.\n\nThe interest in this topic stems from different concerns. FER has applications in various fields. For instance, given that understanding the emotional state of our interlocutor is essential for a deep and rich communication, recognizing emotions from facial expressions could be seen as a wish to further develop the human-machine interaction. Also, having an automatic FER that offers reliable results on complex emotional states could be a breakthrough in the fields of healthcare  (Kashif et al., 2021)  and psychology  (Tadalagi and Joshi, 2021) .\n\nA key element for building a robust machine learning model is the availability and quality of data. In the context of emotion recognition using images of facial expressions, collecting a big amount of data is a manageable task as proven by the multitude of available FER datasets. The samples in these datasets are collected from different sources. For example, images of faces could be collected from the internet and can be considered as expressions captured \"inthe-wild\". The samples could be pre-processed to only keep the facial region  (Barsoum et al., 2016; Mollahosseini et al., 2017)  or we can choose to also incorporate and analyze the context in which the image is captured  (Kosti et al., 2017) . Other datasets contain images taken in the lab, where subjects are asked to act the facial expressions in a controlled environment  (Lyons et al., 1998; Lucey et al., 2010) . Other differences between datasets could come from the existence of noisy data, the lighting, the encoding of the samples, the diversity of the faces in terms of age, gender, race, ethnicity, etc. It is also important to insist of the subjectivity in expressing and identifying emotions, which introduces bias and ambiguity  (Tran et al., 2022)  in the annotation step, as well as during the interpretation of the results step. In fact, one way to try to build a more accurate and precise emotion recognition model is to combine different types of data that could be informative on the emotional state of a person, such as facial expression, speech, tone, body posture, physiological signal, etc. In this situation, processing and combining data from different sources creates an additional challenge to the design of a robust model.\n\nAs a first step to building such systems, we choose to focus this work on methods that predict emotions from facial expression images only. Three state-of-the-art models for FER tasks have been selected for experiments. These models diverge not only in their architectures but also in the level on which they intervene to better the prediction quality. Our experiments provide a fair comparison of their performance on three datasets that differ in terms of their size, the setting in which the samples are captured, and the distribution of classes. The paper starts by giving an overview of three existing facial expression datasets used for emotion prediction and that were chosen for our experiments: FER+, AffectNet and CK+. Then, three neural network architectures are described in section 2, followed by a presentation of the experimental protocol in section 3. In sections 4 and 5, results are respectively presented and analyzed. The code used for the experiments is publicly available at https://anonymous.4open.science/r/FE-rec-0F0B/.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Dataset Description",
      "text": "This section describes the three datasets, FER+, AffectNet and CK+, used for our experiments.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Fer+",
      "text": "FER+, introduced by  Barsoum et al. (2016) , is one of the most known and used datasets for FER tasks. FER+ is annotated with 8 emotions: Neutral, Happiness, Surprise, Sadness, Anger, Disgust, Fear, and Contempt, as well as Unknown and NF (Not a Face). The images are collected from the internet and contain only the facial region. They are gray-scale and are of size 48x48 pixels, which could be considered as very low resolution. To avoid affecting the quality of predictions, we resized the images to 96x96 pixels.\n\nFER+ can be considered as an improvement of the FER-2013 dataset, which was introduced by  Goodfellow et al. (2013) . While the labeling for FER-2013 was done by two persons only (the authors of the dataset), FER+ took advantage of the increasingly popular scheme of crowdsourcing in order to collect ground truth labels and the images of FER-2013 were relabeled by 10 crowdsourced taggers. Each image has an emotion probability distribution by using the annotations given by the 10 taggers. The FER+ dataset is therefore a multi-labeled set of images that better reflects the ambiguity and diversity of facial emotions. In our single-label classification experiments, the label affected to a sample is the one with the highest number of votes. If the label with the highest number of votes is Unknown or NF, we choose to exclude the sample from the experiments.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Affectnet",
      "text": "The AffectNet dataset  (Mollahosseini et al., 2017)  provides 291,651 images of faces annotated with 8 categories of emotions: Neutral, Happy, Sad, Surprise, Fear, Disgust, Anger and Contempt. A larger version of AffectNet provides annotations such as None, Uncertain, and No-Face that were not taken into account in the implementation but are important to ensure the quality of the annotated dataset. The images present in AffectNet were collected from the internet by the means of queries such as \"joyful girl\", \"astonished senior\", etc. Only the face region was kept and 12 annotators were asked to annotate the samples. The authors provide the agreement percentages between two annotators over the annotations. It is interesting to note that all classes resulted in agreements ranging from 50% to 70%, except Happy with 79.6%. These percentages are similar to the accuracy reportedly found by recognition models in the FER literature that are trained on this dataset  (Siqueira et al., 2020; Farzaneh and Qi, 2021) .\n\nThe images are in RGB and of size 224x224 pixels. In addition to the discrete emotion categories, AffectNet gives a continuous annotation of the faces in a two dimensional space, valence-arousal. The valence dimension is an indicator for the pleasantness of the emotion and arousal is a measure of the emotion intensity. These annotations were not used for our experiments, but provide precious information that enables us to extend our research to emotion recognition using continuous modeling.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Ck+",
      "text": "The Extended Cohn-Kanade, or CK+, dataset  (Lucey et al., 2010) , contains 593 sequences of facial expressions captured from 123 subjects in a lab. A sequence starts with a neutral expression and ends with the peak expression where the facial Action Units (AUs) are coded. Action Units were proposed by  Friesen and Ekman (1978)  as a way to model facial expressions by encoding the movements of facial muscles. From these 593 sequences, only 327 are labeled with a category of emotions: Anger, Contempt, Disgust, Fear, Happy, Sadness and Surprise. The unlabeled sequences are considered as non fitting for the prototypical definition of the emotions taken into account and are not used for supervised training. The labeling is done by assigning an emotion to the facial expression if one or more AUs are detected, with respect to the Facial Action Coding System manual  (Friesen and Ekman, 1978) .\n\nGiven the small number of available sequences, we chose to take three images from each sequence instead of only the peak expression. This way, the number of samples is increased for each class of emotions. Moreover, the neutral class is created by taking the first frame of each sequence, so that the set of emotions used for CK+ is the same as the other two datasets. The images are sized either 640x490 or 640x480 pixels. Some were gray-scale while others were RGB. In our experiments, the images were fed to the networks as gray-scale 640x490 pixel arrays.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Studied Networks",
      "text": "This section describes the methods chosen for experiments in order to compare their performance on the previously presented datasets. The choice was based on the fact that each method provides a different approach in terms of improving on facial emotion recognition tasks. Availability of implementations of these models was also an important factor in choosing them.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Esr",
      "text": "In the paper titled \"Efficient Facial Feature Learning with Wide Ensemble-based Convolutional Neural Networks\", by  Siqueira et al. (2020)  introduced the Ensembles with Shared Representations (ESR) network. Ensemble learning combines the predictions obtained by different classifiers that are usually trained in an individual and independent manner, hence producing a more accurate output than a single model would. The ESR network makes use of the translation-invariance property of the patterns learned by each convolutional layer. In fact, the patterns that are learned in the early layers of the network (low-level features) can be considered in some way as common to all the images that the network might encounter. These patterns can be oriented lines, edges, or colors. As we go deeper into the layers, the patterns that the network is learning become more complex and specific to each image. These features, in the case of facial expressions, could be the shape of the eyes, of the mouth, and of the nose. Therefore, in ESR we find two main building pieces:\n\n1. The base of the network: A line-up of convolutional layers that are responsible for learning the low-level features. As mentioned before, patterns learned at this level of the network are very general and therefore can be shared by multiple branches. This base uses a transfer learning mechanism that speeds up the learning process as the ensemble grows, while improving the performance, since the best configuration from the training of each branch is reloaded as base.\n\n2. The independent convolutional branches: A branching of the convolutional layers allows each branch to learn its own individual high-level features.\n\nFinally, the optimization of the network consists in minimizing a loss function that combines the loss function of each branch.\n\nIn the case of in-the-lab databases, the ESR network contains only 4 independent convolutional branches, as input data is expected to be of good quality (good lighting, adjusted head pose, etc.). By contrast, when dealing with in-the-wild datasets, the number of branches is increased to 9.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Scn",
      "text": "In the Self-Cure Network  (Wang et al., 2020) , the authors addressed the problem of uncertainty that comes with the labeling of the facial expression datasets. One of the reasons behind is the problem of subjectiveness when categorizing the human emotional experience. Furthermore, in datasets where images are captured in-the-wild, the uncontrolled setting is definitely a source of inconsistency and uncertainty. To overcome this problem, the SCN network proposes a relabeling step before the recognition task to perform robust feature learning with uncertainty. First, the features are extracted from the images using a \"backbone\" convolutional network, which can be any traditional CNNs. In the implementation provided by the authors and as well as in our experiments, the backbone used is ResNet-18  (He et al., 2016)  pre-trained on the MS-Celeb-1M face recognition dataset  (Guo et al., 2016) . Second, if a sample is considered uncertain, it is relabeled. The relabeling can be summarized in three steps:\n\n1. Self-attention importance weighting: Importance weights are computed for each sample using a linear fully-connected layer with a sigmoid activation function. The assigned importance weight reflects the contribution that each image has on the classifier training.\n\n2. Ranking regularization: The weights are regularized in order to reduce the importance given to uncertain samples. The samples are ranked by importance and two groups of samples are created: the low-importance (30%) and the high-importance images (70%).\n\n3. Relabeling: This module assigns a new emotion label (the one with highest predicted probability produced by the model) to the images from the low-importance group. For that purpose, softmax probabilities are used to determine which image is actually incorrectly labeled. If the difference between the predicted probability for the label initially given to the sample, and the highest predicted probability produced by the model, is higher than a given threshold (set to 0.2 by the authors), then the data is relabeled according to the highest probability found by the model.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Dacl",
      "text": "DACL, or Deep Attentive Center Loss  (Farzaneh and Qi, 2021) , is a facial expression recognition method that uses an attention mechanism to estimate attention weights that are correlated with feature importance. In fact, in the training phase, learning irrelevant features is harmful for the performance of the network. Therefore, the authors proposed the integration of a Deep Metric Learning (DML) approach that enhances the learning of discriminative features by the model.\n\nThe first step is to feed a convolutional neural network with the input images in order to generate the feature maps, followed by the DACL component composed of two building pieces:\n\n1. Context Encoder Unit: This generates latent representations for each spatial feature map that is outputted by the backbone CNN. All these feature maps represent the context, and therefore the obtained latent feature vector is reduced in dimension and is devoid of the noise, only containing relevant information from the initial features extracted by the CNN. The linear layer weights were initialized according to the Kaiming initialization  (He et al., 2015) , which is appropriate when using the ReLU activation function, as it helps to capture non-linear relationships between layers. Finally, these attention weights are used to compute the sparse center loss, which in turn is fractionally used, alongside the loss computed by the CNN backbone to compute the final loss. The authors empirically gave 1% of the final loss to the sparse center loss.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Multi",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experimental Setting",
      "text": "In order to fairly compare the performance of the three networks mentioned above on the FER+, AffectNet and CK+ datasets, we followed an identical experimental protocol for training and testing them. A 5-fold cross-validation is used to evaluate the model. In fact, the three datasets are constructed in different ways: FER+ has training, validation and testing subsets, AffectNet has training and validation subsets, and CK+ has no subsets. Therefore, in the case of FER+ and AffectNet the subsets are mixed before performing cross-validation. The dataset is split as follows: 80% for training, 10% for validation, and 10% for test. Table  1  shows the distribution of the eight emotions for the three datasets. The training is performed over 60 epochs, with Adam optimizer and a 0.001 learning rate. TAB. 1: Class distribution in the three studied dataset.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "This subsection presents the metrics used for the model evaluation, we define TP as the value of true positives and FP is the false positives.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Accuracy And Balanced Accuracy",
      "text": "The overall accuracy is reported in our results. However, it does not take into account the highly imbalanced label distribution found in the three datasets. For this reason, we report the balanced accuracy defined as the average of recall (see further in this section) obtained on each class. This definition proposed by  Mosley (2013)  is equivalent to the most commonly used formula for accuracy where each sample is weighted by the prevalence of its true label. The chosen formula computes an aggregated score of the measurements of the predictive quality for each class independently.\n\nPrecision and Recall For our experiments, we compute precision score, which measures the predictor's ability not to label a negative sample as positive, weighted by the support of the labels. Similarly, the weighted recall score is computed by averaging the recall score for each label after weighting it by the support of the label.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "F1-Score",
      "text": "We also report the F1-score as the harmonic mean of the weighted precision and the weighted recall.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Auc Roc",
      "text": "The AUC ROC score reflects the discrimination ability of the classifier between the different classes. In our experiments, we report the prevalence-weighted average of AUC ROC scores computed for each class against all the others.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Results",
      "text": "In this section we present the results of the five-fold cross-validation. First, we evaluate and compare the models on the whole dataset in terms of predictive performance. Second, the models are compared by taking into account their capability to discriminate each emotion class.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Overall Classification",
      "text": "Table  2  reports the computed metrics on the test subsets averaged over the 5 folds. This fairly quantifies the predictive performance of the models while taking into account the imbalance in the datasets. When trained on FER+, DACL shows a better accuracy than ESR and SCN. Weighted accuracy is decreased compared to overall accuracy, which is expected given the imbalance in the classes distribution in FER+. Also, DACL shows a better precision and recall compared to the other models, as well as a higher ability to discriminate between labels (AUC ROC). For AffectNet, DACL also shows the best performance in terms of recall, precision and AUC ROC scores. However, the best balanced accuracy is achieved by ESR while staying relatively close to the balanced accuracy of DACL. Regarding CK+, ESR shows the best performance with a really high accuracy of 91.5% and a very satisfying balanced accuracy compared to the other models. The F1-score is also the highest for ESR on this dataset. However, SCN produces the best AUC ROC score which shows that it is the best model in terms of discrimination between the categories.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Emotion-Specific Classification",
      "text": "In order to have a closer look on the performance of each model for each of the 8 emotion categories, normalized confusion matrices are shown in figure  1 . When training on FER+, we find a higher TP value for the classes that are over-represented. For example, a high percentage of images that are labeled \"Happy\" in the dataset were correctly classified. However, not more than 1.2% of \"Contempt\" images were predicted as such by the three models. We can clearly see that for FER+ the imbalance in the training sets impacts the learning process by the models. Also, \"Contempt\" expressions are more often than not misclassified as \"Neutral\" by all three of the models. When training on AffectNet, we can see that \"Neutral\" and \"Happy\" have a high TAB. 2: Average performance metrics computed on the test subsets afetr the five-fold crossvalidation. The value after (±) symbol represents the standard deviation across five folds. \"acc\" is accuracy, \"bal acc\" is balanced accuracy, \"pr\" is weighted precision, \"rec\" is weighted recall, \"F1\" is the F1-score, \"AUC ROC\" is the Area under the ROC Curve.\n\nclassification accuracy for the three models. This is not the case for all other classes, which have a high probability of being misclassified as \"Neutral\" (and as \"Happy\" for \"Contempt\" samples). As the distribution classes in AffectNet shows, \"Happy\" has a higher frequency than \"Neutral\" in AffectNet, this was translated by a higher positive values for \"Happy\" than for \"Neutral\". Finally, when training on CK+, ESR predicts correct labels for over 80% of the samples for all classes except for \"Contempt\" which is accurately predicted in 77.1% of cases. However, this is not the case for SCN and DACL, where a high accuracy is only found for classes that are over-represented in CK+ (Happy, Neutral and Surprise). This said, all three models trained on CK+ still manage to provide a better accuracy on almost all underrepresented classes compared to the other two datasets.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Discussion",
      "text": "DACL provides the best scores when training on FER+ and AffectNet. Both these datasets contain facial expressions captured in-the-wild, where the subjects expressed their emotions in a natural way. This is why samples from FER+ and AffectNet might require a representation that takes into account the characteristics of the whole face region. It could be said that the Context Encoder unit of DACL is helping to represent the overall context from the face region of these samples and the multi-head attention mechanism allow the network to retain the most important information that are useful to infer the emotion. ESR gives the best accuracy and F1-score for CK+. Therefore, we can say that the exploitation of shared representations that is characteristic of ESR, proves effective when the samples are taken in a controlled environment and when the expressions are posed and intentional.\n\nIn figure  1 , it is noteworthy that training on FER+ induces a lot of mistakes by predicting \"neutral\" for samples that are labeled \"sad\" (using ESR, 59.4% of \"sad\" are correctly predicted and 31.9% are predicted as \"neutral\"). This is not the case for samples that are labeled \"surprise\" (using ESR, 87.4% of \"surprise\" are correctly classified), despite the fact that both classes, \"sad\" and \"surprise\" are represented in the dataset in very close proportions (3751 and 3941 samples respectively). This raises the question of what is it about \"sad\" expressions that are less discernible than \"surprise\" expressions. However, both ESR and DACL manage to correctly classify 31.9% of the \"disgust\" samples, which is an under-represented class. This is not the case for SCN. In fact, we can see that SCN's characteristic of dealing with uncertainty in the annotations of the training set does not translate well on the predictions done on a test set. As for training on AffectNet, the classification of a higher number of samples from other classes into \"neutral\" than into \"happy\" is proof that this misclassification comes from the ambiguity of emotions and not necessarily from the distribution in the dataset. We can say that \"neutral\" is a safe choice to categorize an emotion when we are somewhat uncertain. Similar to AffectNet, we find that when SCN and DACL are trained on CK+, a high number of misclassified samples are in fact predicted as \"neutral\". This shows that even a controlled manner of capturing emotions induces the mistake of safely labelling an emotion as \"neutral\".\n\nIn our experiments, we chose to compare the performance of three different neural networks, ESR, SCN and DACL. They all have different architectures and different approaches to deal with FER tasks. However, they cannot represent the whole class of methods to which they belong (attention-based, dealing with uncertainty, or ensemble learning). Other architectures for each method class  (Li et al., 2020; Hao et al., 2020)  could be studied in order to have a more comprehensive view of their performance in the context of FER applications. The same could be said about datasets, where we can extend our experiments to other datasets  (Lyons et al., 1998; Li et al., 2017 ) that share characteristics with the ones we discussed in this paper.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Conclusion",
      "text": "This paper compares the performance of three neural networks that have different approaches to tackle FER challenges: the first uses ensemble learning and transfer learning to learn facial emotion patterns, the second addresses the problem of subjectivity and uncertainty when labelling emotions, and the third makes use of an attention mechanism to learn from relevant features. To evaluate these models, we used three datasets that are different in terms of data collection and capture setting. FER+ and AffectNet are both in-the-wild datasets with facial expressions captured in an uncontrolled setting. The model that uses an attention mechanism provides the best results on images that are captured in the wild, which was expected as this type of images is very noisy and it would have been difficult to recognize the emotion if the model was not guided in focusing on the relevant parts of the images. On CK+, containing images with posed expressions, the model based on ensemble and transfer learning is the one that performs the best: the network is build to exploit low-level features, which is suitable in a setting where noisy parameters are mitigated (face in front of the camera with a neutral background). Moreover, our experiments show that models often mistake some emotion classes (e.g. \"contempt\" and \"neutral\") in in-the-wild datasets, showing that emotion ambiguity alters the model's discrimination abilities. In our comparative study, many challenges were identified, such as the under-represented emotion classes found in many FER datasets and the ambiguity of facial expressions. Also, extending the experiments to more models and datasets would provide a reliable benchmark to choose an adapted FER model in terms of the application at hand.",
      "page_start": 10,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Normalized confusion matrices, each computed by averaging the confusion matrices",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "FER+": "Train\nValid\nTest\nTotal",
          "AffectNet": "Train\nValid\nTest\nTotal",
          "CK+": "Train\nValid\nTest\nTotal"
        },
        {
          "FER+": "8796\n1148\n1052\n10996\n7231\n912\n896\n9039\n3001\n351\n399\n3751\n3153\n378\n410\n3941\n546\n67\n69\n682\n126\n15\n17\n158\n2125\n253\n278\n2656\n120\n13\n17\n150",
          "AffectNet": "3737\n464\n471\n4672\n6283\n756\n815\n7854\n1509\n192\n185\n1886\n1043\n147\n113\n1303\n691\n81\n92\n864\n587\n72\n75\n734\n1449\n188\n174\n1811\n563\n83\n58\n704",
          "CK+": "262\n28\n37\n327\n166\n16\n25\n207\n67\n9\n8\n84\n199\n31\n19\n249\n60\n6\n9\n75\n141\n23\n13\n177\n108\n14\n13\n135\n43\n4\n7\n54"
        },
        {
          "FER+": "25098\n3137\n3138\n31373",
          "AffectNet": "15862\n1983\n1983\n19828",
          "CK+": "1046\n131\n131\n1308"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "acc": "0.857\n± 0.013",
          "bal acc": "0.617\n± 0.029",
          "pr": "0.855\n± 0.015",
          "rec": "0.857\n± 0.013",
          "F1": "0.856\n± 0.014",
          "AUC ROC": "0.937\n± 0.023"
        },
        {
          "acc": "0.810\n± 0.012",
          "bal acc": "0.520\n± 0.028",
          "pr": "0.808\n± 0.010",
          "rec": "0.810\n± 0.012",
          "F1": "0.809\n± 0.011",
          "AUC ROC": "0.956\n± 0.002"
        },
        {
          "acc": "0.867\n± 0.005",
          "bal acc": "0.647\n± 0.018",
          "pr": "0.863\n± 0.005",
          "rec": "0.867\n± 0.005",
          "F1": "0.865\n± 0.005",
          "AUC ROC": "0.973\n± 0.002"
        },
        {
          "acc": "0.648\n± 0.002",
          "bal acc": "0.439\n± 0.001",
          "pr": "0.626\n± 0.006",
          "rec": "0.648\n± 0.002",
          "F1": "0.637\n± 0.004",
          "AUC ROC": "0.821\n± 0.002"
        },
        {
          "acc": "0.651\n± 0.002",
          "bal acc": "0.390\n± 0.011",
          "pr": "0.622\n± 0.005",
          "rec": "0.651\n± 0.002",
          "F1": "0.636\n± 0.003",
          "AUC ROC": "0.894\n± 0.004"
        },
        {
          "acc": "0.664\n± 0.016",
          "bal acc": "0.429\n± 0.033",
          "pr": "0.633\n± 0.017",
          "rec": "0.664\n± 0.016",
          "F1": "0.648\n± 0.017",
          "AUC ROC": "0.901\n± 0.007"
        },
        {
          "acc": "0.915\n± 0.018",
          "bal acc": "0.888\n± 0.034",
          "pr": "0.922\n± 0.014",
          "rec": "0.915\n± 0.018",
          "F1": "0.918\n± 0.016",
          "AUC ROC": "0.945\n± 0.008"
        },
        {
          "acc": "0.820\n± 0.030",
          "bal acc": "0.703\n± 0.072",
          "pr": "0.798\n± 0.043",
          "rec": "0.820\n± 0.030",
          "F1": "0.808\n± 0.032",
          "AUC ROC": "0.962\n± 0.010"
        },
        {
          "acc": "0.846\n± 0.125",
          "bal acc": "0.790\n± 0.175",
          "pr": "0.843\n± 0.154",
          "rec": "0.846\n± 0.125",
          "F1": "0.844\n± 0.140",
          "AUC ROC": "0.951\n± 0.051"
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Training deep networks for facial expression recognition with crowd-sourced label distribution",
      "authors": [
        "E Barsoum",
        "C Zhang",
        "C Ferrer",
        "Z Zhang"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "2",
      "title": "Facial expression recognition in the wild via deep attentive center loss",
      "authors": [
        "A Farzaneh",
        "X Qi"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF winter conference on applications of computer vision"
    },
    {
      "citation_id": "3",
      "title": "Facial action coding system: a technique for the measurement of facial movement",
      "authors": [
        "E Friesen",
        "P Ekman"
      ],
      "year": "1978",
      "venue": "Palo Alto"
    },
    {
      "citation_id": "4",
      "title": "Challenges in representation learning: A report on three machine learning contests",
      "authors": [
        "I Goodfellow",
        "D Erhan",
        "P Carrier",
        "A Courville",
        "M Mirza",
        "B Hamner",
        "W Cukierski",
        "Y Tang",
        "D Thaler",
        "D.-H Lee"
      ],
      "year": "2013",
      "venue": "International conference on neural information processing"
    },
    {
      "citation_id": "5",
      "title": "Ms-celeb-1m: A dataset and benchmark for large-scale face recognition",
      "authors": [
        "Y Guo",
        "L Zhang",
        "Y Hu",
        "X He",
        "J Gao"
      ],
      "year": "2016",
      "venue": "European conference on computer vision"
    },
    {
      "citation_id": "6",
      "title": "Visual-audio emotion recognition based on multi-task and ensemble learning with multiple features",
      "authors": [
        "M Hao",
        "W.-H Cao",
        "Z.-T Liu",
        "M Wu",
        "P Xiao"
      ],
      "year": "2020",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "7",
      "title": "Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE international conference on computer vision"
    },
    {
      "citation_id": "8",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "9",
      "title": "A machine learning approach for expression detection in healthcare monitoring systems",
      "authors": [
        "M Kashif",
        "A Hussain",
        "A Munir",
        "A Siddiqui",
        "A Abbasi",
        "M Aakif",
        "A Malik",
        "F Alazemi",
        "O.-Y Song"
      ],
      "year": "2021",
      "venue": "Computers, Materials & Continua"
    },
    {
      "citation_id": "10",
      "title": "Emotion recognition in context",
      "authors": [
        "R Kosti",
        "J Alvarez",
        "A Recasens",
        "A Lapedriza"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "11",
      "title": "Attention mechanism-based cnn for facial expression recognition",
      "authors": [
        "J Li",
        "K Jin",
        "D Zhou",
        "N Kubota",
        "Z Ju"
      ],
      "year": "2020",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "12",
      "title": "Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild",
      "authors": [
        "S Li",
        "W Deng",
        "J Du"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "13",
      "title": "The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression",
      "authors": [
        "P Lucey",
        "J Cohn",
        "T Kanade",
        "J Saragih",
        "Z Ambadar",
        "I Matthews"
      ],
      "year": "2010",
      "venue": "2010 ieee computer society conference on computer vision and pattern recognitionworkshops"
    },
    {
      "citation_id": "14",
      "title": "The Japanese Female Facial Expression",
      "authors": [
        "M Lyons",
        "M Kamachi",
        "J Gyoba"
      ],
      "year": "1998",
      "venue": "The Japanese Female Facial Expression"
    },
    {
      "citation_id": "15",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "16",
      "title": "A balanced approach to the multi-class imbalance problem",
      "authors": [
        "L Mosley"
      ],
      "year": "2013",
      "venue": "A balanced approach to the multi-class imbalance problem"
    },
    {
      "citation_id": "17",
      "title": "Efficient facial feature learning with wide ensemble-based convolutional neural networks",
      "authors": [
        "H Siqueira",
        "S Magg",
        "S Wermter"
      ],
      "year": "2020",
      "venue": "The Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI-20)"
    },
    {
      "citation_id": "18",
      "title": "Autodep: automatic depression detection using facial expressions based on linear binary pattern descriptor",
      "authors": [
        "M Tadalagi",
        "A Joshi"
      ],
      "year": "2021",
      "venue": "Medical & biological engineering & computing"
    },
    {
      "citation_id": "19",
      "title": "L'ambiguïté dans la représentation des émotions: état de l'art des bases de données multimodales",
      "authors": [
        "H Tran",
        "L Brelet",
        "I Falih",
        "X Goblet",
        "E Nguifo"
      ],
      "year": "2022",
      "venue": "Conférence Extraction et Gestion de Connaissances"
    },
    {
      "citation_id": "20",
      "title": "Suppressing uncertainties for largescale facial expression recognition",
      "authors": [
        "K Wang",
        "X Peng",
        "J Yang",
        "S Lu",
        "Y Qiao"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    }
  ]
}