{
  "paper_id": "2406.10598v1",
  "title": "Double Multi-Head Attention Multimodal System For Odyssey 2024 Speech Emotion Recognition Challenge",
  "published": "2024-06-15T11:11:06Z",
  "authors": [
    "Federico Costa",
    "Miquel India",
    "Javier Hernando"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "As computer-based applications are becoming more integrated into our daily lives, the importance of Speech Emotion Recognition (SER) has increased significantly. Promoting research with innovative approaches in SER, the Odyssey 2024 Speech Emotion Recognition Challenge was organized as part of the Odyssey 2024 Speaker and Language Recognition Workshop. In this paper we describe the Double Multi-Head Attention Multimodal System developed for this challenge. Pre-trained self-supervised models were used to extract informative acoustic and text features. An early fusion strategy was adopted, where a Multi-Head Attention layer transforms these mixed features into complementary contextualized representations. A second attention mechanism is then applied to pool these representations into an utterance-level vector. Our proposed system achieved the third position in the categorical task ranking with a 34.41% Macro-F1 score, where 31 teams participated in total.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Natural Human-Computer Interaction (HCI) is becoming more and more important as machines and computer-based applications are gaining significance in our daily lives  [1] . A key step in HCI is to recognize and address people's emotional states. In addition, automatic emotion detection and classification are essential for automated tutoring systems, call centers, gaming, personal assistants and more. For these reasons, progress in these technologies can enable transformative applications in diverse areas such as healthcare, security and defense, education and entertainment. Humans use various modalities to express their emotions, such as speech, text, facial expressions or gestures. Because of the subtle expressive behaviors that arise during human interactions, automatic emotion recognition from speech in realistic domains is still a challenging task  [2] .\n\nSpeech is essential for expressing a person's emotional state through prosody and/or paralinguistic context. In the domain of Speech Emotion Recognition (SER), Machine Learning (ML) models have been constructed relying on the utilization of hand-crafted features, such as Mel-frequency Cepstral Coefficients (MFCC), pitch, energy, entropy or zero-crossing rate  [3] . The correlation between different emotions and specific speech features is still uncertain, and ongoing research is being conducted to investigate specific speech representations that can effectively capture human emotions' patterns. However, deep learning-based models do not require the extraction of a large set of hand-crafted features because they can learn the features that are relevant to the task from spectrograms or even directly from raw waveforms. Nevertheless, a large amount of training data is needed to construct these complex systems because a lack of it could result in poor generalization performance on unseen data conditions.\n\nEmotions can also be captured using text information. Several techniques have been developed to represent and model this type of data. In many Natural Language Processing (NLP) applications, word and sentence embeddings (such as Word2Vec  [4]  or BERT-based  [5] ) have proven to be the most efficient representations, yielding significant improvements in deep learning models performance. Despite offering an enhanced representation of textual information, these techniques have certain limitations when applying them effectively to different tasks. One of the principal difficulties is that training these models requires huge amounts of data and computational power.\n\nThe Odyssey2024 Speech Emotion Recognition Challenge  [6]  was organized as part of the Odyssey 2024 Speaker and Language Recognition Workshop to compare different emotion recognition systems submitted by teams all around the world. The challenge consists of two independent tasks allowing to compare different emotion recognition systems submitted by teams all around the world. The first task objective is to classify speech segments across the eight categorical emotional classes provided: anger, happiness, sadness, fear, surprise, contempt, disgust and a neutral state. The second task consists of predicting emotional attributes for arousal (calm to active), valence (negative to positive), and dominance (weak to strong). The challenge uses recordings from the MSP-Podcast corpus, which contains speech segments obtained from audio-sharing websites  [7] .\n\nIn  [8] , the authors proposed a sub-vector-based multi-head attention pooling to efficiently obtain discriminative speaker embeddings given non-fixed-length speech utterances. In this system, a Convolutional Neural Network (CNN) encodes shortterm speaker features from the spectrogram, which are split into homogeneous sub-vectors. Each self-attention pooling head is applied over a split of sub-vectors. This mechanism allows the model to attend to the most informative patterns from different positions of the encoded sequence. In a later work  [9] , the authors present the Double Multi-Head Attention (DMHA), where a second self-attention pooling is applied. This additional pooling layer allows the model to select which head context vectors are the most relevant to produce an enhanced utterance-level vector. Since this network is trained as a speaker classifier, it can be easily adapted to solve Speaker Classification tasks such as SER  [10] .\n\nIn this paper we describe our Double Multi-Head Attention Multimodal System. We used this architecture for the Odyssey 2024 Speech Emotion Recognition Challenge categorical recognition task. Since self-supervised pre-trained models learn universal representations from massive unlabeled speech data and adapt effectively across various downstream tasks, several pre-trained versions of these architectures were explored to extract acoustic features. Additionally, speech utterances were automatically transcribed with Whisper  [11]  and then text features were extracted using a pre-trained BERT  [5]  model. A Multi-Head Attention layer is used to transform these speech and text features into contextualized representations. Using this attention technique allows the model to learn complex relationships between acoustic and linguistic information. These contextualized representations are then pooled into an utterancelevel vector using a second attention mechanism. Finally, we used a set of fully connected layers to classify the corresponding emotions. Models were trained by applying an on-line data augmentation process. Three different models were used in a hard-voting ensemble to achieve the third position in the Task 1 Challenge ranking, where 31 teams participated in total.\n\nThe rest of this paper is structured as follows. In Section 2 we describe several studies that are related to our work. In Section 3 Double Multi-Head Attention is explained. Section 4 details our proposed system. In Section 5 experimental setups and results are included. Concluding remarks are given in Section 6.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Deep Neural Networks have demonstrated remarkable success in extracting emotions from speech signals in the last years  [12, 13, 14, 15, 16] . Commonly, a SER system includes a feature extraction component and a pooling layer that aggregates the extracted features into a single utterance-level vector. Finally, a classification layer takes the pooled vector as input and outputs each emotion probability. Nevertheless, the automatic recognition of emotions from speech in realistic domains remains a significant challenge, primarily due to the intricate nature of expressive behaviors that manifest during human interactions.\n\nOne major obstacle in SER is still the lack of substantial amounts of labelled data needed to construct complex enough deep learning models  [17] . To address this problem, several data augmentation methods were developed to generate synthetic data  [18, 19] . Additionally, unsupervised or selfsupervised pre-training of neural networks emerged as an effective technique for settings where labelled data is scarce. The key idea is to learn general representations in a setup where substantial amounts of data are available. These learned representations are then used to improve performance on a downstream task for which the amount of data is limited. This is particularly interesting for tasks like SER, where a substantial effort to obtain labelled data is required.\n\nSER systems take audio signals as input. In order to improve the performance of these systems, a feature extraction process is usually applied. In this process, features are extracted from the audio signal by changing the speech waveform to a form of parametric representation. Several options are available to parametrically represent the speech signal for the recognition process. The Mel-frequency Cepstral Coefficients (MFCC) feature extraction strategy is recognized as one of the most effective and universally adopted techniques in the speech recognition domain  [20] . Nevertheless, there are no guarantees that these hand-crafted features are optimal for all speech-related tasks and they might limit the potential of the powerful representation of DNN systems.\n\nTo mitigate this drawback, some works aim to extract learnable acoustic features from the raw audio waveforms. In  [21] , the wav2vec2.0 architecture quantizes continuous speech data into discrete units automatically and then guesses the correct discrete units at randomly masked locations using Transformers  [22] . XLS-R  [23]  is a large-scale model for cross-lingual speech representation learning based on wav2vec2.0. Since it covers a wide range of languages, data regimes and domains, it shows a remarkable generalization capacity. Hidden-Unit BERT (HuBERT)  [24]  is another self-supervised speech representation learning approach, which utilizes an offline clustering step to provide aligned target labels for a BERT-like prediction loss. WavLM  [25]  jointly learns masked speech prediction and denoising in pre-training. Some inputs are simulated noisy/overlapped speech with masks and the target is to predict the pseudo-label of the original speech on the masked region like HuBERT. By doing this, wavLM learns universal speech representations from massive unlabeled speech data and adapts effectively across various speech processing tasks.\n\nRegarding text feature extraction methods, word embeddings have become the standard representation in many natural language processing tasks, being Word2Vec  [4]  and GloVe  [26]  some of the most widely adopted. Both unsupervised models have shown great success in a range of NLP tasks such as sentiment analysis, document indexing, and topic model analysis. Nevertheless, a significant drawback of these models is that they ignore word order, which results in a loss of syntactic and semantic information of words in sentences. By extracting representations from text data to capture the context, BERT  [5]  models are used to solve this limitation. BERT stands for Bidirectional Encoder Representations from Transformers and is a language model trained on large amounts of unlabelled text data that has achieved state-of-the-art results on many NLP tasks. In order to enhance the performance of emotion recognition, several studies have effectively combined speech-based representations with BERT-based embeddings  [27, 28, 12] .\n\nPooling strategies can be classified into two main classes: statistical and learning-based pooling. Within the statistical pooling strategies, the most commonly used are average pooling and statistical pooling  [12, 29] . These strategies assume that all elements of the pooling component input vectors contribute equally to the utterance-level vector creation. Since every frame may not provide equal speech discriminative information, many works applied self-attention mechanisms for weighted learningbased pooling layers. Single-head attention pooling with different self-attentive scoring functions was explored in  [30, 16, 31] . Also, multi-head attention pooling approaches were applied successfully in several works  [32, 33, 9] . Because pooling layers behave differently with different datasets, network structures or loss functions, it is difficult to conclude which one is the best, if there is one. Pooling functions with learnable parameters achieved better results than the simple pooling layers such as the average pooling and statistical pooling in most cases, with a weakness of higher computational complexity than the latter.\n\nMultimodal emotion recognition (MER) aims to identify human emotional states by combining different data modalities such as text, speech, images or videos. With the advance of deep learning technologies and the increasing availability of multimodal datasets many MER systems have been explored  [34, 35] . The expressiveness of unimodal information is naturally restricted, so it frequently fails to fully convey the wide range of human emotions. Since spoken data consists of audio and text information, studies have explored the combination of acoustic features and linguistic content for emotion recognition. The fusion of diverse types of signals to obtain complementary information is key to improving model performance in MER.\n\nThese approaches implement this combination by mixing either each modality embeddings (known as early fusion) or decision scores (late fusion)  [36, 27, 28, 12] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Double Multi-Head Attention",
      "text": "Attention mechanisms have become a standard component of deep learning networks, contributing to remarkable results improvements  [37] . In  [22] , a Multi-Head Attention (MHA) mechanism was proposed. Instead of performing a single attention function, first the queries, keys and values are projected H times with different learned linear projections. Then, an attention function is applied over each of these projected versions of queries, keys and values.\n\nIn  [8] , the authors used a sub-vector-based MHA pooling as an efficient mechanism to obtain discriminative speaker embeddings from speech utterances. Frame-level features are split into H sets of equally-sized sub-vectors and an attention pooling is applied over each set of sub-vectors. The attention performed over each set of sub-vectors is a dot-product attention, where the keys and the values projections are the sub-vectors and there is only one trainable query. In contrast with standard MHA  [22] , this sub-vector MHA outputs H pooled vectors and significantly reduces the number of learnable parameters.\n\nIn the architecture designed in  [8] , the authors extracted speech features from the spectrogram using a CNN component. In this paper we extract features directly from the raw waveform with pre-trained self-supervised models, allowing to remove the CNN component. Combining this removal with a Data Augmentation process enables to significantly increase the number of parameters of the model without losing its generalization ability beyond the training data. Motivated by this, we experiment replacing the efficient sub-vector MHA  [8]  layer with a standard MHA  [22]  layer, allowing the model to capture more complex relationships and patterns.\n\nIn a later work  [9] , the authors presented the Double MHA (DMHA) approach. After a first sub-vector-based MHA layer, a second attention layer is applied. This additional layer uses an attention pooling mechanism over the first layer output vectors by performing a dot-product attention, where the keys and the values projections are the vectors and there is only one trainable query. This second layer allows the model to select which vectors are the most relevant to produce the final pooled vector.\n\nStandard and sub-vector variants for the first attention layer of the DMHA are formulated in subsections 3.1 and 3.2. The second attention layer of the DMHA architecture is explained in subsection 3.3.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Standard Multi-Head Attention",
      "text": "Given a sequence of hidden-states {ht ∈ R D | t = 1, ..., T }, the output of a H-headed standard MHA  [22]  layer are T vectors. These are computed using learnable matrices W Q j , W K j , W V j ∈ R D×D for each head j, and\n\nwhere X ∈ R T ×D is the result of packing together the input sequence h1, ..., hT and\n\nThis mechanism, which we refer to as standard MHA (sometimes also referred to as Global MHA), allows the model to jointly attend to information from different representation subspaces at different positions.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Sub-Vector Multi-Head Attention",
      "text": "Given a sequence of hidden-states {ht ∈ R D | t = 1, ..., T }, the output of a H-headed sub-vector MHA  [8]  layer are H vectors. Each hidden state ht is split into H new hidden states so that ht = [ht1, ..., htH ], where htj ∈ R D/H . Now, for each head j, self-attention is applied over [h1j, ..., hT j ]. Each head weights are defined as:\n\nwhere wtj corresponds to the attention weight of the head j on the step t of the sequence, uj ∈ R D/H is a trainable parameter and d h corresponds to the hidden state dimension D/H. We then compute a pooled representation cj for each head:\n\nEach self-attention pooling for the head j is a dot-product attention over [h1j, ..., hT j ], where the keys and the values are both [h1j, ..., hT j ] and there is only one trainable query uj. With this design, this mechanism allows the model to efficiently attend to the most informative patterns from different positions of the hidden states sequence, significantly reducing the number of learnable parameters.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Attention Pooling",
      "text": "Let {c l ∈ R C | l = 1, ..., L} be the output vectors of the first attention layer. If standard MHA is used in the first layer, L is equal to T ; if sub-vector MHA is used, L is equal to H. Selfattention is now used to pool these vectors to obtain an overall utterance-level vector c:\n\nwhere w ′ i corresponds to the aligned weight of ci and u\n\nWith this method, each vector c is computed as a weighted average of vectors, allowing the system to learn the relevance of each of these vectors for each utterance.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "System Description",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Speech Features",
      "text": "Acoustic features were extracted directly from the waveform using pre-trained self-supervised models. Some of the most widely used and best-performing models to that end are wav2vec2.0  [21] , XLS-R  [23] , HuBERT  [24]  and wavLM  [25] . These pre-trained models get speech features representations using several Transformer layers. Every model has different versions with variations in the number of Transformer layers, the dimension of the speech representations and the training data. We experimented with the following:\n\n• wav2vec2.0: wav2vec2.0 model (\"large-lv60k\" architecture), pre-trained on 60,000 hours of unlabeled audio, not fine-tuned.\n\n• XLS-R: XLS-R model with 300 million parameters, pretrained on 436,000 hours of unlabeled audio from multiple datasets in 128 languages, not fine-tuned.\n\n• HuBERT: HuBERT model (\"large\" architecture), pretrained on 60,000 hours of unlabeled audio, not finetuned.\n\n• wavLM: wavLM Large model (\"large\" architecture), pre-trained on more than 80,000 hours, not fine-tuned.\n\nEvery one of these versions uses 24 Transformer layers to get 1024-dimensional speech representations. Since every Transformer layer may produce a different level of complexity, a multi-level aggregation is often suggested  [38] . To explore this approach, we extract the acoustic features of each model performing a weighted sum of every Transformer layer, where the weights were learned by the model at the training stage.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Text Features",
      "text": "To extract text features from the waveforms we first used Whisper  [11]  to get audio transcriptions. Whisper is an Automatic Speech Recognition Transformer-based system trained with up to 680,000 hours of multilingual and multitask data. When compared to humans, this model approaches their accuracy and robustness. A pre-trained BERT model was used to extract text features from the transcriptions. The BERT large uncased pretrained version was used, which was trained on lower-cased English text. This version has a total of 340M parameters, using 24 Transformer layers to output 1024-dimensional text representations. In this case, only the last Transformer layer output was used.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Double Multi-Head Attention Multimodal Fusion",
      "text": "Given a speech segment, the input of the DMHA Multimodal Fusion component is the acoustic and text features. Several studies have explored the combination of acoustic features and linguistic content for emotion recognition, concluding that the fusion of diverse types of signals is key to improving model performance  [34] . Motivated by this, these acoustic and text features were mixed into a first MHA layer to let the model learn complementary information using self-attention scores. Then, a second attention layer is applied to generate an utterance-level pooled vector.\n\nIf there are T1 acoustic features and T2 text features, the T = T1 + T2 features are input to the first MHA layer. This first attention layer can consist of a sub-vector MHA or a standard MHA as described in section 3. If a H-headed sub-vector MHA is used, this first layer will output H vectors. In the case of a H-headed standard MHA, this first layer will output T vectors. We experimented with 4-headed sub-vector and standard MHA layers. This first MHA layer outputs multimodal contextualized representations which are then aggregated into a single utterance-level vector using a second attention layer with an attention pooling strategy as described in section 3.3.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Classification Layer",
      "text": "The aggregated utterance-level vector c obtained from the pooling component is fed into a classification layer. This component consists of a set of fully connected layers: an input layer, several hidden layers and an output layer. The input layer has the same width as c's dimension. The output layer has the same width as the number of classes of the task and a SoftMax layer is applied to get each class probability.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Database",
      "text": "We evaluate the effectiveness of the proposed multimodal emotion recognition system on the MSP-Podcast dataset  [7] , which contains speech segments obtained from audio-sharing websites. The speaking turns have been perceptually annotated by at least five raters with categorical and attribute-based emotional labels. Categorical eight emotional classes provided in the dataset are happiness, sadness, fear, surprise, contempt, disgust and a neutral state. While the distribution for attributebased emotional labels has balanced content, the distribution for emotional categories is less balanced. The training set has 68,119 speaking turns. The development set has 19,815 speaking segments from 454 speakers. The test set comprises 2,347 unique segments from 187 speakers, for which the labels have not been made publicly available for the challenge.\n\nIt has been proven that using Data Augmentation techniques to increase the volume of the training data significantly improves the performance of speech recognition systems  [19] . An online Data Augmentation process  [19]  was used applying speaker augmentation with speed perturbation  [39] , room impulse responses (RIRs) from  [40]  and background noises from the MUSAN database  [41]  directly to the waveform.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experimental Setup",
      "text": "Raw waveforms were normalized using their overall mean and standard deviation from the training dataset. We trained our models using batches of size 32. Every speech signal was randomly cropped with a 5.5-second window, given that the median of the training utterances length is 5.2 seconds. When needed, a repetition padding was used. Each waveform was augmented with a 0.5 probability. In that case, one augmentation technique was randomly chosen from speed perturbation, RIRs or background noises. Full-length waveforms without augmentation were used at the evaluation stage.\n\nAcoustic and text features were extracted using frozen selfsupervised pre-trained models. Each feature has a dimension equal to 1024. Pre-computed Whisper transcriptions were used before extracting text features. A 4-headed Double Multi-Head Multimodal Fusion was used as the pooling component. We experimented using both standard and sub-vector-based MHA layers. Finally, the 8-emotion classification was done by passing the pooled vector through the classification component. The input layer of the classification component has the same width as the aggregated utterance-level vector dimension obtained from the pooling component. The hidden layers of the classification component are a set of 4 512-dimensional dense layers. Each of the input and hidden layers is followed by a Layer Normalization  [42] , Gaussian Error Linear Units (GELU) activations  [43]  and a 0.1 probability of dropout.\n\nSince the F1-score (macro) was the metric defined to evaluate the categorical emotions prediction task (task 1 from the Challenge), training and validation phases were monitored and evaluated with this score. Each team had a maximum of three submissions per task and the submitted systems were evaluated in the test set, from which labels were not publicly available. Every model was trained for 20 epochs, using early stopping to avoid overfitting. All our models were trained and evaluated using 2 GPUs (NVIDIA GeForce GTX TITAN X, NVIDIA TI-TAN Xp or NVIDIA GeForce RTX 2080 Ti). Every 20 epochs training took around 24 hours to finish. AdamW  [44]  was selected as the optimizer. Different learning rates were tried, using a 50% decay every 5 epochs without validation of F1-score improvement.\n\nRegarding loss functions, we experimented with Weighted Cross-Entropy (WCE) Loss and Focal Loss  [45]  since they both act as effective alternatives to deal with class imbalance. The training inverse frequency of each class was used as weights for the WCE Loss. The Focal Loss function is a dynamically scaled cross-entropy loss, where the scaling factor decays to zero as confidence in the correct class increases. Intuitively, this scaling factor can automatically down-weight the contribution of easy examples during training and rapidly focus the model on hard examples. This scaling factor is governed by a parameter gamma, which was set to 2.\n\nOnce each model was trained, a per-class threshold adjustment was performed. The idea is to adjust each class decision threshold to improve their precision and recall. For each class, we convert the task to a binary classification by treating the rest as a whole one negative class. Then, different thresholds were explored to maximize the training Macro F1-score. Once every threshold is set, the way of predicting the final class is the following: if the class with the highest probability output surpass its corresponding threshold, then that class is predicted (no change is done). If the class with the highest probability output do not surpass its corresponding threshold, then the class with the next highest probability is predicted.\n\nAfter the training phase and threshold adjustments were applied, we experimented with mixing different models to get a 3-model ensemble. The motivation for using ensemble models is to reduce the generalization error of the final prediction. As long as the base models are diverse enough, the prediction error of the ensembled model can decrease. A hard voting strategy was adopted for the ensemble. In case of a tie, we used the prediction of a predefined model (the one with the best validation Macro F1-score). Ensemble of models combine the following three models: WCE Loss and XLS-R; WCE Loss and wav2vec2.0; WCE Loss and wavLM.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Configuration",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Results",
      "text": "We trained several models combining different loss functions (WCE loss and Focal loss), pre-trained self-supervised speech features extractors (wav2vec2.0, XLS-R, HuBERT and wavLM) and MHA mechanisms (standard and sub-vector MHA). A model ensemble using the following three configurations was also evaluated: WCE loss and XLS-R speech features extractor; WCE loss and wav2vec2.0 speech features extractor; WCE loss and wavLM speech features extractor. The choice of these three models was motivated by the idea that extracting different patterns from the same speech utterance may contribute to the final system diversity. After applying threshold adjustment, we evaluate them using Macro F1-score both in the training and validation datasets of the competition. Applying threshold adjustment increased validation Macro F1-score approximately in a 1% absolute both for the Standard and the sub-vector-based models, being a useful strategy for this competition.\n\nWe can see from Table  1  results that our best model is the ensemble of models. This configuration was used to obtain our best submission ranking at the Challenge. This model outperforms the challenge official baseline with a 3.1% absolute increase in the validation Macro F1-score. In terms of relative improvement, our best model gained 10.1% validation Macro F1-score compared to the baseline. When using standard MHA, models with XLS-R  [23]  as speech feature extractor were the ones with the best performance.  Figure  2  shows the validation confusion matrix for the ensemble model. We can see that the system achieves the best results detecting happiness and anger. There are some emotions that may be more similar and the model can confuse: when the real emotion is contempt, it predicts anger 30% of the time; a similar situation arises for surprise and happiness; disgust is predicted as anger or contempt around 45% of the cases. Finally, fear is the worst performance class. We used the trained model corresponding to the WCE loss and XLS-R speech feature extractor configuration with standard MHA to analyze the features fusion in the MHA layer. To that end, each head attention weights of a particular audio sample were calculated. For each head, every contextual representation uses learned attention weights (matrix rows) to attend to each of the speech and text features (matrix columns), as visualized in figures 3 and 4. Figure  3  shows that, for head 0, attention is focused both on some speech and text features (specially in rows 20 to 30) to generate contextual representations. On the other hand, in figure  4  we can see that, for head 1, some contextual representations attend mostly to speech features and others mostly to text features. This shows how our Multi-Head Attention Multimodal Fusion creates complex contextual representations combining speech and text information in different ways.  The results of the experiments using the sub-vector MHA approach are shown in Table  2 . Given that both the configurations with WCE Loss and HuBERT speech feature extractor and with Focal Loss and XLS-R speech feature extractor the baseline validation score, we conclude that combining a powerful self-supervised feature extractor with a sub-vector-based Double Multi-Head Attention Multimodal Fusion component can obtain good results for this task. Since the sub-vector MHA has significantly less parameters than the standard MHA, this efficient architecture could be useful in settings where large models are not allowed and/or training data is scarce, which is a common scenario in SER.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper we have described our Double Multi-Head Attention Multimodal System for the Odyssey 2024 Speech Emotion Recognition Challenge. Acoustic and text features were extracted using pre-trained self-supervised models. These multimodal features are mixed adopting an early fusion strategy. First, an MHA layer generates complementary contextualized representations. A second attention layer is then applied to pool these representations into an utterance-level vector. For the first attention layer, we experimented with different mechanisms: standard MHA and sub-vector MHA. Since the subvector variant has significantly less parameters than the standard one, our obtained results show that this efficient architecture could be useful in settings where large models are not allowed and/or training data is scarce. On the other hand, applying standard MHA allows the model to capture complex relationships jointly attending to information from different representation subspaces. In terms of relative improvement, our best model gained 10.1% validation Macro F1-score compared to the baseline. This model outperforms the challenge official baseline with a 3.1% absolute increase in the validation Macro F1-score, achieving the third position in the categorical task ranking, where 31 teams participated in total.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: illustrates the architecture of our proposed multimodal",
      "page": 3
    },
    {
      "caption": "Figure 1: Double Multi-Head Attention Multimodal System ar-",
      "page": 4
    },
    {
      "caption": "Figure 2: shows the validation confusion matrix for the en-",
      "page": 6
    },
    {
      "caption": "Figure 2: Validation confusion matrix of the ensemble model.",
      "page": 6
    },
    {
      "caption": "Figure 3: shows that, for head 0,",
      "page": 6
    },
    {
      "caption": "Figure 4: we can see that, for head 1, some",
      "page": 6
    },
    {
      "caption": "Figure 3: Head 0 attention weights visualization. The white",
      "page": 6
    },
    {
      "caption": "Figure 4: Head 1 attention weights visualization. The white",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Configuration": "Ensemble of models",
          "Train\nMacro\nF1-score": "34.88%",
          "Validation\nMacro\nF1-score ↓": "33.80%"
        },
        {
          "Configuration": "WCE Loss",
          "Train\nMacro\nF1-score": "34.42%",
          "Validation\nMacro\nF1-score ↓": "33.43%"
        },
        {
          "Configuration": "Focal Loss",
          "Train\nMacro\nF1-score": "39.15%",
          "Validation\nMacro\nF1-score ↓": "33.37%"
        },
        {
          "Configuration": "WCE Loss",
          "Train\nMacro\nF1-score": "34.83%",
          "Validation\nMacro\nF1-score ↓": "32.69%"
        },
        {
          "Configuration": "Focal Loss",
          "Train\nMacro\nF1-score": "37.84%",
          "Validation\nMacro\nF1-score ↓": "32.40%"
        },
        {
          "Configuration": "WCE Loss",
          "Train\nMacro\nF1-score": "36.73%",
          "Validation\nMacro\nF1-score ↓": "32.18%"
        },
        {
          "Configuration": "WCE Loss",
          "Train\nMacro\nF1-score": "32.48%",
          "Validation\nMacro\nF1-score ↓": "31.44%"
        },
        {
          "Configuration": "Focal Loss",
          "Train\nMacro\nF1-score": "35.75%",
          "Validation\nMacro\nF1-score ↓": "31.27%"
        },
        {
          "Configuration": "Focal Loss",
          "Train\nMacro\nF1-score": "33.24%",
          "Validation\nMacro\nF1-score ↓": "30.77%"
        },
        {
          "Configuration": "Challenge Official Baseline",
          "Train\nMacro\nF1-score": "-",
          "Validation\nMacro\nF1-score ↓": "30.70%"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Configuration": "WCE Loss\nHuBERT",
          "Train\nMacro\nF1-score": "37.88%",
          "Validation\nMacro\nF1-score ↓": "30.84%"
        },
        {
          "Configuration": "Focal Loss\nXLS-R",
          "Train\nMacro\nF1-score": "37.41%",
          "Validation\nMacro\nF1-score ↓": "30.73%"
        },
        {
          "Configuration": "Challenge Official Baseline",
          "Train\nMacro\nF1-score": "-",
          "Validation\nMacro\nF1-score ↓": "30.70%"
        },
        {
          "Configuration": "WCE Loss",
          "Train\nMacro\nF1-score": "36.05%",
          "Validation\nMacro\nF1-score ↓": "30.53%"
        },
        {
          "Configuration": "Focal Loss",
          "Train\nMacro\nF1-score": "31.20%",
          "Validation\nMacro\nF1-score ↓": "29.31%"
        },
        {
          "Configuration": "WCE Loss",
          "Train\nMacro\nF1-score": "30.29%",
          "Validation\nMacro\nF1-score ↓": "28.93%"
        },
        {
          "Configuration": "Focal Loss",
          "Train\nMacro\nF1-score": "31.94%",
          "Validation\nMacro\nF1-score ↓": "28.89%"
        },
        {
          "Configuration": "Focal Loss",
          "Train\nMacro\nF1-score": "32.28%",
          "Validation\nMacro\nF1-score ↓": "27.67%"
        },
        {
          "Configuration": "WCE Loss",
          "Train\nMacro\nF1-score": "31.69%",
          "Validation\nMacro\nF1-score ↓": "26.13%"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Speech emotion recognition: Two decades in a nutshell, benchmarks, and ongoing trends",
      "authors": [
        "W Björn",
        "Schuller"
      ],
      "year": "2018",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "3",
      "title": "Social emotions in nature and artifact: emotions in human and human-computer interaction",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Shrikanth Narayanan",
        "J Gratch",
        "Marsella"
      ],
      "year": "2013",
      "venue": "Social emotions in nature and artifact: emotions in human and human-computer interaction"
    },
    {
      "citation_id": "4",
      "title": "Emotional speech recognition: Resources, features, and methods",
      "authors": [
        "Dimitrios Ververidis",
        "Constantine Kotropoulos"
      ],
      "year": "2006",
      "venue": "Speech communication"
    },
    {
      "citation_id": "5",
      "title": "Efficient estimation of word representations in vector space",
      "authors": [
        "Tomas Mikolov",
        "Kai Chen",
        "Greg Corrado",
        "Jeffrey Dean"
      ],
      "year": "2013",
      "venue": "Efficient estimation of word representations in vector space",
      "arxiv": "arXiv:1301.3781"
    },
    {
      "citation_id": "6",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2018",
      "venue": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "7",
      "title": "Odyssey2024 -speech emotion recognition challenge: Dataset, baseline framework, and results",
      "authors": [
        "L Goncalves",
        "A Salman",
        "A Reddy Naini",
        "L Moro-Velazquez",
        "T Thebaud",
        "L Garcia",
        "N Dehak",
        "B Sisman",
        "C Busso"
      ],
      "year": "2024",
      "venue": "Odyssey 2024: The Speaker and Language Recognition Workshop)"
    },
    {
      "citation_id": "8",
      "title": "Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings",
      "authors": [
        "Reza Lotfian",
        "Carlos Busso"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "9",
      "title": "Self multi-head attention for speaker recognition",
      "authors": [
        "Miquel India",
        "Pooyan Safari",
        "Javier Hernando"
      ],
      "year": "2019",
      "venue": "Self multi-head attention for speaker recognition",
      "arxiv": "arXiv:1906.09890"
    },
    {
      "citation_id": "10",
      "title": "Double multi-head attention for speaker verification",
      "authors": [
        "Miquel India",
        "Pooyan Safari",
        "Javier Hernando"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "11",
      "title": "Speaker characterization by means of attention pooling",
      "authors": [
        "Federico Costa",
        "Miquel Àngel",
        "India Massana",
        "Francisco Javier",
        "Hernando Pericás"
      ],
      "year": "2022",
      "venue": "International Speech Communication Association (ISCA)"
    },
    {
      "citation_id": "12",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Tao Xu",
        "Greg Brockman",
        "Christine Mcleavey",
        "Ilya Sutskever"
      ],
      "year": "2023",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "13",
      "title": "Multimodal emotion recognition using transfer learning from speaker recognition and bert-based models",
      "authors": [
        "Sarala Padi",
        "Omid Seyed",
        "Dinesh Sadjadi",
        "Ram Manocha",
        "Sriram"
      ],
      "year": "2022",
      "venue": "Multimodal emotion recognition using transfer learning from speaker recognition and bert-based models",
      "arxiv": "arXiv:2202.08974"
    },
    {
      "citation_id": "14",
      "title": "Arabic speech emotion recognition employing wav2vec2. 0 and hubert based on baved dataset",
      "authors": [
        "Omar Mohamed",
        "A Salah",
        "Aly"
      ],
      "year": "2021",
      "venue": "Arabic speech emotion recognition employing wav2vec2. 0 and hubert based on baved dataset",
      "arxiv": "arXiv:2110.04425"
    },
    {
      "citation_id": "15",
      "title": "Att-net: Enhanced emotion recognition system using lightweight self-attention module",
      "authors": [
        "Soonil Kwon"
      ],
      "year": "2021",
      "venue": "Applied Soft Computing"
    },
    {
      "citation_id": "16",
      "title": "Fusionconvbert: parallel convolution and bert fusion for speech emotion recognition",
      "authors": [
        "Sanghyun Lee",
        "David Han",
        "Hanseok Ko"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "17",
      "title": "Improved end-to-end speech emotion recognition using self attention mechanism and multitask learning",
      "authors": [
        "Yuanchao Li",
        "Tianyu Zhao",
        "Tatsuya Kawahara"
      ],
      "year": "2019",
      "venue": "Interspeech"
    },
    {
      "citation_id": "18",
      "title": "Emotion recognition in speech using cross-modal transfer in the wild",
      "authors": [
        "Arsha Samuel Albanie",
        "Andrea Nagrani",
        "Andrew Vedaldi",
        "Zisserman"
      ],
      "year": "2018",
      "venue": "Proceedings of the 26th ACM international conference on Multimedia"
    },
    {
      "citation_id": "19",
      "title": "Data augmentation using gans for speech emotion recognition",
      "authors": [
        "Aggelina Chatziagapi",
        "Georgios Paraskevopoulos"
      ],
      "year": "2019",
      "venue": "Dimitris Sgouropoulos, Georgios Pantazopoulos, Malvina Nikandrou, Theodoros Giannakopoulos, Athanasios Katsamanis, Alexandros Potamianos, and Shrikanth Narayanan"
    },
    {
      "citation_id": "20",
      "title": "On-the-fly data loader and utterance-level aggregation for speaker and language recognition",
      "authors": [
        "Weicheng Cai",
        "Jinkun Chen",
        "Jun Zhang",
        "Ming Li"
      ],
      "year": "2020",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "21",
      "title": "A survey of speaker recognition: Fundamental theories, recognition methods and opportunities",
      "authors": [
        "M Muhammad Mohsin Kabir",
        "Jungpil Mridha",
        "Israt Shin",
        "Abu Jahan",
        "Ohi Quwsar"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "22",
      "title": "wav2vec 2.0: A framework for selfsupervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "23",
      "title": "Advances in neural information processing systems",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "24",
      "title": "Xls-r: Self-supervised cross-lingual speech representation learning at scale",
      "authors": [
        "Arun Babu",
        "Changhan Wang",
        "Andros Tjandra",
        "Kushal Lakhotia",
        "Qiantong Xu",
        "Naman Goyal",
        "Kritika Singh",
        "Yatharth Patrick Von Platen",
        "Juan Saraf",
        "Pino"
      ],
      "year": "2021",
      "venue": "Xls-r: Self-supervised cross-lingual speech representation learning at scale",
      "arxiv": "arXiv:2111.09296"
    },
    {
      "citation_id": "25",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "Wei-Ning Hsu",
        "Benjamin Bolte",
        "Hubert Yao-Hung",
        "Kushal Tsai",
        "Ruslan Lakhotia",
        "Abdelrahman Salakhutdinov",
        "Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "26",
      "title": "Wavlm: Large-scale selfsupervised pre-training for full stack speech processing",
      "authors": [
        "Sanyuan Chen",
        "Chengyi Wang",
        "Zhengyang Chen",
        "Yu Wu",
        "Shujie Liu",
        "Zhuo Chen",
        "Jinyu Li",
        "Naoyuki Kanda",
        "Takuya Yoshioka",
        "Xiong Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "27",
      "title": "Glove: Global vectors for word representation",
      "authors": [
        "Jeffrey Pennington",
        "Richard Socher",
        "Christopher Manning"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 conference on empirical methods in natural language processing"
    },
    {
      "citation_id": "28",
      "title": "Emotion recognition by fusing time synchronous and time asynchronous representations",
      "authors": [
        "Wen Wu",
        "Chao Zhang",
        "Philip Woodland"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "29",
      "title": "Fusion approaches for emotion recognition from speech using acoustic and text-based features",
      "authors": [
        "Leonardo Pepino",
        "Pablo Riera",
        "Luciana Ferrer",
        "Agustín Gravano"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "30",
      "title": "Learning discriminative features for speaker identification and verification",
      "authors": [
        "Sarthak Yadav",
        "Atul Rai"
      ],
      "year": "2018",
      "venue": "Learning discriminative features for speaker identification and verification"
    },
    {
      "citation_id": "31",
      "title": "Speech emotion recognition from 3d log-mel spectrograms with deep learning network",
      "authors": [
        "Tianhao Hao Meng",
        "Fei Yan",
        "Hongwei Yuan",
        "Wei"
      ],
      "year": "2019",
      "venue": "IEEE access"
    },
    {
      "citation_id": "32",
      "title": "Self attention networks in speaker recognition",
      "authors": [
        "Pooyan Safari",
        "Miquel India",
        "Javier Hernando"
      ],
      "year": "2023",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "33",
      "title": "Multi-resolution multi-head attention in deep speaker embedding",
      "authors": [
        "Zhiming Wang",
        "Kaisheng Yao",
        "Xiaolong Li",
        "Shuo Fang"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "34",
      "title": "Self-attentive speaker embeddings for text-independent speaker verification",
      "authors": [
        "Yingke Zhu",
        "Tom Ko",
        "David Snyder",
        "Brian Mak",
        "Daniel Povey"
      ],
      "year": "2018",
      "venue": "Self-attentive speaker embeddings for text-independent speaker verification"
    },
    {
      "citation_id": "35",
      "title": "A survey of deep learning-based multimodal emotion recognition: Speech, text, and face",
      "authors": [
        "Hailun Lian",
        "Cheng Lu",
        "Sunan Li",
        "Yan Zhao",
        "Chuangao Tang",
        "Yuan Zong"
      ],
      "year": "2023",
      "venue": "Entropy"
    },
    {
      "citation_id": "36",
      "title": "Multimodal emotion recognition using deep learning",
      "authors": [
        "M Sharmeen",
        "Abdullah Saleem",
        "Abdullah",
        "Mohammed Siddeeq Y Ameen Ameen",
        "Subhi Sadeeq",
        "Zeebaree"
      ],
      "year": "2021",
      "venue": "Journal of Applied Science and Technology Trends"
    },
    {
      "citation_id": "37",
      "title": "Multi-modal fusion network with complementarity and importance for emotion recognition",
      "authors": [
        "Shuai Liu",
        "Peng Gao",
        "Yating Li",
        "Weina Fu",
        "Weiping Ding"
      ],
      "year": "2023",
      "venue": "Information Sciences"
    },
    {
      "citation_id": "38",
      "title": "A review on the attention mechanism of deep learning",
      "authors": [
        "Zhaoyang Niu",
        "Guoqiang Zhong",
        "Hui Yu"
      ],
      "year": "2021",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "39",
      "title": "Superb: Speech processing universal performance benchmark",
      "authors": [
        "Shu-Wen Yang",
        "Po-Han Chi",
        "Yung-Sung Chuang",
        "Cheng-I Jeff Lai",
        "Kushal Lakhotia",
        "Andy Yist Y Lin",
        "Jiatong Liu",
        "Xuankai Shi",
        "Guan-Ting Chang",
        "Lin"
      ],
      "year": "2021",
      "venue": "Superb: Speech processing universal performance benchmark",
      "arxiv": "arXiv:2105.01051"
    },
    {
      "citation_id": "40",
      "title": "Speaker augmentation and bandwidth extension for deep speaker embedding",
      "authors": [
        "Hitoshi Yamamoto",
        "Aik Kong",
        "Koji Lee",
        "Takafumi Okabe",
        "Koshinaka"
      ],
      "year": "2019",
      "venue": "Speaker augmentation and bandwidth extension for deep speaker embedding"
    },
    {
      "citation_id": "41",
      "title": "A study on data augmentation of reverberant speech for robust speech recognition",
      "authors": [
        "Tom Ko",
        "Vijayaditya Peddinti",
        "Daniel Povey",
        "Michael Seltzer",
        "Sanjeev Khudanpur"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "42",
      "title": "Musan: A music, speech, and noise corpus",
      "authors": [
        "David Snyder",
        "Guoguo Chen",
        "Daniel Povey"
      ],
      "year": "2015",
      "venue": "Musan: A music, speech, and noise corpus",
      "arxiv": "arXiv:1510.08484"
    },
    {
      "citation_id": "43",
      "title": "Layer normalization",
      "authors": [
        "Jimmy Lei Ba",
        "Jamie Ryan Kiros",
        "Geoffrey Hinton"
      ],
      "year": "2016",
      "venue": "Layer normalization",
      "arxiv": "arXiv:1607.06450"
    },
    {
      "citation_id": "44",
      "title": "Gaussian error linear units (gelus)",
      "authors": [
        "Dan Hendrycks",
        "Kevin Gimpel"
      ],
      "year": "2016",
      "venue": "Gaussian error linear units (gelus)",
      "arxiv": "arXiv:1606.08415"
    },
    {
      "citation_id": "45",
      "title": "Decoupled weight decay regularization",
      "authors": [
        "Ilya Loshchilov",
        "Frank Hutter"
      ],
      "year": "2017",
      "venue": "Decoupled weight decay regularization",
      "arxiv": "arXiv:1711.05101"
    },
    {
      "citation_id": "46",
      "title": "Focal loss for dense object detection",
      "authors": [
        "Tsung-Yi Lin",
        "Priya Goyal",
        "Ross Girshick",
        "Kaiming He",
        "Piotr Dollár"
      ],
      "year": "2017",
      "venue": "Proceedings"
    }
  ]
}