{
  "paper_id": "2308.09013v2",
  "title": "Deep-Seeded Clustering For Emotion Recognition From Wearable Physiological Sensors",
  "published": "2023-08-17T14:37:35Z",
  "authors": [
    "Marta A. Conceição",
    "Antoine Dubois",
    "Sonja Haustein",
    "Bruno Miranda",
    "Carlos Lima Azevedo"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "According to the circumplex model of affect, an emotional response could characterized by a level of pleasure (valence) and intensity (arousal). As it reflects on the autonomic nervous system (ANS) activity, modern wearable wristbands can record non-invasively and during our everyday lives peripheral end-points of this response. While emotion recognition from physiological signals is usually achieved using supervised machine learning algorithms that require ground truth labels for training, collecting it is cumbersome and particularly unfeasible in naturalistic settings, and extracting meaningful insights from these signals requires domain knowledge and might be prone to bias. Here, we propose and test a deep-seeded clustering algorithm that automatically extracts and classifies features from those physiological signals with minimal supervision -combining an autoencoder (AE) for unsupervised feature representation and c-means clustering for fine-grained classification. We also show that the model obtains good performance results across three different datasets frequently used in affective computing studies (accuracies of 80.7% on WESAD, 64.2% on Stress-Predict and 61.0% on CEAP360-VR).",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Emotions play a significant role in the cognitive processes of the human brain, such as decision-making, learning and perception. Therefore, emotions affect our everyday life, and understanding it plays a vital role in understanding how we behave  [Picard, 2000] .\n\nBased on the circumplex model of affect, an emotion is best described as varying along two dimensions that relate to valence (unpleasant to pleasant) and arousal or activation (low to high intensity)  [Russell, 1980] . The autonomic nervous system (ANS) is viewed as a critical component of the deployment of an emotional response, leading to peripheral physiological signals (such as heart rate, temperature or sweating responses) which cannot be intentionally controlled. Hence, measuring such signals may often provide more objective and reliable measures (compared to, for example, self-assessments)  [Kreibig, 2010] .\n\nModern wearable wristbands, such as the Empatica E4  [Empatica, 2015]  (to note that it has been recently discontinued), can be used to record peripheral physiological signals (including blood volume pulse, or BVP); electrodermal activity, or EDA; and skin temperature, or TEMP) in a non-intrusive and convenient way -making it suitable for both lab-based and real-life settings.\n\nIn affective computing works, emotion recognition from physiological signals is usually achieved using supervised machine learning algorithms that require ground truth labels for training  [Anusha et al., 2019]  [  Song et al., 2018]  [  Schmidt et al., 2018 ] [Picard et al., 2001] . However, collecting fine-grained labels (as opposed to single post-stimuli emotion labels) is often hard to implement and time-consuming. It is particularly not efficient (as it increases the risk of dropout and disengagement) when applying such emotion recognition algorithms to real-life settings with longitudinal data collection (e.g., continuous recordings over several days)  [Zhang et al., 2022] .\n\nIn this manuscript, we propose a novel approach by describing and assessing a novel deep-seeded clustering model for the characterisation of emotional responses derived from a single retrospective affective label. In brief, the suggested model combines an autoencoder (AE) -for unsupervised feature representation with c-means clustering -for fine-grained classification. The present work aims to provide the following relevant contributions to affective computing research:\n\n• Propose an end-to-end deep learning framework for emotion recognition -which omits feature extraction and requires no domain knowledge. For this, it takes (as input) raw physiological data collected from a wearable device; learns insightful feature representations (or embeddings); and infer, with minimal supervision (i.e., labels) emotions from those physiological measurements. • Show reasonable to good model performance of our weakly-supervised framework on three validated datasets commonly used in the affective computing literature (WESAD  [Schmidt et al., 2018] ; Stress-Predict  [Iqbal et al., 2022] ; and CEAP360-VR  [Xue et al., 2023] ). Our findings revealed, for subjectdependent classification, the following accuracies: 80.7% accuracy on WESAD, 64.2% on Stress-Predict and 61.0% on CEAP360-VR. To note that these results were either similar or better than those achieved by the original studies -using supervised approaches. Furthermore, it is important to highlight that our framework has achieved such recognition accuracy and clustering quality without any hyper-parameter tuning -leading us to hypothesize that better performance results could be achieved through adequate optimization.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "In this section, we review related work in the field of machine learning for emotion recognition from physiological signals. We provide a brief overview of: (i) the concept of emotion; (ii) how it translates into physiological signals that we can easily and objectively measure; and (iii) how machine learning techniques have been used in the literature for its detection. To note that we will not be addressing neural emotional regulation or processing (i.e., brain processes controlling or modulating the emotional response -beyond the scope of this work); but rather the (self-) perceived or physically deployed (through physiological peripheral responses) emotional experiences.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "A. Emotion Theory, Elicitation And Assessment",
      "text": "Defining emotion in a universally accepted way is challenging. In general, affect is understood as a neurophysiological state that can be consciously experienced, but not necessarily focused on a specific object. Mood refers to a prolonged, low intensity feeling, while emotion is characterized by a brief, intense, and targeted experience  [Fox, 2018] . In this work, we refer to emotions interchangeably with affect and consider the circumplex model of affect to define stress as a high-arousal and low-valence emotion  [Russell, 1980] .\n\nA critical decision for a good experimental design (and for the validity or generalization of a scientific finding) is whether to conduct the study in a laboratory setting or in a real-life (or ecological) environment. In field studies, a key challenge is generating accurate labels; on the other hand, no design of affective stimuli is required, as different emotional states naturally occur in real-world settings. In contrast, when conducting lab-based studies, researchers can follow welldesigned protocols which not only allow for replication, but also can facilitate obtaining high-quality labels, as the desired affective states can be induced using a carefully selected set of stimuli, and dedicated time slots can also be set aside for questionnaires; however, if these stimuli are not appropriate, the intended effects may not be achieved.\n\nMost studies opt for lab-based settings, using images, videos, virtual reality (VR) environments, or stress-inducing protocols as stimuli  [Saganowski et al., 2022] . Among those are  [Schmidt et al., 2018] , which combined videos and a social-evaluative and cognitive stress-inducing protocol, the Trier Social Stress Test (TSST),  [Iqbal et al., 2022] , that applied stress-inducing tasks, including social-evaluative, cognitive and physical -the TSST, the Stroop Color Test and the Hyperventilation Provocation Test, and  [Xue et al., 2023] , which used immersive VR for emotion elicitation.\n\nTo ensure that the desired affective states are successfully evoked, self-assessment questionnaires are often used. These include the Self-Assessment Manikin (SAM), the State-Trait Anxiety Inventory (STAI), Positive and Negative Affect Schedule (PANAS) and self-developed questionnaires, for example, including rating the intensity of emotions or affect on a Likert scale  [Saganowski et al., 2022 ] [Schmidt et al., 2019] . In real-life (field) studies, ecological momentary assessments (EMA) are often used, which consist of short sets of questionnaires that participants occasionally file to report their current affective state  [Schmidt et al., 2019] .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Physiological Indicators Of Affect",
      "text": "Physiological changes in the cardiovascular system, electrical conductance of the skin (electrical electrodermal activity, or EDA) and skin temperature have been associated with some affective states. Relevant for real-life experimental settings, some commercial wearable devices (to note that wearable devices can also be used in lab-based studies; but for such well-controlled studies, preference is given to more robust devices) often allow the recording of such physiological signals  [Saganowski et al., 2022 ][Schmidt et al., 2019] . As a consequence, several studies in the literature have been using these physiological signals for emotion recognition -including the Empatica E4  [Empatica, 2015]  device used in our present work.\n\nThe Blood Volume Pulse (BVP), derived from photoplethysmography (PPG), monitors changes in blood volume within the capillaries and arteries and it is typically measured using non-invasive optical sensors that assess cardiovascular dynamics  [Chu et al., 2017 ] [Jang et al., 2015] . While this sensor can be placed on various parts of the body, in the case of Empatica E4, it is embedded in its wristband. Heart rate information derived from BVP (computed through the inverse) is indicative of emotional states, with lower heart rates associated with relaxation and higher heart rates linked to heightened emotions such as stress, joy, or anger  [Alhargan et al., 2017 ][Wagner et al., 2005] .\n\nThe Electrodermal Activity (EDA), also known as Galvanic Skin Response (GSR), is another (as the cardiovascular and temperature response) non-invasive peripheral measure of the autonomic nervous system (ANS) activity, being particularly related to the arousal state. For example, increased EDA has been associated to intense emotions like joy, anger, and stress  [Haag et al., 2004 ] [Wagner et al., 2005 ] [Jang et al., 2015] . In brief, it reflects changes in the skin's electrical properties (particularly its electrical conductance) due to sweat gland activity. To measure EDA, electrodes are placed on the skin, usually in the palms, fingers or in the wrist  [Greco et al., 2016] ,\n\nSkin temperature, similar to EDA, is influenced by external factors linked to emotional responses; for instance, stress can cause muscle tension and blood vessel constriction, leading to a decrease in temperature  [Jang et al., 2015] . Despite being a useful emotional indicator, it is relatively slow to respond to changes, so a sampling rate of 1 Hz is generally sufficient  [Schmidt et al., 2019] .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "C. Machine Learning For Emotion Recognition",
      "text": "Various machine learning approaches have been used to infer emotional states, exploring both supervised and unsupervised techniques  [Chanel et al., 2009] .\n\nSupervised models learn to recognize emotions using a set of explanatory variables and respective labels (either self-reported emotional ratings or standardized stimuli/contexts well-known to elicit specific emotions)  [Anusha et al., 2019 ] [Song et al., 2018 ] [Schmidt et al., 2018 ] [Picard et al., 2001] . Even though studies using these methods have achieved good performance in emotion recognition, the required ground truth labels are not only time consuming to obtain, but can also be imprecise as, on one hand, a specific stimuli may evoke a combination of emotions rather than a unique affective state, and, on the otber hand, self-reported psychological assessments are exposed to bias  [Diener et al., 2006]  [Reynolds and  Suzuki, 2012]  [  Bota et al., 2019 ] [Shu et al., 2018] .\n\nRegarding the input explanatory variables used for supervised learning, if there is domain knowledge, relevant physiological indicators as those introduced in section II-B can be identified and used for emotion recognition, and further techniques such as dimensionality reduction and feature selection can also be integrated to optimize the model's parameters. Alternatively, feature extraction (or model-free) methods often have shown to outperform models trained on manually selected statistical features. Some of these alternatives include autoencoders -which extract meaningful representations through the compression and reconstruction of unlabeled data; and deep neural networkswhich can automatically extract complex patterns from multimodal signals, and thus be used to learn task-specific representations for each physiological signal in an endto-end manner  [Martinez et al., 2013 ] [Aqajari et al., 2021 ] [Jerritta et al., 2011 ] [Saxena et al., 2020 ] [Yang et al., 2017 ] [Yin et al., 2017] . Nonetheless, some of these model-free methods can easily overfit training data, especially when using deep and complex structures  [Zhang, 2019] .\n\nSeveral studies have investigated the use of these modelfree feature extraction methods for emotion recognition. In one study, stacked convolutional autoencoders were applied to unlabeled cardiac information (derived from electrocardiography) and EDA data to extract generalized latent representations for arousal classification, and outperformed fully supervised methods  [Ringeval et al., 2013] . Although this approach effectively handled the variability of different types of biosignals, it overlooked the complementary nature of multimodal data. In contrast, another study proposed CorrNet -a correlation-based emotion recognition algorithm that first extracted intramodal features (using separate convolutional autoencoders) and then computed covariance as well as crosscovariance between modalities to capture intermodal features  [Sharma et al., 2019] . However, this unsupervised method did not incorporate supervised signals during pre-training, which may have negatively impacted its performance.\n\nOther research employed sequential machine learning algorithms like Long Short-Term Memory (LSTM) networks to model the relationship between input signals and emotions  [Hasanzadeh et al., 2021 ] [Soleymani et al., 2015 ] [Wu et al., 2019b ] [Ma et al., 2019] . However, such algorithms require fine-grained emotion labels for training. When using post-stimuli labels to recognize these fine-grained emotions, the lack of information about which instances specifically correspond to these emotions can lead to overfitting and temporal ambiguity  [Zhang et al., 2020 ] [Wu et al., 2019a ] [Zhou, 2025] .\n\nIn recent years, unsupervised learning has gained attention as a promising solution to the scarcity of reliable labeled data in emotion recognition, with studies demonstrating its capacity to distinguish between valence and arousal. Some of these approaches used algorithms such as k-means, Gaussian Mixture Models (GMM) and Hidden Markov models (HMM) for binary classification  [Neumann and Vu, 2019 ] [Huelle et al., 2014 ] [Lian et al., 2019]  [  Eskimez et al., 2018 ] [Tervonen et al., 2020]  [  Kang et al., 2020]  [  Zhuang et al., 2014 ] [Vildjiounaite et al., 2017] .  [Deldari et al., 2020]  [  Kumar et al., 2021 ] [Liang et al., 2019] .\n\nAlthough promising, unsupervised learning has several limitations. First, the lack of labels makes it difficult to interpret the predicted emotional states. For example, when clustering physiological data, it's often unclear which cluster corresponds to which emotion, as the clusters do not map directly to emotional categories. This can be partially addressed using pseudolabels or by seeding the algorithm with prior knowledge  [Basu et al., 2002] . Second, unsupervised models generally underperform compared to supervised approaches in emotion recognition tasks. Third, while popular clustering algorithms like k-means, DBSCAN, Affinity Propagation, and BIRCH assign data points to mutually exclusive clusters, emotional states are often ambiguous and overlapping. These methods do not allow for degrees of membership, which limits their ability to reflect the continuous nature of emotions  [Russell, 1980] .\n\nThis manuscript introduces a novel deep-seeded clustering framework designed to address key limitations of both supervised and unsupervised approaches in emotion recognition. Our model employs a sequence-to-sequence autoencoder  [Sutskever et al., 2014]  to extract features from physiological signals, which are jointly trained with a deep clustering algorithm. By seeding clusters with self-reported emotional assessments or contextual stimuli, the model effectively maps responses to the four quadrants of Russell's circumplex model of affect. The proposed framework processes EDA, BVP, and skin temperature signals collected via the Empatica E4 wristband -and across multiple publicly available datasetsusing a deep-seeded c-means clustering method for emotion recognition.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Methodology",
      "text": "In this section, we introduce the theoretical framework upon which our proposed deep-seeded clustering model for emotion recognition is based. Our approach relies on three main components: pre-processing, representation learning and clustering. Theoretical and mathematical background regarding each of those steps is presented in the following subsections.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Pre-Processing",
      "text": "A pre-processing step was applied prior to the learning stage, which included smoothing, min-max scaling, and resampling. A Savitzky-Golay filter with polynomial order 1 was used for smoothing, to mitigate errors and remove outliers. Min-max scaling to a 0-1 range was used to accelerate feature extraction and improve clustering results, leading to faster convergence and increased probability of finding a global minimum  [Doherty et al., 2007] . As the Empatica E4 wearable device  [Empatica, 2015]  uses sampling frequencies of 4 Hz for the EDA and Skin Temperature signals and 64 Hz for the BVP signal, we opted for upsampling to the largest frequency, i.e., 64 Hz, to minimize information loss.\n\nWe applied sliding windows of 600 samples (roughly 10 seconds), overlapping with a step of 1 sample, and considered only those that were assigned to a single emotional context (self-reported or stimulus-based; see V-A).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Aes For Representation Learning",
      "text": "Rather than using hand-crafted features, the proposed model uses an unsupervised algorithm to learn an efficient latent representation of the input data  [Xu et al., 2016] , particularly an autoencoder. In detail, an autoencoder consists of a multilayer feed-forward neural network, which includes the input layer, a hidden layer and the output layer, without any directed loops or cycles  [Ju et al., 2015] . It aims to minimize the discrepancy between input and reconstruction by learning an encoder and a decoder, yielding a set of weights W and biases b  [Xu et al., 2014] .\n\nDefining the unlabeled training data as x (1) , ..., x (n) , n being the number of training samples, and the target values of this neural network as y (1) , ..., y (n) , x (i) ∈ R p , y (i) ∈ R p , ∀i, an autoencoder thus aims at setting the target values to be equal to the inputs, using y (i) = x (i) , ∀i  [Ju et al., 2015] . This is equivalent to saying that this algorithm tries to learn an approximation to the identity function, so as to output a reconstructed x that is similar to x, where this discrepancy to be minimized is described by an average sum-of-squares error term in the cost function, as given by:\n\nwhere L AE corresponds to the reconstruction loss that the AE aims to minimize, and h W,b (x (i) ) corresponds to the result of the output layer, for each input training pattern x (i) , thus equivalent to its output reconstruction  [Ju et al., 2015] . We can rewrite this equation as:\n\nIf there is structure in the data (if it is not completely random), by limiting the number of hidden units or enforcing a sparsity constraint, the network can then detect it; particularly, if the number of hidden units is much smaller than the input and output layer, the network is forced to learn a compressed representation of the input, thus allowing to identify existing correlations in the input features, as mentioned  [Ju et al., 2015] .\n\nWhile the vanilla autoencoder consists of this simpler multi-layer feed-forward network, more sophisticated encoder and decoder layers can be designed and implemented in its place, particularly long short-term memory networks (LSTMs) and gated recurrent units (GRUs), which are frequently employed in time-series classification problems  [Sutskever et al., 2014] . Unlike standard feed-forward neural networks, these have feedback connections, allowing them to exploit temporal dependencies across sequences of data  [Ma et al., 2019][Hochreiter and Schmidhuber, 1997] .\n\nLSTMs were originally designed to handle the issue of vanishing or exploding gradients that can occur when training traditional recurrent neural networks, and they are particularly effective for learning and predicting on sequence data due to their ability to retain memory over long sequences  [Hochreiter and Schmidhuber, 1997] . GRUs, on the other hand, use a gating mechanism which is very similar to LSTMs, but are faster to train as they require fewer parameters and only two gates: the update gate (u t ), which tunes the update speed of the hidden state, and the reset gate (r t ), which decides how much of the past information to forget by resetting parts of the memory  [Cahuantzi et al., 2023 ][Cho et al., 2014] .\n\nSince our goal is to to perform feature extraction and signal reconstruction of sequential data, we use sequence-tosequence (S2S) autoencoders integrating encoder and decoder GRU layers, to efficiently reconstruct the EDA, BVP and Skin Temperature signals while minimizing the number of parameters  [Sutskever et al., 2014] . Equation 2 thus becomes:\n\nwhere δ is the window size (in our case, δ = 600 -see III-A), and x t = x t:t+δ , ∀t >= 0.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Clustering",
      "text": "In line with our motivation to develop a model which can be used in real-life settings, requiring minimal labeling of the data, the model we propose combines the AE architecture with c-means -an extension of the popular unsupervised learning algorithm k-means  [Zhang et al., 2019] .\n\nThe goal of k-means is to cluster the different data points in meaningful groups such that each data point is closer to the nearest point (to which the distance should ideally be small) in the same cluster than to all the ones in the remaining clusters, resembling a classification problem (where each cluster can be seen as a different category or, in our case, a different affective state/emotion), but without the need for any expert labeling. Formally, this corresponds to a global optimization problem where the cost function to be minimized is given by  [Jain, 2010] :\n\nwhere T = {x 1 , ..., x n } corresponds to the feature vectors to be clustered (the training set), T i to the ones assigned to the i-th cluster, µ i to the centroid of each i-th cluster and K to the total number of clusters. Assignment to a cluster and centroid update are thus iteratively performed in K-means, in order to find K centroids (corresponding to the center of the clusters) such that after assigning each vector to the nearest center, the sum of squared distances from the centers are minimized  [Jain, 2010] ,  [Leung and Malik, 2001 ], which we be formalized as:\n\nWe can rewrite equation 4 as:\n\ndefining s tk as the assignment vector s t ∈ {0, 1} such that:\n\nC-means generalises k-means by assigning data points to fuzzy clusters instead of mutually exclusive clusters. According to  [Zhang et al., 2019] , observation x : x ∈ T i belongs to cluster i with degree:\n\nwith γ > 0. The higher is u k , the higher is the probability that x belongs to cluster k. Moreover, centroid c k of fuzzy cluster k is defined by:\n\nwhere\n\nC-means' estimation steps are the same as those of k-means. Thus, the algorithm works by successively updating the assignment vectors from equation 9 and the centroids from equation 10, minimizing the loss:",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "D. Deep-Seeded Clustering",
      "text": "Considering that there are multiple solutions to the optimization problem of minimizing the AE reconstruction loss in equation 3, additional criteria can be introduced to ensure that the learned latent representation is the most suitable to the problem at hand. In this case, since our ultimate goal is to perform emotion recognition through clustering, the features learned at this step should also maximize the quality of the clusters found later on through c-means. In our proposed method, we thus jointly estimate the feature representation and clustering models by setting our loss function to be the sum of both parts:\n\nThe output of the hidden layer of the autoencoder is thus given as input for c-means clustering, so as to label the resulting D-dimensional encoded vectors (in our case, considering D = 30, see V-A), while the parameters of the two networks are updated together, at each training epoch of the full model, through backpropagation. The proposed model architecture is depicted in Figure  1 .\n\nCentroid initialization can be done by simply picking a set of random feature vectors, but the algorithm can converge to different solutions depending on the initialization. Rather than following this approach, we initialized all data points at training time according to contextual or else selfreported data (see V-A). Indeed, considering that c-means is an unsupervised model, the resulting clusters are not meaningful unless a seeding method is used, as they only provide information about which points belong to the same group, but not about which group corresponds to each class/emotion  [Basu et al., 2002 ][Bair, 2013] .\n\nIn the self-reported case, if individuals assess that an emotion k * is dominant from T 1 to T 2 , physiological signal windows (x t ) T2 t=T1 are likely to reflect it. Thus, for cmeans clustering, we initialize the assignment probabilities as u tk * = 1, u tk = 0, ∀t ∈ [T 1, T 2], k ̸ = k * . Thereby, emotion k * defines the pseudolabel of cluster k * . Since this is a retrospective label, one should expect that not all time points within that interval would reflect that emotion k * , or at least not with the same intensity. At each iteration of the model, the assignment probabilities are thus adjusted so as to minimize the combined loss from AE and c-means, so that the final probabilities are heterogeneous and reflect cluster membership, i.e. the degree to which each emotion is present. The same reasoning applies to the contextual approach, where self-reported labels are replaced by the stimuli.\n\nThis procedure of centroid initialization works as seeding the model, as, by initializing the different clusters in this way, we can claim that the data points (in this case, the different time windows) assigned to the cluster of each respective centroid (which is also updated throughout time) also belong to that same emotion, and thus extract metrics of semi-supervised model performance (predicted against ground truth).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Iv. Datasets",
      "text": "To evaluate the accuracy and robustness of the presented method, three distinct datasets from the literature were considered (WESAD  [Schmidt et al., 2018] , Stress-Predict  [Iqbal et al., 2022]  and CEAP-360VR  [Xue et al., 2023] ).\n\nIn the case of the WESAD dataset, the goal was to elicit in 17 healthy participants three distinct affective states -neutral, stress (induced with the The Trier Social Stress Test, or TSST -a widely used method in psychology and neuroscience to make participants feel stressed in a controlled and safe way) and amusement (using a set of funny video clips). Although the dataset also includes two guided meditation periods, these were intended to facilitate de-excitation following the stress and amusement conditions. As the emotional response during meditation likely reflects a blend of the primary states rather than a distinct affective category, these periods were not treated as separate affective states for our analysis.\n\nThe full WESAD protocol lasted approximately two hours and included two variations: in one, the stress condition was placed between the two meditation periods; in the other, the amusement condition took that position. These variations were alternated among the participants to counterbalance potential order effects. At the end of each condition, participants reported their emotional state using the Self-Assessment Manikin (SAM) questionnaire  [Bradley and Lang, 1994] . However, as some conditions may not have elicited the intended emotional responses -e.g., a participant might rate the stress condition as high valence and high arousal (or amusement-like) instead of low valence and high arousal (or stress-like) -different labelling strategies can be considered. One approach is to use the contextual label based on the experimental condition (e.g., \"stress\"), while another is to use the self-reported emotional label reflecting the participant's actual experience (e.g., \"amusement\"). However, relying solely on self-reports often leads to data collapsing into the same emotional quadrant of the affect circumplex model, resulting in pseudo-labeled datasets with limited class diversity (e.g., only one or two emotional classes). Thus, contextual data was used, and the results of the classification task using the three stimuli can be found in section V-B.\n\nIn the Stress-Predict dataset, 35 healthy volunteers participated in a series of tasks designed to induce stress (the Stroop Color Test, the Trier Social Stress Test, and the Hyperventilation Provocation Test), each separated by a rest period. Participants also completed questionnaires aimed at inducing stress levels comparable to those encountered in daily life. The entire protocol lasted approximately one hour and was designed to elicit two distinct affective states: neutral (baseline) and stress. The results of the classification task using these two contextual \"stimuli\" labels are presented in Section V-B.\n\nIn the CEAP360-VR dataset, 32 healthy volunteers completed a VR task lasting approximately one hour. Using a commercial VR headset, participants watched eight validated 360°affective video clips selected from  [Li et al., 2017]  two representing each quadrant of the circumplex model of affect. While viewing the clips, participants annotated their perceived valence and arousal using a joystick controller (moving it along a spatial representation of the four-quadrant model). These annotations were recorded during and after each video, although only retrospective ratings were considered in this study.\n\nAs each video of the CEAP360-VR dataset was clipped to a 60-second duration, the number of usable data points was limited, making multi-class classification infeasible. To address this, the four emotional quadrants were regrouped into two classes, transforming the task into a binary classification problem: stress vs. not stress, based either on the contextual \"stimuli\" labels or the retrospective self-reported \"emotional\" labels. The results for both approaches are presented and discussed in Section V-B.\n\nA sample image of the timeline and collected physiological signals for each dataset is presented in Figure  2  while summary information can be found in Table  I .  V. MODEL APPLICATION In this section, we present and discuss the model's implementation details and classification performance results on each dataset, considering a within-subject approach.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "A. Experimental Setup",
      "text": "The deep-seeded clustering model used in this work was trained with the library Pytorch for Python  [Paszke et al., 2019]  [  Van Rossum and Drake Jr, 1995] .\n\nThe encoder and decoder used in the autoencoder model were formed by two gated recurrent units (GRU) layers each, followed by normalisation layers to reduce the training time and improve clustering  [Doherty et al., 2007 ][Ba et al., 2016]  of the latent vectors and a fully connected layer with linear activation at the end of the decoder layer to restore the input dimensions. A hidden size of 30 was used for the autoencoder, i.e., a 30-dimensional embedding vector extracted from the encoder layer was given as input to c-means, and the joint model was trained over 100 epochs.\n\nA within-subject approach was considered for the model. For each subject, the dataset was split in 10 folds for crossvalidation, so that each rotating split was used for testing while the remaining 90% of data was used for training. Two approaches were considered for this split: either using sequential, or non-sequential folds. In the former, the training dataset was downsampled by a factor of 10 -both to avoid overfitting and reduce training time -, while in the latter a downsampling factor of 2000 was considered -to minimize any partial overlap between the training and test sets.\n\nThe model was initialized by pre-training during a single epoch where only the autoencoder's reconstruction loss was considered, before proceeding to training the deep-seeded clustering model as a whole. The model's parameters were fixed at γ = 0.1 and learning rates of η = 5 * 10 -5 and η = 1 * 10 -6 for training and pre-training, respectively. Model performance was estimated using the accuracy, precision and recall metrics:\n\nP recision = T P T P + F P (15)\n\nwhere T P , T N , F P and F N stand for the number of true positives, true negatives, false positives and false negatives, respectively. The experimental details for implementation, i.e., the model parameters, are summarized in Table  II . As previously mentioned, seeding (for both centroid initialization and ground truth labeling) was based on either context or self-reported data, depending on the dataset. The results of those different experiments are presented in section V-B.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "B. Results And Discussion",
      "text": "In this section, we present model performance results, considering the accuracy, precision and recall metrics, as well as silhouette scores. To show that the model did not overfit to the training data, both training and test set results are shown. Both sequential and non-sequential splits were considered for training/testing the model. Since the second approach led to better performance across all datasets, we focus on it, although accuracy results for the sequential split are also presented. Results using the non-sequential approach are summarized in Table  III , for the three different datasets considered. 1) WESAD: Contextual data was used for seeding in WESAD, and the three different classes were considered: baseline, stress and amusement. The 3-class classification problem attained an accuracy of 79.3% using sequential CV and 80.7% using non-sequential CV, showing good performance -similar to the 80.3% obtained in the original paper  [Schmidt et al., 2018] . The confusion matrix for the nonsequential approach (averaged across subjects) is shown in Figure  3 , illustrating how, in WESAD, stress and amusement are easily distinguished from the remaining contexts, while the baseline context is more frequently misclassified as amusement.\n\nFig.  3 . Confusion matrix for WESAD using non-sequential 10-fold CV (within-subject), averaged across all subjects, considering contextual \"stimuli\" labels for seeding.\n\nFigure  4  illustrates the accuracy results in WESAD for individual subjects. It shows that, apart from subject S8, both the median accuracy and the 25% quartile (across folds) is always above 60%, and most often above 75%, at testing. The model performance results are similar at training and testing as the two ranges intersect and there is no significant difference between median results for training versus testing. Thus, the model shows no signs of overfitting. This result is also in line with the silhouette score results presented in Figure  5 , which illustrate how the model was able to extract meaningful and distinct clusters (one for each context) for most subjects (mean and median silhouette scores above 0.5), except S8 and S16, for which the quality of the clusters as given by this metric is extremely low. To inspect the reason behind the model behaving differently for subject S8, its confusion matrix is presented in Figure  6 . This illustrates how, for subject S8, the baseline context is even more frequently misclassified as amusement than baseline, explaining the poor silhouette scores obtained for this participant. This is in line with the participant's selfreported data, where amusement and baseline were not only assigned to the same quadrant of the circumplex model of affect (\"calmness\"), but were also, in fact, scored with the exact same valence and arousal levels (valence: 7, arousal: 3) -unlike stress, which was scored with a lower valence and higher arousal level (valence: 5, arousal: 7).\n\nThe low silhouette scores obtained for S16 are also in line with the self-reported data. Indeed, only two affective states were reported by the participant as well, with amusement and baseline falling once again into the same quadrants. Since half of the data in the baseline was still correctly identified, model accuracy remained high (above 60%), as shown in Figure  7 . 2) Stress-Predict Dataset: Contextual data was also used for seeding in Stress-Predict, and two different classes were considered: stress and baseline. The accuracy obtained for this binary classification problem was close to 60.5% using sequential CV and 64.2% using non-sequential CV, showing reasonable performance -close to the 67% obtained in the original paper  [Iqbal et al., 2022] .\n\nThe confusion matrix (averaged across subjects) is shown in Figure  8 , illustrating that, while baseline contexts are more easily detected, a larger fraction of stress contexts are misclassified as baseline (not stress). Notably, three different stress induction procedures were used in this dataset, including TSST (similar to WESAD) but also the Hyperventilation Provocation Test and the Stroop Test, which might not have been as effective. The fact that different durations were considered for the three stress induction paradigms explains why the nonsequential leads to better performance results compared to the sequential approach. Indeed, in the non-sequential approach, data points from all stress contexts are used for training, while in the sequential approach this distribution is unbalanced, and can lead to out-of-distribution testing.\n\nFigure  9  illustrates the accuracy results in Stress-Predict for individual subjects. It shows that, for most subjects, both the median accuracy and the 25% quartile (across folds) are above 60% at testing. For 12 out of the 35 subjects (S01, S05, S14, S16, S20, S21, S23, S25, S28, S30, S33, S34), both are below 60%, showing poor model performance, while for 5 others (S09, S10, S15, S19, S26) the median value is above 60% but the 25% quartile is below this threshold.\n\nThe model performance results are similar at training and testing as the two ranges most often intersect and there is no significant difference between median results for training versus testing. Thus, the model shows no signs of overfitting. To understand whether the poor model performance results on some participants could be due to the fact that no hyperparameter tuning was performed, we did a sensitivity analysis testing different model parameters. These results are presented in section V-C.\n\n3) CEAP360-VR Dataset: Self-reported retrospective data was used for seeding in CEAP360-VR, and, after regrouping (due to size limitations, as only 8 videos are used, of around 1min duration each), two different classes were considered: stress and non-stress. This led to a binary classification problem (rather than multi-class), for which the model attained an accuracy of 54.6% using sequential CV and 61.0% using nonsequential CV, showing reasonable performance -while below the 67.1% obtained in the original paper  [Xue et al., 2023] .\n\nNotably, only 29 out of the 32 subjects in the dataset were considered, as the remaining 3 did not assign any of the videos to the stress quadrant. The confusion matrix (averaged across subjects) is shown in Figure  10 .\n\nWhile ensuring sufficient data points from all classes, the regrouping into 2 single categories resulted in class imbal-  ance, with the majority of data points being pseudo-labeled as non-stress (both in the emotional and contextual seeding approaches). In some cases, this led the model to simply classify all data samples as the most frequent class (nonstress), achieving good accuracy results even though it could not learn to identify and distinguish between the different affective states elicited by the videos (e.g. for participant P32). This explains why, even though the overall model accuracy is reasonable, low precision and recall metrics were obtained, as previously summarized in Table  III .\n\nThe accuracy results for all individual subjects are presented in Figure  11 . The contextual \"stimuli\" seeding approach was also tested, but led to worse model performance (both globally and considering only this subset of 29 participants).",
      "page_start": 7,
      "page_end": 9
    },
    {
      "section_name": "C. Sensitvity Analysis",
      "text": "As mentioned in section V-B2, a sensitivity analysis was made considering the Stress-Predict dataset, in order to assess whether poor model performance results in some participants could be due to the use of non-optimal model parameters. Different values for sequence length (previously 600) and embedding size (previously 30) were tested. In line with the research from  [Ekman and Revealed, 2007] , who concluded that the duration of emotion typically ranges from 0.5s to 4s, we tested shorter sequence lengths (128 and 256, equivalent to 1s and 2s), and also a longer duration of 15s (960 samples) which should reflect mood, rather than emotion. The results considering those different sequence lengths are shown in Figures 12 (accuracy) and 13 (silhouette scores).\n\nWhile the average classification results across subjects for each fixed parameter remain close to 64.2%, this change is more significant if, instead, the optimal model parameters (out of the 4 tested sequence lengths) are selected for each particular subject -in line with the fact that the model was trained using a within-subject approach. This results in an average accuracy of 66.7%, proving that model performance is largely dependent on the selected parameters. The individual subject results for the four different sequence lengths are presented in Table  IV .\n\nThe results considering different embedding sizes, while keeping the sequence length fixed at the original 600 samples, are shown in Figures 14 (accuracy) and 15 (silhouette scores). Once again, while the average classification results across subjects for each fixed parameter remain close to 64.2%, it increases up to 65.7% when selecting the optimal model parameters (out of the 3 tested embedding sizes) for each particular subject. The individual subject results for the four different sequence lengths are presented in Table  V .\n\nWe conclude that model performance could be optimized if considering hyperparameter tuning. Thus, future work on this or other datasets alike should accommodate this step.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "In this work, we proposed and tested an end-to-end deep learning framework for emotion recognition that requires minimal supervision (i.e., labels) and no domain knowledge. We obtained reasonable to good model performance on three different validated datasets from the literature. A sensitivity analysis also showed that model performance could be improved using different parameters (e.g. sequence length and embedding size), so that hyperparameter tuning should be considered in future applications of the model. One other limitation of the model comes from the fact that some assumptions are required for data clustering with k-means, and consequently, c-means. Particularly, k-means assumes that each cluster is approximately isotropic and well represented by a prototype (the centroid); that each data point is closer to the nearest point (to which the distance should ideally be small) in the same cluster than to all the ones in the remaining clusters; and that each of them has a uniform density of data. If these assumptions are not met, the cmeans algorithm might not be able to capture and identify high quality clusters, reflecting in poor silhouette scores, and ultimately jeopardizing the performance of the full model.\n\nDespite these limitations, the model has great potential for application in outdoor and real-life settings, filling a gap on such naturalistic experiments. Indeed, it has proven to perform well on physiological data collected from non-intrusive wearable devices, showing good accuracy results not only in static video-watching contexts, but also in VR paradigms which include movement. As it also requires minimal supervision, it should be appropriate for longitudinal experiments where the number of prompts to the participants should be minimized at the risk of dropout. Thus, future work should be able to apply this model to outdoor, naturalistic, real-life settings.",
      "page_start": 10,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Centroid initialization can be done by simply picking a",
      "page": 5
    },
    {
      "caption": "Figure 1: Diagram illustrating the proposed model architecture.",
      "page": 5
    },
    {
      "caption": "Figure 2: while summary",
      "page": 6
    },
    {
      "caption": "Figure 2: Sample images for each dataset: (a) WESAD, (b) Stress-Predict, and",
      "page": 6
    },
    {
      "caption": "Figure 3: , illustrating how, in WESAD, stress and amusement",
      "page": 7
    },
    {
      "caption": "Figure 3: Confusion matrix for WESAD using non-sequential 10-fold CV",
      "page": 7
    },
    {
      "caption": "Figure 4: illustrates the accuracy results in WESAD for",
      "page": 7
    },
    {
      "caption": "Figure 4: The non-sequential 10-fold CV accuracy for individual subjects of",
      "page": 8
    },
    {
      "caption": "Figure 5: , which illustrate how the model was",
      "page": 8
    },
    {
      "caption": "Figure 5: The non-sequential 10-fold CV silhouette scores for individual subjects",
      "page": 8
    },
    {
      "caption": "Figure 6: This illustrates how, for subject S8, the baseline context",
      "page": 8
    },
    {
      "caption": "Figure 6: Confusion matrix for subject S8 in WESAD, considering contextual",
      "page": 8
    },
    {
      "caption": "Figure 7: Fig. 7. Confusion matrix for subject S16 in WESAD, considering contextual",
      "page": 8
    },
    {
      "caption": "Figure 8: , illustrating that, while baseline contexts are",
      "page": 8
    },
    {
      "caption": "Figure 8: Confusion matrix for Stress-Predict using non-sequential 10-fold CV",
      "page": 9
    },
    {
      "caption": "Figure 9: illustrates the accuracy results in Stress-Predict for",
      "page": 9
    },
    {
      "caption": "Figure 10: While ensuring sufficient data points from all classes, the",
      "page": 9
    },
    {
      "caption": "Figure 9: The non-sequential 10-fold CV accuracy for individual subjects of",
      "page": 9
    },
    {
      "caption": "Figure 10: Confusion matrix for CEAP360-VR using non-sequential 10-fold CV",
      "page": 9
    },
    {
      "caption": "Figure 11: The contextual ”stimuli” seeding approach was",
      "page": 9
    },
    {
      "caption": "Figure 11: The non-sequential 10-fold CV accuracy for individual subjects of",
      "page": 10
    },
    {
      "caption": "Figure 12: The non-sequential 10-fold CV accuracy in the test set, for individual",
      "page": 10
    },
    {
      "caption": "Figure 13: The non-sequential 10-fold CV silhouette scores, for individual",
      "page": 11
    },
    {
      "caption": "Figure 14: The non-sequential 10-fold CV accuracy in the test set, for individual",
      "page": 12
    },
    {
      "caption": "Figure 15: The non-sequential 10-fold CV silhouette scores, for individual",
      "page": 12
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Parameter": "D\nδ\nEpochs\nγ\nηtraining\nηpre−training",
          "Definition": "Embedding size\nSequence length\nTraining epochs\nTradeoff parameter\nLearning rate (training)\nLearning rate (pre-training)",
          "Value": "30\n600 samples\n100\n0.1\n5 ∗ 10−5\n1 ∗ 10−6"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "seq 128": "61.4\n65.2\n67.8\n69.8\n55.6\n76.5\n72.9\n65.7\n59.1\n62.4\n69.3\n56.0\n72.0\n54.0\n58.6\n53.4\n72.2\n76.1\n60.1\n66.0\n58.5\n74.7\n54.7\n75.5\n59.5\n60.6\n73.2\n53.9\n61.9\n55.2\n76.0\n66.0\n66.1\n46.0\n78.2",
          "seq 256": "61.8\n61.1\n67.5\n70.1\n56.2\n77.8\n72.4\n64.6\n63.0\n64.9\n71.3\n55.9\n66.0\n57.1\n53.7\n56.1\n75.0\n77.6\n58.8\n66.5\n55.7\n74.2\n55.9\n75.3\n56.2\n63.4\n72.8\n54.0\n63.4\n56.4\n73.2\n56.9\n65.6\n53.6\n79.8",
          "seq 600": "60.8\n71.9\n66.6\n69.2\n52.0\n70.0\n72.5\n64.4\n61.3\n57.9\n76.6\n63.2\n66.4\n56.6\n60.0\n57.2\n74.7\n75.8\n61.7\n58.8\n53.4\n75.7\n55.8\n75.1\n54.1\n60.1\n71.5\n54.4\n63.8\n57.1\n76.8\n63.4\n57.6\n53.6\n78.5",
          "seq 960": "59.3\n63.6\n67.1\n69.8\n57.9\n77.6\n74.7\n66.9\n54.7\n71.2\n69.9\n56.6\n66.3\n55.6\n50.0\n48.1\n75.0\n77.5\n64.5\n65.3\n51.6\n75.0\n54.2\n73.7\n57.9\n63.2\n74.0\n54.7\n57.0\n52.3\n75.0\n63.0\n64.1\n51.5\n76.9"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "emb 60": "59.0\n70.1\n65.7\n67.9\n53.1\n73.3\n71.4\n63.7\n59.0\n67.2\n76.1\n62.6\n66.8\n56.4\n59.4\n55.5\n70.8\n76.1\n57.6\n61.1\n48.1\n74.9\n55.1\n75.9\n61.2\n58.6\n72.6\n53.4\n62.3\n58.7\n77.8\n61.9\n58.9\n60.0\n82.1",
          "emb 40": "57.2\n69.6\n66.0\n68.6\n50.6\n74.0\n72.2\n65.2\n59.0\n67.7\n75.4\n61.5\n61.8\n57.3\n54.5\n54.8\n73.6\n75.7\n63.1\n65.8\n51.2\n75.0\n56.8\n75.6\n60.4\n61.7\n71.1\n54.4\n64.3\n56.3\n75.1\n60.2\n59.0\n59.4\n79.5",
          "emb 30": "60.8\n71.9\n66.6\n69.2\n52.0\n70.0\n72.5\n64.4\n61.3\n57.9\n76.6\n63.2\n66.4\n56.6\n60.0\n57.2\n74.7\n75.8\n61.7\n58.8\n53.4\n75.7\n55.8\n75.1\n54.1\n60.1\n71.5\n54.4\n63.8\n57.1\n76.8\n63.4\n57.6\n53.6\n78.5"
        }
      ],
      "page": 13
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Multimodal affect recognition in an interactive gaming environment using eye tracking and speech signals",
      "authors": [
        "Alhargan"
      ],
      "year": "2017",
      "venue": "Proceedings of the 19th ACM international conference on multimodal interaction"
    },
    {
      "citation_id": "2",
      "title": "Electrodermal activity based pre-surgery stress detection using a wrist wearable",
      "authors": [
        "Anusha"
      ],
      "year": "2019",
      "venue": "IEEE journal of biomedical and health informatics"
    },
    {
      "citation_id": "3",
      "title": "pyeda: An open-source python toolkit for pre-processing and feature extraction of electrodermal activity",
      "authors": [
        "Aqajari"
      ],
      "year": "2021",
      "venue": "Procedia Computer Science"
    },
    {
      "citation_id": "4",
      "title": "Layer normalization",
      "authors": [
        "Ba"
      ],
      "year": "2016",
      "venue": "Layer normalization",
      "arxiv": "arXiv:1607.06450"
    },
    {
      "citation_id": "5",
      "title": "Semi-supervised clustering methods",
      "authors": [
        "E Bair ; Bair"
      ],
      "year": "2013",
      "venue": "Wiley Interdisciplinary Reviews: Computational Statistics"
    },
    {
      "citation_id": "6",
      "title": "Semisupervised clustering by seeding",
      "authors": [
        "Basu"
      ],
      "year": "2002",
      "venue": "Proceedings of the nineteenth international conference on machine learning"
    },
    {
      "citation_id": "7",
      "title": "A review, current challenges, and future possibilities on emotion recognition using machine learning and physiological signals",
      "authors": [
        "Bota"
      ],
      "year": "2019",
      "venue": "IEEE access"
    },
    {
      "citation_id": "8",
      "title": "Measuring emotion: The self-assessment manikin and the semantic differential",
      "authors": [
        "Lang ; Bradley",
        "M Bradley",
        "P Lang"
      ],
      "year": "1994",
      "venue": "Journal of Behavior Therapy and Experimental Psychiatry"
    },
    {
      "citation_id": "9",
      "title": "A comparison of lstm and gru networks for learning symbolic sequences",
      "authors": [
        "Cahuantzi"
      ],
      "year": "2009",
      "venue": "Science and Information Conference"
    },
    {
      "citation_id": "10",
      "title": "On the properties of neural machine translation: Encoderdecoder approaches",
      "authors": [
        "Cho"
      ],
      "year": "2014",
      "venue": "Frontiers in neuroscience",
      "arxiv": "arXiv:1409.1259"
    },
    {
      "citation_id": "11",
      "title": "Espresso: Entropy and shape aware time-series segmentation for processing heterogeneous sensor data",
      "authors": [
        "Deldari"
      ],
      "year": "2020",
      "venue": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies"
    },
    {
      "citation_id": "12",
      "title": "Beyond the hedonic treadmill: revising the adaptation theory of well-being",
      "authors": [
        "Diener"
      ],
      "year": "2006",
      "venue": "American psychologist"
    },
    {
      "citation_id": "13",
      "title": "Unsupervised learning with normalised data and non-euclidean norms",
      "authors": [
        "Doherty"
      ],
      "year": "2007",
      "venue": "Applied Soft Computing"
    },
    {
      "citation_id": "14",
      "title": "The non-sequential 10-fold CV silhouette scores, for individual subjects of the Stress-Predict dataset, considering different model parameters (embedding sizes: 15(a)",
      "authors": [
        "Fig"
      ],
      "venue": "The non-sequential 10-fold CV silhouette scores, for individual subjects of the Stress-Predict dataset, considering different model parameters (embedding sizes: 15(a)"
    },
    {
      "citation_id": "15",
      "title": "Recognizing faces and feelings to improve communication and emotional life",
      "authors": [
        "Ekman",
        "P Revealed ; Ekman",
        "E Revealed"
      ],
      "year": "2007",
      "venue": "Recognizing faces and feelings to improve communication and emotional life"
    },
    {
      "citation_id": "16",
      "title": "E4 Wristband User's Manual",
      "authors": [
        "Empatica"
      ],
      "year": "2015",
      "venue": "E4 Wristband User's Manual"
    },
    {
      "citation_id": "17",
      "title": "Unsupervised learning approach to feature analysis for automatic speech emotion recognition",
      "authors": [
        "Eskimez"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "18",
      "title": "Perspectives from affective science on understanding the nature of emotion",
      "authors": [
        "E Fox ; Fox"
      ],
      "year": "2018",
      "venue": "Brain and neuroscience advances"
    },
    {
      "citation_id": "19",
      "title": "Advances in electrodermal activity processing with applications for mental health",
      "authors": [
        "Greco"
      ],
      "year": "2016",
      "venue": "Advances in electrodermal activity processing with applications for mental health"
    },
    {
      "citation_id": "20",
      "title": "Continuous emotion recognition during music listening using eeg signals: a fuzzy parallel cascades model",
      "authors": [
        "Haag"
      ],
      "year": "1997",
      "venue": "Tutorial and research workshop on affective dialogue systems"
    },
    {
      "citation_id": "21",
      "title": "Unsupervised learning of facial emotion decoding skills",
      "authors": [
        "Huelle"
      ],
      "year": "2014",
      "venue": "Frontiers in human neuroscience"
    },
    {
      "citation_id": "22",
      "title": "Stress monitoring using wearable sensors: A pilot study and stress-predict dataset",
      "authors": [
        "Iqbal"
      ],
      "year": "2022",
      "venue": "Stress monitoring using wearable sensors: A pilot study and stress-predict dataset"
    },
    {
      "citation_id": "23",
      "title": "Data clustering: 50 years beyond k-means",
      "authors": [
        "A Jain ; Jain"
      ],
      "year": "2010",
      "venue": "Pattern recognition letters"
    },
    {
      "citation_id": "24",
      "title": "Analysis of physiological signals for recognition of boredom, pain, and surprise emotions",
      "authors": [
        "Jang"
      ],
      "year": "2015",
      "venue": "Journal of physiological anthropology"
    },
    {
      "citation_id": "25",
      "title": "Physiological signals based human emotion recognition: a review",
      "authors": [
        "Jerritta"
      ],
      "year": "2011",
      "venue": "2011 IEEE 7th international colloquium on signal processing and its applications"
    },
    {
      "citation_id": "26",
      "title": "Stress classification using k-means clustering and heart rate variability from electrocardiogram",
      "authors": [
        "Kang"
      ],
      "year": "2020",
      "venue": "Int. J. Bio. Biomed. Eng"
    },
    {
      "citation_id": "27",
      "title": "Autonomic nervous system activity in emotion: A review",
      "authors": [
        "S Kreibig ; Kreibig"
      ],
      "year": "2010",
      "venue": "Biological psychology"
    },
    {
      "citation_id": "28",
      "title": "Genetically optimized fuzzy c-means data clustering of iomt-based biomarkers for fast affective state recognition in intelligent edge analytics",
      "authors": [
        "Kumar"
      ],
      "year": "2021",
      "venue": "Applied Soft Computing"
    },
    {
      "citation_id": "29",
      "title": "Representing and recognizing the visual appearance of materials using three-dimensional textons",
      "authors": [
        "Malik Leung",
        "T Leung",
        "J Malik"
      ],
      "year": "2001",
      "venue": "International journal of computer vision"
    },
    {
      "citation_id": "30",
      "title": "A public database of immersive vr videos with corresponding ratings of arousal, valence, and correlations between head movements and self report measures",
      "authors": [
        "Li"
      ],
      "year": "2017",
      "venue": "Frontiers in Psychology",
      "arxiv": "arXiv:1910.13806"
    },
    {
      "citation_id": "31",
      "title": "Emotion recognition using multimodal residual lstm network",
      "authors": [
        "Ma"
      ],
      "year": "2019",
      "venue": "Proceedings of the 27th ACM international conference on multimedia"
    },
    {
      "citation_id": "32",
      "title": "Learning deep physiological models of affect",
      "authors": [
        "Martinez"
      ],
      "year": "2013",
      "venue": "IEEE Computational intelligence magazine"
    },
    {
      "citation_id": "33",
      "title": "Improving speech emotion recognition with unsupervised representation learning on unlabeled speech",
      "authors": [
        "Neumann",
        "M Vu ; Neumann",
        "N Vu",
        "T ; Paszke"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "34",
      "title": "Toward machine emotional intelligence: Analysis of affective physiological state",
      "authors": [
        "R Picard ; Picard",
        "Picard"
      ],
      "year": "2000",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "35",
      "title": "Bias in psychological assessment: An empirical review and recommendations. Handbook of Psychology",
      "authors": [
        "Suzuki ; Reynolds",
        "C Suzuki"
      ],
      "year": "2012",
      "venue": "Bias in psychological assessment: An empirical review and recommendations. Handbook of Psychology"
    },
    {
      "citation_id": "36",
      "title": "Emotion recognition for everyday life using physiological signals from wearables: A systematic literature review",
      "authors": [
        "Ringeval"
      ],
      "year": "1980",
      "venue": "2013 10th IEEE international conference and workshops on automatic face and gesture recognition (FG)"
    },
    {
      "citation_id": "37",
      "title": "Emotion recognition and detection methods: A comprehensive survey",
      "authors": [
        "Saxena"
      ],
      "year": "2020",
      "venue": "Journal of Artificial Intelligence and Systems"
    },
    {
      "citation_id": "38",
      "title": "Introducing wesad, a multimodal dataset for wearable stress and affect detection",
      "authors": [
        "Schmidt"
      ],
      "year": "2018",
      "venue": "Proceedings of the 20th ACM international conference on multimodal interaction"
    },
    {
      "citation_id": "39",
      "title": "Wearable-based affect recognition-a review",
      "authors": [
        "Schmidt"
      ],
      "year": "2019",
      "venue": "Sensors"
    },
    {
      "citation_id": "40",
      "title": "A dataset of continuous affect annotations and physiological signals for emotion analysis",
      "authors": [
        "Sharma"
      ],
      "year": "2019",
      "venue": "Scientific data"
    },
    {
      "citation_id": "41",
      "title": "A review of emotion recognition using physiological signals",
      "authors": [
        "Shu"
      ],
      "year": "2018",
      "venue": "Sensors"
    },
    {
      "citation_id": "42",
      "title": "Analysis of eeg signals and facial expressions for continuous emotion detection",
      "authors": [
        "Soleymani"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "43",
      "title": "Eeg emotion recognition using dynamical graph convolutional neural networks",
      "authors": [
        "Song"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "44",
      "title": "Sequence to sequence learning with neural networks",
      "authors": [
        "Sutskever"
      ],
      "year": "2014",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "45",
      "title": "Personalized mental stress detection with selforganizing map: From laboratory to the field",
      "authors": [
        "Tervonen"
      ],
      "year": "2020",
      "venue": "Computers in Biology and Medicine"
    },
    {
      "citation_id": "46",
      "title": "Unsupervised stress detection algorithm and experiments with real life data",
      "authors": [
        "Van Rossum",
        "Drake Jr",
        "; Van Rossum",
        "G Drake",
        "F Vildjiounaite"
      ],
      "year": "1995",
      "venue": "Progress in Artificial Intelligence: 18th EPIA Conference on Artificial Intelligence"
    },
    {
      "citation_id": "47",
      "title": "From physiological signals to emotions: Implementing and comparing selected methods for feature extraction and classification",
      "authors": [
        "Wagner"
      ],
      "year": "2005",
      "venue": "2005 IEEE international conference on multimedia and expo"
    },
    {
      "citation_id": "48",
      "title": "Continuous emotion recognition in videos by fusing facial expression, head pose and eye gaze",
      "authors": [
        "Wu"
      ],
      "year": "2019",
      "venue": "2019 International conference on multimodal interaction"
    },
    {
      "citation_id": "49",
      "title": "Stacked sparse autoencoder (SSAE) based framework for nuclei patch classification on breast cancer histopathology",
      "authors": [
        "Xu"
      ],
      "year": "2014",
      "venue": "Biomedical Imaging (ISBI), 2014 IEEE 11th International Symposium on"
    },
    {
      "citation_id": "50",
      "title": "Ceap-360vr: A continuous physiological and behavioral emotion annotation dataset for 360 • vr videos",
      "authors": [
        "Xue"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "51",
      "title": "Physiological feature based emotion recognition via an ensemble deep autoencoder with parsimonious structure",
      "year": "2017",
      "venue": "2017 10th International Congress on Image and Signal Processing"
    },
    {
      "citation_id": "52",
      "title": "Deep fuzzy k-means with adaptive loss and entropy regularization",
      "authors": [
        "Zhang"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Fuzzy Systems"
    },
    {
      "citation_id": "53",
      "title": "Multi-modal fusion methods for robust emotion recognition using body-worn physiological sensors in mobile environments",
      "authors": [
        "T Zhang ; Zhang"
      ],
      "year": "2019",
      "venue": "2019 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "54",
      "title": "Corrnet: Fine-grained emotion recognition for video watching using wearable physiological sensors",
      "authors": [
        "Zhang"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "55",
      "title": "Weakly-supervised learning for fine-grained emotion recognition using physiological signals",
      "authors": [
        "Zhang"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "56",
      "title": "Ensemble methods: foundations and algorithms",
      "authors": [
        "Z.-H Zhou ; Zhou"
      ],
      "year": "2025",
      "venue": "Ensemble methods: foundations and algorithms"
    },
    {
      "citation_id": "57",
      "title": "Compact unsupervised eeg response representation for emotion recognition",
      "authors": [
        "Zhuang"
      ],
      "year": "2014",
      "venue": "IEEE-EMBS international conference on Biomedical and Health Informatics (BHI)"
    }
  ]
}