{
  "paper_id": "2310.16956v1",
  "title": "Datastore Design For Analysis Of Police Broadcast Audio At Scale",
  "published": "2023-10-25T19:52:19Z",
  "authors": [
    "Ayah Ahmad",
    "Christopher Graziul",
    "Margaret Beale Spencer"
  ],
  "keywords": [
    "temporal data",
    "datastores",
    "audio analysis",
    "speech emotion recognition",
    "feature extraction"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "With policing coming under greater scrutiny in recent years, researchers have begun to more thoroughly study the effects of contact between police and minority communities. Despite data archives of hundreds of thousands of recorded Broadcast Police Communications (BPC) being openly available to the public, a closer look at a large-scale analysis of the language of policing has remained largely unexplored. While this research is critical in understanding a \"pre-reflective\" notion of policing, the large quantity of data presents numerous challenges in its organization and analysis. In this paper, we describe preliminary work towards enabling Speech Emotion Recognition (SER) in an analysis of the Chicago Police Department's (CPD) BPC by demonstrating the pipelined creation of a datastore to enable a multimodal analysis of composed raw audio files. \n CCS CONCEPTS • Applied computing → Law, social and behavioral sciences; • Information systems → Temporal data; • Computing methodologies → Machine learning.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Data",
      "text": "The data we operated on consisted of a public archive of 160,000 30minute audio files of the CPD's BPC. Each audio file is 30 minutes long and approximately 3.5 MB, totaling approximately 4.8 million minutes and 560 GB for the raw archive. Associated with each audio file was metadata extracted from the name of the file, including dispatch zone and date, and metadata extracted using Voice Activity Detection (VAD), including timestamps of non-silent slices of audio.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Literature Review",
      "text": "To survey the research in Speech Emotion Recognition, we relied heavily on  [1]  to contextualize and summarize pertinent SER models. In this search, we sought out prevalent research that focused on, or was bound by, the following constraints:\n\n(1) Examined elicited emotions during improvised conversations, as opposed to acted emotions (2) Was robust to noise, particularly at a level close to human speech (3) Used a 3-dimensional model of emotion, instead of a categorical model\n\nThese constraints fundamentally allowed us to look closer at the research more applicable to the data we were analyzing. While no model that we examined fulfilled all three requirements,  [3]  was perhaps the closest. There, they created a 3-D Convolutional Recurrent Neural Network (CRNN) for SER. To avoid adding potential bias, we decided to extract audio features and use those as inputs, instead of using human-labeled data.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Pvest Framework",
      "text": "The Phenomenological Variant of Ecological Systems Theory  [7]  serves as the theoretical framework supporting this project. In this context, the framework seeks to identify adaptive and maladaptive coping mechanisms of police officers' stress responses. Thus, we seek to understand how normal communication can contribute to increasing or decreasing the likelihood of an adverse encounter between police and the general public-and more specifically, Law Enforcement Officers (LEOs) and Male Minority Youth (MMY).",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Challenges",
      "text": "This section will discuss challenges associated with the creation of a database, due to the scale, temporality, and silence of the data.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Scale",
      "text": "In expanding the audio into discrete samples, we ended up with approximately 40 million data points. From there, we extracted 26 temporal features, at approximately 183,000 samples per feature-based on the Geneva Minimalistic Acoustic Parameter Set (GeMAPS) [4]-using openSMILE  [5] , resulting in approximately 690,000 data points per file. Using Praat-Parselmouth  [2, 6]  to extract intensity, harmonicity, and pitch for each file resulted in approximately 230,000 data points per feature, per file. Scaling upwards, to include all 160,000 audio files results in approximately 7.2 trillion data points for the raw audio, GeMAPS, and Parselmouth files.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Temporality",
      "text": "When extracting different features from individual files, the default sample rate varies from one program to another. Thus, for each audio file, we have both raw audio data for each 22kHz sampling period and features extracted at differing periods of time.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Silence",
      "text": "Since some files contain silent slices, clustering on data that contains silence could lead to an inherently binary model, explained by one dimension-silence or sound.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Database Design And Implementation",
      "text": "In searching for a database management system (DBMS) that was scalable, and ACID-compliant, extensible, with high levels of concurrency, we decided to use PostgreSQL.\n\nOperating under a 1TB constraint meant that we could not store our raw and extracted data directly in the database because this composition of features exceeded 30 TB. This was in addition to the constraint set by the misalignment in temporality. Thus, we determined to design a datastore, such that raw and extracted features could simultaneously be accessed and preprocessed as inputs to a CRNN. Each file is stored in a specified location, with the locations used instead of the files in the database. Thus, for any feature stored as a column in the database, a script would extract the file locations, and feed them into a clustering algorithm that would parse the given file and cluster the data accordingly. Similarly, for the SER model, a script would perform the same parsing of the files and use the parsed data as inputs to the model.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Conclusion",
      "text": "In this project, we created a framework that enabled easy interoperability with statistical methods for an unbiased large-scale analysis of police broadcast audio for SER, allowing us to do large-scale pre-processing, clustering and PCA on the dataset.",
      "page_start": 2,
      "page_end": 2
    }
  ],
  "figures": [],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "graziul@uchicago.edu\nayahahmad@berkeley.edu"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "Chicago, Illinois, USA"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "mbspencer@uchicago.edu"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "1.1\nData\nABSTRACT"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "The data we operated on consisted of a public archive of 160,000 30-\nWith policing coming under greater scrutiny in recent years, re-"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "minute audio ﬁles of the CPD’s BPC. Each audio ﬁle is 30 minutes\nsearchers have begun to more thoroughly study the eﬀects of con-"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "long and approximately 3.5 MB, totaling approximately 4.8 million\ntact between police and minority communities. Despite data archives"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "minutes and 560 GB for the raw archive. Associated with each au-\nof hundreds of thousands of recorded Broadcast Police Communi-"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "dio ﬁle was metadata extracted from the name of the ﬁle, including\ncations (BPC) being openly available to the public, a closer look"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "dispatch zone and date, and metadata extracted using Voice Activ-\nat a large-scale analysis of the language of policing has remained"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "ity Detection (VAD),\nincluding timestamps of non-silent slices of\nlargely unexplored. While this research is critical\nin understand-"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "audio.\ning a \"pre-reﬂective\" notion of policing, the large quantity of data"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "presents numerous challenges in its organization and analysis."
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "In this paper, we describe preliminary work towards enabling"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "1.2\nLiterature Review"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "Speech Emotion Recognition (SER) in an analysis of the Chicago"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "To survey the research in Speech Emotion Recognition, we relied"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "Police Department’s (CPD) BPC by demonstrating the pipelined"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "heavily on [1] to contextualize and summarize pertinent SER mod-"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "creation of a datastore to enable a multimodal analysis of com-"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "els. In this search, we sought out prevalent research that focused"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "posed raw audio ﬁles."
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "on, or was bound by, the following constraints:"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "CCS CONCEPTS"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "(1) Examined elicited emotions during improvised conversa-"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "• Applied computing → Law, social and behavioral sciences;"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "tions, as opposed to acted emotions"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "• Information systems → Temporal data; • Computing method-"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "(2) Was robust to noise, particularly at a level close to human"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "ologies → Machine learning."
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "speech"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "(3) Used a 3-dimensional model of emotion, instead of a cat-"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "KEYWORDS\negorical model"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "temporal data, datastores, audio analysis, speech emotion recogni-"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "These constraints fundamentally allowed us to look closer at the"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "tion, feature extraction"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "research more applicable to the data we were analyzing. While no"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "ACM Reference Format:\nmodel that we examined fulﬁlled all three requirements, [3] was"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "Ayah Ahmad, Christopher Graziul\n(advisor), and Margaret Beale Spencer"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "perhaps the closest. There, they created a 3-D Convolutional Recur-"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "(advisor). 2021. Datastore Design for Analysis of Police Broadcast Audio at"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "rent Neural Network (CRNN)\nfor SER. To avoid adding potential"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "Scale.\nIn St. Louis\n’21: The International Conference for High Performance"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "bias, we decided to extract audio features and use those as inputs,"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "Computing, Networking, Storage, and Analysis, November 14–19, 2021, St."
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "instead of using human-labeled data."
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "Louis, MO. ACM, New York, NY, USA, 2 pages. https://doi.org/10.****/*****.*******"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "1.3\nPVEST Framework"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "1\nINTRODUCTION"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "The Phenomenological Variant of Ecological Systems Theory [7]"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "In this section, we discuss the data that we operate on, relevant"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "serves as the theoretical framework supporting this project. In this"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "literature that inﬂuenced datastore design choices, and the frame-"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "context, the framework seeks to identify adaptive and maladaptive"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "work that forms the foundation for this project."
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "coping mechanisms of police oﬃcers’ stress responses. Thus, we"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "seek to understand how normal communication can contribute to"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "Permission to make digital or hard copies of all or part of this work for personal or"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "increasing or decreasing the likelihood of an adverse encounter\nclassroom use is granted without fee provided that copies are not made or distributed"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "for proﬁt or commercial advantage and that copies bear this notice and the full cita-"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "between police and the general public—and more speciﬁcally, Law"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "tion on the ﬁrst page. Copyrights for components of this work owned by others than"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "Enforcement Oﬃcers (LEOs) and Male Minority Youth (MMY)."
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "and/or a fee. Request permissions from permissions@acm.org."
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "St. Louis ’21, November 14–19, 2021, St. Louis, MO\n2\nCHALLENGES"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "© 2021 Association for Computing Machinery."
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "This section will discuss challenges associated with the creation of"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "ACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00"
        },
        {
          "Chicago, Illinois, USA\nBerkeley, California, USA\nDevelopment": "https://doi.org/10.****/*****.*******\na database, due to the scale, temporality, and silence of the data."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "St. Louis ’21, November 14–19, 2021, St. Louis, MO": "2.1\nScale",
          "Ahmad, et al.": "REFERENCES"
        },
        {
          "St. Louis ’21, November 14–19, 2021, St. Louis, MO": "",
          "Ahmad, et al.": "[1] Berkehan\nAkçay\nand\nKaya\nOguz.\n2020.\nSpeech\nemotion\nrecognition:"
        },
        {
          "St. Louis ’21, November 14–19, 2021, St. Louis, MO": "In expanding the audio into discrete samples, we ended up with",
          "Ahmad, et al.": ""
        },
        {
          "St. Louis ’21, November 14–19, 2021, St. Louis, MO": "",
          "Ahmad, et al.": "Emotional\nmodels,\ndatabases,\nfeatures,\npreprocessing\nmethods,\nsupport-"
        },
        {
          "St. Louis ’21, November 14–19, 2021, St. Louis, MO": "approximately 40 million data points. From there, we extracted",
          "Ahmad, et al.": "ing modalities,\nand\nclassiﬁers.\nSpeech\nCommunication\n116\n(01\n2020)."
        },
        {
          "St. Louis ’21, November 14–19, 2021, St. Louis, MO": "26 temporal\nfeatures, at approximately 183,000 samples per",
          "Ahmad, et al.": "https://doi.org/10.1016/j.specom.2019.12.001"
        },
        {
          "St. Louis ’21, November 14–19, 2021, St. Louis, MO": "",
          "Ahmad, et al.": "[2]\nPaul Boersma and David Weenink. 2021.\nPraat: Doing Phonetics by Computer"
        },
        {
          "St. Louis ’21, November 14–19, 2021, St. Louis, MO": "ture—based on the Geneva Minimalistic Acoustic Parameter Set",
          "Ahmad, et al.": ""
        },
        {
          "St. Louis ’21, November 14–19, 2021, St. Louis, MO": "",
          "Ahmad, et al.": "[Computer program].\nhttp://www.praat.org/ retrieved 22 July 2021."
        },
        {
          "St. Louis ’21, November 14–19, 2021, St. Louis, MO": "(GeMAPS) [4]—using openSMILE [5], resulting in approximately",
          "Ahmad, et al.": "[3] Mingyi Chen, Xuanji He,\nJing Yang,\nand Han Zhang.\n2018.\n3-D Convo-"
        },
        {
          "St. Louis ’21, November 14–19, 2021, St. Louis, MO": "",
          "Ahmad, et al.": "lutional Recurrent Neural Networks With Attention Model\nfor Speech Emo-"
        },
        {
          "St. Louis ’21, November 14–19, 2021, St. Louis, MO": "690,000 data points per ﬁle. Using Praat-Parselmouth [2, 6] to ex-",
          "Ahmad, et al.": ""
        },
        {
          "St. Louis ’21, November 14–19, 2021, St. Louis, MO": "",
          "Ahmad, et al.": "tion Recognition.\nIEEE\nSignal\nProcessing\nLetters\n25,\n10\n(2018),\n1440–1444."
        },
        {
          "St. Louis ’21, November 14–19, 2021, St. Louis, MO": "tract intensity, harmonicity, and pitch for each ﬁle resulted in ap-",
          "Ahmad, et al.": ""
        },
        {
          "St. Louis ’21, November 14–19, 2021, St. Louis, MO": "",
          "Ahmad, et al.": "https://doi.org/10.1109/LSP.2018.2860246"
        },
        {
          "St. Louis ’21, November 14–19, 2021, St. Louis, MO": "proximately 230,000 data points per feature, per ﬁle. Scaling up-",
          "Ahmad, et al.": "[4]\nFlorian Eyben, Klaus Scherer, Björn Schuller,\nJohan Sundberg, Elisabeth An-"
        },
        {
          "St. Louis ’21, November 14–19, 2021, St. Louis, MO": "",
          "Ahmad, et al.": "dré, Carlos Busso, Laurence Devillers,\nJulien Epps, Petri Laukka, Shrikanth"
        },
        {
          "St. Louis ’21, November 14–19, 2021, St. Louis, MO": "wards, to include all 160,000 audio ﬁles results in approximately 7.2",
          "Ahmad, et al.": ""
        },
        {
          "St. Louis ’21, November 14–19, 2021, St. Louis, MO": "",
          "Ahmad, et al.": "Narayanan,\nand Khiet\nPhuong\nTruong.\n2016.\nThe Geneva Minimalistic"
        },
        {
          "St. Louis ’21, November 14–19, 2021, St. Louis, MO": "trillion data points for the raw audio, GeMAPS, and Parselmouth",
          "Ahmad, et al.": "Acoustic Parameter Set\n(GeMAPS)\nfor Voice Research and Aﬀective Comput-"
        },
        {
          "St. Louis ’21, November 14–19, 2021, St. Louis, MO": "ﬁles.",
          "Ahmad, et al.": "ing.\nIEEE\ntransactions\non\naﬀective\ncomputing\n7,\n2\n(April\n2016),\n190–202."
        },
        {
          "St. Louis ’21, November 14–19, 2021, St. Louis, MO": "",
          "Ahmad, et al.": "https://doi.org/10.1109/TAFFC.2015.2457417 Open access."
        },
        {
          "St. Louis ’21, November 14–19, 2021, St. Louis, MO": "",
          "Ahmad, et al.": "[5]\nFlorian Eyben, Martin Wöllmer, and Björn Schuller. 2010. Opensmile: The Mu-"
        },
        {
          "St. Louis ’21, November 14–19, 2021, St. Louis, MO": "",
          "Ahmad, et al.": "nich Versatile and Fast Open-Source Audio Feature Extractor.\nIn Proceedings"
        },
        {
          "St. Louis ’21, November 14–19, 2021, St. Louis, MO": "2.2\nTemporality",
          "Ahmad, et al.": ""
        },
        {
          "St. Louis ’21, November 14–19, 2021, St. Louis, MO": "",
          "Ahmad, et al.": "of\nthe 18th ACM International Conference on Multimedia (Firenze,\nItaly)\n(MM"
        },
        {
          "St. Louis ’21, November 14–19, 2021, St. Louis, MO": "",
          "Ahmad, et al.": "’10). Association for Computing Machinery, New York, NY, USA,\n1459–1462."
        },
        {
          "St. Louis ’21, November 14–19, 2021, St. Louis, MO": "When extracting diﬀerent features from individual ﬁles, the default",
          "Ahmad, et al.": ""
        },
        {
          "St. Louis ’21, November 14–19, 2021, St. Louis, MO": "",
          "Ahmad, et al.": "https://doi.org/10.1145/1873951.1874246"
        },
        {
          "St. Louis ’21, November 14–19, 2021, St. Louis, MO": "sample rate varies from one program to another. Thus,",
          "Ahmad, et al.": ""
        },
        {
          "St. Louis ’21, November 14–19, 2021, St. Louis, MO": "",
          "Ahmad, et al.": "[6] Yannick Jadoul, Bill Thompson, and Bart de Boer. 2018.\nIntroducing Parsel-"
        },
        {
          "St. Louis ’21, November 14–19, 2021, St. Louis, MO": "audio ﬁle, we have both raw audio data for each 22kHz sampling",
          "Ahmad, et al.": "mouth: A Python interface\nto Praat.\nJournal\nof Phonetics\n71\n(2018),\n1–15."
        },
        {
          "St. Louis ’21, November 14–19, 2021, St. Louis, MO": "",
          "Ahmad, et al.": "https://doi.org/10.1016/j.wocn.2018.07.001"
        },
        {
          "St. Louis ’21, November 14–19, 2021, St. Louis, MO": "period and features extracted at diﬀering periods of time.",
          "Ahmad, et al.": ""
        },
        {
          "St. Louis ’21, November 14–19, 2021, St. Louis, MO": "",
          "Ahmad, et al.": "[7] Margaret Spencer. 2007. Phenomenology and Ecological Systems Theory: Develop-"
        },
        {
          "St. Louis ’21, November 14–19, 2021, St. Louis, MO": "",
          "Ahmad, et al.": "ment of Diverse Groups. Vol. 1.\nhttps://doi.org/10.1002/9780470147658.chpsy0115"
        },
        {
          "St. Louis ’21, November 14–19, 2021, St. Louis, MO": "2.3\nSilence",
          "Ahmad, et al.": ""
        },
        {
          "St. Louis ’21, November 14–19, 2021, St. Louis, MO": "Since some ﬁles contain silent slices, clustering on data that con-",
          "Ahmad, et al.": ""
        },
        {
          "St. Louis ’21, November 14–19, 2021, St. Louis, MO": "tains silence could lead to an inherently binary model, explained",
          "Ahmad, et al.": ""
        },
        {
          "St. Louis ’21, November 14–19, 2021, St. Louis, MO": "by one dimension—silence or sound.",
          "Ahmad, et al.": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "This figure \"sample-franklin.png\" is available in \"png\"(cid:10) format from:": "http://arxiv.org/ps/2310.16956v1"
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Speech emotion recognition: Emotional models, databases, features, preprocessing methods, supporting modalities, and classifiers",
      "authors": [
        "Berkehan Akçay",
        "Kaya Oguz"
      ],
      "year": "2020",
      "venue": "Speech emotion recognition: Emotional models, databases, features, preprocessing methods, supporting modalities, and classifiers",
      "doi": "10.1016/j.specom.2019.12.001"
    },
    {
      "citation_id": "2",
      "title": "Praat: Doing Phonetics by Computer",
      "authors": [
        "Paul Boersma",
        "David Weenink"
      ],
      "year": "2021",
      "venue": "Praat: Doing Phonetics by Computer"
    },
    {
      "citation_id": "3",
      "title": "3-D Convolutional Recurrent Neural Networks With Attention Model for Speech Emotion Recognition",
      "authors": [
        "Mingyi Chen",
        "Xuanji He",
        "Jing Yang",
        "Han Zhang"
      ],
      "year": "2018",
      "venue": "IEEE Signal Processing Letters",
      "doi": "10.1109/LSP.2018.2860246"
    },
    {
      "citation_id": "4",
      "title": "The Geneva Minimalistic Acoustic Parameter Set (GeMAPS) for Voice Research and Affective Computing",
      "authors": [
        "Florian Eyben",
        "Klaus Scherer",
        "Björn Schuller",
        "Johan Sundberg",
        "Elisabeth André",
        "Carlos Busso",
        "Laurence Devillers",
        "Julien Epps",
        "Petri Laukka",
        "Shrikanth Narayanan",
        "Phuong Khiet",
        "Truong"
      ],
      "year": "2016",
      "venue": "IEEE transactions on affective computing",
      "doi": "10.1109/TAFFC.2015.2457417"
    },
    {
      "citation_id": "5",
      "title": "Opensmile: The Munich Versatile and Fast Open-Source Audio Feature Extractor",
      "authors": [
        "Florian Eyben",
        "Martin Wöllmer",
        "Björn Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM International Conference on Multimedia",
      "doi": "10.1145/1873951.1874246"
    },
    {
      "citation_id": "6",
      "title": "Introducing Parselmouth: A Python interface to Praat",
      "authors": [
        "Yannick Jadoul",
        "Bill Thompson",
        "Bart De Boer"
      ],
      "year": "2018",
      "venue": "Journal of Phonetics",
      "doi": "10.1016/j.wocn.2018.07.001"
    },
    {
      "citation_id": "7",
      "title": "Phenomenology and Ecological Systems Theory: Development of Diverse Groups",
      "authors": [
        "Margaret Spencer"
      ],
      "year": "2007",
      "venue": "Phenomenology and Ecological Systems Theory: Development of Diverse Groups",
      "doi": "10.1002/9780470147658.chpsy0115"
    }
  ]
}