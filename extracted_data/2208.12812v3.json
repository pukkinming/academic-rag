{
  "paper_id": "2208.12812v3",
  "title": "Speech Emotion Recognition Using Supervised Deep Recurrent System For Mental Health Monitoring",
  "published": "2022-08-26T01:14:31Z",
  "authors": [
    "Nelly Elsayed",
    "Zag ElSayed",
    "Navid Asadizanjani",
    "Murat Ozer",
    "Ahmed Abdelgawad",
    "Magdy Bayoumi"
  ],
  "keywords": [
    "Speech emotion recognition",
    "intelligent personal assistants",
    "GRU",
    "speech detection",
    "mental health"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Understanding human behavior and monitoring mental health are essential to maintaining the community and society's safety. As there has been an increase in mental health problems during the COVID-19 pandemic due to uncontrolled mental health, early detection of mental issues is crucial. Nowadays, the usage of Intelligent Virtual Personal Assistants (IVA) has increased worldwide. Individuals use their voices to control these devices to fulfill requests and acquire different services. This paper proposes a novel deep learning model based on the gated recurrent neural network and convolution neural network to understand human emotion from speech to improve their IVA services and monitor their mental health.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Mental health is one of the crucial health aspects that must be monitored and treated for better physical health and a safer community and social life  [1] . Mental disorders cases are rising globally. According to the Institute for Health Metrics and Evaluation (IHME), the number of diagnosed individuals with one of the mental disorders globally has exceeded 1.1 billion individuals in 2016  [2] . According to the World Health Organization (WHO), during the first year of the COVID-19 pandemic, depression and anxiety disorders have increased by 25% globally, especially among young people and women. Due to late or unreceived mental care, the number of related suicide has increased as well. The number of suicides has exceeded 700,000, meaning one person every 40 seconds dies by suicidal action related to a mental disorder  [3] . Moreover, the number of mass shootings in the United States has exceeded 200 cases in less than the first half of the year  [4] .\n\nSpeech is the primary form of communication and emotional expression  [5] . From childhood, even before being able to speak correct words, children express their emotions in their ununderstandable talks, such as their happiness and confusion. Juvenile, adults, and elderly individuals also express their emotions in their speech. All individuals express common emotions such as happy, sad, angry, happy, worry, fear, and neutral in their speech. However, different spoken languages produce differences in how these emotions are expressed in the speech tone and voice  [6] ,  [7] . In this paper, we focused on English as the most widely spoken language worldwide  [8] . In addition, the availability of open-access data that addresses the speech emotion recognition problem is using English as the primary language.\n\nThere are several mental disorders that can be identified from individual's emotion changes  [9] ,  [10]  such as depression disorder  [11] ,  [12] , stress disorder  [13] ,  [14] , and anxiety (worry/fear) disorders  [15] ,  [16] . Early diagnostic of mental disorders allows the individual to recieve the correct treatment and prevent sever illensses and even protect fom suisidal action  [17] ,  [18] .\n\nIntelligent Virtual Personal Assistants (IVA)  [19] ,  [20]  is s a software agent that can perform services for an individual based on processing users' questions or commands via text or voice, depending on the IVA design and purpose. The textbased interaction IVA are sometimes called chatbots, primarily when they are assessed by an online chat. The voice-based interaction IVA is also known as an intelligent voice assistant. The voice assistants can recognize the human speech and interpret its commands and questions  [21] . The most popular voice assistance is either embedded in smartphones such as Google's Assistant  [22]  or a standalone device such as Amazon's Alexa  [23] .\n\nSeveral studies address the effect of the IVA devices on individuals' social life  [24] , markating  [25] , and social communication  [26] . However, there are few studies on understanding the user behavior while using intelligent virtual personal assistant devices to improve the user experience. Yang et al.  [27]  attempted to understand how to improve the IVA user experience by investigating the relationship between perceived enjoyment, perceived usefulness, and productrelated characteristics using a user survey. Coskun et al.  [28]  Fig.  1:  The proposed emotion recogniton system for IVA service improvment and mental health monitoring. also used questionnaire data to address the factors affecting IVA user experience. Venkataramanan et al.  [29]  proposed early attempts to address understanding the speech emotion to improve the user experience in voice-based devices. However, their work is implementation and computational expensive with a low accuracy rate.\n\nAs the usage of the IVA has increased globally, in this paper, we propose a novel framework and emotion recognition model for the improvement of IVA user experience and mentoring mental health. The proposed framework employs the IVA to serve as a user assistant as well as a mental health monitoring device. The proposed framework service considers both the user request and emotion to provide a convenient service and improve the user experience. In addition, based on our investigation of the state-of-the-art speech emotion recognition models, we propose a novel deep architecture that combines the 1D-CNN and gated recurrent unit (GRU). The 1D CNN serves as a feature extractor from the audio signal. Therefore, our proposed model does not require any data preprocessing stage. The GRU preserves the temporal information within the audio data. Thus, both the spatial and the temporal information is processed efficiently in the current proposed model.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Ii. Speech Emotion Recognition",
      "text": "Speech emotion recognition is one of the complex problems to solve as the emotional expression is tightly reliant on the spoken language, dialect, accent, and individuals' cultural background. In addition, the audio signal itself preserves the spatial and temporal features of the speech. There have been several attempts to solve the speech emotion recognition problem. The main two scenario approaches:\n\n• Scenario I: designing a model that used speech signal datasets after performing data preprocessing and feature extraction. • Scenario II: designing a model that converts the speech signal to images (spectrograms) and then performing data preprocessing and feature extraction to fit the data to image-based models.\n\nWani et al.  [30]  and Lotfidereshgi et al.  [31]  are examples of using the scenario II methodology to address the speech emotion recognition. The significant drawbacks of scenario II are that the data is processed without any consideration of temporal features in the speech signal, which significantly limits the ability of such models to recognize different emotions from the speech correctly. In addition, the data transformation and feature extraction require additional implementation costs from both hardware and software aspects. For the scenario I, Zhang et al.  [32] , Bhargava et al.  [33] , Krishnan et al.  [34] , and Venkataramanan et al.  [29]  proposed different machine learning and deep learning approaches to solve the speech emotion recognition under the scenario I approach. This approach requires less data transformation and feature extractions compared to the scenario II approaches. However, these models were not considering the temporal information within the audio signal within their approaches. This paper addresses the spatial and temporal information within the speech signal. We proposed a novel model based on the 1D-CNN and the GRU to gather the spatial and temporal information within the learning approach to reduce the data preprocessing stages to fit the model within the intelligent virtual assistant devices and accelerate processing time to achieve an overall improvement of the user experience.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Proposed System",
      "text": "The proposed emotion system for IVA service improvement and mental health monitoring is shown in Figure  1 . In this system. The IVA consists of a speech recognition system and an emotion recognition system that acts together to the service production based on the user's emotion and the requested service. For example, if a user requests from the IVA to listen to a music track and at the same time the emotion recognition system determines that the user is angry, the music selection will be made to find the music track categorized as calming music to improve the user emotional experience. Another example is if the emotion recognition system determines that the user is in a fear emotion, it can advise the user to seek help. In addition, the system collects the recognized emotions through different users' requests throughout the day and provides both the user and the medical provider with the emotional statuses through the IVA using time which helps the medical provider find early alerts of mental disorders and monitor existing mental disorders treatments.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Proposed Recognition Model",
      "text": "This paper proposed a simple design hybrid model of the gated recurrent neural networks (GRU) with a 1D convolution neural network (1D-CNN) support. The GRU was first introduced by Chung et al.  [35]  as a recurrent neural network that has a simpler gated mechanism compared to the long shortterm memory (LSTM), which reduces the implementation requirements from the software and hardware aspects  [36] . Similar to the LSTM and the recurrent neural network (RNN), the GRU uses its previous time step output and current input to calculate the next output  [35] . The GRU consists of two gates: the update z and reset r gates. At time t, the output h t of the GRU can by:\n\n(1)\n\n(2) ht = tanh(W x t + U (r (t) h t-1 ))\n\n(3)\n\nwhere at time step t, W xz , W xr , and W are the feedforward weights of the update gate z t , the reset gate r t , and the output candidate activation ht . U hz , U hr , U are the recurrent weights are of the update gate z t , the reset gate r t , and the output candidate activation ht , respectively. σ is the logistic sigmoid function, tanh is the hyperbolic tangent function, and the symbol denotes the Hadamard (elementwise) multiplication.\n\nThe GRU has successfully achieved significant results in various applications, especially in signal data such as emotion recognition from EEG signal  [37] , sleep stage classification from EEG and EoG  [38] , arrhythmia supraventricular premature beat detection from ECG signal  [39] , music source separation  [40] , and sound event detection  [41] .\n\nGRU can learn the spatial features of the speech signal and the temporal information due to the recurrent behavior. In this paper, we selected the GRU as a competitive recurrent neural network that requires less budget and can achieve comparable results to the LSTM. The 1D-CNN acts as the feature extractor for the 1D speech signal  [42] -  [44] . Therefore, the significant contribution of this model is that it does not require any data preprocessing prior to the model training compared to the state-of-the-art models. The detailed architecture of the proposed model is shown in Figure  2 . The 1D-CNN is followed by a dropout layer with 30% dropout rate that helps to prevent the model of overfitting problem and prevents the all the neurons in the folllowing layer from synchronously optimizing their weights  [45] . The average pooling layer creates a downsampling feature map and helps increase the robustness of the model  [46] . The flatten layer is used to adujust the input size prior to the fully connected dense layer which using a ReLU activation function  [47] . Finally, a Softmax layer is used to determine the models' recognized emotion from the input speech  [48] .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iv. Experimental Results",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Dataset",
      "text": "The proposed model has been empirically evaluated using the Toronto emotional speech set (TESS)  [49]  which is one of the emotion recognition dataset benchmarks. The speech recordings were done in the Toronto area by two actresses who speak English as their first language. The dataset consists of 2800 stimuli. The data has seven different emotion categories: anger, disgust, fear, happiness, pleasant/surprise, sadness, and neutral. The primary significance of this dataset is that the distribution between the number of stimuli per emotion category is equally likely. Figure  3  shows the waveplot for randomly selected audion from each class of the dataset. The model was trained for 20 epochs, and the batch size was set to 20. The categorical cross-entropy function is used as the loss function  [50] . The RMSProp has been used as the model optimization function  [51]  with learning rate lr = 0.001, momentum = 0.0, discounting factor for the history/coming gradient rho = 0.9, and = 1e -07. For the 1D-CNN layer, the number of kernels was set to 128 with size three. The kernel weights have been initialized using the He-uniform initializer  [52]  and the biases has been initialized to zero. The number of unrollments of each of the three GRU layers has been set to 10. The sigmoid (σ) and hyperbolic   tangent (tanh) functions have been used as the activation and recurrent activation functions, respectively. In addition, the kernel weights of the GRU layers have been initialized using Glorot-uniform initializer  [53] . The biases have been initialized by zero. The 1D Average pooling layer padding has been set to valid with a pooling size of three and stride of two.\n\nTable  I  shows a statistical analysis of the proposed model result for each emotion category recognition, including the accuracy, F1-score, the area under the curve (AUC) under the ROC curve for multiple classes  [54] , the error rate, individual classification success index (ISCI)  [55] , optimized precision (OP)  [56] , sensitivity, and Youden index (Y). Figure  4  shows the confusion matrix of the proposed model testing where the categories anger, disgust, fear, happiness, pleasant/surprise, sadness, and neutral are indicated numerically from zero to six, respectively. The Youden index (Y) is calculated as follows:\n\nThe overall testing results including training and testing accuracies, accuracy macro, precision, recall, F1-score, specificity  [57] , the kappa value  [58] , and the number of trainable   II . The kappa value is calulated using the formula:\n\nwhere p o is the observed positive recognition and p e is the expected positive recognition.\n\nThe train versus validation accuracies of the proposed model through the 20 epochs are shown in Figure  5  where the model shows a stable training process during each epoch.\n\nWe compared our model to the state-of-the-art speech emotion recognition models that are based on machine learning recognition methodologies. To perform a fair comparison among the different state-of-the-art and our proposed models, we selected the models that have been evaluated and trained using the TESS emotion dataset. During our other models' investigations, we found that several state-of-the-art research excluded two or more categories of the recognition task of the TESS dataset without an explicit declaration about the reasons for such reduction in the dataset that may significantly affect the model performance and purpose. Therefore, we excluded these models from our comparison due to the lack of information and uneven data usage. Table  III  shows the comparison between our proposed model and other models from the methodology used and the accuracy. Our model outperformed the state-of-the-art models that address the speech emotion recognition task.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Emotion Analyzer",
      "text": "The emotion analyzer stores the recognized emotion and the time of user request. Then based on the user preference settings, it builds a visual report that can be accessed through mobile and a web application, providing a flowchart of emotional changes and how frequently the emotion changes during the day. Connecting the IVA with a medical provider, the report will be sent to the medical provider for further diagnosis or monitoring for mental health disorders based on the emotions and the frequency of the emotional changes that the emotion analyzer report will provide.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "V. Conclusion",
      "text": "Personal emotion is one of the most significant indicators of mental health normality and issues. In this paper, we proposed a novel deep model based on gated recurrent neural networks (GRU) and the one-dimensional convolutional neural networks (1D-CNN) to recognize different emotions from speech. The model can be applied within an intelligent virtual personal assistant to improve the user experience while combining the user request and emotion to provide the appropriate service as well as it can act as a medical health device that monitors the speech emotions and provides the medical provider with emotion changes of the patient while using the smart virtual personal assistant in their daily basics which can help to adjust medical prescriptions and further mental health issues. The significance of the proposed model is that it does not require any additional data preprocessing due to the 1DCNN that behaves to extract the features from the speech signal. Moreover, the recurrent gated unit learns the spatial and temporal features of the speech signal. Thus, the proposed model outperforms the state-of-the-art models that address the emotion recognition problem.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The proposed emotion recogniton system for IVA",
      "page": 3
    },
    {
      "caption": "Figure 3: shows the waveplot for randomly",
      "page": 4
    },
    {
      "caption": "Figure 2: The proposed GRU and 1D-CNN model for speech",
      "page": 4
    },
    {
      "caption": "Figure 3: A sample of each category of audion emotial wave signal in the TESS dataset.",
      "page": 5
    },
    {
      "caption": "Figure 4: The confusion matrix of the proposed model using the",
      "page": 5
    },
    {
      "caption": "Figure 5: The train versus validation accuracy of the proposed",
      "page": 5
    },
    {
      "caption": "Figure 5: where the model",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Statistical\nAnalysis": "",
          "Emotion Category": "Angry\nDisgust\nFear\nHappiness\nSurprise\nSadness\nNeutral"
        },
        {
          "Statistical\nAnalysis": "Accuracy\nF1-score\nError\nrate\nISCI\nOP\nSensativity\nYouden index",
          "Emotion Category": "99.643%\n99.286 %\n98.571%\n97.857%\n98.036%\n96.786%\n98.393%\n0.98837\n0.97531\n0.94595\n0.92308\n0.93413\n0.89655\n0.93617\n0.00357\n0.00714\n0.01429\n0.02143\n0.01964\n0.03214\n0.01607\n0.97701\n0.95091\n0.89224\n0.84615\n0.87371\n0.79690\n0.87243\n0.9848\n0.97527\n0.96989\n0.94483\n0.97622\n0.89626\n0.95964\n0.97701\n0.96341\n0.9589\n0.92308\n0.98734\n0.85714\n0.94286\n0.97701\n0.96132\n0.94864\n0.91063\n0.96655\n0.84648\n0.93265"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Feature": "Train Accuracy\nTest Accuracy\nAccuracy Macro\nPrecision\nRecall\nF1-score\nSpeciﬁcity\nKappa Value\nNo. Trainable Parameters",
          "Value": "99.107%\n94.285%\n98.367%\n0.94285\n0.94285\n0.94285\n0.99047\n0.93329\n3,763,399"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "Venkataramanan et al.\n[29]",
          "Method": "2D CNN with\nGlobal Avg Pooling",
          "Accuracy": "66.000%"
        },
        {
          "Model": "Sundarprasad [59]",
          "Method": "PCA, SVM, and\nMel-Frequeny\nCepstrum Features",
          "Accuracy": "90.000%"
        },
        {
          "Model": "Krishnan et al.\n[34]",
          "Method": "SoA Classsiﬁer and\nEntropy features from\nPrinciple IMF modes",
          "Accuracy": "93.300%"
        },
        {
          "Model": "Lotﬁdereshgi et al.\n[31]",
          "Method": "Liquid State Machine",
          "Accuracy": "82.350%"
        },
        {
          "Model": "Zhang et al.\n[32]",
          "Method": "Kernel\nIsomap",
          "Accuracy": "80.850%"
        },
        {
          "Model": "Zhang et al.\n[32]",
          "Method": "PCA",
          "Accuracy": "72.350%"
        },
        {
          "Model": "Bhargava et al.\n[33]",
          "Method": "Artiﬁcial Neural Nets",
          "Accuracy": "80.600%"
        },
        {
          "Model": "Bhargava et al.\n[33]",
          "Method": "SVM",
          "Accuracy": "80.270%"
        },
        {
          "Model": "Our Model",
          "Method": "GRU & 1D-CNN",
          "Accuracy": "94.285%"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Why mental health matters to global health",
      "authors": [
        "V Patel"
      ],
      "year": "2014",
      "venue": "Transcultural psychiatry"
    },
    {
      "citation_id": "2",
      "title": "Psychological disorder identifying method based on emotion perception over social networks",
      "authors": [
        "T Zhou",
        "G Hu",
        "L Wang"
      ],
      "year": "2019",
      "venue": "International journal of environmental research and public health"
    },
    {
      "citation_id": "3",
      "title": "Suicide data",
      "venue": "Suicide data"
    },
    {
      "citation_id": "4",
      "title": "There have been over 200 mass shootings so far in 2022",
      "authors": [
        "Washington The",
        "Post"
      ],
      "venue": "There have been over 200 mass shootings so far in 2022"
    },
    {
      "citation_id": "5",
      "title": "The evolution of human speech: Its anatomical and neural bases",
      "authors": [
        "P Lieberman"
      ],
      "year": "2007",
      "venue": "Current anthropology"
    },
    {
      "citation_id": "6",
      "title": "Language and culture",
      "authors": [
        "C Kramsch"
      ],
      "year": "2014",
      "venue": "AILA review"
    },
    {
      "citation_id": "7",
      "title": "Language: An introduction to the study of speech",
      "authors": [
        "E Sapir"
      ],
      "year": "1921",
      "venue": "Language: An introduction to the study of speech"
    },
    {
      "citation_id": "8",
      "title": "What are the most spoken languages in the world",
      "authors": [
        "G Julian"
      ],
      "year": "2020",
      "venue": "Retrieved May"
    },
    {
      "citation_id": "9",
      "title": "Disorders of emotion",
      "authors": [
        "D Barlow"
      ],
      "year": "1991",
      "venue": "Psychological inquiry"
    },
    {
      "citation_id": "10",
      "title": "Emotion processing deficits: a liability spectrum providing insight into comorbidity of mental disorders",
      "authors": [
        "M Kret",
        "A Ploeger"
      ],
      "year": "2015",
      "venue": "Neuroscience & Biobehavioral Reviews"
    },
    {
      "citation_id": "11",
      "title": "Depression: the disorder and the burden",
      "authors": [
        "M Reddy"
      ],
      "year": "2010",
      "venue": "Depression: the disorder and the burden"
    },
    {
      "citation_id": "12",
      "title": "When does depression become a mental disorder?",
      "authors": [
        "M Maj"
      ],
      "year": "2011",
      "venue": "The British Journal of Psychiatry"
    },
    {
      "citation_id": "13",
      "title": "Psychobiology of posttraumatic stress disorder",
      "authors": [
        "R Yehuda",
        "A Mcfarlane"
      ],
      "year": "1997",
      "venue": "Psychobiology of posttraumatic stress disorder"
    },
    {
      "citation_id": "14",
      "title": "Posttraumatic stress disorder: the burden to the individual and to society",
      "authors": [
        "R Kessler"
      ],
      "year": "2000",
      "venue": "Journal of Clinical Psychiatry"
    },
    {
      "citation_id": "15",
      "title": "What is an anxiety disorder?",
      "authors": [
        "M Craske",
        "S Rauch",
        "R Ursano",
        "J Prenoveau",
        "D Pine",
        "R Zinbarg"
      ],
      "year": "2011",
      "venue": "Focus"
    },
    {
      "citation_id": "16",
      "title": "Social anxiety disorder",
      "authors": [
        "M Stein",
        "D Stein"
      ],
      "year": "2008",
      "venue": "The lancet"
    },
    {
      "citation_id": "17",
      "title": "Recognition of mental distress and diagnosis of mental disorder in primary care",
      "authors": [
        "S Jencks"
      ],
      "year": "1985",
      "venue": "Jama"
    },
    {
      "citation_id": "18",
      "title": "Reducing suicide: A national imperative",
      "authors": [
        "W Bunney",
        "A Kleinman",
        "T Pellmar",
        "S Goldsmith"
      ],
      "year": "2002",
      "venue": "Reducing suicide: A national imperative"
    },
    {
      "citation_id": "19",
      "title": "Virtual personal assistant",
      "authors": [
        "P Imrie",
        "P Bednar"
      ],
      "year": "2013",
      "venue": "ItAIS"
    },
    {
      "citation_id": "20",
      "title": "Virtual personal assistants: an emerging trend in artificial intelligence",
      "authors": [
        "A Gaggioli"
      ],
      "year": "2018",
      "venue": "Cyberpsychology, Behavior, and Social Networking"
    },
    {
      "citation_id": "21",
      "title": "Alexa, siri, cortana, and more: an introduction to voice assistants",
      "authors": [
        "M Hoy"
      ],
      "year": "2018",
      "venue": "Medical reference services quarterly"
    },
    {
      "citation_id": "22",
      "title": "Alexa vs. siri vs. cortana vs. google assistant: a comparison of speech-based natural user interfaces",
      "authors": [
        "G López",
        "L Quesada",
        "L Guerrero"
      ],
      "year": "2017",
      "venue": "International conference on applied human factors and ergonomics"
    },
    {
      "citation_id": "23",
      "title": "Talk to me: Exploring user interactions with the amazon alexa",
      "authors": [
        "I Lopatovska",
        "K Rink",
        "I Knight",
        "K Raines",
        "K Cosenza",
        "H Williams",
        "P Sorsche",
        "D Hirsch",
        "Q Li",
        "A Martinez"
      ],
      "year": "2019",
      "venue": "Journal of Librarianship and Information Science"
    },
    {
      "citation_id": "24",
      "title": "Ai invading the workplace: negative emotions towards the organizational use of personal virtual assistants",
      "authors": [
        "O Hornung",
        "S Smolnik"
      ],
      "year": "2022",
      "venue": "Electronic Markets"
    },
    {
      "citation_id": "25",
      "title": "The impact of artificial intelligence and virtual personal assistants on marketing",
      "authors": [
        "C Marinchak",
        "E Forrest",
        "B Hoanca"
      ],
      "year": "2018",
      "venue": "Encyclopedia of Information Science and Technology"
    },
    {
      "citation_id": "26",
      "title": "Modeling behavior of virtual actors: a limited turing test for social-emotional intelligence",
      "authors": [
        "A Chubarov",
        "D Azarnov"
      ],
      "year": "2017",
      "venue": "First International Early Research Career Enhancement School on Biologically Inspired Cognitive Architectures"
    },
    {
      "citation_id": "27",
      "title": "Understanding user behavior of virtual personal assistant devices",
      "authors": [
        "H Yang",
        "H Lee"
      ],
      "year": "2019",
      "venue": "Information Systems and e-Business Management"
    },
    {
      "citation_id": "28",
      "title": "Understanding the adoption of voice activated personal assistants",
      "authors": [
        "A Coskun-Setirek",
        "S Mardikyan"
      ],
      "year": "2017",
      "venue": "International Journal of E-Services and Mobile Applications (IJESMA)"
    },
    {
      "citation_id": "29",
      "title": "Emotion recognition from speech",
      "authors": [
        "K Venkataramanan",
        "H Rajamohan"
      ],
      "year": "2019",
      "venue": "Emotion recognition from speech",
      "arxiv": "arXiv:1912.10458"
    },
    {
      "citation_id": "30",
      "title": "Speech emotion recognition using convolution neural networks and deep stride convolutional neural networks",
      "authors": [
        "T Wani",
        "T Gunawan",
        "S Qadri",
        "H Mansor",
        "M Kartiwi",
        "N Ismail"
      ],
      "year": "2020",
      "venue": "2020 6th International Conference on Wireless and Telematics (ICWT)"
    },
    {
      "citation_id": "31",
      "title": "Biologically inspired speech emotion recognition",
      "authors": [
        "R Lotfidereshgi",
        "P Gournay"
      ],
      "year": "2017",
      "venue": "2017 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "32",
      "title": "Speech emotion recognition using an enhanced kernel isomap for human-robot interaction",
      "authors": [
        "S Zhang",
        "X Zhao",
        "B Lei"
      ],
      "year": "2013",
      "venue": "International Journal of Advanced Robotic Systems"
    },
    {
      "citation_id": "33",
      "title": "Improving automatic emotion recognition from speech using rhythm and temporal feature",
      "authors": [
        "M Bhargava",
        "T Polzehl"
      ],
      "year": "2013",
      "venue": "Improving automatic emotion recognition from speech using rhythm and temporal feature",
      "arxiv": "arXiv:1303.1761"
    },
    {
      "citation_id": "34",
      "title": "Emotion classification from speech signal based on empirical mode decomposition and non-linear features",
      "authors": [
        "P Krishnan",
        "A Joseph Raj",
        "V Rajangam"
      ],
      "year": "2021",
      "venue": "Complex & Intelligent Systems"
    },
    {
      "citation_id": "35",
      "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
      "authors": [
        "J Chung",
        "C Gulcehre",
        "K Cho",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
      "arxiv": "arXiv:1412.3555"
    },
    {
      "citation_id": "36",
      "title": "The fpga hardware implementation of the gated recurrent unit architecture",
      "authors": [
        "Z Zaghloul",
        "N Elsayed"
      ],
      "year": "2021",
      "venue": "SoutheastCon 2021"
    },
    {
      "citation_id": "37",
      "title": "EEG-based emotion recognition using spatialtemporal representation via Bi-GRU",
      "authors": [
        "W.-C Lew",
        "D Wang",
        "K Shylouskaya",
        "Z Zhang",
        "J.-H Lim",
        "K Ang",
        "A.-H Tan"
      ],
      "year": "2020",
      "venue": "2020 42nd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)"
    },
    {
      "citation_id": "38",
      "title": "Sleep stage classification based on eeg, eog, and cnn-gru deep learning model",
      "authors": [
        "I Sm",
        "X Zhu",
        "Y Chen",
        "W Chen"
      ],
      "year": "2019",
      "venue": "2019 IEEE 10th International Conference on Awareness Science and Technology"
    },
    {
      "citation_id": "39",
      "title": "Arrhythmia supraventricular premature beat detection in electrocardiography signal using deep gated recurrent model",
      "authors": [
        "N Elsayed",
        "Z Zaghloul",
        "C Li"
      ],
      "venue": "SoutheastCon 2021"
    },
    {
      "citation_id": "40",
      "title": "Dilated convolution with dilated gru for music source separation",
      "authors": [
        "J.-Y Liu",
        "Y.-H Yang"
      ],
      "year": "2019",
      "venue": "Dilated convolution with dilated gru for music source separation",
      "arxiv": "arXiv:1906.01203"
    },
    {
      "citation_id": "41",
      "title": "Bidirectional GRU for sound event detection",
      "authors": [
        "R Lu",
        "Z Duan"
      ],
      "year": "2017",
      "venue": "Detection and Classification of Acoustic Scenes and Events"
    },
    {
      "citation_id": "42",
      "title": "1d convolutional neural networks for signal processing applications",
      "authors": [
        "S Kiranyaz",
        "T Ince",
        "O Abdeljaber",
        "O Avci",
        "M Gabbouj"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "43",
      "title": "Feature extraction and classification of heart sound using 1d convolutional neural networks",
      "authors": [
        "F Li",
        "M Liu",
        "Y Zhao",
        "L Kong",
        "L Dong",
        "X Liu",
        "M Hui"
      ],
      "year": "2019",
      "venue": "EURASIP Journal on Advances in Signal Processing"
    },
    {
      "citation_id": "44",
      "title": "1d convolutional neural networks and applications: A survey",
      "authors": [
        "S Kiranyaz",
        "O Avci",
        "O Abdeljaber",
        "T Ince",
        "M Gabbouj",
        "D Inman"
      ],
      "year": "2021",
      "venue": "Mechanical systems and signal processing"
    },
    {
      "citation_id": "45",
      "title": "Machine Learning for Developers: Uplift your regular applications with the power of statistics, analytics, and machine learning",
      "authors": [
        "R Bonnin"
      ],
      "year": "2017",
      "venue": "Machine Learning for Developers: Uplift your regular applications with the power of statistics, analytics, and machine learning"
    },
    {
      "citation_id": "46",
      "title": "Gated recurrent neural networks empirical utilization for time series classification",
      "authors": [
        "N Elsayed",
        "A Maida",
        "M Bayoumi"
      ],
      "year": "2019",
      "venue": "2019 International Conference on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber"
    },
    {
      "citation_id": "47",
      "title": "Empirical activation function effects on unsupervised convolutional lstm learning",
      "authors": [
        "N Elsayed",
        "A Maida",
        "M Bayoumi"
      ],
      "year": "2018",
      "venue": "2018 IEEE 30th International Conference on Tools with Artificial Intelligence (ICTAI)"
    },
    {
      "citation_id": "48",
      "title": "Gated softmax classification",
      "authors": [
        "R Memisevic",
        "C Zach",
        "M Pollefeys",
        "G Hinton"
      ],
      "year": "2010",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "49",
      "title": "Toronto emotional speech set (TESS)-younger talker happy",
      "authors": [
        "K Dupuis",
        "M Pichora-Fuller"
      ],
      "year": "2010",
      "venue": "Toronto emotional speech set (TESS)-younger talker happy"
    },
    {
      "citation_id": "50",
      "title": "Introduction to keras",
      "authors": [
        "N Ketkar"
      ],
      "year": "2017",
      "venue": "Deep learning with Python"
    },
    {
      "citation_id": "51",
      "title": "Neural networks for machine learning lecture 6a overview of mini-batch gradient descent",
      "authors": [
        "G Hinton",
        "N Srivastava",
        "K Swersky"
      ],
      "year": "2012",
      "venue": "Neural networks for machine learning lecture 6a overview of mini-batch gradient descent"
    },
    {
      "citation_id": "52",
      "title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE international conference on computer vision"
    },
    {
      "citation_id": "53",
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "authors": [
        "X Glorot",
        "Y Bengio"
      ],
      "year": "2010",
      "venue": "Proceedings of the thirteenth international conference on artificial intelligence and statistics"
    },
    {
      "citation_id": "54",
      "title": "A simple generalisation of the area under the roc curve for multiple class classification problems",
      "authors": [
        "D Hand",
        "R Till"
      ],
      "year": "2001",
      "venue": "Machine learning"
    },
    {
      "citation_id": "55",
      "title": "Accuracy measures for the comparison of classifiers",
      "authors": [
        "V Labatut",
        "H Cherifi"
      ],
      "year": "2012",
      "venue": "Accuracy measures for the comparison of classifiers",
      "arxiv": "arXiv:1207.3790"
    },
    {
      "citation_id": "56",
      "title": "Optimized precision -a new measure for classifier performance evaluation",
      "authors": [
        "R Ranawana",
        "V Palade"
      ],
      "year": "2006",
      "venue": "2006 IEEE International Conference on Evolutionary Computation"
    },
    {
      "citation_id": "57",
      "title": "Deep learning",
      "authors": [
        "I Goodfellow",
        "Y Bengio",
        "A Courville"
      ],
      "year": "2016",
      "venue": "Deep learning"
    },
    {
      "citation_id": "58",
      "title": "Count on kappa",
      "authors": [
        "P Czodrowski"
      ],
      "year": "2014",
      "venue": "Journal of computer-aided molecular design"
    },
    {
      "citation_id": "59",
      "title": "Speech emotion detection using machine learning techniques",
      "authors": [
        "N Sundarprasad"
      ],
      "year": "2018",
      "venue": "Speech emotion detection using machine learning techniques"
    }
  ]
}