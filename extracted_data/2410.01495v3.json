{
  "paper_id": "2410.01495v3",
  "title": "Ov-Mer: Towards Open-Vocabulary Multimodal Emotion Recognition",
  "published": "2024-10-02T12:45:09Z",
  "authors": [
    "Zheng Lian",
    "Haiyang Sun",
    "Licai Sun",
    "Haoyu Chen",
    "Lan Chen",
    "Hao Gu",
    "Zhuofan Wen",
    "Shun Chen",
    "Siyuan Zhang",
    "Hailiang Yao",
    "Bin Liu",
    "Rui Liu",
    "Shan Liang",
    "Ya Li",
    "Jiangyan Yi",
    "Jianhua Tao"
  ],
  "keywords": [
    "humor",
    "mockery",
    "irony"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal Emotion Recognition (MER) is a critical research area that seeks to decode human emotions from diverse data modalities. However, existing machine learning methods predominantly rely on predefined emotion taxonomies, which fail to capture the inherent complexity, subtlety, and multi-appraisal nature of human emotional experiences, as demonstrated by studies in psychology and cognitive science. To overcome this limitation, we advocate for introducing the concept of open vocabulary into MER. This paradigm shift aims to enable models to predict emotions beyond a fixed label space, accommodating a flexible set of categories to better reflect the nuanced spectrum of human emotions. To achieve this, we propose a novel paradigm: Open-Vocabulary MER (OV-MER), which enables emotion prediction without being confined to predefined spaces. However, constructing a dataset that encompasses the full range of emotions for OV-MER is practically infeasible; hence, we present a comprehensive solution including a newly curated database, novel evaluation metrics, and a preliminary benchmark. By advancing MER from basic emotions to more nuanced and diverse emotional states, we hope this work can inspire the next generation of MER, enhancing its generalizability and applicability in real-world scenarios. Code and dataset are available at: https://github.com/zeroQiaoba/AffectGPT.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Research on emotions has a history spanning two centuries. As early as the 19th century, Charles Darwin conducted pioneering research about the evolutionary origins and possible purposes of emotions, explaining the emotional expressions of humans and animals  (Darwin, 1872) . In 1884, James revealed the process of emotion generation, noting that stimuli trigger activities in the autonomic nervous system, which in turn produces an emotional experience in the brain  (James, 1884) . With the rapid development of AI, emotions have garnered increasing attention  (Minsky, 1988) .\n\nThe basis of Multimodal Emotion Recognition (MER) lies in the effective modeling of emotions. Current emotion models are primarily categorized into two types: dimensional and discrete models. Dimensional models, particularly those based on psychological theories such as the Circumplex Model of Affect  (Russell, 1980) , represent emotions within a continuous, multi-dimensional space. The most widely adopted framework uses two or three primary dimensions: valence (the pleasantness-unpleasantness continuum), arousal (the activation-deactivation level), and dominance (the degree of control perceived). These dimensions allow for the quantification of emotional states into measurable numerical values  (Warriner et al., 2013) . However, this sophisticated numerical representation demands specialized psychological expertise for accurate interpretation, making it abstract and less descriptive to the general public. This abstraction can result in inconsistencies among different annotators, particularly in complex emotional states that fall between the primary dimensions, thereby complicating subsequent applications and potentially affecting the reliability of emotion recognition systems.\n\nDiscrete models, which categorize emotions into distinct classes, tend to mirror the way people naturally perceive and express emotions in daily life.  Ekman (1992)  proposed the basic emotion theory, suggesting that there are six basic emotions: anger, disgust, fear, happiness, sadness, and surprise. This theory is widely used in MER, where researchers typically limit the label space to these basic emotions and use multiple annotators to select the most likely label through majority voting. We refer to this task as Onehot MER (OH-MER). Considering that emotions can be We compare the differences among three tasks (one-hot MER, multi-label MER, and OV-MER) across three aspects (label space, label number, and annotation manner). An in-depth comparison is provided in the Appendix A; (b) Label Comparison: We provide an example to visualize the one-hot and OV labels. More examples are provided in Appendix F. Since the original video contains real people, we use DemoAI to remove personal information to address copyright concerns. In this paper, we use emotion-related descriptions as a bridge to extract OV labels. We observe that OV labels offer a more insightful understanding of the emotional state.\n\ncompound, researchers further propose Multi-label MER (ML-MER), allowing each sample to have multiple labels  (Li et al., 2017) . However, both OH-MER and ML-MER generally have limited label spaces.  Plutchik (2001)  pointed out that humans can express approximately 34,000 distinct emotions. Although some efforts have been made to expand label spaces with more emotional categories, current approaches still fail to capture emotional diversity, inevitably overlooking some of these nuanced emotions.\n\nIn this paper, we introduce a new MER paradigm, openvocabulary MER (OV-MER), by introducing the concept of open-vocabulary into MER, enabling the prediction of arbitrary emotion categories. Figure  1 (a) provides a comparison between different tasks, and an in-depth comparison is provided in Appendix A. To support this shift, we build a dataset, define evaluation metrics, and develop solutions. (1) Dataset: we propose a human-LLM collaboration strategy to construct the dataset. Compared to human-only annotation, our strategy can leverage LLM to enhance the label richness; (2) Metrics: since there is no fixed label space, the model may predict closely related but differently expressed emotions (e.g., joyful and happy). To provide more reliable evaluation results, we first group similar emotions and specifically design metrics for this task; (3) Solutions: traditional discriminative classifiers rely on fixed label spaces. However, OV-MER does not restrict the label space, necessitating the definition of new solutions.\n\nA natural question arises: why is OV-MER so important? A simple answer is that it naturally aligns with the way emotions are expressed in our real-life interactions, leading to more accurate and human-centered MER. As illustrated in Figure  1 (b), labeling an emotion solely as happy is not sufficiently informative. In contrast, OV-MER provides emotions like mockery, offering a more comprehensive and insightful understanding of the emotional state. Therefore, OV-MER facilitates the transition from basic to nuanced emotion recognition, advancing the development of emotion AI. Appendix C provides more detailed motivation. In summary, we make the following key contributions:\n\n• Paradigm. We propose a new paradigm in MER, called OV-MER. This paradigm transitions from traditional MER to a framework that enables the prediction of any number and category of emotions, thereby advancing emotion AI toward real-world applicability by capturing the full spectrum of human emotions.\n\n• Groundwork. We lay the groundwork for OV-MER by constructing datasets, defining evaluation metrics, and proposing solutions. Our dataset enhances label richness through human-LLM collaboration. Meanwhile, we introduce new evaluation metrics that leverage emotional relevance to achieve more reliable results.\n\n• Benchmark. We build zero-shot benchmarks for OV-MER through extensive experiments and detailed analysis. This task can serve as an important evaluation benchmark for multimodal LLMs (MLLMs), challenging their ability to integrate multimodal clues and capture subtle temporal variations in emotional expression. For audio and video, we use audio LLM (ALLM) and video LLM (VLLM) to extract initial clues, followed by two rounds of manual checks to eliminate errors and duplicates while adding missing content. Each round involves multiple annotators, with no overlap between annotators in the two rounds. Finally, we merge the checked clues with text to generate CLUE-Multi. (b) Ground-truth OV Label Extraction: There are certain differences in the labels extracted from different languages. To eliminate language influence and achieve consensus labels, we merge these labels and conduct manual checks. These checked labels are regarded as the ground truth.\n\n• Experiments. Our intensive experimental results not only demonstrate the strength of our methods but also prove that OV-MER can effectively enhance the presentation ability of emotions and user experience.",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "The Ov-Merd Dataset Construction",
      "text": "Although the concept of OV-MER is intuitive and holds great promise, its practical implementation faces significant challenges. The main difficulty lies in the broad and subtle range of human emotions, making comprehensive labeling a complex task. Traditional annotation methods are limited by their predefined emotion categories, which are often insufficient for the needs of OV-MER. In Figure  2 , we propose a human-LLM collaboration strategy that consists of two steps: CLUE-Multi generation and emotion label extraction.\n\nUltimately, we create a dataset, OV-MERD, which offers a richer set of emotions compared to existing datasets (see Table  1 ). This dataset is an extension of MER2023  (Lian et al., 2023) . Specifically, MER2023 is collected from movies and TV series, with most samples consisting of single-person videos featuring relatively complete speech content. The use of this dataset has been approved by the dataset own-ers. We randomly selected a subset of MER2023 for further annotation to construct our OV-MERD dataset. Additional details about MER2023 can be found in Appendix E.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Clue-Multi Generation",
      "text": "During the annotation process, we observe that human-LLM collaboration yields more detailed descriptions than the human-only strategy (see Section 5). In this section, we provide a detailed overview of our strategy. For manual checks, to maintain high-quality annotations, all annotators must pass a preliminary test. This test evaluates their performance on 12 samples, each of which was previously annotated by five annotators with full agreement. Annotators who perform poorly are removed from the annotator pool. More annotation details can be found in Appendix L.\n\nPre-annotation. Initially, we attempt to annotate visual and acoustic clues directly. However, the descriptions obtained in this way cannot cover all information. Therefore, we explore using other models for pre-annotation. (1) For video, given the strong visual understanding capabilities of GPT-4V (\"gpt-4-vision-preview\"), we use it as VLLM for pre-annotation. Since GPT-4V only supports image in- A,V,T Dimensional Emotion 1 1 CMU-MOSI  (Zadeh et al., 2017)  A,V,T Dimensional Emotion 1 1 CH-SIMS  (Yu et al., 2020)  A,V,T Dimensional Emotion 1 1 CH-SIMS v2  (Liu et al., 2022b)  A,V,T Dimensional Emotion 1 1 SEMAINE  (McKeown et al., 2011)  A,V,T Dimensional Emotion 5 1 MSP-IMPROV  (Busso et al., 2016)  A,V,T Discrete Emotion 4 1 IEMOCAP  (Busso et al., 2008)  A,V,T Discrete Emotion 10 1 MELD  (Poria et al., 2019)  A,V,T Discrete Emotion 7 1 MER2023  (Lian et al., 2023)  A,V,T Discrete Emotion 6 1 MER2024  (Lian et al., 2024a)  A,V,T Discrete Emotion 6 1\n\nOV-MERD (Ours) A,V,T Discrete Emotion 236 (arbitrary label) 1∼9, most 2∼4 (arbitrary number) put, we uniformly sample three frames from each video and input them into GPT-4V. We discuss the reasons for sampling three frames in Appendix K. (2) For audio, we use the open-source SALMONN  (Tang et al., 2023)  as ALLM for pre-annotation, as GPT-4V does not support audio input.\n\nManual Check. As part of our quality assurance procedures, we perform a detailed examination of the preannotated results. For visual clues, GPT-4V may generate hallucinated responses, i.e., clues that do not actually exist. Additionally, there are repeated expressions and some temporal association clues are missing. Therefore, we hire annotators to eliminate errors and duplicates, as well as add missing content. For acoustic clues, ALLM struggles to capture emotion-related paralinguistic features. The main reason is that current ALLM mainly focuses on tasks like ASR or audio event detection  (Tang et al., 2023) , with less emphasis on paralinguistic information. Hence, we hire multiple annotators to focus on the speaker's intonation and other emotion-related paralinguistic clues. To reduce subjective bias, we conduct two rounds of manual checks. Ultimately, these checked clues can accurately reflect the video content. Appendix L provides the annotation guideline and layout of the annotation platform.\n\nCLUE-Multi Generation. We leverage the reasoning capabilities of LLM to merge all clues. Specifically, we use GPT-3.5 (\"gpt-3.5-turbo-16k-0613\") as the LLM and ask it to merge textual, acoustic, and visual clues. The output is an emotion-related description, denoted as CLUE-Multi (see Figure  2 ). It is worth noting that we did not perform additional manual checks of the generated CLUE-Multi, as GPT-3.5 consistently produced reasonable and logical results. This reliability likely stems from the GPT-series models' exceptional performance in reading comprehension  (Brown et al., 2020)  (close to human performance), where multiclue integration is a core functionality. Therefore, we skip the manual inspection of CLUE-Multi, striking a balance between dataset reliability and construction efficiency. In Appendix K, we discuss the details of this merging process and the reasons behind it. The above annotation pipeline reflects the collaboration between humans and LLMs.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Ground-Truth Ov Label Extraction",
      "text": "Label Extraction. After that, we use the LLM to extract emotion labels from CLUE-Multi. This process relies on GPT-3.5, which we request to identify emotional states based on the provided descriptions without restricting the label space. See Appendix K for more details.\n\nLanguage Impact. We further explore the language impact. In Figure  2 , we first extract OV labels from English and Chinese descriptions, obtaining Y EE and Y CC . Then, we translate them into the other language, yielding Y EC and Y CE . Next, we measure the similarity between different sets and report results in Figure  2 . In Appendix N, we detail our metric calculation process. We observe that the labels extracted from different languages exhibit some differences. For example, the similarity score between Y EE and Y CE is 0.82, which may be due to the varying definitions of emotions in different languages. To eliminate language influence and achieve consensus labels, we merge the labels extracted from both languages and conduct manual checks. These checked labels are regarded as the ground truth.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Ov-Merd Dataset",
      "text": "Finally, we construct a dataset called OV-MERD. This dataset is an extension of MER2023  (Lian et al., 2023) , from which we randomly select a portion of samples for further annotation. Table  1  compares OV-MERD with existing datasets. We observe that our OV-MERD dataset contains 236 emotion categories, and most samples have 2 to 4 labels, far exceeding those in current datasets. In Appendix I, we observe that OV-MERD encompasses a broader range of emotions, including some that have been rarely discussed in previous research, such as shy, nervous, and grateful.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Evaluation Metric",
      "text": "Defining evaluation metrics for OV-MER presents significant challenges: (1) OV-MER supports predicting emotions of any category. Thus, the model may predict closely related but differently expressed emotions. To provide more reliable evaluation results, we first group the emotions based on their similarities. (2) OV-MER allows for the prediction of an arbitrary number of labels. Thus, traditional evaluation metrics designed for a fixed number of labels may not be applicable. In this section, we propose set-based evaluation metrics specifically tailored for this task.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Grouping",
      "text": "We propose two grouping strategies: one based on GPT and the other based on the emotion wheel (EW)  (Plutchik, 1980) .\n\nIn the experiments, we use GPT-based grouping by default.\n\nGPT-based Grouping. The most direct approach is to use GPT-3.5 to group all labels based on their similarity: Please assume the role of an expert in the field of emotions. We provide a set of emotions. Please group the emotions, with each group containing emotions with the same meaning. Directly output the results. The output format should be a list containing multiple lists. However, the evaluation results may be affected by the API version. For example, if OpenAI deprecates an old API, the results based on that API will become difficult to reproduce. Additionally, this process is costly (see Appendix O). Therefore, we attempt to find a replacement for GPT-based grouping.\n\nEW-based Grouping. EW is a psychological model that categorizes emotions in a structured manner. The inner part shows core emotions, while moving to the outer part reveals more nuanced emotions. Therefore, EW naturally provides emotion grouping information. Since there is no consensus on EW, we select five typical wheels (see Appendix P).\n\nBefore calculating the metrics, we define some symbols. We group the labels by their levels from the innermost to the outermost as L 1 w1 , L 2 w1 , and L 3 w1 . Next, we define a function m i→j w1 (•) that maps the labels in L i w1 to the corresponding labels in L j w1 . From inner to outer (i < j), m i→j w1 (•) is a many-to-one mapping; from outer to inner (i > j), m i→j w1 (•) is a one-to-many mapping. We collect all the labels from these emotion wheels and represent them as EW, i.e., {L j wi , 1 ≤ i ≤ 5, 1 ≤ j ≤ 3}. We denote the labels in EW as y w .\n\nConsidering that the emotional categories in EW are still limited, we perform some label expansion operations. Specifically, we repeatedly call GPT-3.5, asking it to generate synonyms for each label. The prompt used is as follows: Please retrieve the synonyms for the following words and output them in a table format. Then, we generate EW-S, i.e., {f (y w ) = {y 1 f , ..., y n f }, y w ∈ EW}, where f (•) is a function that maps each label y w to its synonym y f . We also define its inverse function f ′ (•), which maps different synonyms y f back to their base label y w .\n\nTo eliminate the influence of word forms (e.g., happy and happiness), we further ask GPT-3.5 multiple times to generate different forms for each label. The prompt used is as follows: Please output different forms of the following word in a list format. After that, we obtain EW-SF, i.e., {g(y f ) = {y 1 g , ..., y m g }, y f ∈ EW-S}, where g(•) is a function that maps each label y f to its different forms y g . We also define its inverse function g ′ (•), which maps different labels y g back to their base form y f . Finally, we define different types of metrics:\n\n(1) M1. We use g ′ (•) to map each label to its y f .\n\n(2) M2. We use f ′ (g ′ (•)) to map each label to its y w .\n\n(3) M3. We use the emotion wheel during metric calculation. Specifically, we first use f ′ (g ′ (•)) to map each label to its y w . Then, we define two grouping functions, L1 and L2. For L1, we map all labels to their corresponding L 1 wi :\n\nFor L2, we map all labels to their corresponding L 2 wi :",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Metric Definition",
      "text": "Then, we convert the above emotion grouping information into a function G(•), which can map each label to its group ID. Specifically, suppose {y i } M i=1 and {ŷ i } N i=1 are the ground truth and predictions, where M and N are the number of labels. We first map each label into its group ID:\n\nThen, we design set-based metrics for performance evaluation. Specifically, Precision s indicates the number of correctly predicted labels; Recall s indicates whether the prediction covers all ground truth; F s is the harmonic mean of two metrics, which is used for the final ranking:\n\nIt is important to note that changing the label order in Y and Ŷ does not result in any score change. The subscript",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Baselines For Ov-Mer",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Clue Generation",
      "text": "Figure  2  illustrates the generation process of CLUE-Multi, where we combine text with checked visual and acoustic clues. In this section, we further introduce some variants.\n\nCLUE-A/T/V. To reveal the modality impact, we propose three variants of CLUE-Multi: CLUE-Audio, CLUE-Text, and CLUE-Video. In Figure  3 , we illustrate their generation process. (1) CLUE-Audio: We observe that ALLM cannot fully leverage the text, and using an additional LLM to emphasize the text can further improve performance, which is also verified in Section 5. Therefore, we merge the checked acoustic clues with text using an additional LLM;\n\n(2) CLUE-Text: We only use the text to infer emotional states;\n\n(3) CLUE-Video: Since the visual content does not contain audio and text, we only use the checked visual clues. See Appendix Q for more examples.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Clue-Mllm.",
      "text": "MLLMs can address various multimodal tasks. Since emotion recognition relies on temporal information, we choose models that support at least video or audio.\n\nTo generate CLUE-MLLM, we first use ALLM or VLLM to extract emotion-related descriptions, and then combine these descriptions with text using LLM. Compared with CLUE-Multi, this process does not use manually checked clues.\n\nAppendix R provides model cards and relevant prompts. For MLLMs, we use their 7B version by default. All models are implemented in PyTorch, and all inference processes are executed on a 32GB NVIDIA Tesla V100 GPU.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Metric Calculation",
      "text": "As shown in Figure  2 , there are certain differences in the labels extracted from different languages. Therefore, we report the results for both English and Chinese descriptions.\n\nIn Figure  3 , for the Chinese branch, we first extract OV labels and then translate them into English; for the English branch, we directly extract OV labels. Finally, we compute the evaluation metrics with the ground truth. It is worth noting that the OV labels extracted from the monolingual CLUE-Multi differ from the ground truth. Our ground truth combines the labels extracted from different languages and undergoes further manual checks (see Figure  2 ).",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Results And Discussion",
      "text": "In this section, we default to using GPT-based grouping and employ GPT-3.5 (\"gpt-3.5-turbo-16k-0613\") as LLM. We generally report evaluation results in both languages, but if no specific language is mentioned, we default to reporting results for the English branch. To mitigate the impact of randomness, we conduct each experiment twice and report the average scores and standard deviations. In addition to MLLM-based generative models, we also report the performance of discriminative models in Appendix U.\n\nMain Results on CLUE-M/A/T/V. For CLUE-M/A/T/V, most baselines use manually checked clues, serving as per- formance upper bounds of different modality combinations.\n\nIn Table  2 , we observe that CLUE-Multi performs the best, highlighting the importance of multimodal information in MER. Meanwhile, CLUE-Video outperforms CLUE-Text, consistent with the nature of our OV-MERD dataset. To be specific, OV-MERD is derived from MER2023, where the textual modality contributes less than the visual modality in emotion recognition  (Lian et al., 2024b) . Relying solely on text makes it difficult to recognize emotions accurately. Furthermore, CLUE-Audio achieves superior performance over both CLUE-Text and CLUE-Video, suggesting that although textual expressions may be ambiguous for emotion recognition, combining them with audio cues can effectively resolve these ambiguities.\n\nMain Results on CLUE-MLLM. In Table  2 , we introduce a heuristic baseline called Random, where we randomly select a label from basic emotions. This baseline reflects the lower bound. We observe that MLLM generally outperforms Random, indicating that MLLM can partially address the OV-MER task. However, the performance of MLLM remains unsatisfactory, highlighting the limitations of existing MLLMs and the challenges of OV-MER. Furthermore, models that perform well in Chinese often perform well in English, suggesting that the impact of language differences on rankings is limited. Appendix S presents quantitative analysis results on language differences. Human-only vs. Human-LLM Collaboration. To verify the effectiveness of our human-LLM strategy, we additionally introduce a baseline using human-only annotation. In Figure  4 , we compare two strategies from three aspects: the length distribution of generated descriptions, the distribution of sample-wise label numbers, and the word cloud.\n\nIn Figure  4 , we observe that through human-LLM collaboration, we can obtain longer descriptions, provide more diverse labels for each sample, and generate a broader range of emotions. These results demonstrate that human-only annotation generally focuses on primary emotions while neglecting minor ones. With the pre-annotation and semantic reasoning capabilities of LLMs, we can obtain richer emotional labels. These results validate the effectiveness of our human-LLM collaborative strategy. Meanwhile, these results suggest that the LLM-driven approach does not lead to a narrow or biased interpretation of emotions, but rather helps uncover more subtle emotional nuances. We provide additional analysis in Appendix T. GPT-based vs. Matching-based Metrics. In Table  2 , CLUE-Multi demonstrates the best performance, leading to the hypothesis: Do sentences that are more similar to CLUE-Multi yield better emotion recognition performance?\n\nThe most common way to measure \"similarity\" is through  Experimental results reveal several interesting observations. First, the same metric across different languages typically shows high correlations. However, the correlation between GPT-based and matching-based metrics is relatively weak. For instance, the highest PCC score between \"F(E)\" and matching-based metrics is only 0.77. This discrepancy arises because matching-based metrics focus on low-level word-level matches, whereas emotion understanding is a more complex, high-level perceptual task. Appendix V provides a more detailed explanation of these findings.\n\nGPT-based vs. EW-based Grouping. We propose two grouping strategies: GPT-based and EW-based grouping.\n\nIn this section, we explore the relationship between them. Table  3  reports F s for different EW-based strategies, as this metric is used for the final ranking, and we compute the PCC scores between different metrics. We observe that the PCC scores between GPT-based and EW-based groupings are relatively high, indicating that EW-based metrics can serve as an alternative to GPT-based metrics. Meanwhile, we observe that M3-L2 is always more correlated with the GPT-based metrics than M3-L1. M3-L1 emphasizes coarsegrained clustering information, whereas M3-L2 emphasizes fine-grained clustering information. The higher correlation between M3-L2 and GPT-based metrics suggests that GPT-based metrics primarily rely on fine-grained emotion clustering during the metric calculation.\n\nInformative Comparison. We conducted a user study to evaluate whether our annotation manner provides greater informativeness compared to traditional basic emotions. Specifically, we recruited four annotators and randomly selected 20 samples from our dataset. For each sample, we presented both the basic emotion label and the OV-MERD label. Annotators were instructed as follows: Which label provides greater informativeness? The label with more information was marked as 1, and the other label as 0.\n\nExperimental results show that 97.5% of the annotations favored our OV-MERD labels, confirming their superiority in informativeness over basic emotions and verifying the effectiveness of our OV-MERD in emotion representation.\n\nAlignment with Human Perception. To evaluate how well OV-MER aligns with human perception, we conducted an additional user study. Specifically, we recruited nine annotators and randomly selected 20 samples from our dataset. Each annotator was presented with (sample, OV-MERD label) pairs and asked to judge their alignment with human perception using a binary (Yes/No) response format.\n\nTo ensure annotation quality, we included inspection data consisting of (sample, incorrect label) pairs. The results show that 96% of the annotations confirmed the alignment between OV-MERD labels and human perception. Considering potential annotator errors, this result demonstrates that our OV-MERD labels align well with human perception.",
      "page_start": 6,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "This paper extends traditional MER to OV-MER, allowing for the prediction of arbitrary numbers and types of emotions. For this task, we build a dataset, define metrics, and propose solutions. We observe that current MLLMs struggle to achieve satisfactory results, as this task requires consideration of multimodal clues and subtle temporal changes, placing higher demands on MLLMs. Additionally, EWbased metrics can replace GPT-based metrics, thus reducing evaluation costs while ensuring reproducibility. This paper advances current research from basic to nuanced emotion recognition, which is crucial for emotion AI.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Impact Statements",
      "text": "Ethics Statement. The raw data of the OV-MERD dataset comes from MER2023, from which we select some samples with further annotation. Therefore, we do not collect new data; we just re-annotate existing data. This annotation process has received consent from the dataset owners and has passed our internal review. During the annotation process, we generously pay each annotator approximately ¥3,000 (around $280), which is considered high. After proofreading our annotation results, we find that the annotations focus on the multimodal clues present in the videos, without any discriminatory annotations. Additionally, we restrict the use of the OV-MERD dataset to non-commercial purposes under the CC BY-NC 4.0 license. This license clearly outlines the correct and responsible use of our dataset.\n\nProper Use. MER is a widely discussed research topic.\n\nIn this paper, we extend traditional MER by providing more accurate emotion annotations that go beyond the fixed emotion taxonomy. In the license we provide, we restrict the use of this dataset to academic research; commercial usage is prohibited. Meanwhile, this dataset can only be used in nonsensitive human-computer interaction scenarios to enhance the machine's ability to understand human emotions and respond appropriately. It cannot be used in sensitive areas. For example, in interrogation scenarios, emotion recognition results should not be used to determine whether a criminal has committed a crime; Emotion recognition results should also not be used in recruitment and loan approval processes, as this may lead to unfair treatment of certain groups and affect their employment and financial opportunities; In the field of education, emotion recognition results should not be used to evaluate the performance of teachers and students.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "A. Task Comparison",
      "text": "There are various MER tasks. In this section, we provide an in-depth analysis of the differences between our proposed OV-MER and existing tasks.\n\nOne-Hot MER (OH-MER) is most widely discussed in the community. In this task, each sample is labeled with only one emotion. We should design a framework to predict the most likely label from a predefined emotional taxonomy  (Zhang et al., 2024) . Current research primarily focuses on the framework design and multimodal fusion strategy. For the former, research has shifted from traditional classifiers (e.g., SVM  (Bishop, 2006) ) to deep models (e.g., Transformers  (Vaswani et al., 2017) ). For the latter, researchers mainly focus on how to align heterogeneous features (e.g., word alignment  (Gu et al., 2018)  and implicit alignment  (Tsai et al., 2019a )) for subsequent multimodal fusion.\n\nMulti-Label MER (ML-MER) considers the complexity of emotions (i.e., multiple emotions often occur simultaneously) and allows the model to predict multiple labels. The most straightforward solution is to use binary classifiers for each emotion  (Godbole & Sarawagi, 2004; He & Xia, 2018) . However, this approach is based on the assumption of emotion independence, ignoring the correlations between different emotions. Therefore,  Huang et al. (2021)  and  Deng & Ren (2020)  proposed a sequence-to-emotion model, transforming the multi-label emotion recognition task into an emotion sequence prediction task, to further consider the correlations between emotions.\n\nPartial Label Learning MER (PLL-MER) differs from the tasks mentioned above. In PLL-MER, each sample is associated with a set of candidate labels, only one of which is correct  (Feng et al., 2020) . Research on PLL-MER can be roughly divided into two categories: average-based methods and identification-based approaches. The former assumes that each candidate label has an equal probability of being the ground truth  (Cour et al., 2009) , while the latter directly identifies the ground truth and maximizes its estimated probability  (Lv et al., 2020) .\n\nFine-Grained MER (FG-MER) differs from the above tasks in the label space. Previous tasks typically rely on coarsegrained emotion taxonomies, such as the widely accepted basic emotion taxonomies, like Ekman  (Ekman, 1992)  or Plutchik  (Plutchik, 1980)  emotions. Differently, FG-MER aims to use fine-grained emotion taxonomies to capture more subtle emotions  (Demszky et al., 2020) . After expanding the label space, FG-MER follows the typical solution of OH-MER.\n\nOV-MER shares some similarities with existing tasks, such as allowing the prediction of multiple labels, like ML-MER. However, OV-MER is fundamentally different from previous tasks:\n\n• Task Definition. The main distinction of OV-MER from other tasks lies in its focus on generalizing to unseen or new labels, whereas other tasks operate within a predefined taxonomy, either coarse-grained or fine-grained taxonomies. This is also the reason why we use the term \"open\" in the task name.\n\n• Emotional Complexity. Human emotional states are diverse and nuanced. As psychologist Plutchik pointed out, humans can express approximately 34,000 different emotions  (Plutchik, 1980) . Predefined taxonomies that categorize the full spectrum of emotions into a limited set of labels inevitably overlook some subtle emotional states. In contrast, OV-MER allows the model to understand and predict any emotion, enabling a more accurate modeling of the complex nature of human emotions.\n\n• Relationship Between OV-MER and ML-MER. Theoretically, ML-MER can be converted to OV-MER by spanning the label space to a complete set that includes all the emotion labels in the language (e.g., around 34,000 different emotions  (Plutchik, 1980) ). However, it is not feasible in practice to construct such a kind of dataset, i.e., annotators need to label each sample with 34,000 different emotions, and it's hard to collect sufficient samples for every emotion category, let alone multiple labels).\n\n• Evaluation Metrics. In OV-MER, we can use any emotion word to describe a person's emotional state. Therefore, the test set may contain new emotion labels that are not seen during training. In contrast, traditional methods require the label space in both training and test sets to be strictly consistent. This is why we propose new evaluation metrics for OV-MER.\n\n• Solution. Previous tasks rely on predefined label spaces, meaning that the predicted output is a fixed-size Mdimensional vector, where M is the size of the label space. Therefore, previous tasks can be solved using discriminative methods. However, in OV-MER, we do not constrain the label space, making discriminative methods unsuitable. Therefore, we adopt a generative approach, leveraging the rich output vocabulary of LLMs to construct our solution.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "B. Limitations And Future Work",
      "text": "Firstly, the main contribution of this paper is the definition of a new task and the conduct of foundational research. In the future, we plan to design more effective frameworks to solve OV-MER. Specifically, we will incorporate more emotionrelated instruction datasets to fine-tune MLLMs, thereby enhancing their emotion recognition ability. Meanwhile, how to integrate subtitle information and fuse multimodal inputs also plays a crucial role. We will also consider these aspects in the framework design. Secondly, we evaluate some MLLMs, but not all models are covered. In the future, we will expand the scope of evaluation to cover more MLLMs to enrich our benchmark. Thirdly, this paper does not involve cultural differences. Specifically, our original data is in Chinese, and the annotators we hired are also native Chinese speakers. In the future, we will also try to extend our method to other cultures and further analyze cultural differences. Fourth, we will focus on fairness-aware and unbiased modeling in future work. Fifth, OV-MERD is derived from MER2023, which is sourced from high-rated movies and TV shows. The high ratings serve as an implicit validation of the actors' performances, ensuring spontaneous and realistic emotional expressions. Currently, this type of dataset is the mainstream in the MER research community, as it provides a cost-effective means to expand the dataset scale. In the future, we plan to apply for additional funding to collect data featuring spontaneous, real-life emotional expressions by recruiting participants. Furthermore, we will employ domain adaptation techniques (e.g., domain adversarial neural networks) to address potential domain gaps between different data sources.",
      "page_start": 16,
      "page_end": 17
    },
    {
      "section_name": "C. Detailed Motivation",
      "text": "Video Emotion vs. Facial Emotion. Video emotion is more complex than facial emotion. This is because, in videos, we need to capture subtle changes in the temporal dimension and integrate multimodal clues. Take Figure  1 (b) as an example.\n\nIn the temporal dimension, we need to infer a person's nervousness based on his stuttering; in the multimodal dimension, we need to combine information from different modalities to gain a more comprehensive understanding of emotion. Due to the complexity of video emotion, using a single label is limiting, and more discrete labels are required to better describe video emotion. This is also the motivation behind our OV-MER task.\n\nLabel Importance. In OV-MER, we do not assign different levels of importance to each label. Every emotion holds equal significance, and neglecting anyone can impact the performance of downstream tasks. For example, if a human-computer interaction system overlooks any emotion, it may fail to generate appropriate responses.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "D. Related Work",
      "text": "Multimodal Emotion Recognition. MER has rapidly developed in recent years  (Wu et al., 2014) . Current research mainly focuses on building more efficient architectures to achieve higher accuracy on benchmark datasets  (Sun et al., 2024) . For example,  Zadeh et al. (2017)  proposed a tensor fusion network that addressed the MER task by leveraging interactions among unimodal, bimodal, and trimodal inputs.  Tsai et al. (2019a)  introduced a Transformer-based model that learned implicit alignment between different modalities and achieved promising results.  Lian et al. (2024b)  further established MERBench, involving various features, fusion strategies, and datasets. In emotion recognition, benchmark datasets usually limit the label space to basic emotions and use majority voting to determine the most likely one or more labels  (Lian et al., 2023; Li et al., 2017) . However, emotional categories extend far beyond basic emotions. Restricting the label space will inevitably overlook some nuanced emotions. To address this issue, we extend traditional MER to OV-MER, which allows for the prediction of any number and category of emotions.\n\nOpen Vocabulary Learning. Its main goal is to identify categories beyond the annotated label space  (Wu et al., 2024) , which has been applied in various fields, such as object detection  (Zareian et al., 2021) , segmentation  (Ghiasi et al., 2022) , and scene understanding  (Li et al., 2021) . For example, the object detection dataset COCO  (Lin et al., 2014)",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "E. Mer2023 Details",
      "text": "Our dataset utilizes videos sourced from MER2023  (Lian et al., 2023) , and the use of this dataset has been consented to by the dataset owners. During the construction of this dataset, MER2023 employs a voice activity detection (VAD) tool to split videos based on the presence or absence of human speech. Subsequently, it uses a tool to measure speaker similarity and merge consecutive clips from the same speaker, thereby ensuring relatively complete content for each video clip. Afterward, multiple filters are applied to remove videos with inappropriate lengths or those with multiple speakers. Finally, most samples in MER2023 are single-person videos with relatively complete speech content. Subtitle: Goodness, the relationships on our 22nd floor are getting more and more complicated. It seems like I'll have to speak less in the future.",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "F. More Examples",
      "text": "One-hot Label: surprise Description: In the video, there are two ladies in the frame. The lady on the left has her back to the camera, while the lady on the right is facing the camera. We mainly analyze the emotional state of the lady on the right. At the beginning of the video, her facial expression is not positive, and she seems a bit nervous and uneasy. In the following scenes, her eyebrows are raised, indicating that she is emphasizing something. At the end of the video, she lowers her head and looks down, not looking at the other person while speaking. This behavior may stem from inner pressure, anxiety, uneasiness, or frustration, causing her to tend to hide her emotions or avoid communication with others. In the audio, the volume is low, the tone is low, and there is a sense of pressure in the voice. In the text, the subtitle says, \"Goodness, the relationships on our 22nd floor are getting more and more complicated. It seems like I'll have to speak less in the future.\" Based on the tense and uneasy facial expression, the emphasized movement of raising eyebrows, and the behavior of lowering her head and looking down, it can be inferred that she may be feeling pressure, anxiety, uneasiness, or frustration. Additionally, based on the audio clue describing low volume, low tone, and a sense of pressure in the voice, it can be further confirmed that her emotional state is likely negative. Therefore, this sentence expresses her exclamation about the increasing complexity of the relationship on the 22nd floor and her decision to speak less in the future, implying her uneasiness and pressure. Description → OV labels: pressure, negative, uneasy, surprised Subtitle: Why are you all looking at me like that? So, as long as it's a woman, does she have to have a relationship with me?\n\nOne-hot Label: surprise Description: In the video, the screen shows a male character in an indoor setting. At the beginning of the video, his eyes are wide open and his mouth is also open, indicating a surprised facial expression. In the following scenes, he looks around, seemingly explaining or narrating something to the people around him. Overall, his emotions are not positive or optimistic. In the audio, the character speaks with a stutter, which usually expresses feelings of nervousness, anxiety, or unease. Combined with the text content, the character seems to be unhappy and angry due to the prejudice of the people around him. The subtitle in the text says, \"Why are you all looking at me like that? So, as long as it's a woman, does she have to have a relationship with me?\" This sentence expresses the male character's dissatisfaction and anger towards the people around him. Based on the surprised and negative facial expression of the male character in the video clues, as well as the stuttering speech in the audio clues, we can infer that the male character is expressing a feeling of dissatisfaction and anger in this sentence. He may feel troubled by the prejudice of the people around him and is unhappy with this unfair treatment. Description → OV labels: surprise, nervous, dissatisfied Subtitle: Luyuan will never fall into your hands, because he has found a high-paying job.",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "One-Hot Label: Angry",
      "text": "Description: In the video, the frame includes two women. The woman on the left has her back to the camera, while the woman on the right is facing the camera. We mainly analyze the emotional state of the woman on the right. At the beginning of the video, she slightly furrows her brows, opens her mouth, and her facial expression appears angry, as if she is engaged in a heated argument with the other person. At the same time, she points her finger at the other person and makes a motion towards them, which may indicate that she is accusing the other person or emphasizing her own viewpoint. Overall, she may be going through a debate or intense conversation, and her emotional state may be one of excitement and anger. She seems to be accusing and expressing her dissatisfaction. In the audio, the tone is aggressive and the character's emotions are more excited. Combined with the text content, the tone seems to carry a sense of threat. In the text, the subtitle says, \"Luyuan will never fall into your hands, because he has found a high-paying job.\" This sentence may be an accusation or threat from the woman on the right to the woman on the left. Based on the angry and angry emotions displayed by the woman on the right in the video clues, as well as her pointing finger and motion towards the other person, it can be inferred that she is accusing the other person or emphasizing her own viewpoint. At the same time, based on the aggressive tone and excited emotions described in the audio clues, as well as the mention in the subtitle that Luyuan has found a high-paying job, it can be inferred that this sentence may carry a sense of threat, and the woman on the right may be threatening the woman on the left not to interfere with or harm Luyuan. Therefore, this sentence expresses the woman on the right's anger and threatening emotions. Description → OV labels: warning, angry, threat",
      "page_start": 18,
      "page_end": 19
    },
    {
      "section_name": "G. Summary Of Abbreviations",
      "text": "In Table  4 , we summarize the main abbreviations and their meanings.",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "H. Dataset Comparison",
      "text": "This paper introduces a new task, OV-MER, and constructs a dataset for this task called OV-MERD. Table  5  compares OV-MERD with existing datasets. The annotation types of these datasets can be broadly categorized into two types: dimensional emotions and discrete emotions. We classify sentiment analysis datasets (e.g., CMU-MOSI) as dimensional datasets because the definition of sentiment intensity overlaps with the valence in dimensional emotions. We observe that OV-MERD contains 236 emotion categories, with most samples having 2 to 4 labels, significantly exceeding the number of labels in existing datasets. In the future, as the scale of the dataset increases, the number of candidate labels can be further expanded. Meanwhile, we would like to emphasize that our OV-MERD is the first dataset that uses the human-LLM collaborative annotation strategy, aiming to provide richer labels to capture more nuanced emotions. We believe this work is an important extension of traditional MER and will contribute to the development of the field.\n\nTable  5 : Dataset comparison. In this table, \"I\", \"A\", \"V\", and \"T\" are abbreviations for image, audio, video, and text, respectively. Some datasets (such as CMU-MOSEI and MSP-Podcast) contain both discrete and dimensional emotions.\n\nDataset Modality Annotation Type # Categories # Labels per Sample MSP-Podcast  (Lotfian & Busso, 2017)  A Dimensional 3 1 SST  (Socher et al., 2013)  T Dimensional 1 1 Cornell  (Pang et al., 2002)  T Dimensional 1 1 Large Movie  (Maas et al., 2011)  T Dimensional 1 1 ICT-MMMO  (Wöllmer et al., 2013)  A,V,T Dimensional 1 1 YouTube  (Morency et al., 2011)  A,V,T Dimensional 1 1 MOUD  (Pérez-Rosas et al., 2013)  A,V,T Dimensional 1 1 CMU-MOSI  (Zadeh et al., 2017)  A,V,T Dimensional 1 1 CMU-MOSEI  (Zadeh et al., 2018b)  A,V,T Dimensional 1 1 CH-SIMS  (Yu et al., 2020)  A,V,T Dimensional 1 1 CH-SIMS v2  (Liu et al., 2022b)  A,V,T Dimensional 1 1 VAM  (Grimm et al., 2008)  A,V,T Dimensional 3 1 SEMAINE  (McKeown et al., 2011)  A,V,T Dimensional 5 1 AFEW-VA  (Kossaifi et al., 2017)  A,V,T Dimensional 2 1 SEWA  (Kossaifi et al., 2019)  A,V,T Dimensional 3 1 MSP-Podcast  (Lotfian & Busso, 2017)  A Discrete 8 1 JL-Corpus  (James et al., 2018)  A Discrete 10 1 EmoDB  (Burkhardt et al., 2005)  A Discrete 7 1 EMOVO  (Costantini et al.,     (Zadeh et al., 2018b)  A,V,T Discrete 6 1 eNTERFACE  (Martin et al., 2006)  A,V,T Discrete 6 1 SAVEE  (Jackson & Haq, 2014)  A,V,T Discrete 7 1 AFEW 7.0  (Dhall et al., 2017)  A,V,T Discrete 7 1 MAFW  (Liu et al., 2022a)  A,V,T Discrete 11 1 DFEW  (Jiang et al., 2020)  A,V,T Discrete 7 1 CREMA-D  (Cao et al., 2014)  A,V,T Discrete 6 1 MSP-IMPROV  (Busso et al., 2016)  A,V,T Discrete 4 1 RAVDESS  Livingstone & Russo (2018)  A,V,T Discrete 8 1 IEMOCAP  (Busso et al., 2008)  A,V,T Discrete 10 1 MELD  (Poria et al., 2019)  A,V,T Discrete 7 1 MC-EIU  (Liu et al., 2024)  A,V,T Discrete 7 1 MER2023  (Lian et al., 2023)  A,V,T Discrete 6 1 MER2024  (Lian et al., 2024a)  A,V,T Discrete 6 1\n\nOV-MERD (Ours) A,V,T Discrete 236 (arbitrary label) 1∼9, most 2∼4 (arbitrary number)",
      "page_start": 20,
      "page_end": 21
    },
    {
      "section_name": "I. One-Hot Vs. Ov Labels",
      "text": "This section provides a deeper comparison between the one-hot labels in the MER2023 dataset and the OV labels in the OV-MERD dataset. Figure  11  shows the word cloud and label number distribution of OV labels. In Figure  11 (a), we observe that OV labels cover a wider variety of emotions, some of which (such as shy, nervous, and grateful) are rarely discussed in previous datasets. In Figure  11 (b), we notice that most samples have about 2 to 4 labels, much more than the traditional task where each sample is assigned only one emotion. Therefore, OV-MER provides richer labels. In Table  6 , we report the performance of one-hot labels in OV-MER. We observe that one-hot labels have high precision s but low recall s , indicating that one-hot labels are correct but not comprehensive. Due to the limited label space and the constrained number of labels, one-hot labels cannot cover all emotions, highlighting the limitations of traditional MER and the importance of OV-MER. Additionally, these results reflect the necessity to use F s for the final ranking, which can balance accuracy and completeness. Figure  12  shows the emotion distribution of OV labels. We observe that the number of samples for different emotions follows a long-tail distribution. These results indicate that OV labels not only cover some major labels but also capture subtle emotions that occur infrequently.\n\nFigure  12 : Emotion distribution of OV labels.\n\ncapture these fast movements. Increasing the number of sampled frames does not address this issue. Previous research has also shown that GPT-4V cannot recognize micro-expressions  (Lian et al., 2024c) . To capture these fast movements, we employ multiple professional annotators to manually add this information.\n\nMerging Process. In this paper, we rely on the powerful reasoning capabilities of LLM for multimodal fusion. Specifically, as shown in Table  7 , we ask LLM to integrate textual, acoustic, and visual clues to infer the emotional state. From the output, we observe that LLM can produce reasonable analytical results. However, ambiguities and contradictions in multimodal fusion are inevitable, which is a challenging and open problem. In this paper, we simply use LLM to address this problem, providing a practical solution for the OV-MER task. However, more effective strategies may exist, such as using more complex prompts or incorporating modality control measures. Therefore, we plan to explore this area in the future.\n\nVisualization. In Figure  14 , we visualize the output of the main steps.\n\nSubtitle: Why are you all looking at me like that? So, as long as it's a woman, does she have to have a relationship with me?\n\nVisual Clue (Prelabeled): In the opening scene, we see a man wearing a white lab coat. He looks confident, with a slight smile on his lips and eyes directly facing the camera, as if he is explaining or narrating something. The background features computers and office equipment, suggesting that this might be an office or laboratory setting. As the scene progresses to the middle part, the man's head turns slightly to one side, and his gaze shifts sideways, seemingly interacting with someone next to him. His smile broadens, and his eyebrows lift slightly, indicating that he might be having a friendly conversation or sharing a light-hearted moment. In the next scene, the man's expression becomes more exaggerated; his mouth opens wide as if expressing surprise or emphasizing a point. This kind of facial expression and body language is typically associated with sharing surprising news or reacting strongly to something. In the final scene, he returns to facing the camera directly, his expression calm once again with a slight smile, giving the impression that he might have just finished explaining or narrating something and feels satisfied or confident. Overall, based on these scenes, we can infer that the man in the video is likely conducting some sort of demonstration or explanation. He interacts with the audience, shares information, and possibly expresses surprise at some point. His emotional state throughout the video appears to be positive, exhibiting confidence and a friendly demeanor.\n\nVisual Clue (Checked): In the video, the screen shows a male character in an indoor setting. At the beginning of the video, his eyes are wide open and his mouth is also open, indicating a surprised facial expression. In the following scenes, he looks around, seemingly explaining or narrating something to the people around him. Overall, his emotions are not positive or optimistic.",
      "page_start": 21,
      "page_end": 31
    },
    {
      "section_name": "Acoustic Clue (Prelabeled):",
      "text": "The character in the voice is expressing frustration and annoyance. The tone is sarcastic and mocking, and the intonation is rising at the end of each sentence. There are pauses between each sentence, and the stress is on certain words to emphasize the frustration and annoyance.\n\nAcoustic Clue (Checked): In the audio, the character speaks with a stutter, which usually expresses feelings of nervousness, anxiety, or unease. Combined with the text content, the character seems to be unhappy and angry due to the prejudice of the people around him.",
      "page_start": 23,
      "page_end": 31
    },
    {
      "section_name": "Clue-Multi:",
      "text": "In the video, the screen shows a male character in an indoor setting. At the beginning of the video, his eyes are wide open and his mouth is also open, indicating a surprised facial expression.\n\nIn the following scenes, he looks around, seemingly explaining or narrating something to the people around him. Overall, his emotions are not positive or optimistic. In the audio, the character speaks with a stutter, which usually expresses feelings of nervousness, anxiety, or unease. Combined with the text content, the character seems to be unhappy and angry due to the prejudice of the people around him.\n\nThe subtitle in the text says, \"Why are you all looking at me like that? So, as long as it's a woman, does she have to have a relationship with me?\" This sentence expresses the male character's dissatisfaction and anger towards the people around him. Based on the surprised and negative facial expression of the male character in the video clues, as well as the stuttering speech in the audio clues, we can infer that the male character is expressing a feeling of dissatisfaction and anger in this sentence. He may feel troubled by the prejudice of the people around him and is unhappy with this unfair treatment. Figure  16  displays the layout of the annotation platform used for manually checking emotional labels. During annotation, we use the following instructions: Please select all labels that match the character's emotional state in the \"candidate emotions\". If the provided candidate labels cannot perfectly describe the character's emotional state, you can also manually add new labels to the \"other emotions\" part. Specifically, annotators need to label two parts. First, we list all candidate labels from which annotators can choose what they believe to be the correct labels; second, when the candidate labels cannot perfectly describe the emotions, annotators can manually add additional labels in the \"other emotions\" part.",
      "page_start": 31,
      "page_end": 32
    },
    {
      "section_name": "Other Emotions",
      "text": "Instructions: Please select all labels that match the character's emotional state in the \"candidate emotions\". If the provided candidate labels cannot perfectly describe the character's emotional state, you can also manually add new labels to the \"other emotions\" part.\n\n[ To illustrate which labels are removed or kept, we provide two examples, each with labels from multiple annotators. To ensure annotation quality, we hire professional annotators who are experts in affective computing and familiar with the definition of emotions. Some of these annotators are members of our team who specialize in affective computing. In Figure  17 , as the character doesn't know which doctor to see, most annotators provide labels such as confused or puzzled. Based on his tone and expression, some annotators further provide labels like anxious and serious. In Figure  18 , most annotators notice his disapproval based on the textual content. Combining other modalities, some annotators further note his blame and accuse of what others are planning to do. From these examples, we can observe that these annotators provided relatively reliable labels. However, some annotators may focus only on the most relevant labels and overlook some details. To ensure the comprehensiveness of the annotation results, we merge the labels checked by four annotators. For example, in Figure  17 , the final merged labels are troubled, focused, puzzled, anxious, worried, confused, and serious. In the next round, we invite another four annotators for a second check. Through this process, we can ensure that each preserved label is confirmed by at least one annotator in each round, thereby ensuring the comprehensiveness and accuracy of annotation results.\n\nInter-annotator Agreement. In this section, we calculate the inter-annotator agreement for two-round checks. Unlike the traditional single-label-based annotation method with a fixed label space, OV-MER employs a multi-label-based annotation method without a fixed label space. Therefore, we cannot directly compute the Kappa value between different annotators.\n\nTo this end, we draw inspiration from Section N and utilize the Jaccard similarity coefficient to measure the inter-annotator agreement. Specifically, assume there are N samples and K annotators. For each pair of annotators A m and A n , their annotation results for each sample x i are denoted as Y i m and Y i n , respectively. Here, Y i m and Y i n contain a set of emotion labels. We calculate the agreement score between annotators A m and A n as:\n\nIn our annotation process, we hired 8 annotators and conducted two rounds of checks, with no overlap among annotators in each round. Table  8  presents the inter-annotator agreement for the first round, and Table  9  presents the inter-annotator agreement for the second round. We observe that through multi-round checks, the inter-annotator agreement gradually increases. These results demonstrate the necessity of multi-round checks, which help enhance label reliability.",
      "page_start": 25,
      "page_end": 27
    },
    {
      "section_name": "M. Clue-Multi Analysis",
      "text": "In this section, we further analyze the reliability and comprehensiveness of CLUE-Multi from three aspects: discrete emotion recognition, dimensional emotion recognition, and visual clue statistics. Table  10  provides prompts and models for each part of the analysis.\n\nTable  10 : Prompts and corresponding models used in CLUE-Multi analysis.",
      "page_start": 28,
      "page_end": 28
    },
    {
      "section_name": "Function (Model) Prompt",
      "text": "#1 Discrete Emotion Recognition (GPT-3.5)\n\nPlease assume the role of an expert in the emotional domain. We provide clues that may be related to the emotions of the character. Based on the provided clues, identify the emotional states of the main characters. We provide a set of emotional candidates, please rank them in order of likelihood from high to low. The candidate set is {happy, angry, worried, sad, surprise, neutral}.\n\n#2 Valence Estimation (GPT-3.5)\n\nAs an expert in the emotional domain, we provide clues that may be related to the emotions of characters. Based on the provided clues, please identify the overall positive or negative emotional polarity of the main characters. The output should be a floating-point number ranging from -5 to +5. Here, -5 indicates extremely negative emotions, 0 indicates neutral emotions, and +5 indicates extremely positive emotions. Larger numbers indicate more positive emotions, while smaller numbers indicate more negative emotions. Please provide your judgment as a floating-point number with two decimal places, directly outputting the numerical result without including the analysis process.\n\n#3 Visual Clue Analysis (GPT-3.5)\n\nPlease assume the role of an expert in the field of emotions. We provide clues related to the emotions of the characters in the video. Please output the facial movements and body gestures involved in the description, separated by commas. The output format should be in list form.\n\nDiscrete Emotion Recognition. Our dataset is based on MER2023, which provides relatively reliable one-hot labels. Therefore, we attempt to determine whether these one-hot labels can be identified from CLUE-Multi. This part of the analysis aims to verify whether CLUE-Multi can cover the traditional one-hot emotion recognition task. Experimental results indicate that the top-1 and top-2 scores can reach 93.48 and 96.89, respectively. Further analysis shows that the prediction errors are primarily due to the limitations of one-hot labels. For example, in Figure  1 , the character shows a compound emotional state, including surprised, nervous, and unsatisfied. However, when we rank the candidate emotions, the output is: angry, surprised, worried, neutral, sad, happy. The top-1 label is angry, which differs from surprise in MER2023, leading to a prediction error. These results reveal the limitations of traditional one-hot labels in describing emotions.\n\nValence Estimation. Besides discrete labels, MER2023 also provides relatively reliable valence scores. Therefore, we attempt to verify whether CLUE-Multi can be used for valence estimation. Through experimental analysis, we observe that the PCC score between predictions and annotations can reach 0.88, indicating that CLUE-Multi also contains clues for dimensional emotion recognition.\n\nVisual Clue Analysis. Following that, we attempt to analyze the diversity of visual clues in CLUE-Multi. Through experimental analysis, we observe that each sample has an average of 4.95 visual clues. Therefore, we conclude that CLUE-Multi contains a wealth of clues that can help address discrete emotion recognition and valence estimation. Additionally, these results validate the completeness and reliability of CLUE-Multi.",
      "page_start": 29,
      "page_end": 29
    },
    {
      "section_name": "N. Details Of Language Impact Experiments",
      "text": "Experimental Design. In Figure  2 , we analyze from two perspectives: 1) the impact of descriptive language (Clue-Multi), and 2) the impact of abstract language (OV labels). The Y EE to Y EC (or Y CC to Y CE ) experiment aims to keep the descriptive language consistent to analyze the effect of abstract language, while the Y CE to Y EE (or Y EC to Y CC ) experiment aims to keep the abstract language consistent to analyze the effect of descriptive language.\n\nJaccard Similarity Coefficient. Figure  2  uses the Jaccard similarity coefficient to measure the similarity between two sets, which is slightly different from the evaluation metrics defined in Section 3. Specifically, in Section 3, we use the following metrics:\n\nThe motivation for the above metrics is that Y represents the ground truth, while Ŷ represents the prediction. However, in Figure  2 , the two sets of emotions are considered equally important. As a result, we use the Jaccard similarity coefficient to measure the similarity. This metric evaluates the similarity between two sets by comparing the size of their intersection to the size of their union:",
      "page_start": 30,
      "page_end": 30
    },
    {
      "section_name": "O. Cost Of Gpt-Based Metrics",
      "text": "This paper reports zero-shot performance, focusing only on the inference process. The cost of evaluating our OV-MERD dataset is approximately $1 per evaluation, which may not seem high. However, for future work aimed at training frameworks to better address the OV-MER task, this cost will become prohibitive. For example, if we plan to train a model for 100 epochs, the evaluation cost will rise to $1 × 100 epochs = $100. If we intend to test N different parameter combinations and M different frameworks, the evaluation cost will increase to $100 × M × N . Moreover, we plan to expand the OV-MERD dataset in the future. This cost will further increase. Therefore, this paper explores alternatives to GPT-based metrics. In the following scenes, he looks around, seemingly explaining or narrating something to the people around him. Overall, his emotions are not positive or optimistic. In the audio, the character speaks with a stutter, which usually expresses feelings of nervousness, anxiety, or unease. Combined with the text content, the character seems to be unhappy and angry due to the prejudice of the people around him.",
      "page_start": 29,
      "page_end": 31
    },
    {
      "section_name": "Q. Visualization Of Clue-M/A/T/V",
      "text": "The subtitle in the text says, \"Why are you all looking at me like that? So, as long as it's a woman, does she have to have a relationship with me?\" This sentence expresses the male character's dissatisfaction and anger towards the people around him. Based on the surprised and negative facial expression of the male character in the video clues, as well as the stuttering speech in the audio clues, we can infer that the male character is expressing a feeling of dissatisfaction and anger in this sentence. He may feel troubled by the prejudice of the people around him and is unhappy with this unfair treatment.",
      "page_start": 31,
      "page_end": 31
    },
    {
      "section_name": "Clue-Audio:",
      "text": "In the audio, the character speaks with a stutter, which usually expresses feelings of nervousness, anxiety, or unease. Combined with the text content, the character seems to be unhappy and angry due to the prejudice of the people around him. In the text, the subtitle content: \"Why are you all looking at me like that? So, as long as it's a woman, does she have to have a relationship with me?\" This sentence expresses the character's dissatisfaction and anger. The stuttering described in the audio clues, which is usually associated with emotions like nervousness, anxiety, or unease, combined with the subtitle content suggests that the character feels dissatisfied and angry. The character feels dissatisfied with the prejudices of those around them and believes that they have certain expectations or demands of them. Therefore, combining the audio clues and the subtitle content, we can infer that the character's emotional state is one of nervousness, anxiety, unease, dissatisfaction, and anger.\n\nCLUE-Text: Subtitle: \"Why are you all looking at me like that? So, as long as it's a woman, does she have to have a relationship with me?\" From this piece of text, we can infer that the character's emotional state is one of dissatisfaction or anger. The reasoning is as follows: 1. \"Why are you all looking at me like that?\" This question expresses the character's dissatisfaction with being stared at. It implies that the character feels uncomfortable or displeased with others watching him. 2. \"So, as long as it's a woman, does she have to have a relationship with me?\" This statement conveys the character's frustration with others' expectations or prejudices. The character feels annoyed or angry about the assumption that there should be a special relationship between him and a woman simply because she is female. In summary, the text suggests that the character is experiencing emotions of dissatisfaction or anger.",
      "page_start": 31,
      "page_end": 32
    },
    {
      "section_name": "Clue-Video:",
      "text": "In the video, the screen shows a male character in an indoor setting. At the beginning of the video, his eyes are wide open and his mouth is also open, indicating a surprised facial expression.\n\nIn the following scenes, he looks around, seemingly explaining or narrating something to the people around him. Overall, his emotions are not positive or optimistic.",
      "page_start": 18,
      "page_end": 31
    },
    {
      "section_name": "R. Details Of Clue-Mllm",
      "text": "CLUE-MLLM directly utilizes the output from MLLM without any manual checking process. Table  11  provides model cards for different MLLMs. For each MLLM, we provide two types of prompts (see Table  12 ): one that ignores text and another that considers text. To ensure a fair comparison, we use similar prompts for audio, video, and audio-video LLMs.\n\nTable  11 : Model cards for MLLMs.",
      "page_start": 32,
      "page_end": 32
    },
    {
      "section_name": "Model",
      "text": "Table  12 : Prompts for extracting emotion-related descriptions using MLLMs.",
      "page_start": 33,
      "page_end": 33
    },
    {
      "section_name": "S. Cross-Linguistic Correlation",
      "text": "In Table  14 , we leverage the results from Table  2  to compute PCC scores between the English and Chinese results for each metric. Experimental results demonstrate that all metrics exhibit strong cross-linguistic correlations.",
      "page_start": 34,
      "page_end": 34
    },
    {
      "section_name": "T. Relationship Between Description Length And Label Numbers",
      "text": "This section further discusses the relationship between description length and the number of labels per sample, i.e., whether longer descriptions correlate with more labels. To this end, we compute their PCC scores. We observe that, for the human-only strategy, the PCC score is 0.3416, and for the human-LLM collaboration strategy, the PCC score is 0.2939. Therefore, although from the dataset level, the length of descriptions is related to the richness of labels (see Figure  4 ), these two metrics do not show a strong correlation at the sample level.",
      "page_start": 34,
      "page_end": 34
    },
    {
      "section_name": "U. Performance Of Discriminative Mer Methods",
      "text": "This paper primarily focuses on MLLM-based generative models for OV-MER. Traditional discriminative methods are not applied due to fundamental differences in our experimental setup. Specifically, discriminative methods require identical label spaces Y for both training and testing sets. They cannot predict unseen emotions, i.e., y / ∈ Y. However, we use an open-vocabulary annotation manner in OV-MER, which inherently cannot guarantee alignment between training and testing label spaces (i.e., Y train ̸ = Y test ). MLLM-based generative methods offer greater flexibility in emotion prediction, making them better suited for our task. Consequently, we primarily leverage MLLM-based solutions. If forced to use discriminative approaches, these models could only predict labels within their training label space Y train .\n\nIn Table  15 , we follow the zero-shot experimental setup commonly used in generative models and report results for discriminative models. Specifically, we train on the IEMOCAP  (Busso et al., 2008)  (or MELD  (Poria et al., 2019) ) dataset and evaluate on OV-MERD. For discriminative models, we use CLIP-Large for visual features, HUBERT-Large for acoustic features, and Baichuan-13B for lexical features, comparing the performance of different classifiers. In this table, the \"Attention\" model refers to a foundation model architecture in MERBench  (Lian et al., 2024b) . Specifically, let f a i ∈ R da , f v i ∈ R dv , and f l i ∈ R d l denote the acoustic, visual, and lexical features for a sample x i , respectively. This model first converts all inputs into the same dimension and then computes importance scores α i for each modality. Subsequently, it employs weighted fusion to obtain multimodal features z i , which are utilized for emotion prediction.\n\nHere,\n\n, and b α ∈ R 3 are trainable parameters. For the output, we have h m i ∈ R h , h i ∈ R h×3 , α i ∈ R 3×1 , and z i ∈ R h . Experimental results in Table  15  show that while discriminative models can be adapted to solve OV-MER, they generally perform worse than MLLM-based generative models.",
      "page_start": 34,
      "page_end": 34
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Comparison. (a) Task Comparison: We compare the differences among three tasks (one-hot MER, multi-label",
      "page": 2
    },
    {
      "caption": "Figure 1: (a) provides a compar-",
      "page": 2
    },
    {
      "caption": "Figure 1: (b), labeling an emotion solely as happy is not",
      "page": 2
    },
    {
      "caption": "Figure 2: Dataset construction. (a) CLUE-Multi Generation: For audio and video, we use audio LLM (ALLM) and video",
      "page": 3
    },
    {
      "caption": "Figure 2: , we propose",
      "page": 3
    },
    {
      "caption": "Figure 2: ). It is worth noting that we did not perform addi-",
      "page": 4
    },
    {
      "caption": "Figure 2: , we first extract OV labels from English",
      "page": 4
    },
    {
      "caption": "Figure 2: In Appendix N, we detail",
      "page": 4
    },
    {
      "caption": "Figure 3: Baselines. (a) Preliminary: We begin by defining some preliminary symbols. (b) CLUE Generation: CLUE-",
      "page": 6
    },
    {
      "caption": "Figure 2: illustrates the generation process of CLUE-Multi,",
      "page": 6
    },
    {
      "caption": "Figure 3: , we illustrate their genera-",
      "page": 6
    },
    {
      "caption": "Figure 2: , there are certain differences in the",
      "page": 6
    },
    {
      "caption": "Figure 3: , for the Chinese branch, we first extract OV",
      "page": 6
    },
    {
      "caption": "Figure 3: illustrates the metric calculation process. The primary distinction between CLUE-MLLM",
      "page": 7
    },
    {
      "caption": "Figure 4: Human-only (H) vs. Human-LLM (H+L) strategy.",
      "page": 7
    },
    {
      "caption": "Figure 4: , we compare two strategies from three aspects:",
      "page": 7
    },
    {
      "caption": "Figure 4: , we observe that through human-LLM collab-",
      "page": 7
    },
    {
      "caption": "Figure 5: Performance comparison of different strategies for generating CLUE-MLLM.",
      "page": 8
    },
    {
      "caption": "Figure 6: Ablation.",
      "page": 8
    },
    {
      "caption": "Figure 6: introduces three methods:",
      "page": 8
    },
    {
      "caption": "Figure 3: In Figure 5,",
      "page": 8
    },
    {
      "caption": "Figure 7: GPT- vs. Matching-based metrics.",
      "page": 8
    },
    {
      "caption": "Figure 7: (b). Experimental results reveal several inter-",
      "page": 8
    },
    {
      "caption": "Figure 1: (b) as an example.",
      "page": 16
    },
    {
      "caption": "Figure 8: Example1.",
      "page": 17
    },
    {
      "caption": "Figure 9: Example2.",
      "page": 18
    },
    {
      "caption": "Figure 10: Example3.",
      "page": 18
    },
    {
      "caption": "Figure 11: shows the word cloud and label number distribution of OV labels. In Figure 11(a), we observe",
      "page": 21
    },
    {
      "caption": "Figure 11: (b), we notice that most samples have about 2 to 4 labels, much more than the traditional task",
      "page": 21
    },
    {
      "caption": "Figure 11: Word cloud and label number distribution of OV labels.",
      "page": 21
    },
    {
      "caption": "Figure 12: shows the emotion distribution of OV labels. We observe that the number of samples for different emotions",
      "page": 21
    },
    {
      "caption": "Figure 12: Emotion distribution of OV labels.",
      "page": 21
    },
    {
      "caption": "Figure 13: , we analyze the duration distribution of the OV-MERD dataset. We observe that the majority of the samples",
      "page": 22
    },
    {
      "caption": "Figure 13: Duration distribution of the OV-MERD dataset.",
      "page": 22
    },
    {
      "caption": "Figure 2: presents our dataset construction process. In Table 7, we provide prompts and corresponding models",
      "page": 22
    },
    {
      "caption": "Figure 14: , we visualize the output of the main steps.",
      "page": 23
    },
    {
      "caption": "Figure 14: An example to visualize the output of the main steps.",
      "page": 23
    },
    {
      "caption": "Figure 2: , there are two parts that require manual checking: 1)",
      "page": 24
    },
    {
      "caption": "Figure 15: shows the layout of the annotation platform used for manually checking acoustic and visual clues. During the",
      "page": 24
    },
    {
      "caption": "Figure 15: Layout of the annotation platform used for manually checking acoustic and visual clues.",
      "page": 24
    },
    {
      "caption": "Figure 16: displays the layout of the annotation platform used for manually checking emotional labels. During annotation,",
      "page": 25
    },
    {
      "caption": "Figure 16: Layout of the annotation platform used for manually checking emotional labels.",
      "page": 25
    },
    {
      "caption": "Figure 17: , as the character doesn’t know which doctor to see, most annotators provide labels such as confused or puzzled. Based on",
      "page": 25
    },
    {
      "caption": "Figure 18: , most annotators",
      "page": 25
    },
    {
      "caption": "Figure 17: Example1 with labels from multiple annotators.",
      "page": 26
    },
    {
      "caption": "Figure 18: Example2 with labels from multiple annotators.",
      "page": 26
    },
    {
      "caption": "Figure 1: , the character shows a compound",
      "page": 28
    },
    {
      "caption": "Figure 2: , we analyze from two perspectives: 1) the impact of descriptive language (Clue-Multi),",
      "page": 29
    },
    {
      "caption": "Figure 2: uses the Jaccard similarity coefficient to measure the similarity between two sets,",
      "page": 29
    },
    {
      "caption": "Figure 2: , the two sets of emotions are considered equally important. As a result, we use the Jaccard similarity coefficient to",
      "page": 29
    },
    {
      "caption": "Figure 19: provides more details.",
      "page": 30
    },
    {
      "caption": "Figure 19: Emotion wheels. This paper selects five representative emotion wheels (please zoom in to clearly view the",
      "page": 30
    },
    {
      "caption": "Figure 20: provides an example and visualizes CLUE-M/A/T/V.",
      "page": 31
    },
    {
      "caption": "Figure 20: Visualization of CLUE-M/A/T/V.",
      "page": 31
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "A": "\u0000\u0000\u0000",
          "B": "\u0000\u0000\u0000",
          "Similarity": "0.82"
        },
        {
          "A": "\u0000\u0000\u0000",
          "B": "\u0000\u0000\u0000",
          "Similarity": "0.82"
        },
        {
          "A": "\u0000\u0000\u0000",
          "B": "\u0000\u0000\u0000",
          "Similarity": "0.78"
        },
        {
          "A": "\u0000\u0000\u0000",
          "B": "\u0000\u0000\u0000",
          "Similarity": "0.73"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset": "MOUD (P´erez-Rosas et al., 2013)\nCMU-MOSI (Zadeh et al., 2017)\nCH-SIMS (Yu et al., 2020)\nCH-SIMS v2 (Liu et al., 2022b)\nSEMAINE (McKeown et al., 2011)\nMSP-IMPROV (Busso et al., 2016)\nIEMOCAP (Busso et al., 2008)\nMELD (Poria et al., 2019)\nMER2023 (Lian et al., 2023)\nMER2024 (Lian et al., 2024a)",
          "Modality": "A,V,T\nA,V,T\nA,V,T\nA,V,T\nA,V,T\nA,V,T\nA,V,T\nA,V,T\nA,V,T\nA,V,T",
          "Annotation Type": "Dimensional Emotion\nDimensional Emotion\nDimensional Emotion\nDimensional Emotion\nDimensional Emotion\nDiscrete Emotion\nDiscrete Emotion\nDiscrete Emotion\nDiscrete Emotion\nDiscrete Emotion",
          "# Categories": "1\n1\n1\n1\n5\n4\n10\n7\n6\n6",
          "# Labels per Sample": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1"
        },
        {
          "Dataset": "OV-MERD (Ours)",
          "Modality": "A,V,T",
          "Annotation Type": "Discrete Emotion",
          "# Categories": "236\n(arbitrary label)",
          "# Labels per Sample": "1∼9, most 2∼4\n(arbitrary number)"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "Qwen-Audio\nOtter\nVideo-LLaMA\nVideoChat\nSECap\nVideo-LLaVA\nSALMONN\nVideoChat2\nVideo-ChatGPT\nLLaMA-VID\nmPLUG-Owl\nChat-UniVi\nPCC score",
          "GPT": "38.13±0.05\n43.51±0.09\n44.73±0.14\n45.53±0.11\n45.72±0.09\n47.07±0.16\n47.96±0.04\n49.07±0.26\n50.52±0.06\n51.25±0.09\n52.73±0.13\n53.08±0.01\n—",
          "M1": "20.49±0.01\n22.21±0.06\n23.56±0.08\n22.15±0.00\n26.52±0.01\n25.47±0.12\n23.57±0.02\n26.92±0.09\n28.99±0.04\n28.28±0.04\n27.47±0.17\n28.89±0.02\n0.887",
          "M2": "23.37±0.01\n27.99±0.02\n28.39±0.17\n26.24±0.08\n32.88±0.03\n30.73±0.11\n28.83±0.03\n31.40±0.10\n34.05±0.05\n32.85±0.03\n32.47±0.19\n33.23±0.08\n0.857",
          "M3-W1\nL1\nL2": "43.85±0.03\n26.60±0.01\n49.75±0.11\n33.50±0.06\n52.90±0.12\n36.08±0.13\n47.79±0.07\n32.64±0.07\n52.26±0.03\n37.55±0.03\n54.65±0.10\n37.65±0.24\n54.90±0.15\n38.93±0.15\n52.38±0.13\n36.44±0.11\n57.66±0.04\n41.48±0.09\n56.59±0.04\n41.22±0.02\n57.60±0.23\n41.32±0.04\n57.00±0.06\n42.25±0.04\n0.911\n0.940",
          "M3-W2\nL1\nL2": "41.52±0.28\n26.68±0.01\n49.93±0.11\n33.04±0.06\n53.60±0.04\n35.33±0.08\n47.76±0.11\n32.14±0.04\n52.11±0.03\n37.71±0.03\n54.54±0.02\n38.25±0.22\n54.29±0.06\n37.79±0.07\n53.56±0.13\n36.91±0.11\n57.37±0.00\n40.95±0.08\n57.49±0.03\n40.39±0.04\n56.32±0.26\n40.83±0.07\n57.50±0.03\n42.43±0.03\n0.913\n0.942"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "Qwen-Audio\nOtter\nVideo-LLaMA\nVideoChat\nSECap\nVideo-LLaVA\nSALMONN\nVideoChat2\nVideo-ChatGPT\nLLaMA-VID\nmPLUG-Owl\nChat-UniVi\nPCC score",
          "M3-W3\nL1\nL2": "39.46±0.28\n30.65±0.01\n51.03±0.04\n37.12±0.00\n47.50±0.20\n36.50±0.25\n46.78±0.11\n34.37±0.03\n50.77±0.03\n40.49±0.03\n52.29±0.05\n40.58±0.15\n56.25±0.01\n43.01±0.02\n52.14±0.23\n40.57±0.14\n55.50±0.13\n44.15±0.18\n55.12±0.05\n44.06±0.01\n55.67±0.19\n43.71±0.13\n56.80±0.01\n45.66±0.05\n0.904\n0.927",
          "M3-W4\nL1\nL2": "36.64±0.03\n27.33±0.01\n47.54±0.00\n34.77±0.00\n52.97±0.09\n35.78±0.14\n49.53±0.15\n32.82±0.01\n50.43±0.03\n38.21±0.03\n52.45±0.06\n39.91±0.13\n50.53±0.09\n38.54±0.03\n50.63±0.19\n39.64±0.18\n55.24±0.02\n42.42±0.05\n56.62±0.15\n42.42±0.03\n55.06±0.17\n40.67±0.19\n55.86±0.07\n41.97±0.09\n0.899\n0.922",
          "M3-W5\nL1\nL2": "35.89±0.08\n29.66±0.01\n50.51±0.03\n35.54±0.00\n46.39±0.12\n34.77±0.23\n45.93±0.18\n32.85±0.04\n49.97±0.03\n40.25±0.03\n52.97±0.10\n39.69±0.10\n53.65±0.04\n42.09±0.02\n51.37±0.14\n39.89±0.15\n52.93±0.05\n41.54±0.14\n53.03±0.08\n41.65±0.04\n54.44±0.13\n42.00±0.18\n55.81±0.02\n43.61±0.05\n0.885\n0.894",
          "M-avg": "31.84\n39.41\n40.31\n37.58\n42.43\n43.27\n43.53\n42.65\n46.02\n45.81\n45.63\n46.75\n0.942"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Category": "Label",
          "Abbreviation": "OH label",
          "Explanation": "One-hot label, the most likely label in a limited set of basic emotions."
        },
        {
          "Category": "",
          "Abbreviation": "OV labels",
          "Explanation": "Open-vocabulary labels, a set of labels in an unlimited label space."
        },
        {
          "Category": "Task",
          "Abbreviation": "MER",
          "Explanation": "Multimodal Emotion Recognition, which aims to recognize the one-hot\nemotion label."
        },
        {
          "Category": "",
          "Abbreviation": "OV-MER",
          "Explanation": "Open-vocabulary MER, which aims to identify the OV emotion labels."
        },
        {
          "Category": "Dataset",
          "Abbreviation": "OV-MERD",
          "Explanation": "This is a dataset we built for the OV-MER task."
        },
        {
          "Category": "Metric",
          "Abbreviation": "EW",
          "Explanation": "Emotion Wheel."
        },
        {
          "Category": "",
          "Abbreviation": "M1, M2, M3",
          "Explanation": "Different grouping strategies based on the emotion wheel."
        },
        {
          "Category": "",
          "Abbreviation": "Precisions, Recalls, Fs",
          "Explanation": "Metrics defined for OV-MER."
        },
        {
          "Category": "Model",
          "Abbreviation": "LLM",
          "Explanation": "Large Language Model. Large-scale models and only process text."
        },
        {
          "Category": "",
          "Abbreviation": "ALLM",
          "Explanation": "Audio LLM. Different from LLM, it can also process audio input."
        },
        {
          "Category": "",
          "Abbreviation": "VLLM",
          "Explanation": "Video LLM. Different from LLM, it can also process video input."
        },
        {
          "Category": "",
          "Abbreviation": "MLLM",
          "Explanation": "Multimodal LLM. Unlike LLM, it can process at least one more modality\n(e.g., audio or video). Thus, MLLM includes ALLM and VLLM."
        },
        {
          "Category": "Description",
          "Abbreviation": "CLUE-Multi",
          "Explanation": "It uses the checked acoustic and visual clues to generate descriptions."
        },
        {
          "Category": "",
          "Abbreviation": "CLUE-Audio",
          "Explanation": "Different from CLUE-Multi, it only uses checked acoustic clues."
        },
        {
          "Category": "",
          "Abbreviation": "CLUE-Video",
          "Explanation": "Different from CLUE-Multi, it only uses checked visual clues."
        },
        {
          "Category": "",
          "Abbreviation": "CLUE-Text",
          "Explanation": "It only relies on text to generate descriptions."
        },
        {
          "Category": "",
          "Abbreviation": "CLUE-A/T/V",
          "Explanation": "Any of CLUE-Audio, CLUE-Text, and CLUE-Video."
        },
        {
          "Category": "",
          "Abbreviation": "CLUE-M/A/T/V",
          "Explanation": "Any of CLUE-Multi, CLUE-Audio, CLUE-Text, and CLUE-Video."
        },
        {
          "Category": "",
          "Abbreviation": "CLUE-MLLM",
          "Explanation": "It uses the output from MLLM without any manual checking process."
        },
        {
          "Category": "",
          "Abbreviation": "S0, S1, S2",
          "Explanation": "Different CLUE-MLLM generation strategies."
        }
      ],
      "page": 19
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset": "MSP-Podcast (Lotfian & Busso, 2017)\nSST (Socher et al., 2013)\nCornell (Pang et al., 2002)\nLarge Movie (Maas et al., 2011)\nICT-MMMO (W¨ollmer et al., 2013)\nYouTube (Morency et al., 2011)\nMOUD (P´erez-Rosas et al., 2013)\nCMU-MOSI (Zadeh et al., 2017)\nCMU-MOSEI (Zadeh et al., 2018b)\nCH-SIMS (Yu et al., 2020)\nCH-SIMS v2 (Liu et al., 2022b)\nVAM (Grimm et al., 2008)\nSEMAINE (McKeown et al., 2011)\nAFEW-VA (Kossaifi et al., 2017)\nSEWA(Kossaifi et al., 2019)",
          "Modality": "A\nT\nT\nT\nA,V,T\nA,V,T\nA,V,T\nA,V,T\nA,V,T\nA,V,T\nA,V,T\nA,V,T\nA,V,T\nA,V,T\nA,V,T",
          "Annotation Type": "Dimensional\nDimensional\nDimensional\nDimensional\nDimensional\nDimensional\nDimensional\nDimensional\nDimensional\nDimensional\nDimensional\nDimensional\nDimensional\nDimensional\nDimensional",
          "# Categories": "3\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n3\n5\n2\n3",
          "# Labels per Sample": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1"
        },
        {
          "Dataset": "MSP-Podcast (Lotfian & Busso, 2017)\nJL-Corpus (James et al., 2018)\nEmoDB (Burkhardt et al., 2005)\nEMOVO (Costantini et al., 2014)\nMESD (Duville et al., 2021)\nSFEW 2.0 (Dhall et al., 2015)\nFER-2013 (Goodfellow et al., 2013)\nEmotioNet (Fabian Benitez-Quiroz et al., 2016)\nAffectNet (Mollahosseini et al., 2017)\nExpW (Zhang et al., 2018)\nRAF-DB (Li et al., 2017)\nCMU-MOSEI (Zadeh et al., 2018b)\neNTERFACE (Martin et al., 2006)\nSAVEE (Jackson & Haq, 2014)\nAFEW 7.0 (Dhall et al., 2017)\nMAFW (Liu et al., 2022a)\nDFEW (Jiang et al., 2020)\nCREMA-D (Cao et al., 2014)\nMSP-IMPROV (Busso et al., 2016)\nRAVDESS Livingstone & Russo (2018)\nIEMOCAP (Busso et al., 2008)\nMELD (Poria et al., 2019)\nMC-EIU (Liu et al., 2024)\nMER2023 (Lian et al., 2023)\nMER2024 (Lian et al., 2024a)\nOV-MERD (Ours)",
          "Modality": "A\nA\nA\nA\nA\nI\nI\nI\nI\nI\nI\nA,V,T\nA,V,T\nA,V,T\nA,V,T\nA,V,T\nA,V,T\nA,V,T\nA,V,T\nA,V,T\nA,V,T\nA,V,T\nA,V,T\nA,V,T\nA,V,T\nA,V,T",
          "Annotation Type": "Discrete\nDiscrete\nDiscrete\nDiscrete\nDiscrete\nDiscrete\nDiscrete\nDiscrete\nDiscrete\nDiscrete\nDiscrete\nDiscrete\nDiscrete\nDiscrete\nDiscrete\nDiscrete\nDiscrete\nDiscrete\nDiscrete\nDiscrete\nDiscrete\nDiscrete\nDiscrete\nDiscrete\nDiscrete\nDiscrete",
          "# Categories": "8\n10\n7\n7\n6\n7\n7\n23\n7\n7\n19\n6\n6\n7\n7\n11\n7\n6\n4\n8\n10\n7\n7\n6\n6\n236\n(arbitrary label)",
          "# Labels per Sample": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1∼2\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1∼9, most 2∼4\n(arbitrary number)"
        }
      ],
      "page": 20
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Function (Model)": "#1 Pre-label visual clue\n(VLLM)",
          "Prompt": "As an expert\nin the field of emotions, please focus on facial expressions, body language,\nenvironmental cues, and events in the video and predict the emotional state of the character.\nPlease ignore the character’s identity. We uniformly sample 3 frames from this video. Please\nconsider the temporal relationship between these frames and provide a complete description\nof this video. Avoid using descriptions like “the first image” and “the second image”, and\ninstead use terms like “beginning”, “middle”, and “end” to denote the progression of time."
        },
        {
          "Function (Model)": "#2 Pre-label acoustic clue\n(ALLM)",
          "Prompt": "As an expert in the field of emotions, please focus on the acoustic information in the audio to\ndiscern clues related to the emotions of the individual. Please provide a detailed description\nand ultimately predict the emotional state of the individual."
        },
        {
          "Function (Model)": "#3 Merge\n(LLM)",
          "Prompt": "Please act as an expert in the field of emotions. We provide acoustic and visual clues that\nmay be related to the character’s emotional state, along with the original subtitle of the video.\nPlease analyze which parts can infer the emotional state and explain the reasons. During the\nanalysis, please integrate the textual, audio, and visual clues."
        },
        {
          "Function (Model)": "#4 Translation\n(LLM)",
          "Prompt": "Chinese→English: Please translate the following sentence from Chinese into English.\nEnglish→Chinese: Please translate the following sentence from English into Chinese."
        },
        {
          "Function (Model)": "#5 OV label extraction\n(LLM)",
          "Prompt": "Please assume the role of an expert in the field of emotions. We provide clues that may be\nrelated to the emotions of the characters. Based on the provided clues, please identify the\nemotional states of the main characters. Please separate different emotional categories with\ncommas and output only the clearly identifiable emotional categories in a list format. If none\nare identified, please output an empty list."
        }
      ],
      "page": 22
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Function (Model)": "#1 Discrete Emotion Recognition\n(GPT-3.5)",
          "Prompt": "Please assume the role of an expert\nin the emotional domain. We provide clues\nthat may be related to the emotions of the character. Based on the provided clues,\nidentify the emotional states of the main characters. We provide a set of emotional\ncandidates, please rank them in order of likelihood from high to low. The candidate\nset is {happy, angry, worried, sad, surprise, neutral}."
        },
        {
          "Function (Model)": "#2 Valence Estimation\n(GPT-3.5)",
          "Prompt": "As an expert in the emotional domain, we provide clues that may be related to the\nemotions of characters. Based on the provided clues, please identify the overall\npositive or negative emotional polarity of the main characters. The output should\nbe a floating-point number ranging from -5 to +5. Here,\n-5 indicates extremely\nnegative emotions, 0 indicates neutral emotions, and +5 indicates extremely positive\nemotions. Larger numbers indicate more positive emotions, while smaller numbers\nindicate more negative emotions. Please provide your judgment as a floating-point\nnumber with two decimal places, directly outputting the numerical result without\nincluding the analysis process."
        },
        {
          "Function (Model)": "#3 Visual Clue Analysis\n(GPT-3.5)",
          "Prompt": "Please assume the role of an expert in the field of emotions. We provide clues related\nto the emotions of the characters in the video. Please output the facial movements\nand body gestures involved in the description, separated by commas. The output\nformat should be in list form."
        }
      ],
      "page": 28
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "Audio LLM",
          "Text": "w/o",
          "Prompt": "As an expert in the field of emotions, please focus on the acoustic information in the\naudio to discern clues related to the emotions of the individual. Please provide a detailed\ndescription and ultimately predict the emotional state of the individual."
        },
        {
          "Model": "",
          "Text": "w/",
          "Prompt": "Subtitle content of the audio: {subtitle}; As an expert in the field of emotions, please\nfocus on the acoustic information and subtitle content in the audio to discern clues related\nto the emotions of the individual. Please provide a detailed description and ultimately\npredict the emotional state of the individual in the audio."
        },
        {
          "Model": "Video LLM",
          "Text": "w/o",
          "Prompt": "As an expert\nin the field of emotions, please focus on the facial expressions, body\nmovements, environment, etc., in the video to discern clues related to the emotions of\nthe individual. Please provide a detailed description and ultimately predict the emotional\nstate of the individual in the video."
        },
        {
          "Model": "",
          "Text": "w/",
          "Prompt": "Subtitle content of the video: {subtitle}; As an expert in the field of emotions, please\nfocus on the facial expressions, body movements, environment, subtitle content, etc.,\nin the video to discern clues related to the emotions of the individual. Please provide a\ndetailed description and ultimately predict the emotional state of the individual."
        },
        {
          "Model": "Audio-Video LLM",
          "Text": "w/o",
          "Prompt": "As an expert\nin the field of emotions, please focus on the facial expressions, body\nmovements, environment, acoustic information, etc., in the video to discern clues related\nto the emotions of the individual. Please provide a detailed description and ultimately\npredict the emotional state of the individual in the video."
        },
        {
          "Model": "",
          "Text": "w/",
          "Prompt": "Subtitle content of the video: {subtitle}; As an expert in the field of emotions, please\nfocus on the facial expressions, body movements, environment, acoustic information,\nsubtitle content, etc., in the video to discern clues related to the emotions of the individual.\nPlease provide a detailed description and ultimately predict the emotional state of the\nindividual in the video."
        }
      ],
      "page": 32
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "Otter",
          "Strategy": "S0\nS1\nS2",
          "English\nFs\nPrecisions\nRecalls": "34.75±0.02\n40.41±0.03\n30.48±0.01\n22.54±0.05\n26.05±0.08\n19.86±0.04\n43.51±0.09\n50.71±0.10\n38.09±0.09",
          "Chinese\nFs\nPrecisions\nRecalls": "31.08±0.10\n35.71±0.15\n27.51±0.07\n25.06±0.04\n29.14±0.03\n21.99±0.05\n46.22±0.01\n52.65±0.16\n41.18±0.08"
        },
        {
          "Model": "PandaGPT",
          "Strategy": "S0\nS1\nS2",
          "English\nFs\nPrecisions\nRecalls": "26.99±0.01\n29.18±0.08\n25.10±0.04\n34.75±0.21\n36.77±0.30\n32.94±0.14\n45.89±0.20\n50.03±0.01\n42.38±0.33",
          "Chinese\nFs\nPrecisions\nRecalls": "28.70±0.01\n30.95±0.00\n26.76±0.03\n34.74±0.17\n37.27±0.15\n32.53±0.18\n47.33±0.04\n53.01±0.08\n42.75±0.11"
        },
        {
          "Model": "Video-ChatGPT",
          "Strategy": "S0\nS1\nS2",
          "English\nFs\nPrecisions\nRecalls": "34.77±0.04\n37.66±0.13\n32.30±0.03\n41.74±0.24\n45.59±0.24\n38.49±0.23\n50.52±0.06\n54.03±0.04\n47.44±0.07",
          "Chinese\nFs\nPrecisions\nRecalls": "37.62±0.16\n40.33±0.05\n35.25±0.25\n40.81±0.03\n45.07±0.00\n37.28±0.05\n54.73±0.00\n61.15±0.10\n49.52±0.06"
        },
        {
          "Model": "Video-LLaMA",
          "Strategy": "S0\nS1\nS2",
          "English\nFs\nPrecisions\nRecalls": "28.17±0.26\n28.64±0.36\n27.72±0.18\n34.43±0.16\n35.82±0.20\n33.15±0.11\n44.73±0.14\n44.14±0.13\n45.34±0.15",
          "Chinese\nFs\nPrecisions\nRecalls": "30.70±0.11\n30.09±0.14\n31.34±0.08\n34.01±0.25\n35.16±0.22\n32.94±0.26\n47.26±0.03\n47.98±0.07\n46.56±0.01"
        },
        {
          "Model": "VideoChat",
          "Strategy": "S0\nS1\nS2",
          "English\nFs\nPrecisions\nRecalls": "31.95±0.02\n31.73±0.13\n32.17±0.10\n45.10±0.07\n46.24±0.05\n44.01±0.10\n45.53±0.11\n42.90±0.27\n48.49±0.10",
          "Chinese\nFs\nPrecisions\nRecalls": "34.53±0.02\n33.53±0.01\n35.60±0.05\n44.25±0.09\n44.76±0.02\n43.75±0.16\n45.57±0.03\n47.20±0.12\n44.05±0.05"
        },
        {
          "Model": "VideoChat2",
          "Strategy": "S0\nS1\nS2",
          "English\nFs\nPrecisions\nRecalls": "35.70±0.06\n43.08±0.00\n30.47±0.09\n37.56±0.07\n44.62±0.00\n32.43±0.10\n49.07±0.26\n54.72±0.41\n44.47±0.15",
          "Chinese\nFs\nPrecisions\nRecalls": "35.27±0.01\n41.16±0.00\n30.86±0.01\n38.71±0.10\n45.14±0.13\n33.88±0.08\n48.86±0.05\n57.12±0.08\n42.68±0.04"
        },
        {
          "Model": "mPLUG-Owl",
          "Strategy": "S0\nS1\nS2",
          "English\nFs\nPrecisions\nRecalls": "39.21±0.14\n40.56±0.15\n37.94±0.12\n45.80±0.06\n47.49±0.04\n44.22±0.07\n52.73±0.13\n54.54±0.13\n51.04±0.13",
          "Chinese\nFs\nPrecisions\nRecalls": "40.53±0.33\n40.44±0.24\n40.62±0.43\n47.97±0.04\n49.33±0.03\n46.69±0.05\n50.95±0.06\n56.40±0.11\n46.47±0.18"
        },
        {
          "Model": "SALMONN",
          "Strategy": "S0\nS1\nS2",
          "English\nFs\nPrecisions\nRecalls": "40.71±0.10\n41.38±0.25\n40.07±0.04\n39.79±0.03\n39.54±0.01\n40.05±0.06\n47.96±0.04\n50.20±0.04\n45.92±0.04",
          "Chinese\nFs\nPrecisions\nRecalls": "43.45±0.23\n43.24±0.30\n43.66±0.16\n41.43±0.13\n41.11±0.03\n41.76±0.22\n48.24±0.03\n52.24±0.00\n44.82±0.05"
        },
        {
          "Model": "Qwen-Audio",
          "Strategy": "S0\nS1\nS2",
          "English\nFs\nPrecisions\nRecalls": "30.64±0.06\n41.92±0.00\n24.14±0.08\n35.23±0.10\n46.69±0.15\n28.29±0.08\n38.13±0.05\n49.42±0.18\n31.04±0.00",
          "Chinese\nFs\nPrecisions\nRecalls": "30.50±0.05\n40.84±0.13\n24.33±0.03\n44.09±0.00\n58.08±0.00\n35.53±0.00\n41.14±0.07\n53.71±0.00\n33.34±0.09"
        },
        {
          "Model": "Video-LLaVA",
          "Strategy": "S0\nS1\nS2",
          "English\nFs\nPrecisions\nRecalls": "32.64±0.03\n33.31±0.01\n32.00±0.05\n30.19±0.02\n34.10±0.03\n27.08±0.05\n47.07±0.16\n48.58±0.02\n45.66±0.29",
          "Chinese\nFs\nPrecisions\nRecalls": "32.76±0.03\n33.19±0.06\n32.33±0.00\n31.93±0.11\n33.40±0.19\n30.58±0.04\n49.21±0.06\n53.95±0.03\n45.23±0.13"
        },
        {
          "Model": "LLaMA-VID",
          "Strategy": "S0\nS1\nS2",
          "English\nFs\nPrecisions\nRecalls": "35.14±0.14\n36.71±0.15\n33.69±0.14\n42.37±0.03\n43.97±0.04\n40.89±0.03\n51.25±0.09\n52.71±0.18\n49.87±0.00",
          "Chinese\nFs\nPrecisions\nRecalls": "33.30±0.04\n33.12±0.06\n33.48±0.03\n42.56±0.08\n43.28±0.11\n41.86±0.04\n52.01±0.02\n57.30±0.00\n47.61±0.03"
        },
        {
          "Model": "Chat-UniVi",
          "Strategy": "S0\nS1\nS2",
          "English\nFs\nPrecisions\nRecalls": "39.89±0.18\n42.32±0.21\n37.72±0.15\n47.94±0.19\n50.96±0.20\n45.26±0.18\n53.08±0.01\n53.68±0.00\n52.50±0.02",
          "Chinese\nFs\nPrecisions\nRecalls": "36.83±0.30\n37.74±0.27\n35.96±0.33\n47.02±0.00\n48.07±0.00\n46.01±0.00\n53.86±0.02\n58.54±0.01\n49.86±0.03"
        }
      ],
      "page": 33
    },
    {
      "caption": "Table 16: GPT-based vs. matching-based metrics. “Ps”, “Rs”, “B",
      "data": [
        {
          "MLLM\nL\nV\nA\n√\n√": "",
          "English": "GPT-based\nFs\nPs\nRs",
          "Chinese": "GPT-based\nFs\nPs\nRs"
        },
        {
          "MLLM\nL\nV\nA\n√\n√": "×\nQwen-Audio\n√\n√\n×\nOneLLM\n√ √\n×\nOtter\n√ √\n×\nVideo-LLaMA\n√ √\n×\nVideoChat\n√ √ √\nPandaGPT\n√ √\n×\nVideo-LLaVA\n√\n√\n×\nSALMONN\n√ √\n×\nVideoChat2\n√ √\n×\nVideo-ChatGPT\n√ √\n×\nOneLLM\n√ √\n×\nLLaMA-VID\n√ √\n×\nmPLUG-Owl\n√ √\n×\nChat-UniVi\n√ √\n×\nGPT-4V",
          "English": "38.13\n49.42\n31.04\n42.84\n45.92\n40.15\n43.51\n50.71\n38.09\n44.73\n44.14\n45.34\n45.53\n42.90\n48.49\n45.89\n50.03\n42.38\n47.07\n48.58\n45.66\n47.96\n50.20\n45.92\n49.07\n54.72\n44.47\n50.52\n54.03\n47.44\n50.52\n55.93\n46.06\n51.25\n52.71\n49.87\n52.73\n54.54\n51.04\n53.08\n53.68\n52.50\n55.51\n48.52\n64.86",
          "Chinese": "41.14\n53.71\n33.34\n46.17\n52.07\n41.47\n46.22\n52.65\n41.18\n47.26\n47.98\n46.56\n45.57\n47.20\n44.05\n47.33\n53.01\n42.75\n49.21\n53.95\n45.23\n48.24\n52.24\n44.82\n48.86\n57.12\n42.68\n54.73\n61.15\n49.52\n51.44\n56.43\n47.26\n52.01\n57.30\n47.61\n50.95\n56.40\n46.47\n53.86\n58.54\n49.86\n57.21\n54.61\n60.07"
        }
      ],
      "page": 36
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Pattern recognition and machine learning",
      "authors": [
        "C Bishop"
      ],
      "year": "2006",
      "venue": "Pattern recognition and machine learning"
    },
    {
      "citation_id": "2",
      "title": "Language models are few-shot learners",
      "authors": [
        "T Brown",
        "B Mann",
        "N Ryder",
        "M Subbiah",
        "J Kaplan",
        "P Dhariwal",
        "A Neelakantan",
        "P Shyam",
        "G Sastry",
        "A Askell"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "3",
      "title": "A database of german emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "In Interspeech"
    },
    {
      "citation_id": "4",
      "title": "Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan",
        "Iemocap"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "5",
      "title": "Msp-improv: An acted corpus of dyadic interactions to study emotion perception",
      "authors": [
        "C Busso",
        "S Parthasarathy",
        "A Burmania",
        "M Abdelwahab",
        "N Sadoughi",
        "E Provost"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "6",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "7",
      "title": "Qwen-audio: Advancing universal audio understanding via unified large-scale audiolanguage models",
      "authors": [
        "Y Chu",
        "J Xu",
        "X Zhou",
        "Q Yang",
        "S Zhang",
        "Z Yan",
        "C Zhou",
        "J Zhou"
      ],
      "year": "2023",
      "venue": "Qwen-audio: Advancing universal audio understanding via unified large-scale audiolanguage models",
      "arxiv": "arXiv:2311.07919"
    },
    {
      "citation_id": "8",
      "title": "Emovo corpus: an italian emotional speech database",
      "authors": [
        "G Costantini",
        "I Iaderola",
        "A Paoloni",
        "M Todisco"
      ],
      "year": "2014",
      "venue": "Proceedings of the ninth international conference on language resources and evaluation (LREC'14)"
    },
    {
      "citation_id": "9",
      "title": "Learning from ambiguously labeled images",
      "authors": [
        "T Cour",
        "B Sapp",
        "C Jordan",
        "B Taskar"
      ],
      "year": "2009",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "10",
      "title": "The Expression of Emotions in Man and Animals",
      "authors": [
        "C Darwin"
      ],
      "venue": "The Expression of Emotions in Man and Animals"
    },
    {
      "citation_id": "11",
      "title": "A dataset of fine-grained emotions",
      "authors": [
        "D Demszky",
        "D Movshovitz-Attias",
        "J Ko",
        "A Cowen",
        "G Nemade",
        "S Ravi",
        "Goemotions"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "12",
      "title": "Multi-label emotion detection via emotion-specified feature extraction and emotion correlation learning",
      "authors": [
        "J Deng",
        "F Ren"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "13",
      "title": "Video and image based emotion recognition challenges in the wild: Emotiw 2015",
      "authors": [
        "A Dhall",
        "O Ramana Murthy",
        "R Goecke",
        "J Joshi",
        "T Gedeon"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 ACM on international conference on multimodal interaction"
    },
    {
      "citation_id": "14",
      "title": "From individual to group-level emotion recognition: Emotiw 5.0",
      "authors": [
        "A Dhall",
        "R Goecke",
        "S Ghosh",
        "J Joshi",
        "J Hoey",
        "T Gedeon"
      ],
      "year": "2017",
      "venue": "Proceedings of the 19th ACM international conference on multimodal interaction"
    },
    {
      "citation_id": "15",
      "title": "The mexican emotional speech database (mesd): elaboration and assessment based on machine learning",
      "authors": [
        "M Duville",
        "L Alonso-Valerdi",
        "D Ibarra-Zarate"
      ],
      "year": "2021",
      "venue": "2021 43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)"
    },
    {
      "citation_id": "16",
      "title": "An argument for basic emotions",
      "authors": [
        "P Ekman"
      ],
      "year": "1992",
      "venue": "Cognition & emotion"
    },
    {
      "citation_id": "17",
      "title": "Emotionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild",
      "authors": [
        "Fabian Benitez-Quiroz",
        "C Srinivasan",
        "R Martinez"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "18",
      "title": "Provably consistent partial-label learning",
      "authors": [
        "L Feng",
        "J Lv",
        "B Han",
        "M Xu",
        "G Niu",
        "X Geng",
        "B An",
        "M Sugiyama"
      ],
      "year": "2020",
      "venue": "Proceedings of the Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "19",
      "title": "Scaling openvocabulary image segmentation with image-level labels",
      "authors": [
        "G Ghiasi",
        "X Gu",
        "Y Cui",
        "T.-Y Lin"
      ],
      "year": "2022",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "20",
      "title": "Discriminative methods for multi-labeled classification",
      "authors": [
        "S Godbole",
        "S Sarawagi"
      ],
      "year": "2004",
      "venue": "Pacific-Asia conference on knowledge discovery and data mining"
    },
    {
      "citation_id": "21",
      "title": "Challenges in representation learning: A report on three machine learning contests",
      "authors": [
        "I Goodfellow",
        "D Erhan",
        "P Carrier",
        "A Courville",
        "M Mirza",
        "B Hamner",
        "W Cukierski",
        "Y Tang",
        "D Thaler",
        "D.-H Lee"
      ],
      "year": "2013",
      "venue": "Neural information processing: 20th international conference"
    },
    {
      "citation_id": "22",
      "title": "The vera am mittag german audio-visual emotional speech database",
      "authors": [
        "M Grimm",
        "K Kroschel",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "IEEE International Conference on Multimedia and Expo"
    },
    {
      "citation_id": "23",
      "title": "Multimodal affective analysis using hierarchical attention strategy with word-level alignment",
      "authors": [
        "Y Gu",
        "K Yang",
        "S Fu",
        "S Chen",
        "X Li",
        "I Marsic"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "24",
      "title": "Onellm: One framework to align all modalities with language",
      "authors": [
        "J Han",
        "K Gong",
        "Y Zhang",
        "J Wang",
        "K Zhang",
        "D Lin",
        "Y Qiao",
        "P Gao",
        "X Yue"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "25",
      "title": "Modality-invariant and-specific representations for multimodal sentiment analysis",
      "authors": [
        "D Hazarika",
        "R Zimmermann",
        "S Poria",
        "Misa"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "26",
      "title": "Joint binary neural network for multilabel learning with applications to emotion classification",
      "authors": [
        "H He",
        "R Xia"
      ],
      "year": "2018",
      "venue": "Natural Language Processing and Chinese Computing: 7th CCF International Conference"
    },
    {
      "citation_id": "27",
      "title": "Seq2emo: A sequence to multi-label emotion classification model",
      "authors": [
        "C Huang",
        "A Trabelsi",
        "X Qin",
        "N Farruque",
        "L Mou",
        "O Zaiane"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 conference of the North American chapter of the association for computational linguistics: human language technologies"
    },
    {
      "citation_id": "28",
      "title": "Surrey audio-visual expressed emotion (savee) database",
      "authors": [
        "P Jackson",
        "S Haq"
      ],
      "year": "2014",
      "venue": "Surrey audio-visual expressed emotion (savee) database"
    },
    {
      "citation_id": "29",
      "title": "An open source emotional speech corpus for human robot interaction applications",
      "authors": [
        "J James",
        "L Tian",
        "C Watson"
      ],
      "year": "2018",
      "venue": "An open source emotional speech corpus for human robot interaction applications"
    },
    {
      "citation_id": "30",
      "title": "What is emotion?",
      "authors": [
        "W James"
      ],
      "year": "1884",
      "venue": "Mind"
    },
    {
      "citation_id": "31",
      "title": "Dfew: A large-scale database for recognizing dynamic facial expressions in the wild",
      "authors": [
        "X Jiang",
        "Y Zong",
        "W Zheng",
        "C Tang",
        "W Xia",
        "C Lu",
        "J Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "32",
      "title": "Chat-univi: Unified visual representation empowers large language models with image and video understanding",
      "authors": [
        "P Jin",
        "R Takanobu",
        "W Zhang",
        "X Cao",
        "L Yuan"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "33",
      "title": "Afew-va database for valence and arousal estimation in-the-wild",
      "authors": [
        "J Kossaifi",
        "G Tzimiropoulos",
        "S Todorovic",
        "M Pantic"
      ],
      "year": "2017",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "34",
      "title": "Sewa db: A rich database for audio-visual emotion and sentiment research in the wild",
      "authors": [
        "J Kossaifi",
        "R Walecki",
        "Y Panagakis",
        "J Shen",
        "M Schmitt",
        "F Ringeval",
        "J Han",
        "V Pandit",
        "A Toisoul",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "35",
      "title": "Language-driven semantic segmentation",
      "authors": [
        "B Li",
        "K Weinberger",
        "S Belongie",
        "V Koltun",
        "R Ranftl"
      ],
      "year": "2021",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "36",
      "title": "A multi-modal model with in-context instruction tuning",
      "authors": [
        "B Li",
        "Y Zhang",
        "L Chen",
        "J Wang",
        "J Yang",
        "Z Liu",
        "Otter"
      ],
      "year": "2023",
      "venue": "A multi-modal model with in-context instruction tuning",
      "arxiv": "arXiv:2305.03726"
    },
    {
      "citation_id": "37",
      "title": "Chat-centric video understanding",
      "authors": [
        "K Li",
        "Y He",
        "Y Wang",
        "Y Li",
        "W Wang",
        "P Luo",
        "Y Wang",
        "L Wang",
        "Y Qiao",
        "Videochat"
      ],
      "year": "2023",
      "venue": "Chat-centric video understanding",
      "arxiv": "arXiv:2305.06355"
    },
    {
      "citation_id": "38",
      "title": "Mvbench: A comprehensive multi-modal video understanding benchmark",
      "authors": [
        "K Li",
        "Y Wang",
        "Y He",
        "Y Li",
        "Y Wang",
        "Y Liu",
        "Z Wang",
        "J Xu",
        "G Chen",
        "P Luo",
        "L Wang",
        "Y Qiao"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "39",
      "title": "Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild",
      "authors": [
        "S Li",
        "W Deng",
        "J Du"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "40",
      "title": "Llama-vid: An image is worth 2 tokens in large language models",
      "authors": [
        "Y Li",
        "C Wang",
        "J Jia"
      ],
      "year": "2024",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "41",
      "title": "Multilabel learning, modality robustness, and semi-supervised learning",
      "authors": [
        "Z Lian",
        "H Sun",
        "L Sun",
        "K Chen",
        "M Xu",
        "K Wang",
        "K Xu",
        "Y He",
        "Y Li",
        "J Zhao"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "42",
      "title": "Mer 2024: Semi-supervised learning, noise robustness, and open-vocabulary multimodal emotion recognition",
      "authors": [
        "Z Lian",
        "H Sun",
        "L Sun",
        "Z Wen",
        "S Zhang",
        "S Chen",
        "H Gu",
        "J Zhao",
        "Z Ma",
        "X Chen"
      ],
      "year": "2024",
      "venue": "Mer 2024: Semi-supervised learning, noise robustness, and open-vocabulary multimodal emotion recognition",
      "arxiv": "arXiv:2404.17113"
    },
    {
      "citation_id": "43",
      "title": "A unified evaluation benchmark for multimodal emotion recognition",
      "authors": [
        "Z Lian",
        "L Sun",
        "Y Ren",
        "H Gu",
        "H Sun",
        "L Chen",
        "B Liu",
        "J Tao",
        "Merbench"
      ],
      "year": "2024",
      "venue": "A unified evaluation benchmark for multimodal emotion recognition",
      "arxiv": "arXiv:2401.03429"
    },
    {
      "citation_id": "44",
      "title": "Gpt-4v with emotion: A zero-shot benchmark for generalized emotion recognition",
      "authors": [
        "Z Lian",
        "L Sun",
        "H Sun",
        "K Chen",
        "Z Wen",
        "H Gu",
        "B Liu",
        "J Tao"
      ],
      "year": "2024",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "45",
      "title": "Video-llava: Learning united visual representation by alignment before projection",
      "authors": [
        "B Lin",
        "Y Ye",
        "B Zhu",
        "J Cui",
        "M Ning",
        "P Jin",
        "L Yuan"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "46",
      "title": "Microsoft coco: Common objects in context",
      "authors": [
        "T.-Y Lin",
        "M Maire",
        "S Belongie",
        "J Hays",
        "P Perona",
        "D Ramanan",
        "P Dollár",
        "C Zitnick"
      ],
      "year": "2014",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "47",
      "title": "Emotion and intent joint understanding in multimodal conversation: A benchmarking dataset",
      "authors": [
        "R Liu",
        "H Zuo",
        "Z Lian",
        "X Xing",
        "B Schuller",
        "H Li"
      ],
      "year": "2024",
      "venue": "Emotion and intent joint understanding in multimodal conversation: A benchmarking dataset",
      "arxiv": "arXiv:2407.02751"
    },
    {
      "citation_id": "48",
      "title": "A large-scale, multi-modal, compound affective database for dynamic facial expression recognition in the wild",
      "authors": [
        "Y Liu",
        "W Dai",
        "C Feng",
        "W Wang",
        "G Yin",
        "J Zeng",
        "S Shan",
        "Mafw"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "49",
      "title": "Make acoustic and visual cues matter: Ch-sims v2. 0 dataset and av-mixup consistent module",
      "authors": [
        "Y Liu",
        "Z Yuan",
        "H Mao",
        "Z Liang",
        "W Yang",
        "Y Qiu",
        "T Cheng",
        "X Li",
        "H Xu",
        "K Gao"
      ],
      "year": "2022",
      "venue": "Proceedings of the International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "50",
      "title": "Efficient low-rank multimodal fusion with modality-specific factors",
      "authors": [
        "Z Liu",
        "Y Shen",
        "V Lakshminarasimhan",
        "P Liang",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "51",
      "title": "The ryerson audiovisual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PloS One"
    },
    {
      "citation_id": "52",
      "title": "Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "53",
      "title": "Progressive identification of true labels for partiallabel learning",
      "authors": [
        "J Lv",
        "M Xu",
        "L Feng",
        "G Niu",
        "X Geng",
        "M Sugiyama"
      ],
      "year": "2020",
      "venue": "Progressive identification of true labels for partiallabel learning"
    },
    {
      "citation_id": "54",
      "title": "Learning word vectors for sentiment analysis",
      "authors": [
        "A Maas",
        "R Daly",
        "P Pham",
        "D Huang",
        "A Ng",
        "C Potts"
      ],
      "year": "2011",
      "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "55",
      "title": "Videochatgpt: Towards detailed video understanding via large vision and language models",
      "authors": [
        "M Maaz",
        "H Rasheed",
        "S Khan",
        "F Khan"
      ],
      "year": "2024",
      "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "56",
      "title": "The enterface'05 audio-visual emotion database",
      "authors": [
        "O Martin",
        "I Kotsia",
        "B Macq",
        "I Pitas"
      ],
      "year": "2006",
      "venue": "Proceedings of the 22nd International Conference on Data Engineering Workshops"
    },
    {
      "citation_id": "57",
      "title": "The semaine database: Annotated multimodal records of emotionally colored conversations between a person and a limited agent",
      "authors": [
        "G Mckeown",
        "M Valstar",
        "R Cowie",
        "M Pantic",
        "M Schroder"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "58",
      "title": "Society of mind",
      "authors": [
        "M Minsky"
      ],
      "year": "1988",
      "venue": "Society of mind"
    },
    {
      "citation_id": "59",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "60",
      "title": "Towards multimodal sentiment analysis: Harvesting opinions from the web",
      "authors": [
        "L.-P Morency",
        "R Mihalcea",
        "P Doshi"
      ],
      "year": "2011",
      "venue": "Proceedings of the 13th International Conference on Multimodal Interfaces"
    },
    {
      "citation_id": "61",
      "title": "Gpt-4v(ision) system card",
      "year": "2023",
      "venue": "Gpt-4v(ision) system card"
    },
    {
      "citation_id": "62",
      "title": "Thumbs up? sentiment classification using machine learning techniques",
      "authors": [
        "B Pang",
        "L Lee",
        "S Vaithyanathan"
      ],
      "year": "2002",
      "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "63",
      "title": "Utterance-level multimodal sentiment analysis",
      "authors": [
        "V Pérez-Rosas",
        "R Mihalcea",
        "L.-P Morency"
      ],
      "year": "2013",
      "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "64",
      "title": "A general psychoevolutionary theory of emotion",
      "authors": [
        "R Plutchik"
      ],
      "year": "1980",
      "venue": "Emotion: Theory, research, and experience"
    },
    {
      "citation_id": "65",
      "title": "The nature of emotions: Human emotions have deep evolutionary roots, a fact that may explain their complexity and provide tools for clinical practice",
      "authors": [
        "R Plutchik"
      ],
      "year": "2001",
      "venue": "American Scientist"
    },
    {
      "citation_id": "66",
      "title": "A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea",
        "Meld"
      ],
      "year": "2019",
      "venue": "A multimodal multi-party dataset for emotion recognition in conversations"
    },
    {
      "citation_id": "67",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "68",
      "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
      "authors": [
        "R Socher",
        "A Perelygin",
        "J Wu",
        "J Chuang",
        "C Manning",
        "A Ng",
        "C Potts"
      ],
      "year": "2013",
      "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "69",
      "title": "One model to instruction-follow them all",
      "authors": [
        "Y Su",
        "T Lan",
        "H Li",
        "J Xu",
        "Y Wang",
        "D Cai",
        "Pandagpt"
      ],
      "year": "2023",
      "venue": "Proceedings of the 1st Workshop on Taming Large Language Models: Controllability in the era of Interactive Assistants"
    },
    {
      "citation_id": "70",
      "title": "Hierarchical contrastive masked autoencoder for self-supervised audiovisual emotion recognition",
      "authors": [
        "L Sun",
        "Z Lian",
        "B Liu",
        "J Tao",
        "Hicmae"
      ],
      "year": "2024",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "71",
      "title": "Salmonn: Towards generic hearing abilities for large language models",
      "authors": [
        "C Tang",
        "W Yu",
        "G Sun",
        "X Chen",
        "T Tan",
        "W Li",
        "L Lu",
        "Z Ma",
        "C Zhang"
      ],
      "year": "2023",
      "venue": "Proceedings of the Twelfth International Conference on Learning Representations"
    },
    {
      "citation_id": "72",
      "title": "Label Studio: Data labeling software",
      "authors": [
        "M Tkachenko",
        "M Malyuk",
        "A Holmanyuk",
        "N Liubimov"
      ],
      "year": "2020",
      "venue": "Open source software available from"
    },
    {
      "citation_id": "73",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Y.-H Tsai",
        "S Bai",
        "P Liang",
        "J Kolter",
        "L.-P Morency",
        "R Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Multimodal transformer for unaligned multimodal language sequences"
    },
    {
      "citation_id": "74",
      "title": "Learning factorized multimodal representations",
      "authors": [
        "Y.-H Tsai",
        "P Liang",
        "A Zadeh",
        "L.-P Morency",
        "R Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the 7th International Conference on Learning Representations"
    },
    {
      "citation_id": "75",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Proceedings of the Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "76",
      "title": "Norms of valence, arousal, and dominance for 13,915 english lemmas",
      "authors": [
        "A Warriner",
        "V Kuperman",
        "M Brysbaert"
      ],
      "year": "2013",
      "venue": "Behavior research methods"
    },
    {
      "citation_id": "77",
      "title": "Youtube movie reviews: Sentiment analysis in an audio-visual context",
      "authors": [
        "M Wöllmer",
        "F Weninger",
        "T Knaup",
        "B Schuller",
        "C Sun",
        "K Sagae",
        "L.-P Morency"
      ],
      "year": "2013",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "78",
      "title": "Survey on audiovisual emotion recognition: databases, features, and data fusion strategies",
      "authors": [
        "C.-H Wu",
        "J.-C Lin",
        "W.-L Wei"
      ],
      "year": "2014",
      "venue": "APSIPA Transactions on Signal and Information Processing"
    },
    {
      "citation_id": "79",
      "title": "Towards open vocabulary learning: A survey",
      "authors": [
        "J Wu",
        "X Li",
        "S Xu",
        "H Yuan",
        "H Ding",
        "Y Yang",
        "X Li",
        "J Zhang",
        "Y Tong",
        "X Jiang"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "80",
      "title": "Secap: Speech emotion captioning with large language model",
      "authors": [
        "Y Xu",
        "H Chen",
        "J Yu",
        "Q Huang",
        "Z Wu",
        "S.-X Zhang",
        "G Li",
        "Y Luo",
        "R Gu"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "81",
      "title": "mplug-owl: Modularization empowers large language models with multimodality",
      "authors": [
        "Q Ye",
        "H Xu",
        "G Xu",
        "J Ye",
        "M Yan",
        "Y Zhou",
        "J Wang",
        "A Hu",
        "P Shi",
        "Y Shi"
      ],
      "year": "2023",
      "venue": "mplug-owl: Modularization empowers large language models with multimodality",
      "arxiv": "arXiv:2304.14178"
    },
    {
      "citation_id": "82",
      "title": "Ch-sims: A chinese multimodal sentiment analysis dataset with fine-grained annotation of modality",
      "authors": [
        "W Yu",
        "H Xu",
        "F Meng",
        "Y Zhu",
        "Y Ma",
        "J Wu",
        "J Zou",
        "K Yang"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "83",
      "title": "Tensor fusion network for multimodal sentiment analysis",
      "authors": [
        "A Zadeh",
        "M Chen",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "84",
      "title": "Memory fusion network for multi-view sequential learning",
      "authors": [
        "A Zadeh",
        "P Liang",
        "N Mazumder",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "85",
      "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "A Zadeh",
        "P Liang",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "86",
      "title": "Openvocabulary object detection using captions",
      "authors": [
        "A Zareian",
        "K Rosa",
        "D Hu",
        "S.-F Chang"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "87",
      "title": "Video-llama: An instructiontuned audio-visual language model for video understanding",
      "authors": [
        "H Zhang",
        "X Li"
      ],
      "year": "2023",
      "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing: System Demonstrations"
    },
    {
      "citation_id": "88",
      "title": "Deep learning-based multimodal emotion recognition from audio, visual, and text modalities: A systematic review of recent advancements and future prospects",
      "authors": [
        "S Zhang",
        "Y Yang",
        "C Chen",
        "X Zhang",
        "Q Leng",
        "X Zhao"
      ],
      "year": "2024",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "89",
      "title": "From facial expression recognition to interpersonal relation prediction",
      "authors": [
        "Z Zhang",
        "P Luo",
        "C Loy",
        "X Tang"
      ],
      "year": "2018",
      "venue": "International Journal of Computer Vision"
    }
  ]
}