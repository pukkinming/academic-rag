{
  "paper_id": "2211.15425v1",
  "title": "Faf: A Novel Multimodal Emotion Recognition Approach Integrating Face, Body And Text",
  "published": "2022-11-20T14:43:36Z",
  "authors": [
    "Zhongyu Fang",
    "Aoyun He",
    "Qihui Yu",
    "Baopeng Gao",
    "Weiping Ding",
    "Tong Zhang",
    "Lei Ma"
  ],
  "keywords": [
    "Facial expression",
    "Body posture",
    "Text",
    "Multi-modal",
    "Attention"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal emotion analysis performed better in emotion recognition depending on more comprehensive emotional clues and multimodal emotion dataset. In this paper, we developed a large multimodal emotion dataset, named \"HED\" dataset, to facilitate the emotion recognition task, and accordingly propose a multimodal emotion recognition method. Specifically, the \"HED\" dataset contains happy, sad, disgust, angry and scared emotion-aligned face, body and text samples, which are much larger than existing datasets. Moreover, the emotion labels were correspondingly attached to those samples by strictly following a standard psychological paradigm. To promote recognition accuracy, \"Feature After Feature\" framework was used to explore crucial emotional information from the aligned face-body-text samples. For the images, a residual network was used for feature extraction. To understand the text, we use the BERT word vector for feature selection. To fuse all the emotion clues, the image features were initially fused, and the fused feature vectors were stitched with text features to form the combined features. Then, the combined features are further explored by using convolutional layers to explore the high-level complementary information among the multimodal information, and the attention mechanism was introduced to give different weights and improve the performance of emotion recognition in the fused modality. We employ various benchmarks to evaluate the \"HED\" dataset and compare the performance with our method. The results show that the five-classification accuracy of the proposed multimodal fusion method is about 83.75%, and the performance is improved by 1.83%, 9.38%, and 21.62% respectively compared with that of individual modalities. The complementarity between each channel is effectively used to improve the performance of emotion recognition. We had also established a multimodal online emotion prediction platform, aiming to provide free emotion prediction to more users.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition is an important area of psychological and computer research. How to improve the accuracy of emotion recognition has become a primary issue. In recent years, with the continuous development of artificial intelligence technology, human-computer interaction has become the focus of research in the field of information science. As one of the critical technologies to realize human-computer interaction, emotion recognition has gradually received a lot of attention from researchers. At present, most of the research works on emotion recognition are based on single-modal, such as facial expressions  [1] [2] [3] , body movements  [4] [5]  and speech text  [6] [7] . However, emotion recognition based on unimodal often has limitations and, in most cases, could only reflect a portion of human emotional expression. Multimodal emotion recognition can link individual unimodal channels and use the feature complementarity between channels to combine multiple information to determine the emotional state. Studies have shown that the multimodal emotion recognition approach has better performance than unimodal emotion judgment in most cases  [8] .\n\nThe difficulty of multimodal recognition is not only to control the internal information of individual modality (Intra-modality), but also to complement the interactive features between individual modalities (Inter-modality). It has been extensively studied by scholars, such as Tensor Fusion Network (TFN) proposed by Zadeh et al  [9] , Polynomial Tensor Pooling (PTP) proposed by Hou et al  [10] , and Memory Fusion Network (MFN) presented by Zadeh et al  [11] . (MFN) proposed by Zadeh et al  [11] , etc. These models have achieved good results by adding interactions among the modalities based on the use of information within a single modality. However, considering the time complexity and space complexity as well as the redundancy of high-dimensional feature information, especially in the face of problems such as the excessive dimensionality of fused features, the above methods need to be further explored in multimodal fusion feature processing. In the emotion recognition field, the current multimodal field tends to combine human facial expressions with modalities such as voice and text, and there are also studies that combine expressions with physiological factors such as EEG and ECG, which significantly improve the emotion recognition results. However, some studies have shown that body posture is also an important way to represent human emotions  [12] . Also, the quality of dataset is one of the key factors affecting deep learning. The existing datasets such as Extended Cohn-Kanada (CK+) collected by P. Lucy, CK+ dataset contains 327 sequences of labeled expression pictures of 123 objects, which are divided into seven expressions of normal, angry, contempt, disgust, fear, happy and sad, the number is small and not conducive to large-scale training; EmotionNet dataset collected from Internet about one million images, which contains basic expressions, compound expressions, and the annotation of expression units, with a slightly lower correct rate; Fer2013 contains a total of 26190 48*48 grayscale images with six kinds of expressions, with a lower resolution.\n\nIn summary, our work make the following contributions: Firstly, we build and expose sentiment datasets based on facial emotions and body gestures, which were collected from the Internet and manually filtered and detected; Secondly, we propose a novel multimodal sentiment recognition method that integrates face, body and text; Finally, we build a multimodal sentiment detection platform based on the FAF framework approach to provide free sentiment prediction to more users This paper is organized as follows: Section 2 introduces the work related to multimodal emotion recognition; Section 3 details the specific implementation of multimodal fusion; Section 4 presents the experimental results data and the analysis of the results; the last section concludes the paper and looks forward to the next work.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "With the development of deep learning techniques, some of the complex machine learning problems of the classification have been solved. Deep learning has greatly contributed to the solution of larger and more complex multimodal analysis problems  [15] . Neural networks based multimodal emotion recognition problems has gradually become a research direction for many scholars.\n\nModal fusion approaches can be broadly classified into four types, which are data-level fusion, feature-level fusion, decision-level fusion, and model-level fusion  [16] . For example, Minotto et al  [17]  used SVM for sensing-level fusion to further mine the modal data information; Wang et al  [18]  presented SAL (Select-Additive Learning) to stitch multiple modal features before correlation, and achieved better results; Liu et al  [19]  proposed a multi-level fusion method, which effectively enriched the semantics of the upper and lower layers; Middya et al  [20]  proposed model layer fusion based on audio data, which improved the generalization ability of the model in multimodal sentiment analysis. In terms of emotion recognition, Jiang et al  [21]  combined facial expressions and body movements to more effectively tap the deep semantic correspondence between them; Morency et al  [22]  expanded multimodal emotion recognition to three types of information: image, text and speech, and further complemented the multimodal dataset; Nguyen et al  [23]  even presented to combine facial expressions, pose body movements and voice, among other sources, were combined to recognize emotions. Unlike physical information, research scholars have further explored the physiological information evoked by emotions. Zhang et al  [24]  combined physiological information such as EEG and ECG and mapped them at a high level to effectively improve the accuracy of emotion recognition; Katsigiannis et al  [25]  provided a multimodal database of physiological information to further promote the development of emotion recognition research.\n\nAs discussed, although many efforts have been made on multimodal sentiment recognition methods, the mentioned algorithms have the following limitations and challenges.  (1)  The quality and quantity of previous datasets need to be extended.  (2)  The multimodal fusion algorithm is not accurate enough for the extraction of combined features after feature fusion. (3) Time complexity and space complexity are not ideal and waste excessive computational resources in some aspects. Inspired by the above work, this paper proposes a multimodal emotion recognition method based on face, gesture and text. Unlike the early fusion methods, it has further research in the processing of fused features after fusion. Considering combining the internal information in a single modality and the interaction information between modalities and solving the problems of information redundancy and high dimensionality, this paper achieves the best result of tri-modal fusion by combining features followed by a two-dimensional convolution operation and adding attention weights. To address the problems of insufficient data volume and excessive irrelevant noise in existing datasets, this paper openly constructs its own facial emotion and body movement datasets for sentiment analysis research.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Dataset",
      "text": "4. Tree structure was used to build the dataset, with five emotion nouns as subtrees (angry, disgust, happy, sad, and sacred), each including three modalities (face, body, and text. where the face and body) were segmented from the images as face and body, and the background and associated text of the images were extracted as text. The image dataset was acquired by two experts in psychology from various sources such as movies and TV shows. Among them, the ages include young children, young adults, and old adults; the geographical areas include Asia, Africa, and Europe. In this dataset, five of these emotion categories are taken and the dataset has a total sample size of 17,441 after data enhancement. The samples of facial and physical emotion categories are shown in Figure  1  and Figure  2 . The number of each sample is shown in Table  1 .  The flowchart of the method in this paper is shown in Figure  3 . For the portrait data, it is firstly pre-processed to separate facial expressions and body movements; then facial, body and text features are extracted using Resdual network and Bidirectional Encoder Representations from Transformers respectively; finally, after feature-level fusion to train the fused features, the results of emotion recognition are obtained.\n\nFigure  3  The general flow chart of the method in this paper",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Data Pre-Processing",
      "text": "The image dataset is segmented into face and body by Opencv-DNN and Dilb-Face, and the size was normalized to 224*224. The purpose of using methods is to complement each other and reduce missed detections, and it is not easy to show the full body movements of the person when segmenting the body with Opencv-DNN, so Dilb-Face is used to detect the 68 key points of the face  [26]  before segmentation.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Feature Input Layer",
      "text": "The feature input layer consists of feature data from three modalities, namely face data, body data and text data. Each of the three-modal data was extracted from the primary representation by a linear network and the feature data was aligned, i.e., the three modal data are two-dimensional vectors with the same length in the first dimension but not in the second dimension. The text features are BERT word vectors with dimension 768; the facial data and body data were obtained as feature input vectors through the ResNet network framework, both with dimension 2048.\n\nThe feature information of the three modalities is shown as follows.\n\n} , , {\n\nwhere is the set of f F (Face), b F (Body) and t F (Text), n containing the information of the three modalities, i is the sample sequence, I is the total sample length, n x d is the size of the modal feature dimension, n X is the matrix feature representation of the modality.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Feature Fusion",
      "text": "The facial and body feature data were decomposed from the portrait image, and both have the same number of channels and similar semantics of the feature map, so the first fusion was performed. The fused feature vector was then fused with the text features for a second time to obtain the combined features, which are represented as follows.\n\n) ( ) , , (\n\nThe combined features were further mined for high-level complementary information between multimodal information using convolutional layers, and SE blocks were added to assign different weights to the features.SE blocks act as an attention mechanism on the feature graph to motivate important features and suppress unimportant ones  [27] . The specific representation is as follows.\n\nwhere c represents the number of channels, Conv2D represents the two-dimensional convolution function, K represents the convolution kernel, ÔÅ≥ represents the sigmoid activation function, ÔÅ§ represents the relu activation function, ùëä ‚àà ‚Ñù Ôºåùëä ‚àà ‚Ñù are the weight matrices of the two fully connected layers, respectively.\n\nThe processed combined features were retained the strongest feature information by maximum pooling, and the final classification prediction results were output through the fully connected layer as follows.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "}",
      "text": "where i is the index of the combined features, ÔÇ∂ is the Softmax activation function, C W and C b are the weights and biases of the Softmax layers.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Optimization Strategy",
      "text": "Cross entropy was used as the loss function in all model training processes, and the formula as follows\n\nWhere, y denotes the true label, Y denotes the predicted label, I denotes the total number of training samples, and C denotes the number of categories. The Adam (Adaptive Moment Estimation  [28] ) optimizer was also used to optimize the parameters of the network.\n\nLogitscale ÔÇ¨ Self-learing weighting parameters for each",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experiments And Results",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experimental Setup And Environment Configuration",
      "text": "The operating system environment used in this paper is Windows 10; the CPU version is Intel(R) Xeon(R) Gold 5215M; the GPU version is GTX 1080Ti; the programming language is Python 3.9; the deep learning environment is PyTorch 1.10.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "In this paper, Recall, Precision, F1-score, Accuracy, and confusion matrix and ROC curve are mainly used as the evaluation metrics of the model. The experimental metrics were calculated as follows.\n\nWhere, TP denotes the number of actual true categories and predicted as true categories; TN denotes the number of actual wrong categories and predicted as wrong categories; FN denotes the number of actual wrong categories and predicted as true categories; FN denotes the number of actual true categories and predicted as wrong categories.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Results Analysis",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Unimodal Sentiment Recognition Experiments",
      "text": "The unimodal emotion recognition experiments were conducted for facial expressions, body gestures and text data. VGG16  [30] , ResNet50 and ViT  [31]  were selected for comparison to determine the network for feature extraction in multimodal fusion for the image dataset, and LSTM  [32]  and BERT were selected for the text dataset to determine their feature word vectors. Figure  4  indicates the confusion matrix of each network under single modality, and Figure  5  indicates the ROC curves of each network under single modality. The facial and body modality results show that ResNet50 performs best compared to the traditional VGG16 for the same picture, which is inseparable from its unique residual connectivity and deeper convolutional layers. The use of ViT as a transformer in vision is slightly less effective, which may be related to the size of the dataset and the complexity of the expressions it faces. In the natural language domain, however, the transformer-based BERT network outperforms the LSTM by far, thanks of course to its special attention mechanism.  The ROC curve results in Figure  5  illustrate that happy and disgust have the highest correct rates among the five emotion classes, while scared has the lowest correct rate of recognition. These results may be related to the recognition characteristics of emotion, and the performance of the algorithm model. For example, when people feel happy, they may uplift corners of the mouth, dimples, and with the corners of the mouth dropping while disgust. Besides, scared has a variety of expressions related to personal habits, therefore has the worst recognition results. The rest of the emotions have fewer outwardly expressed features, and it is difficult for the classifier to fully learn the corresponding emotion.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Multimodal Sentiment Fusion Experiments",
      "text": "The unimodal feature vectors have been obtained in section 4.4.1 by emotion recognition of face, body and text, respectively. ResNet50 network was chosen for both face and body; BERT network was chosen for text. Here multimodal feature fusion experiments will be performed. The unimodal feature vectors were fused two by two as well as the final fusion of three feature vectors. To demonstrate the model generalization capability, this paper transitions the five-classification problem in a generalized environment to a classification problem in a specific scenario such as a sports environment. Sports events are mostly positive and negative, so this paper designates sports events as a binary classification problem. We compare the proposed method with the methods shown in Table  4 , where SOTA1 and SOTA2 denote the current models with the best performance and the second-best performance, respectively. Table  3  shows the results of corresponding metrics after multimodal fusion; Figure  6  shows the confusion matrices of two-two fusion and last threemode fusion for each modality respectively; Figure  7  shows the ROC curves of two-two fusion and last three-mode fusion for each modality respectively; Figures  8, 9 , 10 show the original sample dispersion, sample normalized dispersion, three-The dispersion of the original sample, the dispersion of the sample after normalization, and the dispersion of the cluster after trimodal fusion are shown in Figures  8, 9 , and 10, respectively. Figure  11  show the Grad-CAM [35] attention mechanism result of computer and human. CentralNet [33]   0.8053 0.8047 0.8024 0.8215 LWF [34]  0.7326 0.6963 0.6964 0.7214 FAF 0.8195 0.8196 0.8190 0.8375\n\nComparison With current methods: Tables 7 show the comparison results of FAF with current best methods. We can clearly see that FAF is outperform other methods. This allows us to conclude that FAF is a good choice when dealing with complex and imbalanced images. Unsurprisingly, standard SVM performs worst of all of the algorithms, while three other methods try to offset their inability to handle spatial properties of data with advanced instance generation modules. Both CentralNet return the best results from all four tested algorithms, with LWF coming close to GANbased methods. This can be attributed to their compound oversampling solutions, which analyze the difficulty of instances and optimize the placement of new instances, while cleaning overlapping areas. However, this comes at the cost of very high computational complexity and challenging parameter tuning. FAF returns superior balanced training sets compared to pixel-based approaches, while providing an intuitive and easy to tune architecture and, according to both nonparametric and Bayesian tests presented in Table  IV , outperforms all pixel-based approaches in a statistically significant manner.  Figures  6  and 7  show comparison results between the bimodal and trimodal. It can be seen that happy and disgust perform best in the bimodal case, which may be related to the datasets; scared performs the worst, which may be due to the reaction time factor associated with fear, just as when we show fear, the pupils may first dilate and be accompanied by This may be due to the reaction time factor associated with fear, as when we show fear, the pupils may first dilate and be accompanied by a series of coherent responses such as mouth opening and postural changes. In the trimodal case, however, this situation improves, as the network is no longer biased towards one emotion, and the five emotion scores are more evenly distributed. This is also supported by the ROC curves in Figure  7 , where happy and disgust show much lower error rates than scared, and are more balanced in the trimodal case.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Figure 11 Attention Mechanism Result Of Computer And Human",
      "text": "The experimental results show that the recognition rate of each modality fusion exceeds the recognition rate of the corresponding single modality under the premise of using the method of this paper, which illustrates the effectiveness of fusing facial expressions, body movements and text for emotion recognition. Comparing the recognition effects of modal fusion, it can be seen that the combination of features with high-level semantic information extraction and attention mechanism can effectively alleviate the problems of information redundancy and high dimensionality, and improve the accuracy of emotion recognition in single modality by about 1.83%~21.62%.\n\nWe also compared the responses of Resnet and human eye-movement metrics in the face of different emotional expressions. Figures (a) and (b) represent the results of the attention visualization of facial emotions and body movements when passing through the FAF framework, respectively. The first image in figure (a) is the original image, while the second, third, fourth and fifth images show the results of passing through the first, second, third and fourth convolution blocks, respectively. It can be seen that FAF is more accurate in capturing the features of each modality. In the figure, when the character shows a pleasant emotion, the corresponding features are more concentrated on the teeth, dimples, etc. Similarly, to the face, when the character shows pleasure, the physical features tend to be more on the upraised arm and finger posture. Figure  (c ) shows that the human eye visualizes attention through the oculomotor when judging the emotion of a person. As can be seen, the human eye observes the limbs first, followed by the right hand, then the left hand, and finally the face. However, the neural network clearly outperforms the oculomotor in terms of more granular and fine-grained emotion judgements. Operational time plus, respective advantages and disadvantages\n\nConsidering the verification of the accuracy of multimodal emotion fusion experiments and the significance of future applications, several models trained during the experiments were saved and a multimodal emotion online prediction platform was developed. The platform provides four emotion testing portals, namely, unimodal facial expression prediction, unimodal body movement prediction, unimodal text prediction, and arbitrary multimodal fusion prediction(see Figure  12 ).",
      "page_start": 15,
      "page_end": 16
    },
    {
      "section_name": "System Architecture",
      "text": "The platform was designed with a front-end and back-end separated architecture model. The frontend part was developed based on the progressive JavaScript framework Vue.js, and combined with Element UI library, Echarts chart library, Axios network request library, etc. The operation is simple and the interface is intuitive, with good user experience; the back-end part is based on the Python Django web application framework development, using Django Rest Framework interface design style, and deployment of PyTorch deep learning model.\n\nWhen the user enters the operation page and selects an image file or enters text content and clicks submit, the browser declares a FormData object to simulate the form submission data, the image file is converted into a binary data stream and uploaded asynchronously, and the text content is parsed into string type data and sent to the specified server interface in the form of key-value The text content is parsed and sent to the specified server-side interface in the form of key-value pairs using the POST request method of the HTTP protocol.\n\nAfter receiving the data from the user, the server-side interface first preprocesses the image according to the user's selection in the browser, saves the segmented facial or body parts, then loads the corresponding PyTorch model, inputs the processed data into the model one by one in the evaluation mode, and obtains the scores of the five emotion categories output by the model, and returns the information on the key points of the face collected in the data preprocessing process as JSON data. The information is returned to the client as JSON data type, and finally the client calls the relevant library to visualize and report the data returned from the server.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Model Saving And Deployment",
      "text": "The server-side PyTorch 1.11.0 was installed on top of the Django 3.0 web development environment as the environment for model loading and evaluation. The experimenter serializes and saves the model parameters to the server disk after training a specified number of rounds. When it is necessary to input data into the model to get evaluation results, a model object is first initialized, and then the model parameters were read and loaded from the disk. Seven PyTorch models were deployed on the server side, corresponding to unimodal evaluation of facial expressions, body movements, and verbal text, bimodal fusion evaluation of face and body, face and text, and trimodal fusion evaluation of face, body, and text, respectively.\n\nConsidering the problem that the server side repeatedly loads models every time the user side requests an interface, which wastes resources and has low performance, the developers write the code for loading models into the ready() method of Django's AppConfig class, so that no matter how many times the user side requests an interface, all models were only loaded automatically when the server side project starts, and the relevant interfaces directly call the pre-loaded The relevant interface directly calls the pre-loaded model for evaluation, effectively improving serverside performance and evaluation efficiency.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Future Prospects",
      "text": "At present, the multimodal emotion online prediction platform is developed for relevant experimental researchers, invoking deep learning models to evaluate samples in real time and realize data reports. In the future, based on further expansion of the data set and improvement of model prediction accuracy, more emotion-related functions will be combined and put online to provide free emotion prediction for all users. Multimodal emotion recognition outperforms unimodal emotion recognition in most cases, allowing for the fusion of the main features that each unimodal mode. In this study, it was found that the unimodal modality showed better recognition accuracy for the significant positive and negative emotions in the HED dataset, such as happy and disgust. The Face, Body and Text modalities were investigated separately and found to have the highest recognition accuracy for the Face modality, followed by Body and Text modalities in the HED dataset. On the other hand, the ResNet network structure and the BERT network structure both showed the highest Precision, Recall, F1 and Accuracy for the five sentiment categories in unimodal sentiment recognition. Transformer-based strong feature extraction capability. In the multimodal state, the trimodal model outperformed all the bimodal models. Some of the bimodal models did not improve on their unimodal counterparts, probably because the remaining modalities were less relevant in the absence of major emotion expression features, such as facial emotion. In the fusion operations associated with the facial emotion modality, the results were all optimal. Also similar to unimodality, strong positive and negative emotions showed the highest recognition rates in bimodal recognition. However, the performance of each emotion decreases for fusion operations lacking the facial emotion modality. However, in the trimodal fusion operation, the recognition of each emotion was more integrated and no longer biased towards strong positive and negative emotions. Context is also a factor that influences sentiment analysis. In order to explore the problem of emotion recognition in specific scenarios, the context of the emotion is defined as a sporting event, and the five-category problem in the generalized state is replaced by a two-category problem with strong positive and negative emotions. The analysis shows that the recognition rates of facial expressions and body gestures do not differ much and the fused recognition error rate is lower, provided that the background is determined. After removing the background problem and the remaining compound expressions from the five classifications, body movements contributed to the recognition of athletes' emotions, which is consistent with Ahmed F et al.'s study  [36] .",
      "page_start": 17,
      "page_end": 18
    },
    {
      "section_name": "Implications, Limitations And Future Research",
      "text": "Taken together, we developed a real-world multimodal dataset \"HED\" that expanded the existing multimodal dataset and FAF, a deep learning algorithm based on image and text information, was used for emotion recognition. More importantly, our findings highlight significant variances based on important unifacial factors, like posture and the context information. Notably, our findings add to a growing stream of literature that cautions about the prevalence of biases in machine learning datasets and models  [37] . Our findings highlight that the relative performance of emotion recognition systems might vary substantially across gesture and context. Researchers and developers need to be aware of such variances, especially if such systems are being used in a gesture-sensitive context. our study supports previous research in human emotion recognition. Multimodal based method could improve the emotion classification efficiency. Recently, the face expression, gesture and the general background data from the scene are considered as the complementary cues for emotion prediction. However, most of the existing works still have some limitations in deeply exploring the scene-level context when it is not clear enough. So, we try to extract text information from the emotional state prediction method based on visual relationship detection between human and the text from the background. After that, the model incorporates those features with context and body features of the target person to predict their emotional states. There are some limitations in this study that need future work. First, the current study developed a training and test dataset based on face, gesture and text as the three data modalities. A multimodal emotion recognition method was used to compare with others. Not surprisingly, although the latest and more effective cross-fusion model was used, the quality of the dataset is still key to limiting the recognition efficiency. Different people have different perspectives on emotional expression and it may influence the development of emotion dataset. This may change the fusion feature, leading to a significant different classification result. Second, while our dataset provided good performance in emotion classification, as we noted, the HED dataset, though with a relatively big data size, but included relatively fewer representations of disgust, fear and surprise which prevented us from promoting the classification efficiency. The annotators' demographic distribution was not analyzed, which prevented us from studying the sensitivity of our findings. Last, in this study, three important factors: face, posture and text were selected for emotion classification. Other factors like scene background, physiological contexts may also affect the recognition performance. However, these factors may fusion into context feature which sometime use text feature instead. Background provided key evidence when difficult to determine the emotional state, which was not sensitive to the real-time emotion recognition. Further research is needed for deep semantic feature mining and emotion recognition in dynamic environment. Future work in emotion recognition needs high quality dataset, recognition models and more advanced theoretical models to interpretation of multimodal emotion recognition. This not only increase the efficiency of sentiment recognition, but also helps to improve the robustness of the system and provide fine-grained interpretation of the results.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, a large \"HED\" dataset was developed to facilitate the emotion recognition task. To promote recognition accuracy, \"Feature After Feature\" framework was used to explore crucial emotional information from the aligned face-body-text samples. First, feature extraction was performed for each of the three modalities, and the facial and body features were fused, and then the text feature was fused again into facial-body gesture. The high semantic information of the combined features was then extracted after a two-dimensional convolutional layer and an attention mechanism was added to enhance the important features and weaken the unimportant ones. The experimental results show that the proposed method performs well on multimodal emotion recognition tasks, and the accuracy rate can reach 83.75% after trimodal fusion. The accuracy, recall and F1 values achieve better results, which verifies the effectiveness of the method in this paper.",
      "page_start": 20,
      "page_end": 20
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: and Figure 2. The number of each sample is shown in Table 1.",
      "page": 4
    },
    {
      "caption": "Figure 1: Facial emotion dataset                       Figure 2 Physical emotion dataset",
      "page": 4
    },
    {
      "caption": "Figure 3: For the portrait data, it is firstly",
      "page": 4
    },
    {
      "caption": "Figure 3: The general flow chart of the method in this paper",
      "page": 5
    },
    {
      "caption": "Figure 4: indicates the confusion matrix of each network under single modality, and Figure 5 indicates the",
      "page": 9
    },
    {
      "caption": "Figure 4: Confusion matrix for each network in unimodal mode, (a) (b) (c) (d) (e) (f) (g) and (h)",
      "page": 10
    },
    {
      "caption": "Figure 5: ROC curves for each network in unimodal mode, (a) (b) (c) (d) (e) (f) (g) and (h)",
      "page": 10
    },
    {
      "caption": "Figure 5: illustrate that happy and disgust have the highest correct rates",
      "page": 10
    },
    {
      "caption": "Figure 6: shows the confusion matrices of two-two fusion and last three-",
      "page": 11
    },
    {
      "caption": "Figure 7: shows the ROC curves of two-two fusion and",
      "page": 11
    },
    {
      "caption": "Figure 11: show the Grad-CAM[35] attention",
      "page": 11
    },
    {
      "caption": "Figure 6: Multimodal fusion confusion matrix, (i) (j) (k) and (l) correspond to the confusion",
      "page": 13
    },
    {
      "caption": "Figure 7: Multimodal fusion ROC curve, (i) (j) (k) and (l) correspond to the confusion matrix of",
      "page": 13
    },
    {
      "caption": "Figure 7: , where happy and disgust show much lower error rates than scared, and",
      "page": 13
    },
    {
      "caption": "Figure 8: Original sample dispersion                     Figure 9 Sample normalization",
      "page": 14
    },
    {
      "caption": "Figure 10: Sample dispersion after fusion clustering",
      "page": 14
    },
    {
      "caption": "Figure 11: attention mechanism result of computer and human",
      "page": 15
    },
    {
      "caption": "Figure 12: Online multimodal emotion prediction platform",
      "page": 18
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "recognition for human‚Äìcomputer interaction applications[J]. Neural Computing and",
          "Chowdary M K, Nguyen T N, Hemanth D J. Deep learning-based facial emotion": ""
        },
        {
          "Column_1": "Applications, 2021: 1-18.",
          "Chowdary M K, Nguyen T N, Hemanth D J. Deep learning-based facial emotion": ""
        }
      ],
      "page": 21
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "social media applications: A comprehensive review[J]. Wiley Interdisciplinary Reviews:",
          "Chandrasekaran G, Nguyen T N, Hemanth D J. Multimodal sentimental analysis for": ""
        },
        {
          "Column_1": "Data Mining and Knowledge Discovery, 2021, 11(5): e1415.",
          "Chandrasekaran G, Nguyen T N, Hemanth D J. Multimodal sentimental analysis for": ""
        }
      ],
      "page": 21
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "sequential learning[C]//Proceedings of the AAAI conference on artificial intelligence.",
          "Zadeh A, Liang P P, Mazumder N, et al. Memory fusion network for multi-view": ""
        },
        {
          "Column_1": "2018, 32(1).",
          "Zadeh A, Liang P P, Mazumder N, et al. Memory fusion network for multi-view": ""
        }
      ],
      "page": 21
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "recognition[C]//Proceedings of the IEEE conference on computer vision and pattern",
          "He K, Zhang X, Ren S, et al. Deep residual learning for image": ""
        },
        {
          "Column_1": "recognition. 2016: 770-778.",
          "He K, Zhang X, Ren S, et al. Deep residual learning for image": ""
        }
      ],
      "page": 21
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "using deep learning.\" Journal of Applied Science and Technology Trends 2.02 (2021): 52-",
          "Abdullah, Sharmeen M. Saleem Abdullah, et al. \"Multimodal emotion recognition": ""
        },
        {
          "Column_1": "58.",
          "Abdullah, Sharmeen M. Saleem Abdullah, et al. \"Multimodal emotion recognition": ""
        }
      ],
      "page": 22
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "line speaker diarization using sensor fusion through SVM[J]. IEEE Transactions on Multi": "media, 2015, 17(10): 1694-1705."
        }
      ],
      "page": 22
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "additive learning: Improving generalization in multimodal sentiment analysis[C]//2017 IE": "EE International Conference on Multimedia and Expo (ICME). IEEE, 2017: 949-954."
        }
      ],
      "page": 22
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ng model-level fusion of audio‚Äìvisual modalities[J]. Knowledge": "Based Systems, 2022, 244: 108580."
        }
      ],
      "page": 22
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "modal information fusion for data-": "driven emotion recognition[J]. Information Fusion, 2020, 53: 209-221."
        }
      ],
      "page": 22
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "Harvesting opinions from the web[C]//Proceedings of the 13th international conference on",
          "Morency L P, Mihalcea R, Doshi P. Towards multimodal sentiment analysis:": ""
        },
        {
          "Column_1": "multimodal interfaces. 2011: 169-176.",
          "Morency L P, Mihalcea R, Doshi P. Towards multimodal sentiment analysis:": ""
        }
      ],
      "page": 22
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "temporal feature fusion with compact bilinear pooling for multimodal emotion recognitio": "n[J]. Computer Vision and Image Understanding, 2018, 174: 33-42."
        }
      ],
      "page": 22
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "signals using a regularized deep fusion of kernel machine[J]. IEEE transactions on",
          "Zhang X, Liu J, Shen J, et al. Emotion recognition from multimodal physiological": ""
        },
        {
          "Column_1": "cybernetics, 2020, 51(9): 4386-4399.",
          "Zhang X, Liu J, Shen J, et al. Emotion recognition from multimodal physiological": ""
        }
      ],
      "page": 22
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "EEG and ECG signals from wireless low-cost off-the-shelf devices[J]. IEEE journal of",
          "Katsigiannis S, Ramzan N. DREAMER: A database for emotion recognition through": ""
        },
        {
          "Column_1": "biomedical and health informatics, 2017, 22(1): 98-107.",
          "Katsigiannis S, Ramzan N. DREAMER: A database for emotion recognition through": ""
        }
      ],
      "page": 22
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "multimodal fusion[C]//Proceedings of the European Conference on Computer Vision",
          "Vielzeuf V, Lechervy A, Pateux S, et al. Centralnet: a multilayer approach for": ""
        },
        {
          "Column_1": "(ECCV) Workshops. 2018: 0-0.",
          "Vielzeuf V, Lechervy A, Pateux S, et al. Centralnet: a multilayer approach for": ""
        }
      ],
      "page": 23
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "networks via gradient-based localization[C]//Proceedings of the IEEE international",
          "Selvaraju R R, Cogswell M, Das A, et al. Grad-cam: Visual explanations from deep": ""
        },
        {
          "Column_1": "conference on computer vision. 2017: 618-626.",
          "Selvaraju R R, Cogswell M, Das A, et al. Grad-cam: Visual explanations from deep": ""
        }
      ],
      "page": 23
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A brief review of facial emotion recognition based on visual information[J]. sensors",
      "authors": [
        "B Ko"
      ],
      "year": "2018",
      "venue": "A brief review of facial emotion recognition based on visual information[J]. sensors"
    },
    {
      "citation_id": "2",
      "title": "Extended deep neural network for facial emotion recognition",
      "authors": [
        "D K Jain",
        "P Shamsolmoali",
        "P Sehdev"
      ],
      "year": "2019",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "3",
      "title": "Deep learning-based facial emotion recognition for human-computer interaction applications",
      "authors": [
        "M Chowdary",
        "T Nguyen",
        "D Hemanth"
      ],
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "4",
      "title": "Emotion recognition from body movement[J]",
      "authors": [
        "F Ahmed",
        "A S M H Bari",
        "M Gavrilova"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "5",
      "title": "Generalized zero-shot emotion recognition from body gestures",
      "authors": [
        "J Wu",
        "Y Zhang",
        "S Sun"
      ],
      "venue": "Applied Intelligence"
    },
    {
      "citation_id": "6",
      "title": "Speech emotion recognition using deep learning techniques: A review[J]",
      "authors": [
        "R Khalil",
        "E Jones",
        "M Babar"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "7",
      "title": "Deep learning for affective computing: Text-based emotion recognition in decision support[J]",
      "authors": [
        "B Kratzwald",
        "S Iliƒá",
        "M Kraus"
      ],
      "year": "2018",
      "venue": "Decision Support Systems"
    },
    {
      "citation_id": "8",
      "title": "Multimodal sentimental analysis for social media applications: A comprehensive review",
      "authors": [
        "G Chandrasekaran",
        "T Nguyen",
        "D Hemanth"
      ],
      "year": "2021",
      "venue": "Data Mining and Knowledge Discovery"
    },
    {
      "citation_id": "9",
      "title": "Tensor fusion network for multimodal sentiment analysis",
      "authors": [
        "A Zadeh",
        "M Chen",
        "S Poria"
      ],
      "year": "2017",
      "venue": "Tensor fusion network for multimodal sentiment analysis",
      "arxiv": "arXiv:1707.07250"
    },
    {
      "citation_id": "10",
      "title": "Deep multimodal multilinear fusion with high-order polynomial pooling[J]",
      "authors": [
        "M Hou",
        "J Tang",
        "J Zhang"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "11",
      "title": "Memory fusion network for multi-view sequential learning",
      "authors": [
        "A Zadeh",
        "P P Liang",
        "N Mazumder"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "12",
      "title": "Survey on emotional body gesture recognition[J]",
      "authors": [
        "F Noroozi",
        "C Corneanu",
        "D Kami≈Ñska"
      ],
      "year": "2018",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "13",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren"
      ],
      "year": "2016",
      "venue": "Deep residual learning for image recognition"
    },
    {
      "citation_id": "14",
      "title": "Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M Chang",
        "K Lee"
      ],
      "year": "2018",
      "venue": "Pre-training of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "15",
      "title": "Deep multimodal learning: A survey on recent adva nces and trends[J]",
      "authors": [
        "D Ramachandram",
        "G Taylor"
      ],
      "year": "2017",
      "venue": "IEEE signal processing magazine"
    },
    {
      "citation_id": "16",
      "title": "Multimodal emotion recognition using deep learning",
      "authors": [
        "Sharmeen Abdullah",
        "Saleem Abdullah"
      ],
      "year": "2021",
      "venue": "Journal of Applied Science and Technology Trends"
    },
    {
      "citation_id": "17",
      "title": "Multimodal multi-channel online speaker diarization using sensor fusion through SVM[J]",
      "authors": [
        "V Minotto",
        "C Jung",
        "B Lee"
      ],
      "year": "2015",
      "venue": "IEEE Transactions Multi media"
    },
    {
      "citation_id": "18",
      "title": "Selectadditive learning: Improving generalization in multimodal sentiment analysis",
      "authors": [
        "H Wang",
        "A Meghawat",
        "L Morency"
      ],
      "year": "2017",
      "venue": "IE EE International Conference on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "19",
      "title": "Deep multi-level fusion network for multisource image pixel-wise classification[J]. Knowledge-Based Systems",
      "authors": [
        "X Liu",
        "L Jiao",
        "L Li"
      ],
      "year": "2021",
      "venue": "Deep multi-level fusion network for multisource image pixel-wise classification[J]. Knowledge-Based Systems"
    },
    {
      "citation_id": "20",
      "title": "Deep learning based multimodal emotion recognition usi ng model-level fusion of audio-visual modalities[J]. Knowledge Based Systems",
      "authors": [
        "A Middya",
        "B Nag",
        "S Roy"
      ],
      "year": "2022",
      "venue": "Deep learning based multimodal emotion recognition usi ng model-level fusion of audio-visual modalities[J]. Knowledge Based Systems"
    },
    {
      "citation_id": "21",
      "title": "A snapshot research and implementation of multi modal information fusion for datadriven emotion recognition[J]. Information Fusion",
      "authors": [
        "Y Jiang",
        "W Li",
        "M Hossain"
      ],
      "year": "2020",
      "venue": "A snapshot research and implementation of multi modal information fusion for datadriven emotion recognition[J]. Information Fusion"
    },
    {
      "citation_id": "22",
      "title": "Towards multimodal sentiment analysis: Harvesting opinions from the web",
      "authors": [
        "L Morency",
        "R Mihalcea",
        "P Doshi"
      ],
      "year": "2011",
      "venue": "Proceedings of the 13th international conference on multimodal interfaces"
    },
    {
      "citation_id": "23",
      "title": "Deep spatiotemporal feature fusion with compact bilinear pooling for multimodal emotion recognitio n[J]",
      "authors": [
        "D Nguyen",
        "K Nguyen",
        "S Sridharan"
      ],
      "year": "2018",
      "venue": "Computer Vision and Image Understanding"
    },
    {
      "citation_id": "24",
      "title": "Emotion recognition from multimodal physiological signals using a regularized deep fusion of kernel machine[J]",
      "authors": [
        "X Zhang",
        "J Liu",
        "J Shen"
      ],
      "year": "2020",
      "venue": "IEEE transactions on cybernetics"
    },
    {
      "citation_id": "25",
      "title": "DREAMER: A database for emotion recognition through EEG and ECG signals from wireless low-cost off-the-shelf devices[J]",
      "authors": [
        "S Katsigiannis",
        "N Ramzan"
      ],
      "year": "2017",
      "venue": "IEEE journal of biomedical and health informatics"
    },
    {
      "citation_id": "26",
      "title": "Are we face experts?[J]",
      "authors": [
        "A Young",
        "A Burton"
      ],
      "year": "2018",
      "venue": "Trends in cognitive sciences"
    },
    {
      "citation_id": "27",
      "title": "Squeeze-and-excitation networks",
      "authors": [
        "J Hu",
        "L Shen",
        "G Sun"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "28",
      "title": "A method for stochastic optimization",
      "authors": [
        "D P Kingma",
        "J Ba",
        "Adam"
      ],
      "year": "2014",
      "venue": "A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "29",
      "title": "GoEmotions: A dataset of fine-grained emotions",
      "authors": [
        "D Demszky",
        "D Movshovitz-Attias",
        "J Ko"
      ],
      "year": "2020",
      "venue": "GoEmotions: A dataset of fine-grained emotions",
      "arxiv": "arXiv:2005.00547"
    },
    {
      "citation_id": "30",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for large-scale image recognition",
      "arxiv": "arXiv:1409.1556"
    },
    {
      "citation_id": "31",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale[J]",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov"
      ],
      "year": "2020",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale[J]",
      "arxiv": "arXiv:2010.11929"
    },
    {
      "citation_id": "32",
      "title": "Long short-term memory",
      "authors": [
        "S Hochreiter",
        "J Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural computation"
    },
    {
      "citation_id": "33",
      "title": "Centralnet: a multilayer approach for multimodal fusion",
      "authors": [
        "V Vielzeuf",
        "A Lechervy",
        "S Pateux"
      ],
      "year": "2018",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV) Workshops"
    },
    {
      "citation_id": "34",
      "title": "Efficient low-rank multimodal fusion with modality-specific factors",
      "authors": [
        "Z Liu",
        "Y Shen",
        "V Lakshminarasimhan"
      ],
      "year": "2018",
      "venue": "Efficient low-rank multimodal fusion with modality-specific factors",
      "arxiv": "arXiv:1806.00064"
    },
    {
      "citation_id": "35",
      "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
      "authors": [
        "Selvaraju R R",
        "M Cogswell",
        "A Das"
      ],
      "year": "2017",
      "venue": "Grad-cam: Visual explanations from deep networks via gradient-based localization"
    },
    {
      "citation_id": "36",
      "title": "Emotion recognition from body movement[J]",
      "authors": [
        "F Ahmed",
        "A S M H Bari",
        "M Gavrilova"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "37",
      "title": "A survey on bias and fairness in machine l earning[J]",
      "authors": [
        "N Mehrabi",
        "F Morstatter",
        "N Saxena"
      ],
      "year": "2021",
      "venue": "ACM Computing Surveys (CSUR)"
    }
  ]
}