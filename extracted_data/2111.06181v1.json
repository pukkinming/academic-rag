{
  "paper_id": "2111.06181v1",
  "title": "Multilingual And Multilabel Emotion Recognition Using Virtual Adversarial Training",
  "published": "2021-11-11T12:47:44Z",
  "authors": [
    "Vikram Gupta"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Virtual Adversarial Training (VAT) has been effective in learning robust models under supervised and semi-supervised settings for both computer vision and NLP tasks. However, the efficacy of VAT for multilingual and multilabel text classification has not been explored before. In this work, we explore VAT for multilabel emotion recognition with a focus on leveraging unlabelled data from different languages to improve the model performance. We perform extensive semi-supervised experiments on Se-mEval2018 multilabel and multilingual emotion recognition dataset and show performance gains of 6.2% (Arabic), 3.8% (Spanish) and 1.8% (English) over supervised learning with same amount of labelled data (10% of training data). We also improve the existing state-ofthe-art by 7%, 4.5% and 1% (Jaccard Index) for Spanish, Arabic and English respectively and perform probing experiments for understanding the impact of different layers of the contextual models.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition is an active and crucial area of research, especially for social media platforms. Understanding the emotional state of the users from textual data forms an important problem as it helps in discovering signs of fear, anxiety, bullying, hatred etc. and maintaining the emotional health of the people and platform. With the advent of deep neural networks and contextual models, text understanding has advanced dramatically by leveraging huge amount of unlabelled data freely available on web. However, even with these advancements, annotating emotion categories is expensive and time consuming as emotion categories are highly correlated and subjective in nature and can co-occur in the same text. Psychological studies suggest that emotions like \"anger\" and \"sadness\" are corelated and co-occur more frequently than \"anger\" and \"happiness\"  (Plutchik, 1980) . In a multilingual setup, the annotation becomes even more challenging as annotator team are expected to be familiar with different languages and culture for understanding the emotions accurately. Imbalance in availability of the data across languages further creates problems, especially in case of resource impoverished languages. In this work, we investigate the following key points; a) Can unlabelled data from other languages improve recognition performance of target language and help in reducing requirement of labelled data? b) Efficacy of VAT for multilingual and multilabel setup.\n\nTo address the aforementioned questions, we focus our experiments towards semi-supervised learning in a multilingual and multilabel emotion classification framework. We formulate semi-supervised Virtual Adversarial Training (VAT)  (Miyato et al., 2018)  for multilabel emotion classification using contextual models and perform extensive experiments to demonstrate that unlabelled data from other languages L ul = {L 1 , L 2 , . . . , L n } improves the classification on the target language L tgt . We obtain competitive performance by reducing the amount of annotated data demonstrating crosslanguage learning. To effectively leverage the multilingual content, we use multilingual contextual models for representing the text across languages. We also evaluate monolingual contextual models to understand the performance differences between multilingual and monolingual models and explore the advantages of domain-adaptive and task-adaptive pretraining of models for our task and observe substantial gains.\n\nWe perform extensive experiments on the SemEval2018 (Affect in Tweets: Task E-c 1  ) dataset  (Mohammad et al., 2018)  which contains tweets from Twitter annotated with 11 emotion categories across three languages -English, Spanish and Arabic and demonstrate the effectiveness of semi-supervised learning across languages. To the best of our knowledge, our study is the first one to explore semi-supervised adversarial learning across different languages for multilabel classification. In summary, the main contributions of our work are the following:\n\n• We explore Virtual Adversarial Training (VAT) for semi-supervised multilabel classification on multilingual corpus.\n\n• Experiments demonstrating 6.2%, 3.8% and 1.8% improvements (Jaccard Index) on Arabic, Spanish and English by leveraging unlabelled data of other languages while using 10% of labelled samples.\n\n• Improve state-of-the-art of multilabel emotion recognition by 7%, 4.5% and 1% (Jaccard Index) for Spanish, Arabic and English respectively.\n\n• Experiments showcasing the advantages of domain-adaptive and task-adaptive pretraining.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Semi-supervised learning is an important paradigm for tackling the scarcity of labelled data as it marries the advantages of supervised and unsupervised learning by leveraging the information hidden in large amount of unlabelled data along with small amount of labelled data  (Yang et al., 2021) ,  (Van Engelen and Hoos, 2020) .\n\nEarly approaches used self-training for leveraging the model's own predictions on unlabelled data to obtain additional information during training  (Yarowsky, 1995 ) (McClosky et al., 2006) .  Clark et al. (2018)  proposed cross-view training (CVT) for various tasks like chunking, dependency parsing, machine translation and reported state-of-theart results. CVT forces the model to make consistent predictions when using the full input or partial input. Ladder networks  (Laine and Aila, 2016) , Mean Teacher networks  (Tarvainen and Valpola, 2017)  are another way for semi-supervised learning where temporal and model-weights are ensembled. Another popular direction towards semisupervised learning is adversarial training where the data point is perturbed with random or carefully tuned perturbations to create an adversarial sample. The model is then encouraged to maintain consistent predictions for the original sample and the adversarial sample. Adversarial training was initially explored for developing secure and robust models  (Goodfellow et al., 2014) ,  (Xiao et al., 2018) ,  (Saadatpanah et al., 2020)   Emotion recognition is an important problem and has received lot of attention from the community  (Yadollahi et al., 2017) ,  (Sailunaz et al., 2018) . The taxonomies of emotions suggested by Plutchik wheel of emotions  (Plutchik, 1980)  and  (Ekman, 1984)  have been used by the majority of the previous work in emotion recognition. Emotion recognition can be formulated as a multiclass problem  (Scherer and Wallbott, 1994) ,  (Mohammad, 2012)  or a multilabel problem  (Mohammad et al., 2018) ,  (Demszky et al., 2020) . In the multiclass formulation, the objective is to identify the presence of one of the emotion from the taxonomy whereas in a multilabel setting, more than one emotion can be present in the text instance. Binary relevance approach  (Godbole and Sarawagi, 2004)  is another way to break multilabel problem into multiple binary classification problems. However, this approach does not model the co-relation between emotions. Seq2Seq approaches  (Yang et al., 2018) ,  (Huang et al., 2021)  solve this problem by modelling the relationship between emotions by inferring emotion in an incremental manner. An interesting direction for handling data scarcity in emotion recognition is to use distant supervision by exploiting emojis  (Felbo et al., 2017) , hashtags  (Mohammad, 2012)  or pretraining emotion specific embeddings and language models  (Barbieri et al., 2021) ,  (Ghosh et al., 2017) .\n\nWith the emergence of contextual models like BERT  (Devlin et al., 2018) , Roberta  (Liu et al., 2019)  etc., the field of NLP and text classification has been revolutionized as these models are able to learn efficient representations from a huge corpus of unlabelled data across different languages and domains  (Hassan et al., 2021) ,  (Barbieri et al., 2021) . Social media content contains linguistic errors, idiosyncratic styles, spelling mistakes, grammatical inconsistency, slangs, hashtags, emoticons etc.  (Barbieri et al., 2018) ,  (Derczynski et al., 2013)  due to which off-the-shelf contextual models may not be optimum. We use languageadaptive, domain-adaptive and task-adaptive pretraining which has shown performance gains  (Peters et al., 2019) ,  (Gururangan et al., 2020) ,  (Barbieri et al., 2021) ,  (Howard and Ruder, 2018) ,  (Lee et al., 2020)  for different tasks and domains.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methodology",
      "text": "We consider the task of multilabel emotion classification, where given a text t ∈ T and t = {w 1 , w 2 , . . . , w l }, we predict the presence of y emotion categories denoted by {1, 2, . . . , y}. T represents the corpus of all the sentences across the different languages and w i represent the tokens in the sentence. We leverage contextual models as feature extractors φ : t i → x i , where x i ∈ R d and d is the dimension of the text representations and train a classifier over these representations.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Virtual Adversarial Training (Vat)",
      "text": "Virtual Adversarial Training (VAT)  (Miyato et al., 2018)  is a regularization method for learning robust representations by encouraging the models to produce similar outputs for the input data points and local perturbations. VAT creates the adversary by perturbing the input in the direction which maximizes the change in the output of the model. Since VAT does not require labels it is well suited for semi-supervised applications. Consider x ∈ R d as the d dimensional representation of the text and y as the ground truth. Objective function of VAT (L vadv ) is represented as,\n\n(1) where,\n\nand ||r|| 2 < and r vadv ∈ R d . D[p, p'] measures the divergence between the two probability distributions and r vadv is the virtual adversarial perturbation that maximizes this divergence. In order to leverage the unlabelled data, the predictions from the current estimate of the model θ are used as the target. However, it is not possible to exactly compute r vadv by a closed form solution or linear approximation as gradient g (Equation  4 ) with respect to r is always zero at r = 0.  Miyato et al. (2018)  propose fast approximation method to formulate r adv as:\n\nwhere,\n\nand r = * q, where q is a randomly sampled unit vector. With this approximation, we can use backpropagation to compute the gradients g in Equation  4 . The overall training objective, L V AT becomes:\n\nwhere L ce is the multiclass classification loss and L adv is the adversarial loss. α is the balancing hyperparameter between the two losses.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Multilabel Virtual Adversarial Training (Mlvat)",
      "text": "We explore VAT for multilingual contextual models and multilabel classification. For computer vision tasks, perturbing the raw pixel values to generate adversarial examples is intuitive as the input space is continuous. However, contextual models use the indices of the words as input which are not present in the continuous domain and thus perturbing them is not optimal. Perturbing an index k of a word w k to k + r vadv would not result in a word closer to w k . To overcome this problem, instead of perturbing the input, we perturb the intermediate layer of the contextual models which form a continuous representation space and allows us to use VAT with contextual models. Similar strategy for text classification was also explored by  Miyato et al. (2016) .\n\nFor modelling multilabel classification, we measure the divergence of multilabel outputs by Mean Square Error (MSE),\n\n) MSE is calculated over the logits normalized by sigmoid. This is important as the outputs in case of multilabel classification are not probability distributions across classes which renders the usage of KL-Divergence incompatible for this scenario. We also experiment by treating the probability for each emotion separately but our results demonstrate the effectiveness of Mean Square Error (MSE) for our task (Table  4 ). The overall training objective, L mlVAT is:\n\nwhere, L bce is the multilabel binary cross entropy loss. We represent the text instances using monolingual/multilingual contextual representations.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Multilingual Semi-Supervised Setup",
      "text": "mlVAT: For each target language L tgt , we randomly select a percentage of samples from the training set of this language and use them as labelled examples for training. We use the remaining data of the same language and the complete dataset of the other languages L ul as the unlabelled set.\n\nEach training batch is created by maintaining a ratio between labelled and unlabelled examples for stable training. For the labelled set, both multilabel classification loss L bce and adversarial loss L vadv is applied. For the unlabelled examples, only the adversarial loss L vadv is used. Sup: We also train supervised classifiers (Sup) by using the same amount of labelled data for target language L tgt . Supervised classifiers (Sup) act as baseline and help in measuring the gains obtained by semi-supervised learning. We vary the ratio of sampled labelled examples as 10%, 25%, 50% and 100% to study the progression of our framework across different amount of labelled data of the target language.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Multilingual Representation",
      "text": "For leveraging cross-learning between multiple languages in a semi-supervised setup, we experiment with different multilingual models. We experiment with off-the-shelf multilingual BERT, mBERT  (Devlin et al., 2018)  and XLM-R  (Conneau et al., 2019)  models which have been trained with corpus from multiple languages. Since we are performing emotion recognition on multilingual tweets, we evaluate the domain-adaptive multilingual model XLM-Tw  (Barbieri et al., 2021)  trained using a 198M tweet corpus across 30 languages over the XLM-R checkpoint. For exploring the effect of task-adaptive pretraining, we evaluate XLM-Tw-S, which is finetuned for sentiment analysis over tweets which is arguably a task related to emotion recognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Monolingual Representation",
      "text": "We also experiment with monolingual models trained over the corpus from the same language for comparison with multilingual models and setting up the baselines for each language: English BERT (E-BERT)  (Devlin et al., 2018)  for English, BetoBERT  (Cañete et al., 2020)",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Dataset And Evaluation",
      "text": "We evaluate on the SemEval2018 dataset (Affect in Tweets: Task E-c)  (Mohammad et al., 2018)  dataset. The dataset consists of tweets scraped from twitter in English, Spanish and Arabic. Each tweet is annotated with the presence of 11 emotions anger, anticipation, disgust, fear, joy, love, optimism, pessimism, sadness, surprise and trust. Some tweets are neutral and do not have the presence of any emotion. The dataset has 3 splits -train, dev and test (Table  15 ). Following  Mohammad et al. (2018) , we measure the multilabel accuracy using Jaccard Index (JI), Macro F1 (MaF1) and Micro F1 (MiF1) scores  (Chinchor, 1992)  over the test set of these languages.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Semi-Supervised Experiments",
      "text": "We select a percentage (10%, 25%, 50%, 100%) of the data from the target language as labelled data and use the remaining data from same language along with data of other languages as the unlabelled data. In Table  1  for Arabic, we see that by using 10%, 25%, 50% and 100% of the labelled data, mlVAT improves upon the results of training over the same amount of supervised data by 6.2%, 2.8%, 2.2% and 2.7% (Jaccard Index;JI) respectively. Similar improvements are also observed on the micro F1 (MiF1) and macro F1 (MaF1). It is interesting to note that by using only 50% of the labelled data with unlabelled data, we are able to match the performance of supervised learning with 100% of the data for Spanish. This shows that mlVAT is able to leverage the unlabelled data of Spanish and English for improving the performance over Arabic language.\n\nSimilar observations on English can be made from Table  2  also where we notice an improvement of 1.8%, 2.6%, 2.6% and 2% on the Jaccard Index and proportional improvements on other metrices also. For English also, we note that by using 10% of labelled data, mlVAT is able to improve on supervised results with 25% of the data. For Spanish, mlVAT helps for the 10% and 50% split as reported in Table  3  but is not able to improve all the metrics for the other splits. Overall, for majority of the languages and splits, we see that by adding unlabelled data, mlVAT improves upon the performance over supervised learning consistently and helps in decreasing the requirements for annotated data. Frozen backbone: We perform semi-supervised experiments with frozen backbone to investigate the effect of mlVAT on the backbone and classification head. We repeat similar experiments as in previous sections for Spanish and English, but freeze the backbone and only train the classification head. From the Figure  1 , we can observe that mlVAT consistently improves the performance for both languages over all the splits. This demonstrates that the performance gains are backbone-agnostic allowing for application of mlVAT on other back- Unlabelled Batch Ratio: In Table  5 , we study the impact of ratio of the batch size of the unlabelled examples while keeping the batch size of the labelled data fixed. At higher ratios, the adversarial loss overpowers the supervised learning resulting in a performance drop. However, for the lower ratios, the we did not observe a consistent trend. Epsilon: We study the impact of epsilon ( ) on the performance in Table  6  bation while lower values may create insufficient perturbation. From our empirical experiments, we note that 0.5 works better than the other values and we use this for all our semi-supervised experiments.",
      "page_start": 1,
      "page_end": 6
    },
    {
      "section_name": "Domain And Task Adaptive Pretraining",
      "text": "In this section, we perform supervised learning experiments with frozen and finetuned representations by using the labelled data of each language for evaluating the performance of domainadaptive, task-adaptive, monolingual and multilingual contextual models. In Table  8 , 9 and 7, we present the results for different monolingual and multilingual contextual models for the three languages with frozen backbones. We use English BERT (E-BERT), BetoBERT and AraBERT as monolingual models for English, Spanish and Arabic respectively. We note that for all the languages, mBERT performs substantially poorer than the monolingual contextual models of the respective languages. However, XLM-R which is another multilingual model performs competitive with the monolingual models which is not surprising as XLM-R has shown improvements over mBERT in other language tasks also  (Conneau et al., 2019) .\n\nWe further evaluate Domain-adaptive (XLM-Tw) and Task-adaptive (XLM-Tw-S) versions of the XLM-R multilingual model and observe substantial improvements. XLM-Tw-S improves the Jaccard Index (JI) by 5.5%, 6.5% and 8.4% for Arabic, English and Spanish respectively, highlighting the advantages of task-specific pretraining for contextual models. XLM-Tw also improves upon XLM-R for all the languages reiterating the importance of pretraining the contextual models with domain specific data.\n\nWe study the impact of finetuning the monolingual and best performing multilingual model on our task to compare the capabilities of multilingual models with monolingual after finetuning on the task. We notice that finetuning bridges the gap to some extent but still the domain adaptive multilingual XLM-Tw works better than the finetuned monolingual models for all the languages as shown in Table  10 , 11 and 12. For English, the improvement is relatively moderate but for Spanish and Arabic, XLM-Tw demonstrates substantial gains.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Comparison With Existing Methods",
      "text": "English: Alhuzali and Ananiadou (2021) (SpanEmo) use sentences along with emotion categories as input to the contextual model and use label correlation aware loss (LCA) to model correlation among emotions classes. LVC-Seq2Emo  (Huang et al., 2019)  propose a latent variable chain transformation and use it with sequence to emotion for modelling correlation between emotions. BinC  (Jabreel and Moreno, 2019)  transform the multilabel classification problem into binary classification problems and train a recurrent neural network over this transformed setting.  (Baziotis et al., 2018)   Overall, our results improve upon the existing approaches on Jaccard Index(JI) by 7% for Spanish, 4.5% for Arabic and around 1% for English and setup a new state-of-the-art for all the three languages highlighting the efficacy of semi-supervised learning and domain-adaptive multilingual models.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Crosslingual Experiments",
      "text": "We combine data of all the three languages and train a combined model and test this model on the test set of each language. We notice that the combined model improves upon the performance of individual models for Arabic and Spanish (Table  13 ) while the performance of English is comparable.\n\nIn Table  14 , we perform crosslingual experiments to evaluate the performance of a model trained on one language on another language. It is interesting to note that for Arabic and Spanish, the cross lingual performance is competitive with performance using some of the pretrained networks which is encouraging. We also observe that English demonstrates better crosslingual capability than Arabic and Spanish. A possible reason might be the large size of the English training dataset.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Probing Experiments",
      "text": "We perform experiments to evaluate the contribution of different layers of the XLM-Tw-S model. We extract representation of the tokens of a sentence from a particular layer of the contextual model and take an average across tokens for obtaining the representation of the sentence. We train a classifier over these sentence representations and report the results. From Figure  2 , we note that higher layers provide better performance for all the three languages showing that the higher-order contextual information is useful for understanding the emotions in the text. Refer Appendix A for detailed results. Similar to  Tenney et al. (2019) , we also compute the improvement due to incrementally adding more layers to the previous layers and calculate the expected layer:\n\nwhere, ∆ (l) is the change in the Jaccard Index metric when adding layer l to the previous layers. We start from layer 0 and incrementally add higher layers for representing the tokens of the sentence Table  13 : Experiments on the combination of languages followed by averaging for representing the whole sentence. The expected layer for English, Spanish and Arabic computes to 6.9, 6.2 and 6.8 respectively showing that higher layers are useful for the task. This analysis is helpful to understand the improvement achieved by adding layers to the previous layers. For all the three languages, we obtain the best results on using the average of all the layers for representing the sentences which shows that different layers encapsulate complementary information about emotions.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Training Details",
      "text": "We finetune the contextual models following huggingface 2  with a batch size of 8, learning rate of 2e-5 and weight decay of 0.01 using AdamW optimizer for 30 epochs. The classifier is a two layered neural network with 768 hidden dimensions and 11 output dimensions with 0.1 dropout. For mlVAT experiments, the number of examples sampled from the unlabelled set for each batch are 24, and α are set to 0.5 and 1 using cross validation. We apply sigmoid over the logits and train using binary cross entropy loss. We use validation set for finding optimal hyperparameters and evaluate on the test set using combination of training and validation set for training.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we explored semi-supervised learning using Virtual Adversarial Training (VAT) for multilabel emotion classification in a multilingual setup and showed performance improvement by leveraging unlabelled data from different languages. We used Mean Square Error (MSE) as the divergence measure for leveraging VAT for multilabel emotion classification. We also evaluated the performance of monolingual, multilingual and domain-adaptive and task-adaptive multilingual contextual models across three languages -English, Spanish and Arabic for multilabel and multilingual emotion recognition and obtained state-of-the-art results. We also performed probing experiments for understanding the impact of different layers of contextual models.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , we can observe that mlVAT",
      "page": 5
    },
    {
      "caption": "Figure 1: Comparison of Jaccard Index for English and Span-",
      "page": 5
    },
    {
      "caption": "Figure 2: , we note that",
      "page": 7
    },
    {
      "caption": "Figure 2: Performance metrices across different layers for",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "%": "10",
          "Method": "Sup",
          "JI": "44.05",
          "MiF1": "57.86",
          "MaF1": "40.91"
        },
        {
          "%": "",
          "Method": "mlVAT",
          "JI": "46.79",
          "MiF1": "60.36",
          "MaF1": "44.41"
        },
        {
          "%": "25",
          "Method": "Sup",
          "JI": "49.69",
          "MiF1": "62.80",
          "MaF1": "44.19"
        },
        {
          "%": "",
          "Method": "mlVAT",
          "JI": "51.08",
          "MiF1": "63.96",
          "MaF1": "47.31"
        },
        {
          "%": "50",
          "Method": "Sup",
          "JI": "53.95",
          "MiF1": "66.26",
          "MaF1": "48.57"
        },
        {
          "%": "",
          "Method": "mlVAT",
          "JI": "55.11",
          "MiF1": "66.79",
          "MaF1": "52.52"
        },
        {
          "%": "100",
          "Method": "Sup",
          "JI": "55.78",
          "MiF1": "67.41",
          "MaF1": "50.12"
        },
        {
          "%": "",
          "Method": "mlVAT",
          "JI": "57.31",
          "MiF1": "68.18",
          "MaF1": "52.15"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "%": "10",
          "Method": "Sup",
          "JI": "54.15",
          "MiF1": "66.33",
          "MaF1": "48.94"
        },
        {
          "%": "",
          "Method": "mlVAT",
          "JI": "55.15",
          "MiF1": "67.01",
          "MaF1": "50.57"
        },
        {
          "%": "25",
          "Method": "Sup",
          "JI": "55.11",
          "MiF1": "66.99",
          "MaF1": "47.83"
        },
        {
          "%": "",
          "Method": "mlVAT",
          "JI": "56.54",
          "MiF1": "68.52",
          "MaF1": "51.18"
        },
        {
          "%": "50",
          "Method": "Sup",
          "JI": "57.20",
          "MiF1": "69.14",
          "MaF1": "54.14"
        },
        {
          "%": "",
          "Method": "mlVAT",
          "JI": "58.67",
          "MiF1": "70.03",
          "MaF1": "51.55"
        },
        {
          "%": "100",
          "Method": "Sup",
          "JI": "59.78",
          "MiF1": "71.19",
          "MaF1": "53.43"
        },
        {
          "%": "",
          "Method": "mlVAT",
          "JI": "60.70",
          "MiF1": "71.90",
          "MaF1": "56.10"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "%": "10",
          "Method": "Sup",
          "JI": "44.36",
          "MiF1": "53.17",
          "MaF1": "38.28"
        },
        {
          "%": "",
          "Method": "mlVAT",
          "JI": "46.05",
          "MiF1": "54.83",
          "MaF1": "42.49"
        },
        {
          "%": "25",
          "Method": "Sup",
          "JI": "52.89",
          "MiF1": "61.30",
          "MaF1": "48.99"
        },
        {
          "%": "",
          "Method": "mlVAT",
          "JI": "52.05",
          "MiF1": "60.17",
          "MaF1": "49.15"
        },
        {
          "%": "50",
          "Method": "Sup",
          "JI": "55.17",
          "MiF1": "63.20",
          "MaF1": "51.70"
        },
        {
          "%": "",
          "Method": "mlVAT",
          "JI": "55.70",
          "MiF1": "63.39",
          "MaF1": "54.19"
        },
        {
          "%": "100",
          "Method": "Sup",
          "JI": "57.04",
          "MiF1": "65.31",
          "MaF1": "51.53"
        },
        {
          "%": "",
          "Method": "mlVAT",
          "JI": "56.89",
          "MiF1": "64.89",
          "MaF1": "51.77"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 3: but is not able to improve",
      "data": [
        {
          "Loss": "mlVAT",
          "JI": "55.2",
          "MiF1": "67.1",
          "MaF1": "50.6"
        },
        {
          "Loss": "mlVAT(w/o\nsig)",
          "JI": "50.7",
          "MiF1": "63.5",
          "MaF1": "41.6"
        },
        {
          "Loss": "KLDivergence",
          "JI": "21.9",
          "MiF1": "35.9",
          "MaF1": "34.1"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 3: but is not able to improve",
      "data": [
        {
          "Ratio": "JI",
          "1": "55.1",
          "2": "54.4",
          "3": "55.2",
          "4": "53.6",
          "5": "52.9"
        },
        {
          "Ratio": "MiF1",
          "1": "66.9",
          "2": "66.4",
          "3": "67.0",
          "4": "65.8",
          "5": "65.3"
        },
        {
          "Ratio": "MaF1",
          "1": "50.0",
          "2": "50.9",
          "3": "50.8",
          "4": "50.5",
          "5": "47.0"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 8: , 9 and 7, we ment is relatively moderate but for Spanish and",
      "data": [
        {
          "(cid:15)": "JI",
          "0.1": "54.9",
          "0.25": "54.9",
          "0.5": "55.2",
          "0.75": "54.7",
          "1": "54.6"
        },
        {
          "(cid:15)": "MiF1",
          "0.1": "66.7",
          "0.25": "66.8",
          "0.5": "67.0",
          "0.75": "66.6",
          "1": "66.8"
        },
        {
          "(cid:15)": "MaF1",
          "0.1": "50.4",
          "0.25": "50.3",
          "0.5": "50.8",
          "0.75": "50.3",
          "1": "49.9"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 8: , 9 and 7, we ment is relatively moderate but for Spanish and",
      "data": [
        {
          "Model": "XLM-Tw-S",
          "JI": "52.0",
          "MiF1": "64.4",
          "MaF1": "47.9"
        },
        {
          "Model": "XLM-Tw",
          "JI": "49.3",
          "MiF1": "62.2",
          "MaF1": "47.1"
        },
        {
          "Model": "XLM-R",
          "JI": "45.2",
          "MiF1": "58.4",
          "MaF1": "42.9"
        },
        {
          "Model": "mBERT",
          "JI": "37.5",
          "MiF1": "51.2",
          "MaF1": "36.2"
        },
        {
          "Model": "AraBERT",
          "JI": "46.4",
          "MiF1": "59.7",
          "MaF1": "43.7"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 8: , 9 and 7, we ment is relatively moderate but for Spanish and",
      "data": [
        {
          "Model": "XLM-Tw-S",
          "JI": "53.9",
          "MiF1": "66.2",
          "MaF1": "47.8"
        },
        {
          "Model": "XLM-Tw",
          "JI": "50.6",
          "MiF1": "63.5",
          "MaF1": "45.9"
        },
        {
          "Model": "XLM-R",
          "JI": "48.6",
          "MiF1": "61.9",
          "MaF1": "45.7"
        },
        {
          "Model": "mBERT",
          "JI": "44.7",
          "MiF1": "57.6",
          "MaF1": "39.2"
        },
        {
          "Model": "E-BERT",
          "JI": "48.2",
          "MiF1": "61.4",
          "MaF1": "42.9"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 14: , we perform crosslingual experi- tallyaddingmorelayerstothepreviouslayersand",
      "data": [
        {
          "Model": "mlVAT",
          "JI": "60.7",
          "MiF1": "71.9",
          "MaF1": "56.1"
        },
        {
          "Model": "XLM-Tw",
          "JI": "59.8",
          "MiF1": "71.2",
          "MaF1": "53.4"
        },
        {
          "Model": "E-BERT",
          "JI": "59.1",
          "MiF1": "70.4",
          "MaF1": "53.3"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 14: , we perform crosslingual experi- tallyaddingmorelayerstothepreviouslayersand",
      "data": [
        {
          "Model": "XLM-Tw-S",
          "JI": "51.1",
          "MiF1": "60.0",
          "MaF1": "48.8"
        },
        {
          "Model": "XLM-Tw",
          "JI": "47.1",
          "MiF1": "56.6",
          "MaF1": "42.7"
        },
        {
          "Model": "XLM-R",
          "JI": "42.9",
          "MiF1": "51.9",
          "MaF1": "39.8"
        },
        {
          "Model": "mBERT",
          "JI": "37.0",
          "MiF1": "44.8",
          "MaF1": "31.2"
        },
        {
          "Model": "BetoBERT",
          "JI": "41.3",
          "MiF1": "50.3",
          "MaF1": "37.0"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 14: , we perform crosslingual experi- tallyaddingmorelayerstothepreviouslayersand",
      "data": [
        {
          "SpanEmo": "LVC-Seq2Emo",
          "60.1": "59.2",
          "71.3": "70.9",
          "57.8": "-"
        },
        {
          "SpanEmo": "BinC",
          "60.1": "59.0",
          "71.3": "69.2",
          "57.8": "56.4"
        },
        {
          "SpanEmo": "NTUA",
          "60.1": "58.8",
          "71.3": "70.1",
          "57.8": "52.8"
        },
        {
          "SpanEmo": "Seq2Emo",
          "60.1": "58.7",
          "71.3": "70.1",
          "57.8": "51.9"
        },
        {
          "SpanEmo": "DATN",
          "60.1": "58.3",
          "71.3": "-",
          "57.8": "54.4"
        },
        {
          "SpanEmo": "TCS",
          "60.1": "58.2",
          "71.3": "69.3",
          "57.8": "53.0"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 14: , we perform crosslingual experi- tallyaddingmorelayerstothepreviouslayersand",
      "data": [
        {
          "Model": "mlVAT",
          "JI": "56.9",
          "MiF1": "64.9",
          "MaF1": "51.8"
        },
        {
          "Model": "XLM-Tw",
          "JI": "57.0",
          "MiF1": "65.3",
          "MaF1": "51.5"
        },
        {
          "Model": "BetoBERT",
          "JI": "52.7",
          "MiF1": "60.8",
          "MaF1": "48.7"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 14: , we perform crosslingual experi- tallyaddingmorelayerstothepreviouslayersand",
      "data": [
        {
          "SpanEmo": "CER",
          "53.2": "53.7",
          "64.1": "-"
        },
        {
          "SpanEmo": "MILAB",
          "53.2": "40.7",
          "64.1": "55.8"
        },
        {
          "SpanEmo": "ELiRF",
          "53.2": "44.0",
          "64.1": "53.5"
        },
        {
          "SpanEmo": "TW-StAR",
          "53.2": "39.2",
          "64.1": "52.0"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "mlVAT",
          "JI": "57.3",
          "MiF1": "68.2",
          "MaF1": "52.2"
        },
        {
          "Model": "XLM-Tw",
          "JI": "55.8",
          "MiF1": "67.4",
          "MaF1": "50.1"
        },
        {
          "Model": "AraBERT",
          "JI": "54.3",
          "MiF1": "65.9",
          "MaF1": "49.0"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "SpanEmo": "CA-GRU",
          "54.8": "53.2",
          "66.6": "64.8",
          "52.1": "49.5"
        },
        {
          "SpanEmo": "CER",
          "54.8": "52.9",
          "66.6": "-",
          "52.1": "48.9"
        },
        {
          "SpanEmo": "HEF",
          "54.8": "51.2",
          "66.6": "63.1",
          "52.1": "50.2"
        },
        {
          "SpanEmo": "EMA",
          "54.8": "48.9",
          "66.6": "61.8",
          "52.1": "46.1"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Language": "English",
          "Train": "6838",
          "Dev": "886",
          "Test": "3259"
        },
        {
          "Language": "Arabic",
          "Train": "2278",
          "Dev": "585",
          "Test": "1518"
        },
        {
          "Language": "Spanish",
          "Train": "3561",
          "Dev": "679",
          "Test": "2854"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Language": "EN",
          "JI": "59.4",
          "MiF1": "70.6",
          "MaF1": "55.7"
        },
        {
          "Language": "ES",
          "JI": "57.8",
          "MiF1": "65.8",
          "MaF1": "56.6"
        },
        {
          "Language": "AR",
          "JI": "57.8",
          "MiF1": "68.6",
          "MaF1": "55.5"
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Spanemo: Casting multi-label emotion classification as span-prediction",
      "authors": [
        "Hassan Alhuzali",
        "Sophia Ananiadou"
      ],
      "year": "2021",
      "venue": "Spanemo: Casting multi-label emotion classification as span-prediction",
      "arxiv": "arXiv:2101.10038"
    },
    {
      "citation_id": "2",
      "title": "Hybrid feature model for emotion recognition in arabic text",
      "authors": [
        "Nourah Alswaidan",
        "Bachir Menai"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "3",
      "title": "Arabert: Transformer-based model for arabic language understanding",
      "authors": [
        "Fady Wissam Antoun",
        "Hazem Baly",
        "Hajj"
      ],
      "year": "2020",
      "venue": "Arabert: Transformer-based model for arabic language understanding",
      "arxiv": "arXiv:2003.00104"
    },
    {
      "citation_id": "4",
      "title": "EMA at SemEval-2018 task 1: Emotion mining for Arabic",
      "authors": [
        "Gilbert Badaro",
        "Obeida Jundi",
        "Alaa Khaddaj",
        "Alaa Maarouf",
        "Raslan Kain",
        "Hazem Hajj",
        "Wassim El-Hajj"
      ],
      "year": "2018",
      "venue": "Proceedings of The 12th International Workshop on Semantic Evaluation",
      "doi": "10.18653/v1/S18-1036"
    },
    {
      "citation_id": "5",
      "title": "",
      "authors": [
        "Francesco Barbieri",
        "Jose Camacho-Collados"
      ],
      "venue": ""
    },
    {
      "citation_id": "6",
      "title": "Semeval 2018 task 2: Multilingual emoji prediction",
      "authors": [
        "Francesco Ronzano",
        "Luis Espinosa Anke",
        "Miguel Ballesteros",
        "Valerio Basile",
        "Viviana Patti",
        "Horacio Saggion"
      ],
      "year": "2018",
      "venue": "Proceedings of The 12th International Workshop on Semantic Evaluation"
    },
    {
      "citation_id": "7",
      "title": "A Multilingual Language Model Toolkit for Twitter. In arXiv preprint",
      "authors": [
        "Francesco Barbieri",
        "Luis Espinosa-Anke",
        "Jose Camacho-Collados"
      ],
      "year": "2021",
      "venue": "A Multilingual Language Model Toolkit for Twitter. In arXiv preprint",
      "arxiv": "arXiv:2104.12250"
    },
    {
      "citation_id": "8",
      "title": "Ntua-slp at semeval-2018 task 1: Predicting affective content in tweets with deep attentive rnns and transfer learning",
      "authors": [
        "Christos Baziotis",
        "Nikos Athanasiou",
        "Alexandra Chronopoulou",
        "Athanasia Kolovou"
      ],
      "year": "2018",
      "venue": "Ntua-slp at semeval-2018 task 1: Predicting affective content in tweets with deep attentive rnns and transfer learning",
      "arxiv": "arXiv:1804.06658"
    },
    {
      "citation_id": "9",
      "title": "Spanish pre-trained bert model and evaluation data",
      "authors": [
        "José Cañete",
        "Gabriel Chaperon",
        "Rodrigo Fuentes",
        "Jou-Hui Ho",
        "Hojin Kang",
        "Jorge Pérez"
      ],
      "year": "2020",
      "venue": "Spanish pre-trained bert model and evaluation data"
    },
    {
      "citation_id": "10",
      "title": "Robust neural machine translation with doubly adversarial inputs",
      "authors": [
        "Yong Cheng",
        "Lu Jiang",
        "Wolfgang Macherey"
      ],
      "year": "2019",
      "venue": "Robust neural machine translation with doubly adversarial inputs",
      "arxiv": "arXiv:1906.02443"
    },
    {
      "citation_id": "11",
      "title": "MUC-4 evaluation metrics",
      "authors": [
        "Nancy Chinchor"
      ],
      "year": "1992",
      "venue": "Fourth Message Uunderstanding Conference"
    },
    {
      "citation_id": "12",
      "title": "Semi-supervised sequence modeling with cross-view training",
      "authors": [
        "Kevin Clark",
        "Minh-Thang Luong",
        "Christopher Manning",
        "Quoc V Le"
      ],
      "year": "2018",
      "venue": "Semi-supervised sequence modeling with cross-view training",
      "arxiv": "arXiv:1809.08370"
    },
    {
      "citation_id": "13",
      "title": "Unsupervised cross-lingual representation learning at scale",
      "authors": [
        "Alexis Conneau",
        "Kartikay Khandelwal",
        "Naman Goyal",
        "Vishrav Chaudhary",
        "Guillaume Wenzek",
        "Francisco Guzmán",
        "Edouard Grave",
        "Myle Ott",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "year": "2019",
      "venue": "Unsupervised cross-lingual representation learning at scale",
      "arxiv": "arXiv:1911.02116"
    },
    {
      "citation_id": "14",
      "title": "GoEmotions: A dataset of fine-grained emotions",
      "authors": [
        "Dorottya Demszky",
        "Dana Movshovitz-Attias",
        "Jeongwoo Ko",
        "Alan Cowen",
        "Gaurav Nemade",
        "Sujith Ravi"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2020.acl-main.372"
    },
    {
      "citation_id": "15",
      "title": "Twitter part-of-speech tagging for all: Overcoming sparse and noisy data",
      "authors": [
        "Leon Derczynski",
        "Alan Ritter",
        "Sam Clark",
        "Kalina Bontcheva"
      ],
      "year": "2013",
      "venue": "Proceedings of the international conference recent advances in natural language processing ranlp 2013"
    },
    {
      "citation_id": "16",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2018",
      "venue": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "17",
      "title": "Expression and the nature of emotion",
      "authors": [
        "Paul Ekman"
      ],
      "year": "1984",
      "venue": "Approaches to emotion"
    },
    {
      "citation_id": "18",
      "title": "Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm",
      "authors": [
        "Bjarke Felbo",
        "Alan Mislove",
        "Anders Søgaard",
        "Iyad Rahwan",
        "Sune Lehmann"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/D17-1169"
    },
    {
      "citation_id": "19",
      "title": "Affect-LM: A neural language model for customizable affective text generation",
      "authors": [
        "Sayan Ghosh",
        "Mathieu Chollet",
        "Eugene Laksana",
        "Louis-Philippe Morency",
        "Stefan Scherer"
      ],
      "year": "2017",
      "venue": "Affect-LM: A neural language model for customizable affective text generation"
    },
    {
      "citation_id": "20",
      "title": "Discriminative methods for multi-labeled classification",
      "authors": [
        "Shantanu Godbole",
        "Sunita Sarawagi"
      ],
      "year": "2004",
      "venue": "Pacific-Asia conference on knowledge discovery and data mining"
    },
    {
      "citation_id": "21",
      "title": "ELiRF-UPV at SemEval-2018 tasks 1 and 3: Affect and irony detection in tweets",
      "authors": [
        "José-Ángel González",
        "Lluís-F Hurtado",
        "Ferran Pla"
      ],
      "year": "2018",
      "venue": "Proceedings of The 12th International Workshop on Semantic Evaluation",
      "doi": "10.18653/v1/S18-1092"
    },
    {
      "citation_id": "22",
      "title": "Explaining and harnessing adversarial examples",
      "authors": [
        "Ian Goodfellow",
        "Jonathon Shlens",
        "Christian Szegedy"
      ],
      "year": "2014",
      "venue": "Explaining and harnessing adversarial examples",
      "arxiv": "arXiv:1412.6572"
    },
    {
      "citation_id": "23",
      "title": "Don't stop pretraining: adapt language models to domains and tasks",
      "authors": [
        "Suchin Gururangan",
        "Ana Marasović",
        "Swabha Swayamdipta",
        "Kyle Lo",
        "Iz Beltagy",
        "Doug Downey",
        "Noah Smith"
      ],
      "year": "2020",
      "venue": "Don't stop pretraining: adapt language models to domains and tasks",
      "arxiv": "arXiv:2004.10964"
    },
    {
      "citation_id": "24",
      "title": "Cross-lingual emotion detection",
      "authors": [
        "Sabit Hassan",
        "Shaden Shaar",
        "Kareem Darwish"
      ],
      "year": "2021",
      "venue": "Cross-lingual emotion detection",
      "arxiv": "arXiv:2106.06017"
    },
    {
      "citation_id": "25",
      "title": "Universal language model fine-tuning for text classification",
      "authors": [
        "Jeremy Howard",
        "Sebastian Ruder"
      ],
      "year": "2018",
      "venue": "Universal language model fine-tuning for text classification",
      "arxiv": "arXiv:1801.06146"
    },
    {
      "citation_id": "26",
      "title": "Seq2emo: A sequence to multi-label emotion classification model",
      "authors": [
        "Chenyang Huang",
        "Amine Trabelsi",
        "Xuebin Qin",
        "Nawshad Farruque",
        "Lili Mou",
        "Osmar R Zaiane"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "27",
      "title": "Seq2emo for multi-label emotion classification based on latent variable chains transformation",
      "authors": [
        "Chenyang Huang",
        "Amine Trabelsi",
        "Xuebin Qin",
        "Nawshad Farruque",
        "Osmar R Zaïane"
      ],
      "year": "2019",
      "venue": "Seq2emo for multi-label emotion classification based on latent variable chains transformation",
      "arxiv": "arXiv:1911.02147"
    },
    {
      "citation_id": "28",
      "title": "A deep learning-based approach for multi-label emotion classification in tweets",
      "authors": [
        "Mohammed Jabreel",
        "Antonio Moreno"
      ],
      "year": "2019",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "29",
      "title": "Temporal ensembling for semi-supervised learning",
      "authors": [
        "Samuli Laine",
        "Timo Aila"
      ],
      "year": "2016",
      "venue": "Temporal ensembling for semi-supervised learning",
      "arxiv": "arXiv:1610.02242"
    },
    {
      "citation_id": "30",
      "title": "Biobert: a pre-trained biomedical language representation model for biomedical text mining",
      "authors": [
        "Jinhyuk Lee",
        "Wonjin Yoon",
        "Sungdong Kim",
        "Donghyeon Kim",
        "Sunkyu Kim",
        "Chan Ho",
        "Jaewoo Kang"
      ],
      "year": "2020",
      "venue": "Bioinformatics"
    },
    {
      "citation_id": "31",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "32",
      "title": "Effective self-training for parsing",
      "authors": [
        "David Mcclosky",
        "Eugene Charniak",
        "Mark Johnson"
      ],
      "year": "2006",
      "venue": "Proceedings of the Human Language Technology Conference of the NAACL, Main Conference"
    },
    {
      "citation_id": "33",
      "title": "TCS research at SemEval-2018 task 1: Learning robust representations using multi-attention architecture",
      "authors": [
        "Hardik Meisheri",
        "Lipika Dey"
      ],
      "year": "2018",
      "venue": "Proceedings of The 12th International Workshop on Semantic Evaluation",
      "doi": "10.18653/v1/S18-1043"
    },
    {
      "citation_id": "34",
      "title": "Adversarial training methods for semi-supervised text classification",
      "authors": [
        "Takeru Miyato",
        "Andrew Dai",
        "Ian Goodfellow"
      ],
      "year": "2016",
      "venue": "Adversarial training methods for semi-supervised text classification",
      "arxiv": "arXiv:1605.07725"
    },
    {
      "citation_id": "35",
      "title": "Virtual adversarial training: a regularization method for supervised and semisupervised learning",
      "authors": [
        "Takeru Miyato",
        "Shin-Ichi Maeda",
        "Masanori Koyama",
        "Shin Ishii"
      ],
      "year": "2018",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "36",
      "title": "#emotional tweets",
      "authors": [
        "Saif Mohammad"
      ],
      "year": "2012",
      "venue": "SEM 2012: The First Joint Conference on Lexical and Computational Semantics"
    },
    {
      "citation_id": "37",
      "title": "SemEval-2018 task 1: Affect in tweets",
      "authors": [
        "Saif Mohammad",
        "Felipe Bravo-Marquez",
        "Mohammad Salameh",
        "Svetlana Kiritchenko"
      ],
      "year": "2018",
      "venue": "SemEval-2018 task 1: Affect in tweets"
    },
    {
      "citation_id": "38",
      "title": "Tw-StAR at SemEval-2018 task 1: Preprocessing impact on multi-label emotion classification",
      "authors": [
        "Hala Mulki",
        "Bechikh Chedi",
        "Hatem Ali",
        "Ismail Haddad",
        "Babaoglu"
      ],
      "year": "2018",
      "venue": "Proceedings of The 12th International Workshop on Semantic Evaluation",
      "doi": "10.18653/v1/S18-1024"
    },
    {
      "citation_id": "39",
      "title": "To tune or not to tune? adapting pretrained representations to diverse tasks",
      "authors": [
        "Sebastian Matthew E Peters",
        "Noah Ruder",
        "Smith"
      ],
      "year": "2019",
      "venue": "To tune or not to tune? adapting pretrained representations to diverse tasks",
      "arxiv": "arXiv:1903.05987"
    },
    {
      "citation_id": "40",
      "title": "A general psychoevolutionary theory of emotion",
      "authors": [
        "Robert Plutchik"
      ],
      "year": "1980",
      "venue": "Theories of emotion"
    },
    {
      "citation_id": "41",
      "title": "Adversarial attacks on copyright detection systems",
      "authors": [
        "Parsa Saadatpanah",
        "Ali Shafahi",
        "Tom Goldstein"
      ],
      "year": "2020",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "42",
      "title": "Revisiting lstm networks for semi-supervised text classification via mixed objective function",
      "authors": [
        "Devendra Singh Sachan",
        "Manzil Zaheer",
        "Ruslan Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "43",
      "title": "Emotion detection from text and speech: a survey",
      "authors": [
        "Kashfia Sailunaz",
        "Manmeet Dhaliwal",
        "Jon Rokne",
        "Reda Alhajj"
      ],
      "year": "2018",
      "venue": "Social Network Analysis and Mining"
    },
    {
      "citation_id": "44",
      "title": "A context integrated model for multilabel emotion detection",
      "authors": [
        "Ahmed E Samy",
        "Ehab Samhaa R El-Beltagy",
        "Hassanien"
      ],
      "year": "2018",
      "venue": "Procedia computer science"
    },
    {
      "citation_id": "45",
      "title": "Evidence for universality and cultural variation of differential emotion response patterning",
      "authors": [
        "R Klaus",
        "Harald Scherer",
        "Wallbott"
      ],
      "year": "1994",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "46",
      "title": "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results",
      "authors": [
        "Antti Tarvainen",
        "Harri Valpola"
      ],
      "year": "2017",
      "venue": "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results",
      "arxiv": "arXiv:1703.01780"
    },
    {
      "citation_id": "47",
      "title": "Bert rediscovers the classical nlp pipeline",
      "authors": [
        "Ian Tenney",
        "Dipanjan Das",
        "Ellie Pavlick"
      ],
      "year": "2019",
      "venue": "Bert rediscovers the classical nlp pipeline",
      "arxiv": "arXiv:1905.05950"
    },
    {
      "citation_id": "48",
      "title": "A survey on semi-supervised learning",
      "authors": [
        "E Jesper",
        "Van Engelen",
        "H Holger",
        "Hoos"
      ],
      "year": "2020",
      "venue": "Machine Learning"
    },
    {
      "citation_id": "49",
      "title": "Characterizing adversarial examples based on spatial consistency information for semantic segmentation",
      "authors": [
        "Chaowei Xiao",
        "Ruizhi Deng",
        "Bo Li",
        "Fisher Yu",
        "Mingyan Liu",
        "Dawn Song"
      ],
      "year": "2018",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "50",
      "title": "Current state of text sentiment analysis from opinion to emotion mining",
      "authors": [
        "Ali Yadollahi",
        "Ameneh Gholipour Shahraki",
        "Osmar R Zaiane"
      ],
      "year": "2017",
      "venue": "ACM Computing Surveys (CSUR)"
    },
    {
      "citation_id": "51",
      "title": "SGM: Sequence generation model for multi-label classification",
      "authors": [
        "Pengcheng Yang",
        "Xu Sun",
        "Wei Li",
        "Shuming Ma",
        "Wei Wu",
        "Houfeng Wang"
      ],
      "year": "2018",
      "venue": "Proceedings of the 27th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "52",
      "title": "A survey on deep semi-supervised learning",
      "authors": [
        "Xiangli Yang",
        "Zixing Song",
        "Irwin King",
        "Zenglin Xu"
      ],
      "year": "2021",
      "venue": "A survey on deep semi-supervised learning",
      "arxiv": "arXiv:2103.00550"
    },
    {
      "citation_id": "53",
      "title": "Unsupervised word sense disambiguation rivaling supervised methods",
      "authors": [
        "David Yarowsky"
      ],
      "year": "1995",
      "venue": "33rd annual meeting of the association for computational linguistics"
    },
    {
      "citation_id": "54",
      "title": "Improving multilabel emotion classification via sentiment classification with dual attention transfer network",
      "authors": [
        "Jianfei Yu",
        "Luis Marujo",
        "Jing Jiang"
      ],
      "year": "2018",
      "venue": "Improving multilabel emotion classification via sentiment classification with dual attention transfer network"
    },
    {
      "citation_id": "55",
      "title": "Freelb: Enhanced adversarial training for natural language understanding",
      "authors": [
        "Chen Zhu",
        "Yu Cheng",
        "Zhe Gan",
        "Siqi Sun",
        "Tom Goldstein",
        "Jingjing Liu"
      ],
      "year": "2019",
      "venue": "Freelb: Enhanced adversarial training for natural language understanding",
      "arxiv": "arXiv:1909.11764"
    }
  ]
}