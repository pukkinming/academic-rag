{
  "paper_id": "2104.11560v1",
  "title": "Weakly-Supervised Multi-Task Learning For Multimodal Affect Recognition",
  "published": "2021-04-23T12:36:19Z",
  "authors": [
    "Wenliang Dai",
    "Samuel Cahyawijaya",
    "Yejin Bang",
    "Pascale Fung"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal affect recognition constitutes an important aspect for enhancing interpersonal relationships in human-computer interaction. However, relevant data is hard to come by and notably costly to annotate, which poses a challenging barrier to build robust multimodal affect recognition systems. Models trained on these relatively small datasets tend to overfit and the improvement gained by using complex state-of-the-art models is marginal compared to simple baselines. Meanwhile, there are many different multimodal affect recognition datasets, though each may be small. In this paper, we propose to leverage these datasets using weakly-supervised multi-task learning to improve the generalization performance on each of them. Specifically, we explore three multimodal affect recognition tasks: 1) emotion recognition; 2) sentiment analysis; and 3) sarcasm recognition. Our experimental results show that multi-tasking can benefit all these tasks, achieving an improvement up to 2.9% accuracy and 3.3% F1-score. Furthermore, our method also helps to improve the stability of model performance. In addition, our analysis suggests that weak supervision can provide a comparable contribution to strong supervision if the tasks are highly correlated.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Deep learning involving multiple modalities can be seen as a joint field of computer vision and natural language processing, which has become much more popular in recent years  (Vinyals et al., 2015; Goyal et al., 2017; Sanabria et al., 2018) . For human affect recognition tasks (e.g. emotion recognition, sentiment analysis, sarcasm recognition, etc.), more modalities can provide complementary and supplementary information  (Baltru≈°aitis et al., 2018)  and help to recognize affect more accurately. Prior works mainly focus on two major directions: 1) mod-Preprint. Code will be available soon.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Negative Sentiment",
      "text": "Auxiliary Task Figure  1 : An example of multi-task learning (MTL) when emotion recognition is the main task. Given the input (positive text, skeptical face and hesitating voice), it is not clear what emotion the woman has, resulting in a positive prediction due to the strong signal from the textual modality. However, if we could know she is actually being sarcastic or her sentiment is negative, the prediction will be leaned towards negative emotions. MTL is a way to eavesdrop external information by leveraging more useful supervisions. In this paper, we explore weakly-supervised MTL which enables to get more supervisions with zero extra human labor.\n\neling the intra-modal dynamics (unimodal representation learning); and 2) improving the inter-modal dynamics (cross-modal interactions and modality fusion). For example, various fusion methods have been proposed, ranging from the basic modelagnostic ones like Early-Fusion  (Morency et al., 2011)  and Late-Fusion  (Zadeh et al., 2016) , to more complex ones like Tensor-based Fusion  (Zadeh et al., 2017; Liu et al., 2018) , Attention-based Fusion  (Zadeh et al., 2018a; Wang et al., 2019a; Tsai et al., 2019) .\n\nDespite the progress, multimodal affect recognition datasets  (Busso et al., 2008; Zadeh et al., 2016 Zadeh et al., , 2018b;; Castro et al., 2019)  present a performance bottleneck that they are relatively small when compared with unimodal research datasets  (Deng et al., 2009; Lin et al., 2014a; Rajpurkar et al., 2016)  or other multimodal ones  (Lin et al., 2014b; Johnson et al., 2017; Sidorov et al., 2020) , which poses two potential problems: 1) models are easy to overfit and cannot generalize well; and 2) instability of performance (e.g. alter a random seed for weights initialization can lead to a salient change of the model performance). To study and validate the severity these problems, we benchmark 12 different models on two commonly used multimodal emotion recognition datasets (Table  3  and Appendix  A ). The experimental results show that recent stateof-the-art (SOTA) models do not have an obvisou advantage over simple baselines. By constructing a naive hybrid fusion mechanism, the baselines can easily surpass the SOTA models. Furthermore, to investigate the instability issue, we run each model with five different random seeds (0 -4) on those two datasets. Averagely, the standard deviations of the weighted accuracy  (Tong et al., 2017)  and F1 are 1.8% and 1.7%, respectively. The performance improvement gained by just altering a random seed is similar to or even larger than the improvement gained by using a more advanced method from the prior work  (Zadeh et al., 2018a; Tsai et al., 2019; Wang et al., 2019b; Dai et al., 2020a) .\n\nGiven the fact that there are many different multimodal datasets related to affect recognition and most of them are single-task labeled, to mitigate the aforementioned problems, we propose to conduct weakly-supervised multi-task learning (MTL) to improve the generalization performance and stability of models  (Zhang and Yang, 2017)  (Figure  1 ). Compared to MTL with strong supervision, weaklysupervised MTL does not require the datasets to have multiple labels at the same time, which makes it much cheaper and more flexible. It can be seen as an implicit way of achieving data augmentation without any additional human labor (more details in Section 5). According to our experiments, we get an improvement up to 2.9% accuracy and 3.3% F1-score by leveraging weakly-supervised MTL.\n\nThe contributions of this paper are summarized as follows:\n\n‚Ä¢ We thoroughly benchmark 12 models on two widely used multimodal affect recognition datasets. Based on this, we further propose a simple but effective hybrid modelagnostic modality fusion method, which performs equally or even better than previous state-of-the-art models.\n\n‚Ä¢ We show the effectiveness of weaklysupervised MTL on relevant multimodal affect recognition tasks. We achieve an improvement, up to 2.9% accuracy and 3.3% F1-score, on three multimodal affect recognition tasks. Furthermore, our results demonstrate that weak labels can bring comparable improvement as strong labels.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Works",
      "text": "Multimodal affect recognition. Multimodal affect recognition has attracted increasing attention in recent years. It can be seen as a family of tasks, including multimodal emotion recognition, sentiment analysis, sarcasm recognition, etc. There are two major focuses in this research field  (Baltru≈°aitis et al., 2018) : 1) how to better model intra-modal dynamics, i.e. improving the representation learning of a single modality; and 2) how to improve the intermodal dynamics, i.e. the interactions cross different modalities. Diverse methods have been proposed to improve these two parts. For example, quite a few works focus on the fusion of modalities, such as the Tensor Fusion Network  (Zadeh et al., 2017) , Memory Fusion Network  (Zadeh et al., 2018a) , Multimodal Adaptation Gate  (Rahman et al., 2020) . Additionally, Multimodal Transformer  (Tsai et al., 2019)  was introduced to handle unaligned data,  Dai et al. (2020a)  proposed to use emotional embeddings to enable zero-/few-shot learning for lowresource senarios, and  Dai et al. (2021)  introduced the sparse cross-attention to improve performance and reduce computation. Despite the remarkbale progress has been made, we find that most models suffer from the small scale of data on these tasks. For example, the Multimodal Transformer  (Tsai et al., 2019)  has a dimenion of only 40, meanwhile with a large dropout value around 0.3.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Multi-Task Learning (Mtl).",
      "text": "MTL has been widely used in numerous tasks to improve the performance of models. It can be seen as an implicit data augmentation and an eavesdropping of extra supervision to improve the generalization ability of models  (Zhang and Yang, 2017) . For example, in computer vision,  Xu et al. (2018)  proposed PAD-Net, which tackles depth estimation and scene parsing in a joint CNN with four intermediate auxiliary tasks. Moreover,  Kokkinos (2017)  invented UberNet, which solves seven different tasks simultaneously. In natural language processing, MTL is also leveraged in various tasks, such as offensive text detection  (Abu Farha and Magdy, 2020; Dai et al., 2020b) , summarization  (Yu et al., 2020) , question answering  (McCann et al., 2018) , etc. For multimodal affect recognition tasks, not much work has been carried out to incorporate multi-tasking. Of the few works that have considered multi-tasking,  Akhtar et al. (2019)  proposed to tackle sentiment analysis and emotion recognition jointly. However, their method is only verified on one dataset, as it requires the dataset to have multiple human annotation at the same time. Also,  Chauhan et al. (2020)  studied the relationship between sentiment, emotion, and sarcasm by manually annotating a sarcasm dataset  (Castro et al., 2019) , which we believe does not scale. In addition, they only labeled a few hundred of samples, which limits the persuasion of their results. Differently, our work explores weakly-supervised multi-task learning which is much cheaper and more scalable. Moreover, our experiments are done in a wider range with three different datsets.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Data And Evaluation Metrics",
      "text": "In this section, we first introduce three datasets used for model benchmarking and weakly-supervised multi-task learning. Then, we discuss the feature extraction algorithms to pre-process the data. Finally, we illustrate the metrics we use to evaluate models.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Datasets",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Iemocap. The Interactive Emotional Dyadic",
      "text": "Motion Capture (IEMOCAP)  (Busso et al., 2008)  is a dataset for multimodal emotion recognition, which contains 151 videos along with the corresponding transcripts and audios. In each video, two professional actors conduct dyadic dialogues in English. Although the human annotation has nine emotion categories, following the prior works  (Hazarika et al., 2018; Wang et al., 2019a; Tsai et al., 2019; Dai et al., 2020a) , we take four categories: neutral, happy, sad, and angry.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Cmu-Mosei.",
      "text": "The CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI)  (Zadeh et al., 2018b ) is a dataset for both multimodal emotion recognition and sentiment analysis. It comprises 3,837 videos from 1,000 diverse speakers and annotated with six emotion categories: happy, sad, angry, fearful, disgusted, and surprised. In addition, each data sample is also annotated with a sentiment score on a Likert scale  [-3, 3] . MUStARD. The Multimodal Sarcasm Detection Dataset (MUStARD)  (Castro et al., 2019 ) is a multimodal video corpus for sarcasm recognition. The dataset has 690 samples with an even number of sarcastic and non-sarcastic labels.\n\nWe show the data statistics of three datasets in Table  1 .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Data Feature Extraction",
      "text": "Feature extraction is done for each modality to extract high-level features before training. For the textual modality, we use the pre-trained GloVe  (Pennington et al., 2014)  embeddings to represent words (glove.840B.300d ). For the acoustic modality, CO-VAREP  (Degottex et al., 2014)  is used to extract features of dimension 74 from the raw audio data. The features include fundamental frequency (F0), Voice/Unvoiced feature (VUV), quasi open quotient (QOQ), normalized amplitude quotient (NAQ), glottal source parameters (H1H2, Rd, Rd conf), maxima dispersion quotient (MDQ), parabolic spectral parameter (PSP), tilt/slope of wavelet response (peak/slope), harmonic model and phase distortion mean (HMPDM and HMPDD), and Mel Cepstral Coefficients (MCEP). For the visual modality, 35 facial action units  (Ekman et al., 1980)  are extracted from each frame of the video with OpenFace 2.0  (Baltrusaitis et al., 2018) . Following previous works  (Tsai et al., 2018; Zadeh et al., 2018b) , word-level alignment is done with P2FA  (Yuan and Liberman, 2008)  to achieve the same sequence length for each modality. We reduce multiple feature segments within one aligned word into a single segment by taking the mean value over the segments.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "Overall, we use three metrics for different datasets. For the emotion recognition task, as we evaluate on each emotion category, there are much more negative data that positive data, we use the weighted Accuracy (WAcc)  (Tong et al., 2017)  to mitigate the class imbalance issue. The formula of WAcc is\n\nwhere ùëÅ means total negative, ùëá ùëÅ true negative, ùëÉ total positive, and ùëá ùëÉ true positive. For sentiment analysis and sarcasm recognition, we just use normal accuracy as the data is more balanced. In addition, we also use the F1-score for all the tasks. However, we do not use the binary weighted F1-score (WF1) as some previous works  (Zadeh et al., 2018a,b; Tsai et al., 2019; Rahman et al., 2020)  did. The formula of WF1 is shown below,\n\nin which F1 ùëù is the F1 score that treats positive samples as positive, while F1 ùëõ treats negative samples as positive, and they are weighted by their portion of the data (ùêº is the total number of samples). We think that WF1 makes the class imbalance issue even severer, as F1 ùëù only contributes a small portion of the total WF1. According to our experiments on IMEOCAP, when using WF1 to evaluate models, by increasing the threshold of classification, the WF1 increases a lot as well. Even when the threshold is 0.9 (means most of the samples will be classified as negative), the average of WF1 can still be higher than 0.7 while the WAcc is already below 0.5. A similar phenomenon is also observed by  Dai et al. (2020a) . Therefore, we just use the normal unweighted F1-score, which we think can better reflect the model's performance.\n\nhttps://github.com/TadasBaltrusaitis/ OpenFace",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Model Benchmarking",
      "text": "As mentioned in Section 1, we conjecture that the data sacacity issue of multimodal affect recognition tasks causes two problems: 1) models tend to be overfitted; and 2) instability of performance. To verify the severity of them, we benchmark 12 different models on two commonly used multimodal emotion recognition datasets: IEMOCAP  (Busso et al., 2008)  and CMU-MOSEI  (Zadeh et al., 2018b) . The models include six baselines, three recently proposed SOTA models, and three advanced baselines with the hybrid fusion method. All the models are listed in the first column of Table  3 . The implementation details are discussed in Section 6.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Baselines",
      "text": "For the baselines, we use two commonly used model-agnostic fusion methods (Figure  2 ): 1) earlyfusion (EF), which can capture low-level crossmodal interactions; and 2) late-fusion (LF), which can capture high-level ones. They are used to build simple baselines. Besides, based on EF and LF, we propose to combine them as a hybrid fusion method (EF-LF) to construct strong baselines.\n\nWithin each fusion method, we apply three different model architectures, Average of features (AVG), bi-directional Long-Short Term Memory (LSTM)  (Hochreiter and Schmidhuber, 1997) , and Transformer  (Vaswani et al., 2017) , to process sequences of feature vectors. As the simplest baseline, the AVG model just takes the mean of the input vectors as the output vector. For LSTM, we take the output vector at the last time step as the representation of the whole input sequence. For Transformer, following the practice of previous work  (Vaswani et al., 2017; Devlin et al., 2019; Liu et al., 2019) , we prepend a [CLS] token to the input sequence and use the output embedding at that position.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Sota Models",
      "text": "Apart from the baselines, we also select three recently proposed state-of-the-art models on this task for comparison: 1) Memory Fusion Network (MFN)  (Zadeh et al., 2018a) , which has a multi-view gated memory for storing cross-modal interactions over time; 2) Multimodal Transformer (MulT)  (Tsai et al., 2019) , which fuses each pair of modalities with self-attention and can handle unaligned sequences; and 3) the model proposed by  Dai et al. (2020a) , which leverages the information inside emotion embeddings (EMO-EMB) and can hanlde zero-/few-shot senarios.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Weakly-Supervised Multi-Task Learning",
      "text": "In this section, we first formally define the problem settings. Then, we explain our method for multitask learning and how do we generate weak labels.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Problem Definition",
      "text": "We define a multimodal affect recognition dataset ùê∑ with ùêº data samples as ùê∑ = {(ùë° ùëñ , ùëé ùëñ , ùë£ ùëñ , ùë¶ ùëñ )} ùêº ùëñ=1 , in which ùë° ùëñ ‚àà R ùë†√óùëë ùë° is a sequence of word embeddings to represent a sentence, ùëé ùëñ ‚àà R ùë†√óùëë ùëé denotes an aligned sequence of audio features, ùë£ ùëñ ‚àà R ùë†√óùëë ùë£ denotes an aligned sequence of facial action units (FAUs) extracted from the video frames, and ùë¶ ùëñ ‚àà R ùëë ùë¶ denotes the golden label. Each modality has the same sequence ùë†. For different datasets, the inputs ùë°, ùëé, ùë£ have the same dimension as we use the same feature extraction across all datasets, only the dimension of labels ùëë ùë¶ might be different, depending on how many classes the dataset has.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Multi-Task Learning (Mtl) For Multimodal Affect Recognition",
      "text": "As mentioned in Section 1, datasets for multimodal affect recognition are relatively small in nature, which causes two problems. To mitigate them, we propose to utilize MTL by two reasons. Firstly, MTL can potentially improve the generalization performance when tasks are relevant  (Zhang and Yang, 2017) . Secondly, there are many existing datasets related to multimodal affect recognition, even though each can be small, and their data come in a similar format (text, audio, video) and can be pre-processed in the same way. Specifically, we leverage three relevant multimodal tasks: 1) emotion recognition; 2) sentiment analysis; and 3) sarcasm recognition.\n\nWeak Label Acquisition. Given two separate datasets ùê∑ 1 and ùê∑ 2 on two different tasks ùëá 1 and ùëá 2 , we generate weak labels of ùëá 2 for ùê∑ 1 in two steps: 1) first train a model on the data of ùê∑ 2 ; and 2) use the trained model to infer predictions on the data of ùê∑ 1 , and the predictions are treated as weak labels of ùëá 2 . Specifically, in this paper, we train a dedicated EF-LF-LSTM model to generate weak labels on each of the three tasks following the aforementioned procedure. Table  3 : Performance evaluation of 12 models (six simple baselines, three strong baselines, and three SOTA models) on the IEMOCAP  (Busso et al., 2008)  dataset. We report the weighted accuracy (WAcc) and the F1-score on four emotion categories: neutral, happy, sad, and angry. In addition, we also report the average of them as an overall measurement. For each model, we run five random seeds {0, 1, 2, 3, 4} and report the mean ¬± standard_deviation. The best performance is decorated in bold.\n\n2018b;  Liu et al., 2018; Rahman et al., 2020) , we simplify it to a two-class classification problem (either positive or negative) and generate binary labels. For the other tasks, we follow the original categories of the datasets. In addition, we also store the accuracy of the EF-LF-LSTM model on each dataset as a confidence score of the generated weak labels.\n\nWeakly-supervised MTL. After getting the weak labels, we can conduct weakly-supervised learning for different tasks. The training procedure is shown in Eq.1 and 2:\n\nmin\n\nwhere ùëì ùëä ùëè is the backbone model with weights ùëä ùëè shared by all tasks, which generates a representation ùëü ùëñ of the input data, and ùêΩ is the number of tasks.\n\nFor each task ùëó, there is a linear layer ùê¥ ùëä ùëó with parameters ùëä ùëó to perform affine transformation on ùëü ùëñ to get the desired output dimension. The overall objective is to minimize the total loss ùêø of all tasks, and each task has a loss function ùêø ùëó with a weighting factor ùúÜ ùëó . This weighting factor can be either the confidence score mentioned in the last paragraph, or a hyper-parameter searched manually. For the strong labels of the main task, the confidence score is 1.\n\nFor datasets that contain multiple labels, we can directly apply this MTL procedure, while for unlabeled tasks, we perform MTL in a weaklysupervised way. For example, we can train a model on sentiment analysis and use it to predict sentiment scores for a sarcasm recognition dataset. Then, jointly use predicted weak labels with the original labels of the dataset for MTL.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Experiments And Analysis",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experimental Settings",
      "text": "To ensure we make a fair comparison of the models, we perform an elaborate hyper-parameter search with the following strategies. Firstly, for each model, we try the combinations of four learning rates {1e -3 , 5e -4 , 1e -4 , 5e -5 } and six batch sizes {16, 32, 64, 128, 256, 512}, resulting in 24 experiments. Among their hyper-parameter settings achiveing top-5 performance, we further try different model-dependent hyper-parameters, such as the hidden dimension, feed-forwad dimension, number of layers, dropout values, etc. For the previous state-of-the-art models, we also conduct a similar hyper-parameter search based on their reported numbers in the paper. To test the stability of models and eliminate the possible contingency caused by weights initialization, for the best setting of each model, we run five different random seeds {0, 1, 2, 3, 4} and report the mean and standard deviation. The best hyper-parameters are shown in Appendix In the first column, we show the main target task and the corresponding dataset. In the second column, we show the tasks used in the training process. The symbol + means adding an auxiliary task in the training, besides the main target task. All means using all the tasks above for training. (ùëÜ) or (ùëä) indicates whether this external supervision is strong or weak. For emotion recognition, Avg. means the average of all emotion categories. In the bottom block, we also compare the effectiveness of using strong and weak label when conducting MTL.\n\nB. For the emotion recognition, we use the binary cross-entropy loss as the data are multi-labeled (a person can have multiple), with a loss weight for positive samples to alleviate the data imbalance issue. For the sentiment prediction and sarcasm recognition, we use the cross-entropy loss. The Adam optimizer  (Kingma and Ba, 2015)  is used for all of our experiments with ùõΩ 1 = 0.9, ùõΩ 2 = 0.999 and a weight decay of 1e -5 . Our code is implemented in PyTorch  (Paszke et al., 2019)  and run on a single NVIDIA 1080Ti GPU.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Benchmarking Results Analysis",
      "text": "To investigate the two problems aforementioned in Section 1 and have an overall understanding of the performance of various models on multimodal affect recognition, we benchmark 12 models (Section 4) on CMU-MOSEI and IEMOCAP. The results on IEMOCAP are shown in Table  3 , and the results on CMU-MOSEI are included in Appendix A. As explained in Section 3.3, we use slightly different evaluation metrics compared to previous works  (Zadeh et al., 2018a; Tsai et al., 2019)  to better reflect the model performance. First of all, we discover that SOTA models do not have an obvious advantage over the baselines on these two datasets. The hybrid modality fusion (EF+LF) with a simple architecture can surpass the SOTA models. We conjecture that the data scarcity issue makes complex architextures not able to show their full capacity. For example, the Multimodal Transformer (MULT)  (Tsai et al., 2019)  use a hidden dimension of only 40 and a dropout value of 0.3 to achieve its best performance (i.e. avoid overfitting). Secondly, we find out that the performance is unstable given the small size of data. For example, by altering a random seed, the WAcc can change up to 4.7%. On average, we get a standard deviation of 1.8% for WAcc and 1.7% for F1-score on the IEMOCAP. Besides the dataset size, we speculate that another reason for the overfitting is that there is a feature extraction step before training to get the high-level features from the raw data. Thus the information in the resulting input features is  highly concentrated, especially the audio and video features, which makes the problem of data scarcity even severer.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Effects Of Weakly-Supervised Mtl",
      "text": "Experimental results of MTL are shown in Table  4 . We evaluate weakly-supervised MTL using three models for four target tasks across three datasets. Generally, by incorporating auxiliary tasks with weak labels, we can achieve a better performance and this kind of improvement is consistantly observed on four target tasks across all datasets. We use the weak labels as soft labels by weighting the loss down from the auxiliary tasks. As mentioned in Section 5.2, the loss weights can be the confidence scores or manually searched. The best loss weights are reported in Appendix B. Additionally, it also mitigates the instability problem. Still on the emotion recognition task, the standard deviations of WAcc and F1-score are lowered by 0.4% and 0.3%, respectively. On the multimodal emotion recognition task, we observe that either sentiment or sarcasm weak labels can help to increase the performance of the models. On the sarcasm recognition task, the improvement gained by MTL is even larger. For example, the EF-LF LSTM model can achieve 2.9% accuracy and 3.3% F1-score improvement when trained with an additional sentiment or emotion task. This fact shows that even though the auxiliary labels are noisy, MTL is still effective to help the model generalize better, especially when the target dataset has an insufficient number of samples. However, when we try to add two weak labels together to the main task, we do not observe further improvement compared to one auxiliary task. We spectulate that it is because more kinds of weak labels introduce more noise from different domains, which makes it harder for the model to learn and generalize.\n\nOn the multimodal sentiment analysis task, MTL with emotion labels consistently make a greater contribution than with sarcasm labels. This aligns with our label analysis in Table  5  that sentiment has a higher correlation with emotion than sarcasm. Furthermore, we also compare the effectiveness of strong and weak emotion labels on the CMU-MOSEI dataset when sentiment analysis is the main task. We observe that weak emotion labels can contribute similarly or even slightly better than the strong emotion labels. One of the reason is that they contain different emotion categories, and the weak emotion labels have the neutral class, which could be more helpful for recognizing sentiment.\n\nWe think this is beneficial for future work, by providing the alternative to use weak-supervision as a cheaper solution for performance improvement compared to manually annotating the data.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusion And Future Work",
      "text": "In this work, we prove the effectiveness of weaklysupervised multi-task learning (MTL) for multimodal affect recognition tasks. We show that it can significantly improve the performance, especially when the dataset size is small. For instance, on the sarcasm recognition task, the weakly-supervised MTL approach can improve the performance by up to 2.9% accuracy and 3.3% F1-score. We further conduct an empirical analysis on the effects of strong and weak (noisy) supervision, and show that weak supervision can help to boost performance almost as well as strong supervision. It's also a more flexible and cheaper way to incorporate more related supervision. Additionally, we introduce a simple but very effective hybrid modality fusion method by combining early fusion and late fusion. Its performance is on par with or even better than previous state-of-the-art models, which we believe can be used as a strong baseline for future work on multimodal affect recognition tasks.",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: An example of multi-task learning (MTL)",
      "page": 1
    },
    {
      "caption": "Figure 2: ): 1) early-",
      "page": 4
    },
    {
      "caption": "Figure 2: Visualization of three model-agnostic modal-",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "eling the intra-modal dynamics (unimodal represen-": ""
        },
        {
          "eling the intra-modal dynamics (unimodal represen-": "tation learning); and 2) improving the inter-modal"
        },
        {
          "eling the intra-modal dynamics (unimodal represen-": "dynamics (cross-modal interactions and modality"
        },
        {
          "eling the intra-modal dynamics (unimodal represen-": ""
        },
        {
          "eling the intra-modal dynamics (unimodal represen-": "For example, various\nfusion methods"
        },
        {
          "eling the intra-modal dynamics (unimodal represen-": ""
        },
        {
          "eling the intra-modal dynamics (unimodal represen-": "have been proposed, ranging from the basic model-"
        },
        {
          "eling the intra-modal dynamics (unimodal represen-": ""
        },
        {
          "eling the intra-modal dynamics (unimodal represen-": "agnostic ones like Early-Fusion (Morency et al.,"
        },
        {
          "eling the intra-modal dynamics (unimodal represen-": ""
        },
        {
          "eling the intra-modal dynamics (unimodal represen-": "2011) and Late-Fusion (Zadeh et al., 2016), to more"
        },
        {
          "eling the intra-modal dynamics (unimodal represen-": ""
        },
        {
          "eling the intra-modal dynamics (unimodal represen-": "like Tensor-based Fusion (Zadeh"
        },
        {
          "eling the intra-modal dynamics (unimodal represen-": ""
        },
        {
          "eling the intra-modal dynamics (unimodal represen-": "et al., 2017; Liu et al., 2018), Attention-based Fu-"
        },
        {
          "eling the intra-modal dynamics (unimodal represen-": ""
        },
        {
          "eling the intra-modal dynamics (unimodal represen-": "sion (Zadeh et al., 2018a; Wang et al., 2019a; Tsai"
        },
        {
          "eling the intra-modal dynamics (unimodal represen-": ""
        },
        {
          "eling the intra-modal dynamics (unimodal represen-": ""
        },
        {
          "eling the intra-modal dynamics (unimodal represen-": ""
        },
        {
          "eling the intra-modal dynamics (unimodal represen-": "Despite the progress, multimodal aÔ¨Äect recogni-"
        },
        {
          "eling the intra-modal dynamics (unimodal represen-": ""
        },
        {
          "eling the intra-modal dynamics (unimodal represen-": "tion datasets (Busso et al., 2008; Zadeh et al., 2016,"
        },
        {
          "eling the intra-modal dynamics (unimodal represen-": ""
        },
        {
          "eling the intra-modal dynamics (unimodal represen-": "2018b; Castro et al., 2019) present a performance"
        },
        {
          "eling the intra-modal dynamics (unimodal represen-": ""
        },
        {
          "eling the intra-modal dynamics (unimodal represen-": "bottleneck that they are relatively small when com-"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table 3: and Appendix",
      "data": [
        {
          "data augmentation and an eavesdropping of extra": ""
        },
        {
          "data augmentation and an eavesdropping of extra": "supervision to improve the generalization ability"
        },
        {
          "data augmentation and an eavesdropping of extra": ""
        },
        {
          "data augmentation and an eavesdropping of extra": "of models (Zhang and Yang, 2017).\nFor exam-"
        },
        {
          "data augmentation and an eavesdropping of extra": "ple, in computer vision, Xu et al. (2018) proposed"
        },
        {
          "data augmentation and an eavesdropping of extra": "PAD-Net, which tackles depth estimation and scene"
        },
        {
          "data augmentation and an eavesdropping of extra": "parsing in a joint CNN with four intermediate aux-"
        },
        {
          "data augmentation and an eavesdropping of extra": "iliary tasks. Moreover, Kokkinos (2017) invented"
        },
        {
          "data augmentation and an eavesdropping of extra": "UberNet, which solves seven diÔ¨Äerent tasks simul-"
        },
        {
          "data augmentation and an eavesdropping of extra": "taneously.\nIn natural language processing, MTL is"
        },
        {
          "data augmentation and an eavesdropping of extra": "also leveraged in various tasks, such as oÔ¨Äensive"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 3: and Appendix",
      "data": [
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "other multimodal ones (Lin et al., 2014b; Johnson",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": "supervised MTL on relevant multimodal af-"
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "et al., 2017; Sidorov et al., 2020), which poses two",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": "fect\nrecognition tasks. We achieve an im-"
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "potential problems: 1) models are easy to overÔ¨Åt",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": "provement, up to 2.9% accuracy and 3.3%"
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "and cannot generalize well; and 2) instability of",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": "F1-score, on three multimodal aÔ¨Äect recogni-"
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "performance (e.g. alter a random seed for weights",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": "tion tasks. Furthermore, our results demon-"
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "initialization can lead to a salient change of\nthe",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": "strate that weak labels can bring comparable"
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "model performance).\nTo study and validate the",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": "improvement as strong labels."
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "severity these problems, we benchmark 12 diÔ¨Äerent",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": ""
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": "2\nRelated Works"
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "models on two commonly used multimodal emo-",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": ""
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "tion recognition datasets (Table 3 and Appendix",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": ""
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": "Multimodal aÔ¨Äect recognition.\nMultimodal af-"
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "A). The experimental results show that recent state-",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": ""
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": "fect recognition has attracted increasing attention"
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "of-the-art (SOTA) models do not have an obvisou",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": ""
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": "in recent years.\nIt can be seen as a family of tasks,"
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "advantage over simple baselines. By constructing a",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": ""
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": "including multimodal emotion recognition, senti-"
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "naive hybrid fusion mechanism, the baselines can",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": ""
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": "ment analysis, sarcasm recognition, etc. There are"
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "easily surpass the SOTA models. Furthermore, to",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": ""
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": "two major focuses in this research Ô¨Åeld (Baltru≈°aitis"
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "investigate the instability issue, we run each model",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": ""
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": "et al., 2018): 1) how to better model intra-modal dy-"
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "with Ô¨Åve diÔ¨Äerent random seeds (0 - 4) on those",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": ""
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": "namics, i.e.\nimproving the representation learning"
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "two datasets. Averagely, the standard deviations of",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": ""
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": "of a single modality; and 2) how to improve the inter-"
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "the weighted accuracy (Tong et al., 2017) and F1",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": ""
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": "modal dynamics, i.e.\nthe interactions cross diÔ¨Äerent"
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "are 1.8% and 1.7%, respectively. The performance",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": ""
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": "modalities. Diverse methods have been proposed"
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "improvement gained by just altering a random seed",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": ""
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": "to improve these two parts. For example, quite a"
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "is similar to or even larger than the improvement",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": ""
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": "few works focus on the fusion of modalities, such"
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "gained by using a more advanced method from the",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": ""
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": "as the Tensor Fusion Network (Zadeh et al., 2017),"
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "prior work (Zadeh et al., 2018a; Tsai et al., 2019;",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": ""
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": "Memory Fusion Network (Zadeh et al., 2018a),"
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "Wang et al., 2019b; Dai et al., 2020a).",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": ""
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": "Multimodal Adaptation Gate (Rahman et al., 2020)."
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "Given the fact that there are many diÔ¨Äerent mul-",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": ""
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": "Additionally, Multimodal Transformer (Tsai et al.,"
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "timodal datasets related to aÔ¨Äect recognition and",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": ""
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": "2019) was introduced to handle unaligned data,"
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "most of them are single-task labeled, to mitigate the",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": ""
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": "Dai et al. (2020a) proposed to use emotional em-"
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "aforementioned problems, we propose to conduct",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": ""
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": "beddings to enable zero-/few-shot learning for low-"
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "weakly-supervised multi-task learning (MTL) to",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": ""
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": "resource senarios, and Dai et al. (2021) introduced"
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "improve the generalization performance and stabil-",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": ""
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": "the sparse cross-attention to improve performance"
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "ity of models (Zhang and Yang, 2017) (Figure 1).",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": ""
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": "and reduce computation. Despite the remarkbale"
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "Compared to MTL with strong supervision, weakly-",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": ""
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": "progress has been made, we Ô¨Ånd that most models"
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "supervised MTL does not require the datasets to",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": ""
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": "suÔ¨Äer from the small scale of data on these tasks."
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "have multiple labels at the same time, which makes",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": ""
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": "For example,\nthe Multimodal Transformer\n(Tsai"
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "it much cheaper and more Ô¨Çexible.\nIt can be seen",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": ""
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": "et al., 2019) has a dimenion of only 40, meanwhile"
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "as an implicit way of achieving data augmentation",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": ""
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": "with a large dropout value around 0.3."
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "without any additional human labor (more details",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": ""
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": "Multi-task Learning (MTL).\nMTL has been"
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "in Section 5). According to our experiments, we",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": ""
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": "widely used in numerous tasks to improve the per-"
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "get an improvement up to 2.9% accuracy and 3.3%",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": ""
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": "formance of models.\nIt can be seen as an implicit"
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "F1-score by leveraging weakly-supervised MTL.",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": ""
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": "data augmentation and an eavesdropping of extra"
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "The contributions of this paper are summarized",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": ""
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": "supervision to improve the generalization ability"
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "as follows:",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": ""
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": "of models (Zhang and Yang, 2017).\nFor exam-"
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "‚Ä¢ We thoroughly benchmark 12 models on two",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": "ple, in computer vision, Xu et al. (2018) proposed"
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "widely used multimodal\naÔ¨Äect\nrecognition",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": "PAD-Net, which tackles depth estimation and scene"
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "datasets.\nBased\non\nthis, we\nfurther\npro-",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": "parsing in a joint CNN with four intermediate aux-"
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "pose\na\nsimple but\neÔ¨Äective hybrid model-",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": "iliary tasks. Moreover, Kokkinos (2017) invented"
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "agnostic modality fusion method, which per-",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": "UberNet, which solves seven diÔ¨Äerent tasks simul-"
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "forms equally or even better\nthan previous",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": "taneously.\nIn natural language processing, MTL is"
        },
        {
          "2009; Lin et al., 2014a; Rajpurkar et al., 2016) or": "state-of-the-art models.",
          "‚Ä¢ We\nshow\nthe\neÔ¨Äectiveness\nof\nweakly-": "also leveraged in various tasks, such as oÔ¨Äensive"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "our work explores weakly-supervised multi-task": "learning which is much cheaper and more scalable."
        },
        {
          "our work explores weakly-supervised multi-task": "Moreover, our experiments are done in a wider"
        },
        {
          "our work explores weakly-supervised multi-task": "range with three diÔ¨Äerent datsets."
        },
        {
          "our work explores weakly-supervised multi-task": ""
        },
        {
          "our work explores weakly-supervised multi-task": "3\nData and Evaluation Metrics"
        },
        {
          "our work explores weakly-supervised multi-task": ""
        },
        {
          "our work explores weakly-supervised multi-task": "In this section, we Ô¨Årst introduce three datasets used"
        },
        {
          "our work explores weakly-supervised multi-task": ""
        },
        {
          "our work explores weakly-supervised multi-task": "for model benchmarking and weakly-supervised"
        },
        {
          "our work explores weakly-supervised multi-task": ""
        },
        {
          "our work explores weakly-supervised multi-task": "multi-task learning. Then, we discuss the feature"
        },
        {
          "our work explores weakly-supervised multi-task": ""
        },
        {
          "our work explores weakly-supervised multi-task": "extraction algorithms to pre-process the data. Fi-"
        },
        {
          "our work explores weakly-supervised multi-task": ""
        },
        {
          "our work explores weakly-supervised multi-task": "nally, we illustrate the metrics we use to evaluate"
        },
        {
          "our work explores weakly-supervised multi-task": "models."
        },
        {
          "our work explores weakly-supervised multi-task": ""
        },
        {
          "our work explores weakly-supervised multi-task": "3.1\nDatasets"
        },
        {
          "our work explores weakly-supervised multi-task": ""
        },
        {
          "our work explores weakly-supervised multi-task": "IEMOCAP.\nThe Interactive Emotional Dyadic"
        },
        {
          "our work explores weakly-supervised multi-task": ""
        },
        {
          "our work explores weakly-supervised multi-task": "Motion Capture (IEMOCAP) (Busso et al., 2008)"
        },
        {
          "our work explores weakly-supervised multi-task": ""
        },
        {
          "our work explores weakly-supervised multi-task": "is a dataset\nfor multimodal emotion recognition,"
        },
        {
          "our work explores weakly-supervised multi-task": ""
        },
        {
          "our work explores weakly-supervised multi-task": "which contains 151 videos along with the corre-"
        },
        {
          "our work explores weakly-supervised multi-task": ""
        },
        {
          "our work explores weakly-supervised multi-task": "sponding transcripts and audios.\nIn each video,"
        },
        {
          "our work explores weakly-supervised multi-task": ""
        },
        {
          "our work explores weakly-supervised multi-task": "two professional actors conduct dyadic dialogues in"
        },
        {
          "our work explores weakly-supervised multi-task": ""
        },
        {
          "our work explores weakly-supervised multi-task": "English. Although the human annotation has nine"
        },
        {
          "our work explores weakly-supervised multi-task": ""
        },
        {
          "our work explores weakly-supervised multi-task": "emotion categories, following the prior works (Haz-"
        },
        {
          "our work explores weakly-supervised multi-task": ""
        },
        {
          "our work explores weakly-supervised multi-task": "arika et al., 2018; Wang et al., 2019a; Tsai et al.,"
        },
        {
          "our work explores weakly-supervised multi-task": ""
        },
        {
          "our work explores weakly-supervised multi-task": "2019; Dai et al., 2020a), we take four categories:"
        },
        {
          "our work explores weakly-supervised multi-task": ""
        },
        {
          "our work explores weakly-supervised multi-task": "neutral, happy, sad, and angry."
        },
        {
          "our work explores weakly-supervised multi-task": ""
        },
        {
          "our work explores weakly-supervised multi-task": ""
        },
        {
          "our work explores weakly-supervised multi-task": "CMU-MOSEI.\nThe CMU Multimodal Opin-"
        },
        {
          "our work explores weakly-supervised multi-task": ""
        },
        {
          "our work explores weakly-supervised multi-task": "ion\nSentiment\nand\nEmotion\nIntensity\n(CMU-"
        },
        {
          "our work explores weakly-supervised multi-task": ""
        },
        {
          "our work explores weakly-supervised multi-task": "MOSEI) (Zadeh et al., 2018b) is a dataset for both"
        },
        {
          "our work explores weakly-supervised multi-task": ""
        },
        {
          "our work explores weakly-supervised multi-task": "multimodal emotion recognition and sentiment anal-"
        },
        {
          "our work explores weakly-supervised multi-task": ""
        },
        {
          "our work explores weakly-supervised multi-task": "ysis.\nIt comprises 3,837 videos from 1,000 diverse"
        },
        {
          "our work explores weakly-supervised multi-task": ""
        },
        {
          "our work explores weakly-supervised multi-task": "speakers and annotated with six emotion categories:"
        },
        {
          "our work explores weakly-supervised multi-task": "happy, sad, angry, fearful, disgusted, and surprised."
        },
        {
          "our work explores weakly-supervised multi-task": ""
        },
        {
          "our work explores weakly-supervised multi-task": "In addition, each data sample is also annotated with"
        },
        {
          "our work explores weakly-supervised multi-task": ""
        },
        {
          "our work explores weakly-supervised multi-task": "a sentiment score on a Likert scale [-3, 3]."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "et al., 2020b),\nsummarization (Yu et al., 2020),",
          "Label\nTrain\nValid\nTest": ""
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "",
          "Label\nTrain\nValid\nTest": "IEMOCAP"
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "question answering (McCann et al., 2018), etc.",
          "Label\nTrain\nValid\nTest": ""
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "",
          "Label\nTrain\nValid\nTest": "Neutral\n954\n358\n383"
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "For multimodal\naÔ¨Äect\nrecognition tasks,\nnot",
          "Label\nTrain\nValid\nTest": ""
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "",
          "Label\nTrain\nValid\nTest": "Happy\n338\n116\n135"
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "much work has been carried out\nto incorporate",
          "Label\nTrain\nValid\nTest": ""
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "",
          "Label\nTrain\nValid\nTest": "Sad\n690\n188\n193"
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "multi-tasking. Of the few works that have consid-",
          "Label\nTrain\nValid\nTest": ""
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "",
          "Label\nTrain\nValid\nTest": "Angry\n735\n136\n227"
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "ered multi-tasking, Akhtar et al. (2019) proposed",
          "Label\nTrain\nValid\nTest": ""
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "",
          "Label\nTrain\nValid\nTest": "CMU-MOSEI"
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "to tackle sentiment analysis and emotion recogni-",
          "Label\nTrain\nValid\nTest": ""
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "tion jointly. However, their method is only veriÔ¨Åed",
          "Label\nTrain\nValid\nTest": "Angry\n3443\n427\n971"
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "on one dataset, as it requires the dataset\nto have",
          "Label\nTrain\nValid\nTest": "Disgusted\n2720\n352\n922"
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "multiple human annotation at the same time. Also,",
          "Label\nTrain\nValid\nTest": "Fear\n1319\n186\n332"
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "",
          "Label\nTrain\nValid\nTest": "Happy\n8147\n1313\n2522"
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "Chauhan et al. (2020) studied the relationship be-",
          "Label\nTrain\nValid\nTest": ""
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "",
          "Label\nTrain\nValid\nTest": "Sad\n3906\n576\n1334"
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "tween sentiment, emotion, and sarcasm by manually",
          "Label\nTrain\nValid\nTest": ""
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "",
          "Label\nTrain\nValid\nTest": "Surprised\n1562\n201\n479"
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "annotating a sarcasm dataset (Castro et al., 2019),",
          "Label\nTrain\nValid\nTest": ""
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "",
          "Label\nTrain\nValid\nTest": "MUStARD"
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "which we believe does not scale.\nIn addition, they",
          "Label\nTrain\nValid\nTest": ""
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "only labeled a few hundred of\nsamples, which",
          "Label\nTrain\nValid\nTest": ""
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "",
          "Label\nTrain\nValid\nTest": "Sarcastic\n276\n-\n69"
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "limits the persuasion of their results. DiÔ¨Äerently,",
          "Label\nTrain\nValid\nTest": "Non-sarcastic\n276\n-\n69"
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "our work explores weakly-supervised multi-task",
          "Label\nTrain\nValid\nTest": ""
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "learning which is much cheaper and more scalable.",
          "Label\nTrain\nValid\nTest": "Table 1: Per-class data statistics of three datasets."
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "Moreover, our experiments are done in a wider",
          "Label\nTrain\nValid\nTest": ""
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "range with three diÔ¨Äerent datsets.",
          "Label\nTrain\nValid\nTest": ""
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "",
          "Label\nTrain\nValid\nTest": "MUStARD.\nThe Multimodal Sarcasm Detection"
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "3\nData and Evaluation Metrics",
          "Label\nTrain\nValid\nTest": "Dataset (MUStARD) (Castro et al., 2019) is a mul-"
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "",
          "Label\nTrain\nValid\nTest": "timodal video corpus for sarcasm recognition. The"
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "In this section, we Ô¨Årst introduce three datasets used",
          "Label\nTrain\nValid\nTest": ""
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "",
          "Label\nTrain\nValid\nTest": "dataset has 690 samples with an even number of"
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "for model benchmarking and weakly-supervised",
          "Label\nTrain\nValid\nTest": ""
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "",
          "Label\nTrain\nValid\nTest": "sarcastic and non-sarcastic labels."
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "multi-task learning. Then, we discuss the feature",
          "Label\nTrain\nValid\nTest": ""
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "",
          "Label\nTrain\nValid\nTest": "We show the data statistics of three datasets in"
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "extraction algorithms to pre-process the data. Fi-",
          "Label\nTrain\nValid\nTest": ""
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "",
          "Label\nTrain\nValid\nTest": "Table 1."
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "nally, we illustrate the metrics we use to evaluate",
          "Label\nTrain\nValid\nTest": ""
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "models.",
          "Label\nTrain\nValid\nTest": ""
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "",
          "Label\nTrain\nValid\nTest": "3.2\nData Feature Extraction"
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "3.1\nDatasets",
          "Label\nTrain\nValid\nTest": ""
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "",
          "Label\nTrain\nValid\nTest": "Feature extraction is done for each modality to ex-"
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "IEMOCAP.\nThe Interactive Emotional Dyadic",
          "Label\nTrain\nValid\nTest": ""
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "",
          "Label\nTrain\nValid\nTest": "tract high-level features before training.\nFor the"
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "Motion Capture (IEMOCAP) (Busso et al., 2008)",
          "Label\nTrain\nValid\nTest": ""
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "",
          "Label\nTrain\nValid\nTest": "textual modality, we use the pre-trained GloVe (Pen-"
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "is a dataset\nfor multimodal emotion recognition,",
          "Label\nTrain\nValid\nTest": ""
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "",
          "Label\nTrain\nValid\nTest": "nington et al., 2014) embeddings to represent words"
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "which contains 151 videos along with the corre-",
          "Label\nTrain\nValid\nTest": ""
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "",
          "Label\nTrain\nValid\nTest": "(glove.840B.300d1). For the acoustic modality, CO-"
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "sponding transcripts and audios.\nIn each video,",
          "Label\nTrain\nValid\nTest": ""
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "",
          "Label\nTrain\nValid\nTest": "VAREP2 (Degottex et al., 2014) is used to extract"
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "two professional actors conduct dyadic dialogues in",
          "Label\nTrain\nValid\nTest": ""
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "",
          "Label\nTrain\nValid\nTest": "features of dimension 74 from the raw audio data."
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "English. Although the human annotation has nine",
          "Label\nTrain\nValid\nTest": ""
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "",
          "Label\nTrain\nValid\nTest": "The features include fundamental frequency (F0),"
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "emotion categories, following the prior works (Haz-",
          "Label\nTrain\nValid\nTest": ""
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "",
          "Label\nTrain\nValid\nTest": "Voice/Unvoiced feature (VUV), quasi open quotient"
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "arika et al., 2018; Wang et al., 2019a; Tsai et al.,",
          "Label\nTrain\nValid\nTest": ""
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "",
          "Label\nTrain\nValid\nTest": "(QOQ), normalized amplitude quotient (NAQ), glot-"
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "2019; Dai et al., 2020a), we take four categories:",
          "Label\nTrain\nValid\nTest": ""
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "",
          "Label\nTrain\nValid\nTest": "tal source parameters (H1H2, Rd, Rd conf), max-"
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "neutral, happy, sad, and angry.",
          "Label\nTrain\nValid\nTest": ""
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "",
          "Label\nTrain\nValid\nTest": "ima dispersion quotient (MDQ), parabolic spectral"
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "",
          "Label\nTrain\nValid\nTest": "parameter\n(PSP),\ntilt/slope of wavelet\nresponse"
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "CMU-MOSEI.\nThe CMU Multimodal Opin-",
          "Label\nTrain\nValid\nTest": ""
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "",
          "Label\nTrain\nValid\nTest": "(peak/slope), harmonic model and phase distortion"
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "ion\nSentiment\nand\nEmotion\nIntensity\n(CMU-",
          "Label\nTrain\nValid\nTest": ""
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "",
          "Label\nTrain\nValid\nTest": "mean (HMPDM and HMPDD), and Mel Cepstral"
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "MOSEI) (Zadeh et al., 2018b) is a dataset for both",
          "Label\nTrain\nValid\nTest": ""
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "",
          "Label\nTrain\nValid\nTest": "CoeÔ¨Écients (MCEP). For the visual modality, 35"
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "multimodal emotion recognition and sentiment anal-",
          "Label\nTrain\nValid\nTest": ""
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "",
          "Label\nTrain\nValid\nTest": "facial action units\n(Ekman et al., 1980) are ex-"
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "ysis.\nIt comprises 3,837 videos from 1,000 diverse",
          "Label\nTrain\nValid\nTest": ""
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "",
          "Label\nTrain\nValid\nTest": "tracted from each frame of the video with OpenFace"
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "speakers and annotated with six emotion categories:",
          "Label\nTrain\nValid\nTest": ""
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "happy, sad, angry, fearful, disgusted, and surprised.",
          "Label\nTrain\nValid\nTest": ""
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "",
          "Label\nTrain\nValid\nTest": "1http://nlp.stanford.edu/data/glove.840B."
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "In addition, each data sample is also annotated with",
          "Label\nTrain\nValid\nTest": ""
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "",
          "Label\nTrain\nValid\nTest": "300d.zip"
        },
        {
          "text detection (Abu Farha and Magdy, 2020; Dai": "a sentiment score on a Likert scale [-3, 3].",
          "Label\nTrain\nValid\nTest": "2https://github.com/covarep/covarep"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "ous works (Tsai et al., 2018; Zadeh et al., 2018b),",
          "4\nModel Benchmarking": ""
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "",
          "4\nModel Benchmarking": "As mentioned in Section 1, we conjecture that the"
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "word-level alignment\nis done with P2FA (Yuan",
          "4\nModel Benchmarking": ""
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "",
          "4\nModel Benchmarking": "data sacacity issue of multimodal aÔ¨Äect recognition"
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "and Liberman, 2008) to achieve the same sequence",
          "4\nModel Benchmarking": ""
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "",
          "4\nModel Benchmarking": "tasks causes two problems: 1) models tend to be"
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "length for each modality. We reduce multiple fea-",
          "4\nModel Benchmarking": ""
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "",
          "4\nModel Benchmarking": "overÔ¨Åtted; and 2) instability of performance. To ver-"
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "ture segments within one aligned word into a single",
          "4\nModel Benchmarking": ""
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "",
          "4\nModel Benchmarking": "ify the severity of them, we benchmark 12 diÔ¨Äerent"
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "segment by taking the mean value over\nthe seg-",
          "4\nModel Benchmarking": ""
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "",
          "4\nModel Benchmarking": "models on two commonly used multimodal emo-"
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "ments.",
          "4\nModel Benchmarking": ""
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "",
          "4\nModel Benchmarking": "tion recognition datasets:\nIEMOCAP (Busso et al.,"
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "",
          "4\nModel Benchmarking": "2008) and CMU-MOSEI (Zadeh et al., 2018b). The"
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "3.3\nEvaluation Metrics",
          "4\nModel Benchmarking": ""
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "",
          "4\nModel Benchmarking": "models include six baselines,\nthree recently pro-"
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "Overall, we use three metrics for diÔ¨Äerent datasets.",
          "4\nModel Benchmarking": ""
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "",
          "4\nModel Benchmarking": "posed SOTA models, and three advanced baselines"
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "For the emotion recognition task, as we evaluate",
          "4\nModel Benchmarking": ""
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "",
          "4\nModel Benchmarking": "with the hybrid fusion method. All the models are"
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "on each emotion category,\nthere are much more",
          "4\nModel Benchmarking": ""
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "",
          "4\nModel Benchmarking": "listed in the Ô¨Årst column of Table 3. The implemen-"
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "negative data that positive data, we use the weighted",
          "4\nModel Benchmarking": ""
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "",
          "4\nModel Benchmarking": "tation details are discussed in Section 6."
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "Accuracy (WAcc) (Tong et al., 2017) to mitigate",
          "4\nModel Benchmarking": ""
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "the class imbalance issue. The formula of WAcc is",
          "4\nModel Benchmarking": "4.1\nBaselines"
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "",
          "4\nModel Benchmarking": "For\nthe baselines, we use two commonly used"
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "ùëá ùëÉ √ó ùëÅ/ùëÉ + ùëá ùëÅ",
          "4\nModel Benchmarking": ""
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "WAcc =",
          "4\nModel Benchmarking": "model-agnostic fusion methods (Figure 2): 1) early-"
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "2ùëÅ",
          "4\nModel Benchmarking": ""
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "",
          "4\nModel Benchmarking": "fusion (EF), which can capture low-level cross-"
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "where ùëÅ means total negative, ùëá ùëÅ true negative,",
          "4\nModel Benchmarking": "modal interactions; and 2) late-fusion (LF), which"
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "ùëÉ total positive, and ùëá ùëÉ true positive. For senti-",
          "4\nModel Benchmarking": "can capture high-level ones. They are used to build"
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "ment analysis and sarcasm recognition, we just use",
          "4\nModel Benchmarking": "simple baselines. Besides, based on EF and LF, we"
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "normal accuracy as the data is more balanced.\nIn",
          "4\nModel Benchmarking": "propose to combine them as a hybrid fusion method"
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "addition, we also use the F1-score for all the tasks.",
          "4\nModel Benchmarking": "(EF-LF) to construct strong baselines."
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "",
          "4\nModel Benchmarking": "Within each fusion method, we apply three dif-"
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "However, we do not use the binary weighted",
          "4\nModel Benchmarking": ""
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "",
          "4\nModel Benchmarking": "ferent model architectures, Average of\nfeatures"
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "F1-score (WF1) as some previous works (Zadeh",
          "4\nModel Benchmarking": ""
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "",
          "4\nModel Benchmarking": "(AVG), bi-directional Long-Short Term Memory"
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "et al., 2018a,b; Tsai et al., 2019; Rahman et al.,",
          "4\nModel Benchmarking": ""
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "",
          "4\nModel Benchmarking": "(LSTM) (Hochreiter and Schmidhuber, 1997), and"
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "2020) did. The formula of WF1 is shown below,",
          "4\nModel Benchmarking": ""
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "",
          "4\nModel Benchmarking": "Transformer (Vaswani et al., 2017), to process se-"
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "",
          "4\nModel Benchmarking": "quences of feature vectors. As the simplest baseline,"
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "ùëÉ ùêº\nùëÅ ùêº\nWF1 =\n√ó F1 ùëù +\n√ó F1ùëõ",
          "4\nModel Benchmarking": ""
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "",
          "4\nModel Benchmarking": "the AVG model\njust\ntakes the mean of the input"
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "",
          "4\nModel Benchmarking": "vectors as the output vector. For LSTM, we take the"
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "in which F1 ùëù is the F1 score that treats positive sam-",
          "4\nModel Benchmarking": ""
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "",
          "4\nModel Benchmarking": "output vector at the last time step as the representa-"
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "ples as positive, while F1ùëõ treats negative samples",
          "4\nModel Benchmarking": ""
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "",
          "4\nModel Benchmarking": "tion of the whole input sequence. For Transformer,"
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "as positive, and they are weighted by their portion",
          "4\nModel Benchmarking": ""
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "",
          "4\nModel Benchmarking": "following the practice of previous work (Vaswani"
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "of the data (ùêº is the total number of samples). We",
          "4\nModel Benchmarking": ""
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "",
          "4\nModel Benchmarking": "et al., 2017; Devlin et al., 2019; Liu et al., 2019),"
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "think that WF1 makes the class imbalance issue",
          "4\nModel Benchmarking": ""
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "",
          "4\nModel Benchmarking": "we prepend a [CLS] token to the input sequence"
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "even severer, as F1 ùëù only contributes a small por-",
          "4\nModel Benchmarking": ""
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "",
          "4\nModel Benchmarking": "and use the output embedding at that position."
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "tion of the total WF1. According to our experiments",
          "4\nModel Benchmarking": ""
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "on IMEOCAP, when using WF1 to evaluate mod-",
          "4\nModel Benchmarking": "4.2\nSOTA Models"
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "els, by increasing the threshold of classiÔ¨Åcation,",
          "4\nModel Benchmarking": ""
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "",
          "4\nModel Benchmarking": "Apart from the baselines, we also select\nthree re-"
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "the WF1 increases a lot as well. Even when the",
          "4\nModel Benchmarking": ""
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "",
          "4\nModel Benchmarking": "cently proposed state-of-the-art models on this"
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "threshold is 0.9 (means most of the samples will",
          "4\nModel Benchmarking": ""
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "",
          "4\nModel Benchmarking": "task for comparison: 1) Memory Fusion Network"
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "be classiÔ¨Åed as negative), the average of WF1 can",
          "4\nModel Benchmarking": ""
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "",
          "4\nModel Benchmarking": "(MFN) (Zadeh et al., 2018a), which has a multi-view"
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "still be higher than 0.7 while the WAcc is already",
          "4\nModel Benchmarking": ""
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "",
          "4\nModel Benchmarking": "gated memory for storing cross-modal interactions"
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "below 0.5. A similar phenomenon is also observed",
          "4\nModel Benchmarking": ""
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "",
          "4\nModel Benchmarking": "over time; 2) Multimodal Transformer (MulT) (Tsai"
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "by Dai et al. (2020a). Therefore, we just use the",
          "4\nModel Benchmarking": ""
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "",
          "4\nModel Benchmarking": "et al., 2019), which fuses each pair of modalities"
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "normal unweighted F1-score, which we think can",
          "4\nModel Benchmarking": ""
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "",
          "4\nModel Benchmarking": "with self-attention and can handle unaligned se-"
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "better reÔ¨Çect the model‚Äôs performance.",
          "4\nModel Benchmarking": ""
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "",
          "4\nModel Benchmarking": "quences; and 3) the model proposed by Dai et al."
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "",
          "4\nModel Benchmarking": "(2020a), which leverages the information inside"
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "3https://github.com/TadasBaltrusaitis/",
          "4\nModel Benchmarking": ""
        },
        {
          "2.03\n(Baltrusaitis et al., 2018).\nFollowing previ-": "OpenFace",
          "4\nModel Benchmarking": "emotion embeddings (EMO-EMB) and can hanlde"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: For sentiment",
      "data": [
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": ""
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": "(cid:88)\n(cid:88)\nIEMOCAP\nMUStARD\nMOSEI\n-\n-"
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": "(cid:88)\nMOSEI\nMOSEI\nMUStARD\n-\n-\n-"
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": ""
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": ""
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": ""
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": "Table 2: Emo, Sen, and Sar are abbreviations of Emo-"
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": "tion, Sentiment, and Sarcasm, respectively.\nIn the sec-"
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": "ond column, (cid:88)denotes the existence of human anno-"
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": ""
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": "tated strong labels, and - denotes the absence.\nIn the"
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": ""
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": "third column, we show the existence of model generated"
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": "weak labels for each dataset, and also which dataset the"
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": ""
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": "model is trained on."
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": ""
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": ""
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": "ùë£ùëñ ‚àà Rùë†√óùëëùë£ denotes an aligned sequence of facial"
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": "action units (FAUs) extracted from the video frames,"
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": "and ùë¶ùëñ ‚àà Rùëëùë¶ denotes the golden label. Each modal-"
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": "ity has the same sequence ùë†. For diÔ¨Äerent datasets,"
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": "the inputs ùë°, ùëé, ùë£ have the same dimension as we"
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": "use the same feature extraction across all datasets,"
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": ""
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": "only the dimension of labels ùëëùë¶ might be diÔ¨Äerent,"
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": "depending on how many classes the dataset has."
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": "5.2\nMulti-task Learning (MTL) for"
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": ""
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": "Multimodal AÔ¨Äect Recognition"
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": "As mentioned in Section 1, datasets for multimodal"
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": ""
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": "aÔ¨Äect\nrecognition are relatively small\nin nature,"
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": "which causes two problems. To mitigate them, we"
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": ""
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": ""
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": "propose to utilize MTL by two reasons.\nFirstly,"
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": "MTL can potentially improve the generalization"
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": ""
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": "performance when tasks are relevant (Zhang and"
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": ""
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": "Yang, 2017).\nSecondly,\nthere are many existing"
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": ""
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": "datasets related to multimodal aÔ¨Äect recognition,"
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": ""
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": "even though each can be small, and their data come"
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": ""
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": "in a similar\nformat\n(text, audio, video) and can"
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": ""
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": "be pre-processed in the same way.\nSpeciÔ¨Åcally,"
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": ""
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": "we leverage three relevant multimodal\ntasks:\n1)"
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": "emotion recognition; 2) sentiment analysis; and 3)"
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": "sarcasm recognition."
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": ""
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": "Weak Label Acquisition.\nGiven two separate"
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": ""
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": "datasets ùê∑1 and ùê∑2 on two diÔ¨Äerent tasks ùëá1 and"
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": ""
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": "in two\nùëá2, we generate weak labels of ùëá2\nfor ùê∑1"
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": "steps: 1) Ô¨Årst train a model on the data of ùê∑2; and"
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": "2) use the trained model\nto infer predictions on"
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": "the data of ùê∑1, and the predictions are treated as"
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": "in this paper, we\nweak labels of ùëá2. SpeciÔ¨Åcally,"
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": ""
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": "train a dedicated EF-LF-LSTM model to generate"
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": "weak labels on each of the three tasks following"
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": ""
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": ""
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": "kinds of labels each dataset has after weak label"
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": "acquisition are shown in Table 2.\nFor sentiment"
        },
        {
          "(cid:88)\nMOSEI\nMUStARD\nIEMOCAP\n-\n-\n-": "analysis,\nfollowin previous papers (Zadeh et al.,"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 3: Performance evaluation of 12 models (six simple baselines, three strong baselines, and three SOTA",
      "data": [
        {
          "Neutral": "",
          "Happy": "",
          "Sad": "",
          "Angry": "",
          "Average": ""
        },
        {
          "Neutral": "WAcc.\n(%)",
          "Happy": "WAcc.\n(%)",
          "Sad": "WAcc.\n(%)",
          "Angry": "WAcc.\n(%)",
          "Average": "WAcc.\n(%)"
        },
        {
          "Neutral": "",
          "Happy": "",
          "Sad": "Baselines",
          "Angry": "",
          "Average": ""
        },
        {
          "Neutral": "64.9 ¬± 0.8",
          "Happy": "73.0 ¬± 3.2",
          "Sad": "71.4 ¬± 2.4",
          "Angry": "41.7 ¬± 3.7",
          "Average": "67.6 ¬± 4.7"
        },
        {
          "Neutral": "65.7 ¬± 4.6",
          "Happy": "74.0 ¬± 1.8",
          "Sad": "71.9 ¬± 2.4",
          "Angry": "40.9 ¬± 4.7",
          "Average": "70.4 ¬± 1.9"
        },
        {
          "Neutral": "62.6 ¬± 4.0",
          "Happy": "72.9 ¬± 2.3",
          "Sad": "70.0 ¬± 2.2",
          "Angry": "39.5 ¬± 2.7",
          "Average": "64.3 ¬± 3.1"
        },
        {
          "Neutral": "67.1 ¬± 0.5",
          "Happy": "75.2 ¬± 1.0",
          "Sad": "72.6 ¬± 0.4",
          "Angry": "42.2 ¬± 1.3",
          "Average": "68.0 ¬± 0.8"
        },
        {
          "Neutral": "66.5 ¬± 1.5",
          "Happy": "75.5 ¬± 0.8",
          "Sad": "72.1 ¬± 0.7",
          "Angry": "44.2 ¬± 1.5",
          "Average": "67.4 ¬± 1.3"
        },
        {
          "Neutral": "67.2 ¬± 0.5",
          "Happy": "73.4 ¬± 1.6",
          "Sad": "71.2 ¬± 1.3",
          "Angry": "39.5 ¬± 2.8",
          "Average": "65.0 ¬± 3.0"
        },
        {
          "Neutral": "",
          "Happy": "",
          "Sad": "(Hybrid",
          "Angry": "",
          "Average": ""
        },
        {
          "Neutral": "66.3 ¬± 1.1",
          "Happy": "78.8 ¬± 0.8",
          "Sad": "74.0 ¬± 0.3",
          "Angry": "45.5 ¬± 1.6",
          "Average": "67.8 ¬± 0.5"
        },
        {
          "Neutral": "68.8 ¬± 0.9",
          "Happy": "76.2 ¬± 2.0",
          "Sad": "74.4 ¬± 0.4",
          "Angry": "43.5 ¬± 1.9",
          "Average": "72.4 ¬± 2.1"
        },
        {
          "Neutral": "68.4 ¬± 0.6",
          "Happy": "79.0 ¬± 1.4",
          "Sad": "74.2 ¬± 0.8",
          "Angry": "41.2 ¬± 1.9",
          "Average": "68.8 ¬± 2.2"
        },
        {
          "Neutral": "",
          "Happy": "",
          "Sad": "Models",
          "Angry": "",
          "Average": ""
        },
        {
          "Neutral": "65.4 ¬± 0.4",
          "Happy": "75.5 ¬± 2.6",
          "Sad": "72.3 ¬± 0.8",
          "Angry": "43.4 ¬± 2.1",
          "Average": "70.1 ¬± 2.1"
        },
        {
          "Neutral": "68.3 ¬± 1.5",
          "Happy": "74.1 ¬± 1.5",
          "Sad": "72.6 ¬± 0.4",
          "Angry": "44.9 ¬± 1.3",
          "Average": "69.2 ¬± 2.0"
        },
        {
          "Neutral": "65.7 ¬± 1.3",
          "Happy": "78.1 ¬± 2.5",
          "Sad": "73.8 ¬± 0.5",
          "Angry": "44.2 ¬± 2.9",
          "Average": "71.4 ¬± 2.3"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 3: Performance evaluation of 12 models (six simple baselines, three strong baselines, and three SOTA",
      "data": [
        {
          "SOTA": "MULT\n65.4 ¬± 0.4\n66.7 ¬± 1.7\n75.5 ¬± 2.6\n81.6 ¬± 1.4",
          "Models": "72.3 ¬± 0.8\n60.2 ¬± 0.3\n43.4 ¬± 2.1\n53.1 ¬± 2.9\n70.1 ¬± 2.1\n56.7 ¬± 1.0"
        },
        {
          "SOTA": "MFN\n68.3 ¬± 1.5\n67.7 ¬± 1.0\n74.1 ¬± 1.5\n80.3 ¬± 0.7",
          "Models": "65.1 ¬± 1.4\n72.6 ¬± 0.4\n44.9 ¬± 1.3\n55.4 ¬± 1.6\n69.2 ¬± 2.0\n58.7 ¬± 1.1"
        },
        {
          "SOTA": "EMO-EMB\n65.7 ¬± 1.3\n68.8 ¬± 1.2\n78.1 ¬± 2.5\n82.8 ¬± 1.6",
          "Models": "61.5 ¬± 3.2\n59.7 ¬± 1.6\n73.8 ¬± 0.5\n61.8 ¬± 1.7\n44.2 ¬± 2.9\n71.4 ¬± 2.3"
        },
        {
          "SOTA": "Table 3:\nPerformance evaluation of 12 models (six simple baselines,",
          "Models": "three strong baselines, and three SOTA"
        },
        {
          "SOTA": "models) on the IEMOCAP (Busso et al., 2008) dataset. We report the weighted accuracy (WAcc) and the F1-score",
          "Models": ""
        },
        {
          "SOTA": "on four emotion categories:\nneutral, happy,\nsad, and angry.",
          "Models": "In addition, we also report\nthe average of\nthem"
        },
        {
          "SOTA": "as an overall measurement.",
          "Models": "For each model, we run Ô¨Åve random seeds {0, 1, 2, 3, 4} and report\nthe mean ¬±"
        },
        {
          "SOTA": "standard_deviation. The best performance is decorated in bold.",
          "Models": ""
        },
        {
          "SOTA": "2018b; Liu et al., 2018; Rahman et al., 2020), we",
          "Models": "For datasets\nthat contain multiple labels, we"
        },
        {
          "SOTA": "simplify it\nto a two-class classiÔ¨Åcation problem",
          "Models": "can directly apply this MTL procedure, while for"
        },
        {
          "SOTA": "(either positive or negative) and generate binary",
          "Models": "unlabeled tasks, we perform MTL in a weakly-"
        },
        {
          "SOTA": "labels. For the other tasks, we follow the original",
          "Models": "supervised way. For example, we can train a model"
        },
        {
          "SOTA": "categories of the datasets.\nIn addition, we also store",
          "Models": "on sentiment analysis and use it to predict sentiment"
        },
        {
          "SOTA": "the accuracy of the EF-LF-LSTM model on each",
          "Models": "scores for a sarcasm recognition dataset.\nThen,"
        },
        {
          "SOTA": "dataset as a conÔ¨Ådence score of the generated weak",
          "Models": "jointly use predicted weak labels with the original"
        },
        {
          "SOTA": "labels.",
          "Models": "labels of the dataset for MTL."
        },
        {
          "SOTA": "Weakly-supervised MTL.\nAfter\ngetting\nthe",
          "Models": ""
        },
        {
          "SOTA": "",
          "Models": "6\nExperiments and Analysis"
        },
        {
          "SOTA": "weak labels, we can conduct weakly-supervised",
          "Models": ""
        },
        {
          "SOTA": "learning for diÔ¨Äerent tasks. The training procedure",
          "Models": "6.1\nExperimental Settings"
        },
        {
          "SOTA": "is shown in Eq.1 and 2:",
          "Models": ""
        },
        {
          "SOTA": "",
          "Models": "To ensure we make a fair comparison of the models,"
        },
        {
          "SOTA": "",
          "Models": "we perform an elaborate hyper-parameter search"
        },
        {
          "SOTA": "(1)\nùëüùëñ = ùëìùëäùëè (ùë°ùëñ, ùëéùëñ, ùë£ùëñ)",
          "Models": ""
        },
        {
          "SOTA": "",
          "Models": "with the following strategies.\nFirstly,\nfor each"
        },
        {
          "SOTA": "",
          "Models": "model, we try the combinations of four learning"
        },
        {
          "SOTA": "ùêº‚àëÔ∏Å ùëñ\nùêΩ‚àëÔ∏Å ùëó\nùúÜ ùëó\nùêø =\nmin\n(2)\n¬∑ ùêø ùëó ( ùê¥ùëäùëó (ùëüùëñ), ùë¶ùëñ)",
          "Models": ""
        },
        {
          "SOTA": "ùëäùëè ,ùëä1,...,ùëäùêΩ",
          "Models": "rates {1e‚àí3, 5e‚àí4, 1e‚àí4, 5e‚àí5} and six batch sizes"
        },
        {
          "SOTA": "=1\n=1",
          "Models": ""
        },
        {
          "SOTA": "",
          "Models": "{16, 32, 64, 128, 256, 512},\nresulting in 24 ex-"
        },
        {
          "SOTA": "where ùëìùëäùëè is the backbone model with weights ùëäùëè",
          "Models": "periments. Among their hyper-parameter settings"
        },
        {
          "SOTA": "shared by all tasks, which generates a representation",
          "Models": "achiveing top-5 performance, we further try diÔ¨Äer-"
        },
        {
          "SOTA": "ùëüùëñ of the input data, and ùêΩ is the number of tasks.",
          "Models": "ent model-dependent hyper-parameters, such as the"
        },
        {
          "SOTA": "For each task ùëó,\nthere is a linear layer ùê¥ùëäùëó with",
          "Models": "hidden dimension, feed-forwad dimension, number"
        },
        {
          "SOTA": "parameters ùëä ùëó\nto perform aÔ¨Éne transformation",
          "Models": "of\nlayers, dropout values, etc.\nFor\nthe previous"
        },
        {
          "SOTA": "on ùëüùëñ\nto get\nthe desired output dimension.\nThe",
          "Models": "state-of-the-art models, we also conduct a simi-"
        },
        {
          "SOTA": "overall objective is to minimize the total\nloss ùêø",
          "Models": "lar hyper-parameter search based on their reported"
        },
        {
          "SOTA": "of all\ntasks, and each task has a loss function ùêø ùëó",
          "Models": "numbers in the paper. To test the stability of models"
        },
        {
          "SOTA": "with a weighting factor ùúÜ ùëó. This weighting factor",
          "Models": "and eliminate the possible contingency caused by"
        },
        {
          "SOTA": "can be either\nthe conÔ¨Ådence score mentioned in",
          "Models": "weights initialization, for the best setting of each"
        },
        {
          "SOTA": "the last paragraph, or a hyper-parameter searched",
          "Models": "model, we run Ô¨Åve diÔ¨Äerent random seeds {0, 1, 2,"
        },
        {
          "SOTA": "manually. For the strong labels of the main task,",
          "Models": "3, 4} and report the mean and standard deviation."
        },
        {
          "SOTA": "the conÔ¨Ådence score is 1.",
          "Models": "The best hyper-parameters are shown in Appendix"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Target Task": "",
          "Training Tasks": "",
          "EF-LF AVG": "Avg.WAcc",
          "EF-LF LSTM": "Avg.WAcc",
          "EF-LF TRANS": "Avg.F1"
        },
        {
          "Target Task": "",
          "Training Tasks": "Emotion",
          "EF-LF AVG": "67.8",
          "EF-LF LSTM": "72.4",
          "EF-LF TRANS": "58.5"
        },
        {
          "Target Task": "Emotion",
          "Training Tasks": "+ Sentiment(ùëä )",
          "EF-LF AVG": "69.1",
          "EF-LF LSTM": "73.3",
          "EF-LF TRANS": "59.3"
        },
        {
          "Target Task": "(IEMOCAP)",
          "Training Tasks": "+ Sarcasm(ùëä )",
          "EF-LF AVG": "68.6",
          "EF-LF LSTM": "72.8",
          "EF-LF TRANS": "58.6"
        },
        {
          "Target Task": "",
          "Training Tasks": "All",
          "EF-LF AVG": "68.7",
          "EF-LF LSTM": "73.4",
          "EF-LF TRANS": "59.3"
        },
        {
          "Target Task": "",
          "Training Tasks": "Emotion",
          "EF-LF AVG": "66.7",
          "EF-LF LSTM": "65.9",
          "EF-LF TRANS": "42.1"
        },
        {
          "Target Task": "Emotion",
          "Training Tasks": "+ Sentiment(ùëÜ)",
          "EF-LF AVG": "67.2",
          "EF-LF LSTM": "66.5",
          "EF-LF TRANS": "42.3"
        },
        {
          "Target Task": "(CMU-MOSEI)",
          "Training Tasks": "+ Sarcasm(ùëä )",
          "EF-LF AVG": "66.9",
          "EF-LF LSTM": "66.1",
          "EF-LF TRANS": "42.4"
        },
        {
          "Target Task": "",
          "Training Tasks": "All",
          "EF-LF AVG": "67.0",
          "EF-LF LSTM": "66.1",
          "EF-LF TRANS": "42.4"
        },
        {
          "Target Task": "",
          "Training Tasks": "",
          "EF-LF AVG": "Acc",
          "EF-LF LSTM": "Acc",
          "EF-LF TRANS": "F1"
        },
        {
          "Target Task": "",
          "Training Tasks": "Sarcasm",
          "EF-LF AVG": "63.8",
          "EF-LF LSTM": "68.1",
          "EF-LF TRANS": "63.9"
        },
        {
          "Target Task": "Sarcasm",
          "Training Tasks": "+ Emotion(ùëä )",
          "EF-LF AVG": "65.2",
          "EF-LF LSTM": "71.0",
          "EF-LF TRANS": "64.5"
        },
        {
          "Target Task": "(MUStARD)",
          "Training Tasks": "+ Sentiment(ùëä )",
          "EF-LF AVG": "65.2",
          "EF-LF LSTM": "71.0",
          "EF-LF TRANS": "65.8"
        },
        {
          "Target Task": "",
          "Training Tasks": "All",
          "EF-LF AVG": "65.9",
          "EF-LF LSTM": "69.6",
          "EF-LF TRANS": "63.9"
        },
        {
          "Target Task": "",
          "Training Tasks": "",
          "EF-LF AVG": "Acc",
          "EF-LF LSTM": "Acc",
          "EF-LF TRANS": "F1"
        },
        {
          "Target Task": "",
          "Training Tasks": "Sentiment1",
          "EF-LF AVG": "70.9",
          "EF-LF LSTM": "70.8",
          "EF-LF TRANS": "69.5"
        },
        {
          "Target Task": "",
          "Training Tasks": "",
          "EF-LF AVG": "",
          "EF-LF LSTM": "",
          "EF-LF TRANS": ""
        },
        {
          "Target Task": "",
          "Training Tasks": "+ Emotion(ùëÜ)",
          "EF-LF AVG": "71.6",
          "EF-LF LSTM": "71.5",
          "EF-LF TRANS": "71.7"
        },
        {
          "Target Task": "",
          "Training Tasks": "",
          "EF-LF AVG": "",
          "EF-LF LSTM": "",
          "EF-LF TRANS": ""
        },
        {
          "Target Task": "Sentiment",
          "Training Tasks": "+ Emotion(ùëä )",
          "EF-LF AVG": "71.7",
          "EF-LF LSTM": "71.8",
          "EF-LF TRANS": "70.9"
        },
        {
          "Target Task": "",
          "Training Tasks": "",
          "EF-LF AVG": "",
          "EF-LF LSTM": "",
          "EF-LF TRANS": ""
        },
        {
          "Target Task": "(CMU-MOSEI)",
          "Training Tasks": "+ Sarcasm(ùëä )",
          "EF-LF AVG": "71.4",
          "EF-LF LSTM": "71.1",
          "EF-LF TRANS": "70.9"
        },
        {
          "Target Task": "",
          "Training Tasks": "All(1+2+4)",
          "EF-LF AVG": "71.5",
          "EF-LF LSTM": "71.4",
          "EF-LF TRANS": "71.0"
        },
        {
          "Target Task": "",
          "Training Tasks": "All(1+3+4)",
          "EF-LF AVG": "71.8",
          "EF-LF LSTM": "71.9",
          "EF-LF TRANS": "71.0"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4": "(CMU-MOSEI)\n71.4\n69.8\n71.1\n69.4\n72.0\n70.9\n+ Sarcasm(ùëä )"
        },
        {
          "4": "70.8\n71.5\n70.1\n71.4\n71.8\n71.0\nAll(1+2+4)"
        },
        {
          "4": "71.8\n70.4\n71.9\n72.3\n70.6\n71.0\nAll(1+3+4)"
        },
        {
          "4": "Table 4: Experimental results of MTL. In the Ô¨Årst column, we show the main target\ntask and the corresponding"
        },
        {
          "4": "dataset.\nIn the second column, we show the tasks used in the training process. The symbol + means adding an"
        },
        {
          "4": "(ùëÜ)\nauxiliary task in the training, besides the main target\ntask. All means using all\nthe tasks above for training."
        },
        {
          "4": "(ùëä)\nor\nindicates whether this external supervision is strong or weak.\nFor emotion recognition, Avg. means the"
        },
        {
          "4": "average of all emotion categories.\nIn the bottom block, we also compare the eÔ¨Äectiveness of using strong and weak"
        },
        {
          "4": "label when conducting MTL."
        },
        {
          "4": "B. For the emotion recognition, we use the binary\nprevious works (Zadeh et al., 2018a; Tsai et al.,"
        },
        {
          "4": "cross-entropy loss as the data are multi-labeled (a\n2019) to better reÔ¨Çect the model performance. First"
        },
        {
          "4": "person can have multiple), with a loss weight for\nof all, we discover that SOTA models do not have"
        },
        {
          "4": "positive samples to alleviate the data imbalance\nan obvious advantage over the baselines on these"
        },
        {
          "4": "issue.\nFor the sentiment prediction and sarcasm\ntwo datasets. The hybrid modality fusion (EF+LF)"
        },
        {
          "4": "recognition, we use the cross-entropy loss.\nThe\nwith a simple architecture can surpass the SOTA"
        },
        {
          "4": "Adam optimizer (Kingma and Ba, 2015) is used for\nmodels. We conjecture that the data scarcity issue"
        },
        {
          "4": "makes complex architextures not able to show their\nall of our experiments with ùõΩ1 = 0.9, ùõΩ2 = 0.999"
        },
        {
          "4": "and a weight decay of 1e‚àí5. Our code is imple-\nfull capacity. For example, the Multimodal Trans-"
        },
        {
          "4": "mented in PyTorch (Paszke et al., 2019) and run on\nformer\n(MULT)\n(Tsai et al., 2019) use a hidden"
        },
        {
          "4": "a single NVIDIA 1080Ti GPU.\ndimension of only 40 and a dropout value of 0.3 to"
        },
        {
          "4": "achieve its best performance (i.e. avoid overÔ¨Åtting)."
        },
        {
          "4": "6.2\nBenchmarking Results Analysis\nSecondly, we Ô¨Ånd out that the performance is un-"
        },
        {
          "4": "stable given the small size of data. For example, by"
        },
        {
          "4": "To investigate the two problems aforementioned"
        },
        {
          "4": "altering a random seed,\nthe WAcc can change up"
        },
        {
          "4": "in Section 1 and have an overall understanding"
        },
        {
          "4": "to 4.7%. On average, we get a standard deviation"
        },
        {
          "4": "of\nthe performance of various models on multi-"
        },
        {
          "4": "of 1.8% for WAcc and 1.7% for F1-score on the"
        },
        {
          "4": "modal aÔ¨Äect recognition, we benchmark 12 models"
        },
        {
          "4": "IEMOCAP. Besides the dataset size, we speculate"
        },
        {
          "4": "(Section 4) on CMU-MOSEI and IEMOCAP. The"
        },
        {
          "4": "that another reason for the overÔ¨Åtting is that there"
        },
        {
          "4": "results on IEMOCAP are shown in Table 3, and"
        },
        {
          "4": "is a feature extraction step before training to get"
        },
        {
          "4": "the results on CMU-MOSEI are included in Ap-"
        },
        {
          "4": "the high-level\nfeatures from the raw data.\nThus"
        },
        {
          "4": "pendix A. As explained in Section 3.3, we use"
        },
        {
          "4": "the information in the resulting input\nfeatures is"
        },
        {
          "4": "slightly diÔ¨Äerent evaluation metrics compared to"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "59‚Äì66.",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": "1980. Facial signs of emotional experience. Journal"
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": "of personality and social psychology, 39(6):1125."
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "Tadas\nBaltru≈°aitis,\nChaitanya\nAhuja,\nand\nLouis-",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": ""
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "Philippe Morency. 2018. Multimodal machine learn-",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": "Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv"
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "IEEE\ntransac-\ning:\nA survey\nand\ntaxonomy.",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": "Batra, and Devi Parikh. 2017. Making the v in vqa"
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "tions on pattern analysis and machine intelligence,",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": "matter:\nElevating the role of\nimage understanding"
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "41(2):423‚Äì443.",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": "2017 IEEE Confer-\nin visual question answering."
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": "ence on Computer Vision and Pattern Recognition"
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "C. Busso, M. Bulut, Chi-Chun Lee, A. Kazemzadeh,",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": "(CVPR), pages 6325‚Äì6334."
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "E.\nProvost,\nS. Kim,\nJ. N. Chang,\nS. Lee,\nand",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": ""
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "Shrikanth S. Narayanan. 2008.\nIemocap:\ninterac-",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": "Devamanyu Hazarika, Soujanya Poria, Amir Zadeh,"
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "tive emotional dyadic motion capture database. Lan-",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": "E. Cambria, Louis-Philippe Morency,\nand Roger"
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "guage Resources and Evaluation, 42:335‚Äì359.",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": "Zimmermann. 2018.\nConversational memory net-"
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": "work\nfor\nemotion\nrecognition\nin\ndyadic\ndialogue"
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "Santiago Castro, Devamanyu Hazarika, Ver√≥nica P√©rez-",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": "Proceedings\nof\nthe\nconference. Associa-\nvideos."
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "Rosas, R. Zimmermann, R. Mihalcea,\nand Sou-",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": "tion for Computational Linguistics. North American"
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "janya Poria. 2019.\nTowards multimodal\nsarcasm",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": "Chapter. Meeting, 2018:2122‚Äì2132."
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "detection\n(an\nobviously\nperfect\npaper).\nArXiv,",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": ""
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "abs/1906.01815.",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": "Sepp Hochreiter and J√ºrgen Schmidhuber. 1997. Long"
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": "short-term memory. Neural computation, 9(8):1735‚Äì"
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "D. Chauhan, R. DhanushS., Asif Ekbal, and P. Bhat-",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": "1780."
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "tacharyya. 2020.\nSentiment and emotion help sar-",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": ""
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "casm?\na multi-task learning framework for multi-",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": "J.\nJohnson, B. Hariharan, Laurens van der Maaten,"
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "modal sarcasm, sentiment and emotion analysis.\nIn",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": "Li Fei-Fei, C. L. Zitnick, and Ross B. Girshick. 2017."
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "ACL.",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": "Clevr: A diagnostic dataset\nfor compositional\nlan-"
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": "guage and elementary visual reasoning. 2017 IEEE"
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "Wenliang\nDai,\nSamuel\nCahyawƒ≥aya,\nZihan\nLiu,",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": "Conference on Computer Vision and Pattern Recog-"
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "and Pascale Fung. 2021.\nMultimodal\nend-to-end",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": "nition (CVPR), pages 1988‚Äì1997."
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "sparse model\nfor\nemotion\nrecognition.\nArXiv,",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": ""
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "abs/2103.09666.",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": "Diederik P. Kingma\nand\nJimmy Ba.\n2015.\nAdam:"
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": "A method\nfor\nstochastic\noptimization.\nCoRR,"
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "Wenliang Dai, Zihan Liu, Tiezheng Yu, and Pascale",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": "abs/1412.6980."
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "Fung. 2020a. Modality-transferable emotion embed-",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": ""
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "dings for low-resource multimodal emotion recogni-",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": "I. Kokkinos. 2017. Ubernet: Training a universal con-"
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "the 1st Conference of\nthe\ntion.\nIn Proceedings of",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": "volutional neural network for\nlow-, mid-, and high-"
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "Asia-PaciÔ¨Åc Chapter of\nthe Association for Compu-",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": "level vision using diverse datasets and limited mem-"
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "tational Linguistics and the 10th International Joint",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": "ory. 2017 IEEE Conference on Computer Vision and"
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "Conference on Natural Language Processing, pages",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": "Pattern Recognition (CVPR), pages 5454‚Äì5463."
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "269‚Äì280, Suzhou, China. Association for Computa-",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": ""
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "tional Linguistics.",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": "Tsung-Yi Lin, M. Maire, Serge J. Belongie, James Hays,"
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": "P. Perona, D. Ramanan, Piotr Doll√°r, and C. L. Zit-"
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "Wenliang Dai, Tiezheng Yu, Zihan Liu, and Pascale",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": "nick. 2014a. Microsoft coco: Common objects in"
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "Fung. 2020b. Kungfupanda at SemEval-2020 task",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": "context.\nIn ECCV."
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "12: BERT-based multi-TaskLearning for oÔ¨Äensive",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": ""
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "the Four-\nlanguage detection.\nIn Proceedings of",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": "Tsung-Yi Lin, M. Maire, Serge J. Belongie, James Hays,"
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "teenth Workshop\non\nSemantic Evaluation,\npages",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": "P. Perona, D. Ramanan, Piotr Doll√°r, and C. L. Zit-"
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "2060‚Äì2066, Barcelona (online).\nInternational Com-",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": "nick. 2014b. Microsoft coco: Common objects in"
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "mittee for Computational Linguistics.",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": "context.\nIn ECCV."
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "G. Degottex, John Kane, Thomas Drugman, T. Raitio,",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": "Y. Liu, Myle Ott, Naman Goyal,\nJingfei Du, Mandar"
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "and Stefan Scherer. 2014. Covarep ‚Äî a collabora-",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": "Joshi, Danqi Chen, Omer Levy, M. Lewis, Luke"
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "tive voice analysis repository for speech technologies.",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": "Zettlemoyer, and Veselin Stoyanov. 2019. Roberta:"
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "2014 IEEE International Conference on Acoustics,",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": "A robustly\noptimized\nbert\npretraining\napproach."
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "Speech and Signal Processing (ICASSP), pages 960‚Äì",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": "ArXiv, abs/1907.11692."
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "964.",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": ""
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": "Zhun Liu, Ying Shen, V. Lakshminarasimhan, P. P."
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "Jia Deng, W. Dong, R. Socher, L. Li, K. Li, and Li Fei-",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": "Liang, Amir Zadeh,\nand Louis-Philippe Morency."
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "Fei. 2009.\nImagenet: A large-scale hierarchical\nim-",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": "2018.\nEÔ¨Écient\nlow-rank multimodal\nfusion with"
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "age database.\nIn CVPR 2009.",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": "modality-speciÔ¨Åc factors.\nIn ACL."
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "J. Devlin, Ming-Wei Chang, Kenton Lee, and Kristina",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": "Bryan McCann,\nN. Keskar,\nCaiming Xiong,\nand"
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "Toutanova. 2019. Bert: Pre-training of deep bidirec-",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": "R. Socher. 2018.\nThe natural\nlanguage decathlon:"
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "tional\ntransformers for\nlanguage understanding.\nIn",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": "Multitask learning as question answering.\nArXiv,"
        },
        {
          "tomatic Face Gesture Recognition (FG 2018), pages": "NAACL-HLT.",
          "Paul Ekman, Wallace V Freisen,\nand Sonia Ancoli.": "abs/1806.08730."
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "Doshi. 2011.\nTowards multimodal sentiment anal-",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": "2015. Show and tell: A neural image caption genera-"
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "ysis: Harvesting opinions from the web.\nIn Proceed-",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": "tor. 2015 IEEE Conference on Computer Vision and"
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "ings of\nthe 13th international conference on multi-",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": "Pattern Recognition (CVPR), pages 3156‚Äì3164."
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "modal interfaces, pages 169‚Äì176.",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": ""
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": "Yansen Wang, Ying Shen, Zhun Liu, P. P. Liang, Amir"
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "Adam Paszke, S. Gross, Francisco Massa, A. Lerer,",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": ""
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": "Zadeh, and Louis-Philippe Morency. 2019a. Words"
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "James Bradbury, Gregory Chanan, Trevor Killeen,",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": ""
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": "can shift: Dynamically adjusting word representa-"
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "Z. Lin, N. Gimelshein, L. Antiga, Alban Desmaison,",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": ""
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": "tions using nonverbal behaviors. Proceedings of the"
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "Andreas K√∂pf, Edward Yang, Zach DeVito, Mar-",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": ""
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": "... AAAI Conference on ArtiÔ¨Åcial Intelligence. AAAI"
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "tin Raison, Alykhan Tejani, Sasank Chilamkurthy,",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": ""
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": "Conference\non ArtiÔ¨Åcial\nIntelligence,\n33\n1:7216‚Äì"
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "B. Steiner, Lu Fang, Junjie Bai, and Soumith Chin-",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": ""
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": "7223."
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "tala.\n2019.\nPytorch:\nAn\nimperative\nstyle,\nhigh-",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": ""
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "performance deep learning library.\nIn NeurIPS.",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": ""
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": "Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang,"
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": "Amir Zadeh,\nand Louis-Philippe Morency. 2019b."
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "JeÔ¨Ärey Pennington, Richard Socher, and Christopher D",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": ""
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": "Words can shift: Dynamically adjusting word rep-"
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "Manning. 2014. Glove: Global vectors for word rep-",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": ""
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": "resentations using nonverbal behaviors.\nIn Proceed-"
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "the 2014 conference\nresentation.\nIn Proceedings of",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": ""
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": "ings of the AAAI Conference on ArtiÔ¨Åcial Intelligence,"
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "on empirical methods in natural language processing",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": ""
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": "volume 33, pages 7216‚Äì7223."
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "(EMNLP), pages 1532‚Äì1543.",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": ""
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "Wasifur Rahman, Md Kamrul Hasan,\nSangwu Lee,",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": ""
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": "D. Xu, Wanli Ouyang, Xiaogang Wang, and N. Sebe."
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "AmirAli Bagher Zadeh, Chengfeng Mao,\nLouis-",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": ""
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": "2018.\nPad-net: Multi-tasks guided prediction-and-"
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "Philippe Morency,\nand Ehsan Hoque. 2020.\nInte-",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": ""
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": "distillation network for simultaneous depth estima-"
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "grating multimodal\ninformation in large pretrained",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": ""
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": "tion and scene parsing. 2018 IEEE/CVF Conference"
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "the 58th Annual\ntransformers.\nIn Proceedings of",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": ""
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": "on Computer Vision and Pattern Recognition, pages"
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "Meeting of\nthe Association for Computational Lin-",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": ""
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": "675‚Äì684."
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "guistics, pages 2359‚Äì2369.",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": ""
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": "Tiezheng Yu, Dan Su, Wenliang Dai, and Pascale Fung."
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": ""
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": "2020. Dimsum @LaySumm 20.\nIn Proceedings of"
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "Percy Liang. 2016. Squad: 100, 000+ questions for",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": ""
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": "the First Workshop on Scholarly Document Process-"
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "machine comprehension of text.\nIn EMNLP.",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": ""
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": "ing, pages 303‚Äì309, Online. Association for Compu-"
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": "tational Linguistics."
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "Ramon\nSanabria, Ozan Caglayan,\nShruti\nPalaskar,",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": ""
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "Desmond Elliott, Lo√Øc Barrault, Lucia Specia, and",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": ""
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "Florian Metze. 2018. How2: A large-scale dataset",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": "Jiahong Yuan and M. Liberman. 2008. Speaker identiÔ¨Å-"
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "for multimodal\nlanguage\nunderstanding.\nArXiv,",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": "cation on the scotus corpus. Journal of the Acoustical"
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "abs/1811.00347.",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": "Society of America, 123:3878‚Äì3878."
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "O. Sidorov, Ronghang Hu, Marcus Rohrbach,\nand",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": ""
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": "Amir Zadeh, M. Chen, Soujanya Poria, E. Cambria, and"
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "Amanpreet Singh. 2020. Textcaps: a dataset for im-",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": ""
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": "Louis-Philippe Morency. 2017.\nTensor\nfusion net-"
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "age captioning with reading comprehension. ArXiv,",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": ""
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": "work for multimodal sentiment analysis.\nIn EMNLP."
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "abs/2003.12462.",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": ""
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": "Amir Zadeh, P. P. Liang, N. Mazumder, Soujanya Po-"
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "Edmund Tong, Amir Zadeh, Cara Jones,\nand Louis-",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": ""
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": "ria, E. Cambria, and Louis-Philippe Morency. 2018a."
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "Philippe Morency. 2017.\nCombating human traf-",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": ""
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": "Memory fusion network for multi-view sequential"
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "Ô¨Åcking with multimodal\ndeep models.\nArXiv,",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": ""
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": "learning. ArXiv, abs/1802.00927."
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "abs/1705.02735.",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": ""
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "Yao-Hung Hubert Tsai, Shaojie Bai, P. P. Liang, J. Z.",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": ""
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": "Amir Zadeh, Rowan Zellers, Eli Pincus,\nand Louis-"
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "Kolter, Louis-Philippe Morency, and R. Salakhutdi-",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": ""
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": "Philippe Morency. 2016.\nMosi:\nmultimodal\ncor-"
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "nov. 2019. Multimodal\ntransformer\nfor unaligned",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": ""
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": "pus\nof\nsentiment\nintensity\nand\nsubjectivity\nanal-"
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "multimodal language sequences. Proceedings of the",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": ""
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": "arXiv\npreprint\nysis\nin\nonline\nopinion\nvideos."
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "conference. Association for Computational Linguis-",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": ""
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": "arXiv:1606.06259."
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "tics. Meeting, 2019:6558‚Äì6569.",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": ""
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": "AmirAli Bagher Zadeh, Paul Pu Liang, Soujanya Po-"
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "Yao-Hung Hubert Tsai, Paul Pu Liang, Amir Zadeh,",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": ""
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": "ria,\nErik Cambria,\nand Louis-Philippe Morency."
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "Louis-Philippe Morency, and Ruslan Salakhutdinov.",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": ""
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": "2018b. Multimodal\nlanguage analysis in the wild:"
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "2018.\nLearning factorized multimodal\nrepresenta-",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": ""
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": "Cmu-mosei dataset and interpretable dynamic fusion"
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "tions. arXiv preprint arXiv:1806.06176.",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": ""
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": "graph.\nIn Proceedings of the 56th Annual Meeting of"
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": "the Association for Computational Linguistics (Vol-"
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": ""
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": "ume 1: Long Papers), pages 2236‚Äì2246."
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": ""
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "Kaiser, and Illia Polosukhin. 2017. Attention is all",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": ""
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "information pro-\nyou need.\nIn Advances in neural",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": "Y. Zhang and Qiang Yang. 2017. A survey on multi-task"
        },
        {
          "Louis-Philippe Morency, Rada Mihalcea,\nand Payal": "cessing systems, pages 5998‚Äì6008.",
          "Oriol Vinyals, A. Toshev, S. Bengio,\nand D. Erhan.": "learning. ArXiv, abs/1707.08114."
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "A\nBenchmarking results on the": "CMU-MOSEI"
        },
        {
          "A\nBenchmarking results on the": "We\nshow the\nbenchmarking\nresults\non CMU-"
        },
        {
          "A\nBenchmarking results on the": "MOSEI in Table 6."
        },
        {
          "A\nBenchmarking results on the": "B\nBest Hyper-parameters"
        },
        {
          "A\nBenchmarking results on the": "We include the best hyper-parameters\nfrom our"
        },
        {
          "A\nBenchmarking results on the": "hyper-parameter search in Table 7 and Table 8.\nIn"
        },
        {
          "A\nBenchmarking results on the": "addition, we also show the best\nloss weights we"
        },
        {
          "A\nBenchmarking results on the": "used for MTL in Table 9."
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "",
          "Angry": "",
          "Disgust": "",
          "Fear": "Weighted Acc.",
          "Happy": "(%)",
          "Sad": "",
          "Surprised": "",
          "Average": ""
        },
        {
          "Model": "EF-AVG",
          "Angry": "67.6 ¬± 0.3",
          "Disgust": "73.2 ¬± 0.4",
          "Fear": "66.2 ¬± 0.8",
          "Happy": "68.1 ¬± 0.3",
          "Sad": "63.0 ¬± 0.4",
          "Surprised": "61.1 ¬± 0.6",
          "Average": "66.6 ¬± 0.2"
        },
        {
          "Model": "EF-LSTM",
          "Angry": "66.3 ¬± 0.3",
          "Disgust": "72.3 ¬± 0.2",
          "Fear": "67.1 ¬± 0.6",
          "Happy": "67.3 ¬± 0.4",
          "Sad": "63.0 ¬± 0.3",
          "Surprised": "58.4 ¬± 0.3",
          "Average": "65.8 ¬± 0.1"
        },
        {
          "Model": "EF-TRANS",
          "Angry": "60.5 ¬± 0.8",
          "Disgust": "71.1 ¬± 1.3",
          "Fear": "62.9 ¬± 1.1",
          "Happy": "65.6 ¬± 0.3",
          "Sad": "62.1 ¬± 0.6",
          "Surprised": "55.7 ¬± 0.7",
          "Average": "63.0 ¬± 0.4"
        },
        {
          "Model": "LF-AVG",
          "Angry": "66.5 ¬± 0.6",
          "Disgust": "72.8 ¬± 0.2",
          "Fear": "66.3 ¬± 0.8",
          "Happy": "68.3 ¬± 0.3",
          "Sad": "62.9 ¬± 0.5",
          "Surprised": "61.6 ¬± 0.1",
          "Average": "66.4 ¬± 0.3"
        },
        {
          "Model": "LF-LSTM",
          "Angry": "66.3 ¬± 0.6",
          "Disgust": "72.6 ¬± 0.3",
          "Fear": "66.7 ¬± 0.7",
          "Happy": "68.1 ¬± 0.1",
          "Sad": "62.5 ¬± 0.5",
          "Surprised": "61.0 ¬± 0.6",
          "Average": "66.2 ¬± 0.1"
        },
        {
          "Model": "LF-TRANS",
          "Angry": "63.6 ¬± 1.0",
          "Disgust": "72.1 ¬± 0.5",
          "Fear": "64.2 ¬± 1.2",
          "Happy": "67.2 ¬± 0.4",
          "Sad": "62.6 ¬± 0.5",
          "Surprised": "60.9 ¬± 1.2",
          "Average": "65.1 ¬± 0.4"
        },
        {
          "Model": "MULT",
          "Angry": "64.6 ¬± 0.7",
          "Disgust": "73.4 ¬± 0.6",
          "Fear": "64.4 ¬± 1.2",
          "Happy": "68.9 ¬± 0.3",
          "Sad": "62.8 ¬± 0.2",
          "Surprised": "61.2 ¬± 0.2",
          "Average": "65.9 ¬± 0.4"
        },
        {
          "Model": "MFN",
          "Angry": "66.2 ¬± 1.2",
          "Disgust": "72.9 ¬± 0.4",
          "Fear": "65.5 ¬± 1.3",
          "Happy": "67.9 ¬± 0.4",
          "Sad": "62.1 ¬± 0.4",
          "Surprised": "61.2 ¬± 0.8",
          "Average": "66.0 ¬± 0.3"
        },
        {
          "Model": "EMO-EMB",
          "Angry": "66.5 ¬± 0.1",
          "Disgust": "73.3 ¬± 0.3",
          "Fear": "66.6 ¬± 1.3",
          "Happy": "67.8 ¬± 0.6",
          "Sad": "62.5 ¬± 0.5",
          "Surprised": "61.2 ¬± 0.6",
          "Average": "66.3 ¬± 0.4"
        },
        {
          "Model": "EF-LF AVG",
          "Angry": "66.9 ¬± 0.1",
          "Disgust": "73.1 ¬± 0.1",
          "Fear": "66.8 ¬± 0.2",
          "Happy": "68.3 ¬± 0.5",
          "Sad": "62.9 ¬± 0.4",
          "Surprised": "62.2 ¬± 0.4",
          "Average": "66.7 ¬± 0.2"
        },
        {
          "Model": "EF-LF LSTM",
          "Angry": "66.3 ¬± 0.2",
          "Disgust": "72.6 ¬± 0.6",
          "Fear": "66.2 ¬± 0.3",
          "Happy": "67.6 ¬± 0.4",
          "Sad": "63.0 ¬± 0.5",
          "Surprised": "59.6 ¬± 0.5",
          "Average": "65.9 ¬± 0.1"
        },
        {
          "Model": "EF-LF TRANS",
          "Angry": "63.0 ¬± 0.4",
          "Disgust": "72.4 ¬± 0.4",
          "Fear": "65.0 ¬± 0.2",
          "Happy": "68.6 ¬± 0.3",
          "Sad": "63.2 ¬± 0.6",
          "Surprised": "61.4 ¬± 1.3",
          "Average": "65.6 ¬± 0.1"
        },
        {
          "Model": "",
          "Angry": "",
          "Disgust": "",
          "Fear": "F1-Score (%)",
          "Happy": "",
          "Sad": "",
          "Surprised": "",
          "Average": ""
        },
        {
          "Model": "EF-AVG",
          "Angry": "45.0 ¬± 0.4",
          "Disgust": "49.9 ¬± 0.6",
          "Fear": "21.1 ¬± 0.6",
          "Happy": "64.5 ¬± 0.8",
          "Sad": "49.5 ¬± 0.3",
          "Surprised": "23.6 ¬± 0.4",
          "Average": "42.3 ¬± 0.2"
        },
        {
          "Model": "EF-LSTM",
          "Angry": "43.9 ¬± 0.3",
          "Disgust": "50.3 ¬± 0.2",
          "Fear": "21.8 ¬± 0.3",
          "Happy": "66.5 ¬± 0.7",
          "Sad": "48.7 ¬± 0.3",
          "Surprised": "22.0 ¬± 0.2",
          "Average": "42.2 ¬± 0.1"
        },
        {
          "Model": "EF-TRANS",
          "Angry": "37.2 ¬± 1.4",
          "Disgust": "50.1 ¬± 1.2",
          "Fear": "18.3 ¬± 0.6",
          "Happy": "65.2 ¬± 0.6",
          "Sad": "48.0 ¬± 0.5",
          "Surprised": "20.0 ¬± 0.6",
          "Average": "39.8 ¬± 0.4"
        },
        {
          "Model": "LF-AVG",
          "Angry": "44.1 ¬± 0.6",
          "Disgust": "49.6 ¬± 0.3",
          "Fear": "21.1 ¬± 0.8",
          "Happy": "67.1 ¬± 0.4",
          "Sad": "49.0 ¬± 0.6",
          "Surprised": "24.3 ¬± 0.1",
          "Average": "42.5 ¬± 0.4"
        },
        {
          "Model": "LF-LSTM",
          "Angry": "44.0 ¬± 0.6",
          "Disgust": "50.0 ¬± 0.4",
          "Fear": "22.1 ¬± 0.3",
          "Happy": "67.1 ¬± 0.4",
          "Sad": "48.3 ¬± 0.5",
          "Surprised": "24.2 ¬± 0.5",
          "Average": "42.6 ¬± 0.1"
        },
        {
          "Model": "LF-TRANS",
          "Angry": "41.4 ¬± 1.1",
          "Disgust": "49.8 ¬± 0.6",
          "Fear": "18.7 ¬± 0.4",
          "Happy": "67.9 ¬± 0.4",
          "Sad": "48.5 ¬± 0.4",
          "Surprised": "23.9 ¬± 0.9",
          "Average": "41.7 ¬± 0.3"
        },
        {
          "Model": "MULT",
          "Angry": "42.4 ¬± 0.8",
          "Disgust": "50.5 ¬± 0.8",
          "Fear": "20.6 ¬± 0.6",
          "Happy": "68.9 ¬± 0.9",
          "Sad": "48.7 ¬± 0.2",
          "Surprised": "23.8 ¬± 0.3",
          "Average": "42.5 ¬± 0.3"
        },
        {
          "Model": "MFN",
          "Angry": "43.8 ¬± 1.0",
          "Disgust": "50.1 ¬± 0.6",
          "Fear": "21.1 ¬± 1.5",
          "Happy": "67.0 ¬± 1.5",
          "Sad": "48.1 ¬± 0.5",
          "Surprised": "24.4 ¬± 0.7",
          "Average": "42.4 ¬± 0.3"
        },
        {
          "Model": "EMO-EMB",
          "Angry": "44.0 ¬± 0.2",
          "Disgust": "50.2 ¬± 0.4",
          "Fear": "21.2 ¬± 0.4",
          "Happy": "66.3 ¬± 0.8",
          "Sad": "48.6 ¬± 0.8",
          "Surprised": "23.9 ¬± 0.3",
          "Average": "42.4 ¬± 0.3"
        },
        {
          "Model": "EF-LF AVG",
          "Angry": "44.4 ¬± 0.2",
          "Disgust": "49.9 ¬± 0.1",
          "Fear": "21.1 ¬± 0.4",
          "Happy": "66.4 ¬± 1.2",
          "Sad": "49.2 ¬± 0.4",
          "Surprised": "24.3 ¬± 0.4",
          "Average": "42.6 ¬± 0.3"
        },
        {
          "Model": "EF-LF LSTM",
          "Angry": "43.9 ¬± 0.2",
          "Disgust": "50.2 ¬± 0.7",
          "Fear": "20.9 ¬± 0.6",
          "Happy": "66.2 ¬± 0.9",
          "Sad": "48.8 ¬± 0.5",
          "Surprised": "22.8 ¬± 0.4",
          "Average": "42.1 ¬± 0.1"
        },
        {
          "Model": "EF-LF TRANS",
          "Angry": "40.8 ¬± 0.6",
          "Disgust": "50.3 ¬± 0.5",
          "Fear": "19.0 ¬± 0.3",
          "Happy": "69.2 ¬± 0.4",
          "Sad": "48.7 ¬± 0.8",
          "Surprised": "24.6 ¬± 1.0",
          "Average": "42.1 ¬± 0.1"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "EF_AVG",
          "LR": "1e-3",
          "BS": "64",
          "Seed": "1",
          "Model SpeciÔ¨Åc Hyper-params": "hidden_dim=128"
        },
        {
          "Model": "LF_AVG",
          "LR": "1e-3",
          "BS": "32",
          "Seed": "3",
          "Model SpeciÔ¨Åc Hyper-params": "hidden_dims=[128,32,16]"
        },
        {
          "Model": "EF_LSTM",
          "LR": "5e-4",
          "BS": "128",
          "Seed": "0",
          "Model SpeciÔ¨Åc Hyper-params": "lstm_dim=300, bidirect, layer=1, dropout=0.1"
        },
        {
          "Model": "LF_LSTM",
          "LR": "1e-3",
          "BS": "128",
          "Seed": "0",
          "Model SpeciÔ¨Åc Hyper-params": "lstm_dim=[300,128,128], bidirect, layer=1, dropout=0.1"
        },
        {
          "Model": "EF_TRANS",
          "LR": "5e-5",
          "BS": "64",
          "Seed": "0",
          "Model SpeciÔ¨Åc Hyper-params": "layers=2, heads=5, Ô¨Ä_dim=512, dropout=0.1"
        },
        {
          "Model": "LF_TRANS",
          "LR": "1e-4",
          "BS": "128",
          "Seed": "0",
          "Model SpeciÔ¨Åc Hyper-params": "layers=2, heads=[4,2,2], Ô¨Ä_dim=[300,128,64], dropout=0.1"
        },
        {
          "Model": "MFN",
          "LR": "1e-3",
          "BS": "128",
          "Seed": "0",
          "Model SpeciÔ¨Åc Hyper-params": "Refer to the original paper (Zadeh et al., 2018a)"
        },
        {
          "Model": "MULT",
          "LR": "2e-4",
          "BS": "32",
          "Seed": "1",
          "Model SpeciÔ¨Åc Hyper-params": "Refer to the original paper (Tsai et al., 2019)"
        },
        {
          "Model": "EMO_EMB",
          "LR": "5e-4",
          "BS": "64",
          "Seed": "0",
          "Model SpeciÔ¨Åc Hyper-params": "Refer to the original paper (Dai et al., 2020a)"
        },
        {
          "Model": "EF_LF_AVG",
          "LR": "5e-4",
          "BS": "64",
          "Seed": "0",
          "Model SpeciÔ¨Åc Hyper-params": "-"
        },
        {
          "Model": "EF_LF_LSTM",
          "LR": "5e-4",
          "BS": "128",
          "Seed": "0",
          "Model SpeciÔ¨Åc Hyper-params": "-"
        },
        {
          "Model": "EF_LF_TRANS",
          "LR": "1e-4",
          "BS": "256",
          "Seed": "1",
          "Model SpeciÔ¨Åc Hyper-params": "-"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "EF_AVG",
          "LR": "5e-4",
          "BS": "256",
          "Seed": "1",
          "Model SpeciÔ¨Åc Hyper-params": "hidden_dim=128"
        },
        {
          "Model": "LF_AVG",
          "LR": "5e-4",
          "BS": "256",
          "Seed": "0",
          "Model SpeciÔ¨Åc Hyper-params": "hidden_dims=[128,64,32]"
        },
        {
          "Model": "EF_LSTM",
          "LR": "5e-5",
          "BS": "32",
          "Seed": "2",
          "Model SpeciÔ¨Åc Hyper-params": "lstm_dim=512, bidirect, layer=1, dropout=0.1"
        },
        {
          "Model": "LF_LSTM",
          "LR": "5e-5",
          "BS": "32",
          "Seed": "1",
          "Model SpeciÔ¨Åc Hyper-params": "lstm_dim=[300,128,128], bidirect, layer=1, dropout=0.1"
        },
        {
          "Model": "EF_TRANS",
          "LR": "5e-5",
          "BS": "32",
          "Seed": "0",
          "Model SpeciÔ¨Åc Hyper-params": "layers=2, heads=5, Ô¨Ä_dim=512, dropout=0.1"
        },
        {
          "Model": "LF_TRANS",
          "LR": "5e-5",
          "BS": "64",
          "Seed": "0",
          "Model SpeciÔ¨Åc Hyper-params": "layers=2, heads=[4,2,2], Ô¨Ä_dim=[300,128,64], dropout=0.1"
        },
        {
          "Model": "MFN",
          "LR": "1e-4",
          "BS": "128",
          "Seed": "0",
          "Model SpeciÔ¨Åc Hyper-params": "Refer to the original paper (Zadeh et al., 2018a)"
        },
        {
          "Model": "MULT",
          "LR": "1e-4",
          "BS": "32",
          "Seed": "1",
          "Model SpeciÔ¨Åc Hyper-params": "Refer to the original paper (Tsai et al., 2019)"
        },
        {
          "Model": "EMO_EMB",
          "LR": "5e-5",
          "BS": "256",
          "Seed": "0",
          "Model SpeciÔ¨Åc Hyper-params": "Refer to the original paper (Dai et al., 2020a)"
        },
        {
          "Model": "EF_LF_AVG",
          "LR": "5e-4",
          "BS": "256",
          "Seed": "0",
          "Model SpeciÔ¨Åc Hyper-params": "-"
        },
        {
          "Model": "EF_LF_LSTM",
          "LR": "1e-4",
          "BS": "32",
          "Seed": "0",
          "Model SpeciÔ¨Åc Hyper-params": "-"
        },
        {
          "Model": "EF_LF_TRANS",
          "LR": "5e-5",
          "BS": "64",
          "Seed": "1",
          "Model SpeciÔ¨Åc Hyper-params": "-"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Target": "",
          "Train": "",
          "EF-LF AVG": "Loss weights",
          "EF-LF LSTM": "Loss weights",
          "EF-LF TRANS": "Loss weights"
        },
        {
          "Target": "",
          "Train": "Emotion",
          "EF-LF AVG": "-",
          "EF-LF LSTM": "-",
          "EF-LF TRANS": "-"
        },
        {
          "Target": "Emotion",
          "Train": "+Sentiment(ùëä )",
          "EF-LF AVG": "1.0 0.6 0.0",
          "EF-LF LSTM": "1.0 0.8 0.0",
          "EF-LF TRANS": "1.0 0.7 0.0"
        },
        {
          "Target": "(IEMOCAP)",
          "Train": "+Sarcasm(ùëä )",
          "EF-LF AVG": "1.0 0.0 0.5",
          "EF-LF LSTM": "1.0 0.0 0.6",
          "EF-LF TRANS": "1.0 0.0 0.7"
        },
        {
          "Target": "",
          "Train": "All",
          "EF-LF AVG": "1.0 0.8 0.3",
          "EF-LF LSTM": "1.0 0.7 0.5",
          "EF-LF TRANS": "1.0 0.7 0.3"
        },
        {
          "Target": "",
          "Train": "Emotion",
          "EF-LF AVG": "-",
          "EF-LF LSTM": "-",
          "EF-LF TRANS": "-"
        },
        {
          "Target": "Emotion",
          "Train": "+Sentiment(ùëÜ)",
          "EF-LF AVG": "1.0 0.4 0.0",
          "EF-LF LSTM": "1.0 1.0 0.0",
          "EF-LF TRANS": "1.0 1.0 0.0"
        },
        {
          "Target": "(CMU-MOSEI)",
          "Train": "+Sarcasm(ùëä )",
          "EF-LF AVG": "1.0 0.0 0.6",
          "EF-LF LSTM": "1.0 0.0 0.8",
          "EF-LF TRANS": "1.0 0.0 0.5"
        },
        {
          "Target": "",
          "Train": "All",
          "EF-LF AVG": "1.0 0.7 0.5",
          "EF-LF LSTM": "1.0 0.8 0.6",
          "EF-LF TRANS": "1.0 0.9 0.4"
        },
        {
          "Target": "",
          "Train": "Sarcasm",
          "EF-LF AVG": "-",
          "EF-LF LSTM": "-",
          "EF-LF TRANS": "-"
        },
        {
          "Target": "Sarcasm",
          "Train": "+Emotion(ùëä )",
          "EF-LF AVG": "0.4 0.0 1.0",
          "EF-LF LSTM": "0.4 0.0 1.0",
          "EF-LF TRANS": "1.0 0.0 1.0"
        },
        {
          "Target": "(MUStARD)",
          "Train": "+Sentiment(ùëä )",
          "EF-LF AVG": "0.0 1.0 1.0",
          "EF-LF LSTM": "0.0 0.5 1.0",
          "EF-LF TRANS": "0.0 0.6 1.0"
        },
        {
          "Target": "",
          "Train": "All",
          "EF-LF AVG": "0.4 1.0 1.0",
          "EF-LF LSTM": "0.1 0.5 1.0",
          "EF-LF TRANS": "1.0 0.1 1.0"
        },
        {
          "Target": "",
          "Train": "Sentiment1",
          "EF-LF AVG": "-",
          "EF-LF LSTM": "-",
          "EF-LF TRANS": "-"
        },
        {
          "Target": "",
          "Train": "",
          "EF-LF AVG": "",
          "EF-LF LSTM": "",
          "EF-LF TRANS": ""
        },
        {
          "Target": "Sentiment",
          "Train": "+Emotion(ùëÜ)",
          "EF-LF AVG": "0.8 1.0 0.0",
          "EF-LF LSTM": "1.0 1.0 0.0",
          "EF-LF TRANS": "0.8 1.0 0.0"
        },
        {
          "Target": "",
          "Train": "",
          "EF-LF AVG": "",
          "EF-LF LSTM": "",
          "EF-LF TRANS": ""
        },
        {
          "Target": "(CMU-MOSEI)",
          "Train": "+Emotion(ùëä )",
          "EF-LF AVG": "0.6 1.0 0.0",
          "EF-LF LSTM": "0.6 1.0 0.0",
          "EF-LF TRANS": "0.8 1.0 0.0"
        },
        {
          "Target": "",
          "Train": "",
          "EF-LF AVG": "",
          "EF-LF LSTM": "",
          "EF-LF TRANS": ""
        },
        {
          "Target": "",
          "Train": "+Sarcasm(ùëä )",
          "EF-LF AVG": "0.0 1.0 0.6",
          "EF-LF LSTM": "0.0 1.0 0.6",
          "EF-LF TRANS": "0.0 1.0 0.2"
        },
        {
          "Target": "",
          "Train": "All(1+2+4)",
          "EF-LF AVG": "0.8 1.0 0.8",
          "EF-LF LSTM": "0.1 1.0 0.1",
          "EF-LF TRANS": "0.8 1.0 0.2"
        },
        {
          "Target": "",
          "Train": "All(1+3+4)",
          "EF-LF AVG": "1.0 1.0 1.0",
          "EF-LF LSTM": "1.0 1.0 0.5",
          "EF-LF TRANS": "1.0 1.0 0.5"
        }
      ],
      "page": 13
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Multitask learning for Arabic offensive language and hatespeech detection",
      "authors": [
        "Ibrahim Abu",
        "Walid Magdy"
      ],
      "year": "2020",
      "venue": "Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection"
    },
    {
      "citation_id": "2",
      "title": "Multi-task learning for multi-modal emotion recognition and sentiment analysis",
      "authors": [
        "Shad Md",
        "D Akhtar",
        "Deepanway Chauhan",
        "Soujanya Ghosal",
        "Asif Poria",
        "P Ekbal",
        "Bhattacharyya"
      ],
      "year": "2019",
      "venue": "Multi-task learning for multi-modal emotion recognition and sentiment analysis"
    },
    {
      "citation_id": "3",
      "title": "Openface 2.0: Facial behavior analysis toolkit",
      "authors": [
        "T Baltrusaitis",
        "A Zadeh",
        "Y Lim",
        "L Morency"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE International Conference on Au-tomatic Face Gesture Recognition (FG 2018)",
      "doi": "10.1109/FG.2018.00019"
    },
    {
      "citation_id": "4",
      "title": "Multimodal machine learning: A survey and taxonomy",
      "authors": [
        "Tadas Baltru≈°aitis",
        "Chaitanya Ahuja",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "5",
      "title": "Iemocap: interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "Chi-Chun Lee",
        "A Kazemzadeh",
        "E Provost",
        "S Kim",
        "J Chang",
        "S Lee",
        "Shrikanth Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "6",
      "title": "Towards multimodal sarcasm detection (an obviously perfect paper)",
      "authors": [
        "Santiago Castro",
        "Devamanyu Hazarika",
        "Ver√≥nica P√©rez-Rosas",
        "R Zimmermann",
        "R Mihalcea",
        "Soujanya Poria"
      ],
      "year": "2019",
      "venue": "Towards multimodal sarcasm detection (an obviously perfect paper)"
    },
    {
      "citation_id": "7",
      "title": "Sentiment and emotion help sarcasm? a multi-task learning framework for multimodal sarcasm, sentiment and emotion analysis",
      "authors": [
        "D Chauhan",
        "R Dhanushs",
        "P Asif Ekbal",
        "Bhattacharyya"
      ],
      "year": "2020",
      "venue": "ACL"
    },
    {
      "citation_id": "8",
      "title": "Multimodal end-to-end sparse model for emotion recognition",
      "authors": [
        "Wenliang Dai",
        "Samuel Cahyawƒ≥aya",
        "Zihan Liu",
        "Pascale Fung"
      ],
      "year": "2021",
      "venue": "Multimodal end-to-end sparse model for emotion recognition"
    },
    {
      "citation_id": "9",
      "title": "2020a. Modality-transferable emotion embeddings for low-resource multimodal emotion recognition",
      "authors": [
        "Wenliang Dai",
        "Zihan Liu",
        "Tiezheng Yu",
        "Pascale Fung"
      ],
      "venue": "Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "10",
      "title": "Kungfupanda at SemEval-2020 task 12: BERT-based multi-TaskLearning for offensive language detection",
      "authors": [
        "Wenliang Dai",
        "Tiezheng Yu",
        "Zihan Liu",
        "Pascale Fung"
      ],
      "year": "2020",
      "venue": "Proceedings of the Fourteenth Workshop on Semantic Evaluation"
    },
    {
      "citation_id": "11",
      "title": "Covarep -a collaborative voice analysis repository for speech technologies",
      "authors": [
        "G Degottex",
        "John Kane",
        "T Thomas Drugman",
        "Stefan Raitio",
        "Scherer"
      ],
      "year": "2014",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "12",
      "title": "Imagenet: A large-scale hierarchical image database",
      "authors": [
        "Jia Deng",
        "W Dong",
        "R Socher",
        "L Li",
        "K Li",
        "Li Fei-Fei"
      ],
      "year": "2009",
      "venue": "CVPR"
    },
    {
      "citation_id": "13",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "NAACL-HLT"
    },
    {
      "citation_id": "14",
      "title": "Facial signs of emotional experience",
      "authors": [
        "Paul Ekman",
        "Sonia Wallace V Freisen",
        "Ancoli"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "15",
      "title": "Making the v in vqa matter: Elevating the role of image understanding in visual question answering",
      "authors": [
        "Yash Goyal",
        "Tejas Khot",
        "Douglas Summers-Stay",
        "Dhruv Batra",
        "Devi Parikh"
      ],
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "16",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Amir Zadeh",
        "E Cambria",
        "Louis-Philippe Morency",
        "Roger Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the conference"
    },
    {
      "citation_id": "17",
      "title": "Long short-term memory",
      "authors": [
        "Sepp Hochreiter",
        "J√ºrgen Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural computation"
    },
    {
      "citation_id": "18",
      "title": "Clevr: A diagnostic dataset for compositional language and elementary visual reasoning",
      "authors": [
        "J Johnson",
        "B Hariharan",
        "Laurens Van Der Maaten",
        "Li Fei-Fei",
        "C Zitnick",
        "Ross Girshick"
      ],
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "19",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2015",
      "venue": "Adam: A method for stochastic optimization"
    },
    {
      "citation_id": "20",
      "title": "Ubernet: Training a universal convolutional neural network for low-, mid-, and highlevel vision using diverse datasets and limited memory",
      "authors": [
        "I Kokkinos"
      ],
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "21",
      "title": "Zitnick. 2014a. Microsoft coco: Common objects in context",
      "authors": [
        "Tsung-Yi Lin",
        "M Maire",
        "Serge Belongie",
        "James Hays",
        "P Perona",
        "D Ramanan",
        "Piotr Doll√°r"
      ],
      "venue": "ECCV"
    },
    {
      "citation_id": "22",
      "title": "Zitnick. 2014b. Microsoft coco: Common objects in context",
      "authors": [
        "Tsung-Yi Lin",
        "M Maire",
        "Serge Belongie",
        "James Hays",
        "P Perona",
        "D Ramanan",
        "Piotr Doll√°r"
      ],
      "venue": "ECCV"
    },
    {
      "citation_id": "23",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Y Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "M Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach"
    },
    {
      "citation_id": "24",
      "title": "Efficient low-rank multimodal fusion with modality-specific factors",
      "authors": [
        "Zhun Liu",
        "Ying Shen",
        "V Lakshminarasimhan",
        "P Liang",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "ACL"
    },
    {
      "citation_id": "25",
      "title": "The natural language decathlon: Multitask learning as question answering",
      "authors": [
        "N Bryan Mccann",
        "Caiming Keskar",
        "R Xiong",
        "Socher"
      ],
      "year": "2018",
      "venue": "The natural language decathlon: Multitask learning as question answering"
    },
    {
      "citation_id": "26",
      "title": "Towards multimodal sentiment analysis: Harvesting opinions from the web",
      "authors": [
        "Louis-Philippe Morency",
        "Rada Mihalcea",
        "Payal Doshi"
      ],
      "year": "2011",
      "venue": "Proceedings of the 13th international conference on multimodal interfaces"
    },
    {
      "citation_id": "27",
      "title": "Pytorch: An imperative style, highperformance deep learning library",
      "authors": [
        "S Adam Paszke",
        "Francisco Gross",
        "A Massa",
        "James Lerer",
        "Gregory Bradbury",
        "Trevor Chanan",
        "Z Killeen",
        "N Lin",
        "L Gimelshein",
        "Alban Antiga",
        "Andreas Desmaison",
        "Edward K√∂pf",
        "Zach Yang",
        "Martin Devito",
        "Alykhan Raison",
        "Sasank Tejani",
        "B Chilamkurthy",
        "Lu Steiner",
        "Junjie Fang",
        "Soumith Bai",
        "Chintala"
      ],
      "year": "2019",
      "venue": "Pytorch: An imperative style, highperformance deep learning library"
    },
    {
      "citation_id": "28",
      "title": "Glove: Global vectors for word representation",
      "authors": [
        "Jeffrey Pennington",
        "Richard Socher",
        "Christopher Manning"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)"
    },
    {
      "citation_id": "29",
      "title": "Integrating multimodal information in large pretrained transformers",
      "authors": [
        "Wasifur Rahman",
        "Md Kamrul Hasan",
        "Sangwu Lee",
        "Amirali Bagher Zadeh",
        "Chengfeng Mao",
        "Louis-Philippe Morency",
        "Ehsan Hoque"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "30",
      "title": "Squad: 100, 000+ questions for machine comprehension of text",
      "authors": [
        "Pranav Rajpurkar",
        "Jian Zhang",
        "Konstantin Lopyrev",
        "Percy Liang"
      ],
      "year": "2016",
      "venue": "EMNLP"
    },
    {
      "citation_id": "31",
      "title": "How2: A large-scale dataset for multimodal language understanding",
      "authors": [
        "Ramon Sanabria",
        "Ozan Caglayan",
        "Shruti Palaskar",
        "Desmond Elliott",
        "Lo√Øc Barrault",
        "Lucia Specia",
        "Florian Metze"
      ],
      "year": "2018",
      "venue": "How2: A large-scale dataset for multimodal language understanding"
    },
    {
      "citation_id": "32",
      "title": "Textcaps: a dataset for image captioning with reading comprehension",
      "authors": [
        "O Sidorov",
        "Ronghang Hu",
        "Marcus Rohrbach",
        "Amanpreet Singh"
      ],
      "year": "2020",
      "venue": "Textcaps: a dataset for image captioning with reading comprehension"
    },
    {
      "citation_id": "33",
      "title": "Combating human trafficking with multimodal deep models",
      "authors": [
        "Edmund Tong",
        "Amir Zadeh",
        "Cara Jones",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Combating human trafficking with multimodal deep models"
    },
    {
      "citation_id": "34",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Shaojie Bai",
        "P Liang",
        "J Kolter",
        "Louis-Philippe Morency",
        "R Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the conference. Association for Computational Linguistics. Meeting"
    },
    {
      "citation_id": "35",
      "title": "Learning factorized multimodal representations",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Paul Liang",
        "Amir Zadeh",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2018",
      "venue": "Learning factorized multimodal representations",
      "arxiv": "arXiv:1806.06176"
    },
    {
      "citation_id": "36",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "≈Åukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "37",
      "title": "Show and tell: A neural image caption generator",
      "authors": [
        "Oriol",
        "A Vinyals",
        "S Toshev",
        "D Bengio",
        "Erhan"
      ],
      "year": "2015",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "38",
      "title": "Words can shift: Dynamically adjusting word representations using nonverbal behaviors",
      "authors": [
        "Yansen Wang",
        "Ying Shen",
        "Zhun Liu",
        "P Liang",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2019",
      "venue": "Proceedings of the ... AAAI Conference on Artificial Intelligence. AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "39",
      "title": "Words can shift: Dynamically adjusting word representations using nonverbal behaviors",
      "authors": [
        "Yansen Wang",
        "Ying Shen",
        "Zhun Liu",
        "Paul Liang",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "40",
      "title": "Pad-net: Multi-tasks guided prediction-anddistillation network for simultaneous depth estimation and scene parsing",
      "authors": [
        "D Xu",
        "Wanli Ouyang",
        "Xiaogang Wang",
        "N Sebe"
      ],
      "year": "2018",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "41",
      "title": "Dimsum @LaySumm 20",
      "authors": [
        "Tiezheng Yu",
        "Dan Su",
        "Wenliang Dai",
        "Pascale Fung"
      ],
      "year": "2020",
      "venue": "Proceedings of the First Workshop on Scholarly Document Processing",
      "doi": "10.18653/v1/2020.sdp-1.35"
    },
    {
      "citation_id": "42",
      "title": "Speaker identification on the scotus corpus",
      "authors": [
        "Jiahong Yuan",
        "M Liberman"
      ],
      "year": "2008",
      "venue": "Journal of the Acoustical Society of America"
    },
    {
      "citation_id": "43",
      "title": "Tensor fusion network for multimodal sentiment analysis",
      "authors": [
        "M Amir Zadeh",
        "Soujanya Chen",
        "E Poria",
        "Louis-Philippe Cambria",
        "Morency"
      ],
      "year": "2017",
      "venue": "EMNLP"
    },
    {
      "citation_id": "44",
      "title": "Memory fusion network for multi-view sequential learning",
      "authors": [
        "P Amir Zadeh",
        "N Liang",
        "Mazumder",
        "E Soujanya Poria",
        "Louis-Philippe Cambria",
        "Morency"
      ],
      "year": "2018",
      "venue": "Memory fusion network for multi-view sequential learning"
    },
    {
      "citation_id": "45",
      "title": "Mosi: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos",
      "authors": [
        "Amir Zadeh",
        "Rowan Zellers",
        "Eli Pincus",
        "Louis-Philippe Morency"
      ],
      "year": "2016",
      "venue": "Mosi: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos",
      "arxiv": "arXiv:1606.06259"
    },
    {
      "citation_id": "46",
      "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "Amirali Bagher Zadeh",
        "Paul Liang",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "47",
      "title": "A survey on multi-task learning",
      "authors": [
        "Y Zhang",
        "Qiang Yang"
      ],
      "year": "2017",
      "venue": "A survey on multi-task learning"
    }
  ]
}