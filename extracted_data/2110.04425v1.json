{
  "paper_id": "2110.04425v1",
  "title": "Arabic Speech Emotion Recognition Employing Wav2Vec2.0 And Hubert Based On Baved Dataset",
  "published": "2021-10-09T00:58:12Z",
  "authors": [
    "Omar Mohamed",
    "Salah A. Aly"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Recently, there have been tremendous research outcomes in the fields of speech recognition and natural language processing. This is due to the well-developed multilayers deep learning paradigms such as wav2vec2.0, Wav2vecU, WavBERT, and HuBERT that provide better representation learning and high information capturing. Such paradigms run on hundreds of unlabeled data, then fine-tuned on a small dataset for specific tasks. This paper introduces a deep learning constructed emotional recognition model for Arabic speech dialogues. The developed model employs the state of the art audio representations include wav2vec2.0 and HuBERT. The experiment and performance results of our model overcome the previous known outcomes.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction Researchers And Scientists Have Used Deep Learning As",
      "text": "Feature extraction in recent decades, utilizing the power of deep learning to capture important features that have resulted in significant improvements in literature and real-world applications.\n\nThere are several hitches when pursuing research work in emotion recognition:\n\n• emotion detection of humans does not reply on the text or on their speeches, but also in the way they talk to other persons, • the leak of the available dataset sets particularly in Arabic dialogues, • emotion detection does not depend only on one word but also in several words in the contexts, • Some words can be used in different styles, in which they express the speakers' attitude and emotion. The prosodic properties of human speeches are represented by acoustic features such as pitch, intensity, duration, and voice quality.\n\nDespite the enormous success contributions in emotion recognition in English datasets, there is still a gab in Arabic dataset and emotion recognition systems utilizes these Arabic datasets. Various Arabic speeches emotion datasets have been proposed in the literature, whether audio or visual, see  [1] -  [4] .\n\nThere are well-known several datasets for English, Basic Arabic Vocal Emotions Dataset (BAVED) is a dataset that contains Arabic words spelled in different levels of emotions recorded in an audio/wav format  [5] .\n\nThe problem of emotion recognition analysis on written text or audio speeches has an immense impact and can affect many sectors in the society and relations between persons. A Multi-Task Learning Emotion recognition system has been proposed to detect hate speeches and offensive languages, see  [6]  and the references therein. This work can also be extended to detect wav audio hate and offensive speeches.\n\nThe paper structure is described as follows. In Sections 2 and 3, we introduce the related work and note about the BAVED dataset, rspectively. In Section 4 we describe the proposed mispronunciation error detection model. In Section 5 we present simulation studies for the proposed model, and finally, the paper is concluded in Section 6.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Recent tremendous results in speech emotion recognition (SER) have been focused on the utilizations of deep learning and convolutional networks  [7] ,  [8] ,  [9] ,  [10] ,  [11] ,  [12] . The task is also investigated in Arabic speech emotion recognition (ASER) in several recent results  [13] ,  [14] ,  [15] .\n\nThe problem has a business side effect in the case of customer's satisfactions and given services. For example, the model can measure if the customers are satisfied about certain products in the market. The system can Figure  1 . Architecture of the system for emotion recognition of Arabic speeches also be used to happiness and sadness of persons by listening many of their conversations.\n\nAttention-based deep neural networks (DNNs)are employed to give better results than classical neural networks. Klaylat etc. proposed Arabic emotion recognition system based on a data TV news for three labeled emotions: happy, angry or surprised  [15] . In their work, classification models are proposed that gave approximately 90% Accuracy.\n\nRecent progress in emotion recognition for certain arabic dialect has been also investigated, see for example  [16] ,  [17] .\n\nKSU Emotions was developed by King Saud University (KSU) and contains approximately five hours of emotional Modern Standard Arabic (MSA) speech from 23 subjects. Speakers were from three countries: Yemen, Saudi Arabia and Syria  [3] .\n\nKlaylat in  [18]  described an Arabic dataset that consists of eight videos of live calls between an anchor and a human outside the studio were downloaded from online Arabic talk shows. Each video was then divided into turns: callers and receivers. To label each video, 18 listeners were asked to listen to each video and select whether they perceive a happy, angry or surprised emotion. Silence, laughs and noisy chunks were removed. Every chunk was then automatically divided into 1 sec speech units forming our final corpus composed of 1384 records.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Arabic Baved Dataset",
      "text": "Despite the enormous success contributions in emotion recognition in English datasets, there is still gab in Arabic dataset and emotion recognition systems utilizes these Arabic datasets. Some Arabic speeches emotion datasets have been proposed in the literature, see  [1] -  [3] ,  [5] ,  [19] . Each dataset has a different set of classes or labels, for example, the Arabic audio acted dataset proposed in  [20]  has five labels (Happiness, Sadness, Neutral, Anger, Fear), and the dataset proposed in  [15]  has three classes (Happy, Surprised, and Angry), while the dataset proposed in  [19]  has labels (Happy, Sad, Neutral, Angry, Surprise, Disgust).\n\nFigure  2 . BAVED defined Words , see  [5]  BAVED dataset is a collection of audio/wav recorded Arabic words spoken in various expressed emotions  [5] . The BAVED dataset includes 7 words given as 0-like, 1unlike, 2-this, 3-file, 4-good, 5-neutral, and 6-bad. The BAVED dataset word is pronounced in three levels corresponding to the person's emotions: 0 for low emotion (tired or exhausted), 1 for neutral emotion, and 2 for high emotion positive or negative emotions (happiness, joy, sadness, anger). The dataset contains 1935 recordings that are recorded by 61 speakers (45 males and 16 females). This is a drawback in the dataset that we will investigate in a future work.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Methods",
      "text": "Let A = {a 1 , a 2 , .., a n } be a set of wav audio signals produced by several native speakers L1. Let S = {s 1 , s 2 , ..., s n } be a set of words or sentences corresponding to the recordings A, where each sentence (word) s j corresponding to only one phoneme a j . Our goal is to detect the emotion of the wav audio signals A, and check if it is one of the 3 cases in the given dataset BAVED  [5] .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Feature Extraction",
      "text": "It's relatively simple for a human to understand what's in an image-finding an object, such as a car or a face; classifying a structure as damaged or undamaged; or visually identifying different land cover kinds are all basic tasks. The task is far more complex for machines. To tackle real-world problems, however, it's vital to be able to leverage and automate machine-based feature extraction. Deep learning is a machine learning technique for detecting features in images. It makes use of a multilayer neural network, which is a computer system that mimics the functions of the human brain.\n\nResearchers and scientists have used deep learning as Feature extraction in recent decades, utilizing the power of deep learning to capture important features that have resulted in significant improvements in literature and real-world applications.\n\nIn the machine learning community, representation learning has evolved into its own area, commonly referred to as Deep Learning or Feature Learning. Although depth is an important aspect of the story, there are many other priors that are intriguing and may be easily captured when the challenge is framed as learning a representation. a more accurate representation allows the model to comprehend the data. When it comes to representation learning, two recent advanced algorithms produce the state of the art in the field of speech recognition: Wav2vec2.0 c  [21]  and HuBERT  [22] .\n\nWav2vec2.0: Wav2vec2.0 is a self-supervised speech representation model that pursues to capture the crucial properties of raw audios by using the power of transformers and Contrastive learning. The wav2vec2.0 training procedure is divided into two phases: i) the model is trained on hundreds of unlabeled data in the first phase, ii) fine-tuned on a small dataset for specific tasks.\n\nThe Wav2vec2.0 model consists of:\n\n• convolutional layers that process the raw waveform input to get latent representation -Z,\n\n• transformer layers, creating contextualized representation -C linear projection to output -Y. We used the pre-trained model Elgeish  [23] , which is Fine-tuned facebook/wav2vec2-large-xlsr-53 on Arabic using the train splits of Common Voice and Arabic Speech Corpus.\n\nHuBERT: Innovative method for self-supervised speech representation learning HuBERT for speech representation learning matches or outperforms SOTA techniques for speech recognition, generation, and compression. HuBERT learns the structure of spoken input by predicting the proper cluster for masked audio segments using an offline k-means clustering step. By alternating between clustering and prediction processes, HuBERT improves its learnt discrete representations over time. Furthermore, the high quality of HuBERT's learned presentations allows for simple deployment to a wide range of downstream speech applications.\n\nHuBERT uses continuous inputs to train both acoustic and linguistic models. The model must first encode unmasked audio inputs into meaningful continuous latent representations, which correspond to the traditional acoustic modelling problem. Second, the model must capture the long-term temporal relationships between learned representations in order to reduce prediction error. One key finding driving this research is the importance of consistency, not simply correctness, of the k-means mapping from auditory inputs to discrete targets, which allows the model to focus on modelling the sequential structure of input data.\n\nIf an early clustering iteration can't tell the difference between the /k/ and /g/ sounds, the prediction loss will learn representations that explain how additional consonant and vowel sounds operate together with this supercluster to generate words, resulting in a single supercluster comprising both of them. As a result, the next clustering iteration uses the newly learned representation to produce superior clusters. Our results show that by alternating clustering and prediction phases, representations improve with time.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Mlp And Bi-Lstm Classifiers",
      "text": "After extracting features with wav2vec2.0 and Hu-BERT, we feed the output into a classifier head: We utilized MLP Classifier stands for Multi-layer Perception Classifier, which is linked to a Neural Network by its name, and a Bi-LSTM Layer with 50 hidden units, Both classifiers produced results that were close to each other.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Results And Performance Evaluations",
      "text": "To test our models, we use three different measurements to evaluate the performance: F 1 score, validation loss, and confusion matrix.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "F 1 Score",
      "text": "We use F-1 score to measure the accuracy of the proposed model. The reason we use F-1 score is that it gives better measurement for unbalanced data.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Validation Loss And Training Loss",
      "text": "The models were run for a total of 5 epochs. The Wav2vec2.0 and HuBERT models' outputs are represented in the performance measures used to calculate the categorization problem's performance. Wav2vec2.0 converges faster than HuBERT model and is more stable during the training phase. As shown in Figure  4 , the train loss of wave2vec2 has the lowest training loss in comparison to the other three models.\n\nEvaluation loss of wav2vec2 has the lowest among the three models , and more stable in the converging process  Wav2vec2 achieves the best accuracy among the three models. Despite HuBERT achieves the best results on the downstream tasks, and captures important feature representation, the wav2vec outperforms for some reasons 1) Wav2vec has been trained on Arabic dataset especially Elgeish Pre-trained model, it has been trained on Common Voice  [24]  and Arabic Speech Corpus  [25] .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Confusion Matrix",
      "text": "2) Both Hubert models (base , large) have been trained As shown in table I, Wav2vec2.0 outperforms Hu-BERT base and large because Wave2vec2.0's pre-trained model \"Elgeish\" was trained on Arabic datasets (common voice and Arabic speech corpus), whereas both HuBERT models were trained on multi-language tasks. Despite HuBERT's robustness against noise and ability to capture more information than Wav2vec2.0, it failed this task. The proposed speech emotion procedure is described in Alg. 1. Let T L and V L be the training loss and validation loss, respectively.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "We developed the state of the art deep learning models to detect emotions of Arabic speeches. We implemented these learning models and demonstrated their results on the Arabic BAVED audio dataset. Several experiments are performed using wav2vec2 and HuBERT different validation techniques. The model is successfully performed by using wav2vec2 and yielded an accuracy of  for i =1 to max-iter do 4:\n\nSample a mini-batch of pairs from A 5:\n\nV ← wav2vec F eatureExtractor(A i )\n\n6:\n\nCompute h o as the init. of A.N.L. Print a j emotion recognition 12: end procedure 89%. As future work, we plan to extend the proposed method to incorporate more feature sets and increase the size of the dataset for words, sentence, and paragraph recognition.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Architecture of the system for emotion recognition of Arabic speeches",
      "page": 2
    },
    {
      "caption": "Figure 2: BAVED deﬁned Words , see [5]",
      "page": 2
    },
    {
      "caption": "Figure 3: , illustrates that Wav2vec2.0 achieves best",
      "page": 4
    },
    {
      "caption": "Figure 3: Comprise of the results and accuracy of the proposed three",
      "page": 4
    },
    {
      "caption": "Figure 4: The evaluation loss of the three models.",
      "page": 4
    },
    {
      "caption": "Figure 5: The training loss of the three models.",
      "page": 4
    },
    {
      "caption": "Figure 6: displays that the prediction matrix of the start",
      "page": 4
    },
    {
      "caption": "Figure 6: Figures: (a)Results of Confusion Matrix wav2vec2.0, (b) Results of Confusion Matrix HuBEERT base, (c) Results of Confusion",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "model": "wav2vec2.0\nHubert Base\nHubert Large",
          "Length": "19 Min\n19 Min\n19 Min",
          "no.\nrecords": "1935\n1935\n1935",
          "accuracy": "89\n87\n84"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Multi-path and group-loss-based network for speech emotion recognition in multi-domain datasets",
      "authors": [
        "K Noh",
        "C Jeong",
        "J Lim",
        "S Chung",
        "G Kim",
        "J Lim",
        "H Jeong"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "2",
      "title": "A new arabic dataset for emotion recognition",
      "authors": [
        "A Almahdawi",
        "W Teahan"
      ],
      "year": "2019",
      "venue": "Advances in Intelligent Systems and Computing"
    },
    {
      "citation_id": "3",
      "title": "Ksuemotions ldc2017s12",
      "authors": [
        "A Meftah",
        "Y Alotaibi",
        "S.-A Selouani"
      ],
      "year": "2017",
      "venue": "Web Download. Philadelphia: Linguistic Data Consortium"
    },
    {
      "citation_id": "4",
      "title": "Novel hybrid dnn approaches for speaker verification in emotional and stressful talking environments",
      "authors": [
        "I Shahin",
        "A Nassif",
        "N Nemmour",
        "A Elnagar",
        "A Alhudhaif",
        "K Polat"
      ],
      "year": "2021",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "5",
      "title": "Basic arabic vocal emotions dataset (baved) -github",
      "authors": [
        "A Aouf"
      ],
      "year": "2019",
      "venue": "Basic arabic vocal emotions dataset (baved) -github"
    },
    {
      "citation_id": "6",
      "title": "Multitask learning with sentiment, emotion, and target detection to recognize hate speech and offensive language",
      "authors": [
        "F Del Arco1",
        "S Halat",
        "S Padó",
        "R Klinger"
      ],
      "year": "2021",
      "venue": "Multitask learning with sentiment, emotion, and target detection to recognize hate speech and offensive language"
    },
    {
      "citation_id": "7",
      "title": "Efficient emotion recognition from speech using deep learning on spectrograms",
      "authors": [
        "A Satt",
        "S Rozenberg",
        "R Hoory"
      ],
      "year": "2017",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "8",
      "title": "A review on speech emotion recognition using deep learning and attention mechanism",
      "authors": [
        "E Lieskovská",
        "M Jakubec",
        "R Jarina",
        "M Chmulík"
      ],
      "year": "1163",
      "venue": "A review on speech emotion recognition using deep learning and attention mechanism",
      "doi": "10.3390/electronics10101163"
    },
    {
      "citation_id": "9",
      "title": "Speech emotion recognition using spectrogram & phoneme embedding",
      "authors": [
        "P Yenigalla",
        "A Kumar",
        "S Tripathi",
        "C Singh",
        "S Kar",
        "J Vepa"
      ],
      "year": "2018",
      "venue": "Proceedings of the INTERSPEECH"
    },
    {
      "citation_id": "10",
      "title": "Speech emotion recognition using deep convolutional neural network and discriminant temporal pyramid matching",
      "authors": [
        "S Zhang",
        "S Zhang",
        "T Huang",
        "W Gao"
      ],
      "year": "2018",
      "venue": "IEEE Trans. Multimed"
    },
    {
      "citation_id": "11",
      "title": "Speech emotion recognition using deep learning techniques: A review",
      "authors": [
        "R Khalil",
        "E Jones",
        "M Babar",
        "T Jan",
        "M Zafar",
        "T Alhussain"
      ],
      "year": "2019",
      "venue": "Speech emotion recognition using deep learning techniques: A review"
    },
    {
      "citation_id": "12",
      "title": "Multi-scale discrepancy adversarial network for crosscorpus speech emotion recognition",
      "authors": [
        "W Zheng",
        "Y Zong"
      ],
      "year": "2021",
      "venue": "Virtual Real. Intell. Hardw"
    },
    {
      "citation_id": "13",
      "title": "Efficient arabic emotion recognition using deep neural networks",
      "authors": [
        "Y Hifny",
        "A Ali"
      ],
      "year": "2019",
      "venue": "IEEE Intern. Conf. on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "14",
      "title": "Efficient arabic emotion recognition using deep neural networks",
      "authors": [
        "Y Hifny",
        "A Ali"
      ],
      "year": "2019",
      "venue": "IEEE International Conf. on Acoustics, Speech and Signal Processing",
      "doi": "10.1109/ICASSP.2019.8683632"
    },
    {
      "citation_id": "15",
      "title": "Emotion recognition in arabic speech",
      "authors": [
        "S Klaylat",
        "Z Osman",
        "L Hamandi",
        "R Zantout"
      ],
      "year": "2018",
      "venue": "Analog Integr Circ Sig Process"
    },
    {
      "citation_id": "16",
      "title": "Effective speech emotion recognition using deep learning approaches for algerian dialect",
      "authors": [
        "R Cherif",
        "A Moussaoui",
        "N Frahta",
        "M Berrimi"
      ],
      "venue": "Intern. Conf. of Women in Data Science at Taif University (WiDSTaif)"
    },
    {
      "citation_id": "17",
      "title": "Egyptian arabic speech emotion recognition using prosodic, spectral and wavelet features",
      "authors": [
        "L Abdel-Hamid"
      ],
      "year": "2020",
      "venue": "Speech Commun."
    },
    {
      "citation_id": "18",
      "title": "Arabic natural audio dataset",
      "authors": [
        "S Klaylat"
      ],
      "year": "2019",
      "venue": "Arabic natural audio dataset"
    },
    {
      "citation_id": "19",
      "title": "The audiovisual arabic dataset for natural emotions",
      "authors": [
        "F Shaqra",
        "R Duwairi",
        "M Al-Ayyoub"
      ],
      "year": "2019",
      "venue": "7th International Conference on Future Internet of Things and Cloud (FiCloud)"
    },
    {
      "citation_id": "20",
      "title": "Speech emotion recognition based on arabic features",
      "authors": [
        "M Meddeb",
        "H Karray",
        "A Alimi"
      ],
      "year": "2015",
      "venue": "th IEEE International Conference In Intelligent Systems Design and Applications (ISDA)"
    },
    {
      "citation_id": "21",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "H Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2006",
      "venue": "CoRR"
    },
    {
      "citation_id": "22",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units"
    },
    {
      "citation_id": "23",
      "title": "",
      "authors": [
        "Elgeish"
      ],
      "year": "2020",
      "venue": ""
    },
    {
      "citation_id": "24",
      "title": "",
      "authors": [
        "Commonvoice"
      ],
      "year": "2021",
      "venue": ""
    },
    {
      "citation_id": "25",
      "title": "Arabic speech corpus",
      "authors": [
        "N Halabi"
      ],
      "year": "2021",
      "venue": "Arabic speech corpus"
    }
  ]
}