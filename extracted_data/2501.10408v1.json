{
  "paper_id": "2501.10408v1",
  "title": "Leveraging Cross-Attention Transformer And Multi-Feature Fusion For Cross-Linguistic Speech Emotion Recognition",
  "published": "2025-01-06T14:31:25Z",
  "authors": [
    "Ruoyu Zhao",
    "Xiantao Jiang",
    "F. Richard Yu",
    "Victor C. M. Leung",
    "Tao Wang",
    "Shaohu Zhang"
  ],
  "keywords": [
    "Cross-linguistic speech emotion recognition",
    "Multi-feature fusion",
    "Cross-attention transformer"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech Emotion Recognition (SER) plays a crucial role in enhancing human-computer interaction. Cross-Linguistic SER (CLSER) has been a challenging research problem due to significant variability in linguistic and acoustic features of different languages. In this study, we propose a novel approach HuMP-CAT, which combines HuBERT, MFCC, and prosodic characteristics. These features are fused using a cross-attention transformer (CAT) mechanism during feature extraction. Transfer learning is applied to gain from a source emotional speech dataset to the target corpus for emotion recognition. We use IEMOCAP as the source dataset to train the source model and evaluate the proposed method on seven datasets in five languages (e.g., English, German, Spanish, Italian, and Chinese). We show that, by fine-tuning the source model with a small portion of speech from the target datasets, HuMP-CAT achieves an average accuracy of 78.75% across the seven datasets, with notable performance of 88.69% on EMODB (German language) and 79.48% on EMOVO (Italian language). Our extensive evaluation demonstrates that HuMP-CAT outperforms existing methods across multiple target languages.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "In this paper, we present a novel CLSER framework that integrates CAT models and multi-feature fusion. This section reviews the related work on various aspects of existing SER and CLSER approaches. We also highlight the differences and contributions of our proposed framework compared to the existing methods. Table  I  lists the abbreviations and their corresponding meanings that often appear in the paper.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Speech Emotion Recognition",
      "text": "As shown in Fig.  1 , the computational pipeline of SER comprises three critical modules: speech preprocessing, feature extraction, and sentiment classification, each playing a pivotal role in transforming raw acoustic signals into meaningful emotional representations. The preprocessing stage is the prerequisite for the subsequent extraction of features, including speech framing, windowing, normalization, and noise reduction. Feature extraction represents the transformation stage, where preprocessed signals are converted into discriminative emotional representations. This module extracts both time-and frequency-domain features, including prosodic, spectral, pitch, and energy-based features, and passes the extracted feature to the emotion classification module for SER.\n\nTraditional machine learning methods  [5] -  [7] , such as SVM, Random Forest (RF), and Hidden Markov Models (HMM), depend heavily on manually selected features, such as MFCCs, prosodic features (e.g., pitch and energy contours), and statistical models, to classify emotions in speech  [23] . Although effective to some extent, these approaches are limited by the quality and relevance of hand-crafted features, which may fail to capture the complex, high-dimensional nature of speech signals.\n\nIn contrast, deep learning (DL) techniques  [8] -  [10] ,  [24] , including CNNs, Recurrent Neural Networks (RNNs), LSTMs, and Autoencoders, can automatically learn hierarchical feature representations directly from raw or minimally processed speech data. This ability to extract and model intricate temporal and spatial patterns has improved significantly. For example, Wu et al.  [24]  introduced sequential capsule networks that excel in capturing spatial and contextual information, further enhancing the performance of SER.\n\nSSRL is an unsupervised learning designed to extract rich and meaningful representations directly from raw speech signals  [11] -  [14] . Using the structure inherent in speech data, SSRL methods eliminate the need for extensive labeled data sets and have been adopted in SER  [14] . Popular SSRL frameworks, such as HuBERT  [17]  and Wav2vec  [18] , have demonstrated significant potential in SER, offering robust pre-trained representations that enhance downstream model performance, even in resource-constrained emotion dataset. For instance, Zhang et al.  [25]  used an encoder based on Transformer, which can be pretrained by using a large amount of unlabeled audio from various datasets and is able to learn more general and robust acoustic representations. Li et al.  [26]  used contrastive predictive coding to learn important representations in unlabeled datasets for SER.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Cross-Linguistic Ser",
      "text": "CLSER has been a challenge problem in the field of speech processing. Early approaches  [27] -  [29]  relied on prosodic features such as pitch, intensity, and rhythm, which were combined with classical machine learning models such as SVM and RF. While these methods showed initial promise,  they struggled with generalization across languages due to significant variability in linguistic and acoustic features. For instance, Zehra et al.  [29]  trained their model using an Urdu dataset  [30]  and tested it on datasets in other languages, including English, German, and Italian, achieving accuracies ranging from 50% ∼ 60%. With advances in deep learning, Braunschweiler et al.  [31]  applied the CNN-RNN-ATT model, achieving an improved accuracy of 60% to 70%.\n\nRecent studies have also explored the use of self-supervised speech representation learning (SSRL) such as HuBERT  [17]  and WavLM  [32] , which have shown better performance over earlier models such as Wav2Vec2  [18] . However, few studies have combined these representations with Cross-Linguistic strategies. Pepino et al.  [33]  evaluated the performance of Wav2vec2 for SER systems, but did not use a Cross-Linguistic strategy. Unlike traditional spectral and cepstral representations, SSRL models can learn richer and more robust feature embeddings directly from raw audio data, offering promising improvements for SER tasks. Therefore, emerging directions in CLSER include integrating SSRL techniques with domain adaptation or transfer learning methods to bridge the gap between source and target languages. Combining SSRL models with advanced architectures, such as transformer-based systems, may further enhance the performance of CLSER by more effectively capturing acoustic and linguistic variability.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "C. Distinction With Related Work",
      "text": "Unlike previous studies, our proposed framework HuMP-CAT combines the recent SSRL HuBERT model and CAT with two additional sets of speech characteristics including MFCC and prosodic features. This multi-feature fusion enhances the performance of SSRL in CLSER by leveraging complementary information from both traditional and learned representations. In addition, to mitigate complex databases that negatively impact performance, we use IEMOCAP, a large emotion English dataset, as the training corpus. This avoids data scarcity issues while ensuring a robust and representative model pretraining phase.\n\nTo extend the applicability of our system to CLSER, we employ transfer learning to adapt trained models, leveraging both the feature fusion module and the classifier to lowresource target languages. Fine-tuning is then performed on a small subset of the target corpus, allowing the system to learn language-specific characteristics while preserving the generalization capabilities gained during pretraining.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Architecture Design Of Hump-Cat A. Overview",
      "text": "HuMP-CAT is comprised three main components including Feature Extraction, Multi-feature Fusion and Classification. As shown in Fig.  3 , HuMP-CAT extracts the audio feature using the HuBERT model, MFCC, and prosody. The HuBERT model computes a 768-dimensional representation of the audio and passes the speech representation to the convolutional block with a kernel size of 10*18 and a stride of 4*3. CATs are used to fuse the extracted features from the three input blocks and reduce the dimensionality of the input features.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Hubert",
      "text": "Hidden-Unit BERT (HuBERT) is a self-supervised speech representation learning framework  [34] , drawing inspiration from the BERT model  [35]  and Wav2vec2. Its architecture combines a CNN encoder with 12 transformer blocks. Hu-BERT employs the K-means algorithm to perform an offline clustering operation on 39-dimensional MFCC features, along with their first-order (∆) and second-order differences ( ∆∆). The framework calculates the prediction loss using a combination of masked masked Loss m and unmasked Loss u time steps, aligned with the Wav2vec2 approach. The loss of crossentropy for the masked, unmasked and overall time steps is defined as follows:  [34] ,  [36] .\n\nwhere, S ′ denotes the set of indices in the input sequence S that need to be masked, while Z represents the target sequence derived through K-means clustering. The masked embedding sequence is denoted by S if t ∈ S ′ , otherwise t / ∈ S ′ . The overall loss function Loss is a weighted sum of the masked Loss m and unmasked Loss u losses by using a weighted constant λ\n\nHuBERT model outputs 768-dimensional audio representations. During training, random masking is applied to contiguous time steps of the local encoder's representations Labels for pre-training are initially generated by clustering MFCC features using K-means, while later iterations leverage latent features derived from the preceding iterations to refine the target quality. HuMP-CAT leverages the pre-train model trained on 960 hours of the Librispeech dataset and selects the transformer blocks that are most robust for SER as features. Specifically, the 1st and 9th transformer layers representing acoustics and phonetic information are selected  [37] . HuMP-CAT combines these features through attention-based fusion to derive the final HuBERT feature representation.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "C. Cross-Attention Transformer",
      "text": "A Cross-Attention Transformer (CAT) is an architectural component within transformer models that enables different input sequences to interact and exchange information. CAT transfers information from one source to another derived from the concept of self-attention transformers (SAT)  [38] . As illustrated in Fig.  2 , the CAT takes two input sequences including A and B: one sequence serves as the query (Q), while the other provides the keys (K) and values (V ). When Q (A) , K (B) and V (B) are fed into the transformer, it learns the potential representation from R (A) to R (B) , denoted CAT (A2B) . In contrast, when Q (B) , K (A) and V (A) are inputted into another self-attention transformer, the transformer learns the representation from R (B) to R (A) , expressed as CAT (B2A) . The general operation of the CAT is formulated as shown in Equation  4 .\n\nHuMP-CAT applies two stages of CAT operation as shown in Fig.  3 . The detail is illustrated in the following section.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "D. Speech Features",
      "text": "The SER task relies on extracting global and local speech features. Global features capture statistics properties such as maximum, minimum, standard deviation, and mean, while local features use the temporal dynamics of speech, providing a more detailed understanding of the state of the speaker over time  [39] .\n\n1) Prosodic Features: Prosodic information characterizes the intonation and speech rhythm of the speaker  [40] . The most widely used prosodic characteristics are pitch, fundamental frequency, duration, and energy. Prosodic features are more indicative of happiness and anger and less indicative of fear and sadness. Heterogeneous sound features do not affect prosodic features  [41] . Research shows  [42]  that the combination of prosodic characteristics and spectral characteristics has significantly improved SER. We adopt the DisVoice  [40] ,  [43]  prosodic feature extraction tool to obtain features with a dimension of 103, including the feature of fundamental frequency, energy, and voice/unvoiced duration, and their statistic value such as average, standard deviation, maximum, minimum, skewness, and kurtosis. We suggest referring to this website 1  for details.\n\n2) MFCC Features: MFCCs are widely used spectral features in speech processing. We refer to the MFCC feature extraction process from  [44] . The raw speech audio (x(t)) is first normalized to minimize noise and disturbances and then divided into 40-ms frames with a frame shift of 20ms. Each frame is processed using a single Hamming window (H(k)) with a frame length (N) of 30 ms. Next, a discrete Fourier transform is applied to convert the emotional speech signal in the time domain into its frequency domain representation, Y (k), as defined in Equation  5 . The power spectrum of the DFT, representing the characteristics of the vocal tract, is calculated using Equation  6 . Subsequently, the signal is processed through Mel-frequency triangular filter banks, ∇ m (k), as described in Equation  7 . Finally, the discrete cosine transform is applied to the logarithm of the filter bank energy signal to extract L cepstral coefficients, as defined in Equation  8 . Thus, we get 12 MFCC coefficients. As in earlier work  [45] ,  [46]  show that derivative features are important features to characterize speech emotion, we also include 26 first-and second-order derivatives of the MFCC features. In total, we extracted 39 features, including the signal energy.\n\nThe prosodic characteristics (including F0, energy, duration, and their related statistical measures) are passed through 2 fully connected non-linear layers to fit the first input of CAT R (p) . For the MFCC features, we first averaged them and used a Bi-LSTM layer for embedding extraction to obtain the input R (m) .\n\nIn order to improve the generalization performance of HuMP-CAT, the embedding of the prosodic features is combined with the MFCC features through a CAT module, Next, the second CAT module is used to fuse R (pm) with the HuBERT embedding R (h) . The two outputs are interacted through two attention layers to obtain the final representation R.\n\nThe average and variance of the attention output are calculated, which constructs a feature vector of size 64. To classify the embeddings obtained, the AM-Softmax algorithm  [47] is used , which introduces an additional margin to the standard Softmax loss function. The algorithm improves the discriminative power of the model, enabling it to better classify the embeddings of the features into different emotion categories.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Iv. Experiments And Results",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Speech Emotional Datasets",
      "text": "We evaluate seven emotional speech data sets in five different spoken languages (e.g. English, German, Italian, Spanish, and Chinese). Table  II  summarizes the datasets. We adopt four categories of emotions including happiness, sadness, neutrality, and anger, as they are the most common across all datasets. The following provides a detailed description of each dataset.\n\n1) IEMOCAP: The IEMOCAP)  [48]  is an English multimodal emotional data set which comprises 5,531 utterances from 10 speakers (5 male and 5 female). The actors performed selected emotional scripts and performed nine specific forms of emotions. Most studies have selected improvised data with four emotions (happy, neutral, angry, and sad) and considered excitement as happy emotion to better balance the data. we also relabel excitement samples as happiness. 2) RAVDESS: The RAVDESS  [49]  contains 1440 utterance. The database includes 24 professional actors (12 female, 12 male), who voice two lexically matched statements in North American accent. Speech includes calm, happy, sad, angry, fearful, surprised, and disgusted expressions.\n\n3) TESS: Toronto emotional speech set (TESS)  [50]  is an English speech database consisting of 2800 samples from two actresses who expressed seven emotions: anger, disgust, fear, happiness, and sadness.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "4) Emodb:",
      "text": "The Berlin Database of Emotional Speech (EMODB)  [51]  is a German emotional language dataset with ten actors, including five males and five females. The ten actors simulated seven emotions including the neutral, angry, fearful, happy, sad, disgusted, and bored state. The dataset consists of 535 sentences with 233 sentences of male emotion statements and 302 sentences of female emotion statements.\n\n5) EMOVO: EMOVO  [52]  is an Italian speech database consisting of 588 records with seven emotions including happiness, sadness, anger, fear, disgust, surprise, and neutral. Three men and three women uttered 14 sentences for each emotion.\n\n6) MESD: The Mexican Emotional Speech Database (MESD)  [53]  provides single-word utterances in Spanish for anger, disgust, fear, happiness, neutral, and sadness with Mexican cultural shaping. 864 utterances were uttered by nonprofessional actors including 3 female, 2 male, and 6 child.\n\n7) ESD: ESD  [54]  is a emotional speech database developed by the National University of Singapore (NUS) and the Singapore University of Technology and Design (SUTD). It consists of 350 parallel utterances spoken by 10 native English speakers in a controlled acoustic environment, covering 5 emotion categories (e.g., neutral, happy, angry, sad, and surprised).",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Experimental Setup",
      "text": "All audio data sets were resampled at 16 kHz in 16 bits. Each utterance was clipped in 7-second segments, while we padded a short utterance with its repeated parts. We used four common emotions available in all datasets, including happy, neutral, angry, and sad.\n\nThe experiments were carried out on an NVIDIA GeForce RTX 4070Ti GPU. To train the models, we chose crossentropy loss as the loss function and used Adam as the optimizer with an initial learning rate of 1e-3. The models were trained for a maximum of 50 epochs with a batch size of 32. Two evaluation metrics are used for the measurement, including weighted accuracy (WA) and unweighted accuracy (UA).",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "C. Hubert Performance Of Ser On Iemocap",
      "text": "We run the IEMOCAP corpus to compare the SSRL model and speech feature. We use 10-fold cross validation, where 80% of the speakers (4 males and 4 females) are used to train the model. 10% of the speakers (one speaker) are used for validation through training, and the remaining 10% (the other speaker) are used to test the trained model.\n\n1) Comparison with other SSRL models: In order to evaluate the impact of transformer blocks of different speech pre-training models on experimental results, several common SSRL models were used, including HuBERT (HuBERTbase and HuBERT-large), Wav2vec2 (Wav2vec-base and Wav2vec2-large), and WavLM. These models have similar structures, consisting of a CNN encoder and several transformer blocks. These pre-training models were downloaded and called from Huggingface. HuBERT and Wav2vec2 both include base and large versions. The unweighted accuracy results on IEMOCAP are shown in Table  III . The result shows that the HuBERT base model has the highest accuracy of 78. 26%, which outperforms other SSRL models.\n\n2) Effect of speech features: Next, we run the HuBERT base model on different combination of prosodic features, MFCC, LPCC, and spectrogram features. As shown in Table  IV , when prosody and MFCC are combined, the experiment has the highest SER accuracy of 74.26%, which shows that these two features have the bast performance.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "D. Hump-Cat Performance Of Ser On Iemocap",
      "text": "We compare HuMP-CAT with other works running on the IEMOCAP data set with four emotions including happiness, sadness, neutrality, and anger. The result is summarized in Table  V .\n\nLi et al.  [55]  explored the effectiveness of angular softmax loss on two baselines (CRNN and CNN-TF-GAP models) with different class-agnostic margins. The experiments show the class-specific margin on CNN-TF-GAP baseline performs the best.\n\nGA-GRU  [56]  is a graph attention approach in a gated recurrent unit network (GA-GRU), which combines both the long-range attentional time series modeling with the salient frame-wise graph structure within an emotional utterance, and achieves the accuracy of 63.8% WA and 62.27% UA.\n\nHFGM  [57]  is a hierarchical grained and feature mode, is a hierarchical grained feature, which uses RNN to process the frame-level and utterance-level structures of acoustic samples so that it can capture features of different granularities and improve the sensitivity of the model.\n\nHuBERT-LinearLayer  [58]  first applied HuBERT to get representation and then tested on three DNN-based classifiers: Pooling + Linear Layer, CNN with Self-Attention, and CT-Transformer, respectively. The result shows that the Pooling + Linear Layer has a better accuracy of 65.60%. TIM-Net  [59]  extracted the MFCC features by applying the Hamming window to each speech signal with a frame length of 50 ms and a 12.5 ms shift. The 39 extracted features were fed into Bi-Temporal-Aware Blocks (TABs). The 10-fold crossvalidation with 90% and 10% samples in train and test sets was applied.\n\nAMSNet  [60]  applied a spatial CNN with the squeeze-andexcitation block (SCNN) to extract spectrogram representations and an Attention Bi-LSTM to extract temporal features. The training and validation set of the IEMOCAP was split into 80% and 20% of the total samples. The accuracy is 69.22% (WA) and 70.51 % (UA).\n\nWav2Vec2 P-TAPT  [61]  is a Wav2Vec2-based fine-tuning   [55]  64.80 73.33 GA-GRU (2020)  [56]  62.27 63.80 HFGM (2020)  [57]  66.54 70.48 HuBERT-LinearLayer (2022)  [58]  65.60 TIM-Net (2023)  [59]  69 68.29 AMSNet (2023)  [60]  70.51 69.22 Wav2Vec2 P-TAPT (2023)  [61]  74.3 approach using adaptive pseudo-label task pretraining (P-TAPT), which achieved a significant improvement of more than 5% in unweighted accuracy (UA) over state-of-the-art performance on IEMOCAP.\n\nHuMP-CAT achieved the accuracy of the UA of 82.45% and the accuracy of the WA of 81.27%, which outperforms other state-of-the-art learning-based feature fusion and transfer methods as shown in Table  V . Fig.  4  shows the confusion matrix, HuMP-CAT has a higher SER accuracy above 80% for strong emotions (e.g., angry, happy, and sad) except for the neutral emotion with a lower accuracy of 71%. we use this trained model for the following evaluation and comparison.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "E. Hump-Cat Performance For Clser",
      "text": "We adopt a transfer learning approach, fine-tuning the model pre-trained on the IEMOCAP dataset using a small subset of the target datasets. Specifically, we evaluated our model on seven speech emotion datasets: EMODB, MESD, EMOVO, RAVDESS, SAVEE, TESS, and ESD. For fine-tuning, we used 20% of the speakers' speech from EMODB, one third of the speakers' speech from EMOVO, and 10% of the data from the remaining datasets. We compare with other state-of-theart work in the tableVI.\n\nLatif et al.  [62]  applied a generative adversarial network (GAN)-based model to encode the given data into the underlying feature structure and then uses SVM for emotion classification. By including the EMODB, URDU, SAVEE, and EMOVO dataset, the study adopted the one-language dataout approach, and the remaining three corpora are mixed and used to train the model. TableVI lists the best performance of the GAN-SVM approach. Meng et al.  [63]  introduced an architecture ADRNN which applied dilated CNN with residual block and BiLSTM based on the attention mechanism. ADRNN uses the log-mel spectrogram as the feature. The experiment adopted IEMOCAP as a training set and tested the model in the EMODB database, showing an accuracy of 63.84%. The VACNN + BOVW approach  [64]  designed a visual attention convolutional neural network (VACNN) pre-trained with TESS and RAVDESS datasets by extracting the feature on a log-mel spectrogram using a bag of visual words(BOVW). The pretrained model is fine-tuned with the target dataset through five-fold cross validation.  [65]  compared the performance of an ensemble learning approach against traditional machine learning including RF, SVM, and Decision Tree on four corpora (e.g., SAVEE, URDU, EMO-DB, and EMOVO). This study tested a classifier performance trained in one corpus with data from another corpus to evaluate its efficiency for CLSER and showed that ensemble learning performs better. All of the works used either low-level acoustic features or log-mel spectrogram.\n\nWe also compared HuMP-CATwith other SSRL approaches. Zehra et al. Ahn et al.  [66]  introduced Few-shot Learning and Unsupervised Domain Adaptation (FLUDA), which trains an embedding and a metric module to project utterances into a meaningful shared feature space and discern class differences, respectively. During training, an auxiliary module is incorporated to differentiate between real and pseudo-labeled samples. The proposed method uses IEMOCAP and CREMA-D  [67]  as source corpora. In experiments with EMODB as the target corpus, FLUDA achieved an accuracy of 56.8%. MDAT  [68]  is a multimodal model that leverages pre-trained models including RoBERTa  [69]  with wav2vec2-XLS-R  [70]  to extract multi-language speech and text embedding EmoBox  [16]  provided toolkit and benchmark for Multilingual Multicorpus SER. The cross-corpus SER results on 4 datasets (e.g., IEMOCAP, MELD (English), RAVDESS and SAVEE) with fully balanced test sets. The best result with the Whisper large v3  [71]  demonstrates the best performance among other SSRL such as HuBERT base/large, WavLM base/large, data2vec base/large, and data2vec 2.0 base/large.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Table Vi Lists The Performance Of Our Proposed Model",
      "text": "HuMP-CAT, which achieves an impressive average accuracy of 78.75% across the seven diverse data sets. Notably, the ESD test (Chinese) showed a lower accuracy of 60.35%, likely due to significant linguistic and acoustic differences between English and Chinese, but in the worst case, HuMP-CAT still perform better than others. Fig.  5  illustrates the comparison of accuracy between HuMP-CAT and two other representative methods GAN  [62]  and VACNN+BOVW  [64] , on the same target data set. Although the source datasets used by these methods differ, their results provide useful reference points. As shown, HuMP-CAT significantly outperforms GAN and achieves slightly better results than VACNN+BOVW across the three target datasets, highlighting the effectiveness of our proposed approach.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "F. Impact Of Speech Features With Different Dataset On Clser",
      "text": "To assess the effectiveness of HuMP-CAT in incorporating prosodic and MFCC features in CLSER, we compared the performance of the transfer learning method with and without these features, while consistently including the HuBERT model. The training and testing configurations are consistent with those described in the previous section. As summarized in Table VII, the results indicate that HuMP-CAT with both prosodic and MFCC features consistently outperforms the model with one of the two features alone.\n\nV. CONCLUSION In this paper, we proposed a novel CLSER framework, HuMP-CAT, designed for low-resource emotional speech datasets. The model leverages convolutional layers and crossattention mechanisms to effectively fuse three key features: outputs from the HuBERT transformer block, MFCCs, and prosodic features. This fusion enhances the model's adaptability to diverse language datasets. HuMP-CAT was pre-trained on the IEMOCAP corpus as the source domain and finetuned on seven target datasets-EMODB, EMOVO, MESD, TESS, SAVEE, RAVDESS, and ESD-spanning five spoken languages: German, English, Spanish, Chinese, and Italian.\n\nTo evaluate its performance on low-resource speech emotion datasets, we fine-tuned the model using only a small subset of each target dataset. HuMP-CAT achieved an impressive average accuracy of 78.75% across the seven datasets, with notable results such as 88.69% on EMODB and 79.48% on EMOVO. Our extensive evaluation demonstrates that HuMP-CAT outperforms existing methods across multiple target languages, demonstrating its robustness and effectiveness.\n\nFor future work, we aim to expand the source training dataset by including additional emotional speech corpora to further improve the generalization and cross-linguistic adaptability of the model.",
      "page_start": 8,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: General process of SER.",
      "page": 3
    },
    {
      "caption": "Figure 3: The detail is illustrated in the following section.",
      "page": 4
    },
    {
      "caption": "Figure 2: Architecture of Cross-Attention Transformer.",
      "page": 4
    },
    {
      "caption": "Figure 3: Structure of proposed HuMP-CAT.",
      "page": 5
    },
    {
      "caption": "Figure 4: Confusion matrix of HuMP-CAT on IEMOCAP corpus.",
      "page": 7
    },
    {
      "caption": "Figure 5: Comparison of three methods on the same target dataset.",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          ", Fellow,\nVictor C.M. Leung": "Abstract—Speech Emotion Recognition (SER) plays a crucial",
          "IEEE, Tao Wang\n, and Shaohu Zhang": "SER involves on identifying and understanding emotional"
        },
        {
          ", Fellow,\nVictor C.M. Leung": "role in enhancing human-computer interaction. Cross-Linguistic",
          "IEEE, Tao Wang\n, and Shaohu Zhang": ""
        },
        {
          ", Fellow,\nVictor C.M. Leung": "",
          "IEEE, Tao Wang\n, and Shaohu Zhang": "states\nthrough\nvocal\ncues\nsuch\nas\npitch,\nprosody,\nand Mel"
        },
        {
          ", Fellow,\nVictor C.M. Leung": "SER (CLSER) has been a\nchallenging\nresearch\nproblem due",
          "IEEE, Tao Wang\n, and Shaohu Zhang": ""
        },
        {
          ", Fellow,\nVictor C.M. Leung": "",
          "IEEE, Tao Wang\n, and Shaohu Zhang": "frequency cepstral coefficients\n(MFCC)\n[4]. Traditional SER"
        },
        {
          ", Fellow,\nVictor C.M. Leung": "to\nsignificant\nvariability\nin linguistic\nand acoustic\nfeatures\nof",
          "IEEE, Tao Wang\n, and Shaohu Zhang": ""
        },
        {
          ", Fellow,\nVictor C.M. Leung": "",
          "IEEE, Tao Wang\n, and Shaohu Zhang": "systems\nthat\nleverage\nclassical machine\nlearning,\nsuch\nas"
        },
        {
          ", Fellow,\nVictor C.M. Leung": "different\nlanguages. In this study, we propose a novel approach",
          "IEEE, Tao Wang\n, and Shaohu Zhang": ""
        },
        {
          ", Fellow,\nVictor C.M. Leung": "HuMP-CAT, which\ncombines HuBERT, MFCC,\nand\nprosodic",
          "IEEE, Tao Wang\n, and Shaohu Zhang": "support\nvector machine\n(SVM)\n[5],\nnaive Bayes\nclassifier"
        },
        {
          ", Fellow,\nVictor C.M. Leung": "characteristics. These features are fused using a cross-attention",
          "IEEE, Tao Wang\n, and Shaohu Zhang": "[6], and k-nearest neighbor\n(KNN)\n[7], have been limited by"
        },
        {
          ", Fellow,\nVictor C.M. Leung": "transformer (CAT) mechanism during feature extraction. Trans-",
          "IEEE, Tao Wang\n, and Shaohu Zhang": ""
        },
        {
          ", Fellow,\nVictor C.M. Leung": "",
          "IEEE, Tao Wang\n, and Shaohu Zhang": "language-specific constraints and have struggled to generalize"
        },
        {
          ", Fellow,\nVictor C.M. Leung": "fer learning is applied to gain from a source emotional\nspeech",
          "IEEE, Tao Wang\n, and Shaohu Zhang": ""
        },
        {
          ", Fellow,\nVictor C.M. Leung": "",
          "IEEE, Tao Wang\n, and Shaohu Zhang": "emotional\npatterns\nacross\nlinguistic\nspeech. The\nemerging"
        },
        {
          ", Fellow,\nVictor C.M. Leung": "dataset\nto\nthe\ntarget\ncorpus\nfor\nemotion recognition. We use",
          "IEEE, Tao Wang\n, and Shaohu Zhang": ""
        },
        {
          ", Fellow,\nVictor C.M. Leung": "",
          "IEEE, Tao Wang\n, and Shaohu Zhang": "field of cross-linguistic speech emotion recognition (CLSER)"
        },
        {
          ", Fellow,\nVictor C.M. Leung": "IEMOCAP as the source dataset\nto train the source model and",
          "IEEE, Tao Wang\n, and Shaohu Zhang": ""
        },
        {
          ", Fellow,\nVictor C.M. Leung": "evaluate the proposed method on seven datasets in five languages",
          "IEEE, Tao Wang\n, and Shaohu Zhang": "seeks\nto\ntranscend\nthese\nlimitations\nby\ndeveloping\nrobust,"
        },
        {
          ", Fellow,\nVictor C.M. Leung": "(e.g., English, German, Spanish, Italian, and Chinese). We show",
          "IEEE, Tao Wang\n, and Shaohu Zhang": "adaptive models that can interpret emotional states irrespective"
        },
        {
          ", Fellow,\nVictor C.M. Leung": "that, by fine-tuning the\nsource model with a small portion of",
          "IEEE, Tao Wang\n, and Shaohu Zhang": ""
        },
        {
          ", Fellow,\nVictor C.M. Leung": "",
          "IEEE, Tao Wang\n, and Shaohu Zhang": "of\nlinguistic variations."
        },
        {
          ", Fellow,\nVictor C.M. Leung": "speech from the target datasets, HuMP-CAT achieves an average",
          "IEEE, Tao Wang\n, and Shaohu Zhang": ""
        },
        {
          ", Fellow,\nVictor C.M. Leung": "",
          "IEEE, Tao Wang\n, and Shaohu Zhang": "However, CLSER faces\nseveral\nchallenges. The first key"
        },
        {
          ", Fellow,\nVictor C.M. Leung": "accuracy\nof\n78.75% across\nthe\nseven\ndatasets, with\nnotable",
          "IEEE, Tao Wang\n, and Shaohu Zhang": ""
        },
        {
          ", Fellow,\nVictor C.M. Leung": "",
          "IEEE, Tao Wang\n, and Shaohu Zhang": "challenge is the scarcity of large and balanced cross-linguistic"
        },
        {
          ", Fellow,\nVictor C.M. Leung": "performance\nof\n88.69% on EMODB (German\nlanguage)\nand",
          "IEEE, Tao Wang\n, and Shaohu Zhang": ""
        },
        {
          ", Fellow,\nVictor C.M. Leung": "79.48% on EMOVO (Italian language). Our extensive evaluation",
          "IEEE, Tao Wang\n, and Shaohu Zhang": "emotion\ndatasets. Most\nexisting\ndatasets\nare\nheavily\nbi-"
        },
        {
          ", Fellow,\nVictor C.M. Leung": "demonstrates\nthat HuMP-CAT\noutperforms\nexisting methods",
          "IEEE, Tao Wang\n, and Shaohu Zhang": "ased\ntowards\na\nfew languages\nsuch\nas\nEnglish\nand\nEu-"
        },
        {
          ", Fellow,\nVictor C.M. Leung": "across multiple target\nlanguages.",
          "IEEE, Tao Wang\n, and Shaohu Zhang": ""
        },
        {
          ", Fellow,\nVictor C.M. Leung": "",
          "IEEE, Tao Wang\n, and Shaohu Zhang": "ropean\nlanguages, which\ncreate\nsignificant\nrepresentational"
        },
        {
          ", Fellow,\nVictor C.M. Leung": "Index\nTerms—Cross-linguistic\nspeech\nemotion\nrecognition,",
          "IEEE, Tao Wang\n, and Shaohu Zhang": "gaps. The\nsecond challenge\nis\nto identify effective\nfeatures"
        },
        {
          ", Fellow,\nVictor C.M. Leung": "Multi-feature fusion, Cross-attention transformer.",
          "IEEE, Tao Wang\n, and Shaohu Zhang": "for\nemotion\nrecognition. Although\nnumerous\nfeatures\nsuch"
        },
        {
          ", Fellow,\nVictor C.M. Leung": "",
          "IEEE, Tao Wang\n, and Shaohu Zhang": "as\npitch,\nprosody, MFCC,\nlinear\nprediction\ncepstral\ncoeffi-"
        },
        {
          ", Fellow,\nVictor C.M. Leung": "I.\nINTRODUCTION",
          "IEEE, Tao Wang\n, and Shaohu Zhang": "cients (LPCC), and Gammatone frequency cepstral coefficients"
        },
        {
          ", Fellow,\nVictor C.M. Leung": "",
          "IEEE, Tao Wang\n, and Shaohu Zhang": "(GFCC) [3], have been shown to correlate with emotions, it re-"
        },
        {
          ", Fellow,\nVictor C.M. Leung": "S PEECH emotion recognition (SER) has attracted consid-",
          "IEEE, Tao Wang\n, and Shaohu Zhang": "mains to be explored how to combine these features optimally"
        },
        {
          ", Fellow,\nVictor C.M. Leung": "",
          "IEEE, Tao Wang\n, and Shaohu Zhang": "to improve CLSER performance. The third challenge involves"
        },
        {
          ", Fellow,\nVictor C.M. Leung": "eas,\nincluding human-computer interaction, healthcare, educa-",
          "IEEE, Tao Wang\n, and Shaohu Zhang": ""
        },
        {
          ", Fellow,\nVictor C.M. Leung": "",
          "IEEE, Tao Wang\n, and Shaohu Zhang": "improving\nthe\ngeneralization\nand\nversatility\nof\nrecognition"
        },
        {
          ", Fellow,\nVictor C.M. Leung": "tion, entertainment,\ntransportation systems, and the Internet of",
          "IEEE, Tao Wang\n, and Shaohu Zhang": ""
        },
        {
          ", Fellow,\nVictor C.M. Leung": "",
          "IEEE, Tao Wang\n, and Shaohu Zhang": "across\nlanguages. Due\nto\nsignificant\nvariations\nin\nlanguage"
        },
        {
          ", Fellow,\nVictor C.M. Leung": "Things (IoT)\n[1]–[3]. SER enable IoT devices or applications",
          "IEEE, Tao Wang\n, and Shaohu Zhang": ""
        },
        {
          ", Fellow,\nVictor C.M. Leung": "",
          "IEEE, Tao Wang\n, and Shaohu Zhang": "such as accent, culture, and other factors between training and"
        },
        {
          ", Fellow,\nVictor C.M. Leung": "to better understand the user’s emotions and to respond more",
          "IEEE, Tao Wang\n, and Shaohu Zhang": ""
        },
        {
          ", Fellow,\nVictor C.M. Leung": "",
          "IEEE, Tao Wang\n, and Shaohu Zhang": "testing datasets,\nthe majority of SER systems are limited to a"
        },
        {
          ", Fellow,\nVictor C.M. Leung": "appropriately to the user’s psychological\nstate. For example,",
          "IEEE, Tao Wang\n, and Shaohu Zhang": ""
        },
        {
          ", Fellow,\nVictor C.M. Leung": "",
          "IEEE, Tao Wang\n, and Shaohu Zhang": "single\nlanguage\ncorpus but do not generalize well\nto other"
        },
        {
          ", Fellow,\nVictor C.M. Leung": "in\nsmart\nassistants\n(such\nas Siri, Alexa,\netc.),\nif\nanger\nor",
          "IEEE, Tao Wang\n, and Shaohu Zhang": ""
        },
        {
          ", Fellow,\nVictor C.M. Leung": "",
          "IEEE, Tao Wang\n, and Shaohu Zhang": "language corpus."
        },
        {
          ", Fellow,\nVictor C.M. Leung": "happiness\nin the user’s voice can be recognized,\nthe system",
          "IEEE, Tao Wang\n, and Shaohu Zhang": ""
        },
        {
          ", Fellow,\nVictor C.M. Leung": "",
          "IEEE, Tao Wang\n, and Shaohu Zhang": "Recent advances in deep learning and representation learn-"
        },
        {
          ", Fellow,\nVictor C.M. Leung": "can adjust the voice tone and respond appropriately to enhance",
          "IEEE, Tao Wang\n, and Shaohu Zhang": ""
        },
        {
          ", Fellow,\nVictor C.M. Leung": "",
          "IEEE, Tao Wang\n, and Shaohu Zhang": "ing have provided opportunities\nfor SER due to their\nrobust"
        },
        {
          ", Fellow,\nVictor C.M. Leung": "the interactive experience.",
          "IEEE, Tao Wang\n, and Shaohu Zhang": ""
        },
        {
          ", Fellow,\nVictor C.M. Leung": "",
          "IEEE, Tao Wang\n, and Shaohu Zhang": "feature representation capability, capacity to manage complex"
        },
        {
          ", Fellow,\nVictor C.M. Leung": "Ruoyu Zhao and Xiantao Jiang are with the Department of Information En-",
          "IEEE, Tao Wang\n, and Shaohu Zhang": ""
        },
        {
          ", Fellow,\nVictor C.M. Leung": "",
          "IEEE, Tao Wang\n, and Shaohu Zhang": "features, ability to learn from unlabeled data, and scalability"
        },
        {
          ", Fellow,\nVictor C.M. Leung": "gineering, Shanghai Maritime University, NO.1550, Haigang Ave., Shanghai",
          "IEEE, Tao Wang\n, and Shaohu Zhang": ""
        },
        {
          ", Fellow,\nVictor C.M. Leung": "",
          "IEEE, Tao Wang\n, and Shaohu Zhang": "with larger datasets. Various deep learning models,\nsuch as"
        },
        {
          ", Fellow,\nVictor C.M. Leung": "201306, China (email: xtjiang@shmtu.edu.cn).",
          "IEEE, Tao Wang\n, and Shaohu Zhang": ""
        },
        {
          ", Fellow,\nVictor C.M. Leung": "F.\nRichard\nYu\nis\nwith\nthe\nDepartment\nof\nSystems\nand\nComputer",
          "IEEE, Tao Wang\n, and Shaohu Zhang": "convolutional neural networks\n(CNN), deep neural networks"
        },
        {
          ", Fellow,\nVictor C.M. Leung": "Engineering,\nCarleton\nUniversity,\nOttawa,\nON\nKIS\n5B6,\nCanada\n(e-",
          "IEEE, Tao Wang\n, and Shaohu Zhang": ""
        },
        {
          ", Fellow,\nVictor C.M. Leung": "",
          "IEEE, Tao Wang\n, and Shaohu Zhang": "(DNN),\nlong- and short-term memory networks (LSTM) and"
        },
        {
          ", Fellow,\nVictor C.M. Leung": "mail:richard.yu@carleton.ca).",
          "IEEE, Tao Wang\n, and Shaohu Zhang": ""
        },
        {
          ", Fellow,\nVictor C.M. Leung": "",
          "IEEE, Tao Wang\n, and Shaohu Zhang": "transformers, have been applied for automatic SER [8]–[10]."
        },
        {
          ", Fellow,\nVictor C.M. Leung": "V\n. C. M. Leung\nis with\nthe Department\nof Electrical\nand Computer",
          "IEEE, Tao Wang\n, and Shaohu Zhang": ""
        },
        {
          ", Fellow,\nVictor C.M. Leung": "Engineering, The University of British Columbia, Vancouver, BC V6T 1Z4,",
          "IEEE, Tao Wang\n, and Shaohu Zhang": "Self-supervised speech representation learning (SSRL)\nis a"
        },
        {
          ", Fellow,\nVictor C.M. Leung": "Canada (e-mail:vleung@ece.ubc.ca).",
          "IEEE, Tao Wang\n, and Shaohu Zhang": ""
        },
        {
          ", Fellow,\nVictor C.M. Leung": "",
          "IEEE, Tao Wang\n, and Shaohu Zhang": "form of unsupervised learning that seeks to capture rich repre-"
        },
        {
          ", Fellow,\nVictor C.M. Leung": "Tao Wang\nis\nwith\nStanford\nSchool\nof Medicine,\nStanford\nUniver-",
          "IEEE, Tao Wang\n, and Shaohu Zhang": ""
        },
        {
          ", Fellow,\nVictor C.M. Leung": "",
          "IEEE, Tao Wang\n, and Shaohu Zhang": "sentations from the input speech signal\nitself [11]–[13]. which"
        },
        {
          ", Fellow,\nVictor C.M. Leung": "sity,\n450\nJane\nStanford Way\nStanford,\nCalifornia\n94305,\nUSA\n(e-",
          "IEEE, Tao Wang\n, and Shaohu Zhang": ""
        },
        {
          ", Fellow,\nVictor C.M. Leung": "mail:taowang9@stanford.edu).",
          "IEEE, Tao Wang\n, and Shaohu Zhang": "have been widely adopted in various speech processing tasks,"
        },
        {
          ", Fellow,\nVictor C.M. Leung": "Shaohu Zhang\nis with\nthe Department\nof Mathematics\nand Computer",
          "IEEE, Tao Wang\n, and Shaohu Zhang": ""
        },
        {
          ", Fellow,\nVictor C.M. Leung": "",
          "IEEE, Tao Wang\n, and Shaohu Zhang": "including SER [14]–[16]. HuBERT [17]\nand Wav2vec\n[18]"
        },
        {
          ", Fellow,\nVictor C.M. Leung": "Science, The University of North Carolina at Pembroke. 1 University Drive",
          "IEEE, Tao Wang\n, and Shaohu Zhang": ""
        },
        {
          ", Fellow,\nVictor C.M. Leung": "",
          "IEEE, Tao Wang\n, and Shaohu Zhang": "are prominent\nexamples of SSRL methods\napplied in SER."
        },
        {
          ", Fellow,\nVictor C.M. Leung": "Pembroke, North Carolina 28372, USA (e-mail:shaohu.zhang@uncp.edu)",
          "IEEE, Tao Wang\n, and Shaohu Zhang": ""
        },
        {
          ", Fellow,\nVictor C.M. Leung": "Xiaotao Jiang and Shaohu Zhang are the corresponding authors.",
          "IEEE, Tao Wang\n, and Shaohu Zhang": "These approaches have also been successfully applied in mul-"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "attention transformer\n(CAT) models to create a more univer-": ""
        },
        {
          "attention transformer\n(CAT) models to create a more univer-": "sal approach to understanding emotions\nin diverse linguistic"
        },
        {
          "attention transformer\n(CAT) models to create a more univer-": ""
        },
        {
          "attention transformer\n(CAT) models to create a more univer-": "speech. The main contributions of\nthis paper are as follows."
        },
        {
          "attention transformer\n(CAT) models to create a more univer-": ""
        },
        {
          "attention transformer\n(CAT) models to create a more univer-": "• We propose\na novel CLSER framework that\nintegrates"
        },
        {
          "attention transformer\n(CAT) models to create a more univer-": "CAT models and multi-feature fusion to achieve higher"
        },
        {
          "attention transformer\n(CAT) models to create a more univer-": "SER accuracy compared to existing methods."
        },
        {
          "attention transformer\n(CAT) models to create a more univer-": "• HuMP-CAT\nleverages\ntransfer\nlearning\nby\nusing\nonly"
        },
        {
          "attention transformer\n(CAT) models to create a more univer-": "10% or 20% of\nthe speaker data to fine-tune the source"
        },
        {
          "attention transformer\n(CAT) models to create a more univer-": "model,\nachieving\nan\nimpressive\naverage\naccuracy\nof"
        },
        {
          "attention transformer\n(CAT) models to create a more univer-": "78.75% across seven diverse datasets. This demonstrates"
        },
        {
          "attention transformer\n(CAT) models to create a more univer-": "the model’s capacity to generalize SER with small\ntrain-"
        },
        {
          "attention transformer\n(CAT) models to create a more univer-": "ing data."
        },
        {
          "attention transformer\n(CAT) models to create a more univer-": "• We perform a comprehensive evaluation across five lan-"
        },
        {
          "attention transformer\n(CAT) models to create a more univer-": "guages (e.g., English, German, Spanish, Italian, and Chi-"
        },
        {
          "attention transformer\n(CAT) models to create a more univer-": "nese)\nin seven datasets,\nexamining the performance of"
        },
        {
          "attention transformer\n(CAT) models to create a more univer-": "various SSRL models\nand\nthe\ncombination\nof\nspeech"
        },
        {
          "attention transformer\n(CAT) models to create a more univer-": "characteristics. Our\nextensive\nexperiments\ndemonstrate"
        },
        {
          "attention transformer\n(CAT) models to create a more univer-": "HuMP-CAT’s generalization and versatility in CLSER."
        },
        {
          "attention transformer\n(CAT) models to create a more univer-": "The remainder of this paper is organized as follows. Section"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2": "the existing methods. Table I\nlists the abbreviations and their"
        },
        {
          "2": ""
        },
        {
          "2": "corresponding meanings that often appear\nin the paper."
        },
        {
          "2": ""
        },
        {
          "2": ""
        },
        {
          "2": "A.\nSpeech Emotion Recognition"
        },
        {
          "2": ""
        },
        {
          "2": "As\nshown\nin\nFig.1,\nthe\ncomputational\npipeline\nof\nSER"
        },
        {
          "2": ""
        },
        {
          "2": "comprises three critical modules: speech preprocessing, feature"
        },
        {
          "2": ""
        },
        {
          "2": "extraction, and sentiment classification, each playing a pivotal"
        },
        {
          "2": ""
        },
        {
          "2": "role\nin\ntransforming\nraw acoustic\nsignals\ninto meaningful"
        },
        {
          "2": ""
        },
        {
          "2": "emotional\nrepresentations.\nThe\npreprocessing\nstage\nis\nthe"
        },
        {
          "2": ""
        },
        {
          "2": "prerequisite for the subsequent extraction of features, including"
        },
        {
          "2": ""
        },
        {
          "2": "speech framing, windowing, normalization, and noise reduc-"
        },
        {
          "2": "tion. Feature\nextraction represents\nthe\ntransformation stage,"
        },
        {
          "2": "where preprocessed signals are converted into discriminative"
        },
        {
          "2": ""
        },
        {
          "2": "emotional representations. This module extracts both time- and"
        },
        {
          "2": ""
        },
        {
          "2": "frequency-domain features,\nincluding prosodic, spectral, pitch,"
        },
        {
          "2": ""
        },
        {
          "2": "and energy-based features, and passes the extracted feature to"
        },
        {
          "2": ""
        },
        {
          "2": "the emotion classification module for SER."
        },
        {
          "2": ""
        },
        {
          "2": "Traditional machine\nlearning methods\n[5]–[7],\nsuch\nas"
        },
        {
          "2": ""
        },
        {
          "2": "SVM, Random Forest\n(RF),\nand Hidden Markov Models"
        },
        {
          "2": ""
        },
        {
          "2": "(HMM), depend heavily on manually selected features,\nsuch"
        },
        {
          "2": ""
        },
        {
          "2": "as MFCCs, prosodic features (e.g., pitch and energy contours),"
        },
        {
          "2": ""
        },
        {
          "2": "and statistical models,\nto classify emotions in speech [23]. Al-"
        },
        {
          "2": ""
        },
        {
          "2": "though effective to some extent,\nthese approaches are limited"
        },
        {
          "2": ""
        },
        {
          "2": "by the quality and relevance of hand-crafted features, which"
        },
        {
          "2": ""
        },
        {
          "2": "may fail\nto capture the complex, high-dimensional nature of"
        },
        {
          "2": ""
        },
        {
          "2": "speech signals."
        },
        {
          "2": ""
        },
        {
          "2": "In contrast, deep learning (DL)\ntechniques\n[8]–[10],\n[24],"
        },
        {
          "2": ""
        },
        {
          "2": "including CNNs, Recurrent Neural Networks (RNNs), LSTMs,"
        },
        {
          "2": ""
        },
        {
          "2": "and Autoencoders, can automatically learn hierarchical feature"
        },
        {
          "2": "representations\ndirectly\nfrom raw or minimally\nprocessed"
        },
        {
          "2": "speech data. This ability to extract and model\nintricate tempo-"
        },
        {
          "2": "ral and spatial patterns has improved significantly. For exam-"
        },
        {
          "2": "ple, Wu et al. [24] introduced sequential capsule networks that"
        },
        {
          "2": "excel\nin capturing spatial and contextual\ninformation,\nfurther"
        },
        {
          "2": "enhancing the performance of SER."
        },
        {
          "2": "SSRL is an unsupervised learning designed to extract\nrich"
        },
        {
          "2": "and meaningful\nrepresentations directly from raw speech sig-"
        },
        {
          "2": "nals\n[11]–[14]. Using the\nstructure\ninherent\nin speech data,"
        },
        {
          "2": "SSRL methods eliminate the need for extensive labeled data"
        },
        {
          "2": "sets\nand\nhave\nbeen\nadopted\nin\nSER [14].\nPopular\nSSRL"
        },
        {
          "2": "frameworks,\nsuch as HuBERT [17] and Wav2vec [18], have"
        },
        {
          "2": "demonstrated\nsignificant\npotential\nin\nSER,\noffering\nrobust"
        },
        {
          "2": "pre-trained\nrepresentations\nthat\nenhance\ndownstream model"
        },
        {
          "2": "performance,\neven\nin\nresource-constrained\nemotion\ndataset."
        },
        {
          "2": "For\ninstance, Zhang\net\nal.\n[25]\nused\nan\nencoder\nbased\non"
        },
        {
          "2": "Transformer, which can be pretrained by using a large amount"
        },
        {
          "2": "of unlabeled audio from various datasets and is able to learn"
        },
        {
          "2": "more\ngeneral\nand\nrobust\nacoustic\nrepresentations. Li\net\nal."
        },
        {
          "2": "[26]\nused\ncontrastive\npredictive\ncoding\nto\nlearn\nimportant"
        },
        {
          "2": "representations in unlabeled datasets for SER."
        },
        {
          "2": "B. Cross-Linguistic SER"
        },
        {
          "2": "CLSER has been a challenge problem in the field of speech"
        },
        {
          "2": "processing. Early\napproaches\n[27]–[29]\nrelied\non\nprosodic"
        },
        {
          "2": "features\nsuch\nas\npitch,\nintensity,\nand\nrhythm, which were"
        },
        {
          "2": "combined with\nclassical machine\nlearning models\nsuch\nas"
        },
        {
          "2": "SVM and RF. While these methods\nshowed initial promise,"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Speech": "Processing",
          "Features": "Extraction",
          "Emotion": "Classification"
        },
        {
          "Speech": "Framing",
          "Features": "Audio signal",
          "Emotion": "Classical Machine Learning"
        },
        {
          "Speech": "",
          "Features": "Prosodic",
          "Emotion": "Deep Learning Classifier"
        },
        {
          "Speech": "Voice Activity Detector",
          "Features": "Spectral",
          "Emotion": "Transformer"
        },
        {
          "Speech": "Normalization",
          "Features": "Pitch",
          "Emotion": ""
        },
        {
          "Speech": "Noise Reduction",
          "Features": "Signal Energy",
          "Emotion": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Voice Activity Detector\nSpectral\n•\n•": "Normalization\nPitch\n•\n•",
          "Transformer\n•": ""
        },
        {
          "Voice Activity Detector\nSpectral\n•\n•": "Noise Reduction\n•\n•",
          "Transformer\n•": "Signal Energy"
        },
        {
          "Voice Activity Detector\nSpectral\n•\n•": "Fig. 1. General process of SER.",
          "Transformer\n•": ""
        },
        {
          "Voice Activity Detector\nSpectral\n•\n•": "they\nstruggled with\ngeneralization\nacross\nlanguages\ndue\nto",
          "Transformer\n•": "both\nthe\nfeature\nfusion module\nand\nthe\nclassifier\nto\nlow-"
        },
        {
          "Voice Activity Detector\nSpectral\n•\n•": "significant variability in linguistic and acoustic features. For",
          "Transformer\n•": "resource target\nlanguages. Fine-tuning is\nthen performed on"
        },
        {
          "Voice Activity Detector\nSpectral\n•\n•": "instance, Zehra et al.\n[29]\ntrained their model using an Urdu",
          "Transformer\n•": "a\nsmall\nsubset of\nthe\ntarget\ncorpus,\nallowing the\nsystem to"
        },
        {
          "Voice Activity Detector\nSpectral\n•\n•": "dataset\n[30]\nand\ntested\nit\non\ndatasets\nin\nother\nlanguages,",
          "Transformer\n•": "learn\nlanguage-specific\ncharacteristics while\npreserving\nthe"
        },
        {
          "Voice Activity Detector\nSpectral\n•\n•": "including English, German, and Italian, achieving accuracies",
          "Transformer\n•": "generalization capabilities gained during pretraining."
        },
        {
          "Voice Activity Detector\nSpectral\n•\n•": "ranging from 50% ∼ 60%. With advances\nin deep learning,",
          "Transformer\n•": ""
        },
        {
          "Voice Activity Detector\nSpectral\n•\n•": "Braunschweiler et al. [31] applied the CNN-RNN-ATT model,",
          "Transformer\n•": "III. ARCHITECTURE DESIGN OF HuMP-CAT"
        },
        {
          "Voice Activity Detector\nSpectral\n•\n•": "achieving an improved accuracy of 60% to 70%.",
          "Transformer\n•": ""
        },
        {
          "Voice Activity Detector\nSpectral\n•\n•": "",
          "Transformer\n•": "A. Overview"
        },
        {
          "Voice Activity Detector\nSpectral\n•\n•": "Recent studies have also explored the use of self-supervised",
          "Transformer\n•": ""
        },
        {
          "Voice Activity Detector\nSpectral\n•\n•": "",
          "Transformer\n•": "HuMP-CAT is comprised three main components including"
        },
        {
          "Voice Activity Detector\nSpectral\n•\n•": "speech representation learning (SSRL) such as HuBERT [17]",
          "Transformer\n•": ""
        },
        {
          "Voice Activity Detector\nSpectral\n•\n•": "",
          "Transformer\n•": "Feature Extraction, Multi-feature Fusion and Classification. As"
        },
        {
          "Voice Activity Detector\nSpectral\n•\n•": "and WavLM [32], which have shown better performance over",
          "Transformer\n•": ""
        },
        {
          "Voice Activity Detector\nSpectral\n•\n•": "",
          "Transformer\n•": "shown in Fig.3, HuMP-CAT extracts\nthe audio feature using"
        },
        {
          "Voice Activity Detector\nSpectral\n•\n•": "earlier models such as Wav2Vec2 [18]. However,\nfew studies",
          "Transformer\n•": ""
        },
        {
          "Voice Activity Detector\nSpectral\n•\n•": "",
          "Transformer\n•": "the HuBERT model, MFCC, and prosody. The HuBERT model"
        },
        {
          "Voice Activity Detector\nSpectral\n•\n•": "have\ncombined\nthese\nrepresentations with Cross-Linguistic",
          "Transformer\n•": ""
        },
        {
          "Voice Activity Detector\nSpectral\n•\n•": "",
          "Transformer\n•": "computes a 768-dimensional\nrepresentation of\nthe audio and"
        },
        {
          "Voice Activity Detector\nSpectral\n•\n•": "strategies. Pepino\net\nal.\n[33]\nevaluated\nthe\nperformance\nof",
          "Transformer\n•": ""
        },
        {
          "Voice Activity Detector\nSpectral\n•\n•": "",
          "Transformer\n•": "passes\nthe\nspeech representation to the\nconvolutional block"
        },
        {
          "Voice Activity Detector\nSpectral\n•\n•": "Wav2vec2 for SER systems, but did not use a Cross-Linguistic",
          "Transformer\n•": ""
        },
        {
          "Voice Activity Detector\nSpectral\n•\n•": "",
          "Transformer\n•": "with a kernel\nsize of 10*18 and a\nstride of 4*3. CATs\nare"
        },
        {
          "Voice Activity Detector\nSpectral\n•\n•": "strategy. Unlike\ntraditional\nspectral\nand cepstral\nrepresenta-",
          "Transformer\n•": ""
        },
        {
          "Voice Activity Detector\nSpectral\n•\n•": "",
          "Transformer\n•": "used to fuse the extracted features from the three input blocks"
        },
        {
          "Voice Activity Detector\nSpectral\n•\n•": "tions, SSRL models can learn richer and more robust\nfeature",
          "Transformer\n•": ""
        },
        {
          "Voice Activity Detector\nSpectral\n•\n•": "",
          "Transformer\n•": "and reduce the dimensionality of\nthe input\nfeatures."
        },
        {
          "Voice Activity Detector\nSpectral\n•\n•": "embeddings directly from raw audio data, offering promising",
          "Transformer\n•": ""
        },
        {
          "Voice Activity Detector\nSpectral\n•\n•": "improvements\nfor SER tasks. Therefore, emerging directions",
          "Transformer\n•": ""
        },
        {
          "Voice Activity Detector\nSpectral\n•\n•": "",
          "Transformer\n•": "B. HuBERT"
        },
        {
          "Voice Activity Detector\nSpectral\n•\n•": "in CLSER include integrating SSRL techniques with domain",
          "Transformer\n•": ""
        },
        {
          "Voice Activity Detector\nSpectral\n•\n•": "adaptation\nor\ntransfer\nlearning methods\nto\nbridge\nthe\ngap",
          "Transformer\n•": "Hidden-Unit BERT (HuBERT)\nis a self-supervised speech"
        },
        {
          "Voice Activity Detector\nSpectral\n•\n•": "between source and target\nlanguages. Combining SSRL mod-",
          "Transformer\n•": "representation learning framework [34], drawing inspiration"
        },
        {
          "Voice Activity Detector\nSpectral\n•\n•": "els with\nadvanced\narchitectures,\nsuch\nas\ntransformer-based",
          "Transformer\n•": "from the BERT model\n[35]\nand Wav2vec2.\nIts\narchitecture"
        },
        {
          "Voice Activity Detector\nSpectral\n•\n•": "systems, may further enhance the performance of CLSER by",
          "Transformer\n•": "combines\na CNN encoder with 12 transformer blocks. Hu-"
        },
        {
          "Voice Activity Detector\nSpectral\n•\n•": "more effectively capturing acoustic and linguistic variability.",
          "Transformer\n•": "BERT employs the K-means algorithm to perform an offline"
        },
        {
          "Voice Activity Detector\nSpectral\n•\n•": "",
          "Transformer\n•": "clustering operation on 39-dimensional MFCC features, along"
        },
        {
          "Voice Activity Detector\nSpectral\n•\n•": "C. Distinction with Related Work",
          "Transformer\n•": "with their first-order (∆) and second-order differences ( ∆∆)."
        },
        {
          "Voice Activity Detector\nSpectral\n•\n•": "Unlike previous\nstudies, our proposed framework HuMP-",
          "Transformer\n•": "The framework calculates the prediction loss using a combi-"
        },
        {
          "Voice Activity Detector\nSpectral\n•\n•": "CAT combines the recent SSRL HuBERT model and CAT with",
          "Transformer\n•": "nation of masked masked Lossm and unmasked Lossu time"
        },
        {
          "Voice Activity Detector\nSpectral\n•\n•": "two additional sets of speech characteristics including MFCC",
          "Transformer\n•": "steps, aligned with the Wav2vec2 approach. The loss of cross-"
        },
        {
          "Voice Activity Detector\nSpectral\n•\n•": "and prosodic features. This multi-feature fusion enhances the",
          "Transformer\n•": "entropy for\nthe masked, unmasked and overall\ntime steps\nis"
        },
        {
          "Voice Activity Detector\nSpectral\n•\n•": "performance of SSRL in CLSER by leveraging complementary",
          "Transformer\n•": "defined as follows:\n[34],\n[36]."
        },
        {
          "Voice Activity Detector\nSpectral\n•\n•": "information from both traditional and learned representations.",
          "Transformer\n•": ""
        },
        {
          "Voice Activity Detector\nSpectral\n•\n•": "",
          "Transformer\n•": "(1)\nLossm(f, S, S′, Z) = Σt∈S ′ log pf (zt| ˜S, t)"
        },
        {
          "Voice Activity Detector\nSpectral\n•\n•": "In addition,\nto mitigate complex databases that negatively im-",
          "Transformer\n•": ""
        },
        {
          "Voice Activity Detector\nSpectral\n•\n•": "pact performance, we use IEMOCAP, a large emotion English",
          "Transformer\n•": ""
        },
        {
          "Voice Activity Detector\nSpectral\n•\n•": "dataset, as the training corpus. This avoids data scarcity issues",
          "Transformer\n•": "(2)\nLossu(f, S, S′, Z) = Σt̸∈S ′ log pf (zt| ˜S, t)"
        },
        {
          "Voice Activity Detector\nSpectral\n•\n•": "while ensuring a robust and representative model pretraining",
          "Transformer\n•": ""
        },
        {
          "Voice Activity Detector\nSpectral\n•\n•": "",
          "Transformer\n•": "(3)\nLoss = λLossm + (1 − λ)Lossu"
        },
        {
          "Voice Activity Detector\nSpectral\n•\n•": "phase.",
          "Transformer\n•": ""
        },
        {
          "Voice Activity Detector\nSpectral\n•\n•": "To extend the\napplicability of our\nsystem to CLSER, we",
          "Transformer\n•": "where, S′\ndenotes\nthe\nset of\nindices\nin the\ninput\nsequence"
        },
        {
          "Voice Activity Detector\nSpectral\n•\n•": "employ transfer\nlearning to adapt\ntrained models,\nleveraging",
          "Transformer\n•": "S\nthat\nneed\nto\nbe masked, while Z represents\nthe\ntarget"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": "Fig. 2.\nArchitecture of Cross-Attention Transformer."
        },
        {
          "4": ""
        },
        {
          "4": "We adopt the DisVoice [40], [43] prosodic feature extraction"
        },
        {
          "4": ""
        },
        {
          "4": "tool\nto obtain features with a dimension of 103,\nincluding the"
        },
        {
          "4": ""
        },
        {
          "4": "feature of\nfundamental\nfrequency, energy, and voice/unvoiced"
        },
        {
          "4": ""
        },
        {
          "4": "duration,\nand their\nstatistic value\nsuch as\naverage,\nstandard"
        },
        {
          "4": ""
        },
        {
          "4": "deviation, maximum, minimum,\nskewness, and kurtosis. We"
        },
        {
          "4": ""
        },
        {
          "4": "suggest\nreferring to this website 1\nfor details."
        },
        {
          "4": ""
        },
        {
          "4": "2) MFCC Features: MFCCs are widely used spectral\nfea-"
        },
        {
          "4": ""
        },
        {
          "4": "tures\nin speech processing. We\nrefer\nto the MFCC feature"
        },
        {
          "4": ""
        },
        {
          "4": "extraction\nprocess\nfrom [44]. The\nraw speech\naudio\n(x(t))"
        },
        {
          "4": ""
        },
        {
          "4": "is first normalized to minimize noise\nand disturbances\nand"
        },
        {
          "4": ""
        },
        {
          "4": "then\ndivided\ninto\n40-ms\nframes with\na\nframe\nshift\nof\n20-"
        },
        {
          "4": ""
        },
        {
          "4": "ms. Each frame\nis processed using a\nsingle Hamming win-"
        },
        {
          "4": ""
        },
        {
          "4": "dow (H(k)) with\na\nframe\nlength\n(N)\nof\n30 ms. Next,\na"
        },
        {
          "4": ""
        },
        {
          "4": "discrete Fourier\ntransform is applied to convert\nthe emotional"
        },
        {
          "4": ""
        },
        {
          "4": "speech signal\nin the time domain into its\nfrequency domain"
        },
        {
          "4": ""
        },
        {
          "4": "representation, Y (k),\nas defined in Equation 5. The power"
        },
        {
          "4": "spectrum of\nthe DFT,\nrepresenting the characteristics of\nthe"
        },
        {
          "4": ""
        },
        {
          "4": "vocal\ntract,\nis calculated using Equation 6. Subsequently,\nthe"
        },
        {
          "4": "signal\nis\nprocessed\nthrough Mel-frequency\ntriangular\nfilter"
        },
        {
          "4": ""
        },
        {
          "4": "banks, ∇m(k), as described in Equation 7. Finally, the discrete"
        },
        {
          "4": "cosine transform is applied to the logarithm of\nthe filter bank"
        },
        {
          "4": "energy signal\nto extract L cepstral coefficients, as defined in"
        },
        {
          "4": ""
        },
        {
          "4": "Equation 8."
        },
        {
          "4": ""
        },
        {
          "4": "N −1"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "EMOVO [52]\nItalian": "MESD [53]\nSpanish",
          "6 (3 M & 3 F)": "11 (2 F, 3 M, and 6 children)",
          "588": "864",
          "7": "5"
        },
        {
          "EMOVO [52]\nItalian": "ESD [54]\nChinese",
          "6 (3 M & 3 F)": "10",
          "588": "420",
          "7": "5"
        },
        {
          "EMOVO [52]\nItalian": "Thus, we get 12 MFCC coefficients. As\nin earlier work",
          "6 (3 M & 3 F)": "",
          "588": "",
          "7": ""
        },
        {
          "EMOVO [52]\nItalian": "[45],\n[46] show that derivative features are important\nfeatures",
          "6 (3 M & 3 F)": "",
          "588": "Audio",
          "7": ""
        },
        {
          "EMOVO [52]\nItalian": "to characterize speech emotion, we also include 26 first- and",
          "6 (3 M & 3 F)": "",
          "588": "",
          "7": ""
        },
        {
          "EMOVO [52]\nItalian": "second-order derivatives of\nthe MFCC features.\nIn total, we",
          "6 (3 M & 3 F)": "",
          "588": "",
          "7": ""
        },
        {
          "EMOVO [52]\nItalian": "",
          "6 (3 M & 3 F)": "",
          "588": "",
          "7": "MFCC"
        },
        {
          "EMOVO [52]\nItalian": "extracted 39 features,\nincluding the signal energy.",
          "6 (3 M & 3 F)": "",
          "588": "",
          "7": ""
        },
        {
          "EMOVO [52]\nItalian": "The prosodic characteristics (including F0, energy, duration,",
          "6 (3 M & 3 F)": "",
          "588": "",
          "7": ""
        },
        {
          "EMOVO [52]\nItalian": "and their\nrelated statistical measures)\nare passed through 2",
          "6 (3 M & 3 F)": "",
          "588": "",
          "7": "Pooling"
        },
        {
          "EMOVO [52]\nItalian": "fully connected non-linear\nlayers to fit\nthe first\ninput of CAT",
          "6 (3 M & 3 F)": "",
          "588": "",
          "7": ""
        },
        {
          "EMOVO [52]\nItalian": "R(p). For the MFCC features, we first averaged them and used",
          "6 (3 M & 3 F)": "",
          "588": "",
          "7": ""
        },
        {
          "EMOVO [52]\nItalian": "",
          "6 (3 M & 3 F)": "",
          "588": "",
          "7": "Bi-LSTM"
        },
        {
          "EMOVO [52]\nItalian": "a Bi-LSTM layer for embedding extraction to obtain the input",
          "6 (3 M & 3 F)": "",
          "588": "",
          "7": ""
        },
        {
          "EMOVO [52]\nItalian": "R(m).",
          "6 (3 M & 3 F)": "",
          "588": "",
          "7": ""
        },
        {
          "EMOVO [52]\nItalian": "",
          "6 (3 M & 3 F)": "",
          "588": "",
          "7": ""
        },
        {
          "EMOVO [52]\nItalian": "In\norder\nto\nimprove\nthe\ngeneralization\nperformance\nof",
          "6 (3 M & 3 F)": "",
          "588": "",
          "7": ""
        },
        {
          "EMOVO [52]\nItalian": "HuMP-CAT,\nthe embedding of\nthe prosodic features is com-",
          "6 (3 M & 3 F)": "",
          "588": "",
          "7": ""
        },
        {
          "EMOVO [52]\nItalian": "bined with the MFCC features through a CAT module, Next,",
          "6 (3 M & 3 F)": "",
          "588": "",
          "7": ""
        },
        {
          "EMOVO [52]\nItalian": "the\nsecond CAT module\nis\nused\nto\nfuse R(pm) with\nthe",
          "6 (3 M & 3 F)": "",
          "588": "",
          "7": ""
        },
        {
          "EMOVO [52]\nItalian": "HuBERT embedding R(h). The\ntwo\noutputs\nare\ninteracted",
          "6 (3 M & 3 F)": "",
          "588": "",
          "7": "CAT"
        },
        {
          "EMOVO [52]\nItalian": "through two attention layers to obtain the final\nrepresentation",
          "6 (3 M & 3 F)": "",
          "588": "",
          "7": ""
        },
        {
          "EMOVO [52]\nItalian": "R.",
          "6 (3 M & 3 F)": "",
          "588": "",
          "7": ""
        },
        {
          "EMOVO [52]\nItalian": "R(pm) = CAT (R(p), R(m))\n(9)",
          "6 (3 M & 3 F)": "",
          "588": "",
          "7": ""
        },
        {
          "EMOVO [52]\nItalian": "",
          "6 (3 M & 3 F)": "",
          "588": "",
          "7": ""
        },
        {
          "EMOVO [52]\nItalian": "R = CAT (R(h), R(pm))\n(10)",
          "6 (3 M & 3 F)": "",
          "588": "",
          "7": ""
        },
        {
          "EMOVO [52]\nItalian": "The average and variance of the attention output are calcu-",
          "6 (3 M & 3 F)": "",
          "588": "",
          "7": "Concatenate"
        },
        {
          "EMOVO [52]\nItalian": "lated, which constructs a feature vector of size 64. To classify",
          "6 (3 M & 3 F)": "",
          "588": "",
          "7": ""
        },
        {
          "EMOVO [52]\nItalian": "",
          "6 (3 M & 3 F)": "",
          "588": "",
          "7": "FC & softmax"
        },
        {
          "EMOVO [52]\nItalian": "the\nembeddings obtained,\nthe AM-Softmax algorithm [47]is",
          "6 (3 M & 3 F)": "",
          "588": "",
          "7": ""
        },
        {
          "EMOVO [52]\nItalian": "used , which introduces an additional margin to the standard",
          "6 (3 M & 3 F)": "",
          "588": "",
          "7": ""
        },
        {
          "EMOVO [52]\nItalian": "",
          "6 (3 M & 3 F)": "",
          "588": "",
          "7": ""
        },
        {
          "EMOVO [52]\nItalian": "Softmax loss function. The algorithm improves the discrimi-",
          "6 (3 M & 3 F)": "",
          "588": "",
          "7": ""
        },
        {
          "EMOVO [52]\nItalian": "native power of\nthe model, enabling it\nto better classify the",
          "6 (3 M & 3 F)": "",
          "588": "",
          "7": ""
        },
        {
          "EMOVO [52]\nItalian": "embeddings of the features into different emotion categories.",
          "6 (3 M & 3 F)": "",
          "588": "",
          "7": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "embeddings of the features into different emotion categories.": ""
        },
        {
          "embeddings of the features into different emotion categories.": "IV. EXPERIMENTS AND RESULTS"
        },
        {
          "embeddings of the features into different emotion categories.": "A. Speech Emotional Datasets"
        },
        {
          "embeddings of the features into different emotion categories.": ""
        },
        {
          "embeddings of the features into different emotion categories.": "We evaluate seven emotional speech data sets in five differ-"
        },
        {
          "embeddings of the features into different emotion categories.": ""
        },
        {
          "embeddings of the features into different emotion categories.": "ent spoken languages (e.g. English, German,\nItalian, Spanish,"
        },
        {
          "embeddings of the features into different emotion categories.": ""
        },
        {
          "embeddings of the features into different emotion categories.": "and Chinese). Table\nII\nsummarizes\nthe\ndatasets. We\nadopt"
        },
        {
          "embeddings of the features into different emotion categories.": ""
        },
        {
          "embeddings of the features into different emotion categories.": "four\ncategories\nof\nemotions\nincluding\nhappiness,\nsadness,"
        },
        {
          "embeddings of the features into different emotion categories.": ""
        },
        {
          "embeddings of the features into different emotion categories.": "neutrality,\nand anger,\nas\nthey are\nthe most\ncommon across"
        },
        {
          "embeddings of the features into different emotion categories.": ""
        },
        {
          "embeddings of the features into different emotion categories.": "all datasets. The following provides a detailed description of"
        },
        {
          "embeddings of the features into different emotion categories.": ""
        },
        {
          "embeddings of the features into different emotion categories.": "each dataset."
        },
        {
          "embeddings of the features into different emotion categories.": ""
        },
        {
          "embeddings of the features into different emotion categories.": "1)\nIEMOCAP:\nThe\nIEMOCAP)\n[48]\nis\nan English mul-"
        },
        {
          "embeddings of the features into different emotion categories.": "timodal emotional data set which comprises 5,531 utterances"
        },
        {
          "embeddings of the features into different emotion categories.": "from 10 speakers (5 male and 5 female). The actors performed"
        },
        {
          "embeddings of the features into different emotion categories.": "selected emotional scripts and performed nine specific forms"
        },
        {
          "embeddings of the features into different emotion categories.": "of emotions. Most studies have selected improvised data with"
        },
        {
          "embeddings of the features into different emotion categories.": "four emotions (happy, neutral, angry, and sad) and considered"
        },
        {
          "embeddings of the features into different emotion categories.": "excitement as happy emotion to better balance the data. we"
        },
        {
          "embeddings of the features into different emotion categories.": "also relabel excitement samples as happiness."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "",
          "MFCC + LPCC\n68.57\n65.39": "MFCC + spectrogram\n67.32\n62.24"
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "",
          "MFCC + LPCC\n68.57\n65.39": "LPCC + spectrogram\n64.82\n60.09"
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "B. Experimental setup",
          "MFCC + LPCC\n68.57\n65.39": ""
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "All audio data sets were resampled at 16 kHz in 16 bits.",
          "MFCC + LPCC\n68.57\n65.39": ""
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "Each utterance was clipped in the 7-second segments, while we",
          "MFCC + LPCC\n68.57\n65.39": ""
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "",
          "MFCC + LPCC\n68.57\n65.39": "D. HuMP-CAT Performance of SER on IEMOCAP"
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "padded a short utterance with its repeated parts. We used four",
          "MFCC + LPCC\n68.57\n65.39": ""
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "",
          "MFCC + LPCC\n68.57\n65.39": "We compare HuMP-CAT with other works running on the"
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "common emotions available in all datasets,\nincluding happy,",
          "MFCC + LPCC\n68.57\n65.39": ""
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "",
          "MFCC + LPCC\n68.57\n65.39": "IEMOCAP data set with four emotions\nincluding happiness,"
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "neutral, angry, and sad.",
          "MFCC + LPCC\n68.57\n65.39": ""
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "",
          "MFCC + LPCC\n68.57\n65.39": "sadness, neutrality,\nand anger. The\nresult\nis\nsummarized in"
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "The experiments were carried out on an NVIDIA GeForce",
          "MFCC + LPCC\n68.57\n65.39": ""
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "",
          "MFCC + LPCC\n68.57\n65.39": "Table V."
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "RTX 4070Ti GPU. To\ntrain\nthe models, we\nchose\ncross-",
          "MFCC + LPCC\n68.57\n65.39": ""
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "",
          "MFCC + LPCC\n68.57\n65.39": "Li et al.\n[55] explored the effectiveness of angular softmax"
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "entropy\nloss\nas\nthe\nloss\nfunction\nand\nused Adam as\nthe",
          "MFCC + LPCC\n68.57\n65.39": ""
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "",
          "MFCC + LPCC\n68.57\n65.39": "loss on two baselines (CRNN and CNN-TF-GAP models) with"
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "optimizer with an initial\nlearning rate of 1e-3. The models",
          "MFCC + LPCC\n68.57\n65.39": ""
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "",
          "MFCC + LPCC\n68.57\n65.39": "different\nclass-agnostic margins. The\nexperiments\nshow the"
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "were trained for a maximum of 50 epochs with a batch size",
          "MFCC + LPCC\n68.57\n65.39": ""
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "",
          "MFCC + LPCC\n68.57\n65.39": "class-specific margin on CNN-TF-GAP baseline performs the"
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "of 32. Two evaluation metrics are used for\nthe measurement,",
          "MFCC + LPCC\n68.57\n65.39": ""
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "",
          "MFCC + LPCC\n68.57\n65.39": "best."
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "including weighted accuracy (WA) and unweighted accuracy",
          "MFCC + LPCC\n68.57\n65.39": ""
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "",
          "MFCC + LPCC\n68.57\n65.39": "GA-GRU [56]\nis\na\ngraph\nattention\napproach\nin\na\ngated"
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "(UA).",
          "MFCC + LPCC\n68.57\n65.39": ""
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "",
          "MFCC + LPCC\n68.57\n65.39": "recurrent unit network (GA-GRU), which combines both the"
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "C. HuBERT Performance of SER on IEMOCAP",
          "MFCC + LPCC\n68.57\n65.39": "long-range\nattentional\ntime\nseries modeling with the\nsalient"
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "",
          "MFCC + LPCC\n68.57\n65.39": "frame-wise graph structure within an emotional utterance, and"
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "We run the IEMOCAP corpus to compare the SSRL model",
          "MFCC + LPCC\n68.57\n65.39": ""
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "",
          "MFCC + LPCC\n68.57\n65.39": "achieves the accuracy of 63.8% WA and 62.27% UA."
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "and speech feature. We use 10-fold cross validation, where",
          "MFCC + LPCC\n68.57\n65.39": ""
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "",
          "MFCC + LPCC\n68.57\n65.39": "HFGM [57]\nis a hierarchical grained and feature mode,\nis"
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "80% of the speakers (4 males and 4 females) are used to train",
          "MFCC + LPCC\n68.57\n65.39": ""
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "",
          "MFCC + LPCC\n68.57\n65.39": "a hierarchical grained feature, which uses RNN to process the"
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "the model. 10% of\nthe\nspeakers\n(one\nspeaker)\nare used for",
          "MFCC + LPCC\n68.57\n65.39": ""
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "",
          "MFCC + LPCC\n68.57\n65.39": "frame-level and utterance-level structures of acoustic samples"
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "validation through training, and the remaining 10% (the other",
          "MFCC + LPCC\n68.57\n65.39": ""
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "",
          "MFCC + LPCC\n68.57\n65.39": "so that\nit\ncan capture\nfeatures of different granularities\nand"
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "speaker) are used to test\nthe trained model.",
          "MFCC + LPCC\n68.57\n65.39": ""
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "",
          "MFCC + LPCC\n68.57\n65.39": "improve the sensitivity of\nthe model."
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "1) Comparison with other SSRL models:\nIn order\nto eval-",
          "MFCC + LPCC\n68.57\n65.39": ""
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "",
          "MFCC + LPCC\n68.57\n65.39": "HuBERT-LinearLayer\n[58]\nfirst\napplied HuBERT to\nget"
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "uate\nthe\nimpact\nof\ntransformer\nblocks\nof\ndifferent\nspeech",
          "MFCC + LPCC\n68.57\n65.39": ""
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "",
          "MFCC + LPCC\n68.57\n65.39": "representation and then tested on three DNN-based classifiers:"
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "pre-training models\non\nexperimental\nresults,\nseveral\ncom-",
          "MFCC + LPCC\n68.57\n65.39": ""
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "",
          "MFCC + LPCC\n68.57\n65.39": "Pooling + Linear Layer, CNN with Self-Attention,\nand CT-"
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "mon SSRL models were used,\nincluding HuBERT (HuBERT-",
          "MFCC + LPCC\n68.57\n65.39": ""
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "",
          "MFCC + LPCC\n68.57\n65.39": "Transformer, respectively. The result shows that\nthe Pooling +"
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "base\nand\nHuBERT-large), Wav2vec2\n(Wav2vec-base\nand",
          "MFCC + LPCC\n68.57\n65.39": ""
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "",
          "MFCC + LPCC\n68.57\n65.39": "Linear Layer has a better accuracy of 65.60%."
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "Wav2vec2-large),\nand WavLM. These models\nhave\nsimilar",
          "MFCC + LPCC\n68.57\n65.39": ""
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "structures,\nconsisting of\na CNN encoder\nand several\ntrans-",
          "MFCC + LPCC\n68.57\n65.39": "TIM-Net [59] extracted the MFCC features by applying the"
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "former blocks. These pre-training models were downloaded",
          "MFCC + LPCC\n68.57\n65.39": "Hamming window to each speech signal with a frame length of"
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "and called from Huggingface. HuBERT and Wav2vec2 both",
          "MFCC + LPCC\n68.57\n65.39": "50 ms and a 12.5 ms shift. The 39 extracted features were fed"
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "include\nbase\nand\nlarge\nversions. The\nunweighted\naccuracy",
          "MFCC + LPCC\n68.57\n65.39": "into Bi-Temporal-Aware Blocks\n(TABs). The 10-fold cross-"
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "results on IEMOCAP are shown in Table III. The result shows",
          "MFCC + LPCC\n68.57\n65.39": "validation with 90% and 10% samples\nin train and test\nsets"
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "that\nthe HuBERT base model has the highest accuracy of 78.",
          "MFCC + LPCC\n68.57\n65.39": "was applied."
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "26%, which outperforms other SSRL models.",
          "MFCC + LPCC\n68.57\n65.39": "AMSNet [60] applied a spatial CNN with the squeeze-and-"
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "2) Effect of\nspeech features: Next, we\nrun the HuBERT",
          "MFCC + LPCC\n68.57\n65.39": "excitation block (SCNN)\nto extract\nspectrogram representa-"
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "base model\non\ndifferent\ncombination\nof\nprosodic\nfeatures,",
          "MFCC + LPCC\n68.57\n65.39": "tions and an Attention Bi-LSTM to extract\ntemporal\nfeatures."
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "MFCC, LPCC, and spectrogram features. As shown in Table",
          "MFCC + LPCC\n68.57\n65.39": "The training and validation set of the IEMOCAP was split into"
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "IV, when prosody and MFCC are combined,\nthe experiment",
          "MFCC + LPCC\n68.57\n65.39": "80% and 20% of\nthe total\nsamples. The accuracy is 69.22%"
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "has\nthe highest SER accuracy of 74.26%, which shows\nthat",
          "MFCC + LPCC\n68.57\n65.39": "(WA) and 70.51 % (UA)."
        },
        {
          "tion categories (e.g., neutral, happy, angry, sad, and surprised).": "these two features have the bast performance.",
          "MFCC + LPCC\n68.57\n65.39": "Wav2Vec2 P-TAPT [61]\nis a Wav2Vec2-based fine-tuning"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6": "TABLE III"
        },
        {
          "6": "UNWEIGHTED SER ACCURACY ON IEMOCAP FOR TRANSFORMER"
        },
        {
          "6": ""
        },
        {
          "6": "BLOCKS OF DIFFERENT SPEECH PRE-TRAINED MODELS"
        },
        {
          "6": ""
        },
        {
          "6": "Speech pre-trained models\nValidation accuracy\nTest accuracy"
        },
        {
          "6": "HuBERT-base\n81.70\n78.26"
        },
        {
          "6": ""
        },
        {
          "6": "HuBERT-large\n78.53\n75.25"
        },
        {
          "6": ""
        },
        {
          "6": "WavLM\n69.36\n67.80"
        },
        {
          "6": "Wav2vec2-base\n72.79\n71.58"
        },
        {
          "6": "Wav2vec2-large\n70.54\n68.93"
        },
        {
          "6": ""
        },
        {
          "6": ""
        },
        {
          "6": "TABLE IV"
        },
        {
          "6": "UNWEIGHTED SER ACCURACY ON IEMOCAP FOR DIFFERENT"
        },
        {
          "6": ""
        },
        {
          "6": "COMBINATIONS OF SPEECH FEATURES"
        },
        {
          "6": ""
        },
        {
          "6": "Speech features\nValidation accuracy\nTest accuracy"
        },
        {
          "6": "Prosody + MFCC\n78.60\n74.58"
        },
        {
          "6": ""
        },
        {
          "6": "Prosody + LPCC\n76.01\n72.45"
        },
        {
          "6": ""
        },
        {
          "6": "Prosody + spectrogram\n75.36\n70.12"
        },
        {
          "6": "MFCC + LPCC\n68.57\n65.39"
        },
        {
          "6": "MFCC + spectrogram\n67.32\n62.24"
        },
        {
          "6": "LPCC + spectrogram\n64.82\n60.09"
        },
        {
          "6": ""
        },
        {
          "6": ""
        },
        {
          "6": ""
        },
        {
          "6": "D. HuMP-CAT Performance of SER on IEMOCAP"
        },
        {
          "6": ""
        },
        {
          "6": "We compare HuMP-CAT with other works running on the"
        },
        {
          "6": ""
        },
        {
          "6": "IEMOCAP data set with four emotions\nincluding happiness,"
        },
        {
          "6": ""
        },
        {
          "6": "sadness, neutrality,\nand anger. The\nresult\nis\nsummarized in"
        },
        {
          "6": ""
        },
        {
          "6": "Table V."
        },
        {
          "6": ""
        },
        {
          "6": "Li et al.\n[55] explored the effectiveness of angular softmax"
        },
        {
          "6": ""
        },
        {
          "6": "loss on two baselines (CRNN and CNN-TF-GAP models) with"
        },
        {
          "6": ""
        },
        {
          "6": "different\nclass-agnostic margins. The\nexperiments\nshow the"
        },
        {
          "6": ""
        },
        {
          "6": "class-specific margin on CNN-TF-GAP baseline performs the"
        },
        {
          "6": ""
        },
        {
          "6": "best."
        },
        {
          "6": ""
        },
        {
          "6": "GA-GRU [56]\nis\na\ngraph\nattention\napproach\nin\na\ngated"
        },
        {
          "6": ""
        },
        {
          "6": "recurrent unit network (GA-GRU), which combines both the"
        },
        {
          "6": "long-range\nattentional\ntime\nseries modeling with the\nsalient"
        },
        {
          "6": "frame-wise graph structure within an emotional utterance, and"
        },
        {
          "6": ""
        },
        {
          "6": "achieves the accuracy of 63.8% WA and 62.27% UA."
        },
        {
          "6": ""
        },
        {
          "6": "HFGM [57]\nis a hierarchical grained and feature mode,\nis"
        },
        {
          "6": ""
        },
        {
          "6": "a hierarchical grained feature, which uses RNN to process the"
        },
        {
          "6": ""
        },
        {
          "6": "frame-level and utterance-level structures of acoustic samples"
        },
        {
          "6": ""
        },
        {
          "6": "so that\nit\ncan capture\nfeatures of different granularities\nand"
        },
        {
          "6": ""
        },
        {
          "6": "improve the sensitivity of\nthe model."
        },
        {
          "6": ""
        },
        {
          "6": "HuBERT-LinearLayer\n[58]\nfirst\napplied HuBERT to\nget"
        },
        {
          "6": ""
        },
        {
          "6": "representation and then tested on three DNN-based classifiers:"
        },
        {
          "6": ""
        },
        {
          "6": "Pooling + Linear Layer, CNN with Self-Attention,\nand CT-"
        },
        {
          "6": ""
        },
        {
          "6": "Transformer, respectively. The result shows that\nthe Pooling +"
        },
        {
          "6": ""
        },
        {
          "6": "Linear Layer has a better accuracy of 65.60%."
        },
        {
          "6": ""
        },
        {
          "6": "TIM-Net [59] extracted the MFCC features by applying the"
        },
        {
          "6": "Hamming window to each speech signal with a frame length of"
        },
        {
          "6": "50 ms and a 12.5 ms shift. The 39 extracted features were fed"
        },
        {
          "6": "into Bi-Temporal-Aware Blocks\n(TABs). The 10-fold cross-"
        },
        {
          "6": "validation with 90% and 10% samples\nin train and test\nsets"
        },
        {
          "6": "was applied."
        },
        {
          "6": "AMSNet [60] applied a spatial CNN with the squeeze-and-"
        },
        {
          "6": "excitation block (SCNN)\nto extract\nspectrogram representa-"
        },
        {
          "6": "tions and an Attention Bi-LSTM to extract\ntemporal\nfeatures."
        },
        {
          "6": "The training and validation set of the IEMOCAP was split into"
        },
        {
          "6": "80% and 20% of\nthe total\nsamples. The accuracy is 69.22%"
        },
        {
          "6": "(WA) and 70.51 % (UA)."
        },
        {
          "6": "Wav2Vec2 P-TAPT [61]\nis a Wav2Vec2-based fine-tuning"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Zehra\net\nal. Ahn\net\nal.\n[66]\nintroduced Few-shot Learning": "and Unsupervised Domain Adaptation (FLUDA), which trains"
        },
        {
          "Zehra\net\nal. Ahn\net\nal.\n[66]\nintroduced Few-shot Learning": "an embedding and a metric module to project utterances into"
        },
        {
          "Zehra\net\nal. Ahn\net\nal.\n[66]\nintroduced Few-shot Learning": "a meaningful\nshared feature\nspace\nand discern class differ-"
        },
        {
          "Zehra\net\nal. Ahn\net\nal.\n[66]\nintroduced Few-shot Learning": ""
        },
        {
          "Zehra\net\nal. Ahn\net\nal.\n[66]\nintroduced Few-shot Learning": "ences,\nrespectively. During training,\nan auxiliary module\nis"
        },
        {
          "Zehra\net\nal. Ahn\net\nal.\n[66]\nintroduced Few-shot Learning": "incorporated to differentiate between real and pseudo-labeled"
        },
        {
          "Zehra\net\nal. Ahn\net\nal.\n[66]\nintroduced Few-shot Learning": "samples. The proposed method uses IEMOCAP and CREMA-"
        },
        {
          "Zehra\net\nal. Ahn\net\nal.\n[66]\nintroduced Few-shot Learning": "D [67]\nas\nsource\ncorpora.\nIn experiments with EMODB as"
        },
        {
          "Zehra\net\nal. Ahn\net\nal.\n[66]\nintroduced Few-shot Learning": "the\ntarget\ncorpus, FLUDA achieved an accuracy of 56.8%."
        },
        {
          "Zehra\net\nal. Ahn\net\nal.\n[66]\nintroduced Few-shot Learning": "MDAT [68]\nis a multimodal model\nthat\nleverages pre-trained"
        },
        {
          "Zehra\net\nal. Ahn\net\nal.\n[66]\nintroduced Few-shot Learning": "models including RoBERTa [69] with wav2vec2-XLS-R [70]"
        },
        {
          "Zehra\net\nal. Ahn\net\nal.\n[66]\nintroduced Few-shot Learning": "to extract multi-language speech and text embedding EmoBox"
        },
        {
          "Zehra\net\nal. Ahn\net\nal.\n[66]\nintroduced Few-shot Learning": "[16] provided toolkit and benchmark for Multilingual Multi-"
        },
        {
          "Zehra\net\nal. Ahn\net\nal.\n[66]\nintroduced Few-shot Learning": "corpus SER. The cross-corpus SER results on 4 datasets (e.g.,"
        },
        {
          "Zehra\net\nal. Ahn\net\nal.\n[66]\nintroduced Few-shot Learning": "IEMOCAP, MELD (English), RAVDESS and SAVEE) with"
        },
        {
          "Zehra\net\nal. Ahn\net\nal.\n[66]\nintroduced Few-shot Learning": "fully balanced test sets. The best result with the Whisper large"
        },
        {
          "Zehra\net\nal. Ahn\net\nal.\n[66]\nintroduced Few-shot Learning": "v3 [71] demonstrates the best performance among other SSRL"
        },
        {
          "Zehra\net\nal. Ahn\net\nal.\n[66]\nintroduced Few-shot Learning": "such\nas HuBERT base/large, WavLM base/large,\ndata2vec"
        },
        {
          "Zehra\net\nal. Ahn\net\nal.\n[66]\nintroduced Few-shot Learning": "base/large, and data2vec 2.0 base/large."
        },
        {
          "Zehra\net\nal. Ahn\net\nal.\n[66]\nintroduced Few-shot Learning": ""
        },
        {
          "Zehra\net\nal. Ahn\net\nal.\n[66]\nintroduced Few-shot Learning": "Table VI\nlists\nthe\nperformance\nof\nour\nproposed model"
        },
        {
          "Zehra\net\nal. Ahn\net\nal.\n[66]\nintroduced Few-shot Learning": ""
        },
        {
          "Zehra\net\nal. Ahn\net\nal.\n[66]\nintroduced Few-shot Learning": "HuMP-CAT, which achieves an impressive average accuracy"
        },
        {
          "Zehra\net\nal. Ahn\net\nal.\n[66]\nintroduced Few-shot Learning": ""
        },
        {
          "Zehra\net\nal. Ahn\net\nal.\n[66]\nintroduced Few-shot Learning": "of 78.75% across\nthe\nseven diverse data\nsets. Notably,\nthe"
        },
        {
          "Zehra\net\nal. Ahn\net\nal.\n[66]\nintroduced Few-shot Learning": ""
        },
        {
          "Zehra\net\nal. Ahn\net\nal.\n[66]\nintroduced Few-shot Learning": "ESD test (Chinese) showed a lower accuracy of 60.35%, likely"
        },
        {
          "Zehra\net\nal. Ahn\net\nal.\n[66]\nintroduced Few-shot Learning": ""
        },
        {
          "Zehra\net\nal. Ahn\net\nal.\n[66]\nintroduced Few-shot Learning": "due to significant\nlinguistic and acoustic differences between"
        },
        {
          "Zehra\net\nal. Ahn\net\nal.\n[66]\nintroduced Few-shot Learning": ""
        },
        {
          "Zehra\net\nal. Ahn\net\nal.\n[66]\nintroduced Few-shot Learning": "English and Chinese, but\nin the worst case, HuMP-CAT still"
        },
        {
          "Zehra\net\nal. Ahn\net\nal.\n[66]\nintroduced Few-shot Learning": ""
        },
        {
          "Zehra\net\nal. Ahn\net\nal.\n[66]\nintroduced Few-shot Learning": "perform better\nthan others."
        },
        {
          "Zehra\net\nal. Ahn\net\nal.\n[66]\nintroduced Few-shot Learning": ""
        },
        {
          "Zehra\net\nal. Ahn\net\nal.\n[66]\nintroduced Few-shot Learning": "Fig.5 illustrates the comparison of accuracy between HuMP-"
        },
        {
          "Zehra\net\nal. Ahn\net\nal.\n[66]\nintroduced Few-shot Learning": "CAT\nand\ntwo\nother\nrepresentative methods GAN [62]\nand"
        },
        {
          "Zehra\net\nal. Ahn\net\nal.\n[66]\nintroduced Few-shot Learning": "VACNN+BOVW [64], on the same target data set. Although"
        },
        {
          "Zehra\net\nal. Ahn\net\nal.\n[66]\nintroduced Few-shot Learning": "the source datasets used by these methods differ,\ntheir\nresults"
        },
        {
          "Zehra\net\nal. Ahn\net\nal.\n[66]\nintroduced Few-shot Learning": "provide useful reference points. As shown, HuMP-CAT signif-"
        },
        {
          "Zehra\net\nal. Ahn\net\nal.\n[66]\nintroduced Few-shot Learning": "icantly outperforms GAN and achieves slightly better\nresults"
        },
        {
          "Zehra\net\nal. Ahn\net\nal.\n[66]\nintroduced Few-shot Learning": "than VACNN+BOVW across\nthe three target datasets, high-"
        },
        {
          "Zehra\net\nal. Ahn\net\nal.\n[66]\nintroduced Few-shot Learning": "lighting the effectiveness of our proposed approach."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "7": "used to train the model. TableVI\nlists\nthe best performance"
        },
        {
          "7": ""
        },
        {
          "7": "of\nthe GAN-SVM approach. Meng\net\nal.\n[63]\nintroduced"
        },
        {
          "7": ""
        },
        {
          "7": "an\narchitecture ADRNN which\napplied\ndilated CNN with"
        },
        {
          "7": "residual block and BiLSTM based on the attention mechanism."
        },
        {
          "7": ""
        },
        {
          "7": "ADRNN uses\nthe\nlog-mel\nspectrogram as\nthe\nfeature. The"
        },
        {
          "7": ""
        },
        {
          "7": "experiment\nadopted IEMOCAP as\na\ntraining set\nand tested"
        },
        {
          "7": ""
        },
        {
          "7": "the model\nin\nthe EMODB database,\nshowing\nan\naccuracy"
        },
        {
          "7": ""
        },
        {
          "7": "of 63.84%. The VACNN + BOVW approach [64] designed"
        },
        {
          "7": ""
        },
        {
          "7": "a\nvisual\nattention\nconvolutional\nneural\nnetwork\n(VACNN)"
        },
        {
          "7": ""
        },
        {
          "7": "pre-trained with TESS and RAVDESS datasets by extracting"
        },
        {
          "7": "the feature on a log-mel\nspectrogram using a bag of visual"
        },
        {
          "7": "words(BOVW). The pretrained model\nis fine-tuned with the"
        },
        {
          "7": "target dataset through five-fold cross validation. [65] compared"
        },
        {
          "7": "the\nperformance\nof\nan\nensemble\nlearning\napproach\nagainst"
        },
        {
          "7": "traditional machine learning including RF, SVM, and Decision"
        },
        {
          "7": "Tree on four\ncorpora\n(e.g., SAVEE, URDU, EMO-DB,\nand"
        },
        {
          "7": "EMOVO). This\nstudy tested a classifier performance trained"
        },
        {
          "7": "in one corpus with data from another corpus\nto evaluate its"
        },
        {
          "7": "efficiency\nfor CLSER and\nshowed\nthat\nensemble\nlearning"
        },
        {
          "7": "performs better. All of the works used either low-level acoustic"
        },
        {
          "7": "features or\nlog-mel spectrogram."
        },
        {
          "7": "We also compared HuMP-CATwith other SSRL approaches."
        },
        {
          "7": "Zehra\net\nal. Ahn\net\nal.\n[66]\nintroduced Few-shot Learning"
        },
        {
          "7": "and Unsupervised Domain Adaptation (FLUDA), which trains"
        },
        {
          "7": "an embedding and a metric module to project utterances into"
        },
        {
          "7": "a meaningful\nshared feature\nspace\nand discern class differ-"
        },
        {
          "7": ""
        },
        {
          "7": "ences,\nrespectively. During training,\nan auxiliary module\nis"
        },
        {
          "7": "incorporated to differentiate between real and pseudo-labeled"
        },
        {
          "7": "samples. The proposed method uses IEMOCAP and CREMA-"
        },
        {
          "7": "D [67]\nas\nsource\ncorpora.\nIn experiments with EMODB as"
        },
        {
          "7": "the\ntarget\ncorpus, FLUDA achieved an accuracy of 56.8%."
        },
        {
          "7": "MDAT [68]\nis a multimodal model\nthat\nleverages pre-trained"
        },
        {
          "7": "models including RoBERTa [69] with wav2vec2-XLS-R [70]"
        },
        {
          "7": "to extract multi-language speech and text embedding EmoBox"
        },
        {
          "7": "[16] provided toolkit and benchmark for Multilingual Multi-"
        },
        {
          "7": "corpus SER. The cross-corpus SER results on 4 datasets (e.g.,"
        },
        {
          "7": "IEMOCAP, MELD (English), RAVDESS and SAVEE) with"
        },
        {
          "7": "fully balanced test sets. The best result with the Whisper large"
        },
        {
          "7": "v3 [71] demonstrates the best performance among other SSRL"
        },
        {
          "7": "such\nas HuBERT base/large, WavLM base/large,\ndata2vec"
        },
        {
          "7": "base/large, and data2vec 2.0 base/large."
        },
        {
          "7": ""
        },
        {
          "7": "Table VI\nlists\nthe\nperformance\nof\nour\nproposed model"
        },
        {
          "7": ""
        },
        {
          "7": "HuMP-CAT, which achieves an impressive average accuracy"
        },
        {
          "7": ""
        },
        {
          "7": "of 78.75% across\nthe\nseven diverse data\nsets. Notably,\nthe"
        },
        {
          "7": ""
        },
        {
          "7": "ESD test (Chinese) showed a lower accuracy of 60.35%, likely"
        },
        {
          "7": ""
        },
        {
          "7": "due to significant\nlinguistic and acoustic differences between"
        },
        {
          "7": ""
        },
        {
          "7": "English and Chinese, but\nin the worst case, HuMP-CAT still"
        },
        {
          "7": ""
        },
        {
          "7": "perform better\nthan others."
        },
        {
          "7": ""
        },
        {
          "7": "Fig.5 illustrates the comparison of accuracy between HuMP-"
        },
        {
          "7": "CAT\nand\ntwo\nother\nrepresentative methods GAN [62]\nand"
        },
        {
          "7": "VACNN+BOVW [64], on the same target data set. Although"
        },
        {
          "7": "the source datasets used by these methods differ,\ntheir\nresults"
        },
        {
          "7": "provide useful reference points. As shown, HuMP-CAT signif-"
        },
        {
          "7": "icantly outperforms GAN and achieves slightly better\nresults"
        },
        {
          "7": "than VACNN+BOVW across\nthe three target datasets, high-"
        },
        {
          "7": "lighting the effectiveness of our proposed approach."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE VI": "COMPARISON WITH THE STATE-OF-THE-ARTS CLSER"
        },
        {
          "TABLE VI": "Source dataset"
        },
        {
          "TABLE VI": ""
        },
        {
          "TABLE VI": ""
        },
        {
          "TABLE VI": ""
        },
        {
          "TABLE VI": "IEMOCAP"
        },
        {
          "TABLE VI": ""
        },
        {
          "TABLE VI": ""
        },
        {
          "TABLE VI": ""
        },
        {
          "TABLE VI": ""
        },
        {
          "TABLE VI": "EMODB, URDU, SAVEE, or URDU"
        },
        {
          "TABLE VI": ""
        },
        {
          "TABLE VI": "IEMOCAP"
        },
        {
          "TABLE VI": ""
        },
        {
          "TABLE VI": "TESS+RAVDESS"
        },
        {
          "TABLE VI": ""
        },
        {
          "TABLE VI": "EMOVO"
        },
        {
          "TABLE VI": "CREMA-D"
        },
        {
          "TABLE VI": ""
        },
        {
          "TABLE VI": "IEMOCAP"
        },
        {
          "TABLE VI": ""
        },
        {
          "TABLE VI": ""
        },
        {
          "TABLE VI": "IEMOCAP"
        },
        {
          "TABLE VI": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "notable results": "",
          "such as 88.69% on EMODB and 79.48% on": ""
        },
        {
          "notable results": "",
          "such as 88.69% on EMODB and 79.48% on": ""
        },
        {
          "notable results": "",
          "such as 88.69% on EMODB and 79.48% on": ""
        },
        {
          "notable results": "CAT",
          "such as 88.69% on EMODB and 79.48% on": "existing methods"
        },
        {
          "notable results": "",
          "such as 88.69% on EMODB and 79.48% on": ""
        },
        {
          "notable results": "",
          "such as 88.69% on EMODB and 79.48% on": ""
        },
        {
          "notable results": "",
          "such as 88.69% on EMODB and 79.48% on": ""
        },
        {
          "notable results": "For",
          "such as 88.69% on EMODB and 79.48% on": "aim to"
        },
        {
          "notable results": "",
          "such as 88.69% on EMODB and 79.48% on": ""
        },
        {
          "notable results": "",
          "such as 88.69% on EMODB and 79.48% on": ""
        },
        {
          "notable results": "",
          "such as 88.69% on EMODB and 79.48% on": ""
        },
        {
          "notable results": "further",
          "such as 88.69% on EMODB and 79.48% on": "improve the generalization and cross-linguistic adapt-"
        },
        {
          "notable results": "",
          "such as 88.69% on EMODB and 79.48% on": ""
        },
        {
          "notable results": "ability of",
          "such as 88.69% on EMODB and 79.48% on": ""
        },
        {
          "notable results": "",
          "such as 88.69% on EMODB and 79.48% on": ""
        },
        {
          "notable results": "",
          "such as 88.69% on EMODB and 79.48% on": "REFERENCES"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "9": "[27]\nF. Eyben, A. Batliner, B. Schuller, D. Seppi, and S. Steidl, “Cross-corpus"
        },
        {
          "9": "classification of realistic emotions–some pilot experiments,” in Proc. 7th"
        },
        {
          "9": "international conference on language resources and evaluation, 2010."
        },
        {
          "9": "[28] B. Schuller, Z. Zhang, F. Weninger,\nand G. Rigoll,\n“Using multiple"
        },
        {
          "9": "databases for\ntraining in emotion recognition: To unite or\nto vote?” in"
        },
        {
          "9": "Proc. Twelfth Annual Conference of\nthe International Speech Commu-"
        },
        {
          "9": "nication Association, Aug. 2011, pp. 1553–1556."
        },
        {
          "9": "[29] W. Zehra, A. R.\nJaved, Z.\nJalil, H. U. Khan,\nand T. R. Gadekallu,"
        },
        {
          "9": "“Cross corpus multi-lingual speech emotion recognition using ensemble"
        },
        {
          "9": "learning,” Complex & Intelligent Systems, vol. 7, no. 4, pp. 1845–1854,"
        },
        {
          "9": "2021."
        },
        {
          "9": "[30]\nS. Latif, A. Qayyum, M. Usman, and J. Qadir, “Cross\nlingual\nspeech"
        },
        {
          "9": "emotion recognition: Urdu vs. western languages,” in Proc. International"
        },
        {
          "9": "conference on frontiers of information technology, Dec. 2018, pp. 88–93."
        },
        {
          "9": "[31] N. Braunschweiler, R. Doddipatla, S. Keizer,\nand S. Stoyanchev,\n“A"
        },
        {
          "9": "study on cross-corpus speech emotion recognition and data augmenta-"
        },
        {
          "9": "IEEE Automatic Speech Recognition and Understanding\ntion,” in Proc."
        },
        {
          "9": "Workshop (ASRU), 2021, pp. 24–30."
        },
        {
          "9": ""
        },
        {
          "9": "[32]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,"
        },
        {
          "9": ""
        },
        {
          "9": "T. Yoshioka, X. Xiao et al., “Wavlm: Large-scale self-supervised pre-"
        },
        {
          "9": "IEEE Journal\nof\nSelected"
        },
        {
          "9": "training\nfor\nfull\nstack\nspeech\nprocessing,”"
        },
        {
          "9": ""
        },
        {
          "9": "Topics in Signal Processing, vol. 16, no. 6, pp. 1505–1518, 2022."
        },
        {
          "9": ""
        },
        {
          "9": "[33]\nL. Pepino, P. Riera, and L. Ferrer, “Emotion recognition from speech"
        },
        {
          "9": ""
        },
        {
          "9": "arXiv\npreprint\nusing wav2vec\n2.0\nembeddings,”\narXiv:2104.03502,"
        },
        {
          "9": ""
        },
        {
          "9": "2021."
        },
        {
          "9": ""
        },
        {
          "9": "[34] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and"
        },
        {
          "9": ""
        },
        {
          "9": "A. Mohamed, “Hubert: Self-supervised speech representation learning"
        },
        {
          "9": ""
        },
        {
          "9": "IEEE/ACM transactions\non\nby masked\nprediction\nof\nhidden\nunits,”"
        },
        {
          "9": ""
        },
        {
          "9": "audio, speech, and language processing, vol. 29, pp. 3451–3460, 2021."
        },
        {
          "9": ""
        },
        {
          "9": "[35]\nJ. Devlin,\n“Bert: Pre-training\nof\ndeep\nbidirectional\ntransformers\nfor"
        },
        {
          "9": ""
        },
        {
          "9": "language understanding,” arXiv preprint arXiv:1810.04805, 2018."
        },
        {
          "9": ""
        },
        {
          "9": "[36] A. CHAKHTOUNA, S. SEKKATE, and A. Abdellah, “Unveiling em-"
        },
        {
          "9": ""
        },
        {
          "9": "bedded features\nin wav2vec2 and hubert msodels\nfor\nspeech emotion"
        },
        {
          "9": ""
        },
        {
          "9": "recognition,” Procedia Computer\nScience,\nvol.\n232,\npp.\n2560–2569,"
        },
        {
          "9": ""
        },
        {
          "9": "2024."
        },
        {
          "9": ""
        },
        {
          "9": "[37] A. Pasad,\nJ.-C. Chou, and K. Livescu, “Layer-wise analysis of a self-"
        },
        {
          "9": ""
        },
        {
          "9": "IEEE Automatic\nsupervised\nspeech\nrepresentation model,”\nin Proc."
        },
        {
          "9": ""
        },
        {
          "9": "Speech Recognition and Understanding Workshop (ASRU), Dec. 2021,"
        },
        {
          "9": ""
        },
        {
          "9": "pp. 914–921."
        },
        {
          "9": ""
        },
        {
          "9": "[38] Y. He, N. Minematsu, and D. Saito, “Multiple acoustic features speech"
        },
        {
          "9": ""
        },
        {
          "9": "IEEE\nemotion recognition using cross-attention transformer,” in Proc."
        },
        {
          "9": ""
        },
        {
          "9": "International Conference on Acoustics, Speech and Signal Processing,"
        },
        {
          "9": ""
        },
        {
          "9": "2023, pp. 1–5."
        },
        {
          "9": ""
        },
        {
          "9": "[39] Y. Gao, B. Li, N. Wang, and T. Zhu, “Speech emotion recognition using"
        },
        {
          "9": ""
        },
        {
          "9": "Informatics:\nInternational\nlocal\nand\nglobal\nfeatures,”\nin Proc. Brain"
        },
        {
          "9": ""
        },
        {
          "9": "Conference, Nov. 2017, pp. 3–13."
        },
        {
          "9": ""
        },
        {
          "9": "[40] N. Dehak, P. Dumouchel, and P. Kenny, “Modeling prosodic features"
        },
        {
          "9": ""
        },
        {
          "9": "with joint\nfactor analysis\nfor\nspeaker verification,” IEEE Transactions"
        },
        {
          "9": ""
        },
        {
          "9": "on Audio, Speech, and Language Processing, vol. 15, no. 7, pp. 2095–"
        },
        {
          "9": ""
        },
        {
          "9": "2103, 2007."
        },
        {
          "9": ""
        },
        {
          "9": "¨\n[41]\nT.\nOzseven,\n“A novel\nfeature\nselection method\nfor\nspeech\nemotion"
        },
        {
          "9": ""
        },
        {
          "9": "recognition,” Applied Acoustics, vol. 146, pp. 320–326, 2019."
        },
        {
          "9": ""
        },
        {
          "9": "[42]\nL. Abdel-Hamid, N. H. Shaker, and I. Emara, “Analysis of\nlinguistic"
        },
        {
          "9": ""
        },
        {
          "9": "and prosodic\nfeatures of bilingual\narabic–english speakers\nfor\nspeech"
        },
        {
          "9": ""
        },
        {
          "9": "emotion recognition,” IEEE Access, vol. 8, pp. 72 957–72 970, 2020."
        },
        {
          "9": ""
        },
        {
          "9": "[43]\nJ. C. V´asquez-Correa,\nJ. Orozco-Arroyave, T. Bocklet,\nand E. N¨oth,"
        },
        {
          "9": ""
        },
        {
          "9": "“Towards\nan automatic\nevaluation of\nthe dysarthria\nlevel of patients"
        },
        {
          "9": ""
        },
        {
          "9": "with parkinson’s disease,” Journal of communication disorders, vol. 76,"
        },
        {
          "9": ""
        },
        {
          "9": "pp. 21–36, 2018."
        },
        {
          "9": ""
        },
        {
          "9": "[44] K. Bhangale\nand M. Kothandaraman,\n“Speech\nemotion\nrecognition"
        },
        {
          "9": "based\non multiple\nacoustic\nfeatures\nand\ndeep\nconvolutional\nneural"
        },
        {
          "9": "network,” Electronics, vol. 12, no. 4, p. 839, 2023."
        },
        {
          "9": "[45] K. Bhangale and K. Mohanaprasad, “Speech emotion recognition using"
        },
        {
          "9": "mel frequency log spectrogram and deep convolutional neural network,”"
        },
        {
          "9": "International Conference\non Futuristic Communication\nand\nin Proc."
        },
        {
          "9": "Network Technologies, Nov. 2020, pp. 241–250."
        },
        {
          "9": "[46]\nJ. Zhao, X. Mao, and L. Chen, “Learning deep features\nto recognise"
        },
        {
          "9": "IET\nspeech\nemotion\nusing merged\ndeep CNN,”\nSignal Processing,"
        },
        {
          "9": "vol. 12, no. 6, pp. 713–721, 2018."
        },
        {
          "9": "[47] Y. Zheng,\nJ. Peng, Y. Chen, Y. Zhang,\nJ. Wang, M. Liu, and M. Xu,"
        },
        {
          "9": "“The speakin speaker verification system for far-field speaker verification"
        },
        {
          "9": "challenge 2022,” arXiv preprint arXiv:2209.11625, 2022."
        },
        {
          "9": "[48] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower, S. Kim, J. N."
        },
        {
          "9": "Chang, S. Lee, and S. S. Narayanan, “IEMOCAP: Interactive emotional"
        },
        {
          "9": "dyadic motion capture database,” Language resources and evaluation,"
        },
        {
          "9": "vol. 42, pp. 335–359, 2008."
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "9": "[27]\nF. Eyben, A. Batliner, B. Schuller, D. Seppi, and S. Steidl, “Cross-corpus"
        },
        {
          "9": "classification of realistic emotions–some pilot experiments,” in Proc. 7th"
        },
        {
          "9": "international conference on language resources and evaluation, 2010."
        },
        {
          "9": "[28] B. Schuller, Z. Zhang, F. Weninger,\nand G. Rigoll,\n“Using multiple"
        },
        {
          "9": "databases for\ntraining in emotion recognition: To unite or\nto vote?” in"
        },
        {
          "9": "Proc. Twelfth Annual Conference of\nthe International Speech Commu-"
        },
        {
          "9": "nication Association, Aug. 2011, pp. 1553–1556."
        },
        {
          "9": "[29] W. Zehra, A. R.\nJaved, Z.\nJalil, H. U. Khan,\nand T. R. Gadekallu,"
        },
        {
          "9": "“Cross corpus multi-lingual speech emotion recognition using ensemble"
        },
        {
          "9": "learning,” Complex & Intelligent Systems, vol. 7, no. 4, pp. 1845–1854,"
        },
        {
          "9": "2021."
        },
        {
          "9": "[30]\nS. Latif, A. Qayyum, M. Usman, and J. Qadir, “Cross\nlingual\nspeech"
        },
        {
          "9": "emotion recognition: Urdu vs. western languages,” in Proc. International"
        },
        {
          "9": "conference on frontiers of information technology, Dec. 2018, pp. 88–93."
        },
        {
          "9": "[31] N. Braunschweiler, R. Doddipatla, S. Keizer,\nand S. Stoyanchev,\n“A"
        },
        {
          "9": "study on cross-corpus speech emotion recognition and data augmenta-"
        },
        {
          "9": "IEEE Automatic Speech Recognition and Understanding\ntion,” in Proc."
        },
        {
          "9": "Workshop (ASRU), 2021, pp. 24–30."
        },
        {
          "9": ""
        },
        {
          "9": "[32]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,"
        },
        {
          "9": ""
        },
        {
          "9": "T. Yoshioka, X. Xiao et al., “Wavlm: Large-scale self-supervised pre-"
        },
        {
          "9": "IEEE Journal\nof\nSelected"
        },
        {
          "9": "training\nfor\nfull\nstack\nspeech\nprocessing,”"
        },
        {
          "9": ""
        },
        {
          "9": "Topics in Signal Processing, vol. 16, no. 6, pp. 1505–1518, 2022."
        },
        {
          "9": ""
        },
        {
          "9": "[33]\nL. Pepino, P. Riera, and L. Ferrer, “Emotion recognition from speech"
        },
        {
          "9": ""
        },
        {
          "9": "arXiv\npreprint\nusing wav2vec\n2.0\nembeddings,”\narXiv:2104.03502,"
        },
        {
          "9": ""
        },
        {
          "9": "2021."
        },
        {
          "9": ""
        },
        {
          "9": "[34] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and"
        },
        {
          "9": ""
        },
        {
          "9": "A. Mohamed, “Hubert: Self-supervised speech representation learning"
        },
        {
          "9": ""
        },
        {
          "9": "IEEE/ACM transactions\non\nby masked\nprediction\nof\nhidden\nunits,”"
        },
        {
          "9": ""
        },
        {
          "9": "audio, speech, and language processing, vol. 29, pp. 3451–3460, 2021."
        },
        {
          "9": ""
        },
        {
          "9": "[35]\nJ. Devlin,\n“Bert: Pre-training\nof\ndeep\nbidirectional\ntransformers\nfor"
        },
        {
          "9": ""
        },
        {
          "9": "language understanding,” arXiv preprint arXiv:1810.04805, 2018."
        },
        {
          "9": ""
        },
        {
          "9": "[36] A. CHAKHTOUNA, S. SEKKATE, and A. Abdellah, “Unveiling em-"
        },
        {
          "9": ""
        },
        {
          "9": "bedded features\nin wav2vec2 and hubert msodels\nfor\nspeech emotion"
        },
        {
          "9": ""
        },
        {
          "9": "recognition,” Procedia Computer\nScience,\nvol.\n232,\npp.\n2560–2569,"
        },
        {
          "9": ""
        },
        {
          "9": "2024."
        },
        {
          "9": ""
        },
        {
          "9": "[37] A. Pasad,\nJ.-C. Chou, and K. Livescu, “Layer-wise analysis of a self-"
        },
        {
          "9": ""
        },
        {
          "9": "IEEE Automatic\nsupervised\nspeech\nrepresentation model,”\nin Proc."
        },
        {
          "9": ""
        },
        {
          "9": "Speech Recognition and Understanding Workshop (ASRU), Dec. 2021,"
        },
        {
          "9": ""
        },
        {
          "9": "pp. 914–921."
        },
        {
          "9": ""
        },
        {
          "9": "[38] Y. He, N. Minematsu, and D. Saito, “Multiple acoustic features speech"
        },
        {
          "9": ""
        },
        {
          "9": "IEEE\nemotion recognition using cross-attention transformer,” in Proc."
        },
        {
          "9": ""
        },
        {
          "9": "International Conference on Acoustics, Speech and Signal Processing,"
        },
        {
          "9": ""
        },
        {
          "9": "2023, pp. 1–5."
        },
        {
          "9": ""
        },
        {
          "9": "[39] Y. Gao, B. Li, N. Wang, and T. Zhu, “Speech emotion recognition using"
        },
        {
          "9": ""
        },
        {
          "9": "Informatics:\nInternational\nlocal\nand\nglobal\nfeatures,”\nin Proc. Brain"
        },
        {
          "9": ""
        },
        {
          "9": "Conference, Nov. 2017, pp. 3–13."
        },
        {
          "9": ""
        },
        {
          "9": "[40] N. Dehak, P. Dumouchel, and P. Kenny, “Modeling prosodic features"
        },
        {
          "9": ""
        },
        {
          "9": "with joint\nfactor analysis\nfor\nspeaker verification,” IEEE Transactions"
        },
        {
          "9": ""
        },
        {
          "9": "on Audio, Speech, and Language Processing, vol. 15, no. 7, pp. 2095–"
        },
        {
          "9": ""
        },
        {
          "9": "2103, 2007."
        },
        {
          "9": ""
        },
        {
          "9": "¨\n[41]\nT.\nOzseven,\n“A novel\nfeature\nselection method\nfor\nspeech\nemotion"
        },
        {
          "9": ""
        },
        {
          "9": "recognition,” Applied Acoustics, vol. 146, pp. 320–326, 2019."
        },
        {
          "9": ""
        },
        {
          "9": "[42]\nL. Abdel-Hamid, N. H. Shaker, and I. Emara, “Analysis of\nlinguistic"
        },
        {
          "9": ""
        },
        {
          "9": "and prosodic\nfeatures of bilingual\narabic–english speakers\nfor\nspeech"
        },
        {
          "9": ""
        },
        {
          "9": "emotion recognition,” IEEE Access, vol. 8, pp. 72 957–72 970, 2020."
        },
        {
          "9": ""
        },
        {
          "9": "[43]\nJ. C. V´asquez-Correa,\nJ. Orozco-Arroyave, T. Bocklet,\nand E. N¨oth,"
        },
        {
          "9": ""
        },
        {
          "9": "“Towards\nan automatic\nevaluation of\nthe dysarthria\nlevel of patients"
        },
        {
          "9": ""
        },
        {
          "9": "with parkinson’s disease,” Journal of communication disorders, vol. 76,"
        },
        {
          "9": ""
        },
        {
          "9": "pp. 21–36, 2018."
        },
        {
          "9": ""
        },
        {
          "9": "[44] K. Bhangale\nand M. Kothandaraman,\n“Speech\nemotion\nrecognition"
        },
        {
          "9": "based\non multiple\nacoustic\nfeatures\nand\ndeep\nconvolutional\nneural"
        },
        {
          "9": "network,” Electronics, vol. 12, no. 4, p. 839, 2023."
        },
        {
          "9": "[45] K. Bhangale and K. Mohanaprasad, “Speech emotion recognition using"
        },
        {
          "9": "mel frequency log spectrogram and deep convolutional neural network,”"
        },
        {
          "9": "International Conference\non Futuristic Communication\nand\nin Proc."
        },
        {
          "9": "Network Technologies, Nov. 2020, pp. 241–250."
        },
        {
          "9": "[46]\nJ. Zhao, X. Mao, and L. Chen, “Learning deep features\nto recognise"
        },
        {
          "9": "IET\nspeech\nemotion\nusing merged\ndeep CNN,”\nSignal Processing,"
        },
        {
          "9": "vol. 12, no. 6, pp. 713–721, 2018."
        },
        {
          "9": "[47] Y. Zheng,\nJ. Peng, Y. Chen, Y. Zhang,\nJ. Wang, M. Liu, and M. Xu,"
        },
        {
          "9": "“The speakin speaker verification system for far-field speaker verification"
        },
        {
          "9": "challenge 2022,” arXiv preprint arXiv:2209.11625, 2022."
        },
        {
          "9": "[48] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower, S. Kim, J. N."
        },
        {
          "9": "Chang, S. Lee, and S. S. Narayanan, “IEMOCAP: Interactive emotional"
        },
        {
          "9": "dyadic motion capture database,” Language resources and evaluation,"
        },
        {
          "9": "vol. 42, pp. 335–359, 2008."
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "10": "[61]\nL.-W. Chen and A. Rudnicky, “Exploring wav2vec 2.0 fine tuning for"
        },
        {
          "10": "IEEE International\nimproved\nspeech\nemotion\nrecognition,”\nin Proc."
        },
        {
          "10": "Conference on Acoustics, Speech and Signal Processing, Jun. 2023, pp."
        },
        {
          "10": "1–5."
        },
        {
          "10": ""
        },
        {
          "10": "[62]\nS. Latif,\nJ. Qadir,\nand M. Bilal,\n“Unsupervised\nadversarial\ndomain"
        },
        {
          "10": ""
        },
        {
          "10": "adaptation for cross-lingual speech emotion recognition,” in Proc. IEEE"
        },
        {
          "10": ""
        },
        {
          "10": "International Conference on Affective Computing and Intelligent\nInter-"
        },
        {
          "10": ""
        },
        {
          "10": "action, Sep. 2019, pp. 732–737."
        },
        {
          "10": ""
        },
        {
          "10": "[63] H. Meng, T. Yan, F. Yuan, and H. Wei, “Speech emotion recognition"
        },
        {
          "10": ""
        },
        {
          "10": "IEEE\nfrom 3D log-mel\nspectrograms with\ndeep\nlearning\nnetwork,”"
        },
        {
          "10": ""
        },
        {
          "10": "access, vol. 7, pp. 125 868–125 881, 2019."
        },
        {
          "10": ""
        },
        {
          "10": "[64] M. Seo and M. Kim, “Fusing visual attention CNN and bag of visual"
        },
        {
          "10": "words\nfor cross-corpus\nspeech emotion recognition,” Sensors, vol. 20,"
        },
        {
          "10": "no. 19, p. 5559, 2020."
        },
        {
          "10": "[65] W. Zehra, A. R.\nJaved, Z.\nJalil, H. U. Khan,\nand T. R. Gadekallu,"
        },
        {
          "10": "“Cross corpus multi-lingual speech emotion recognition using ensemble"
        },
        {
          "10": "learning,” Complex & Intelligent Systems, vol. 7, no. 4, pp. 1845–1854,"
        },
        {
          "10": "2021."
        },
        {
          "10": ""
        },
        {
          "10": "[66] Y. Ahn,\nS.\nJ. Lee,\nand\nJ. W.\nShin,\n“Cross-corpus\nspeech\nemotion"
        },
        {
          "10": ""
        },
        {
          "10": "recognition based on few-shot\nlearning and domain adaptation,” IEEE"
        },
        {
          "10": ""
        },
        {
          "10": "Signal Processing Letters, vol. 28, pp. 1190–1194, 2021."
        },
        {
          "10": ""
        },
        {
          "10": "[67] H. Cao, D. G. Cooper, M. K. Keutmann, R. C. Gur, A. Nenkova,"
        },
        {
          "10": "and R. Verma, “Crema-d: Crowd-sourced emotional multimodal actors"
        },
        {
          "10": "dataset,” IEEE transactions on affective computing, vol. 5, no. 4, pp."
        },
        {
          "10": "377–390, 2014."
        },
        {
          "10": "[68]\nS. A. M. Zaidi,\nS. Latif,\nand\nJ. Qadir,\n“Enhancing\ncross-language"
        },
        {
          "10": "multimodal emotion recognition with dual attention transformers,” IEEE"
        },
        {
          "10": "Open Journal of\nthe Computer Society, 2024."
        },
        {
          "10": ""
        },
        {
          "10": "[69] Y.\nLiu, M. Ott, N. Goyal,\nJ. Du, M.\nJoshi, D. Chen, O.\nLevy,"
        },
        {
          "10": ""
        },
        {
          "10": "M. Lewis, L. Zettlemoyer,\nand V.\nStoyanov,\n“Roberta: A robustly"
        },
        {
          "10": ""
        },
        {
          "10": "optimized\nbert\npretraining\napproach,”\n2019.\n[Online].\nAvailable:"
        },
        {
          "10": ""
        },
        {
          "10": "https://arxiv.org/abs/1907.11692"
        },
        {
          "10": ""
        },
        {
          "10": "[70] A. Babu, C. Wang, A.\nTjandra, K.\nLakhotia, Q. Xu, N. Goyal,"
        },
        {
          "10": ""
        },
        {
          "10": "K. Singh, P. Von Platen, Y. Saraf, J. Pino et al., “Xls-r: Self-supervised"
        },
        {
          "10": ""
        },
        {
          "10": "arXiv\npreprint\ncross-lingual\nspeech\nrepresentation\nlearning\nat\nscale,”"
        },
        {
          "10": ""
        },
        {
          "10": "arXiv:2111.09296, 2021."
        },
        {
          "10": ""
        },
        {
          "10": "[71] A. Radford,\nJ. W. Kim, T. Xu, G. Brockman, C. McLeavey,\nand"
        },
        {
          "10": "I. Sutskever, “Robust speech recognition via large-scale weak supervi-"
        },
        {
          "10": "sion,” in International conference on machine learning.\nPMLR, 2023,"
        },
        {
          "10": "pp. 28 492–28 518."
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "M Ayadi",
        "M Kamel",
        "F Karray"
      ],
      "year": "2011",
      "venue": "Pattern recognition"
    },
    {
      "citation_id": "2",
      "title": "Automated screening for distress: A perspective for the future",
      "authors": [
        "R Rana",
        "S Latif",
        "R Gururajan",
        "A Gray",
        "G Mackenzie",
        "G Humphris",
        "J Dunn"
      ],
      "year": "2019",
      "venue": "European journal of cancer care"
    },
    {
      "citation_id": "3",
      "title": "A comprehensive review of speech emotion recognition systems",
      "authors": [
        "T Wani",
        "T Gunawan",
        "S Qadri",
        "M Kartiwi",
        "E Ambikairajah"
      ],
      "year": "2021",
      "venue": "IEEE access"
    },
    {
      "citation_id": "4",
      "title": "A systematic review on affective computing: Emotion models, databases, and recent advances",
      "authors": [
        "Y Wang",
        "W Song",
        "W Tao",
        "A Liotta",
        "D Yang",
        "X Li",
        "S Gao",
        "Y Sun",
        "W Ge",
        "W Zhang"
      ],
      "year": "2022",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "5",
      "title": "Speech emotion recognition based on hmm and svm",
      "authors": [
        "Y.-L Lin",
        "G Wei"
      ],
      "year": "2005",
      "venue": "Proc. IEEE International conference on machine learning and cybernetics"
    },
    {
      "citation_id": "6",
      "title": "Speech emotion recognition using fourier parameters",
      "authors": [
        "K Wang",
        "N An",
        "B Li",
        "Y Zhang",
        "L Li"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on affective computing"
    },
    {
      "citation_id": "7",
      "title": "Implementation and comparison of speech emotion recognition system using gaussian mixture model (gmm) and k-nearest neighbor K-NN techniques",
      "authors": [
        "R Lanjewar",
        "S Mathurkar",
        "N Patel"
      ],
      "year": "2015",
      "venue": "Procedia computer science"
    },
    {
      "citation_id": "8",
      "title": "Speech emotion recognition using deep learning techniques: A review",
      "authors": [
        "R Khalil",
        "E Jones",
        "M Babar",
        "T Jan",
        "M Zafar",
        "T Alhussain"
      ],
      "year": "2019",
      "venue": "IEEE access"
    },
    {
      "citation_id": "9",
      "title": "Deep learning approaches for speech emotion recognition: State of the art and research challenges",
      "authors": [
        "R Jahangir",
        "Y Teh",
        "F Hanif",
        "G Mujtaba"
      ],
      "year": "2021",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "10",
      "title": "Speech emotion recognition: A review",
      "authors": [
        "A Thakur",
        "S Dhull"
      ],
      "year": "2019",
      "venue": "Proc. International Conference on Advanced Communication and Computational Technology"
    },
    {
      "citation_id": "11",
      "title": "Self-supervised speech representation learning: A review",
      "authors": [
        "A Mohamed",
        "H -Y. Lee",
        "L Borgholt",
        "J Havtorn",
        "J Edin",
        "C Igel",
        "K Kirchhoff",
        "S.-W Li",
        "K Livescu",
        "L Maaløe",
        "T Sainath",
        "W Shinji"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "12",
      "title": "Cross-corpus speech emotion recognition using semi-supervised transfer non-negative matrix factorization with adaptation regularization",
      "authors": [
        "H Luo",
        "J Han"
      ],
      "year": "2019",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "13",
      "title": "Multisource i-vectors domain adaptation using maximum mean discrepancy based autoencoders",
      "authors": [
        "M.-W W.-W. Lin",
        "J.-T Mak",
        "Chien"
      ],
      "year": "2018",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "14",
      "title": "Self-supervised learning for multimedia recommendation",
      "authors": [
        "Z Tao",
        "X Liu",
        "Y Xia",
        "X Wang",
        "L Yang",
        "X Huang",
        "T.-S Chua"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "15",
      "title": "Voicepm: A robust privacy measurement on voice anonymity",
      "authors": [
        "S Zhang",
        "Z Li",
        "A Das"
      ],
      "year": "2023",
      "venue": "Proc. 16th ACM Conference on Security and Privacy in Wireless and Mobile Networks"
    },
    {
      "citation_id": "16",
      "title": "Emobox: Multilingual multi-corpus speech emotion recognition toolkit and benchmark",
      "authors": [
        "Z Ma",
        "M Chen",
        "H Zhang",
        "Z Zheng",
        "W Chen",
        "X Li",
        "J Ye",
        "X Chen",
        "T Hain"
      ],
      "year": "2024",
      "venue": "Emobox: Multilingual multi-corpus speech emotion recognition toolkit and benchmark",
      "arxiv": "arXiv:2406.07162"
    },
    {
      "citation_id": "17",
      "title": "Distilhubert: Speech representation learning by layer-wise distillation of hidden-unit bert",
      "authors": [
        "H.-J Chang",
        "S.-W Yang",
        "H.-Y Lee"
      ],
      "year": "2022",
      "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "18",
      "title": "Exploring wav2vec 2.0 fine tuning for improved speech emotion recognition",
      "authors": [
        "L.-W Chen",
        "A Rudnicky"
      ],
      "year": "2023",
      "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "19",
      "title": "Crosscorpus speech emotion recognition with hubert self-supervised representation",
      "authors": [
        "M Pastor",
        "D Ribas",
        "A Ortega",
        "A Miguel",
        "E Lleida"
      ],
      "year": "2022",
      "venue": "Proc. ISCA Conference IberSPEECH"
    },
    {
      "citation_id": "20",
      "title": "Multi-lingual multi-task speech emotion recognition using wav2vec 2.0",
      "authors": [
        "M Sharma"
      ],
      "year": "2022",
      "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "21",
      "title": "Representation learning through cross-modal conditional teacher-student training for speech emotion recognition",
      "authors": [
        "S Srinivasan",
        "Z Huang",
        "K Kirchhoff"
      ],
      "year": "2022",
      "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "22",
      "title": "Multi-lingual multi-task speech emotion recognition using wav2vec 2.0",
      "authors": [
        "M Sharma"
      ],
      "year": "2022",
      "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "23",
      "title": "A systematic literature review of speech emotion recognition approaches",
      "authors": [
        "Y Singh",
        "S Goel"
      ],
      "year": "2022",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "24",
      "title": "Speech emotion recognition using sequential capsule networks",
      "authors": [
        "X Wu",
        "Y Cao",
        "H Lu",
        "S Liu",
        "D Wang",
        "Z Wu",
        "X Liu",
        "H Meng"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "25",
      "title": "Transformer based unsupervised pre-training for acoustic representation learning",
      "authors": [
        "R Zhang",
        "H Wu",
        "W Li",
        "D Jiang",
        "W Zou",
        "X Li"
      ],
      "year": "2021",
      "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "26",
      "title": "Contrastive unsupervised learning for speech emotion recognition",
      "authors": [
        "M Li",
        "B Yang",
        "J Levy",
        "A Stolcke",
        "V Rozgic",
        "S Matsoukas",
        "C Papayiannis",
        "D Bone",
        "C Wang"
      ],
      "year": "2021",
      "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "27",
      "title": "Cross-corpus classification of realistic emotions-some pilot experiments",
      "authors": [
        "F Eyben",
        "A Batliner",
        "B Schuller",
        "D Seppi",
        "S Steidl"
      ],
      "year": "2010",
      "venue": "Proc. 7th international conference on language resources and evaluation"
    },
    {
      "citation_id": "28",
      "title": "Using multiple databases for training in emotion recognition: To unite or to vote?",
      "authors": [
        "B Schuller",
        "Z Zhang",
        "F Weninger",
        "G Rigoll"
      ],
      "year": "2011",
      "venue": "Proc. Twelfth Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "29",
      "title": "Cross corpus multi-lingual speech emotion recognition using ensemble learning",
      "authors": [
        "W Zehra",
        "A Javed",
        "Z Jalil",
        "H Khan",
        "T Gadekallu"
      ],
      "year": "2021",
      "venue": "Complex & Intelligent Systems"
    },
    {
      "citation_id": "30",
      "title": "Cross lingual speech emotion recognition: Urdu vs. western languages",
      "authors": [
        "S Latif",
        "A Qayyum",
        "M Usman",
        "J Qadir"
      ],
      "year": "2018",
      "venue": "Proc. International conference on frontiers of information technology"
    },
    {
      "citation_id": "31",
      "title": "A study on cross-corpus speech emotion recognition and data augmentation",
      "authors": [
        "N Braunschweiler",
        "R Doddipatla",
        "S Keizer",
        "S Stoyanchev"
      ],
      "year": "2021",
      "venue": "Proc. IEEE Automatic Speech Recognition and Understanding Workshop"
    },
    {
      "citation_id": "32",
      "title": "Wavlm: Large-scale self-supervised pretraining for full stack speech processing",
      "authors": [
        "S Chen",
        "C Wang",
        "Z Chen",
        "Y Wu",
        "S Liu",
        "Z Chen",
        "J Li",
        "N Kanda",
        "T Yoshioka",
        "X Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "33",
      "title": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "authors": [
        "L Pepino",
        "P Riera",
        "L Ferrer"
      ],
      "year": "2021",
      "venue": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "arxiv": "arXiv:2104.03502"
    },
    {
      "citation_id": "34",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM transactions on audio, speech, and language processing"
    },
    {
      "citation_id": "35",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin"
      ],
      "year": "2018",
      "venue": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "36",
      "title": "Unveiling embedded features in wav2vec2 and hubert msodels for speech emotion recognition",
      "authors": [
        "A Chakhtouna",
        "S Sekkate",
        "A Abdellah"
      ],
      "year": "2024",
      "venue": "Procedia Computer Science"
    },
    {
      "citation_id": "37",
      "title": "Layer-wise analysis of a selfsupervised speech representation model",
      "authors": [
        "A Pasad",
        "J.-C Chou",
        "K Livescu"
      ],
      "year": "2021",
      "venue": "Proc. IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)"
    },
    {
      "citation_id": "38",
      "title": "Multiple acoustic features speech emotion recognition using cross-attention transformer",
      "authors": [
        "Y He",
        "N Minematsu",
        "D Saito"
      ],
      "year": "2023",
      "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "39",
      "title": "Speech emotion recognition using local and global features",
      "authors": [
        "Y Gao",
        "B Li",
        "N Wang",
        "T Zhu"
      ],
      "year": "2017",
      "venue": "Proc. Brain Informatics: International Conference"
    },
    {
      "citation_id": "40",
      "title": "Modeling prosodic features with joint factor analysis for speaker verification",
      "authors": [
        "N Dehak",
        "P Dumouchel",
        "P Kenny"
      ],
      "year": "2007",
      "venue": "IEEE Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "41",
      "title": "A novel feature selection method for speech emotion recognition",
      "authors": [
        "T Özseven"
      ],
      "year": "2019",
      "venue": "Applied Acoustics"
    },
    {
      "citation_id": "42",
      "title": "Analysis of linguistic and prosodic features of bilingual arabic-english speakers for speech emotion recognition",
      "authors": [
        "L Abdel-Hamid",
        "N Shaker",
        "I Emara"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "43",
      "title": "Towards an automatic evaluation of the dysarthria level of patients with parkinson's disease",
      "authors": [
        "J Vásquez-Correa",
        "J Orozco-Arroyave",
        "T Bocklet",
        "E Nöth"
      ],
      "year": "2018",
      "venue": "Journal of communication disorders"
    },
    {
      "citation_id": "44",
      "title": "Speech emotion recognition based on multiple acoustic features and deep convolutional neural network",
      "authors": [
        "K Bhangale",
        "M Kothandaraman"
      ],
      "year": "2023",
      "venue": "Electronics"
    },
    {
      "citation_id": "45",
      "title": "Speech emotion recognition using mel frequency log spectrogram and deep convolutional neural network",
      "authors": [
        "K Bhangale",
        "K Mohanaprasad"
      ],
      "year": "2020",
      "venue": "Proc. International Conference on Futuristic Communication and Network Technologies"
    },
    {
      "citation_id": "46",
      "title": "Learning deep features to recognise speech emotion using merged deep CNN",
      "authors": [
        "J Zhao",
        "X Mao",
        "L Chen"
      ],
      "year": "2018",
      "venue": "IET Signal Processing"
    },
    {
      "citation_id": "47",
      "title": "The speakin speaker verification system for far-field speaker verification challenge 2022",
      "authors": [
        "Y Zheng",
        "J Peng",
        "Y Chen",
        "Y Zhang",
        "J Wang",
        "M Liu",
        "M Xu"
      ],
      "year": "2022",
      "venue": "The speakin speaker verification system for far-field speaker verification challenge 2022",
      "arxiv": "arXiv:2209.11625"
    },
    {
      "citation_id": "48",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "49",
      "title": "The ryerson audio-visual database of emotional speech and song RAVDESS: A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "50",
      "title": "Realtime end-to-end speech emotion recognition with cross-domain adaptation",
      "authors": [
        "K Wongpatikaseree",
        "S Singkul",
        "N Hnoohom",
        "S Yuenyong"
      ],
      "year": "2022",
      "venue": "Big Data and Cognitive Computing"
    },
    {
      "citation_id": "51",
      "title": "A database of german emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "52",
      "title": "Emovo corpus: an italian emotional speech database",
      "authors": [
        "G Costantini",
        "I Iaderola",
        "A Paoloni",
        "M Todisco"
      ],
      "year": "2014",
      "venue": "Proc. the ninth international conference on language resources and evaluation"
    },
    {
      "citation_id": "53",
      "title": "The mexican emotional speech database (mesd): elaboration and assessment based on machine learning",
      "authors": [
        "M Duville",
        "L Alonso-Valerdi",
        "D Ibarra-Zarate"
      ],
      "year": "2021",
      "venue": "IEEE Engineering in Medicine & Biology Society"
    },
    {
      "citation_id": "54",
      "title": "Emotional voice conversion: Theory, databases and esd",
      "authors": [
        "K Zhou",
        "B Sisman",
        "R Liu",
        "H Li"
      ],
      "year": "2022",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "55",
      "title": "Towards discriminative representations and unbiased predictions: Class-specific angular softmax for speech emotion recognition",
      "authors": [
        "Z Li",
        "L He",
        "J Li",
        "L Wang",
        "W.-Q Zhang"
      ],
      "year": "2019",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "56",
      "title": "Improving speech emotion recognition using graph attentive bi-directional gated recurrent unit network",
      "authors": [
        "B.-H Su",
        "C.-M Chang",
        "Y.-S Lin",
        "C.-C Lee"
      ],
      "year": "2020",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "57",
      "title": "Hgfm: A hierarchical grained and feature model for acoustic emotion recognition",
      "authors": [
        "Y Xu",
        "H Xu",
        "J Zou"
      ],
      "year": "2020",
      "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "58",
      "title": "Crosscorpus speech emotion recognition with hubert self-supervised representation",
      "authors": [
        "M Pastor",
        "D Ribas",
        "A Ortega",
        "A Miguel",
        "E Lleida"
      ],
      "year": "2022",
      "venue": "Proc. IberSPEECH. ISCA"
    },
    {
      "citation_id": "59",
      "title": "Temporal modeling matters: A novel temporal emotional modeling approach for speech emotion recognition",
      "authors": [
        "J Ye",
        "X.-C Wen",
        "Y Wei",
        "Y Xu",
        "K Liu",
        "H Shan"
      ],
      "year": "2023",
      "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "60",
      "title": "Learning multi-scale features for speech emotion recognition with connection attention mechanism",
      "authors": [
        "Z Chen",
        "J Li",
        "H Liu",
        "X Wang",
        "H Wang",
        "Q Zheng"
      ],
      "year": "2023",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "61",
      "title": "Exploring wav2vec 2.0 fine tuning for improved speech emotion recognition",
      "authors": [
        "L.-W Chen",
        "A Rudnicky"
      ],
      "year": "2023",
      "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "62",
      "title": "Unsupervised adversarial domain adaptation for cross-lingual speech emotion recognition",
      "authors": [
        "S Latif",
        "J Qadir",
        "M Bilal"
      ],
      "year": "2019",
      "venue": "Proc. IEEE International Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "63",
      "title": "Speech emotion recognition from 3D log-mel spectrograms with deep learning network",
      "authors": [
        "H Meng",
        "T Yan",
        "F Yuan",
        "H Wei"
      ],
      "year": "2019",
      "venue": "IEEE access"
    },
    {
      "citation_id": "64",
      "title": "Fusing visual attention CNN and bag of visual words for cross-corpus speech emotion recognition",
      "authors": [
        "M Seo",
        "M Kim"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "65",
      "title": "Cross corpus multi-lingual speech emotion recognition using ensemble learning",
      "authors": [
        "W Zehra",
        "A Javed",
        "Z Jalil",
        "H Khan",
        "T Gadekallu"
      ],
      "year": "2021",
      "venue": "Complex & Intelligent Systems"
    },
    {
      "citation_id": "66",
      "title": "Cross-corpus speech emotion recognition based on few-shot learning and domain adaptation",
      "authors": [
        "Y Ahn",
        "S Lee",
        "J Shin"
      ],
      "year": "2021",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "67",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "68",
      "title": "Enhancing cross-language multimodal emotion recognition with dual attention transformers",
      "authors": [
        "S Zaidi",
        "S Latif",
        "J Qadir"
      ],
      "year": "2024",
      "venue": "IEEE Open Journal of the Computer Society"
    },
    {
      "citation_id": "69",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal",
        "J Du",
        "M Joshi",
        "D Chen",
        "O Levy",
        "M Lewis",
        "L Zettlemoyer",
        "V Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach"
    },
    {
      "citation_id": "70",
      "title": "Xls-r: Self-supervised cross-lingual speech representation learning at scale",
      "authors": [
        "A Babu",
        "C Wang",
        "A Tjandra",
        "K Lakhotia",
        "Q Xu",
        "N Goyal",
        "K Singh",
        "P Von Platen",
        "Y Saraf",
        "J Pino"
      ],
      "year": "2021",
      "venue": "Xls-r: Self-supervised cross-lingual speech representation learning at scale",
      "arxiv": "arXiv:2111.09296"
    },
    {
      "citation_id": "71",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "T Xu",
        "G Brockman",
        "C Mcleavey",
        "I Sutskever"
      ],
      "year": "2023",
      "venue": "International conference on machine learning"
    }
  ]
}