{
  "paper_id": "2506.20199v2",
  "title": "How To Retrieve Examples In In-Context Learning To Improve Conversational Emotion Recognition Using Large Language Models?",
  "published": "2025-06-25T07:39:19Z",
  "authors": [
    "Mengqi Wang",
    "Tiantian Feng",
    "Shrikanth Narayanan"
  ],
  "keywords": [
    "Conversational Emotion Recognition",
    "Large Language Models",
    "In-context Learning",
    "Example Retrieval"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Large language models (LLMs) have enabled a wide variety of real-world applications in various domains. However, creating a high-performing application with high accuracy remains challenging, particularly for subjective tasks like emotion recognition. Inspired by the SLT 2024 GenSER Challenge, this study investigates approaches to improving conversational emotion recognition (CER) by LLMs. Specifically, we explore how to retrieve high-quality examples in in-context learning (ICL) to enhance CER. We propose various strategies based on random and augmented example retrieval and also analyze the impact of conversational context on CER accuracy. Experiments were conducted on the three datasets including IEMO-CAP, MELD and EmoryNLP. The results show that augmented example retrieval consistently outperforms other techniques under investigation across all datasets, highlighting the importance of retrieving coherent targeted examples and enhancing them through paraphrasing.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The advancement in artificial intelligence based on deep learning has catalyzed the development of progressively sophisticated large language models (LLMs) capable of understanding and interpreting human context  [1] . This in turn has facilitated a wide range of applications, including conversational agents, document processing, and educational tools. These varied applications of LLMs have garnered considerable interest in exploring their abilities to acquire \"human-like\" skills. While LLMs have demonstrated skills akin to those of humans, their ability to generate accurate and consistent results requires further investigation to ensure their robustness across various tasks, especially in subjective reasoning tasks such as emotion recognition in conversations. Conversational emotion recognition (CER) involves extracting and analyzing contextual and emotional cues from speech or language contexts to infer human emotions. Recent studies  [2, 3, 4, 5, 6]  have investigated various strategies to enhance the performance of LLMs in emotion prediction. In this paper, inspired by the LLM-Based Post ASR Speech Emotion Recognition Challenge from SLT 2024 GenSER Challenge  [7] , we investigate how to retrieve examples in in-context learning (ICL) to improve CER using the LLMs, as shown in Figure  1 . Our study is based on three distinct datasets: IEMOCAP  [8] , MELD  [9] , and EmoryNLP Emotion Prediction Dataset  [10] .\n\nOne closely related work is by Santoso et al.  [11] , which incorporates conversational context and acoustic features into the prompt to improve the CER performance of GPT-3.5-turbo  1  . They conducted two experiments: one without conversational context (zero-shot) and the other with a maximum of 20 history utterances included (zero-shot with context). Then, the acoustic features (speaking rate, pitch, intensity, etc.) were extracted and categorized as low, normal, and high based on the thresholds derived from the lowest 30% and highest 30% of the numerical values of those features. The evaluations indicated that including conversation segments and acoustic features substantially improves performance compared to single utterances.\n\nDespite this, there are some limitations tied to this work. First, the data confidentiality cannot be guaranteed. The GPT-3.5-turbo is utilized as a cloud-based model, and their prompt includes real speakers' utterances, potentially posing a risk to data privacy. Instead, we mitigate this uncertainty in our study by employing locally deployable open-source models, like the Llama-3 model family  [12] . Second, the study exclusively relied on one dataset, which restricts the generalization of the results. We address this issue by examining the patterns in three datasets.\n\nAnother related study was conducted by Wu et al.  [13]  about ICL  [14] . They introduced self-adaptive in-context learning, which is a general select-then-rank framework supported by Top-K selection  [15]  and the Minimal Description Length (MDL) principle. The Top-K selection filters out a smaller candidate set based on the highest semantic similarity and then decides on the best examples by ranking them with the MDL principle. Compared with prompting without in-context examples, ICL methods generally increase the performance of the language modeling, while the performance is inconsistent with random examples. The proposed self-adaptive ICL method can Figure  2 : Prompts for zero-shot with context and in-context learning. The purple color is the conversation context, the blue color is the target utterance, and the green color is the answers for reference utterances.\n\nsignificantly outperform standard approaches in ICL. We note that our study on conversational emotion recognition using ICL is inspired by the TopK approach proposed in  [13] . In summary, the main contributions and findings of this paper are summarized below:\n\n• First, we investigate how the size of the conversation context impacts the emotion recognition performance and find that expanding the context does not always increase the performance. Specifically, the performance starts to saturate beyond a certain context size. • Second, we explore creating a reference dataset by combining MELD and EmoryNLP used for the baseline ICL. The baseline ICL is performed by providing a set of randomly selected annotated emotional utterances in each prediction. However, giving random utterances as ICL examples does not yield consistent performance improvements. • We develop an ICL framework, the Augmented Example Retrieval (AER), which aims to improve the performance of LLMs by selecting the most coherent example from a reference dataset based on similarity measures computed using SentenceTransformer  [16] . Our results show that AER consistently improves macro F1 across all three test datasets. • We finally find that AER can be adapted to noisy text data obtained from Automatic Speech Recognition (ASR) transcriptions, yielding consistent performance improvements compared to zero-shot baselines.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methods",
      "text": "Our experiment is designed as three steps. First, we evaluate the zero-shot baseline by providing examples in the prompt. Further, we study how the size of the conversation context impacts zero-shot emotion recognition. Next, we perform the ICL baseline experiment by including external reference datasets. Specifically, we construct the prompt by incorporating randomly selected example utterances. Finally, we refine the prompt by choosing more coherent examples for the LLMs with more deterministic cues. Figure  2  shows the prompt templates for different methods.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Zero-Shot Baseline",
      "text": "Given that the conversation context may provide relevant information for LLMs to predict the emotions, our prompt followed the SLT GenSER challenge to include previous utterances of the target utterance. The conversation context size is defined as the number of preceding utterances included in the prompt prior to the target utterance. Specifically, we experiment with the conversation context in {0,",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Icl With Random Examples",
      "text": "Following the zero-shot experiments, either with or without conversation contexts, we further investigate methods to improve the emotion prediction made by the LLMs. Specifically, we focus on in-context learning where prompts are constructed using examples from the combined reference dataset from MELD and EmoryNLP. Here, the reference dataset includes only the training subsets of these two datasets. Unlike conventional ICL, where the examples are directly incorporated into the prompt, we augment the dataset by generating distinct paraphrases. Subsequently, we form the prompt by selecting the examples from the augmented reference dataset. We employ the Mistral-7B model  [17]  for rephrasing. When designing the final prompt, we randomly selected four example utterances, each associated with a distinct emotion from the public dataset.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Icl With Augmented Example Retrieval (Aer)",
      "text": "We introduced a more advanced technique to construct the prompt to the LLM through coherent example retrieval. Specifically, we build the knowledge database by encoding utterances from public datasets using the SentenceTransformer. We empirically investigate the retrieval from the in-domain and outof-domain data sources. By retrieving the more coherent examples of the targeted utterance, we aim to design a better prompt incorporating more relevant context for the prediction. Overall, our technique of Augmented Example Retrieval (AER), as illustrated in Figure  3 , is described below: We employ SentenceTransformer  [16]  to encode all the utterances in the reference dataset. For each target utterance that requires emotion prediction, after encoding, we search for the example utterance based on the cosine similarity. We record its four paraphrases, while the other three examples with the remaining emotions are chosen arbitrarily. We carry out five prediction rounds on the same public dataset and prompt a distinct paraphrase of the most coherent example in each round",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Datasets And Models",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Slt Baseline",
      "text": "This work starts with the SLT 2024 GenSEC Challenge. The objective of the challenge is to scrutinize and enhance the accuracy of emotion perception by large language models (LLMs) with ICL. Currently, it focuses on the prediction of four emotions [happy, sad, neutral, angry]. We evaluate the performance of the prediction with macro F1.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Test Datasets For The Icl",
      "text": "We utilized the Interactive Emotional Dyadic Motion Capture (IEMOCAP)  [8]  dataset provided by the Challenge, which contains the scripts from distinct speakers, with both ground-truth version and automatic speech recognition (ASR) versions provided. In addition, we consider two public datasets, Multimodal EmotionLines Dataset (MELD)  [9]  test data and EmoryNLP Emotion Prediction  [10]  test data. The criterion of our selection is that the utterance will be considered to predict if its emotion is in [happy, sad, neutral, angry]. The statistics of the datasets are given in Table  1 . IEMOCAP  [8]  contains five sessions and each one has a conversation between a male speaker and a female speaker. It has 2577 valid utterances to be predicted. There are groundtruth version and eleven ASR versions for each utterance and we specifically inspect 'Groundtruth', 'Hubertlarge'  [18] , 'W2v2100'  [19] , and 'Whispertiny'  [20]  transcripts in our experiments. MELD  [9]  is referenced from Declare-Lab and consists of utterances from multiple speakers from Friends TV series. We only focus on predicting the utterances with emotion in [Joy, Sadness, Neutral, Anger], which are converted to [happy, sad, neutral, angry]. After the data processing, we predict the emotions of 2211 utterances within the group. For the reference dataset, we utilize 8245 utterances from the training data. EmoryNLP  [10]  is adapted from the public emotion detection data from EmoryNLP group from Emory University. There are 7 emotions in this dataset and we only focus on utterances with emotion in [joyful, sad, neutral, mad] which is mapped to [happy, sad, neutral, angry]. After data processing, the number of utterances we experiment with inlcudes 842 utterances from the test data and 6965 utterances from training data for the reference dataset.\n\nThe rest of the utterances that do not comply with the stan-Figure  4 : The statistics of the augmented reference dataset.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Model Dimension",
      "text": "Mistral-7B-Instruct-v0.2 4096 Meta-Llama-3.1-8B-Instruct 4096\n\nTable  2 : The dimension of the LLMs dard will be incorporated into the conversation context. We also apply data augmentation on the reference dataset to form an augmented reference dataset. The statistics of the dataset are shown in Figure  4 .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Models",
      "text": "Large Language Models (LLMs). We use Mistral-7B-Instruct-v0.2  [17]  to carry out data augmentation. In addition, we use Llama-3.1-8B-Instruct  [12]  in our experiments. Both of these were downloaded from huggingface 2  . The data type for tensor data for these LLMs is Brain Floating Point 16 (bfloat16).\n\nThe models are run with a temperature 0.0001. The dimensions of these LLMs are shown in Table  2 .\n\nText Embedding Model. We use Sentence-Transformer-all-MiniLM-L6-v2  [16] , imported from the SentenceTransformer package, to encode utterances. It maps the sentences to a dense vector space of dimension 384.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Baselines -Zero-Shot Without Examples",
      "text": "The results reveal different prediction behaviors between IEMOCAP and the other two datasets from Figure  5 . On IEMOCAP, we observe that an increase in the conversation context size tends to improve the macro F1 score. Specifically, there is a significant improvement of performance from 0.465 to 0.536 when the conversation context size expands from 0 to 5. This increase in the performance saturated to around 0.567 once the conversation context size reaches 10. However, the LLM achieves the highest performance with no conversation context provided on the other two datasets, 0.576 in MELD and 0.547 in EmoryNLP. The macro F1 decreases to approximately 0.5 as the size of conversation context increases to 10. Based on the results, we continue our experiments on the conversation context size with best results across three datasets.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Icl With Random Examples",
      "text": "According to the results in Table  3 , the incorporation of randomly selected examples from the augmented reference dataset does not consistently boost the overall performance in all three datasets. For IEMOCAP and MELD, the macro F1 is comparable with the baseline results, while there is about 2% improvement on the MELD dataset.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Icl With Aer Results",
      "text": "As shown in",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Icl With Aer On Asr Transcription",
      "text": "Given the improvement of CER with AER, we also achieve more accurate results after using noisy ASR transcripts, relative to the baseline method. As seen in Table  4 , the accuracy improves in all the ASR transcripts cases.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Discussion",
      "text": "In our experimental settings, we found that the larger conversation context size can not consistently improve CER across all three datasets examined. We categorize other predicted emotions to neutral, and MELD and EmoryNLP datasets have relatively imbalanced emotion distribution, where utterances with neutral emotion are significantly more prevalent than the others. We speculate that the LLM tends to respond with more diverse emotions that are not in our emotion list, which are treated as neutral, as less conversation context provided. In addition, when we retrieve random examples for the LLM, it may introduce bias which induces inaccurate prediction, as reflected by our results.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we study how to retrieve examples in ICL to improve emotion recognition in conversations using LLMs. We specifically proposed an augmented example retrieval approach to prompt the LLMs with the most coherent example to the target utterance. Our experiments show that ICL with randomly selected examples performs comparable to baseline zero-shot learning. Moreover, our proposed AER can effectively improve emotion recognition performance over zero-shot learning and ICL with randomly chosen examples. The performance improvement is consistent across all datasets and text input sources (e.g., ASR transcript, human transcript).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Limitations And Future Work",
      "text": "Despite the promise of the proposed AER method, there are limitations that need further refinement. 1) Due to the restriction of GPU capacity, we chose to only experiment on Llama-3.1-8B-Instruct; more complex models should be investigated.\n\n2) The test datasets are limited and cannot assure the generalization of our method. For example, some utterances in the test datasets do not have accurate emotion labels, and some sessions do not have enough conversation context to support the prediction. In future work, we will extend our experiments to more diverse LLMs, such as Gemma-2 Family  [21] . We will also incorporate other richer public datasets, such as MSP-Improv  [22] , and perform data screening and processing on the datasets to be more accurate and complete. Additionally, we also aim to refine the method to enhance the conversational emotion recognition to a wider spectrum of expressed emotions.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Our study is based on three distinct datasets:",
      "page": 1
    },
    {
      "caption": "Figure 1: Overview of our framework: Our goal is to use the",
      "page": 1
    },
    {
      "caption": "Figure 2: Prompts for zero-shot with context and in-context learning. The purple color is the conversation context, the blue color is the",
      "page": 2
    },
    {
      "caption": "Figure 2: shows the prompt templates",
      "page": 2
    },
    {
      "caption": "Figure 3: , is described below:",
      "page": 2
    },
    {
      "caption": "Figure 3: Flow of our proposed ICL with Augmented Example Retrieval (AER). Given reference utterances, we prompt LLM to generate",
      "page": 3
    },
    {
      "caption": "Figure 4: The statistics of the augmented reference dataset.",
      "page": 3
    },
    {
      "caption": "Figure 4: 3.3. Models",
      "page": 3
    },
    {
      "caption": "Figure 5: The fluctuation of macro F1 in different conversation",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Abstract": "Large language models (LLMs) have enabled a wide variety of"
        },
        {
          "Abstract": "real-world applications in various domains. However, creating"
        },
        {
          "Abstract": "a high-performing application with high accuracy remains chal-"
        },
        {
          "Abstract": "lenging, particularly for subjective tasks like emotion recogni-"
        },
        {
          "Abstract": "tion.\nInspired by the SLT 2024 GenSER Challenge,\nthis study"
        },
        {
          "Abstract": "investigates approaches\nto improving conversational emotion"
        },
        {
          "Abstract": "recognition (CER) by LLMs.\nSpecifically, we\nexplore how"
        },
        {
          "Abstract": "to retrieve high-quality examples in in-context\nlearning (ICL)"
        },
        {
          "Abstract": "to enhance CER. We propose various strategies based on ran-"
        },
        {
          "Abstract": "dom and augmented example\nretrieval\nand also analyze\nthe"
        },
        {
          "Abstract": "impact of conversational context on CER accuracy.\nExperi-"
        },
        {
          "Abstract": "ments were conducted on the three datasets including IEMO-"
        },
        {
          "Abstract": "CAP, MELD and EmoryNLP. The results show that augmented"
        },
        {
          "Abstract": "example retrieval consistently outperforms other techniques un-"
        },
        {
          "Abstract": "der investigation across all datasets, highlighting the importance"
        },
        {
          "Abstract": "of\nretrieving coherent\ntargeted examples and enhancing them"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "through paraphrasing."
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "Index Terms:\nConversational Emotion Recognition, Large"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "Language Models, In-context Learning, Example Retrieval"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "1.\nIntroduction"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "The advancement in artificial intelligence based on deep learn-"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "ing has catalyzed the development of progressively sophisti-"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "cated large language models\n(LLMs) capable of understand-"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "ing and interpreting human context\n[1].\nThis\nin turn has\nfa-"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "cilitated a wide range of applications,\nincluding conversational"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "agents, document processing, and educational tools. These var-"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "ied applications of LLMs have garnered considerable interest in"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "exploring their abilities to acquire “human-like” skills. While"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "LLMs have demonstrated skills akin to those of humans,\ntheir"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "ability to generate accurate and consistent results requires fur-"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "ther investigation to ensure their robustness across various tasks,"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "especially in subjective reasoning tasks such as emotion recog-"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "nition in conversations.\nConversational emotion recognition"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "(CER)\ninvolves extracting and analyzing contextual and emo-"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "tional cues from speech or\nlanguage contexts to infer human"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "emotions. Recent studies [2, 3, 4, 5, 6] have investigated var-"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "ious strategies to enhance the performance of LLMs in emo-"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "tion prediction.\nIn this paper,\ninspired by the LLM-Based Post"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "ASR Speech Emotion Recognition Challenge from SLT 2024"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "GenSER Challenge [7], we investigate how to retrieve examples"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "in in-context learning (ICL) to improve CER using the LLMs, as"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "shown in Figure 1. Our study is based on three distinct datasets:"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "IEMOCAP [8], MELD [9], and EmoryNLP Emotion Prediction"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "Dataset [10]."
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "One closely related work is by Santoso et al. [11], which in-"
        },
        {
          "Abstract": "corporates conversational context and acoustic features into the"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 2: Prompts for zero-shot with context and in-context learning. The purple color is the conversation context, the blue color is the": "target utterance, and the green color is the answers for reference utterances."
        },
        {
          "Figure 2: Prompts for zero-shot with context and in-context learning. The purple color is the conversation context, the blue color is the": "significantly outperform standard approaches in ICL. We note"
        },
        {
          "Figure 2: Prompts for zero-shot with context and in-context learning. The purple color is the conversation context, the blue color is the": "that our study on conversational emotion recognition using ICL"
        },
        {
          "Figure 2: Prompts for zero-shot with context and in-context learning. The purple color is the conversation context, the blue color is the": ""
        },
        {
          "Figure 2: Prompts for zero-shot with context and in-context learning. The purple color is the conversation context, the blue color is the": "is inspired by the TopK approach proposed in [13]. In summary,"
        },
        {
          "Figure 2: Prompts for zero-shot with context and in-context learning. The purple color is the conversation context, the blue color is the": ""
        },
        {
          "Figure 2: Prompts for zero-shot with context and in-context learning. The purple color is the conversation context, the blue color is the": "the main contributions and findings of\nthis paper are summa-"
        },
        {
          "Figure 2: Prompts for zero-shot with context and in-context learning. The purple color is the conversation context, the blue color is the": ""
        },
        {
          "Figure 2: Prompts for zero-shot with context and in-context learning. The purple color is the conversation context, the blue color is the": "rized below:"
        },
        {
          "Figure 2: Prompts for zero-shot with context and in-context learning. The purple color is the conversation context, the blue color is the": "•\nFirst, we investigate how the size of the conversation context"
        },
        {
          "Figure 2: Prompts for zero-shot with context and in-context learning. The purple color is the conversation context, the blue color is the": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "that our study on conversational emotion recognition using ICL": ""
        },
        {
          "that our study on conversational emotion recognition using ICL": "is inspired by the TopK approach proposed in [13]. In summary,"
        },
        {
          "that our study on conversational emotion recognition using ICL": ""
        },
        {
          "that our study on conversational emotion recognition using ICL": "the main contributions and findings of\nthis paper are summa-"
        },
        {
          "that our study on conversational emotion recognition using ICL": ""
        },
        {
          "that our study on conversational emotion recognition using ICL": "rized below:"
        },
        {
          "that our study on conversational emotion recognition using ICL": "•\nFirst, we investigate how the size of the conversation context"
        },
        {
          "that our study on conversational emotion recognition using ICL": ""
        },
        {
          "that our study on conversational emotion recognition using ICL": "impacts the emotion recognition performance and find that"
        },
        {
          "that our study on conversational emotion recognition using ICL": ""
        },
        {
          "that our study on conversational emotion recognition using ICL": "expanding the context does not always increase the perfor-"
        },
        {
          "that our study on conversational emotion recognition using ICL": "mance.\nSpecifically,\nthe performance starts to saturate be-"
        },
        {
          "that our study on conversational emotion recognition using ICL": "yond a certain context size."
        },
        {
          "that our study on conversational emotion recognition using ICL": "•\nSecond, we explore creating a reference dataset by combin-"
        },
        {
          "that our study on conversational emotion recognition using ICL": ""
        },
        {
          "that our study on conversational emotion recognition using ICL": "ing MELD and EmoryNLP used for\nthe baseline ICL. The"
        },
        {
          "that our study on conversational emotion recognition using ICL": "baseline ICL is performed by providing a set of\nrandomly"
        },
        {
          "that our study on conversational emotion recognition using ICL": ""
        },
        {
          "that our study on conversational emotion recognition using ICL": "selected annotated emotional utterances in each prediction."
        },
        {
          "that our study on conversational emotion recognition using ICL": "However, giving random utterances as\nICL examples does"
        },
        {
          "that our study on conversational emotion recognition using ICL": "not yield consistent performance improvements."
        },
        {
          "that our study on conversational emotion recognition using ICL": "• We develop an ICL framework, the Augmented Example Re-"
        },
        {
          "that our study on conversational emotion recognition using ICL": "trieval\n(AER), which aims\nto improve the performance of"
        },
        {
          "that our study on conversational emotion recognition using ICL": "LLMs by selecting the most coherent example from a ref-"
        },
        {
          "that our study on conversational emotion recognition using ICL": "erence dataset based on similarity measures computed using"
        },
        {
          "that our study on conversational emotion recognition using ICL": "SentenceTransformer [16]. Our results show that AER con-"
        },
        {
          "that our study on conversational emotion recognition using ICL": "sistently improves macro F1 across all three test datasets."
        },
        {
          "that our study on conversational emotion recognition using ICL": ""
        },
        {
          "that our study on conversational emotion recognition using ICL": "• We finally find that AER can be adapted to noisy text data ob-"
        },
        {
          "that our study on conversational emotion recognition using ICL": ""
        },
        {
          "that our study on conversational emotion recognition using ICL": "tained from Automatic Speech Recognition (ASR) transcrip-"
        },
        {
          "that our study on conversational emotion recognition using ICL": ""
        },
        {
          "that our study on conversational emotion recognition using ICL": "tions, yielding consistent performance improvements com-"
        },
        {
          "that our study on conversational emotion recognition using ICL": ""
        },
        {
          "that our study on conversational emotion recognition using ICL": "pared to zero-shot baselines."
        },
        {
          "that our study on conversational emotion recognition using ICL": ""
        },
        {
          "that our study on conversational emotion recognition using ICL": ""
        },
        {
          "that our study on conversational emotion recognition using ICL": "2. Methods"
        },
        {
          "that our study on conversational emotion recognition using ICL": "Our experiment\nis designed as three steps.\nFirst, we evaluate"
        },
        {
          "that our study on conversational emotion recognition using ICL": "the zero-shot baseline by providing examples\nin the prompt."
        },
        {
          "that our study on conversational emotion recognition using ICL": ""
        },
        {
          "that our study on conversational emotion recognition using ICL": "Further, we study how the size of the conversation context\nim-"
        },
        {
          "that our study on conversational emotion recognition using ICL": ""
        },
        {
          "that our study on conversational emotion recognition using ICL": "pacts zero-shot emotion recognition. Next, we perform the ICL"
        },
        {
          "that our study on conversational emotion recognition using ICL": ""
        },
        {
          "that our study on conversational emotion recognition using ICL": "baseline experiment by including external\nreference datasets."
        },
        {
          "that our study on conversational emotion recognition using ICL": ""
        },
        {
          "that our study on conversational emotion recognition using ICL": "Specifically, we\nconstruct\nthe prompt by incorporating ran-"
        },
        {
          "that our study on conversational emotion recognition using ICL": ""
        },
        {
          "that our study on conversational emotion recognition using ICL": "domly\nselected\nexample\nutterances.\nFinally, we\nrefine\nthe"
        },
        {
          "that our study on conversational emotion recognition using ICL": ""
        },
        {
          "that our study on conversational emotion recognition using ICL": "prompt by choosing more coherent examples for the LLMs with"
        },
        {
          "that our study on conversational emotion recognition using ICL": ""
        },
        {
          "that our study on conversational emotion recognition using ICL": "more deterministic cues. Figure 2 shows the prompt\ntemplates"
        },
        {
          "that our study on conversational emotion recognition using ICL": ""
        },
        {
          "that our study on conversational emotion recognition using ICL": "for different methods."
        },
        {
          "that our study on conversational emotion recognition using ICL": ""
        },
        {
          "that our study on conversational emotion recognition using ICL": ""
        },
        {
          "that our study on conversational emotion recognition using ICL": "2.1. Zero-Shot Baseline"
        },
        {
          "that our study on conversational emotion recognition using ICL": ""
        },
        {
          "that our study on conversational emotion recognition using ICL": "Given that the conversation context may provide relevant infor-"
        },
        {
          "that our study on conversational emotion recognition using ICL": "mation for LLMs to predict the emotions, our prompt followed"
        },
        {
          "that our study on conversational emotion recognition using ICL": "the SLT GenSER challenge to include previous utterances of the"
        },
        {
          "that our study on conversational emotion recognition using ICL": "target utterance. The conversation context size is defined as the"
        },
        {
          "that our study on conversational emotion recognition using ICL": "number of preceding utterances included in the prompt prior to"
        },
        {
          "that our study on conversational emotion recognition using ICL": "the target utterance. Specifically, we experiment with the con-"
        },
        {
          "that our study on conversational emotion recognition using ICL": "versation context in {0, 5, 10, 15, 20}. Note that a conversation"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 3: Flow of our proposed ICL with Augmented Example Retrieval (AER). Given reference utterances, we prompt LLM to generate": "four paraphrases for each one and encode them with SentenceTransformer. Every time we have a target utterance during prediction, we"
        },
        {
          "Figure 3: Flow of our proposed ICL with Augmented Example Retrieval (AER). Given reference utterances, we prompt LLM to generate": "encode it and find the reference utterance with the highest cosine similarity, while selecting another three examples with the remaining"
        },
        {
          "Figure 3: Flow of our proposed ICL with Augmented Example Retrieval (AER). Given reference utterances, we prompt LLM to generate": "emotions arbitrarily. We design prompts for in-context learning accordingly, as shown above."
        },
        {
          "Figure 3: Flow of our proposed ICL with Augmented Example Retrieval (AER). Given reference utterances, we prompt LLM to generate": "while keeping the other examples unchanged. The results are"
        },
        {
          "Figure 3: Flow of our proposed ICL with Augmented Example Retrieval (AER). Given reference utterances, we prompt LLM to generate": "determined by the majority votes after five rounds."
        },
        {
          "Figure 3: Flow of our proposed ICL with Augmented Example Retrieval (AER). Given reference utterances, we prompt LLM to generate": "3. Datasets and Models"
        },
        {
          "Figure 3: Flow of our proposed ICL with Augmented Example Retrieval (AER). Given reference utterances, we prompt LLM to generate": "3.1.\nSLT Baseline"
        },
        {
          "Figure 3: Flow of our proposed ICL with Augmented Example Retrieval (AER). Given reference utterances, we prompt LLM to generate": "This work starts with the SLT 2024 GenSEC Challenge. The"
        },
        {
          "Figure 3: Flow of our proposed ICL with Augmented Example Retrieval (AER). Given reference utterances, we prompt LLM to generate": "objective of the challenge is to scrutinize and enhance the accu-"
        },
        {
          "Figure 3: Flow of our proposed ICL with Augmented Example Retrieval (AER). Given reference utterances, we prompt LLM to generate": "racy of emotion perception by large language models (LLMs)"
        },
        {
          "Figure 3: Flow of our proposed ICL with Augmented Example Retrieval (AER). Given reference utterances, we prompt LLM to generate": "with ICL. Currently,\nit focuses on the prediction of four emo-"
        },
        {
          "Figure 3: Flow of our proposed ICL with Augmented Example Retrieval (AER). Given reference utterances, we prompt LLM to generate": "tions [happy,\nsad, neutral, angry]. We evaluate the perfor-"
        },
        {
          "Figure 3: Flow of our proposed ICL with Augmented Example Retrieval (AER). Given reference utterances, we prompt LLM to generate": "mance of the prediction with macro F1."
        },
        {
          "Figure 3: Flow of our proposed ICL with Augmented Example Retrieval (AER). Given reference utterances, we prompt LLM to generate": "3.2. Test Datasets for the ICL"
        },
        {
          "Figure 3: Flow of our proposed ICL with Augmented Example Retrieval (AER). Given reference utterances, we prompt LLM to generate": "We utilized the Interactive Emotional Dyadic Motion Capture"
        },
        {
          "Figure 3: Flow of our proposed ICL with Augmented Example Retrieval (AER). Given reference utterances, we prompt LLM to generate": "(IEMOCAP) [8] dataset provided by the Challenge, which con-"
        },
        {
          "Figure 3: Flow of our proposed ICL with Augmented Example Retrieval (AER). Given reference utterances, we prompt LLM to generate": "tains the scripts from distinct speakers, with both ground-truth"
        },
        {
          "Figure 3: Flow of our proposed ICL with Augmented Example Retrieval (AER). Given reference utterances, we prompt LLM to generate": "version and automatic speech recognition (ASR) versions pro-"
        },
        {
          "Figure 3: Flow of our proposed ICL with Augmented Example Retrieval (AER). Given reference utterances, we prompt LLM to generate": "vided. In addition, we consider two public datasets, Multimodal"
        },
        {
          "Figure 3: Flow of our proposed ICL with Augmented Example Retrieval (AER). Given reference utterances, we prompt LLM to generate": ""
        },
        {
          "Figure 3: Flow of our proposed ICL with Augmented Example Retrieval (AER). Given reference utterances, we prompt LLM to generate": "EmotionLines Dataset\n(MELD)\n[9]\ntest data and EmoryNLP"
        },
        {
          "Figure 3: Flow of our proposed ICL with Augmented Example Retrieval (AER). Given reference utterances, we prompt LLM to generate": ""
        },
        {
          "Figure 3: Flow of our proposed ICL with Augmented Example Retrieval (AER). Given reference utterances, we prompt LLM to generate": "Emotion Prediction [10] test data. The criterion of our selection"
        },
        {
          "Figure 3: Flow of our proposed ICL with Augmented Example Retrieval (AER). Given reference utterances, we prompt LLM to generate": "is that the utterance will be considered to predict if its emotion"
        },
        {
          "Figure 3: Flow of our proposed ICL with Augmented Example Retrieval (AER). Given reference utterances, we prompt LLM to generate": ""
        },
        {
          "Figure 3: Flow of our proposed ICL with Augmented Example Retrieval (AER). Given reference utterances, we prompt LLM to generate": "is in [happy, sad, neutral, angry]. The statistics of the datasets"
        },
        {
          "Figure 3: Flow of our proposed ICL with Augmented Example Retrieval (AER). Given reference utterances, we prompt LLM to generate": "are given in Table 1."
        },
        {
          "Figure 3: Flow of our proposed ICL with Augmented Example Retrieval (AER). Given reference utterances, we prompt LLM to generate": "IEMOCAP [8]\ncontains five\nsessions\nand\neach\none\nhas\na"
        },
        {
          "Figure 3: Flow of our proposed ICL with Augmented Example Retrieval (AER). Given reference utterances, we prompt LLM to generate": "conversation between a male\nspeaker\nand a\nfemale\nspeaker."
        },
        {
          "Figure 3: Flow of our proposed ICL with Augmented Example Retrieval (AER). Given reference utterances, we prompt LLM to generate": "It\nhas\n2577\nvalid\nutterances\nto\nbe\npredicted.\nThere\nare"
        },
        {
          "Figure 3: Flow of our proposed ICL with Augmented Example Retrieval (AER). Given reference utterances, we prompt LLM to generate": "groundtruth version and eleven ASR versions\nfor each utter-"
        },
        {
          "Figure 3: Flow of our proposed ICL with Augmented Example Retrieval (AER). Given reference utterances, we prompt LLM to generate": "ance and we specifically inspect ’Groundtruth’,\n’Hubertlarge’"
        },
        {
          "Figure 3: Flow of our proposed ICL with Augmented Example Retrieval (AER). Given reference utterances, we prompt LLM to generate": "[18], ’W2v2100’ [19], and ’Whispertiny’ [20] transcripts in our"
        },
        {
          "Figure 3: Flow of our proposed ICL with Augmented Example Retrieval (AER). Given reference utterances, we prompt LLM to generate": "experiments."
        },
        {
          "Figure 3: Flow of our proposed ICL with Augmented Example Retrieval (AER). Given reference utterances, we prompt LLM to generate": "MELD [9] is referenced from Declare-Lab and consists of ut-"
        },
        {
          "Figure 3: Flow of our proposed ICL with Augmented Example Retrieval (AER). Given reference utterances, we prompt LLM to generate": ""
        },
        {
          "Figure 3: Flow of our proposed ICL with Augmented Example Retrieval (AER). Given reference utterances, we prompt LLM to generate": "terances from multiple speakers from Friends TV series. We"
        },
        {
          "Figure 3: Flow of our proposed ICL with Augmented Example Retrieval (AER). Given reference utterances, we prompt LLM to generate": "only focus on predicting the utterances with emotion in [Joy,"
        },
        {
          "Figure 3: Flow of our proposed ICL with Augmented Example Retrieval (AER). Given reference utterances, we prompt LLM to generate": "Sadness, Neutral, Anger], which are converted to [happy, sad,"
        },
        {
          "Figure 3: Flow of our proposed ICL with Augmented Example Retrieval (AER). Given reference utterances, we prompt LLM to generate": "neutral, angry]. After the data processing, we predict\nthe emo-"
        },
        {
          "Figure 3: Flow of our proposed ICL with Augmented Example Retrieval (AER). Given reference utterances, we prompt LLM to generate": "tions of 2211 utterances within the group.\nFor\nthe reference"
        },
        {
          "Figure 3: Flow of our proposed ICL with Augmented Example Retrieval (AER). Given reference utterances, we prompt LLM to generate": "dataset, we utilize 8245 utterances from the training data."
        },
        {
          "Figure 3: Flow of our proposed ICL with Augmented Example Retrieval (AER). Given reference utterances, we prompt LLM to generate": "EmoryNLP [10] is adapted from the public emotion detection"
        },
        {
          "Figure 3: Flow of our proposed ICL with Augmented Example Retrieval (AER). Given reference utterances, we prompt LLM to generate": "data from EmoryNLP group from Emory University.\nThere"
        },
        {
          "Figure 3: Flow of our proposed ICL with Augmented Example Retrieval (AER). Given reference utterances, we prompt LLM to generate": "are 7 emotions in this dataset and we only focus on utterances"
        },
        {
          "Figure 3: Flow of our proposed ICL with Augmented Example Retrieval (AER). Given reference utterances, we prompt LLM to generate": ""
        },
        {
          "Figure 3: Flow of our proposed ICL with Augmented Example Retrieval (AER). Given reference utterances, we prompt LLM to generate": "with emotion in [joyful, sad, neutral, mad] which is mapped to"
        },
        {
          "Figure 3: Flow of our proposed ICL with Augmented Example Retrieval (AER). Given reference utterances, we prompt LLM to generate": ""
        },
        {
          "Figure 3: Flow of our proposed ICL with Augmented Example Retrieval (AER). Given reference utterances, we prompt LLM to generate": "[happy, sad, neutral, angry]. After data processing,\nthe num-"
        },
        {
          "Figure 3: Flow of our proposed ICL with Augmented Example Retrieval (AER). Given reference utterances, we prompt LLM to generate": ""
        },
        {
          "Figure 3: Flow of our proposed ICL with Augmented Example Retrieval (AER). Given reference utterances, we prompt LLM to generate": "ber of utterances we experiment with inlcudes 842 utterances"
        },
        {
          "Figure 3: Flow of our proposed ICL with Augmented Example Retrieval (AER). Given reference utterances, we prompt LLM to generate": ""
        },
        {
          "Figure 3: Flow of our proposed ICL with Augmented Example Retrieval (AER). Given reference utterances, we prompt LLM to generate": "from the test data and 6965 utterances from training data for the"
        },
        {
          "Figure 3: Flow of our proposed ICL with Augmented Example Retrieval (AER). Given reference utterances, we prompt LLM to generate": "reference dataset."
        },
        {
          "Figure 3: Flow of our proposed ICL with Augmented Example Retrieval (AER). Given reference utterances, we prompt LLM to generate": "The rest of the utterances that do not comply with the stan-"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 4: , theaccuracy",
      "data": [
        {
          "4.4.\nICL with AER on ASR Transcription": "Given the improvement of CER with AER, we also achieve"
        },
        {
          "4.4.\nICL with AER on ASR Transcription": "more accurate results after using noisy ASR transcripts,\nrela-"
        },
        {
          "4.4.\nICL with AER on ASR Transcription": "tive to the baseline method. As seen in Table 4,\nthe accuracy"
        },
        {
          "4.4.\nICL with AER on ASR Transcription": "improves in all the ASR transcripts cases."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[9]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": "and R. Mihalcea, “Meld: A multimodal multi-party dataset\nfor"
        },
        {
          "[9]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": "emotion recognition in conversations,” 2019. [Online]. Available:"
        },
        {
          "[9]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": "https://arxiv.org/abs/1810.02508"
        },
        {
          "[9]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": "[10]\nS. M. Zahiri and J. D. Choi, “Emotion detection on tv show tran-"
        },
        {
          "[9]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": "scripts with sequence-based convolutional neural networks,”\nin"
        },
        {
          "[9]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": "Workshops at\nthe thirty-second aaai conference on artificial\nin-"
        },
        {
          "[9]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": "telligence, 2018."
        },
        {
          "[9]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": "[11]\nJ. Santoso, K.\nIshizuka,\nand T. Hashimoto,\n“Large\nlanguage"
        },
        {
          "[9]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": "model-based\nemotional\nspeech\nannotation\nusing\ncontext\nand"
        },
        {
          "[9]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": "acoustic feature for speech emotion recognition,” in ICASSP 2024"
        },
        {
          "[9]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": "- 2024 IEEE International Conference on Acoustics, Speech and"
        },
        {
          "[9]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": "Signal Processing (ICASSP), 2024, pp. 11 026–11 030."
        },
        {
          "[9]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": "[12] A. Dubey, A.\nJauhri, A.\nPandey, A. Kadian, A. Al-Dahle"
        },
        {
          "[9]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": "et al., “The llama 3 herd of models,” 2024. [Online]. Available:"
        },
        {
          "[9]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": "https://arxiv.org/abs/2407.21783"
        },
        {
          "[9]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": "[13]\nZ. Wu,\nY\n. Wang,\nJ. Ye,\nand\nL. Kong,\n“Self-adaptive\nin-"
        },
        {
          "[9]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": "context\nlearning: An information compression perspective\nfor"
        },
        {
          "[9]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": "arXiv\npreprint\nin-context\nexample\nselection\nand\nordering,”"
        },
        {
          "[9]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": "arXiv:2212.10375, 2022."
        },
        {
          "[9]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": "[14]\nS. Min, X. Lyu, A. Holtzman, M. Artetxe, M. Lewis, H. Ha-"
        },
        {
          "[9]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": "jishirzi, and L. Zettlemoyer, “Rethinking the role of demonstra-"
        },
        {
          "[9]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": "tions: What makes in-context\nlearning work?” in Proceedings of"
        },
        {
          "[9]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": "the 2022 Conference on Empirical Methods in Natural Language"
        },
        {
          "[9]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": "Processing, 2022, pp. 11 048–11 064."
        },
        {
          "[9]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": "[15]\nJ. Liu, D. Shen, Y. Zhang, B. Dolan, L. Carin, and W. Chen, “What"
        },
        {
          "[9]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": "arXiv\npreprint\nmakes\ngood\nin-context\nexamples\nfor\ngpt-3?”"
        },
        {
          "[9]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": "arXiv:2101.06804, 2021."
        },
        {
          "[9]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": "[16] N.\nReimers\nand\nI.\nGurevych,\n“Sentence-bert:\nSentence"
        },
        {
          "[9]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": "embeddings\nusing\nsiamese\nbert-networks,”\n2019.\n[Online]."
        },
        {
          "[9]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": "Available: https://arxiv.org/abs/1908.10084"
        },
        {
          "[9]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": "[17] A. Q.\nJiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S."
        },
        {
          "[9]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": "Chaplot, D. de las Casas, F. Bressand, G. Lengyel, G. Lample,"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "7. References": "",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": "nov, and A. Mohamed, “Hubert: Self-supervised speech represen-"
        },
        {
          "7. References": "[1] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” nature, vol.",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": "tation learning by masked prediction of hidden units,” IEEE/ACM"
        },
        {
          "7. References": "521, no. 7553, pp. 436–444, 2015.",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": "transactions on audio, speech, and language processing, vol. 29,"
        },
        {
          "7. References": "[2] H. Wu, H.-C. Chou, K.-W. Chan, L. Goncalves, J. Du, J.-S. Jang,",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": "pp. 3451–3460, 2021."
        },
        {
          "7. References": "C.-C. Lee, and H.-y. Lee, “Empower typed descriptions by large",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec"
        },
        {
          "7. References": "language models for speech emotion recognition,” 09 2024.",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": "2.0: A framework for self-supervised learning of speech repre-"
        },
        {
          "7. References": "[3] Y. Li, Y. Gong, C.-H. H. Yang,\nP. Bell,\nand C. Lai,\n“Re-",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": "sentations,” Advances in neural\ninformation processing systems,"
        },
        {
          "7. References": "vise,\nreason, and recognize: Llm-based emotion recognition via",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": "vol. 33, pp. 12 449–12 460, 2020."
        },
        {
          "7. References": "emotion-specific prompts and asr error correction,” arXiv preprint",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": "[20] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and"
        },
        {
          "7. References": "arXiv:2409.15551, 2024.",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": "I. Sutskever,\n“Robust\nspeech recognition via\nlarge-scale weak"
        },
        {
          "7. References": "[4]\nJ. Kyung, S. Heo, and J.-H. Chang, “Enhancing multimodal emo-",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": "supervision,”\nin International conference on machine learning."
        },
        {
          "7. References": "tion recognition through asr\nerror\ncompensation and llm fine-",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": "PMLR, 2023, pp. 28 492–28 518."
        },
        {
          "7. References": "tuning,” in Proc. Interspeech 2024, 2024, pp. 4683–4687.",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": "[21] G. Team, M. Riviere, S. Pathak, P. G. Sessa, C. Hardin, S. Bhu-"
        },
        {
          "7. References": "[5]\nZ. Wu, Z. Gong, L. Ai, P. Shi, K. Donbekci, and J. Hirschberg,",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": "patiraju, L. Hussenot, T. Mesnard, B. Shahriari, A. Ram´e et al.,"
        },
        {
          "7. References": "“Beyond silent\nletters: Amplifying llms in emotion recognition",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": "“Gemma 2: Improving open language models at a practical size,”"
        },
        {
          "7. References": "with vocal nuances,” arXiv preprint arXiv:2407.21315, 2024.",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": "arXiv preprint arXiv:2408.00118, 2024."
        },
        {
          "7. References": "[6]\nT. Feng and S. Narayanan, “Foundation model assisted automatic",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "speech emotion recognition: Transcribing, annotating, and aug-",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": "[22] C. Busso,\nS.\nParthasarathy, A. Burmania, M. AbdelWahab,"
        },
        {
          "7. References": "menting,” in ICASSP 2024-2024 IEEE International Conference",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": "N. Sadoughi, and E. M. Provost, “Msp-improv: An acted corpus"
        },
        {
          "7. References": "on Acoustics, Speech and Signal Processing (ICASSP).\nIEEE,",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": "of dyadic interactions to study emotion perception,” IEEE Trans-"
        },
        {
          "7. References": "2024, pp. 12 116–12 120.",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": "actions on Affective Computing, vol. 8, no. 1, pp. 67–80, 2016."
        },
        {
          "7. References": "[7] C.-H. H. Yang, T. Park, Y. Gong, Y. Li, Z. Chen, Y.-T. Lin,",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "˙\nC. Chen, Y. Hu, K. Dhawan, P.\nZelasko, C. Zhang, Y.-N. Chen,",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "Y\n. Tsao,\nJ. Balam, B. Ginsburg, S. M. Siniscalchi, E. S. Chng,",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "P. Bell, C. Lai, S. Watanabe, and A. Stolcke, “Large language",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "model based generative error correction: A challenge and base-",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "lines for speech recognition, speaker tagging, and emotion recog-",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "nition,”\nin 2024 IEEE Spoken Language Technology Workshop",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "(SLT), 2024, pp. 371–378.",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "[8] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower,",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "S. Kim,\nJ. N. Chang, S. Lee, and S. S. Narayanan, “Iemocap:",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "Interactive emotional dyadic motion capture database,” Language",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "resources and evaluation, vol. 42, pp. 335–359, 2008.",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "[9]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "and R. Mihalcea, “Meld: A multimodal multi-party dataset\nfor",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "emotion recognition in conversations,” 2019. [Online]. Available:",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "https://arxiv.org/abs/1810.02508",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "[10]\nS. M. Zahiri and J. D. Choi, “Emotion detection on tv show tran-",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "scripts with sequence-based convolutional neural networks,”\nin",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "Workshops at\nthe thirty-second aaai conference on artificial\nin-",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "telligence, 2018.",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "[11]\nJ. Santoso, K.\nIshizuka,\nand T. Hashimoto,\n“Large\nlanguage",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "model-based\nemotional\nspeech\nannotation\nusing\ncontext\nand",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "acoustic feature for speech emotion recognition,” in ICASSP 2024",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "- 2024 IEEE International Conference on Acoustics, Speech and",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "Signal Processing (ICASSP), 2024, pp. 11 026–11 030.",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "[12] A. Dubey, A.\nJauhri, A.\nPandey, A. Kadian, A. Al-Dahle",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "et al., “The llama 3 herd of models,” 2024. [Online]. Available:",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "https://arxiv.org/abs/2407.21783",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "[13]\nZ. Wu,\nY\n. Wang,\nJ. Ye,\nand\nL. Kong,\n“Self-adaptive\nin-",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "context\nlearning: An information compression perspective\nfor",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "arXiv\npreprint\nin-context\nexample\nselection\nand\nordering,”",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "arXiv:2212.10375, 2022.",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "[14]\nS. Min, X. Lyu, A. Holtzman, M. Artetxe, M. Lewis, H. Ha-",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "jishirzi, and L. Zettlemoyer, “Rethinking the role of demonstra-",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "tions: What makes in-context\nlearning work?” in Proceedings of",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "the 2022 Conference on Empirical Methods in Natural Language",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "Processing, 2022, pp. 11 048–11 064.",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "[15]\nJ. Liu, D. Shen, Y. Zhang, B. Dolan, L. Carin, and W. Chen, “What",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "arXiv\npreprint\nmakes\ngood\nin-context\nexamples\nfor\ngpt-3?”",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "arXiv:2101.06804, 2021.",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "[16] N.\nReimers\nand\nI.\nGurevych,\n“Sentence-bert:\nSentence",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "embeddings\nusing\nsiamese\nbert-networks,”\n2019.\n[Online].",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "Available: https://arxiv.org/abs/1908.10084",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "[17] A. Q.\nJiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S.",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "Chaplot, D. de las Casas, F. Bressand, G. Lengyel, G. Lample,",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "L. Saulnier, L. R. Lavaud, M.-A. Lachaux, P. Stock, T. L. Scao,",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "T. Lavril, T. Wang, T. Lacroix, and W. E. Sayed, “Mistral 7b,”",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        },
        {
          "7. References": "2023. [Online]. Available: https://arxiv.org/abs/2310.06825",
          "[18] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Deep learning",
      "authors": [
        "Y Lecun",
        "Y Bengio",
        "G Hinton"
      ],
      "year": "2015",
      "venue": "nature"
    },
    {
      "citation_id": "3",
      "title": "Empower typed descriptions by large language models for speech emotion recognition",
      "authors": [
        "H Wu",
        "H.-C Chou",
        "K.-W Chan",
        "L Goncalves",
        "J Du",
        "J.-S Jang",
        "C.-C Lee",
        "H.-Y Lee"
      ],
      "venue": "Empower typed descriptions by large language models for speech emotion recognition"
    },
    {
      "citation_id": "4",
      "title": "Revise, reason, and recognize: Llm-based emotion recognition via emotion-specific prompts and asr error correction",
      "authors": [
        "Y Li",
        "Y Gong",
        "C.-H Yang",
        "P Bell",
        "C Lai"
      ],
      "year": "2024",
      "venue": "Revise, reason, and recognize: Llm-based emotion recognition via emotion-specific prompts and asr error correction",
      "arxiv": "arXiv:2409.15551"
    },
    {
      "citation_id": "5",
      "title": "Enhancing multimodal emotion recognition through asr error compensation and llm finetuning",
      "authors": [
        "J Kyung",
        "S Heo",
        "J.-H Chang"
      ],
      "year": "2024",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "6",
      "title": "Beyond silent letters: Amplifying llms in emotion recognition with vocal nuances",
      "authors": [
        "Z Wu",
        "Z Gong",
        "L Ai",
        "P Shi",
        "K Donbekci",
        "J Hirschberg"
      ],
      "year": "2024",
      "venue": "Beyond silent letters: Amplifying llms in emotion recognition with vocal nuances",
      "arxiv": "arXiv:2407.21315"
    },
    {
      "citation_id": "7",
      "title": "Foundation model assisted automatic speech emotion recognition: Transcribing, annotating, and augmenting",
      "authors": [
        "T Feng",
        "S Narayanan"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "8",
      "title": "Large language model based generative error correction: A challenge and baselines for speech recognition, speaker tagging, and emotion recognition",
      "authors": [
        "C.-H Yang",
        "T Park",
        "Y Gong",
        "Y Li",
        "Z Chen",
        "Y.-T Lin",
        "C Chen",
        "Y Hu",
        "K Dhawan",
        "P Żelasko",
        "C Zhang",
        "Y.-N Chen",
        "Y Tsao",
        "J Balam",
        "B Ginsburg",
        "S Siniscalchi",
        "E Chng",
        "P Bell",
        "C Lai",
        "S Watanabe",
        "A Stolcke"
      ],
      "year": "2024",
      "venue": "2024 IEEE Spoken Language Technology Workshop"
    },
    {
      "citation_id": "9",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "10",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "Meld: A multimodal multi-party dataset for emotion recognition in conversations"
    },
    {
      "citation_id": "11",
      "title": "Emotion detection on tv show transcripts with sequence-based convolutional neural networks",
      "authors": [
        "S Zahiri",
        "J Choi"
      ],
      "year": "2018",
      "venue": "Workshops at the thirty-second aaai conference on artificial intelligence"
    },
    {
      "citation_id": "12",
      "title": "Large language model-based emotional speech annotation using context and acoustic feature for speech emotion recognition",
      "authors": [
        "J Santoso",
        "K Ishizuka",
        "T Hashimoto"
      ],
      "year": "2024",
      "venue": "ICASSP 2024 -2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "13",
      "title": "The llama 3 herd of models",
      "authors": [
        "A Dubey",
        "A Jauhri",
        "A Pandey",
        "A Kadian",
        "A Al-Dahle"
      ],
      "year": "2024",
      "venue": "The llama 3 herd of models"
    },
    {
      "citation_id": "14",
      "title": "Self-adaptive incontext learning: An information compression perspective for in-context example selection and ordering",
      "authors": [
        "Z Wu",
        "Y Wang",
        "J Ye",
        "L Kong"
      ],
      "year": "2022",
      "venue": "Self-adaptive incontext learning: An information compression perspective for in-context example selection and ordering",
      "arxiv": "arXiv:2212.10375"
    },
    {
      "citation_id": "15",
      "title": "Rethinking the role of demonstrations: What makes in-context learning work",
      "authors": [
        "S Min",
        "X Lyu",
        "A Holtzman",
        "M Artetxe",
        "M Lewis",
        "H Hajishirzi",
        "L Zettlemoyer"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "16",
      "title": "What makes good in-context examples for gpt-3?",
      "authors": [
        "J Liu",
        "D Shen",
        "Y Zhang",
        "B Dolan",
        "L Carin",
        "W Chen"
      ],
      "year": "2021",
      "venue": "What makes good in-context examples for gpt-3?",
      "arxiv": "arXiv:2101.06804"
    },
    {
      "citation_id": "17",
      "title": "Sentence-bert: Sentence embeddings using siamese bert-networks",
      "authors": [
        "N Reimers",
        "I Gurevych"
      ],
      "year": "2019",
      "venue": "Sentence-bert: Sentence embeddings using siamese bert-networks"
    },
    {
      "citation_id": "18",
      "title": "",
      "authors": [
        "A Jiang",
        "A Sablayrolles",
        "A Mensch",
        "C Bamford",
        "D Chaplot",
        "D De Las Casas",
        "F Bressand",
        "G Lengyel",
        "G Lample",
        "L Saulnier",
        "L Lavaud",
        "M.-A Lachaux",
        "P Stock",
        "T Scao",
        "T Lavril",
        "T Wang",
        "T Lacroix",
        "W Sayed"
      ],
      "venue": ""
    },
    {
      "citation_id": "19",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM transactions on audio, speech, and language processing"
    },
    {
      "citation_id": "20",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "21",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "T Xu",
        "G Brockman",
        "C Mcleavey",
        "I Sutskever"
      ],
      "year": "2023",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "22",
      "title": "Gemma 2: Improving open language models at a practical size",
      "authors": [
        "G Team",
        "M Riviere",
        "S Pathak",
        "P Sessa",
        "C Hardin",
        "S Bhupatiraju",
        "L Hussenot",
        "T Mesnard",
        "B Shahriari",
        "A Ramé"
      ],
      "year": "2024",
      "venue": "Gemma 2: Improving open language models at a practical size",
      "arxiv": "arXiv:2408.00118"
    },
    {
      "citation_id": "23",
      "title": "Msp-improv: An acted corpus of dyadic interactions to study emotion perception",
      "authors": [
        "C Busso",
        "S Parthasarathy",
        "A Burmania",
        "M Abdelwahab",
        "N Sadoughi",
        "E Provost"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    }
  ]
}