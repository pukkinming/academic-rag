{
  "paper_id": "2505.20511v2",
  "title": "Multimodal Emotion Recognition In Conversations: A Survey Of Methods, Trends, Challenges And Prospects",
  "published": "2025-05-26T20:23:24Z",
  "authors": [
    "Chengyan Wu",
    "Yiqiang Cai",
    "Yang Liu",
    "Pengxu Zhu",
    "Yun Xue",
    "Ziwei Gong",
    "Julia Hirschberg",
    "Bolei Ma"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "While text-based emotion recognition methods have achieved notable success, real-world dialogue systems often demand a more nuanced emotional understanding than any single modality can offer. Multimodal Emotion Recognition in Conversations (MERC) has thus emerged as a crucial direction for enhancing the naturalness and emotional understanding of humancomputer interaction. Its goal is to accurately recognize emotions by integrating information from various modalities such as text, speech, and visual signals. This survey offers a systematic overview of MERC, including its motivations, core tasks, representative methods, and evaluation strategies. We further examine recent trends, highlight key challenges, and outline future directions. As interest in emotionally intelligent systems grows, this survey provides timely guidance for advancing MERC research.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition in conversations (ERC)  (Peng et al., 2022; Deng and Ren, 2023 ) is an increasingly important task in natural language processing (NLP), focusing on identifying the emotional state associated with each utterance in a dialogue. Unlike conventional emotion classification on isolated sentences, ERC requires understanding the interplay between utterances and tracking speakerspecific context across the conversation  (Gao et al., 2024) . Its relevance has grown due to its potential in various real-world applications, including social media monitoring  (Kumar et al., 2015) , intelligent healthcare services  (Hu et al., 2021b) , and the design of emotionally aware dialogue agents  (Jiao et al., 2020; Gong et al., 2023) .\n\nHowever, human emotions are typically conveyed through multiple modalities, including audi- tory (e.g., speech), visual (e.g., facial expressions, gestures), and linguistic (e.g., the semantic content conveyed by transcribed text). As a result, recent research (e.g.,  Ma et al., 2024a; Van et al., 2025; Dutta and Ganapathy, 2025)  has increasingly focused on multimodal settings in dialogue, a direction we refer to as Multimodal Emotion Recognition in Conversations (MERC). Researchers aim to identify the emotional state of a given utterance by integrating contextual information from different modalities, which often includes subtle personal emotional states such as happiness, anger, and hatred  (Poria et al., 2019b; Gong et al., 2024) , thereby improving the effectiveness of emotion recognition in dialogues. Figure  1  illustrates an example of ERC with textual, acoustic, and visual inputs.\n\nMultimodal emotion recognition (MER) itself has gained increasing attention due to the challenges of integrating diverse modalities, prompting research in both non-conversational and conversational settings. Existing surveys such as  A.V. et al. (2024)  and Aruna  Gladys and Vetriselvi (2023)  focus on non-conversational MER but overlook key aspects like interlocutor modeling and context.  Fu et al. (2023)  reviews both unimodal and multimodal conversational MER, yet primarily centers on fea-ture fusion, offering limited insight into core challenges such as cross-modal alignment, reasoning, modality missingness, and conflicts.\n\nDespite growing interest, the MERC task remains underexplored. Existing surveys  (Fu et al., 2023; Zhang and Tan, 2024 ) also lag behind recent advances, particularly the rise of multimodal large language models (MLLMs). To bridge this gap, we present a timely and comprehensive review of MERC. We first introduce the task definition and our survey methodology ( §2), benchmark datasets and evaluation methods ( §3), followed by a review of preprocessing techniques ( §4); then categorize recent methods ( §5) and outline key challenges and prospects ( §6).\n\nIn summary, the specific contributions of this survey are threefold:\n\n• Compilation of recent MERC research developments. We systematically review and integrate the latest research progress made by MERC in recent years, covering diverse datasets and methodologies.\n\n• Summarizing and comparing various MERC methods. We evaluate the strengths and limitations of various MERC approaches, offering theoretical insights and practical guidance to help researchers and practitioners select appropriate methods.\n\n• Proposing challenges and future directions.\n\nWe identify key open issues in the MERC domain and put forward several potential future research directions, aiming to guide ongoing and future investigations by researchers and practitioners in MERC.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Task Settings And Review Methodology",
      "text": "In this section, we present the task settings of MERC and outline the methodology employed to compile the content of this survey, which details the strategy and selection criteria used to curate the final content for this survey paper.\n\nModalities. In the context of MERC, we define a modality as a distinct source or channel of information that conveys emotional or communicative signals. These modalities correspond to observable representations derived from different sensory inputs and are typically processed in separate feature spaces. The most common modalities include:\n\n• Textual modality: The transcribed representation of spoken utterances, capturing the semantic and syntactic content of language.\n\n• Acoustic modality: Prosodic and paralinguistic features extracted from speech, such as tone, pitch, and energy.\n\n• Visual modality: Non-verbal cues such as facial expressions, head movements, eye gaze, and gestures.\n\nTask Definition. Given a dialogue D = {u 1 , u 2 , . . . , u N } consisting of N utterances spoken by multiple speakers, the goal of the MERC task is to predict an emotion label e i ∈ Y for each utterance u i . Each utterance is associated with three modalities: textual (u t i ), acoustic (u a i ), and visual (u v i ), which provide complementary information for emotion recognition. The multimodal representation of an utterance is denoted as:\n\nwhere\n\nMethodology for Literature Compilation:\n\nStrategy. We conduct a comprehensive literature search using sources such as the ACL Anthology, Google Scholar, and general search engines (e.g., Google Chrome). Within the ACL Anthology, we focus on top venues including EMNLP, ACL, NAACL, and related workshops.  1 Selection Criteria. We select papers that are directly relevant to MERC, with a focus on works that used at least two modalities (e.g., text, audio, visual), included conversational context, and evaluated on benchmark datasets such as IEMOCAP, MELD, and CMU-MOSEI. We prioritize recent papers from 2020 onward to reflect the state-ofthe-art, while including foundational work when appropriate for historical context. The selection is based on a careful review of the abstract, introduction, conclusion, and limitations of each paper.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Datasets And Evaluation",
      "text": "In this section, we describe the evaluation datasets and the evaluation metrics used for the MERC task, focusing on multimodal resources across multiple languages. For more detailed information about the single benchmarks, see Appendix §A.\n\n(1) English-centered: IEMOCAP, MELD, CMU-MOSEI, AVEC, EmoryNLP, and MEmoR dataset.\n\n(2) Non-English: M-MELD (French, Spanish, Greek, Polish), ACE (African), M 3 ED (Mandarin).\n\nAs shown in Table  1 , the domains covered by multimodal datasets have become increasingly diverse over time. The sources of these datasets include TV series, videos, and movies. At the same time, linguistic diversity has expanded to include languages such as French, Spanish, Greek, Polish, and Mandarin. Notably, there has also been a growing emergence of datasets targeting low-resource languages, such as African languages.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Datasets",
      "text": "Lang.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Source Year",
      "text": "IEMOCAP  (Busso et al., 2008)  en Videos 2008 AVEC  (Schuller et al., 2012)  en Videos 2012 EmoryNLP  (Zahiri and Choi, 2017)  en TV series 2017 CMU-MOSEI  (Bagher Zadeh et al., 2018)  en Videos 2018 MELD  (Poria et al., 2019a)  en TV series 2019 MEmoR  (Shen et al., 2020)  en Videos 2020 M 3 ED  (Zhao et al., 2022)  zh TV series 2022 M-MELD  (Ghosh et al., 2023)  fr,es,el,pl TV series 2023 ACE  (Sasu et al., 2025)  Akan Movies 2025 Datasets. We divide the existing mainstream dataset into the following two categories:\n\nEvaluation Metrics. Existing studies  (Majumder et al., 2019; Ghosal et al., 2019, inter alia)  typically adopt multiple evaluation metrics to comprehensively assess the overall performance of models, including Accuracy (e.g.,  Shou et al., 2024) , Weighted-F1 (e.g.,  Ma et al., 2024a) , Macro-F1 (e.g.,  Chudasama et al., 2022) , and Micro-F1 (e.g.,  Xie et al., 2021)  scores. To enable fine-grained analysis, these works also report per-emotion metric scores.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Feature Processing",
      "text": "Preprocessing dataset features is essential for effectively extracting meaningful information. We summarize feature preprocessing methods used in prior MERC research and analyze the typical preprocessing pipeline, which is often tailored to conversational settings. Specifically, we distinguish between two key components: feature extraction and context modeling.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Feature Extraction",
      "text": "For effective multimodal analysis, features must first be extracted from each modality stream  (text, audio, visual) . Mainstream approaches (e.g.,  Shi and Huang, 2023)  typically process these modalities separately at this initial stage. While the core extraction techniques often overlap, the key difference in the multimodal setting lies in the objective and subsequent use of these features. In unimodal ERC, the extractor aims to capture enough information within that single modality for emotion classification. Table  2  provides an overview of common feature extraction models employed in the multimodal studies surveyed in this paper.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Modality Models",
      "text": "LSTM  (Hochreiter and Schmidhuber, 1997)  CNN  (Kim, 2014; Tran et al., 2015)",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Text",
      "text": "Transformer  (Vaswani et al., 2017)  RoBERTa  (Liu et al., 2019)  sBERT  (Reimers and Gurevych, 2019)  3D-CNN  (Tran et al., 2015)  OpenFace  (Baltrušaitis et al., 2016)  Visual MTCNN  (Zhang et al., 2016)  DenseNet  (Huang et al., 2017 ) VisExtNet  (Shi and Huang, 2023)  Audio openSMILE  (Eyben et al., 2010)  COVAREP  (Degottex et al., 2014)  librosa  (McFee et al., 2015)  DialogueRNN  (Majumder et al., 2019)  Table  2 : Feature Extraction models.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Context Modeling",
      "text": "Context modeling primarily involves two types of contextual dependencies: situation-level and speaker-level modeling.\n\nSituation-level. The emotional state of a speaker is influenced not only by the semantic content of the current utterance but also by the surrounding contextual semantics. Therefore, existing approaches  (Hu et al., 2021a; Majumder et al., 2019)  commonly employ specialized networks to model the sequential dependencies among utterances, aiming to more accurately capture the speaker's temporal emotional dynamics. Given the textual feature u i ∈ R du for each utterance, the sequential context representation c s i ∈ R 2du is computed as follows:\n\nwhere h s i ∈ R du represents the i-th hidden state of the contextual sequence.\n\nSpeaker-level. Speaker identity information typically exhibits temporal and relational properties of emotions, which can enhance the model's ability Lightweight Multimodal Fusion and Adaptation E.g. UniMSE  (Hu et al., 2022) ; UniSA  (Li et al., 2023c) ; Facing-Bot  (Tanioka et al., 2024) ; MSE-Adapter  (Yang et al., 2025)  Behavior-Aware and Multimodal Instruction-Tuned E.g. DialogueLLM  (Zhang et al., 2025) ; BeMERC  (Fu et al., 2025b) ; MERITS-L  (Dutta and Ganapathy, 2025)  Instruction-Tuned with Speaker and Context Modeling E.g. InstructERC  (Lei et al., 2023) ; CKERC  (Fu, 2024) ; LaERC-S  (Fu et al., 2025a)  Fusionbased ( §5.2)\n\nText-Dominant Modality E.g. MMT  (Zou et al., 2022) ; MPT  (Zou et al., 2023) ; MM-NodeFormer  (Huang et al., 2024b) ; CMATH  (Zhu et al., 2024)  Equal Modality Weights E.g. DialogueTRM  (Mao et al., 2021) ; Emoformer  (Li et al., 2022) ; CMCF-SRNet  (Zhang and Li, 2023) ; SDT  (Ma et al., 2024a)  Graph-based ( §5.1) Fourier Graph Neural Networks E.g. ADGCN  (Liu et al., 2024b) ; DGODE  (Shou et al., 2025) ; TL-RGCN  (Su et al., 2024) ; GS-MCC  (Ai et al., 2025b)  Hypergraph Neural Networks E.g. GCNet  (Lian et al., 2023) ; MER-HGraph  (Li et al., 2023b) ; MDH  (Huang et al., 2024a) ;  ConxGNN (Van et al., 2025)  Traditional Graph Neural Networks E.g. DialogueGCN  (Ghosal et al., 2019) ; MMGCN  (Hu et al., 2021b) ; CORECT  (Nguyen et al., 2023) ; GraphSmile  (Li et al., 2024b) ; MERC-GCN  (Feng and Fan, 2025)  Figure  2 : A taxonomy of mainstream methods.\n\nto perceive speaker role information. Therefore, to more effectively learn and distinguish speaker-level contextual representations, many studies have further introduced speaker-related structured information on top of dialogue context modeling. Common approaches include using Speaker Embeddings  (Ma et al., 2024a; Shen et al., 2020)  to explicitly differentiate between different speakers, or leveraging Graph Neural Networks  (Ai et al., 2025a; Van et al., 2025)  to construct interaction graphs between speakers, thereby more comprehensively modeling the dependencies between them: Speaker Embeddings: The speaker embedding S i is combined with the modality features (e.g., text, audio, or vision) to produce modality representations that are speaker-and context-aware.\n\nGraph Neural Networks: Speaker interactions can be further modeled by constructing a dialogue graph to capture inter-utterance and interspeaker dependencies beyond sequential semantics. A dialogue graph is typically defined as G = (V, E, W, R), where each node v i ∈ V corresponds to the i-th utterance, and its associated feature vector c s i is obtained from sequential modeling of contextual dependencies. Edges e ij ∈ E represent interaction links between utterances, with associated weights ω ij ∈ W reflecting the interaction strength and types r ij ∈ R encoding speakerrelated or structural relationships.\n\nBased on this graph, the context-aware representation h g i for node v i is computed using a graph neural network as follows:\n\nwhere c s j are the contextual node features from neighboring utterances. The GNN aggregates information from connected nodes to enhance c s i with speaker-and structure-aware interaction knowledge.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Methodology",
      "text": "This section discusses state-of-the-art approaches to the MERC tasks. We summarize them from three perspectives: Graph-based Methods ( §5.1), Fusionbased Methods ( §5.2), and Generation-based Methods ( §5.3). An overview of the methods and subcategories with representative examples is presented in Figure  2 .\n\nIt is worth noticing that some methods in MERC inherently involve multiple components (e.g., fusion modules within graph-based or generationbased frameworks). Our taxonomy is not intended to be strictly disjoint, but rather to organize the literature based on the core modeling paradigm or innovation focus of each method. The categorization of methods is thus as follows:\n\n• Graph-based: Methods are categorized as graphbased when the primary architecture centers around graph neural networks for modeling conversational structure, even if auxiliary modules (e.g., fusion layers) are integrated.\n\n• Fusion-based: Methods are grouped under fusion-based when their main contribution lies in the design of cross-modal interaction mechanisms, regardless of the backbone architecture (e.g., Transformer, LSTM).\n\n• Generation-based: Generation-based methods refer to recent approaches that leverage LLMs to generate predictions or intermediate reasoning, often using prompt engineering or instruction tuning, even if lightweight fusion components are present.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Graph-Based Methods",
      "text": "Dialogues can be naturally interpreted as graph structures due to the intrinsic correlations and dependencies among utterances. Conversations often feature multi-turn interactions with complex dependency and interaction patterns, which can be effectively modeled through the edge structures of Graph Neural Networks (GNNs)  (Scarselli et al., 2009) . With the increasing interest in multimodal dialogue understanding, GNNs have evolved beyond their application  (Liu et al., 2024a)  in textual data to embrace multimodal inputs. Besides, recent methods also integrate auxiliary modules (e.g., convolution, contrastive learning, and fusion) to enhance the performance. Figure  3  illustrates recent advancements in graph-based methods. We categorize them into traditional graphs, hypergraphs, and fourier graph neural networks. Traditional Graph Neural Networks. Previous works such as bc-LSTM  (Poria et al., 2017)  and ICON  (Hazarika et al., 2018)  primarily relied on sequential approaches. DialogueGCN  (Ghosal et al., 2019)  was the first to introduce GNNs into ERC, addressing the limitations of earlier sequence-based models like DialogueRNN  (Majumder et al., 2019)  in capturing contextual dependencies. To effectively integrate information from different modalities,  Hu et al. (2021b)  constructed a graph structure that fuses multimodal features, enabling the model to capture inter-modal dependencies through graph convolutional networks and incorporating speaker information to enhance the representation of conversational semantics. Inspired by the application of graph convolutions in ERC,  Li et al. (2024b)  proposed the GSF module, which introduces an alternating graph convolution mechanism to hierarchically extract both cross-modal and intra-modal emotional information. Some studies further enhanced graph-based models with attention mechanisms for multimodal fusion; for example, Feng and Fan (2025) integrated a Cross-modal Attention Module to better fuse information from different modalities, while Nguyen et al. (  2023 ) designed a cross-modal attention mechanism to explicitly model the heterogeneity between modalities.\n\nHypergraph Neural Networks. Although traditional graph-based methods can capture long-range and multimodal contextual information, they are often challenged by missing modalities during conversations.  Lian et al. (2023)  tackled this issue by jointly optimizing classification and reconstruction tasks in an end-to-end manner to effectively model incomplete data. The related works  (Li et al., 2023b; Huang et al., 2024a)  considered the limitations imposed by the pairwise relationships between GNNs nodes.  Van et al. (2025)  constructed a multimodal fusion graph and introduced Hypergraph Neural Networks  (Feng et al., 2019)  to connect multiple modalities or utterance nodes simultaneously, thereby capturing more complex multivariate dependencies and high-order interactions in conversations, and enhancing the modeling of emotion propagation.\n\nFourier Graph Neural Networks. Increasing the depth of GNN layers can lead to the oversmoothing problem  (Liu et al., 2022; Yi et al., 2023) , which hampers the modeling of long-range semantic dependencies and complementary modality relations. To address this issue, GS-MMC  (Ai et al., 2025b)  proposes a graph-based framework for multimodal consistency and complementarity learning. This method employs a Fourier graph operator to extract high-and low-frequency emotional signals from the frequency domain, capturing both local variations and global semantic trends. Additionally, a contrastive learning mechanism (van den  Oord et al., 2019)  is designed to enhance the semantic consistency and complementarity of these signals in a self-supervised manner, thereby improving the model's ability to recognize true emotional states.\n\nGraph-based methods effectively capture long-range dependencies and speaker interactions by modeling utterances as nodes and relations as edges. In traditional NLP tasks, they are particularly effective for structured inputs such as tokenlevel representations  (Zhang et al., 2023 (Zhang et al., , 2024b)) . However, integrating heterogeneous multimodal signals into graph structures remains challenging, as naïve connections may introduce noise without proper modality alignment.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Fusion-Based Methods",
      "text": "In MERC, effective fusion of heterogeneous multimodal features is crucial but challenging due to noise introduced during interaction modeling. The advancements of the Transformers architecture  (Vaswani et al., 2017) , with its self-attention mechanism, promote advancements of the MERC methods for capturing cross-modal and contextual dependencies. To enhance cross-modal interactions, recent methods build on Transformers with tailored fusion strategies. We refer to these methods as fusion-based and illustrate them in Figure  4 . Some approaches promote equal interaction among modalities to improve robustness, while others adopt a primary-auxiliary scheme, typically using text as the core, with other modalities providing complementary signals. Equal Modality Weights. Equal interaction can fully utilize information from various modalities, preventing over-reliance on a single modality.  Li et al. (2022)  proposed achieving emotion recognition by equally integrating emotional vectors and sentence vectors from different modalities to form emotion capsules.  Zhang and Li (2023)  designed a local constraint module for modalities within the Transformer to promote modality interaction and incorporates a semantic graph to address the lack of semantic relationship information between utterances.  Mao et al. (2021)  constructed a hierarchical Transformer where each modality can flexibly switch between sequential and feedforward struc-tures based on contextual information. Inspired by hierarchical modality interaction,  Ma et al. (2024a)  introduced a hierarchical gating  (Ma et al., 2019)  fusion strategy into the Transformer architecture to enable fine-grained modality interaction and designs self-distillation  (Zhang et al., 2019)  to further learn better modality representations.\n\nText-Dominant Modality. Some methods propose models based on primary-auxiliary modality collaboration, where auxiliary modalities are used to enhance the performance of the primary (textual) modality.  Huang et al. (2024b)  suggested that enhancing a text-dominant model with auxiliary modalities can improve performance.  Zou et al. (2022)  employed a Transformer architecture to design cross-modal attention for learning fusion relationships between different modalities, preserving the integrity of the primary modality's features while enhancing the representation of weaker modality features. It also uses a two-stage emotional cue extractor to extract emotional evidence. Building on this,  Zou et al. (2023)  proposed using weaker modalities as multimodal prompts while performing deep emotional cue extraction for stronger modalities. The cue information is embedded into various attention layers of the Transformer to facilitate the fusion of information between the primary and auxiliary modalities.  Zhu et al. (2024)  introduced an asymmetric CMA-Transformer module for central and auxiliary modalities to obtain fused modality information and proposes a hierarchical distillation  (Yang et al., 2021)  framework to perform coarse-and fine-grained distillation. This approach ensures the consistency of modality fusion information at different granularities.\n\nFusion-based methods focus on learning crossmodal interactions through attention mechanisms, with Transformer-based models achieving strong generalization. These approaches are efficient for tasks with well-aligned modality inputs but often overlook dialogue-level structures such as speaker dependencies. Compared to graph-based models, they emphasize modality-level fusion over relational reasoning.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Generation-Based Methods",
      "text": "In recent years, pretrained LLMs have achieved remarkable success in natural language processing tasks  (Chu et al., 2024) , demonstrating strong emergent capabilities  (Wei et al., 2022) . However, despite their powerful general-purpose abilities, leveraging their full potential in specific sub-tasks still requires carefully crafted, high-quality prompts  (Wei et al., 2021)  to bridge the gap in reasoning capabilities.As shown in Figure  5 , researchers have proposed various model improvement strategies to effectively integrate contextual and multimodal information into LLMs while addressing their substantial computational resource demands. Step 1…(n-1)",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "…………….",
      "text": "Case Retrieve\n\nStep n",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Llm Output",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Text Embedder",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Embedding",
      "text": "Step 1\n\nStep 2 LLM Alignment Module\n\n(1) Prompt\n\n(2) Multimodal Feature Instruction-Tuned with Speaker and Context Modeling. ERC tasks have predominantly relied on discriminative modeling frameworks. With the emergence of LLMs, InstructERC  (Lei et al., 2023)  was the first to propose a generative framework for ERC. It introduces a simple yet effective retrievalbased prompting module that helps LLMs explicitly integrate multi-granularity supervisory signals from dialogues. Additionally, it incorporates an auxiliary emotion alignment task to better model the complex emotional transitions between interlocutors in conversations. Inspired by the integration of commonsense knowledge in COSMIC  (Ghosal et al., 2020) , recent work  (Fu, 2024; Fu et al., 2025a)  designed a prompt generation approach based on dialogue history to elicit speakerrelated commonsense using LLMs by injecting commonsense knowledge into ERC.\n\nBehavior-Aware and Multimodal Instruction-Tuned. To address the lack of multimodal integration, Dutta and Ganapathy (2025) incorporated both acoustic and textual modalities. Considering that visual information may provide richer emotional cues, Zhang et al. (  2025 ) constructed a high-quality instruction dataset using image and text data, and fine-tunes the model using Low-Rank Adaptation (LoRA)  (Song et al., 2024) . Furthermore,  Fu et al. (2025b)  introduced a novel behavior-aware Multimodal LLM (MLLM)-based ERC framework. It consists of three core components: a video-derived behavior generation module, a behavior alignment and refinement module, and an instruction tuning module  (Wei et al., 2021) . The first two modules enable the model to infer human behaviors from limited information, thereby enhancing its behavioral perception capability. The instruction tuning module improves the model's emotion recognition performance by aligning and fine-tuning the concatenated multimodal inputs.\n\nLightweight Multimodal Fusion and Adaptation. As LLMs become increasingly large, the computational costs for ERC also rise significantly  (Zhang et al., 2025) . Inspired by domain-specific LLM paradigms tailored for affective computing  (Hu et al., 2022; Li et al., 2023c; Tanioka et al., 2024) , MSE-Adapter  (Yang et al., 2025)  proposed a lightweight and adaptable plug-in architecture with two modules: TGM, for aligning textual and nontextual features, and MSF, for multi-scale crossmodal fusion. Built on a frozen LLM backbone and trained via backpropagation, it enables efficient and multimodally-aware ERC with minimal computational cost. Similarly, SpeechCueLLM  (Wu et al., 2025)  introduced a lightweight plug-in that converts speech features into natural language prompts, enabling LLMs to perform multimodal emotion recognition without architectural changes.\n\nGeneration-based methods leverage LLMs to reformulate ERC as a text generation task, enabling flexible adaptation across datasets and domains. Their end-to-end nature simplifies input processing but limits fine-grained control over multimodal integration. In contrast to structured models, LLMs excel at scalability but require further refinement to model multimodal dependencies explicitly.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Challenges And Prospects",
      "text": "Based on current trends and developments in the MERC task, in this section, we outline several existing challenges and open questions, highlighting opportunities for future improvements. We structure our discussion along a logical progression: starting with foundational limitations in data collection and FAIR compliance, we then examine challenges in multimodal modeling and conclude with considerations for real-world deployment. This trajectory reflects how upstream issues in data and modeling propagate downstream, ultimately shaping the robustness, inclusivity, and applicability of MERC systems.\n\nFAIR-related Issues Pose Challenges in MERC. The FAIR principles provide guidelines for improving the Findability, Accessibility, Interoperability, and Reusability of digital assets  (Wilkinson et al., 2016) . Collecting large and diverse multimodal emotion data is costly and time-consuming; some large dialogue datasets (e.g., M 3 ED,  Zhao et al., 2022)  are still monolingual and domain-bound. These limitations directly conflict with the FAIR principles. Some ERC datasets lack rich metadata or persistent identifiers, undermining findability and interoperability. Others are subject to access restrictions or copyright constraints, while many adopt inconsistent labeling schemes that hinder reusability. Consequently, researchers often have to train on small or biased samples, which undermines generalization and the reuse of models. To address these issues, future work could prioritize the development of multilingual benchmark datasets with standardized metadata and open licensing, possibly through collaborative consortiums that align with FAIR principles.\n\nLow-Resource, Multilingual, and Multicultural Settings. As described in the previous paragraph, most state-of-the-art MERC systems are trained on English-language datasets, which limits their global applicability. Although building a large-scale, diverse MER corpus is essential, it remains an obvious challenge as it requires expert annotation of data. The limited annotated data forces researchers to rely on transfer learning  (Ananthram et al., 2020) , zero-shot  (Qi et al., 2021) , or few-shot methods  (Yang et al., 2023) . However, data scarcity and the high cost of emotion annotation continue to be major obstacles for MER in low-resource domains  (Hussain et al., 2025) . Emotions are expressed differently across languages and cultures, further compounding the challenges of MER. Variations in emotional expression and interpretation due to cultural differences can lead to inconsistencies in labeling. Most existing corpora are culturespecific, limiting their generalizability. Although researchers have acknowledged this challenge  (A. and V., 2024; Ghaayathri Devi et al., 2024) , MER systems aiming for global applicability must account for both linguistic diversity and culturally driven display rules. Future work could explore culture-adaptive pretraining and cross-lingual transfer learning methods that embed culturally sensitive emotion semantics across languages.\n\nComplexities of Fusion Strategies across Modalities. Multimodal fusion techniques include early fusion, mid-level fusion, late fusion, hybrid fusion, and others  (Atrey et al., 2010; Gandhi et al., 2023) . A key challenge is that conversational signals, such as voice, facial expressions, and transcripts, are inherently asynchronous and occur at different time scales, making it difficult to align them at the utterance level. Emotions also depend on the context of preceding and subsequent conversation turns, so the model must capture temporal dynamics  (Wang et al., 2024) . Previous studies have used recurrent or self-attention layers to model sequential context  (Houssein et al., 2024; Dutta et al., 2024) , but long-range dependencies remain challenging to learn. How to balance and integrate contextual sentiment cue features with multimodal fusion features in decision-making, and how to determine which fusion strategies are most effective across different modalities, remain open and important research questions  (A.V. et al., 2024; Ramaswamy and Palaniswamy, 2024) . Recent advances in adaptive fusion strategies and dynamic attention mechanisms show promise, and future methods could explore transformer-based fusion that dynamically reweights modalities based on conversational context.\n\nCross-Modal Alignment, Noise Modality, Missing Modality, and Modality Conflicts. Misaligned or inconsistent features can inhibit the ability of a model to fully utilize multimodal signals, affecting its robustness and generalization  (Ma et al., 2024b; Li and Tang, 2024) . Noise modality, missing modality, or imbalanced modality distributions may bias simple fusion strategies  (Mai et al., 2024; Zhang et al., 2024a) . Even when all modalities are available, they may convey conflicting emotional signals, further complicating fusion and decisionmaking. Perceiving the uncertainty of different modalities for feature enhancement and resolving conflicts between modality features are important areas that need further exploration in MER research. Therefore, exploring cross-modal transfer and fusion to improve generalization in ERC has attracted the attention of more and more researchers  (Fan et al., 2024; Li et al., 2024a; Feng and Fan, 2025) . Some ERC methods incorporate variants of crossmodal attention  (Guo et al., 2024; N. and Patil, 2020) , graph-based fusion  (Li et al., 2023a; Hu et al., 2021b) , or mutual learning to align features during training and improve cross-domain performance  (Lian et al., 2021) . Future research can further investigate robust training frameworks that include modality dropout, uncertainty-aware fusion, or reinforcement learning to selectively attend to trustworthy modalities.\n\nEffective Modality Selection. Multimodal learning refers to the integration of information from various heterogeneous sources and aims to effectively leverage data from diverse modalities  (He et al., 2024; Tsai et al., 2024) . In multimodal representation learning, not all modalities contribute equally to the task. Some modalities may introduce noise and need to be removed, while others may not be essential for the task at hand but could be indispensable for other subtasks. Existing research proposes modality selection algorithms that identify the contribution of each modality  (Marinov et al., 2023; Mai et al., 2024) . However, selecting the most appropriate subset of modalities for a task remains one of the key challenges in multimodal learning. An emerging direction is to integrate learnable modality gates or sparsity-inducing regularization into fusion models to automatically suppress uninformative modalities during training.\n\nEfficient Fine-tuning Approach Using Multimodal LLMs. Multimodal LLMs have brought major advances in enabling machines to learn across modalities. Some models are increasingly used in MERC, offering zero-shot or few-shot generalization across different modalities  (Li et al., 2024c; Yang et al., 2024; Bo-Hao et al., 2025) . The use of LLMs in MERC opens up new possibilities for capturing deeper semantic and conversational cues beyond surface-level emotion signals. However, efficiently fine-tuning these models for emotion understanding still presents challenges, especially in low-resource and culturally diverse settings. Efficient adaptation of MLLMs to capture the nuance of emotions across diverse datasets, languages, or cross-cultural settings remains an open research frontier. Promising future directions include using adapter modules or low-rank finetuning techniques to adapt large models to specific emotion tasks with minimal data.\n\nMERC Application. With the growing use of interactive machine applications, MERC has become a critical research area. Applications in humancomputer interaction  (Ahmad et al., 2024; Moin et al., 2023) , healthcare  (Ayata et al., 2020; Islam et al., 2024) , education  (Villegas-Ch et al., 2025; Vani and Jayashree, 2025) , and virtual collaboration demand robust and adaptable emotion recognition technologies that function effectively in naturalistic and dynamic environments.  Yang et al. (2022)  investigated MER in contexts affected by face occlusions, such as those introduced by surgical and fabric masks.  Khan et al. (2024)  studied contactless techniques in MER, surveying a range of nonintrusive modalities (e.g., visual cues, physiological signals). Huang (2024) developed a MER system for online learning to enable real-time monitoring and feedback on learners' emotional states. These studies highlight key directions for advancing MERC systems, particularly to make them more robust and context-aware. The ongoing research should continue to focus on real-world deployment scenarios. Future MERC systems could benefit from incremental learning techniques and user-in-the-loop feedback mechanisms that allow adaptation in dynamic, real-time environments.\n\nExpanding the Modality Space in Emotion Recognition. Current MER systems mainly rely on vision, audio, and text, given their accessibility and prevalence in datasets. Yet human emotion is expressed through additional channels such as gaze, gestures, posture, and physiological signals (e.g., heart rate, skin conductance, brain activity)  (Noroozi et al., 2021; Udahemuka et al., 2024; Wang et al., 2023; He et al., 2020; Liu et al., 2024c) . These modalities remain underrepresented due to challenges in collecting synchronized, high-quality data and evolving annotation standards  (Udahemuka et al., 2024; He et al., 2020; Kim and Hong, 2024) . Expanding beyond the traditional three can reduce reliance on potentially misleading cues and open new application domains. For instance, biosignals and contextual cues could enhance emotion sensing in health and education, while wearable sensors and eye-trackers may enable emotionaware experiences in VR, driver monitoring, or social robotics. In summary, gaze, physiological, and other embodied modalities are promising but underexplored in affective computing. Future work should explore how to systematically integrate these diverse modalities into large-scale benchmarks and develop models capable of robustly leveraging them in real-world scenarios.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Conclusion",
      "text": "MERC seeks to understand emotions by integrating various modalities in to dialogue of linguistic, acoustic, visual signals, and beyond. While recent progress has introduced diverse modeling strategies, significant challenges remain in data scarcity, modality alignment, and generalization across languages and cultures.\n\nThis survey provides a structured review of the MERC landscape, compares representative approaches, and highlights key open research problems. We hope it serves as a practical reference to support the future development of robust and inclusive emotion recognition systems.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Limitations",
      "text": "This survey offers a structured and up-to-date overview of MERC, with an emphasis on recent deep learning-based approaches. However, several limitations should be acknowledged to contextualize the scope of our work.\n\nFirst, due to the focus on more advanced recent technologies, we provide only high-level summaries of representative methods, without delving into full technical details. Additionally, approaches developed prior to 2020 receive limited coverage, as our focus is primarily on recent trends that align with the rapid evolution of large-scale multimodal systems.\n\nSecond, our literature review is largely drawn from English-language publications in major conferences and repositories, including Interspeech, ICASSP, ACM, *ACL, ICML, AAAI, CVPR, COL-ING, and preprints on arXiv. While these venues represent core research communities in MERC, relevant contributions from other regions, languages, or domains may be underrepresented.\n\nIn the benchmark section, we highlight widelyused datasets, but do not aim for exhaustive comparison. For more in-depth benchmarking, we refer readers to complementary surveys such as  Zhao et al. (2022) ,  Sasu et al. (2025)  and  Gan et al. (2024) .\n\nFinally, while we identify several open challenges and underexplored directions, our discussion is not exhaustive. Rather than providing definitive answers, we aim to surface critical issues and foster further inquiry. We view these open questions as productive entry points for future research and hope this survey supports ongoing efforts to develop more advanced MERC systems.\n\nA Datasets Details IEMOCAP. The IEMOCAP dataset  (Busso et al., 2008)  consists of dyadic conversation videos from ten speakers, comprising 151 dialogues and 7,433 utterances. Sessions 1 to 4 are used as the training set, while the last session is held out as the test set. Each utterance is annotated with one of six emotion labels: happy, sad, neutral, angry, excited, and frustrated.\n\nMELD. The Multimodal EmotionLines Dataset (MELD)  (Poria et al., 2019a ) is an extension of the EmotionLines corpus  (Hsu et al., 2018) , constructed from the TV series Friends. It contains 1,433 multi-party conversations and 13,708 utterances. Each utterance is annotated with one of seven emotion categories: anger, disgust, fear, joy, neutral, sadness, and surprise. Unlike dyadic datasets, MELD captures the complexity of multispeaker interactions, making it well-suited for studying emotion recognition in multi-party conversational settings.\n\nCMU-MOSEI. The CMU-MOSEI dataset  (Bagher Zadeh et al., 2018)  consists of 23,453 sentence-level video segments from over 1,000 speakers covering 250 topics, collected from YouTube monologue videos. Each segment is annotated for sentiment on a 7-point Likert scale ([-3: highly negative, -2: negative, -1: weakly negative, 0: neutral, +1: weakly positive, +2: positive, +3: highly positive]) and for the intensity of six Ekman  (Ekman et al., 1980)  emotions: happiness, sadness, anger, fear, disgust, and surprise. The dataset provides aligned language, visual, and acoustic modalities, making it a large-scale benchmark for multimodal sentiment and emotion recognition.\n\nM 3 ED. The M 3 ED dataset  (Zhao et al., 2022)  consists of 990 dyadic dialogues and 24,449 utterances collected from 56 Chinese TV series. Each utterance is annotated with one or more of seven emotion labels: happy, surprise, sad, disgust, anger, fear, and neutral. The dataset covers text, audio, and visual modalities and features blended emotions and speaker metadata, making it the first large-scale multimodal emotional dialogue corpus in Chinese.\n\nACE. The ACE dataset  (Sasu et al., 2025)  contains 385 emotion-labeled dialogues and 6,162 utterances in the Akan language, collected from 21 movie sources. It includes multimodal information across text, audio, and visual modalities, and is annotated with one of seven emotion categories: neutral, sadness, anger, fear, surprise, disgust, and happiness. Word-level prosodic prominence is also provided, making it the first such dataset for an African language. The dataset is gender-balanced, featuring 308 speakers, and is split into training, validation, and test sets in a 7:1.5:1.5 ratio.\n\nMEmoR. The MEmoR dataset  (Shen et al., 2020)  consists of 5,502 video clips and 8,536 person-level samples extracted from the sitcom The Big Bang Theory, annotated with 14 fine-grained emotions. Unlike most datasets focusing solely on speakers, MEmoR includes both speakers and non-speakers, even when modalities are partially or completely missing. Each sample comprises a target person, an emotion moment, and multimodal inputs (text, audio, visual, and personality features), making it a challenging benchmark for multimodal emotion reasoning beyond direct recognition.\n\nAVEC. The AVEC dataset  (Schuller et al., 2012) , derived from the SEMAINE corpus  (McKeown et al., 2012) , features human-agent interactions annotated with four continuous affective dimensions: valence, arousal, expectancy, and power. While the original labels are provided at 0.2-second intervals, we aggregate them over each utterance to obtain utterance-level annotations for emotion analysis.\n\nM-MELD. M-MELD  (Ghosh et al., 2023)  is a multilingual extension of the MELD dataset, created to support emotion recognition in conversations across different languages. While MELD is an English-only multimodal dataset, M-MELD includes human-translated versions of the original utterances in four additional languages: French, Spanish, Greek, and Polish. This multilingual corpus retains the original multimodal structure and emotion labels, enabling research in crosslingual and multimodal emotion analysis. By balancing high-resource and low-resource languages, M-MELD offers a valuable benchmark for developing and evaluating multilingual ERC models.\n\nEmoryNLP. EmoryNLP (Zahiri and Choi, 2017) is a textual emotion-labeled dataset derived from the Friends TV series, containing over 12,000 utterances across multi-party dialogues. Each utterance is annotated with one of seven emotions: neutral, joyful, peaceful, powerful, mad, sad, or scared, offering a fine-grained resource for emotion recognition in conversational settings.",
      "page_start": 12,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: An example of MERC. Text, audio, and visual",
      "page": 1
    },
    {
      "caption": "Figure 1: illustrates an example of",
      "page": 1
    },
    {
      "caption": "Figure 2: A taxonomy of mainstream methods.",
      "page": 4
    },
    {
      "caption": "Figure 2: It is worth noticing that some methods in MERC",
      "page": 4
    },
    {
      "caption": "Figure 3: illustrates recent",
      "page": 5
    },
    {
      "caption": "Figure 3: Development of Graph-based Methods.",
      "page": 5
    },
    {
      "caption": "Figure 4: Some approaches promote equal interaction",
      "page": 6
    },
    {
      "caption": "Figure 4: Development of Fusion-based Methods.",
      "page": 6
    },
    {
      "caption": "Figure 5: , researchers have",
      "page": 7
    },
    {
      "caption": "Figure 5: Development of Generation-based Methods.",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "focused on multimodal settings in dialogue, a direc-": ""
        },
        {
          "focused on multimodal settings in dialogue, a direc-": "tion we refer to as Multimodal Emotion Recogni-"
        },
        {
          "focused on multimodal settings in dialogue, a direc-": "tion in Conversations (MERC). Researchers aim to"
        },
        {
          "focused on multimodal settings in dialogue, a direc-": ""
        },
        {
          "focused on multimodal settings in dialogue, a direc-": "identify the emotional state of a given utterance by"
        },
        {
          "focused on multimodal settings in dialogue, a direc-": "integrating contextual information from different"
        },
        {
          "focused on multimodal settings in dialogue, a direc-": ""
        },
        {
          "focused on multimodal settings in dialogue, a direc-": "modalities, which often includes subtle personal"
        },
        {
          "focused on multimodal settings in dialogue, a direc-": ""
        },
        {
          "focused on multimodal settings in dialogue, a direc-": "emotional states such as happiness, anger, and ha-"
        },
        {
          "focused on multimodal settings in dialogue, a direc-": ""
        },
        {
          "focused on multimodal settings in dialogue, a direc-": "tred (Poria et al., 2019b; Gong et al., 2024), thereby"
        },
        {
          "focused on multimodal settings in dialogue, a direc-": ""
        },
        {
          "focused on multimodal settings in dialogue, a direc-": "improving the effectiveness of emotion recognition"
        },
        {
          "focused on multimodal settings in dialogue, a direc-": ""
        },
        {
          "focused on multimodal settings in dialogue, a direc-": "in dialogues.\nFigure 1 illustrates an example of"
        },
        {
          "focused on multimodal settings in dialogue, a direc-": ""
        },
        {
          "focused on multimodal settings in dialogue, a direc-": "ERC with textual, acoustic, and visual inputs."
        },
        {
          "focused on multimodal settings in dialogue, a direc-": ""
        },
        {
          "focused on multimodal settings in dialogue, a direc-": "Multimodal emotion recognition (MER) itself"
        },
        {
          "focused on multimodal settings in dialogue, a direc-": "has gained increasing attention due to the chal-"
        },
        {
          "focused on multimodal settings in dialogue, a direc-": "lenges of integrating diverse modalities, prompting"
        },
        {
          "focused on multimodal settings in dialogue, a direc-": "research in both non-conversational and conversa-"
        },
        {
          "focused on multimodal settings in dialogue, a direc-": "tional settings. Existing surveys such as A.V. et al."
        },
        {
          "focused on multimodal settings in dialogue, a direc-": "(2024) and Aruna Gladys and Vetriselvi (2023) fo-"
        },
        {
          "focused on multimodal settings in dialogue, a direc-": "cus on non-conversational MER but overlook key"
        },
        {
          "focused on multimodal settings in dialogue, a direc-": "aspects like interlocutor modeling and context. Fu"
        },
        {
          "focused on multimodal settings in dialogue, a direc-": "et al. (2023) reviews both unimodal and multimodal"
        },
        {
          "focused on multimodal settings in dialogue, a direc-": "conversational MER, yet primarily centers on fea-"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "{chengyan.wu, yiqiangcai, xueyun}@m.scnu.edu.cn,": "Abstract",
          "zg2272@columbia.edu,\nbolei.ma@lmu.de": "I’m fine…"
        },
        {
          "{chengyan.wu, yiqiangcai, xueyun}@m.scnu.edu.cn,": "While text-based emotion recognition methods",
          "zg2272@columbia.edu,\nbolei.ma@lmu.de": "Text"
        },
        {
          "{chengyan.wu, yiqiangcai, xueyun}@m.scnu.edu.cn,": "have achieved notable success, real-world dia-",
          "zg2272@columbia.edu,\nbolei.ma@lmu.de": ""
        },
        {
          "{chengyan.wu, yiqiangcai, xueyun}@m.scnu.edu.cn,": "logue systems often demand a more nuanced",
          "zg2272@columbia.edu,\nbolei.ma@lmu.de": ""
        },
        {
          "{chengyan.wu, yiqiangcai, xueyun}@m.scnu.edu.cn,": "",
          "zg2272@columbia.edu,\nbolei.ma@lmu.de": "Audio"
        },
        {
          "{chengyan.wu, yiqiangcai, xueyun}@m.scnu.edu.cn,": "emotional understanding than any single modal-",
          "zg2272@columbia.edu,\nbolei.ma@lmu.de": ""
        },
        {
          "{chengyan.wu, yiqiangcai, xueyun}@m.scnu.edu.cn,": "ity can offer. Multimodal Emotion Recognition",
          "zg2272@columbia.edu,\nbolei.ma@lmu.de": "Multimodal Emotion"
        },
        {
          "{chengyan.wu, yiqiangcai, xueyun}@m.scnu.edu.cn,": "",
          "zg2272@columbia.edu,\nbolei.ma@lmu.de": "Recognition Model"
        },
        {
          "{chengyan.wu, yiqiangcai, xueyun}@m.scnu.edu.cn,": "in Conversations (MERC) has thus emerged as",
          "zg2272@columbia.edu,\nbolei.ma@lmu.de": ""
        },
        {
          "{chengyan.wu, yiqiangcai, xueyun}@m.scnu.edu.cn,": "",
          "zg2272@columbia.edu,\nbolei.ma@lmu.de": "Visual"
        },
        {
          "{chengyan.wu, yiqiangcai, xueyun}@m.scnu.edu.cn,": "a crucial direction for enhancing the natural-",
          "zg2272@columbia.edu,\nbolei.ma@lmu.de": ""
        },
        {
          "{chengyan.wu, yiqiangcai, xueyun}@m.scnu.edu.cn,": "ness and emotional understanding of human-",
          "zg2272@columbia.edu,\nbolei.ma@lmu.de": "Figure 1: An example of MERC. Text, audio, and visual"
        },
        {
          "{chengyan.wu, yiqiangcai, xueyun}@m.scnu.edu.cn,": "computer interaction. Its goal is to accurately",
          "zg2272@columbia.edu,\nbolei.ma@lmu.de": "inputs are integrated through a multimodal model\nto"
        },
        {
          "{chengyan.wu, yiqiangcai, xueyun}@m.scnu.edu.cn,": "recognize emotions by integrating information",
          "zg2272@columbia.edu,\nbolei.ma@lmu.de": "detect various emotional states."
        },
        {
          "{chengyan.wu, yiqiangcai, xueyun}@m.scnu.edu.cn,": "from various modalities such as text, speech,",
          "zg2272@columbia.edu,\nbolei.ma@lmu.de": ""
        },
        {
          "{chengyan.wu, yiqiangcai, xueyun}@m.scnu.edu.cn,": "and visual signals.",
          "zg2272@columbia.edu,\nbolei.ma@lmu.de": ""
        },
        {
          "{chengyan.wu, yiqiangcai, xueyun}@m.scnu.edu.cn,": "This survey offers a systematic overview of",
          "zg2272@columbia.edu,\nbolei.ma@lmu.de": "tory (e.g., speech), visual (e.g., facial expressions,"
        },
        {
          "{chengyan.wu, yiqiangcai, xueyun}@m.scnu.edu.cn,": "MERC,\nincluding its motivations, core tasks,",
          "zg2272@columbia.edu,\nbolei.ma@lmu.de": ""
        },
        {
          "{chengyan.wu, yiqiangcai, xueyun}@m.scnu.edu.cn,": "",
          "zg2272@columbia.edu,\nbolei.ma@lmu.de": "gestures), and linguistic (e.g.,\nthe semantic con-"
        },
        {
          "{chengyan.wu, yiqiangcai, xueyun}@m.scnu.edu.cn,": "representative methods, and evaluation strate-",
          "zg2272@columbia.edu,\nbolei.ma@lmu.de": ""
        },
        {
          "{chengyan.wu, yiqiangcai, xueyun}@m.scnu.edu.cn,": "",
          "zg2272@columbia.edu,\nbolei.ma@lmu.de": "tent conveyed by transcribed text). As a result,"
        },
        {
          "{chengyan.wu, yiqiangcai, xueyun}@m.scnu.edu.cn,": "gies. We further examine recent trends, high-",
          "zg2272@columbia.edu,\nbolei.ma@lmu.de": ""
        },
        {
          "{chengyan.wu, yiqiangcai, xueyun}@m.scnu.edu.cn,": "",
          "zg2272@columbia.edu,\nbolei.ma@lmu.de": "recent research (e.g., Ma et al., 2024a; Van et al.,"
        },
        {
          "{chengyan.wu, yiqiangcai, xueyun}@m.scnu.edu.cn,": "light key challenges, and outline future direc-",
          "zg2272@columbia.edu,\nbolei.ma@lmu.de": ""
        },
        {
          "{chengyan.wu, yiqiangcai, xueyun}@m.scnu.edu.cn,": "",
          "zg2272@columbia.edu,\nbolei.ma@lmu.de": "2025; Dutta and Ganapathy, 2025) has increasingly"
        },
        {
          "{chengyan.wu, yiqiangcai, xueyun}@m.scnu.edu.cn,": "tions. As interest in emotionally intelligent sys-",
          "zg2272@columbia.edu,\nbolei.ma@lmu.de": ""
        },
        {
          "{chengyan.wu, yiqiangcai, xueyun}@m.scnu.edu.cn,": "",
          "zg2272@columbia.edu,\nbolei.ma@lmu.de": "focused on multimodal settings in dialogue, a direc-"
        },
        {
          "{chengyan.wu, yiqiangcai, xueyun}@m.scnu.edu.cn,": "tems grows, this survey provides timely guid-",
          "zg2272@columbia.edu,\nbolei.ma@lmu.de": ""
        },
        {
          "{chengyan.wu, yiqiangcai, xueyun}@m.scnu.edu.cn,": "ance for advancing MERC research.",
          "zg2272@columbia.edu,\nbolei.ma@lmu.de": "tion we refer to as Multimodal Emotion Recogni-"
        },
        {
          "{chengyan.wu, yiqiangcai, xueyun}@m.scnu.edu.cn,": "",
          "zg2272@columbia.edu,\nbolei.ma@lmu.de": "tion in Conversations (MERC). Researchers aim to"
        },
        {
          "{chengyan.wu, yiqiangcai, xueyun}@m.scnu.edu.cn,": "1\nIntroduction",
          "zg2272@columbia.edu,\nbolei.ma@lmu.de": ""
        },
        {
          "{chengyan.wu, yiqiangcai, xueyun}@m.scnu.edu.cn,": "",
          "zg2272@columbia.edu,\nbolei.ma@lmu.de": "identify the emotional state of a given utterance by"
        },
        {
          "{chengyan.wu, yiqiangcai, xueyun}@m.scnu.edu.cn,": "",
          "zg2272@columbia.edu,\nbolei.ma@lmu.de": "integrating contextual information from different"
        },
        {
          "{chengyan.wu, yiqiangcai, xueyun}@m.scnu.edu.cn,": "Emotion recognition in conversations (ERC) (Peng",
          "zg2272@columbia.edu,\nbolei.ma@lmu.de": ""
        },
        {
          "{chengyan.wu, yiqiangcai, xueyun}@m.scnu.edu.cn,": "",
          "zg2272@columbia.edu,\nbolei.ma@lmu.de": "modalities, which often includes subtle personal"
        },
        {
          "{chengyan.wu, yiqiangcai, xueyun}@m.scnu.edu.cn,": "et al., 2022; Deng and Ren, 2023) is an increas-",
          "zg2272@columbia.edu,\nbolei.ma@lmu.de": ""
        },
        {
          "{chengyan.wu, yiqiangcai, xueyun}@m.scnu.edu.cn,": "",
          "zg2272@columbia.edu,\nbolei.ma@lmu.de": "emotional states such as happiness, anger, and ha-"
        },
        {
          "{chengyan.wu, yiqiangcai, xueyun}@m.scnu.edu.cn,": "ingly important\ntask in natural\nlanguage process-",
          "zg2272@columbia.edu,\nbolei.ma@lmu.de": ""
        },
        {
          "{chengyan.wu, yiqiangcai, xueyun}@m.scnu.edu.cn,": "",
          "zg2272@columbia.edu,\nbolei.ma@lmu.de": "tred (Poria et al., 2019b; Gong et al., 2024), thereby"
        },
        {
          "{chengyan.wu, yiqiangcai, xueyun}@m.scnu.edu.cn,": "ing (NLP), focusing on identifying the emotional",
          "zg2272@columbia.edu,\nbolei.ma@lmu.de": ""
        },
        {
          "{chengyan.wu, yiqiangcai, xueyun}@m.scnu.edu.cn,": "",
          "zg2272@columbia.edu,\nbolei.ma@lmu.de": "improving the effectiveness of emotion recognition"
        },
        {
          "{chengyan.wu, yiqiangcai, xueyun}@m.scnu.edu.cn,": "state associated with each utterance in a dialogue.",
          "zg2272@columbia.edu,\nbolei.ma@lmu.de": ""
        },
        {
          "{chengyan.wu, yiqiangcai, xueyun}@m.scnu.edu.cn,": "",
          "zg2272@columbia.edu,\nbolei.ma@lmu.de": "in dialogues.\nFigure 1 illustrates an example of"
        },
        {
          "{chengyan.wu, yiqiangcai, xueyun}@m.scnu.edu.cn,": "Unlike conventional emotion classification on iso-",
          "zg2272@columbia.edu,\nbolei.ma@lmu.de": ""
        },
        {
          "{chengyan.wu, yiqiangcai, xueyun}@m.scnu.edu.cn,": "",
          "zg2272@columbia.edu,\nbolei.ma@lmu.de": "ERC with textual, acoustic, and visual inputs."
        },
        {
          "{chengyan.wu, yiqiangcai, xueyun}@m.scnu.edu.cn,": "lated sentences, ERC requires understanding the",
          "zg2272@columbia.edu,\nbolei.ma@lmu.de": ""
        },
        {
          "{chengyan.wu, yiqiangcai, xueyun}@m.scnu.edu.cn,": "interplay between utterances and tracking speaker-",
          "zg2272@columbia.edu,\nbolei.ma@lmu.de": "Multimodal emotion recognition (MER) itself"
        },
        {
          "{chengyan.wu, yiqiangcai, xueyun}@m.scnu.edu.cn,": "specific context across the conversation (Gao et al.,",
          "zg2272@columbia.edu,\nbolei.ma@lmu.de": "has gained increasing attention due to the chal-"
        },
        {
          "{chengyan.wu, yiqiangcai, xueyun}@m.scnu.edu.cn,": "2024). Its relevance has grown due to its potential",
          "zg2272@columbia.edu,\nbolei.ma@lmu.de": "lenges of integrating diverse modalities, prompting"
        },
        {
          "{chengyan.wu, yiqiangcai, xueyun}@m.scnu.edu.cn,": "in various real-world applications, including social",
          "zg2272@columbia.edu,\nbolei.ma@lmu.de": "research in both non-conversational and conversa-"
        },
        {
          "{chengyan.wu, yiqiangcai, xueyun}@m.scnu.edu.cn,": "media monitoring (Kumar et al., 2015), intelligent",
          "zg2272@columbia.edu,\nbolei.ma@lmu.de": "tional settings. Existing surveys such as A.V. et al."
        },
        {
          "{chengyan.wu, yiqiangcai, xueyun}@m.scnu.edu.cn,": "healthcare services (Hu et al., 2021b), and the de-",
          "zg2272@columbia.edu,\nbolei.ma@lmu.de": "(2024) and Aruna Gladys and Vetriselvi (2023) fo-"
        },
        {
          "{chengyan.wu, yiqiangcai, xueyun}@m.scnu.edu.cn,": "sign of emotionally aware dialogue agents (Jiao",
          "zg2272@columbia.edu,\nbolei.ma@lmu.de": "cus on non-conversational MER but overlook key"
        },
        {
          "{chengyan.wu, yiqiangcai, xueyun}@m.scnu.edu.cn,": "et al., 2020; Gong et al., 2023).",
          "zg2272@columbia.edu,\nbolei.ma@lmu.de": "aspects like interlocutor modeling and context. Fu"
        },
        {
          "{chengyan.wu, yiqiangcai, xueyun}@m.scnu.edu.cn,": "However, human emotions are typically con-",
          "zg2272@columbia.edu,\nbolei.ma@lmu.de": "et al. (2023) reviews both unimodal and multimodal"
        },
        {
          "{chengyan.wu, yiqiangcai, xueyun}@m.scnu.edu.cn,": "veyed through multiple modalities, including audi-",
          "zg2272@columbia.edu,\nbolei.ma@lmu.de": "conversational MER, yet primarily centers on fea-"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Each utterance is associated with\nutterance ui.": ""
        },
        {
          "Each utterance is associated with\nutterance ui.": "three modalities:\ni), acoustic (ua\ni ), and"
        },
        {
          "Each utterance is associated with\nutterance ui.": "visual (uv"
        },
        {
          "Each utterance is associated with\nutterance ui.": "i ), which provide complementary infor-"
        },
        {
          "Each utterance is associated with\nutterance ui.": "mation for emotion recognition. The multimodal"
        },
        {
          "Each utterance is associated with\nutterance ui.": "representation of an utterance is denoted as:"
        },
        {
          "Each utterance is associated with\nutterance ui.": ""
        },
        {
          "Each utterance is associated with\nutterance ui.": "for i = 1, 2, . . . , N\n(1)\nui = [ut\ni; ua\ni ; uv\ni ],"
        },
        {
          "Each utterance is associated with\nutterance ui.": ""
        },
        {
          "Each utterance is associated with\nutterance ui.": ""
        },
        {
          "Each utterance is associated with\nutterance ui.": "where [ · ; · ; · ] denotes combination of modalities."
        },
        {
          "Each utterance is associated with\nutterance ui.": "Methodology for Literature Compilation:"
        },
        {
          "Each utterance is associated with\nutterance ui.": ""
        },
        {
          "Each utterance is associated with\nutterance ui.": "Strategy. We conduct a comprehensive litera-"
        },
        {
          "Each utterance is associated with\nutterance ui.": ""
        },
        {
          "Each utterance is associated with\nutterance ui.": "ture search using sources such as the ACL Anthol-"
        },
        {
          "Each utterance is associated with\nutterance ui.": ""
        },
        {
          "Each utterance is associated with\nutterance ui.": "ogy, Google Scholar, and general search engines"
        },
        {
          "Each utterance is associated with\nutterance ui.": ""
        },
        {
          "Each utterance is associated with\nutterance ui.": "(e.g., Google Chrome). Within the ACL Anthology,"
        },
        {
          "Each utterance is associated with\nutterance ui.": ""
        },
        {
          "Each utterance is associated with\nutterance ui.": "we focus on top venues including EMNLP, ACL,"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ture fusion, offering limited insight into core chal-": "lenges such as cross-modal alignment, reasoning,",
          "• Textual modality: The transcribed representa-": "tion of spoken utterances, capturing the semantic"
        },
        {
          "ture fusion, offering limited insight into core chal-": "modality missingness, and conflicts.",
          "• Textual modality: The transcribed representa-": "and syntactic content of language."
        },
        {
          "ture fusion, offering limited insight into core chal-": "Despite growing interest,\nthe MERC task re-",
          "• Textual modality: The transcribed representa-": ""
        },
        {
          "ture fusion, offering limited insight into core chal-": "",
          "• Textual modality: The transcribed representa-": "• Acoustic modality: Prosodic and paralinguis-"
        },
        {
          "ture fusion, offering limited insight into core chal-": "mains underexplored. Existing surveys (Fu et al.,",
          "• Textual modality: The transcribed representa-": ""
        },
        {
          "ture fusion, offering limited insight into core chal-": "",
          "• Textual modality: The transcribed representa-": "tic features extracted from speech, such as tone,"
        },
        {
          "ture fusion, offering limited insight into core chal-": "2023; Zhang and Tan, 2024) also lag behind recent",
          "• Textual modality: The transcribed representa-": ""
        },
        {
          "ture fusion, offering limited insight into core chal-": "",
          "• Textual modality: The transcribed representa-": "pitch, and energy."
        },
        {
          "ture fusion, offering limited insight into core chal-": "advances, particularly the rise of multimodal large",
          "• Textual modality: The transcribed representa-": ""
        },
        {
          "ture fusion, offering limited insight into core chal-": "language models (MLLMs). To bridge this gap,",
          "• Textual modality: The transcribed representa-": ""
        },
        {
          "ture fusion, offering limited insight into core chal-": "",
          "• Textual modality: The transcribed representa-": "• Visual modality: Non-verbal cues such as fa-"
        },
        {
          "ture fusion, offering limited insight into core chal-": "we present a timely and comprehensive review of",
          "• Textual modality: The transcribed representa-": ""
        },
        {
          "ture fusion, offering limited insight into core chal-": "",
          "• Textual modality: The transcribed representa-": "cial expressions, head movements, eye gaze, and"
        },
        {
          "ture fusion, offering limited insight into core chal-": "MERC. We first introduce the task definition and",
          "• Textual modality: The transcribed representa-": ""
        },
        {
          "ture fusion, offering limited insight into core chal-": "",
          "• Textual modality: The transcribed representa-": "gestures."
        },
        {
          "ture fusion, offering limited insight into core chal-": "our survey methodology (§2), benchmark datasets",
          "• Textual modality: The transcribed representa-": ""
        },
        {
          "ture fusion, offering limited insight into core chal-": "",
          "• Textual modality: The transcribed representa-": "=\nTask\nDefinition.\nGiven\na\ndialogue D"
        },
        {
          "ture fusion, offering limited insight into core chal-": "and evaluation methods (§3), followed by a review",
          "• Textual modality: The transcribed representa-": ""
        },
        {
          "ture fusion, offering limited insight into core chal-": "of preprocessing techniques (§4); then categorize",
          "• Textual modality: The transcribed representa-": "{u1, u2, . . . , uN } consisting of N utterances spo-"
        },
        {
          "ture fusion, offering limited insight into core chal-": "recent methods (§5) and outline key challenges and",
          "• Textual modality: The transcribed representa-": "ken by multiple speakers,\nthe goal of the MERC"
        },
        {
          "ture fusion, offering limited insight into core chal-": "prospects (§6).",
          "• Textual modality: The transcribed representa-": "task is to predict an emotion label ei ∈ Y for each"
        },
        {
          "ture fusion, offering limited insight into core chal-": "",
          "• Textual modality: The transcribed representa-": "Each utterance is associated with\nutterance ui."
        },
        {
          "ture fusion, offering limited insight into core chal-": "In summary,\nthe specific contributions of\nthis",
          "• Textual modality: The transcribed representa-": ""
        },
        {
          "ture fusion, offering limited insight into core chal-": "survey are threefold:",
          "• Textual modality: The transcribed representa-": "three modalities:\ni), acoustic (ua\ni ), and"
        },
        {
          "ture fusion, offering limited insight into core chal-": "",
          "• Textual modality: The transcribed representa-": "visual (uv"
        },
        {
          "ture fusion, offering limited insight into core chal-": "",
          "• Textual modality: The transcribed representa-": "i ), which provide complementary infor-"
        },
        {
          "ture fusion, offering limited insight into core chal-": "• Compilation of recent MERC research devel-",
          "• Textual modality: The transcribed representa-": "mation for emotion recognition. The multimodal"
        },
        {
          "ture fusion, offering limited insight into core chal-": "opments. We systematically review and integrate",
          "• Textual modality: The transcribed representa-": "representation of an utterance is denoted as:"
        },
        {
          "ture fusion, offering limited insight into core chal-": "the latest research progress made by MERC in re-",
          "• Textual modality: The transcribed representa-": ""
        },
        {
          "ture fusion, offering limited insight into core chal-": "",
          "• Textual modality: The transcribed representa-": "for i = 1, 2, . . . , N\n(1)\nui = [ut\ni; ua\ni ; uv\ni ],"
        },
        {
          "ture fusion, offering limited insight into core chal-": "cent years, covering diverse datasets and method-",
          "• Textual modality: The transcribed representa-": ""
        },
        {
          "ture fusion, offering limited insight into core chal-": "ologies.",
          "• Textual modality: The transcribed representa-": ""
        },
        {
          "ture fusion, offering limited insight into core chal-": "",
          "• Textual modality: The transcribed representa-": "where [ · ; · ; · ] denotes combination of modalities."
        },
        {
          "ture fusion, offering limited insight into core chal-": "• Summarizing and comparing various MERC",
          "• Textual modality: The transcribed representa-": "Methodology for Literature Compilation:"
        },
        {
          "ture fusion, offering limited insight into core chal-": "methods. We evaluate the strengths and lim-",
          "• Textual modality: The transcribed representa-": ""
        },
        {
          "ture fusion, offering limited insight into core chal-": "",
          "• Textual modality: The transcribed representa-": "Strategy. We conduct a comprehensive litera-"
        },
        {
          "ture fusion, offering limited insight into core chal-": "itations of various MERC approaches, offering",
          "• Textual modality: The transcribed representa-": ""
        },
        {
          "ture fusion, offering limited insight into core chal-": "",
          "• Textual modality: The transcribed representa-": "ture search using sources such as the ACL Anthol-"
        },
        {
          "ture fusion, offering limited insight into core chal-": "theoretical insights and practical guidance to help",
          "• Textual modality: The transcribed representa-": ""
        },
        {
          "ture fusion, offering limited insight into core chal-": "",
          "• Textual modality: The transcribed representa-": "ogy, Google Scholar, and general search engines"
        },
        {
          "ture fusion, offering limited insight into core chal-": "researchers and practitioners select appropriate",
          "• Textual modality: The transcribed representa-": ""
        },
        {
          "ture fusion, offering limited insight into core chal-": "",
          "• Textual modality: The transcribed representa-": "(e.g., Google Chrome). Within the ACL Anthology,"
        },
        {
          "ture fusion, offering limited insight into core chal-": "methods.",
          "• Textual modality: The transcribed representa-": ""
        },
        {
          "ture fusion, offering limited insight into core chal-": "",
          "• Textual modality: The transcribed representa-": "we focus on top venues including EMNLP, ACL,"
        },
        {
          "ture fusion, offering limited insight into core chal-": "",
          "• Textual modality: The transcribed representa-": "NAACL, and related workshops.1"
        },
        {
          "ture fusion, offering limited insight into core chal-": "• Proposing challenges and future directions.",
          "• Textual modality: The transcribed representa-": ""
        },
        {
          "ture fusion, offering limited insight into core chal-": "",
          "• Textual modality: The transcribed representa-": "Selection Criteria. We select papers that are di-"
        },
        {
          "ture fusion, offering limited insight into core chal-": "We identify key open issues in the MERC domain",
          "• Textual modality: The transcribed representa-": ""
        },
        {
          "ture fusion, offering limited insight into core chal-": "",
          "• Textual modality: The transcribed representa-": "rectly relevant\nto MERC, with a focus on works"
        },
        {
          "ture fusion, offering limited insight into core chal-": "and put forward several potential future research",
          "• Textual modality: The transcribed representa-": ""
        },
        {
          "ture fusion, offering limited insight into core chal-": "",
          "• Textual modality: The transcribed representa-": "that used at least two modalities (e.g., text, audio,"
        },
        {
          "ture fusion, offering limited insight into core chal-": "directions, aiming to guide ongoing and future",
          "• Textual modality: The transcribed representa-": ""
        },
        {
          "ture fusion, offering limited insight into core chal-": "",
          "• Textual modality: The transcribed representa-": "visual), included conversational context, and eval-"
        },
        {
          "ture fusion, offering limited insight into core chal-": "investigations by researchers and practitioners in",
          "• Textual modality: The transcribed representa-": ""
        },
        {
          "ture fusion, offering limited insight into core chal-": "",
          "• Textual modality: The transcribed representa-": "uated on benchmark datasets such as IEMOCAP,"
        },
        {
          "ture fusion, offering limited insight into core chal-": "MERC.",
          "• Textual modality: The transcribed representa-": ""
        },
        {
          "ture fusion, offering limited insight into core chal-": "",
          "• Textual modality: The transcribed representa-": "MELD, and CMU-MOSEI. We prioritize recent"
        },
        {
          "ture fusion, offering limited insight into core chal-": "",
          "• Textual modality: The transcribed representa-": "papers from 2020 onward to reflect\nthe state-of-"
        },
        {
          "ture fusion, offering limited insight into core chal-": "2\nTask Settings and Review Methodology",
          "• Textual modality: The transcribed representa-": ""
        },
        {
          "ture fusion, offering limited insight into core chal-": "",
          "• Textual modality: The transcribed representa-": "the-art, while including foundational work when"
        },
        {
          "ture fusion, offering limited insight into core chal-": "In this\nsection, we present\nthe task settings of",
          "• Textual modality: The transcribed representa-": "appropriate for historical context. The selection is"
        },
        {
          "ture fusion, offering limited insight into core chal-": "MERC and outline the methodology employed to",
          "• Textual modality: The transcribed representa-": "based on a careful review of the abstract, introduc-"
        },
        {
          "ture fusion, offering limited insight into core chal-": "compile the content of this survey, which details",
          "• Textual modality: The transcribed representa-": "tion, conclusion, and limitations of each paper."
        },
        {
          "ture fusion, offering limited insight into core chal-": "the strategy and selection criteria used to curate the",
          "• Textual modality: The transcribed representa-": ""
        },
        {
          "ture fusion, offering limited insight into core chal-": "final content for this survey paper.",
          "• Textual modality: The transcribed representa-": "3\nDatasets and Evaluation"
        },
        {
          "ture fusion, offering limited insight into core chal-": "",
          "• Textual modality: The transcribed representa-": "In this section, we describe the evaluation datasets"
        },
        {
          "ture fusion, offering limited insight into core chal-": "Modalities.\nIn the context of MERC, we define",
          "• Textual modality: The transcribed representa-": ""
        },
        {
          "ture fusion, offering limited insight into core chal-": "",
          "• Textual modality: The transcribed representa-": "and the evaluation metrics used for the MERC task,"
        },
        {
          "ture fusion, offering limited insight into core chal-": "a modality as a distinct source or channel of infor-",
          "• Textual modality: The transcribed representa-": ""
        },
        {
          "ture fusion, offering limited insight into core chal-": "",
          "• Textual modality: The transcribed representa-": "focusing on multimodal resources across multiple"
        },
        {
          "ture fusion, offering limited insight into core chal-": "mation that conveys emotional or communicative",
          "• Textual modality: The transcribed representa-": ""
        },
        {
          "ture fusion, offering limited insight into core chal-": "signals. These modalities correspond to observable",
          "• Textual modality: The transcribed representa-": "1We used keywords such as “multimodal emotion recogni-"
        },
        {
          "ture fusion, offering limited insight into core chal-": "",
          "• Textual modality: The transcribed representa-": "tion”, “emotion recognition in conversation”, “multimodal af-"
        },
        {
          "ture fusion, offering limited insight into core chal-": "representations derived from different sensory in-",
          "• Textual modality: The transcribed representa-": ""
        },
        {
          "ture fusion, offering limited insight into core chal-": "",
          "• Textual modality: The transcribed representa-": "fective computing”, “dialogue emotion recognition”, “MERC"
        },
        {
          "ture fusion, offering limited insight into core chal-": "puts and are typically processed in separate feature",
          "• Textual modality: The transcribed representa-": ""
        },
        {
          "ture fusion, offering limited insight into core chal-": "",
          "• Textual modality: The transcribed representa-": "benchmark”, “context-aware emotion detection”, and “multi-"
        },
        {
          "ture fusion, offering limited insight into core chal-": "spaces. The most common modalities include:",
          "• Textual modality: The transcribed representa-": "modal conversational modeling”."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 1: , the domains covered by ERC,theextractoraimstocaptureenoughinforma-",
      "data": [
        {
          "languages. For more detailed information about": "the single benchmarks, see Appendix §A.",
          "4.1\nFeature Extraction": ""
        },
        {
          "languages. For more detailed information about": "",
          "4.1\nFeature Extraction": "For effective multimodal analysis,\nfeatures must"
        },
        {
          "languages. For more detailed information about": "(1) English-centered: IEMOCAP, MELD, CMU-",
          "4.1\nFeature Extraction": "first be extracted from each modality stream (text,"
        },
        {
          "languages. For more detailed information about": "MOSEI, AVEC, EmoryNLP,\nand MEmoR",
          "4.1\nFeature Extraction": "audio, visual). Mainstream approaches (e.g., Shi"
        },
        {
          "languages. For more detailed information about": "dataset.",
          "4.1\nFeature Extraction": "and Huang, 2023) typically process these modali-"
        },
        {
          "languages. For more detailed information about": "",
          "4.1\nFeature Extraction": "ties separately at this initial stage. While the core"
        },
        {
          "languages. For more detailed information about": "(2) Non-English: M-MELD (French, Spanish,",
          "4.1\nFeature Extraction": ""
        },
        {
          "languages. For more detailed information about": "",
          "4.1\nFeature Extraction": "extraction techniques often overlap, the key differ-"
        },
        {
          "languages. For more detailed information about": "Greek, Polish), ACE (African), M3ED (Man-",
          "4.1\nFeature Extraction": ""
        },
        {
          "languages. For more detailed information about": "",
          "4.1\nFeature Extraction": "ence in the multimodal setting lies in the objective"
        },
        {
          "languages. For more detailed information about": "darin).",
          "4.1\nFeature Extraction": ""
        },
        {
          "languages. For more detailed information about": "",
          "4.1\nFeature Extraction": "and subsequent use of these features. In unimodal"
        },
        {
          "languages. For more detailed information about": "As shown in Table 1,\nthe domains covered by",
          "4.1\nFeature Extraction": "ERC, the extractor aims to capture enough informa-"
        },
        {
          "languages. For more detailed information about": "multimodal datasets have become increasingly di-",
          "4.1\nFeature Extraction": "tion within that single modality for emotion classi-"
        },
        {
          "languages. For more detailed information about": "verse over time. The sources of these datasets in-",
          "4.1\nFeature Extraction": "fication. Table 2 provides an overview of common"
        },
        {
          "languages. For more detailed information about": "clude TV series, videos, and movies. At the same",
          "4.1\nFeature Extraction": "feature extraction models employed in the multi-"
        },
        {
          "languages. For more detailed information about": "time, linguistic diversity has expanded to include",
          "4.1\nFeature Extraction": "modal studies surveyed in this paper."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: , the domains covered by ERC,theextractoraimstocaptureenoughinforma-",
      "data": [
        {
          "verse over time. The sources of these datasets in-": "clude TV series, videos, and movies. At the same",
          "fication. Table 2 provides an overview of common": "feature extraction models employed in the multi-"
        },
        {
          "verse over time. The sources of these datasets in-": "time, linguistic diversity has expanded to include",
          "fication. Table 2 provides an overview of common": "modal studies surveyed in this paper."
        },
        {
          "verse over time. The sources of these datasets in-": "languages such as French, Spanish, Greek, Polish,",
          "fication. Table 2 provides an overview of common": ""
        },
        {
          "verse over time. The sources of these datasets in-": "",
          "fication. Table 2 provides an overview of common": "Modality\nModels"
        },
        {
          "verse over time. The sources of these datasets in-": "and Mandarin. Notably, there has also been a grow-",
          "fication. Table 2 provides an overview of common": ""
        },
        {
          "verse over time. The sources of these datasets in-": "",
          "fication. Table 2 provides an overview of common": "LSTM (Hochreiter and Schmidhuber, 1997)"
        },
        {
          "verse over time. The sources of these datasets in-": "ing emergence of datasets targeting low-resource",
          "fication. Table 2 provides an overview of common": "CNN (Kim, 2014; Tran et al., 2015)"
        },
        {
          "verse over time. The sources of these datasets in-": "",
          "fication. Table 2 provides an overview of common": "Text\nTransformer (Vaswani et al., 2017)"
        },
        {
          "verse over time. The sources of these datasets in-": "languages, such as African languages.",
          "fication. Table 2 provides an overview of common": ""
        },
        {
          "verse over time. The sources of these datasets in-": "",
          "fication. Table 2 provides an overview of common": "RoBERTa (Liu et al., 2019)"
        },
        {
          "verse over time. The sources of these datasets in-": "",
          "fication. Table 2 provides an overview of common": "sBERT (Reimers and Gurevych, 2019)"
        },
        {
          "verse over time. The sources of these datasets in-": "Datasets\nLang.\nSource\nYear",
          "fication. Table 2 provides an overview of common": "3D-CNN (Tran et al., 2015)"
        },
        {
          "verse over time. The sources of these datasets in-": "",
          "fication. Table 2 provides an overview of common": "OpenFace (Baltrušaitis et al., 2016)"
        },
        {
          "verse over time. The sources of these datasets in-": "IEMOCAP (Busso et al., 2008)\nen\nVideos\n2008",
          "fication. Table 2 provides an overview of common": "Visual\nMTCNN (Zhang et al., 2016)"
        },
        {
          "verse over time. The sources of these datasets in-": "AVEC (Schuller et al., 2012)\nen\nVideos\n2012",
          "fication. Table 2 provides an overview of common": "DenseNet (Huang et al., 2017)"
        },
        {
          "verse over time. The sources of these datasets in-": "",
          "fication. Table 2 provides an overview of common": "VisExtNet (Shi and Huang, 2023)"
        },
        {
          "verse over time. The sources of these datasets in-": "EmoryNLP (Zahiri and Choi, 2017)\nen\nTV series\n2017",
          "fication. Table 2 provides an overview of common": ""
        },
        {
          "verse over time. The sources of these datasets in-": "CMU-MOSEI (Bagher Zadeh et al., 2018)\nen\nVideos\n2018",
          "fication. Table 2 provides an overview of common": "openSMILE (Eyben et al., 2010)"
        },
        {
          "verse over time. The sources of these datasets in-": "MELD (Poria et al., 2019a)\nen\nTV series\n2019",
          "fication. Table 2 provides an overview of common": "COVAREP (Degottex et al., 2014)"
        },
        {
          "verse over time. The sources of these datasets in-": "",
          "fication. Table 2 provides an overview of common": "Audio"
        },
        {
          "verse over time. The sources of these datasets in-": "MEmoR (Shen et al., 2020)\nen\nVideos\n2020",
          "fication. Table 2 provides an overview of common": "librosa (McFee et al., 2015)"
        },
        {
          "verse over time. The sources of these datasets in-": "",
          "fication. Table 2 provides an overview of common": "DialogueRNN (Majumder et al., 2019)"
        },
        {
          "verse over time. The sources of these datasets in-": "M3ED (Zhao et al., 2022)\nzh\nTV series\n2022",
          "fication. Table 2 provides an overview of common": ""
        },
        {
          "verse over time. The sources of these datasets in-": "M-MELD (Ghosh et al., 2023)\nfr,es,el,pl TV series\n2023",
          "fication. Table 2 provides an overview of common": ""
        },
        {
          "verse over time. The sources of these datasets in-": "ACE (Sasu et al., 2025)\nAkan\nMovies\n2025",
          "fication. Table 2 provides an overview of common": "Table 2: Feature Extraction models."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "E.g. DialogueGCN (Ghosal et al., 2019); MMGCN (Hu et al.,": ""
        },
        {
          "E.g. DialogueGCN (Ghosal et al., 2019); MMGCN (Hu et al.,": "2021b); CORECT (Nguyen et al., 2023); GraphSmile (Li et al.,"
        },
        {
          "E.g. DialogueGCN (Ghosal et al., 2019); MMGCN (Hu et al.,": ""
        },
        {
          "E.g. DialogueGCN (Ghosal et al., 2019); MMGCN (Hu et al.,": "2024b); MERC-GCN (Feng and Fan, 2025)"
        },
        {
          "E.g. DialogueGCN (Ghosal et al., 2019); MMGCN (Hu et al.,": "E.g. GCNet (Lian et al., 2023); MER-HGraph (Li et al., 2023b);"
        },
        {
          "E.g. DialogueGCN (Ghosal et al., 2019); MMGCN (Hu et al.,": ""
        },
        {
          "E.g. DialogueGCN (Ghosal et al., 2019); MMGCN (Hu et al.,": "MDH (Huang et al., 2024a); ConxGNN (Van et al., 2025)"
        },
        {
          "E.g. DialogueGCN (Ghosal et al., 2019); MMGCN (Hu et al.,": "E.g. ADGCN (Liu et al., 2024b); DGODE (Shou et al., 2025);"
        },
        {
          "E.g. DialogueGCN (Ghosal et al., 2019); MMGCN (Hu et al.,": ""
        },
        {
          "E.g. DialogueGCN (Ghosal et al., 2019); MMGCN (Hu et al.,": "TL-RGCN (Su et al., 2024); GS-MCC (Ai et al., 2025b)"
        },
        {
          "E.g. DialogueGCN (Ghosal et al., 2019); MMGCN (Hu et al.,": "E.g. DialogueTRM (Mao et al., 2021); Emoformer (Li et al.,"
        },
        {
          "E.g. DialogueGCN (Ghosal et al., 2019); MMGCN (Hu et al.,": "2022); CMCF-SRNet (Zhang and Li, 2023); SDT (Ma et al.,"
        },
        {
          "E.g. DialogueGCN (Ghosal et al., 2019); MMGCN (Hu et al.,": "2024a)"
        },
        {
          "E.g. DialogueGCN (Ghosal et al., 2019); MMGCN (Hu et al.,": ""
        },
        {
          "E.g. DialogueGCN (Ghosal et al., 2019); MMGCN (Hu et al.,": "E.g. MMT (Zou et al., 2022); MPT (Zou et al., 2023); MM-"
        },
        {
          "E.g. DialogueGCN (Ghosal et al., 2019); MMGCN (Hu et al.,": ""
        },
        {
          "E.g. DialogueGCN (Ghosal et al., 2019); MMGCN (Hu et al.,": "NodeFormer (Huang et al., 2024b); CMATH (Zhu et al., 2024)"
        },
        {
          "E.g. DialogueGCN (Ghosal et al., 2019); MMGCN (Hu et al.,": "E.g.\nInstructERC (Lei et al., 2023); CKERC (Fu, 2024); LaERC-"
        },
        {
          "E.g. DialogueGCN (Ghosal et al., 2019); MMGCN (Hu et al.,": "S (Fu et al., 2025a)"
        },
        {
          "E.g. DialogueGCN (Ghosal et al., 2019); MMGCN (Hu et al.,": "E.g. DialogueLLM (Zhang et al., 2025); BeMERC (Fu et al.,"
        },
        {
          "E.g. DialogueGCN (Ghosal et al., 2019); MMGCN (Hu et al.,": "2025b); MERITS-L (Dutta and Ganapathy, 2025)"
        },
        {
          "E.g. DialogueGCN (Ghosal et al., 2019); MMGCN (Hu et al.,": "E.g. UniMSE (Hu et al., 2022); UniSA (Li et al., 2023c); Facing-"
        },
        {
          "E.g. DialogueGCN (Ghosal et al., 2019); MMGCN (Hu et al.,": "Bot\n(Tanioka et al., 2024); MSE-Adapter\n(Yang et al., 2025)"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fusion and Adaptation": "Figure 2: A taxonomy of mainstream methods.",
          "Bot\n(Tanioka et al., 2024); MSE-Adapter\n(Yang et al., 2025)": ""
        },
        {
          "Fusion and Adaptation": "to perceive speaker role information. Therefore, to",
          "Bot\n(Tanioka et al., 2024); MSE-Adapter\n(Yang et al., 2025)": "neural network as follows:"
        },
        {
          "Fusion and Adaptation": "more effectively learn and distinguish speaker-level",
          "Bot\n(Tanioka et al., 2024); MSE-Adapter\n(Yang et al., 2025)": ""
        },
        {
          "Fusion and Adaptation": "contextual representations, many studies have fur-",
          "Bot\n(Tanioka et al., 2024); MSE-Adapter\n(Yang et al., 2025)": ""
        },
        {
          "Fusion and Adaptation": "ther introduced speaker-related structured informa-",
          "Bot\n(Tanioka et al., 2024); MSE-Adapter\n(Yang et al., 2025)": "hg"
        },
        {
          "Fusion and Adaptation": "",
          "Bot\n(Tanioka et al., 2024); MSE-Adapter\n(Yang et al., 2025)": "(4)\ni , {(cs\nj, ωij, rij) | eij ∈ E})\ni = GNN(cs"
        },
        {
          "Fusion and Adaptation": "tion on top of dialogue context modeling. Common",
          "Bot\n(Tanioka et al., 2024); MSE-Adapter\n(Yang et al., 2025)": ""
        },
        {
          "Fusion and Adaptation": "approaches\ninclude using Speaker Embeddings",
          "Bot\n(Tanioka et al., 2024); MSE-Adapter\n(Yang et al., 2025)": "where cs\nare the contextual node features from"
        },
        {
          "Fusion and Adaptation": "",
          "Bot\n(Tanioka et al., 2024); MSE-Adapter\n(Yang et al., 2025)": "j"
        },
        {
          "Fusion and Adaptation": "(Ma et al., 2024a; Shen et al., 2020) to explicitly",
          "Bot\n(Tanioka et al., 2024); MSE-Adapter\n(Yang et al., 2025)": "neighboring utterances. The GNN aggregates infor-"
        },
        {
          "Fusion and Adaptation": "differentiate between different speakers, or lever-",
          "Bot\n(Tanioka et al., 2024); MSE-Adapter\n(Yang et al., 2025)": "mation from connected nodes to enhance cs"
        },
        {
          "Fusion and Adaptation": "",
          "Bot\n(Tanioka et al., 2024); MSE-Adapter\n(Yang et al., 2025)": "i with"
        },
        {
          "Fusion and Adaptation": "aging Graph Neural Networks (Ai et al., 2025a;",
          "Bot\n(Tanioka et al., 2024); MSE-Adapter\n(Yang et al., 2025)": "speaker- and structure-aware interaction knowl-"
        },
        {
          "Fusion and Adaptation": "Van et al., 2025)\nto construct\ninteraction graphs",
          "Bot\n(Tanioka et al., 2024); MSE-Adapter\n(Yang et al., 2025)": "edge."
        },
        {
          "Fusion and Adaptation": "between speakers, thereby more comprehensively",
          "Bot\n(Tanioka et al., 2024); MSE-Adapter\n(Yang et al., 2025)": ""
        },
        {
          "Fusion and Adaptation": "modeling the dependencies between them:",
          "Bot\n(Tanioka et al., 2024); MSE-Adapter\n(Yang et al., 2025)": ""
        },
        {
          "Fusion and Adaptation": "",
          "Bot\n(Tanioka et al., 2024); MSE-Adapter\n(Yang et al., 2025)": "5\nMethodology"
        },
        {
          "Fusion and Adaptation": "Speaker Embeddings: The speaker embedding",
          "Bot\n(Tanioka et al., 2024); MSE-Adapter\n(Yang et al., 2025)": ""
        },
        {
          "Fusion and Adaptation": "",
          "Bot\n(Tanioka et al., 2024); MSE-Adapter\n(Yang et al., 2025)": "This section discusses state-of-the-art approaches"
        },
        {
          "Fusion and Adaptation": "is combined with the modality features (e.g.,\nSi",
          "Bot\n(Tanioka et al., 2024); MSE-Adapter\n(Yang et al., 2025)": ""
        },
        {
          "Fusion and Adaptation": "",
          "Bot\n(Tanioka et al., 2024); MSE-Adapter\n(Yang et al., 2025)": "to the MERC tasks. We summarize them from three"
        },
        {
          "Fusion and Adaptation": "text, audio, or vision) to produce modality repre-",
          "Bot\n(Tanioka et al., 2024); MSE-Adapter\n(Yang et al., 2025)": ""
        },
        {
          "Fusion and Adaptation": "",
          "Bot\n(Tanioka et al., 2024); MSE-Adapter\n(Yang et al., 2025)": "perspectives: Graph-based Methods (§5.1), Fusion-"
        },
        {
          "Fusion and Adaptation": "sentations that are speaker- and context-aware.",
          "Bot\n(Tanioka et al., 2024); MSE-Adapter\n(Yang et al., 2025)": ""
        },
        {
          "Fusion and Adaptation": "",
          "Bot\n(Tanioka et al., 2024); MSE-Adapter\n(Yang et al., 2025)": "based Methods (§5.2), and Generation-based Meth-"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "fusion-based when their main contribution lies",
          "convolutional networks and incorporating speaker": "information to enhance the representation of con-"
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "in the design of cross-modal\ninteraction mech-",
          "convolutional networks and incorporating speaker": "versational semantics. Inspired by the application"
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "anisms, regardless of the backbone architecture",
          "convolutional networks and incorporating speaker": "of graph convolutions in ERC, Li et al.\n(2024b)"
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "(e.g., Transformer, LSTM).",
          "convolutional networks and incorporating speaker": "proposed the GSF module, which introduces an"
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "",
          "convolutional networks and incorporating speaker": "alternating graph convolution mechanism to hierar-"
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "• Generation-based: Generation-based methods",
          "convolutional networks and incorporating speaker": ""
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "",
          "convolutional networks and incorporating speaker": "chically extract both cross-modal and intra-modal"
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "refer to recent approaches that leverage LLMs to",
          "convolutional networks and incorporating speaker": ""
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "",
          "convolutional networks and incorporating speaker": "emotional\ninformation. Some studies further en-"
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "generate predictions or intermediate reasoning,",
          "convolutional networks and incorporating speaker": ""
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "",
          "convolutional networks and incorporating speaker": "hanced graph-based models with attention mech-"
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "often using prompt engineering or\ninstruction",
          "convolutional networks and incorporating speaker": ""
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "",
          "convolutional networks and incorporating speaker": "anisms for multimodal fusion; for example, Feng"
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "tuning, even if lightweight fusion components",
          "convolutional networks and incorporating speaker": ""
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "",
          "convolutional networks and incorporating speaker": "and Fan (2025) integrated a Cross-modal Attention"
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "are present.",
          "convolutional networks and incorporating speaker": ""
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "",
          "convolutional networks and incorporating speaker": "Module to better fuse information from different"
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "",
          "convolutional networks and incorporating speaker": "modalities, while Nguyen et al. (2023) designed"
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "5.1\nGraph-based Methods",
          "convolutional networks and incorporating speaker": ""
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "",
          "convolutional networks and incorporating speaker": "a cross-modal attention mechanism to explicitly"
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "Dialogues can be naturally interpreted as graph",
          "convolutional networks and incorporating speaker": ""
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "",
          "convolutional networks and incorporating speaker": "model the heterogeneity between modalities."
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "structures due to the intrinsic correlations and de-",
          "convolutional networks and incorporating speaker": ""
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "",
          "convolutional networks and incorporating speaker": "Hypergraph Neural Networks. Although tradi-"
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "pendencies among utterances. Conversations often",
          "convolutional networks and incorporating speaker": ""
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "",
          "convolutional networks and incorporating speaker": "tional graph-based methods can capture long-range"
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "feature multi-turn interactions with complex de-",
          "convolutional networks and incorporating speaker": ""
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "",
          "convolutional networks and incorporating speaker": "and multimodal contextual\ninformation,\nthey are"
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "pendency and interaction patterns, which can be",
          "convolutional networks and incorporating speaker": ""
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "",
          "convolutional networks and incorporating speaker": "often challenged by missing modalities during con-"
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "effectively modeled through the edge structures of",
          "convolutional networks and incorporating speaker": ""
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "",
          "convolutional networks and incorporating speaker": "versations.\nLian et al.\n(2023)\ntackled this issue"
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "Graph Neural Networks (GNNs) (Scarselli et al.,",
          "convolutional networks and incorporating speaker": ""
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "",
          "convolutional networks and incorporating speaker": "by jointly optimizing classification and reconstruc-"
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "2009). With the increasing interest in multimodal",
          "convolutional networks and incorporating speaker": ""
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "",
          "convolutional networks and incorporating speaker": "tion tasks in an end-to-end manner to effectively"
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "dialogue understanding, GNNs have evolved be-",
          "convolutional networks and incorporating speaker": ""
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "",
          "convolutional networks and incorporating speaker": "model incomplete data. The related works (Li et al.,"
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "yond their application (Liu et al., 2024a) in textual",
          "convolutional networks and incorporating speaker": ""
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "",
          "convolutional networks and incorporating speaker": "2023b; Huang et al., 2024a) considered the limi-"
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "data to embrace multimodal\ninputs. Besides,\nre-",
          "convolutional networks and incorporating speaker": ""
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "",
          "convolutional networks and incorporating speaker": "tations imposed by the pairwise relationships be-"
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "cent methods also integrate auxiliary modules (e.g.,",
          "convolutional networks and incorporating speaker": ""
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "",
          "convolutional networks and incorporating speaker": "tween GNNs nodes.Van et al. (2025) constructed"
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "convolution, contrastive learning, and fusion) to en-",
          "convolutional networks and incorporating speaker": ""
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "",
          "convolutional networks and incorporating speaker": "a multimodal fusion graph and introduced Hyper-"
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "hance the performance. Figure 3 illustrates recent",
          "convolutional networks and incorporating speaker": ""
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "",
          "convolutional networks and incorporating speaker": "graph Neural Networks (Feng et al., 2019) to con-"
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "advancements in graph-based methods. We catego-",
          "convolutional networks and incorporating speaker": ""
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "",
          "convolutional networks and incorporating speaker": "nect multiple modalities or utterance nodes simul-"
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "rize them into traditional graphs, hypergraphs, and",
          "convolutional networks and incorporating speaker": ""
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "",
          "convolutional networks and incorporating speaker": "taneously, thereby capturing more complex multi-"
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "fourier graph neural networks.",
          "convolutional networks and incorporating speaker": ""
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "",
          "convolutional networks and incorporating speaker": "variate dependencies and high-order interactions"
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "(c) Fourier Graph Neural Networks\n(a) Traditional Graph Neural Networks\n(b) Hypergraph Neural Networks",
          "convolutional networks and incorporating speaker": "in conversations, and enhancing the modeling of"
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "audio\ntext\nvisual\naudio\naudio\ntext\nvisual\ntext\nvisual",
          "convolutional networks and incorporating speaker": "emotion propagation."
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "Graph Module\nHypergraph Module\nGraph Module\nGraph Module",
          "convolutional networks and incorporating speaker": "Fourier Graph Neural Networks. Increasing"
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "",
          "convolutional networks and incorporating speaker": "the depth of GNN layers can lead to the over-"
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "DFT\nDFT\n+\nContrastive",
          "convolutional networks and incorporating speaker": ""
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "Learning\nlow frequency\nhigh frequency\nContrastive \nConvolutional \ninformation\nFusion \ninformation",
          "convolutional networks and incorporating speaker": "smoothing problem (Liu et al., 2022; Yi et al.,"
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "Contrastive \nConvolutional \nFusion \nLearning Module\nModule\nModule",
          "convolutional networks and incorporating speaker": ""
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "Learning Module\nModule\nModule\n+",
          "convolutional networks and incorporating speaker": ""
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "IDFT\nIDFT",
          "convolutional networks and incorporating speaker": "2023), which hampers the modeling of long-range"
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "Output\nOutput\nOutput",
          "convolutional networks and incorporating speaker": ""
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "",
          "convolutional networks and incorporating speaker": "semantic dependencies and complementary modal-"
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "Figure 3: Development of Graph-based Methods.",
          "convolutional networks and incorporating speaker": "ity relations. To address this issue, GS-MMC (Ai"
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "",
          "convolutional networks and incorporating speaker": "et al., 2025b) proposes a graph-based framework"
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "Traditional Graph Neural Networks. Previous",
          "convolutional networks and incorporating speaker": "for multimodal consistency and complementarity"
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "works such as bc-LSTM (Poria et al., 2017) and",
          "convolutional networks and incorporating speaker": "learning. This method employs a Fourier graph op-"
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "ICON (Hazarika et al., 2018) primarily relied on se-",
          "convolutional networks and incorporating speaker": "erator to extract high- and low-frequency emotional"
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "quential approaches. DialogueGCN (Ghosal et al.,",
          "convolutional networks and incorporating speaker": "signals from the frequency domain, capturing both"
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "2019) was the first to introduce GNNs into ERC, ad-",
          "convolutional networks and incorporating speaker": "local variations and global semantic trends. Addi-"
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "dressing the limitations of earlier sequence-based",
          "convolutional networks and incorporating speaker": "tionally, a contrastive learning mechanism (van den"
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "models like DialogueRNN (Majumder et al., 2019)",
          "convolutional networks and incorporating speaker": "Oord et al., 2019) is designed to enhance the se-"
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "in capturing contextual dependencies.\nTo effec-",
          "convolutional networks and incorporating speaker": "mantic consistency and complementarity of these"
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "tively integrate information from different modali-",
          "convolutional networks and incorporating speaker": "signals in a self-supervised manner,\nthereby im-"
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "ties, Hu et al. (2021b) constructed a graph structure",
          "convolutional networks and incorporating speaker": "proving the model’s ability to recognize true emo-"
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "that fuses multimodal features, enabling the model",
          "convolutional networks and incorporating speaker": "tional states."
        },
        {
          "• Fusion-based:\nMethods\nare\ngrouped\nunder": "to capture inter-modal dependencies through graph",
          "convolutional networks and incorporating speaker": "Graph-based methods effectively capture long-"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "range dependencies and speaker\ninteractions by": "modeling utterances\nas nodes\nand relations\nas",
          "tures based on contextual information. Inspired by": "hierarchical modality interaction, Ma et al. (2024a)"
        },
        {
          "range dependencies and speaker\ninteractions by": "edges.\nIn traditional NLP tasks,\nthey are partic-",
          "tures based on contextual information. Inspired by": "introduced a hierarchical gating (Ma et al., 2019)"
        },
        {
          "range dependencies and speaker\ninteractions by": "ularly effective for structured inputs such as token-",
          "tures based on contextual information. Inspired by": "fusion strategy into the Transformer architecture"
        },
        {
          "range dependencies and speaker\ninteractions by": "level representations (Zhang et al., 2023, 2024b).",
          "tures based on contextual information. Inspired by": "to enable fine-grained modality interaction and de-"
        },
        {
          "range dependencies and speaker\ninteractions by": "However,\nintegrating heterogeneous multimodal",
          "tures based on contextual information. Inspired by": "signs self-distillation (Zhang et al., 2019) to further"
        },
        {
          "range dependencies and speaker\ninteractions by": "signals into graph structures remains challenging,",
          "tures based on contextual information. Inspired by": "learn better modality representations."
        },
        {
          "range dependencies and speaker\ninteractions by": "as naïve connections may introduce noise without",
          "tures based on contextual information. Inspired by": "Text-Dominant Modality. Some methods pro-"
        },
        {
          "range dependencies and speaker\ninteractions by": "proper modality alignment.",
          "tures based on contextual information. Inspired by": "pose models based on primary–auxiliary modality"
        },
        {
          "range dependencies and speaker\ninteractions by": "",
          "tures based on contextual information. Inspired by": "collaboration, where auxiliary modalities are used"
        },
        {
          "range dependencies and speaker\ninteractions by": "5.2\nFusion-based Methods",
          "tures based on contextual information. Inspired by": ""
        },
        {
          "range dependencies and speaker\ninteractions by": "",
          "tures based on contextual information. Inspired by": "to enhance the performance of\nthe primary (tex-"
        },
        {
          "range dependencies and speaker\ninteractions by": "In MERC, effective fusion of heterogeneous mul-",
          "tures based on contextual information. Inspired by": "tual) modality. Huang et al.\n(2024b) suggested"
        },
        {
          "range dependencies and speaker\ninteractions by": "timodal\nfeatures\nis crucial but challenging due",
          "tures based on contextual information. Inspired by": "that enhancing a text-dominant model with auxil-"
        },
        {
          "range dependencies and speaker\ninteractions by": "to noise introduced during interaction modeling.",
          "tures based on contextual information. Inspired by": "iary modalities can improve performance.\nZou"
        },
        {
          "range dependencies and speaker\ninteractions by": "The advancements of\nthe Transformers architec-",
          "tures based on contextual information. Inspired by": "et al.\n(2022) employed a Transformer architec-"
        },
        {
          "range dependencies and speaker\ninteractions by": "ture (Vaswani et al., 2017), with its self-attention",
          "tures based on contextual information. Inspired by": "ture to design cross-modal attention for learning"
        },
        {
          "range dependencies and speaker\ninteractions by": "mechanism, promote advancements of the MERC",
          "tures based on contextual information. Inspired by": "fusion relationships between different modalities,"
        },
        {
          "range dependencies and speaker\ninteractions by": "methods for capturing cross-modal and contextual",
          "tures based on contextual information. Inspired by": "preserving the integrity of the primary modality’s"
        },
        {
          "range dependencies and speaker\ninteractions by": "dependencies.\nTo enhance cross-modal\ninterac-",
          "tures based on contextual information. Inspired by": "features while\nenhancing the\nrepresentation of"
        },
        {
          "range dependencies and speaker\ninteractions by": "tions, recent methods build on Transformers with",
          "tures based on contextual information. Inspired by": "weaker modality features. It also uses a two-stage"
        },
        {
          "range dependencies and speaker\ninteractions by": "tailored fusion strategies. We refer to these meth-",
          "tures based on contextual information. Inspired by": "emotional cue extractor to extract emotional evi-"
        },
        {
          "range dependencies and speaker\ninteractions by": "ods as fusion-based and illustrate them in Figure",
          "tures based on contextual information. Inspired by": "dence. Building on this, Zou et al. (2023) proposed"
        },
        {
          "range dependencies and speaker\ninteractions by": "4.\nSome approaches promote equal\ninteraction",
          "tures based on contextual information. Inspired by": "using weaker modalities as multimodal prompts"
        },
        {
          "range dependencies and speaker\ninteractions by": "among modalities to improve robustness, while",
          "tures based on contextual information. Inspired by": "while performing deep emotional cue extraction for"
        },
        {
          "range dependencies and speaker\ninteractions by": "others adopt a primary-auxiliary scheme, typically",
          "tures based on contextual information. Inspired by": "stronger modalities. The cue information is embed-"
        },
        {
          "range dependencies and speaker\ninteractions by": "using text as the core, with other modalities provid-",
          "tures based on contextual information. Inspired by": "ded into various attention layers of the Transformer"
        },
        {
          "range dependencies and speaker\ninteractions by": "ing complementary signals.",
          "tures based on contextual information. Inspired by": "to facilitate the fusion of information between the"
        },
        {
          "range dependencies and speaker\ninteractions by": "",
          "tures based on contextual information. Inspired by": "primary and auxiliary modalities. Zhu et al. (2024)"
        },
        {
          "range dependencies and speaker\ninteractions by": "(a) Equal Modality Weights\n(b) Text-Dominant Modality",
          "tures based on contextual information. Inspired by": ""
        },
        {
          "range dependencies and speaker\ninteractions by": "",
          "tures based on contextual information. Inspired by": "introduced an asymmetric CMA-Transformer mod-"
        },
        {
          "range dependencies and speaker\ninteractions by": "text",
          "tures based on contextual information. Inspired by": ""
        },
        {
          "range dependencies and speaker\ninteractions by": "audio\ntext\nvisual",
          "tures based on contextual information. Inspired by": "ule for central and auxiliary modalities to obtain"
        },
        {
          "range dependencies and speaker\ninteractions by": "Contextual Memory \nGraph Module",
          "tures based on contextual information. Inspired by": ""
        },
        {
          "range dependencies and speaker\ninteractions by": "Network\nAttention Module\nLSTM",
          "tures based on contextual information. Inspired by": "fused modality information and proposes a hierar-"
        },
        {
          "range dependencies and speaker\ninteractions by": "Intra-Modal \nGRU",
          "tures based on contextual information. Inspired by": ""
        },
        {
          "range dependencies and speaker\ninteractions by": "Interation",
          "tures based on contextual information. Inspired by": "chical distillation (Yang et al., 2021) framework to"
        },
        {
          "range dependencies and speaker\ninteractions by": "Transformer\n+\n…………….",
          "tures based on contextual information. Inspired by": ""
        },
        {
          "range dependencies and speaker\ninteractions by": "Inter-Modal",
          "tures based on contextual information. Inspired by": ""
        },
        {
          "range dependencies and speaker\ninteractions by": "Interation\ntva\nAttention \na\nv",
          "tures based on contextual information. Inspired by": "perform coarse- and fine-grained distillation. This"
        },
        {
          "range dependencies and speaker\ninteractions by": "t\nvisual\naudio",
          "tures based on contextual information. Inspired by": ""
        },
        {
          "range dependencies and speaker\ninteractions by": "Module",
          "tures based on contextual information. Inspired by": ""
        },
        {
          "range dependencies and speaker\ninteractions by": "Classifier",
          "tures based on contextual information. Inspired by": "approach ensures the consistency of modality fu-"
        },
        {
          "range dependencies and speaker\ninteractions by": "Classifier",
          "tures based on contextual information. Inspired by": ""
        },
        {
          "range dependencies and speaker\ninteractions by": "student\nteacher\nOutput",
          "tures based on contextual information. Inspired by": "sion information at different granularities."
        },
        {
          "range dependencies and speaker\ninteractions by": "Distillation \nOutput",
          "tures based on contextual information. Inspired by": ""
        },
        {
          "range dependencies and speaker\ninteractions by": "",
          "tures based on contextual information. Inspired by": "Fusion-based methods focus on learning cross-"
        },
        {
          "range dependencies and speaker\ninteractions by": "Figure 4: Development of Fusion-based Methods.",
          "tures based on contextual information. Inspired by": "modal interactions through attention mechanisms,"
        },
        {
          "range dependencies and speaker\ninteractions by": "",
          "tures based on contextual information. Inspired by": "with Transformer-based models achieving strong"
        },
        {
          "range dependencies and speaker\ninteractions by": "Equal Modality Weights. Equal interaction can",
          "tures based on contextual information. Inspired by": "generalization. These approaches are efficient for"
        },
        {
          "range dependencies and speaker\ninteractions by": "fully utilize information from various modalities,",
          "tures based on contextual information. Inspired by": "tasks with well-aligned modality inputs but often"
        },
        {
          "range dependencies and speaker\ninteractions by": "preventing over-reliance on a single modality. Li",
          "tures based on contextual information. Inspired by": "overlook dialogue-level structures such as speaker"
        },
        {
          "range dependencies and speaker\ninteractions by": "et al. (2022) proposed achieving emotion recogni-",
          "tures based on contextual information. Inspired by": "dependencies. Compared to graph-based models,"
        },
        {
          "range dependencies and speaker\ninteractions by": "tion by equally integrating emotional vectors and",
          "tures based on contextual information. Inspired by": "they emphasize modality-level\nfusion over\nrela-"
        },
        {
          "range dependencies and speaker\ninteractions by": "sentence vectors from different modalities to form",
          "tures based on contextual information. Inspired by": "tional reasoning."
        },
        {
          "range dependencies and speaker\ninteractions by": "emotion capsules. Zhang and Li (2023) designed a",
          "tures based on contextual information. Inspired by": ""
        },
        {
          "range dependencies and speaker\ninteractions by": "",
          "tures based on contextual information. Inspired by": "5.3\nGeneration-based Methods"
        },
        {
          "range dependencies and speaker\ninteractions by": "local constraint module for modalities within the",
          "tures based on contextual information. Inspired by": ""
        },
        {
          "range dependencies and speaker\ninteractions by": "Transformer to promote modality interaction and",
          "tures based on contextual information. Inspired by": "In recent years, pretrained LLMs have achieved"
        },
        {
          "range dependencies and speaker\ninteractions by": "incorporates a semantic graph to address the lack",
          "tures based on contextual information. Inspired by": "remarkable success in natural language processing"
        },
        {
          "range dependencies and speaker\ninteractions by": "of semantic relationship information between utter-",
          "tures based on contextual information. Inspired by": "tasks (Chu et al., 2024), demonstrating strong emer-"
        },
        {
          "range dependencies and speaker\ninteractions by": "ances. Mao et al. (2021) constructed a hierarchi-",
          "tures based on contextual information. Inspired by": "gent capabilities (Wei et al., 2022). However, de-"
        },
        {
          "range dependencies and speaker\ninteractions by": "cal Transformer where each modality can flexibly",
          "tures based on contextual information. Inspired by": "spite their powerful general-purpose abilities, lever-"
        },
        {
          "range dependencies and speaker\ninteractions by": "switch between sequential and feedforward struc-",
          "tures based on contextual information. Inspired by": "aging their full potential in specific sub-tasks still"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "requires carefully crafted, high-quality prompts": "(Wei et al., 2021) to bridge the gap in reasoning",
          "enhancing its behavioral perception capability. The": "instruction tuning module improves the model’s"
        },
        {
          "requires carefully crafted, high-quality prompts": "capabilities.As shown in Figure 5, researchers have",
          "enhancing its behavioral perception capability. The": "emotion recognition performance by aligning and"
        },
        {
          "requires carefully crafted, high-quality prompts": "proposed various model\nimprovement strategies",
          "enhancing its behavioral perception capability. The": "fine-tuning the concatenated multimodal inputs."
        },
        {
          "requires carefully crafted, high-quality prompts": "to effectively integrate contextual and multimodal",
          "enhancing its behavioral perception capability. The": "Lightweight Multimodal Fusion and Adap-"
        },
        {
          "requires carefully crafted, high-quality prompts": "information into LLMs while addressing their sub-",
          "enhancing its behavioral perception capability. The": "tation. As LLMs become increasingly large,\nthe"
        },
        {
          "requires carefully crafted, high-quality prompts": "stantial computational resource demands.",
          "enhancing its behavioral perception capability. The": "computational costs for ERC also rise significantly"
        },
        {
          "requires carefully crafted, high-quality prompts": "",
          "enhancing its behavioral perception capability. The": "(Zhang et al., 2025). Inspired by domain-specific"
        },
        {
          "requires carefully crafted, high-quality prompts": "(a) Instruction-Tuned with \n(b) Behavior-Aware and \n(c) Lightweight Multimodal",
          "enhancing its behavioral perception capability. The": ""
        },
        {
          "requires carefully crafted, high-quality prompts": "Speaker and Context Modeling\nMultimodal Instruction-Finetuned\nFusion and Adaptation",
          "enhancing its behavioral perception capability. The": "LLM paradigms tailored for affective computing"
        },
        {
          "requires carefully crafted, high-quality prompts": "audio\nvisual\ntext",
          "enhancing its behavioral perception capability. The": ""
        },
        {
          "requires carefully crafted, high-quality prompts": "audio\ntext\nvisual\ntext",
          "enhancing its behavioral perception capability. The": "(Hu et al., 2022; Li et al., 2023c; Tanioka et al.,"
        },
        {
          "requires carefully crafted, high-quality prompts": "Auxiliary task\nMain task",
          "enhancing its behavioral perception capability. The": ""
        },
        {
          "requires carefully crafted, high-quality prompts": "Behavior-Aware Module\nText Embedder\nText Embedder\nPlugin Module\n(1) Instruction\nSpeaker Identity",
          "enhancing its behavioral perception capability. The": ""
        },
        {
          "requires carefully crafted, high-quality prompts": "Facial Expression\nSpeaker Background\n(2) History Content\nVisual Adapter\nPosture\nEmbedding\nEmbedding",
          "enhancing its behavioral perception capability. The": "2024), MSE-Adapter (Yang et al., 2025) proposed a"
        },
        {
          "requires carefully crafted, high-quality prompts": "Body Language\nAudio Adapter\nCase Retrieve\n(3) Emotional Label",
          "enhancing its behavioral perception capability. The": ""
        },
        {
          "requires carefully crafted, high-quality prompts": "…………….\nEmotional Influence",
          "enhancing its behavioral perception capability. The": ""
        },
        {
          "requires carefully crafted, high-quality prompts": "Predict: utterance\n…………….",
          "enhancing its behavioral perception capability. The": "lightweight and adaptable plug-in architecture with"
        },
        {
          "requires carefully crafted, high-quality prompts": "Feature Fusion\nMulti-Stage Training",
          "enhancing its behavioral perception capability. The": ""
        },
        {
          "requires carefully crafted, high-quality prompts": "Step n\nStep 1…(n-1)\nAlignment Module\nStep 1",
          "enhancing its behavioral perception capability. The": ""
        },
        {
          "requires carefully crafted, high-quality prompts": "(1) Prompt\nStep 2\nLLM\nLLM\nLLM",
          "enhancing its behavioral perception capability. The": "two modules: TGM, for aligning textual and non-"
        },
        {
          "requires carefully crafted, high-quality prompts": "(2) Multimodal Feature",
          "enhancing its behavioral perception capability. The": ""
        },
        {
          "requires carefully crafted, high-quality prompts": "Output\nOutput",
          "enhancing its behavioral perception capability. The": "textual\nfeatures, and MSF,\nfor multi-scale cross-"
        },
        {
          "requires carefully crafted, high-quality prompts": "Output",
          "enhancing its behavioral perception capability. The": ""
        },
        {
          "requires carefully crafted, high-quality prompts": "",
          "enhancing its behavioral perception capability. The": "modal fusion. Built on a frozen LLM backbone"
        },
        {
          "requires carefully crafted, high-quality prompts": "Figure 5: Development of Generation-based Methods.",
          "enhancing its behavioral perception capability. The": "and trained via backpropagation,\nit enables effi-"
        },
        {
          "requires carefully crafted, high-quality prompts": "",
          "enhancing its behavioral perception capability. The": "cient and multimodally-aware ERC with minimal"
        },
        {
          "requires carefully crafted, high-quality prompts": "Instruction-Tuned with Speaker and Context",
          "enhancing its behavioral perception capability. The": "computational cost.\nSimilarly, SpeechCueLLM"
        },
        {
          "requires carefully crafted, high-quality prompts": "Modeling. ERC tasks have predominantly relied",
          "enhancing its behavioral perception capability. The": "(Wu et al., 2025) introduced a lightweight plug-in"
        },
        {
          "requires carefully crafted, high-quality prompts": "on discriminative modeling frameworks. With the",
          "enhancing its behavioral perception capability. The": "that converts speech features into natural language"
        },
        {
          "requires carefully crafted, high-quality prompts": "emergence of LLMs, InstructERC (Lei et al., 2023)",
          "enhancing its behavioral perception capability. The": "prompts, enabling LLMs to perform multimodal"
        },
        {
          "requires carefully crafted, high-quality prompts": "was the first to propose a generative framework for",
          "enhancing its behavioral perception capability. The": "emotion recognition without architectural changes."
        },
        {
          "requires carefully crafted, high-quality prompts": "ERC. It introduces a simple yet effective retrieval-",
          "enhancing its behavioral perception capability. The": ""
        },
        {
          "requires carefully crafted, high-quality prompts": "",
          "enhancing its behavioral perception capability. The": "Generation-based methods leverage LLMs to re-"
        },
        {
          "requires carefully crafted, high-quality prompts": "based prompting module that helps LLMs explic-",
          "enhancing its behavioral perception capability. The": ""
        },
        {
          "requires carefully crafted, high-quality prompts": "",
          "enhancing its behavioral perception capability. The": "formulate ERC as a text generation task, enabling"
        },
        {
          "requires carefully crafted, high-quality prompts": "itly integrate multi-granularity supervisory signals",
          "enhancing its behavioral perception capability. The": ""
        },
        {
          "requires carefully crafted, high-quality prompts": "",
          "enhancing its behavioral perception capability. The": "flexible adaptation across datasets and domains."
        },
        {
          "requires carefully crafted, high-quality prompts": "from dialogues. Additionally,\nit\nincorporates an",
          "enhancing its behavioral perception capability. The": ""
        },
        {
          "requires carefully crafted, high-quality prompts": "",
          "enhancing its behavioral perception capability. The": "Their end-to-end nature simplifies input processing"
        },
        {
          "requires carefully crafted, high-quality prompts": "auxiliary emotion alignment task to better model",
          "enhancing its behavioral perception capability. The": ""
        },
        {
          "requires carefully crafted, high-quality prompts": "",
          "enhancing its behavioral perception capability. The": "but limits fine-grained control over multimodal in-"
        },
        {
          "requires carefully crafted, high-quality prompts": "the complex emotional\ntransitions between inter-",
          "enhancing its behavioral perception capability. The": ""
        },
        {
          "requires carefully crafted, high-quality prompts": "",
          "enhancing its behavioral perception capability. The": "tegration. In contrast to structured models, LLMs"
        },
        {
          "requires carefully crafted, high-quality prompts": "locutors\nin conversations.\nInspired by the inte-",
          "enhancing its behavioral perception capability. The": ""
        },
        {
          "requires carefully crafted, high-quality prompts": "",
          "enhancing its behavioral perception capability. The": "excel at scalability but require further refinement"
        },
        {
          "requires carefully crafted, high-quality prompts": "gration of commonsense knowledge in COSMIC",
          "enhancing its behavioral perception capability. The": ""
        },
        {
          "requires carefully crafted, high-quality prompts": "",
          "enhancing its behavioral perception capability. The": "to model multimodal dependencies explicitly."
        },
        {
          "requires carefully crafted, high-quality prompts": "(Ghosal et al., 2020), recent work (Fu, 2024; Fu",
          "enhancing its behavioral perception capability. The": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "consuming;\nsome\nlarge dialogue datasets\n(e.g.,": "M3ED, Zhao et al., 2022) are still monolingual and",
          "them at the utterance level. Emotions also depend": "on the context of preceding and subsequent con-"
        },
        {
          "consuming;\nsome\nlarge dialogue datasets\n(e.g.,": "domain-bound. These limitations directly conflict",
          "them at the utterance level. Emotions also depend": "versation turns, so the model must capture tempo-"
        },
        {
          "consuming;\nsome\nlarge dialogue datasets\n(e.g.,": "with the FAIR principles. Some ERC datasets lack",
          "them at the utterance level. Emotions also depend": "ral dynamics (Wang et al., 2024). Previous stud-"
        },
        {
          "consuming;\nsome\nlarge dialogue datasets\n(e.g.,": "rich metadata or persistent identifiers, undermining",
          "them at the utterance level. Emotions also depend": "ies have used recurrent or self-attention layers to"
        },
        {
          "consuming;\nsome\nlarge dialogue datasets\n(e.g.,": "findability and interoperability. Others are subject",
          "them at the utterance level. Emotions also depend": "model sequential context (Houssein et al., 2024;"
        },
        {
          "consuming;\nsome\nlarge dialogue datasets\n(e.g.,": "to access restrictions or copyright constraints, while",
          "them at the utterance level. Emotions also depend": "Dutta et al., 2024), but\nlong-range dependencies"
        },
        {
          "consuming;\nsome\nlarge dialogue datasets\n(e.g.,": "many adopt inconsistent labeling schemes that hin-",
          "them at the utterance level. Emotions also depend": "remain challenging to learn. How to balance and in-"
        },
        {
          "consuming;\nsome\nlarge dialogue datasets\n(e.g.,": "der\nreusability. Consequently,\nresearchers often",
          "them at the utterance level. Emotions also depend": "tegrate contextual sentiment cue features with mul-"
        },
        {
          "consuming;\nsome\nlarge dialogue datasets\n(e.g.,": "have to train on small or biased samples, which",
          "them at the utterance level. Emotions also depend": "timodal\nfusion features in decision-making, and"
        },
        {
          "consuming;\nsome\nlarge dialogue datasets\n(e.g.,": "undermines generalization and the reuse of mod-",
          "them at the utterance level. Emotions also depend": "how to determine which fusion strategies are most"
        },
        {
          "consuming;\nsome\nlarge dialogue datasets\n(e.g.,": "els.\nTo address these issues,\nfuture work could",
          "them at the utterance level. Emotions also depend": "effective across different modalities, remain open"
        },
        {
          "consuming;\nsome\nlarge dialogue datasets\n(e.g.,": "prioritize the development of multilingual bench-",
          "them at the utterance level. Emotions also depend": "and important research questions (A.V. et al., 2024;"
        },
        {
          "consuming;\nsome\nlarge dialogue datasets\n(e.g.,": "mark datasets with standardized metadata and open",
          "them at the utterance level. Emotions also depend": "Ramaswamy and Palaniswamy, 2024). Recent ad-"
        },
        {
          "consuming;\nsome\nlarge dialogue datasets\n(e.g.,": "licensing, possibly through collaborative consor-",
          "them at the utterance level. Emotions also depend": "vances in adaptive fusion strategies and dynamic"
        },
        {
          "consuming;\nsome\nlarge dialogue datasets\n(e.g.,": "tiums that align with FAIR principles.",
          "them at the utterance level. Emotions also depend": "attention mechanisms show promise, and future"
        },
        {
          "consuming;\nsome\nlarge dialogue datasets\n(e.g.,": "Low-Resource, Multilingual, and Multicul-",
          "them at the utterance level. Emotions also depend": "methods could explore transformer-based fusion"
        },
        {
          "consuming;\nsome\nlarge dialogue datasets\n(e.g.,": "tural Settings. As described in the previous para-",
          "them at the utterance level. Emotions also depend": "that dynamically reweights modalities based on"
        },
        {
          "consuming;\nsome\nlarge dialogue datasets\n(e.g.,": "graph, most\nstate-of-the-art MERC systems are",
          "them at the utterance level. Emotions also depend": "conversational context."
        },
        {
          "consuming;\nsome\nlarge dialogue datasets\n(e.g.,": "trained on English-language datasets, which lim-",
          "them at the utterance level. Emotions also depend": "Cross-Modal Alignment,\nNoise Modality,"
        },
        {
          "consuming;\nsome\nlarge dialogue datasets\n(e.g.,": "its their global applicability. Although building a",
          "them at the utterance level. Emotions also depend": "Missing Modality, and Modality Conflicts. Mis-"
        },
        {
          "consuming;\nsome\nlarge dialogue datasets\n(e.g.,": "large-scale, diverse MER corpus is essential, it re-",
          "them at the utterance level. Emotions also depend": "aligned or inconsistent features can inhibit the abil-"
        },
        {
          "consuming;\nsome\nlarge dialogue datasets\n(e.g.,": "mains an obvious challenge as it requires expert an-",
          "them at the utterance level. Emotions also depend": "ity of a model to fully utilize multimodal signals, af-"
        },
        {
          "consuming;\nsome\nlarge dialogue datasets\n(e.g.,": "notation of data. The limited annotated data forces",
          "them at the utterance level. Emotions also depend": "fecting its robustness and generalization (Ma et al.,"
        },
        {
          "consuming;\nsome\nlarge dialogue datasets\n(e.g.,": "researchers to rely on transfer learning (Ananthram",
          "them at the utterance level. Emotions also depend": "2024b; Li and Tang, 2024). Noise modality, miss-"
        },
        {
          "consuming;\nsome\nlarge dialogue datasets\n(e.g.,": "et al., 2020), zero-shot (Qi et al., 2021), or few-shot",
          "them at the utterance level. Emotions also depend": "ing modality, or imbalanced modality distributions"
        },
        {
          "consuming;\nsome\nlarge dialogue datasets\n(e.g.,": "methods (Yang et al., 2023). However, data scarcity",
          "them at the utterance level. Emotions also depend": "may bias simple fusion strategies (Mai et al., 2024;"
        },
        {
          "consuming;\nsome\nlarge dialogue datasets\n(e.g.,": "and the high cost of emotion annotation continue",
          "them at the utterance level. Emotions also depend": "Zhang et al., 2024a). Even when all modalities are"
        },
        {
          "consuming;\nsome\nlarge dialogue datasets\n(e.g.,": "to be major obstacles for MER in low-resource",
          "them at the utterance level. Emotions also depend": "available, they may convey conflicting emotional"
        },
        {
          "consuming;\nsome\nlarge dialogue datasets\n(e.g.,": "domains (Hussain et al., 2025). Emotions are ex-",
          "them at the utterance level. Emotions also depend": "signals, further complicating fusion and decision-"
        },
        {
          "consuming;\nsome\nlarge dialogue datasets\n(e.g.,": "pressed differently across languages and cultures,",
          "them at the utterance level. Emotions also depend": "making.\nPerceiving the uncertainty of different"
        },
        {
          "consuming;\nsome\nlarge dialogue datasets\n(e.g.,": "further compounding the challenges of MER. Vari-",
          "them at the utterance level. Emotions also depend": "modalities for feature enhancement and resolving"
        },
        {
          "consuming;\nsome\nlarge dialogue datasets\n(e.g.,": "ations in emotional expression and interpretation",
          "them at the utterance level. Emotions also depend": "conflicts between modality features are important"
        },
        {
          "consuming;\nsome\nlarge dialogue datasets\n(e.g.,": "due to cultural differences can lead to inconsisten-",
          "them at the utterance level. Emotions also depend": "areas that need further exploration in MER research."
        },
        {
          "consuming;\nsome\nlarge dialogue datasets\n(e.g.,": "cies in labeling. Most existing corpora are culture-",
          "them at the utterance level. Emotions also depend": "Therefore, exploring cross-modal transfer and fu-"
        },
        {
          "consuming;\nsome\nlarge dialogue datasets\n(e.g.,": "specific, limiting their generalizability. Although",
          "them at the utterance level. Emotions also depend": "sion to improve generalization in ERC has attracted"
        },
        {
          "consuming;\nsome\nlarge dialogue datasets\n(e.g.,": "researchers have acknowledged this challenge (A.",
          "them at the utterance level. Emotions also depend": "the attention of more and more researchers (Fan"
        },
        {
          "consuming;\nsome\nlarge dialogue datasets\n(e.g.,": "and V., 2024; Ghaayathri Devi et al., 2024), MER",
          "them at the utterance level. Emotions also depend": "et al., 2024; Li et al., 2024a; Feng and Fan, 2025)."
        },
        {
          "consuming;\nsome\nlarge dialogue datasets\n(e.g.,": "systems aiming for global applicability must ac-",
          "them at the utterance level. Emotions also depend": "Some ERC methods incorporate variants of cross-"
        },
        {
          "consuming;\nsome\nlarge dialogue datasets\n(e.g.,": "count\nfor both linguistic diversity and culturally",
          "them at the utterance level. Emotions also depend": "modal attention (Guo et al., 2024; N. and Patil,"
        },
        {
          "consuming;\nsome\nlarge dialogue datasets\n(e.g.,": "driven display rules. Future work could explore",
          "them at the utterance level. Emotions also depend": "2020), graph-based fusion (Li et al., 2023a; Hu"
        },
        {
          "consuming;\nsome\nlarge dialogue datasets\n(e.g.,": "culture-adaptive pretraining and cross-lingual trans-",
          "them at the utterance level. Emotions also depend": "et al., 2021b), or mutual learning to align features"
        },
        {
          "consuming;\nsome\nlarge dialogue datasets\n(e.g.,": "fer learning methods that embed culturally sensitive",
          "them at the utterance level. Emotions also depend": "during training and improve cross-domain perfor-"
        },
        {
          "consuming;\nsome\nlarge dialogue datasets\n(e.g.,": "emotion semantics across languages.",
          "them at the utterance level. Emotions also depend": "mance (Lian et al., 2021). Future research can fur-"
        },
        {
          "consuming;\nsome\nlarge dialogue datasets\n(e.g.,": "Complexities\nof\nFusion\nStrategies\nacross",
          "them at the utterance level. Emotions also depend": "ther investigate robust training frameworks that in-"
        },
        {
          "consuming;\nsome\nlarge dialogue datasets\n(e.g.,": "Modalities. Multimodal fusion techniques include",
          "them at the utterance level. Emotions also depend": "clude modality dropout, uncertainty-aware fusion,"
        },
        {
          "consuming;\nsome\nlarge dialogue datasets\n(e.g.,": "early fusion, mid-level fusion, late fusion, hybrid",
          "them at the utterance level. Emotions also depend": "or reinforcement learning to selectively attend to"
        },
        {
          "consuming;\nsome\nlarge dialogue datasets\n(e.g.,": "fusion, and others (Atrey et al., 2010; Gandhi et al.,",
          "them at the utterance level. Emotions also depend": "trustworthy modalities."
        },
        {
          "consuming;\nsome\nlarge dialogue datasets\n(e.g.,": "2023). A key challenge is that conversational sig-",
          "them at the utterance level. Emotions also depend": "Effective Modality Selection.\nMultimodal"
        },
        {
          "consuming;\nsome\nlarge dialogue datasets\n(e.g.,": "nals, such as voice,\nfacial expressions, and tran-",
          "them at the utterance level. Emotions also depend": "learning refers to the integration of\ninformation"
        },
        {
          "consuming;\nsome\nlarge dialogue datasets\n(e.g.,": "scripts, are inherently asynchronous and occur at",
          "them at the utterance level. Emotions also depend": "from various heterogeneous sources and aims to ef-"
        },
        {
          "consuming;\nsome\nlarge dialogue datasets\n(e.g.,": "different\ntime scales, making it difficult\nto align",
          "them at the utterance level. Emotions also depend": "fectively leverage data from diverse modalities (He"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "et al., 2024; Tsai et al., 2024). In multimodal rep-": "resentation learning, not all modalities contribute",
          "monitoring and feedback on learners’ emotional": "states. These studies highlight key directions for"
        },
        {
          "et al., 2024; Tsai et al., 2024). In multimodal rep-": "equally to the task. Some modalities may introduce",
          "monitoring and feedback on learners’ emotional": "advancing MERC systems, particularly to make"
        },
        {
          "et al., 2024; Tsai et al., 2024). In multimodal rep-": "noise and need to be removed, while others may",
          "monitoring and feedback on learners’ emotional": "them more robust and context-aware. The ongoing"
        },
        {
          "et al., 2024; Tsai et al., 2024). In multimodal rep-": "not be essential for the task at hand but could be",
          "monitoring and feedback on learners’ emotional": "research should continue to focus on real-world de-"
        },
        {
          "et al., 2024; Tsai et al., 2024). In multimodal rep-": "indispensable for other subtasks. Existing research",
          "monitoring and feedback on learners’ emotional": "ployment scenarios. Future MERC systems could"
        },
        {
          "et al., 2024; Tsai et al., 2024). In multimodal rep-": "proposes modality selection algorithms that iden-",
          "monitoring and feedback on learners’ emotional": "benefit from incremental learning techniques and"
        },
        {
          "et al., 2024; Tsai et al., 2024). In multimodal rep-": "tify the contribution of each modality (Marinov",
          "monitoring and feedback on learners’ emotional": "user-in-the-loop feedback mechanisms that allow"
        },
        {
          "et al., 2024; Tsai et al., 2024). In multimodal rep-": "et al., 2023; Mai et al., 2024). However, select-",
          "monitoring and feedback on learners’ emotional": "adaptation in dynamic, real-time environments."
        },
        {
          "et al., 2024; Tsai et al., 2024). In multimodal rep-": "ing the most appropriate subset of modalities for",
          "monitoring and feedback on learners’ emotional": "Expanding the Modality Space in Emotion"
        },
        {
          "et al., 2024; Tsai et al., 2024). In multimodal rep-": "a task remains one of the key challenges in multi-",
          "monitoring and feedback on learners’ emotional": "Recognition. Current MER systems mainly rely"
        },
        {
          "et al., 2024; Tsai et al., 2024). In multimodal rep-": "modal learning. An emerging direction is to inte-",
          "monitoring and feedback on learners’ emotional": "on vision, audio, and text, given their accessibil-"
        },
        {
          "et al., 2024; Tsai et al., 2024). In multimodal rep-": "grate learnable modality gates or sparsity-inducing",
          "monitoring and feedback on learners’ emotional": "ity and prevalence in datasets. Yet human emo-"
        },
        {
          "et al., 2024; Tsai et al., 2024). In multimodal rep-": "regularization into fusion models to automatically",
          "monitoring and feedback on learners’ emotional": "tion is expressed through additional channels such"
        },
        {
          "et al., 2024; Tsai et al., 2024). In multimodal rep-": "suppress uninformative modalities during training.",
          "monitoring and feedback on learners’ emotional": "as gaze, gestures, posture, and physiological sig-"
        },
        {
          "et al., 2024; Tsai et al., 2024). In multimodal rep-": "Efficient Fine-tuning Approach Using Multi-",
          "monitoring and feedback on learners’ emotional": "nals (e.g., heart rate, skin conductance, brain activ-"
        },
        {
          "et al., 2024; Tsai et al., 2024). In multimodal rep-": "modal LLMs. Multimodal LLMs have brought",
          "monitoring and feedback on learners’ emotional": "ity) (Noroozi et al., 2021; Udahemuka et al., 2024;"
        },
        {
          "et al., 2024; Tsai et al., 2024). In multimodal rep-": "major\nadvances\nin enabling machines\nto learn",
          "monitoring and feedback on learners’ emotional": "Wang et al., 2023; He et al., 2020; Liu et al., 2024c)."
        },
        {
          "et al., 2024; Tsai et al., 2024). In multimodal rep-": "across modalities. Some models are increasingly",
          "monitoring and feedback on learners’ emotional": "These modalities remain underrepresented due to"
        },
        {
          "et al., 2024; Tsai et al., 2024). In multimodal rep-": "used in MERC, offering zero-shot or few-shot gen-",
          "monitoring and feedback on learners’ emotional": "challenges in collecting synchronized, high-quality"
        },
        {
          "et al., 2024; Tsai et al., 2024). In multimodal rep-": "eralization across different modalities (Li et al.,",
          "monitoring and feedback on learners’ emotional": "data and evolving annotation standards (Udahe-"
        },
        {
          "et al., 2024; Tsai et al., 2024). In multimodal rep-": "2024c; Yang et al., 2024; Bo-Hao et al., 2025). The",
          "monitoring and feedback on learners’ emotional": "muka et al., 2024; He et al., 2020; Kim and Hong,"
        },
        {
          "et al., 2024; Tsai et al., 2024). In multimodal rep-": "use of LLMs in MERC opens up new possibili-",
          "monitoring and feedback on learners’ emotional": "2024). Expanding beyond the traditional three can"
        },
        {
          "et al., 2024; Tsai et al., 2024). In multimodal rep-": "ties for capturing deeper semantic and conversa-",
          "monitoring and feedback on learners’ emotional": "reduce reliance on potentially misleading cues and"
        },
        {
          "et al., 2024; Tsai et al., 2024). In multimodal rep-": "tional cues beyond surface-level emotion signals.",
          "monitoring and feedback on learners’ emotional": "open new application domains. For instance, bio-"
        },
        {
          "et al., 2024; Tsai et al., 2024). In multimodal rep-": "However, efficiently fine-tuning these models for",
          "monitoring and feedback on learners’ emotional": "signals and contextual cues could enhance emo-"
        },
        {
          "et al., 2024; Tsai et al., 2024). In multimodal rep-": "emotion understanding still presents challenges,",
          "monitoring and feedback on learners’ emotional": "tion sensing in health and education, while wear-"
        },
        {
          "et al., 2024; Tsai et al., 2024). In multimodal rep-": "especially in low-resource and culturally diverse",
          "monitoring and feedback on learners’ emotional": "able sensors and eye-trackers may enable emotion-"
        },
        {
          "et al., 2024; Tsai et al., 2024). In multimodal rep-": "settings. Efficient adaptation of MLLMs to cap-",
          "monitoring and feedback on learners’ emotional": "aware experiences in VR, driver monitoring, or"
        },
        {
          "et al., 2024; Tsai et al., 2024). In multimodal rep-": "ture the nuance of emotions across diverse datasets,",
          "monitoring and feedback on learners’ emotional": "social\nrobotics.\nIn summary, gaze, physiologi-"
        },
        {
          "et al., 2024; Tsai et al., 2024). In multimodal rep-": "languages, or cross-cultural settings remains an",
          "monitoring and feedback on learners’ emotional": "cal, and other embodied modalities are promis-"
        },
        {
          "et al., 2024; Tsai et al., 2024). In multimodal rep-": "open research frontier. Promising future directions",
          "monitoring and feedback on learners’ emotional": "ing but underexplored in affective computing. Fu-"
        },
        {
          "et al., 2024; Tsai et al., 2024). In multimodal rep-": "include using adapter modules or\nlow-rank fine-",
          "monitoring and feedback on learners’ emotional": "ture work should explore how to systematically"
        },
        {
          "et al., 2024; Tsai et al., 2024). In multimodal rep-": "tuning techniques to adapt large models to specific",
          "monitoring and feedback on learners’ emotional": "integrate these diverse modalities into large-scale"
        },
        {
          "et al., 2024; Tsai et al., 2024). In multimodal rep-": "emotion tasks with minimal data.",
          "monitoring and feedback on learners’ emotional": "benchmarks and develop models capable of\nro-"
        },
        {
          "et al., 2024; Tsai et al., 2024). In multimodal rep-": "MERC Application. With the growing use of in-",
          "monitoring and feedback on learners’ emotional": "bustly leveraging them in real-world scenarios."
        },
        {
          "et al., 2024; Tsai et al., 2024). In multimodal rep-": "teractive machine applications, MERC has become",
          "monitoring and feedback on learners’ emotional": ""
        },
        {
          "et al., 2024; Tsai et al., 2024). In multimodal rep-": "a critical\nresearch area. Applications in human-",
          "monitoring and feedback on learners’ emotional": ""
        },
        {
          "et al., 2024; Tsai et al., 2024). In multimodal rep-": "",
          "monitoring and feedback on learners’ emotional": "7\nConclusion"
        },
        {
          "et al., 2024; Tsai et al., 2024). In multimodal rep-": "computer interaction (Ahmad et al., 2024; Moin",
          "monitoring and feedback on learners’ emotional": ""
        },
        {
          "et al., 2024; Tsai et al., 2024). In multimodal rep-": "et al., 2023), healthcare (Ayata et al., 2020; Islam",
          "monitoring and feedback on learners’ emotional": "MERC seeks to understand emotions by integrat-"
        },
        {
          "et al., 2024; Tsai et al., 2024). In multimodal rep-": "et al., 2024), education (Villegas-Ch et al., 2025;",
          "monitoring and feedback on learners’ emotional": "ing various modalities in to dialogue of linguistic,"
        },
        {
          "et al., 2024; Tsai et al., 2024). In multimodal rep-": "Vani and Jayashree, 2025), and virtual collabora-",
          "monitoring and feedback on learners’ emotional": "acoustic, visual signals, and beyond. While recent"
        },
        {
          "et al., 2024; Tsai et al., 2024). In multimodal rep-": "tion demand robust and adaptable emotion recog-",
          "monitoring and feedback on learners’ emotional": "progress has introduced diverse modeling strate-"
        },
        {
          "et al., 2024; Tsai et al., 2024). In multimodal rep-": "nition technologies that function effectively in nat-",
          "monitoring and feedback on learners’ emotional": "gies, significant challenges remain in data scarcity,"
        },
        {
          "et al., 2024; Tsai et al., 2024). In multimodal rep-": "uralistic and dynamic environments. Yang et al.",
          "monitoring and feedback on learners’ emotional": "modality alignment, and generalization across lan-"
        },
        {
          "et al., 2024; Tsai et al., 2024). In multimodal rep-": "(2022) investigated MER in contexts affected by",
          "monitoring and feedback on learners’ emotional": "guages and cultures."
        },
        {
          "et al., 2024; Tsai et al., 2024). In multimodal rep-": "face occlusions, such as those introduced by sur-",
          "monitoring and feedback on learners’ emotional": "This\nsurvey provides\na\nstructured review of"
        },
        {
          "et al., 2024; Tsai et al., 2024). In multimodal rep-": "gical and fabric masks. Khan et al. (2024) stud-",
          "monitoring and feedback on learners’ emotional": "the MERC landscape, compares representative ap-"
        },
        {
          "et al., 2024; Tsai et al., 2024). In multimodal rep-": "ied contactless techniques in MER, surveying a",
          "monitoring and feedback on learners’ emotional": "proaches, and highlights key open research prob-"
        },
        {
          "et al., 2024; Tsai et al., 2024). In multimodal rep-": "range of nonintrusive modalities (e.g., visual cues,",
          "monitoring and feedback on learners’ emotional": "lems. We hope it serves as a practical reference"
        },
        {
          "et al., 2024; Tsai et al., 2024). In multimodal rep-": "physiological signals). Huang (2024) developed a",
          "monitoring and feedback on learners’ emotional": "to support\nthe future development of\nrobust and"
        },
        {
          "et al., 2024; Tsai et al., 2024). In multimodal rep-": "MER system for online learning to enable real-time",
          "monitoring and feedback on learners’ emotional": "inclusive emotion recognition systems."
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "First, due to the focus on more advanced re-": "cent technologies, we provide only high-level sum-"
        },
        {
          "First, due to the focus on more advanced re-": "maries of representative methods, without delving"
        },
        {
          "First, due to the focus on more advanced re-": "into full technical details. Additionally, approaches"
        },
        {
          "First, due to the focus on more advanced re-": ""
        },
        {
          "First, due to the focus on more advanced re-": "developed prior to 2020 receive limited coverage,"
        },
        {
          "First, due to the focus on more advanced re-": ""
        },
        {
          "First, due to the focus on more advanced re-": "as our focus is primarily on recent trends that align"
        },
        {
          "First, due to the focus on more advanced re-": "with the rapid evolution of large-scale multimodal"
        },
        {
          "First, due to the focus on more advanced re-": "systems."
        },
        {
          "First, due to the focus on more advanced re-": ""
        },
        {
          "First, due to the focus on more advanced re-": "Second, our literature review is largely drawn"
        },
        {
          "First, due to the focus on more advanced re-": ""
        },
        {
          "First, due to the focus on more advanced re-": "from English-language publications in major con-"
        },
        {
          "First, due to the focus on more advanced re-": ""
        },
        {
          "First, due to the focus on more advanced re-": "ferences and repositories,\nincluding Interspeech,"
        },
        {
          "First, due to the focus on more advanced re-": ""
        },
        {
          "First, due to the focus on more advanced re-": "ICASSP, ACM, *ACL, ICML, AAAI, CVPR, COL-"
        },
        {
          "First, due to the focus on more advanced re-": "ING, and preprints on arXiv. While these venues"
        },
        {
          "First, due to the focus on more advanced re-": "represent core research communities in MERC, rel-"
        },
        {
          "First, due to the focus on more advanced re-": ""
        },
        {
          "First, due to the focus on more advanced re-": "evant contributions from other regions, languages,"
        },
        {
          "First, due to the focus on more advanced re-": ""
        },
        {
          "First, due to the focus on more advanced re-": "or domains may be underrepresented."
        },
        {
          "First, due to the focus on more advanced re-": ""
        },
        {
          "First, due to the focus on more advanced re-": "In the benchmark section, we highlight widely-"
        },
        {
          "First, due to the focus on more advanced re-": ""
        },
        {
          "First, due to the focus on more advanced re-": "used datasets, but do not aim for exhaustive com-"
        },
        {
          "First, due to the focus on more advanced re-": ""
        },
        {
          "First, due to the focus on more advanced re-": "parison. For more in-depth benchmarking, we refer"
        },
        {
          "First, due to the focus on more advanced re-": ""
        },
        {
          "First, due to the focus on more advanced re-": "readers to complementary surveys such as Zhao"
        },
        {
          "First, due to the focus on more advanced re-": ""
        },
        {
          "First, due to the focus on more advanced re-": "et al.\n(2022), Sasu et al.\n(2025) and Gan et al."
        },
        {
          "First, due to the focus on more advanced re-": ""
        },
        {
          "First, due to the focus on more advanced re-": "(2024)."
        },
        {
          "First, due to the focus on more advanced re-": "Finally, while we identify several open chal-"
        },
        {
          "First, due to the focus on more advanced re-": ""
        },
        {
          "First, due to the focus on more advanced re-": "lenges and underexplored directions, our discus-"
        },
        {
          "First, due to the focus on more advanced re-": "sion is not exhaustive. Rather than providing defini-"
        },
        {
          "First, due to the focus on more advanced re-": ""
        },
        {
          "First, due to the focus on more advanced re-": "tive answers, we aim to surface critical issues and"
        },
        {
          "First, due to the focus on more advanced re-": "foster further inquiry. We view these open ques-"
        },
        {
          "First, due to the focus on more advanced re-": ""
        },
        {
          "First, due to the focus on more advanced re-": "tions as productive entry points for future research"
        },
        {
          "First, due to the focus on more advanced re-": "and hope this survey supports ongoing efforts to"
        },
        {
          "First, due to the focus on more advanced re-": "develop more advanced MERC systems."
        },
        {
          "First, due to the focus on more advanced re-": ""
        },
        {
          "First, due to the focus on more advanced re-": ""
        },
        {
          "First, due to the focus on more advanced re-": "Acknowledgments"
        },
        {
          "First, due to the focus on more advanced re-": ""
        },
        {
          "First, due to the focus on more advanced re-": ""
        },
        {
          "First, due to the focus on more advanced re-": "The authors acknowledge the use of ChatGPT ex-"
        },
        {
          "First, due to the focus on more advanced re-": "clusively to refine the text for grammatical checks"
        },
        {
          "First, due to the focus on more advanced re-": ""
        },
        {
          "First, due to the focus on more advanced re-": "in the final manuscript. This work was supported in"
        },
        {
          "First, due to the focus on more advanced re-": ""
        },
        {
          "First, due to the focus on more advanced re-": "part by the Guangdong Basic and Applied Basic Re-"
        },
        {
          "First, due to the focus on more advanced re-": ""
        },
        {
          "First, due to the focus on more advanced re-": "search Foundation under Grant 2023A1515011370,"
        },
        {
          "First, due to the focus on more advanced re-": ""
        },
        {
          "First, due to the focus on more advanced re-": "the National Natural Science Foundation of China"
        },
        {
          "First, due to the focus on more advanced re-": ""
        },
        {
          "First, due to the focus on more advanced re-": "(32371114), the Characteristic Innovation Projects"
        },
        {
          "First, due to the focus on more advanced re-": ""
        },
        {
          "First, due to the focus on more advanced re-": "of Guangdong Colleges\nand Universities\n(No."
        },
        {
          "First, due to the focus on more advanced re-": ""
        },
        {
          "First, due to the focus on more advanced re-": "2018KTSCX049), and the Guangdong Provincial"
        },
        {
          "First, due to the focus on more advanced re-": "Key Laboratory [No.2023B1212060076]. YL ac-"
        },
        {
          "First, due to the focus on more advanced re-": ""
        },
        {
          "First, due to the focus on more advanced re-": "knowledges\nsupport\nfrom the Duke Power En-"
        },
        {
          "First, due to the focus on more advanced re-": ""
        },
        {
          "First, due to the focus on more advanced re-": "dowed Professorship at\nthe School of Business,"
        },
        {
          "First, due to the focus on more advanced re-": ""
        },
        {
          "First, due to the focus on more advanced re-": "North Carolina Central University, and the IBM-"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Limitations": "",
          "HBCU Quantum Center Faculty Fellowship. ZG": "is supported by the National Science Foundation"
        },
        {
          "Limitations": "This\nsurvey offers\na\nstructured and up-to-date",
          "HBCU Quantum Center Faculty Fellowship. ZG": ""
        },
        {
          "Limitations": "",
          "HBCU Quantum Center Faculty Fellowship. ZG": "via ARNI (The NSF AI Institute for Artificial and"
        },
        {
          "Limitations": "overview of MERC, with an emphasis on recent",
          "HBCU Quantum Center Faculty Fellowship. ZG": ""
        },
        {
          "Limitations": "",
          "HBCU Quantum Center Faculty Fellowship. ZG": "Natural\nIntelligence), under\nthe Columbia 2025"
        },
        {
          "Limitations": "deep learning-based approaches. However, several",
          "HBCU Quantum Center Faculty Fellowship. ZG": ""
        },
        {
          "Limitations": "",
          "HBCU Quantum Center Faculty Fellowship. ZG": "Research Project\n(\"Towards Safe, Robust,\nInter-"
        },
        {
          "Limitations": "limitations should be acknowledged to contextual-",
          "HBCU Quantum Center Faculty Fellowship. ZG": ""
        },
        {
          "Limitations": "",
          "HBCU Quantum Center Faculty Fellowship. ZG": "pretable Dialogue Agents for Democratized Medi-"
        },
        {
          "Limitations": "ize the scope of our work.",
          "HBCU Quantum Center Faculty Fellowship. ZG": ""
        },
        {
          "Limitations": "",
          "HBCU Quantum Center Faculty Fellowship. ZG": "cal Care\")."
        },
        {
          "Limitations": "First, due to the focus on more advanced re-",
          "HBCU Quantum Center Faculty Fellowship. ZG": ""
        },
        {
          "Limitations": "cent technologies, we provide only high-level sum-",
          "HBCU Quantum Center Faculty Fellowship. ZG": ""
        },
        {
          "Limitations": "maries of representative methods, without delving",
          "HBCU Quantum Center Faculty Fellowship. ZG": "References"
        },
        {
          "Limitations": "into full technical details. Additionally, approaches",
          "HBCU Quantum Center Faculty Fellowship. ZG": ""
        },
        {
          "Limitations": "",
          "HBCU Quantum Center Faculty Fellowship. ZG": "Aruna Gladys A. and Vetriselvi V. 2024. Sentiment anal-"
        },
        {
          "Limitations": "developed prior to 2020 receive limited coverage,",
          "HBCU Quantum Center Faculty Fellowship. ZG": ""
        },
        {
          "Limitations": "",
          "HBCU Quantum Center Faculty Fellowship. ZG": "ysis on a low-resource language dataset using multi-"
        },
        {
          "Limitations": "as our focus is primarily on recent trends that align",
          "HBCU Quantum Center Faculty Fellowship. ZG": "modal representation learning and cross-lingual trans-"
        },
        {
          "Limitations": "with the rapid evolution of large-scale multimodal",
          "HBCU Quantum Center Faculty Fellowship. ZG": "fer learning. Applied Soft Computing, 157:111553."
        },
        {
          "Limitations": "systems.",
          "HBCU Quantum Center Faculty Fellowship. ZG": ""
        },
        {
          "Limitations": "",
          "HBCU Quantum Center Faculty Fellowship. ZG": "Akram Ahmad, Vaishali Singh, and Kamal Upreti. 2024."
        },
        {
          "Limitations": "Second, our literature review is largely drawn",
          "HBCU Quantum Center Faculty Fellowship. ZG": ""
        },
        {
          "Limitations": "",
          "HBCU Quantum Center Faculty Fellowship. ZG": "A systematic study on unimodal and multimodal hu-"
        },
        {
          "Limitations": "from English-language publications in major con-",
          "HBCU Quantum Center Faculty Fellowship. ZG": "man computer interface for emotion recognition.\nIn"
        },
        {
          "Limitations": "",
          "HBCU Quantum Center Faculty Fellowship. ZG": "Computing, Internet of Things and Data Analytics,"
        },
        {
          "Limitations": "ferences and repositories,\nincluding Interspeech,",
          "HBCU Quantum Center Faculty Fellowship. ZG": ""
        },
        {
          "Limitations": "",
          "HBCU Quantum Center Faculty Fellowship. ZG": "pages 363–375, Cham. Springer Nature Switzerland."
        },
        {
          "Limitations": "ICASSP, ACM, *ACL, ICML, AAAI, CVPR, COL-",
          "HBCU Quantum Center Faculty Fellowship. ZG": ""
        },
        {
          "Limitations": "ING, and preprints on arXiv. While these venues",
          "HBCU Quantum Center Faculty Fellowship. ZG": "Wei Ai, Yuntao Shou, Tao Meng, and Keqin Li. 2025a."
        },
        {
          "Limitations": "represent core research communities in MERC, rel-",
          "HBCU Quantum Center Faculty Fellowship. ZG": "Der-gcn: Dialog and event relation-aware graph con-"
        },
        {
          "Limitations": "",
          "HBCU Quantum Center Faculty Fellowship. ZG": "volutional neural network for multimodal dialog emo-"
        },
        {
          "Limitations": "evant contributions from other regions, languages,",
          "HBCU Quantum Center Faculty Fellowship. ZG": ""
        },
        {
          "Limitations": "",
          "HBCU Quantum Center Faculty Fellowship. ZG": "IEEE Transactions on Neural Net-\ntion recognition."
        },
        {
          "Limitations": "or domains may be underrepresented.",
          "HBCU Quantum Center Faculty Fellowship. ZG": ""
        },
        {
          "Limitations": "",
          "HBCU Quantum Center Faculty Fellowship. ZG": "works and Learning Systems, 36(3):4908–4921."
        },
        {
          "Limitations": "In the benchmark section, we highlight widely-",
          "HBCU Quantum Center Faculty Fellowship. ZG": ""
        },
        {
          "Limitations": "",
          "HBCU Quantum Center Faculty Fellowship. ZG": "Wei Ai,\nFuchen Zhang, Yuntao Shou, Tao Meng,"
        },
        {
          "Limitations": "used datasets, but do not aim for exhaustive com-",
          "HBCU Quantum Center Faculty Fellowship. ZG": ""
        },
        {
          "Limitations": "",
          "HBCU Quantum Center Faculty Fellowship. ZG": "Haowen Chen, and Keqin Li. 2025b. Revisiting mul-"
        },
        {
          "Limitations": "parison. For more in-depth benchmarking, we refer",
          "HBCU Quantum Center Faculty Fellowship. ZG": ""
        },
        {
          "Limitations": "",
          "HBCU Quantum Center Faculty Fellowship. ZG": "timodal emotion recognition in conversation from"
        },
        {
          "Limitations": "readers to complementary surveys such as Zhao",
          "HBCU Quantum Center Faculty Fellowship. ZG": ""
        },
        {
          "Limitations": "",
          "HBCU Quantum Center Faculty Fellowship. ZG": "Proceedings\nthe perspective of graph spectrum."
        },
        {
          "Limitations": "et al.\n(2022), Sasu et al.\n(2025) and Gan et al.",
          "HBCU Quantum Center Faculty Fellowship. ZG": "of\nthe AAAI Conference on Artificial\nIntelligence,"
        },
        {
          "Limitations": "",
          "HBCU Quantum Center Faculty Fellowship. ZG": "39(11):11418–11426."
        },
        {
          "Limitations": "(2024).",
          "HBCU Quantum Center Faculty Fellowship. ZG": ""
        },
        {
          "Limitations": "Finally, while we identify several open chal-",
          "HBCU Quantum Center Faculty Fellowship. ZG": ""
        },
        {
          "Limitations": "",
          "HBCU Quantum Center Faculty Fellowship. ZG": "Amith Ananthram, Kailash Karthik Saravanakumar, Jes-"
        },
        {
          "Limitations": "lenges and underexplored directions, our discus-",
          "HBCU Quantum Center Faculty Fellowship. ZG": "sica Huynh,\nand Homayoon Beigi. 2020. Multi-"
        },
        {
          "Limitations": "sion is not exhaustive. Rather than providing defini-",
          "HBCU Quantum Center Faculty Fellowship. ZG": "modal\nemotion\ndetection with\ntransfer\nlearning."
        },
        {
          "Limitations": "",
          "HBCU Quantum Center Faculty Fellowship. ZG": "arXiv preprint arXiv:2011.07065."
        },
        {
          "Limitations": "tive answers, we aim to surface critical issues and",
          "HBCU Quantum Center Faculty Fellowship. ZG": ""
        },
        {
          "Limitations": "foster further inquiry. We view these open ques-",
          "HBCU Quantum Center Faculty Fellowship. ZG": ""
        },
        {
          "Limitations": "",
          "HBCU Quantum Center Faculty Fellowship. ZG": "A. Aruna Gladys and V. Vetriselvi. 2023.\nSurvey on"
        },
        {
          "Limitations": "tions as productive entry points for future research",
          "HBCU Quantum Center Faculty Fellowship. ZG": "multimodal approaches to emotion recognition. Neu-"
        },
        {
          "Limitations": "and hope this survey supports ongoing efforts to",
          "HBCU Quantum Center Faculty Fellowship. ZG": "rocomputing, 556:126693."
        },
        {
          "Limitations": "develop more advanced MERC systems.",
          "HBCU Quantum Center Faculty Fellowship. ZG": ""
        },
        {
          "Limitations": "",
          "HBCU Quantum Center Faculty Fellowship. ZG": "Pradeep K. Atrey, M. Anwar Hossain, Abdulmotaleb El"
        },
        {
          "Limitations": "",
          "HBCU Quantum Center Faculty Fellowship. ZG": "Saddik, and Mohan S. Kankanhalli. 2010. Multi-"
        },
        {
          "Limitations": "Acknowledgments",
          "HBCU Quantum Center Faculty Fellowship. ZG": ""
        },
        {
          "Limitations": "",
          "HBCU Quantum Center Faculty Fellowship. ZG": "modal fusion for multimedia analysis: a survey. Mul-"
        },
        {
          "Limitations": "",
          "HBCU Quantum Center Faculty Fellowship. ZG": "timedia Systems, 16(6):345–379."
        },
        {
          "Limitations": "The authors acknowledge the use of ChatGPT ex-",
          "HBCU Quantum Center Faculty Fellowship. ZG": ""
        },
        {
          "Limitations": "clusively to refine the text for grammatical checks",
          "HBCU Quantum Center Faculty Fellowship. ZG": "Geetha A.V., Mala T., Priyanka D., and Uma E. 2024."
        },
        {
          "Limitations": "",
          "HBCU Quantum Center Faculty Fellowship. ZG": "Multimodal emotion recognition with deep learning:"
        },
        {
          "Limitations": "in the final manuscript. This work was supported in",
          "HBCU Quantum Center Faculty Fellowship. ZG": ""
        },
        {
          "Limitations": "",
          "HBCU Quantum Center Faculty Fellowship. ZG": "In-\nAdvancements, challenges, and future directions."
        },
        {
          "Limitations": "part by the Guangdong Basic and Applied Basic Re-",
          "HBCU Quantum Center Faculty Fellowship. ZG": ""
        },
        {
          "Limitations": "",
          "HBCU Quantum Center Faculty Fellowship. ZG": "formation Fusion, 105:102218."
        },
        {
          "Limitations": "search Foundation under Grant 2023A1515011370,",
          "HBCU Quantum Center Faculty Fellowship. ZG": ""
        },
        {
          "Limitations": "",
          "HBCU Quantum Center Faculty Fellowship. ZG": "De˘ger Ayata, Yusuf Yaslan, and Mustafa E. Kamasak."
        },
        {
          "Limitations": "the National Natural Science Foundation of China",
          "HBCU Quantum Center Faculty Fellowship. ZG": ""
        },
        {
          "Limitations": "",
          "HBCU Quantum Center Faculty Fellowship. ZG": "2020. Emotion recognition from multimodal phys-"
        },
        {
          "Limitations": "(32371114), the Characteristic Innovation Projects",
          "HBCU Quantum Center Faculty Fellowship. ZG": ""
        },
        {
          "Limitations": "",
          "HBCU Quantum Center Faculty Fellowship. ZG": "iological signals for emotion aware healthcare sys-"
        },
        {
          "Limitations": "of Guangdong Colleges\nand Universities\n(No.",
          "HBCU Quantum Center Faculty Fellowship. ZG": ""
        },
        {
          "Limitations": "",
          "HBCU Quantum Center Faculty Fellowship. ZG": "tems. Journal of Medical and Biological Engineer-"
        },
        {
          "Limitations": "2018KTSCX049), and the Guangdong Provincial",
          "HBCU Quantum Center Faculty Fellowship. ZG": "ing, 40(2):149–157."
        },
        {
          "Limitations": "Key Laboratory [No.2023B1212060076]. YL ac-",
          "HBCU Quantum Center Faculty Fellowship. ZG": ""
        },
        {
          "Limitations": "",
          "HBCU Quantum Center Faculty Fellowship. ZG": "AmirAli Bagher Zadeh, Paul Pu Liang, Soujanya Poria,"
        },
        {
          "Limitations": "knowledges\nsupport\nfrom the Duke Power En-",
          "HBCU Quantum Center Faculty Fellowship. ZG": ""
        },
        {
          "Limitations": "",
          "HBCU Quantum Center Faculty Fellowship. ZG": "Erik Cambria, and Louis-Philippe Morency. 2018."
        },
        {
          "Limitations": "dowed Professorship at\nthe School of Business,",
          "HBCU Quantum Center Faculty Fellowship. ZG": ""
        },
        {
          "Limitations": "",
          "HBCU Quantum Center Faculty Fellowship. ZG": "Multimodal\nlanguage analysis in the wild: CMU-"
        },
        {
          "Limitations": "North Carolina Central University, and the IBM-",
          "HBCU Quantum Center Faculty Fellowship. ZG": "MOSEI dataset and interpretable dynamic fusion"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "the Association for Computational Linguistics (Vol-",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": "2010. Opensmile:\nthe munich versatile and fast open-"
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "ume 1: Long Papers), pages 2236–2246, Melbourne,",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": "source audio feature extractor.\nIn Proceedings of"
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "Australia. Association for Computational Linguistics.",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": "the 18th ACM International Conference on Multi-"
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": "media, MM ’10, page 1459–1462. Association for"
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "Tadas Baltrušaitis, Peter Robinson, and Louis-Philippe",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": "Computing Machinery."
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "Morency. 2016. Openface: An open source facial",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": ""
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": "Yunfeng Fan, Wenchao Xu, Haohao Wang, and Song"
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "behavior analysis toolkit.\nIn 2016 IEEE Winter Con-",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": ""
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": "Guo. 2024. Cross-modal representation flattening for"
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "ference on Applications of Computer Vision (WACV),",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": ""
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": "multi-modal domain generalization.\nIn Advances in"
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "pages 1–10.",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": ""
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": "Neural Information Processing Systems, volume 37,"
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": "pages 66773–66795. Curran Associates, Inc."
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "Su Bo-Hao, Shreya G. Upadhyay, and Lee Chi-Chun.",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": ""
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "2025.\nToward zero-shot speech emotion recogni-",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": ""
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": "Junwei Feng and Xueyan Fan. 2025. Cross-modal con-"
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "tion using llms\nin the absence of\ntarget data.\nIn",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": ""
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": "text fusion and adaptive graph convolutional network"
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "ICASSP 2025 - 2025 IEEE International Confer-",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": ""
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": "for multimodal conversational emotion recognition."
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "ence on Acoustics, Speech and Signal Processing",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": ""
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": "arXiv preprint arXiv:2501.15063."
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "(ICASSP), pages 1–5.",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": ""
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": "Yifan Feng, Haoxuan You, Zizhao Zhang, Rongrong"
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": "Ji, and Yue Gao. 2019. Hypergraph neural networks."
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": ""
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": "Proceedings of\nthe AAAI Conference on Artificial"
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "Kazemzadeh, Emily Mower, Samuel Kim,\nJean-",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": ""
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": "Intelligence, 33(01):3558–3565."
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "nette N. Chang, Sungbok Lee,\nand Shrikanth S.",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": ""
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "Narayanan. 2008.\nIemocap:\ninteractive emotional",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": ""
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": "Yao Fu, Shaoyang Yuan, Chi Zhang, and Juan Cao."
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "Language Re-\ndyadic motion capture database.",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": ""
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": "2023. Emotion recognition in conversations: A sur-"
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "sources and Evaluation, page 335–359.",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": ""
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": "vey focusing on context, speaker dependencies, and"
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": "fusion methods. Electronics, 12(22):4714."
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei,",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": ""
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "Zhifang Guo, Yichong Leng, Yuanjun Lv, Jinzheng",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": "Yumeng Fu.\n2024.\nCkerc:\nJoint\nlarge\nlanguage"
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "He, Junyang Lin, et al. 2024. Qwen2-audio technical",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": "models with\ncommonsense\nknowledge\nfor\nemo-"
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "report. arXiv preprint arXiv:2407.10759.",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": "arXiv preprint\ntion recognition in conversation."
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": "arXiv:2403.07260."
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "Vishal Chudasama, Purbayan Kar, Ashish Gudmalwar,",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": ""
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": "Yumeng Fu,\nJunjie Wu, Zhongjie Wang, Meishan"
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "Nirmesh Shah, Pankaj Wasnik, and Naoyuki Onoe.",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": ""
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": "Zhang, Lili Shan, Yulin Wu,\nand Bingquan Liu."
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "2022. M2fnet: Multi-modal fusion network for emo-",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": ""
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": "2025a. LaERC-S: Improving LLM-based emotion"
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "tion recognition in conversation.\nIn 2022 IEEE/CVF",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": ""
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": "recognition in conversation with speaker character-"
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "Conference on Computer Vision and Pattern Recog-",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": ""
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": "istics.\nIn Proceedings of the 31st International Con-"
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "nition Workshops (CVPRW), pages 4651–4660.",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": ""
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": "ference on Computational Linguistics, pages 6748–"
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": "6761, Abu Dhabi, UAE. Association for Computa-"
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "Gilles Degottex, John Kane, Thomas Drugman, Tuomo",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": ""
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": "tional Linguistics."
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "Raitio, and Stefan Scherer. 2014. Covarep — a col-",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": ""
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "laborative voice analysis repository for speech tech-",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": ""
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": "Yumeng Fu,\nJunjie Wu, Zhongjie Wang, Meishan"
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "2014 IEEE International Conference on\nnologies.",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": ""
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": "Zhang, Yulin Wu, and Bingquan Liu. 2025b. Bemerc:"
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "Acoustics, Speech and Signal Processing (ICASSP),",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": ""
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": "Behavior-aware mllm-based framework for multi-"
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "pages 960–964.",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": ""
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": "arXiv\nmodal emotion recognition in conversation."
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": "preprint arXiv:2503.23990."
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "Jiawen Deng and Fuji Ren. 2023. A survey of textual",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": ""
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "IEEE Trans-\nemotion recognition and its challenges.",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": "Chenquan Gan, Jiahao Zheng, Qingyi Zhu, Yang Cao,"
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "actions on Affective Computing, 14(1):49–67.",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": "and Ye Zhu. 2024. A survey of dialogic emotion"
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": "analysis: Developments, approaches and perspec-"
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": "tives. Pattern Recognition, 156:110794."
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "Ashit Kumar Dutta, Mohan Raparthi, Mahmood Al-",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": ""
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "saadi, Mohammed Wasim Bhatt, Sarath Babu Dodda,",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": ""
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": "Ankita Gandhi, Kinjal Adhvaryu, Soujanya Poria, Erik"
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "Prashant G. C., Mukta Sandhu, and Jagdish Chandra",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": ""
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": "Cambria,\nand Amir Hussain. 2023.\nMultimodal"
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "Patni. 2024. Deep learning-based multi-head self-",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": ""
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": "sentiment analysis: A systematic review of history,"
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "attention model\nfor human epilepsy identification",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": ""
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": "datasets, multimodal fusion methods, applications,"
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "from eeg signal for biomedical\ntraits. Multimedia",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": ""
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": "challenges and future directions.\nInformation Fusion,"
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "Tools and Applications, 83(33):80201–80223.",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": ""
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": "91:424–444."
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "Soumya Dutta and Sriram Ganapathy. 2025. Llm super-",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": "Zixian Gao, Disen Hu, Xun Jiang, Huimin Lu, Heng Tao"
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "vised pre-training for multimodal emotion recogni-",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": "Shen, and Xing Xu. 2024. Enhanced experts with"
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "tion in conversations. Preprint, arXiv:2501.11468.",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": "uncertainty-aware routing for multimodal sentiment"
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": "analysis.\nIn Proceedings of the 32nd ACM Interna-"
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "Paul Ekman, Wallace V Freisen, and Sonia Ancoli. 1980.",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": "tional Conference on Multimedia, MM 2024, Mel-"
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "Journal of\nFacial signs of emotional experience.",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": "bourne, VIC, Australia, 28 October 2024 - 1 Novem-"
        },
        {
          "graph.\nIn Proceedings of the 56th Annual Meeting of": "personality and social psychology, 39(6):1125.",
          "Florian Eyben, Martin Wöllmer, and Björn Schuller.": "ber 2024, pages 9650–9659. ACM."
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "Gokul,\nand G Jyothish Lal. 2024. Multi-lingual",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": "Yao-Hung Hubert Tsai, and Han Zhao. 2024.\nEf-"
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "speech emotion recognition:\nInvestigating similar-",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": "ficient modality selection in multimodal\nlearning."
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "ities between english and german languages.\nIn 2024",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": "Journal of Machine Learning Research, 25(47):1–39."
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "International Conference on Advances in Computing,",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": ""
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": "Zhipeng He, Zina Li, Fuzhou Yang, Lei Wang, Jing-"
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "Communication and Applied Informatics (ACCAI),",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": ""
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": "cong Li, Chengju Zhou, and Jiahui Pan. 2020. Ad-"
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "pages 1–10.",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": ""
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": "vances in multimodal emotion recognition based on"
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": "brain–computer interfaces. Brain Sciences, 10(10)."
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "Deepanway Ghosal, Navonil Majumder, Alexander Gel-",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": ""
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "bukh, Rada Mihalcea,\nand Soujanya Poria. 2020.",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": ""
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": "Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long"
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "COSMIC: COmmonSense knowledge for eMotion",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": ""
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": "short-term memory.\nNeural Computation,\npage"
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "identification in conversations.\nIn Findings of the As-",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": ""
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": "1735–1780."
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "sociation for Computational Linguistics: EMNLP",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": ""
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "2020, pages 2470–2481, Online. Association for",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": "Essam H. Houssein, Asmaa Hammad, Nagwan Abdel"
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "Computational Linguistics.",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": "Samee, Manal Abdullah Alohali, and Abdelmgeid A."
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": "Ali. 2024.\nTfcnn-bigru with self-attention mecha-"
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": "nism for automatic human emotion recognition us-"
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "Deepanway Ghosal, Navonil Majumder, Soujanya Poria,",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": ""
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": "ing multi-channel\neeg data.\nCluster Computing,"
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "Niyati Chhaya, and Alexander Gelbukh. 2019. Di-",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": ""
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": "27(10):14365–14385."
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "alogueGCN: A graph convolutional neural network",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": ""
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "for emotion recognition in conversation.\nIn Proceed-",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": ""
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": "Chao-Chun Hsu, Sheng-Yeh Chen, Chuan-Chun Kuo,"
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "ings of the 2019 Conference on Empirical Methods",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": ""
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": "Ting-Hao Huang, and Lun-Wei Ku. 2018. Emotion-"
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "in Natural Language Processing and the 9th Inter-",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": ""
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": "Lines: An emotion corpus of multi-party conversa-"
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "national Joint Conference on Natural Language Pro-",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": ""
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": "tions.\nIn Proceedings of the Eleventh International"
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "cessing (EMNLP-IJCNLP), pages 154–164, Hong",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": ""
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": "Conference on Language Resources and Evaluation"
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "Kong, China. Association for Computational Lin-",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": ""
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": "(LREC 2018), Miyazaki, Japan. European Language"
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "guistics.",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": ""
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": "Resources Association (ELRA)."
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "Sreyan Ghosh, S Ramaneswaran, Utkarsh Tyagi, Harsh-",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": "Dou Hu, Lingwei Wei, and Xiaoyong Huai. 2021a. Dia-"
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "vardhan Srivastava, Samden Lepcha, S Sakshi, and",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": "logueCRN: Contextual reasoning networks for emo-"
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "Dinesh Manocha. 2023. M-meld: A multilingual",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": "tion recognition in conversations.\nIn Proceedings"
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "multi-party dataset for emotion recognition in con-",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": "of\nthe 59th Annual Meeting of\nthe Association for"
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "versations. Preprint, arXiv:2203.16799.",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": "Computational Linguistics and the 11th International"
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": "Joint Conference on Natural Language Processing"
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": "(Volume 1: Long Papers), pages 7042–7052, Online."
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "Ziwei Gong, Qingkai Min, and Yue Zhang. 2023. Elic-",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": ""
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": "Association for Computational Linguistics."
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "iting rich positive emotions in dialogue generation.",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": ""
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "the First Workshop on Social In-\nIn Proceedings of",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": ""
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": "Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu,"
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "fluence in Conversations (SICon 2023), pages 1–8,",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": ""
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": "Yuchuan Wu, and Yongbin Li. 2022. UniMSE: To-"
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "Toronto, Canada. Association for Computational Lin-",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": ""
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": "wards unified multimodal\nsentiment analysis and"
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "guistics.",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": ""
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": "the 2022\nemotion recognition.\nIn Proceedings of"
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": "Conference on Empirical Methods in Natural Lan-"
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "Ziwei Gong, Muyin Yao, Xinyi Hu, Xiaoning Zhu, and",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": ""
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": "guage Processing, pages 7837–7851, Abu Dhabi,"
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "Julia Hirschberg. 2024. A mapping on current classi-",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": ""
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": "United Arab Emirates. Association for Computa-"
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "fying categories of emotions used in multimodal mod-",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": ""
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": "tional Linguistics."
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "the\nels for emotion recognition.\nIn Proceedings of",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": ""
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "18th Linguistic Annotation Workshop (LAW-XVIII),",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": "Jingwen Hu, Yuchen Liu, Jinming Zhao, and Qin Jin."
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "pages 19–28, St. Julians, Malta. Association for Com-",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": "2021b. MMGCN: Multimodal fusion via deep graph"
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "putational Linguistics.",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": "convolution network for emotion recognition in con-"
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": "versation.\nIn Proceedings of the 59th Annual Meet-"
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": "ing of the Association for Computational Linguistics"
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "Lili Guo, Yikang Song, and Shifei Ding. 2024. Speaker-",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": ""
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": "and the 11th International Joint Conference on Natu-"
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "aware cognitive network with cross-modal attention",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": ""
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": "ral Language Processing (Volume 1: Long Papers),"
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "for multimodal emotion recognition in conversation.",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": ""
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": "pages 5666–5675, Online. Association for Computa-"
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "Knowledge-Based Systems, 296:111969.",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": ""
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": "tional Linguistics."
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "Devamanyu Hazarika, Soujanya Poria, Amir Zadeh,",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": ""
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": "Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and"
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "Erik Cambria, Louis-Philippe Morency, and Roger",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": ""
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": "Kilian Q. Weinberger. 2017. Densely connected con-"
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "Zimmermann. 2018.\nConversational memory net-",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": ""
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": "volutional networks.\nIn 2017 IEEE Conference on"
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "work for emotion recognition in dyadic dialogue",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": ""
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": "Computer Vision and Pattern Recognition (CVPR)."
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "the 2018 Conference of\nvideos.\nIn Proceedings of",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": ""
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "the North American Chapter of the Association for",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": "Jian Huang, Yuanyuan Pu, Dongming Zhou,\nJinde"
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "Computational Linguistics: Human Language Tech-",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": "Cao,\nJinjing Gu, Zhengpeng Zhao,\nand Dan Xu."
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "nologies, Volume 1 (Long Papers), pages 2122–2132,",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": "2024a. Dynamic hypergraph convolutional network"
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "New Orleans, Louisiana. Association for Computa-",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": "for multimodal sentiment analysis. Neurocomputing,"
        },
        {
          "K Ghaayathri Devi, Kolluru Likhitha, J Akshaya, Rfj": "tional Linguistics.",
          "Yifei He, Runxiang Cheng, Gargi Balasubramaniam,": "565:126992."
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Ying Huang. 2024. Real-time application and effect": "evaluation of multimodal emotion recognition model",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": "Zeng. 2023a. Graphmft: A graph network based"
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "in online learning.\nIn Proceedings of the 2024 10th",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": "multimodal fusion technique for emotion recognition"
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "International Conference on Computing and Data",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": "in conversation. Neurocomputing, 550:126427."
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "Engineering,\nICCDE ’24, page 58–63, New York,",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": ""
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "NY, USA. Association for Computing Machinery.",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": "Jiang Li, Xiaoping Wang, and Zhigang Zeng. 2024b."
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": "Tracing intricate cues in dialogue: Joint graph struc-"
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "Zilong Huang, Man-Wai Mak,\nand Kong Aik Lee.",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": ""
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": "ture and sentiment dynamics for multimodal emotion"
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "2024b. Mm-nodeformer: Node transformer multi-",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": ""
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": "recognition. arXiv preprint arXiv:2407.21536."
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "modal fusion for emotion recognition in conversation.",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": ""
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "In Interspeech 2024, pages 4069–4073.",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": ""
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": "Jiaze Li, Hongyan Mei, Liyun Jia, and Xing Zhang."
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": "2023b. Multimodal emotion recognition in conversa-"
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "Muhammad Hussain,\nCaikou Chen,\nSami\nS. Al-",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": ""
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": "tion based on hypergraphs. Electronics, 12(22)."
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "bouq, Khlood Shinan, Fatmah Alanazi, Muham-",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": ""
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "mad Waseem Iqbal, and M. Usman Ashraf. 2025.",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": ""
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "Low-resource mobilebert for emotion recognition in",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": "Songtao Li and Hao Tang. 2024. Multimodal align-"
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "imbalanced text datasets mitigating challenges with",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": "arXiv\npreprint\nment\nand\nfusion:\nA survey."
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "limited resources. PLOS ONE, 20(1):1–17.",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": "arXiv:2411.17040."
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "Md. Milon Islam, Sheikh Nooruddin, Fakhri Karray, and",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": ""
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": "Wei Li, Hehe Fan, Yongkang Wong, Yi Yang, and Mo-"
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "Ghulam Muhammad. 2024. Enhanced multimodal",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": ""
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": "han Kankanhalli. 2024c.\nImproving context under-"
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "emotion recognition in healthcare analytics: A deep",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": ""
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": "standing in multimodal\nlarge language models via"
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "learning based model-level fusion approach. Biomed-",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": ""
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": "multimodal composition learning.\nIn Proceedings of"
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "ical Signal Processing and Control, 94:106241.",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": ""
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": "the 41st International Conference on Machine Learn-"
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": "ing, volume 235 of Proceedings of Machine Learning"
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "Wenxiang Jiao, Michael Lyu, and Irwin King. 2020.",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": ""
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": "Research, pages 27732–27751. PMLR."
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "Real-time emotion recognition via attention gated",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": ""
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "hierarchical memory network.\nIn Proceedings of",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": ""
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": "Zaijing Li, Ting-En Lin, Yuchuan Wu, Meng Liu, Fengx-"
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "the AAAI Conference on Artificial Intelligence, vol-",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": ""
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": "iao Tang, Ming Zhao, and Yongbin Li. 2023c. Unisa:"
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "ume 34, pages 8002–8009, Palo Alto, CA, USA.",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": ""
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": "Unified generative framework for sentiment analysis."
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "AAAI Press.",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": ""
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": "In Proceedings of the 31st ACM International Con-"
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "Umair Ali Khan, Qianru Xu, Yang Liu, Altti Lagstedt,",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": "ference on Multimedia, MM ’23, page 6132–6142,"
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "Ari Alamäki, and Janne Kauttonen. 2024. Explor-",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": "New York, NY, USA. Association for Computing"
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "ing contactless techniques in multimodal emotion",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": "Machinery."
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "recognition:\ninsights into diverse applications, chal-",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": ""
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "lenges, solutions, and prospects. Multimedia Systems,",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": "Zaijing Li, Fengxiao Tang, Ming Zhao, and Yusen Zhu."
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "30(3):115–115.",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": "2022. EmoCaps: Emotion capsule based model for"
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": "conversational emotion recognition.\nIn Findings of"
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "Hakpyeong Kim and Taehoon Hong. 2024.\nEnhanc-",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": ""
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": "the Association for Computational Linguistics: ACL"
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "ing emotion recognition using multimodal fusion of",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": ""
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": "2022, pages 1610–1618, Dublin, Ireland. Association"
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "physiological, environmental, personal data. Expert",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": ""
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": "for Computational Linguistics."
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "Systems with Applications, 249:123723.",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": ""
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": "Zheng Lian, Lan Chen, Licai Sun, Bin Liu, and Jian-"
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "Yoon Kim.\n2014.\nConvolutional\nneural\nnetworks",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": ""
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": "hua Tao. 2023. Gcnet: Graph completion network"
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "for sentence classification.\nIn Proceedings of",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": ""
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "2014 Conference on Empirical Methods in Natural",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": "for incomplete multimodal learning in conversation."
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": "IEEE Transactions on Pattern Analysis and Machine"
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "Language Processing (EMNLP), pages 1746–1751,",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": ""
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": "Intelligence, 45(7):8419–8432."
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "Doha, Qatar. Association for Computational Linguis-",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": ""
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "tics.",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": ""
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": "Zheng Lian, Bin Liu,\nand Jianhua Tao. 2021.\nCt-"
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "Akshi Kumar, Prakhar Dogra, and Vikrant Dabas. 2015.",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": ""
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": "net: Conversational\ntransformer network for emo-"
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "Emotion analysis of twitter using opinion mining.\nIn",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": "IEEE/ACM Transactions on Audio,"
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": "tion recognition."
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "2015 Eighth International Conference on Contempo-",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": ""
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": "Speech, and Language Processing, 29:985–1000."
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "rary Computing (IC3).",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": ""
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": "Chenyu Liu, Xinliang Zhou, Yihao Wu, Ruizhi Yang,"
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "Shanglin Lei, Guanting Dong, Xiaoping Wang, Keheng",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": ""
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": "Zhongruo Wang, Liming Zhai, Ziyu Jia, and Yang"
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "Wang, Runqi Qiao, and Sirui Wang. 2023.\nInstruc-",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": ""
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": "Liu. 2024a.\nGraph neural networks in eeg-based"
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "terc: Reforming emotion recognition in conversation",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": ""
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": "arXiv\npreprint\nemotion\nrecognition:\na\nsurvey."
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "with multi-task retrieval-augmented large language",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": ""
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": "arXiv:2402.01138."
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "models. arXiv preprint arXiv:2309.11911.",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": ""
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "Jiang Li, Xiaoping Wang, Yingjian Liu, and Zhigang",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": "Jiaxing Liu, Sheng Wu, Longbiao Wang, and Jianwu"
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "Zeng. 2024a. Cfn-esa: A cross-modal fusion network",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": "Dang. 2024b. Adaptive deep graph convolutional"
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "with emotion-shift awareness for dialogue emotion",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": "network for dialogical speech emotion recognition."
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "IEEE Transactions on Affective Comput-\nrecognition.",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": "In Man-Machine Speech Communication, pages 248–"
        },
        {
          "Ying Huang. 2024. Real-time application and effect": "ing, 15(4):1919–1933.",
          "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang": "255, Singapore. Springer Nature Singapore."
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "Pei. 2022. Revisiting graph contrastive learning from",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": "Ellis, Matt McVicar, Eric Battenberg, and Oriol Nieto."
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "the perspective of graph spectrum.\nIn Advances in",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": "2015.\nlibrosa: Audio and music signal analysis in"
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "Neural Information Processing Systems, volume 35,",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": "python.\nIn SciPy."
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "pages 2972–2983. Curran Associates, Inc.",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": ""
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": "Gary McKeown, Michel Valstar, Roddy Cowie, Maja"
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": "Pantic,\nand Marc Schroder. 2012.\nThe semaine"
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": ""
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": "database: Annotated multimodal records of emotion-"
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "dar\nJoshi, Danqi Chen, Omer Levy, Mike Lewis,",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": ""
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": "ally colored conversations between a person and a"
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "Luke Zettlemoyer,\nand Veselin\nStoyanov.\n2019.",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": ""
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": "IEEE Transactions on Affective Com-\nlimited agent."
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "Roberta: A robustly optimized bert pretraining ap-",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": ""
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": "puting, 3(1):5–17."
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "proach. ArXiv, abs/1907.11692.",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": ""
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": "Anam Moin, Farhan Aadil, Zeeshan Ali, and Dong-"
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "Yuanyuan Liu, Lin Wei, Kejun Liu, Yibing Zhan, Zijing",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": ""
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": "wann Kang. 2023.\nEmotion recognition frame-"
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "Chen, Zhe Chen, and Shiguang Shan. 2024c. Smile",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": ""
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": "work using multiple modalities for an effective hu-"
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "upon the face but sadness in the eyes: Emotion recog-",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": ""
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": "The Journal of Super-\nman–computer\ninteraction."
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "nition based on facial expressions and eye behaviors.",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": ""
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": "computing, 79(8):9320–9349."
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "Preprint, arXiv:2411.05879.",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": ""
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": "Krishna D. N. and Ankita Patil. 2020. Multimodal"
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "Chen Ma, Peng Kang, and Xue Liu. 2019. Hierarchical",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": ""
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": "emotion recognition using cross-modal attention and"
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "gating networks for sequential recommendation.\nIn",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": ""
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": "1d convolutional neural networks.\nIn Interspeech"
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "Proceedings of the 25th ACM SIGKDD International",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": ""
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": "2020, pages 4243–4247."
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "Conference on Knowledge Discovery & Data Min-",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": ""
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "ing, KDD ’19, page 825–833, New York, NY, USA.",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": "Cam-Van Thi Nguyen, Anh-Tuan Mai, The-Son Le, Hai-"
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "Association for Computing Machinery.",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": "Dang Kieu, and Duc-Trong Le. 2023. Conversation"
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": "understanding using relational temporal graph neural"
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "Hui Ma,\nJian Wang, Hongfei Lin, Bo Zhang, Yijia",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": "networks with auxiliary cross-modality interaction."
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "Zhang, and Bo Xu. 2024a.\nA transformer-based",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": "the 2023 Conference on Empiri-\nIn Proceedings of"
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "model with self-distillation for multimodal emotion",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": "cal Methods in Natural Language Processing, pages"
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "IEEE Transactions on\nrecognition in conversations.",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": "15154–15167, Singapore. Association for Computa-"
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "Multimedia, 26:776–788.",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": "tional Linguistics."
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "Wenxuan Ma, Shuang Li, Lincan Cai, and Jingxuan",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": "Fatemeh Noroozi, Ciprian Adrian Corneanu, Dorota"
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "Kang. 2024b. Learning modality knowledge align-",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": "Kami´nska, Tomasz Sapi´nski, Sergio Escalera, and"
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "ment for cross-modality transfer.\nIn Proceedings of",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": "Gholamreza Anbarjafari. 2021. Survey on emotional"
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "the 41st International Conference on Machine Learn-",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": "IEEE Transactions on\nbody gesture recognition."
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "ing, volume 235 of Proceedings of Machine Learning",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": "Affective Computing, 12(2):505–523."
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "Research, pages 33777–33793. PMLR.",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": ""
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": "Sancheng Peng, Lihong Cao, Yongmei Zhou, Zhouhao"
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": "Ouyang, Aimin Yang, Xinguang Li, Weijia Jia, and"
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "Sijie Mai, Ya Sun, Aolin Xiong, Ying Zeng, and Haifeng",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": ""
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": "Shui Yu. 2022. A survey on deep learning for tex-"
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "Hu. 2024. Multimodal boosting: Addressing noisy",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": ""
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": "Digital\ntual emotion analysis in social networks."
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "modalities\nand identifying modality contribution.",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": ""
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": "Communications and Networks, 8(5):745–762."
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "IEEE Transactions on Multimedia, 26:3018–3033.",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": ""
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": "Soujanya Poria, Erik Cambria, Devamanyu Hazarika,"
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "Navonil Majumder, Soujanya Poria, Devamanyu Haz-",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": ""
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": "Navonil Majumder, Amir Zadeh, and Louis-Philippe"
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "arika, Rada Mihalcea, Alexander Gelbukh, and Erik",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": ""
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": "Morency. 2017. Context-dependent sentiment anal-"
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "Cambria. 2019. Dialoguernn: An attentive rnn for",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": ""
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": "ysis in user-generated videos.\nIn Proceedings of the"
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "emotion detection in conversations. Proceedings of",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": ""
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": "55th Annual Meeting of the Association for Compu-"
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "the AAAI Conference on Artificial Intelligence, page",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": ""
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": "tational Linguistics (Volume 1: Long Papers), pages"
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "6818–6825.",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": ""
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": "873–883, Vancouver, Canada. Association for Com-"
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": "putational Linguistics."
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "Yuzhao Mao, Guang Liu, Xiaojie Wang, Weiguo Gao,",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": ""
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "and Xuan Li. 2021. DialogueTRM: Exploring multi-",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": ""
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-"
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "modal emotional dynamics in a conversation.\nIn",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": ""
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": "jumder, Gautam Naik, Erik Cambria, and Rada Mi-"
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "Findings of the Association for Computational Lin-",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": ""
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": "halcea. 2019a. MELD: A multimodal multi-party"
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "guistics: EMNLP 2021, pages 2694–2704, Punta",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": ""
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": "dataset for emotion recognition in conversations.\nIn"
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "Cana, Dominican Republic. Association for Compu-",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": ""
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": "Proceedings of\nthe 57th Annual Meeting of\nthe As-"
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "tational Linguistics.",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": ""
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": "sociation for Computational Linguistics, pages 527–"
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": "536, Florence, Italy. Association for Computational"
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "Zdravko Marinov, Alina Roitberg, David Schneider,",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": ""
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": "Linguistics."
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "and Rainer Stiefelhagen. 2023. Modselect: Auto-",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": ""
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "matic modality selection for synthetic-to-real domain",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": "Soujanya Poria, Navonil Majumder, Rada Mihalcea, and"
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "generalization.\nIn Computer Vision – ECCV 2022",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": "Eduard Hovy. 2019b. Emotion recognition in con-"
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "Workshops, pages 326–346, Cham. Springer Nature",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": "versation: Research challenges, datasets, and recent"
        },
        {
          "Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian": "Switzerland.",
          "Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.": "advances.\nIEEE Access, 7:100943–100953."
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "Zero-shot video emotion recognition via multimodal",
          "emotion recognition in conversation.\nIn Proceedings": "of\nthe 31st\nInternational Conference on Computa-"
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "protagonist-aware transformer network.\nIn Proceed-",
          "emotion recognition in conversation.\nIn Proceedings": "tional Linguistics, pages 256–268, Abu Dhabi, UAE."
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "ings of\nthe 29th ACM International Conference on",
          "emotion recognition in conversation.\nIn Proceedings": "Association for Computational Linguistics."
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "Multimedia, MM ’21, page 1074–1083, New York,",
          "emotion recognition in conversation.\nIn Proceedings": ""
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "NY, USA. Association for Computing Machinery.",
          "emotion recognition in conversation.\nIn Proceedings": "Lin Song, Yukang Chen, Shuai Yang, Xiaohan Ding,"
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "",
          "emotion recognition in conversation.\nIn Proceedings": "Yixiao Ge, Ying-Cong Chen, and Ying Shan. 2024."
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "Manju Priya Arthanarisamy Ramaswamy and Suja",
          "emotion recognition in conversation.\nIn Proceedings": "Low-rank approximation for sparse attention in multi-"
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "Palaniswamy. 2024.\nMultimodal emotion recog-",
          "emotion recognition in conversation.\nIn Proceedings": "modal llms.\nIn 2024 IEEE/CVF Conference on Com-"
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "nition: A comprehensive review,\ntrends, and chal-",
          "emotion recognition in conversation.\nIn Proceedings": "puter Vision and Pattern Recognition (CVPR), pages"
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "lenges. WIREs Data Mining and Knowledge Discov-",
          "emotion recognition in conversation.\nIn Proceedings": "13763–13773."
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "ery, 14(6):e1563.",
          "emotion recognition in conversation.\nIn Proceedings": ""
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "",
          "emotion recognition in conversation.\nIn Proceedings": "Yanyuan Su, Cuijuan Han, Yaming Zhang, and Haiou"
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "Nils Reimers and Iryna Gurevych. 2019.\nSentence-",
          "emotion recognition in conversation.\nIn Proceedings": "Liu. 2024. Multimodal sentiment recognition driven"
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "BERT: Sentence embeddings using Siamese BERT-",
          "emotion recognition in conversation.\nIn Proceedings": "by feature\nfusion strategy for\nsocial media\ncom-"
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "networks.\nIn Proceedings of the 2019 Conference on",
          "emotion recognition in conversation.\nIn Proceedings": "ments on sudden natural disasters. Available at SSRN"
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "Empirical Methods in Natural Language Processing",
          "emotion recognition in conversation.\nIn Proceedings": "5011893."
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "and the 9th International Joint Conference on Natu-",
          "emotion recognition in conversation.\nIn Proceedings": ""
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "ral Language Processing (EMNLP-IJCNLP), pages",
          "emotion recognition in conversation.\nIn Proceedings": "Hiroki Tanioka, Tetsushi Ueta,\nand Masahiko Sano."
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "3982–3992, Hong Kong, China. Association for Com-",
          "emotion recognition in conversation.\nIn Proceedings": "2024. Toward a dialogue system using a large lan-"
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "putational Linguistics.",
          "emotion recognition in conversation.\nIn Proceedings": "guage model to recognize user emotions with a cam-"
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "",
          "emotion recognition in conversation.\nIn Proceedings": "era. arXiv preprint arXiv:2408.07982."
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "David\nSasu,\nZehui Wu,\nZiwei Gong, Run Chen,",
          "emotion recognition in conversation.\nIn Proceedings": ""
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "Pengyuan Shi, Lin Ai, Julia Hirschberg, and Natalie",
          "emotion recognition in conversation.\nIn Proceedings": "Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torre-"
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "Schluter. 2025. Akan cinematic emotions (ACE): A",
          "emotion recognition in conversation.\nIn Proceedings": "sani, and Manohar Paluri. 2015. Learning Spatiotem-"
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "multimodal multi-party dataset\nfor emotion recog-",
          "emotion recognition in conversation.\nIn Proceedings": "poral Features with 3D Convolutional Networks .\nIn"
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "nition in movie dialogues.\nIn Findings of the Asso-",
          "emotion recognition in conversation.\nIn Proceedings": "2015 IEEE International Conference on Computer"
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "ciation for Computational Linguistics: ACL 2025,",
          "emotion recognition in conversation.\nIn Proceedings": "Vision (ICCV), pages 4489–4497, Los Alamitos, CA,"
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "pages 9820–9831, Vienna, Austria. Association for",
          "emotion recognition in conversation.\nIn Proceedings": "USA. IEEE Computer Society."
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "Computational Linguistics.",
          "emotion recognition in conversation.\nIn Proceedings": ""
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "",
          "emotion recognition in conversation.\nIn Proceedings": "Yun-Da Tsai, Ting-Yu Yen,\nPei-Fu Guo, Zhe-Yan"
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus",
          "emotion recognition in conversation.\nIn Proceedings": "Li,\nand Shou-De Lin. 2024.\nText-centric\nalign-"
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "Hagenbuchner, and Gabriele Monfardini. 2009. The",
          "emotion recognition in conversation.\nIn Proceedings": "arXiv preprint\nment\nfor multi-modality learning."
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "IEEE Transactions on\ngraph neural network model.",
          "emotion recognition in conversation.\nIn Proceedings": "arXiv:2402.08086."
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "Neural Networks, 20(1):61–80.",
          "emotion recognition in conversation.\nIn Proceedings": ""
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "",
          "emotion recognition in conversation.\nIn Proceedings": "Gustave Udahemuka, Karim Djouani, and Anish M."
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "Björn W. Schuller, Michel François Valstar, Roddy",
          "emotion recognition in conversation.\nIn Proceedings": "Kurien. 2024. Multimodal emotion recognition using"
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "Cowie, and Maja Pantic. 2012. AVEC 2012:\nthe",
          "emotion recognition in conversation.\nIn Proceedings": "visual, vocal and physiological signals: A review."
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "continuous audio/visual emotion challenge - an intro-",
          "emotion recognition in conversation.\nIn Proceedings": "Applied Sciences, 14(17)."
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "duction.\nIn International Conference on Multimodal",
          "emotion recognition in conversation.\nIn Proceedings": ""
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "Interaction, ICMI ’12, Santa Monica, CA, USA, Oc-",
          "emotion recognition in conversation.\nIn Proceedings": "Cuong Tran Van, Thanh V. T. Tran, Van Nguyen, and"
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "tober 22-26, 2012, pages 361–362. ACM.",
          "emotion recognition in conversation.\nIn Proceedings": "Truong Son Hy. 2025. Effective context modeling"
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "",
          "emotion recognition in conversation.\nIn Proceedings": "framework for emotion recognition in conversations."
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "Guangyao Shen, Xin Wang, Xuguang Duan, Hongzhi",
          "emotion recognition in conversation.\nIn Proceedings": "In ICASSP 2025 - 2025 IEEE International Confer-"
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "Li, and Wenwu Zhu. 2020. Memor: A dataset for",
          "emotion recognition in conversation.\nIn Proceedings": "ence on Acoustics, Speech and Signal Processing"
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "multimodal emotion reasoning in videos.\nIn Proceed-",
          "emotion recognition in conversation.\nIn Proceedings": "(ICASSP), pages 1–5."
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "ings of\nthe 28th ACM International Conference on",
          "emotion recognition in conversation.\nIn Proceedings": ""
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "Multimedia, MM ’20, page 493–502, New York, NY,",
          "emotion recognition in conversation.\nIn Proceedings": "Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2019."
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "USA. Association for Computing Machinery.",
          "emotion recognition in conversation.\nIn Proceedings": "Representation learning with contrastive predictive"
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "",
          "emotion recognition in conversation.\nIn Proceedings": "coding. Preprint, arXiv:1807.03748."
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "Tao Shi and Shao-Lun Huang. 2023. MultiEMO: An",
          "emotion recognition in conversation.\nIn Proceedings": ""
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "attention-based correlation-aware multimodal fusion",
          "emotion recognition in conversation.\nIn Proceedings": "RK Vani and P Jayashree. 2025. Multimodal emotion"
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "framework for emotion recognition in conversations.",
          "emotion recognition in conversation.\nIn Proceedings": "recognition system for e-learning platform. Educa-"
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "the 61st Annual Meeting of\nthe\nIn Proceedings of",
          "emotion recognition in conversation.\nIn Proceedings": "tion and Information Technologies, pages 1–32."
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "Association for Computational Linguistics (Volume 1:",
          "emotion recognition in conversation.\nIn Proceedings": ""
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "",
          "emotion recognition in conversation.\nIn Proceedings": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob"
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "Long Papers), pages 14752–14766, Toronto, Canada.",
          "emotion recognition in conversation.\nIn Proceedings": ""
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "",
          "emotion recognition in conversation.\nIn Proceedings": "Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz"
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "Association for Computational Linguistics.",
          "emotion recognition in conversation.\nIn Proceedings": ""
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "",
          "emotion recognition in conversation.\nIn Proceedings": "Kaiser, and Illia Polosukhin. 2017. Attention is all"
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "Yuntao Shou, Wei Ai, Jiayi Du, Tao Meng, Haiyan Liu,",
          "emotion recognition in conversation.\nIn Proceedings": "you need.\nIn Advances in Neural Information Pro-"
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "and Nan Yin. 2024.\nEfficient\nlong-distance latent",
          "emotion recognition in conversation.\nIn Proceedings": "cessing Systems, volume 30. Curran Associates, Inc."
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "relation-aware graph neural network for multi-modal",
          "emotion recognition in conversation.\nIn Proceedings": ""
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "",
          "emotion recognition in conversation.\nIn Proceedings": "William Villegas-Ch, Rommel Gutierrez, and Aracely"
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "emotion recognition in conversations. arXiv preprint",
          "emotion recognition in conversation.\nIn Proceedings": ""
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "",
          "emotion recognition in conversation.\nIn Proceedings": "Mera-Navarrete. 2025. Multimodal emotional de-"
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "arXiv:2407.00119.",
          "emotion recognition in conversation.\nIn Proceedings": ""
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "",
          "emotion recognition in conversation.\nIn Proceedings": "tection system for virtual educational environments:"
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "Yuntao Shou, Tao Meng, Wei Ai, and Keqin Li. 2025.",
          "emotion recognition in conversation.\nIn Proceedings": "Integration into microsoft teams to improve student"
        },
        {
          "Fan Qi, Xiaoshan Yang, and Changsheng Xu. 2021.": "Dynamic graph neural ODE network for multi-modal",
          "emotion recognition in conversation.\nIn Proceedings": "engagement.\nIEEE Access, 13:42910–42933."
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "multi-emotion recognition based on heart rate vari-",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": "modal emotional understanding meets large language"
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "ability signal features mining. Sensors, 23(20).",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": "models. arXiv preprint arXiv:2406.16442."
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "Peng Wang, Lesya Ganushchak, Camille Welie, and",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": "Xiaocui Yang, Shi Feng, Daling Wang, Yifei Zhang,"
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "Roel\nvan Steensel.\n2024.\nThe\ndynamic\nnature",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": "and Soujanya Poria. 2023.\nFew-shot multimodal"
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "of emotions in language learning context: Theory,",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": "sentiment analysis based on multimodal probabilistic"
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "method, and analysis. Educational Psychology Re-",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": "the 31st ACM\nfusion prompts.\nIn Proceedings of"
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "view, 36(4):105.",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": "International Conference on Multimedia, MM ’23,"
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": "page 6045–6053, New York, NY, USA. Association"
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": "for Computing Machinery."
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "Guu, Adams Wei Yu, Brian Lester, Nan Du, An-",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": ""
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "drew M Dai, and Quoc V Le. 2021. Finetuned lan-",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": "Yang Yang, Xunde Dong, and Yupeng Qiang. 2025."
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "guage models are zero-shot learners. arXiv preprint",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": "Mse-adapter: A lightweight plugin endowing llms"
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "arXiv:2109.01652.",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": "with the\ncapability to perform multimodal\nsenti-"
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": "ment analysis and emotion recognition. Proceedings"
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": "of\nthe AAAI Conference on Artificial\nIntelligence,"
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "Bosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le,",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": "39(24):25642–25650."
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "and Denny Zhou. 2022. Chain-of-thought prompt-",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": ""
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "ing elicits reasoning in large language models.\nIn",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": "Ziqing Yang, Katherine Nayan, Zehao Fan, and Houwei"
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "Advances in Neural Information Processing Systems,",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": "Cao. 2022. Multimodal emotion recognition with"
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "volume 35, pages 24824–24837. Curran Associates,",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": "surgical and fabric masks.\nIn ICASSP 2022 - 2022"
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": "IEEE International Conference on Acoustics, Speech"
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "Inc.",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": ""
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": "and Signal Processing (ICASSP), pages 4678–4682."
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "Mark D. Wilkinson, Michel Dumontier, I. J. Aalbers-",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": ""
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": "Kun Yi, Qi Zhang, Wei Fan, Hui He, Liang Hu,"
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "berg, Gabrielle Appleton, Myles Axton, Arie Baak,",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": ""
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": "Pengyang Wang, Ning An, Longbing Cao, and Zhen-"
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "Niklas Blomberg, Jan-Willem Boiten, Luiz Bonino",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": ""
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": "dong Niu. 2023. Fouriergnn: Rethinking multivariate"
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "da Silva Santos, Philip E. Bourne, Jildau Bouwman,",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": ""
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": "time series forecasting from a pure graph perspective."
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "Anthony J. Brookes, Tim Clark, Mercè Crosas, In-",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": ""
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": "In Advances in Neural Information Processing Sys-"
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "grid Dillo, Olivier Dumon, Scott Edmunds, Chris T.",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": ""
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": "tems, volume 36, pages 69638–69660. Curran Asso-"
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "Evelo, Richard Finkers, Alejandra Gonzalez-Beltran,",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": ""
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": "ciates, Inc."
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "Alasdair J. G. Gray, Paul Groth, Carole Goble, Jef-",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": ""
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "frey S. Grethe, Jaap Heringa, Peter A. C. ’t Hoen, Rob",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": ""
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": "Sayyed M. Zahiri and Jinho D. Choi. 2017. Emotion"
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "Hooft, Tobias Kuhn, Ruben Kok, Joost Kok, Scott J.",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": ""
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": "detection on TV show transcripts with sequence-"
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "Lusher, Maryann E. Martone, Albert Mons, Abel L.",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": ""
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": "based\nconvolutional\nneural\nnetworks.\nCoRR,"
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "Packer, Bengt Persson, Philippe Rocca-Serra, Marco",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": ""
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": "abs/1708.04299."
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "Roos, Rene van Schaik, Susanna-Assunta Sansone,",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": ""
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "and Erik Schultes. 2016.\nThe fair guiding princi-",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": ""
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": "Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li,\nand"
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "ples for scientific data management and stewardship.",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": ""
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": "Yu Qiao. 2016.\nJoint face detection and alignment"
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "Scientific Data, 3:160018.",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": ""
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": "using multitask cascaded convolutional networks."
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": "IEEE Signal Processing Letters, 23(10):1499–1503."
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": ""
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "Donbekci, and Julia Hirschberg. 2025. Beyond silent",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": ""
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": "Linfeng Zhang, Jiebo Song, Anni Gao, Jingwei Chen,"
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "letters: Amplifying LLMs in emotion recognition",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": ""
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": "Chenglong Bao, and Kaisheng Ma. 2019. Be your"
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "with vocal nuances.\nIn Findings of the Association",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": ""
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": "own teacher:\nImprove the performance of convolu-"
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "for Computational Linguistics: NAACL 2025, pages",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": ""
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": "tional neural networks via self distillation.\nIn 2019"
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "2202–2218, Albuquerque, New Mexico. Association",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": ""
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": "IEEE/CVF International Conference on Computer"
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "for Computational Linguistics.",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": ""
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": "Vision (ICCV), pages 3712–3721."
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "Yunhe Xie, Kailai Yang, Chengjie Sun, Bingquan Liu,",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": ""
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": "Qingyang Zhang, Yake Wei, Zongbo Han, Huazhu Fu,"
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "and Zhenzhou Ji. 2021. Knowledge-interactive net-",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": ""
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": "Xi Peng, Cheng Deng, Qinghua Hu, Cai Xu,\nJie"
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "work with sentiment polarity intensity-aware multi-",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": ""
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": "Wen, Di Hu, et al. 2024a. Multimodal\nfusion on"
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "task learning for emotion recognition in conversa-",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": ""
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": "arXiv\nlow-quality data: A comprehensive survey."
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "the Association for Computa-\ntions.\nIn Findings of",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": ""
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": "preprint arXiv:2404.18947."
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "tional Linguistics: EMNLP 2021, pages 2879–2889,",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": ""
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "Punta Cana, Dominican Republic. Association for",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": "Tao Zhang and Zhenhua Tan. 2024. Survey of deep emo-"
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "Computational Linguistics.",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": "tion recognition in dynamic data using facial, speech"
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": "and textual cues. Multimedia Tools and Applications,"
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "Chuanguang Yang,\nZhulin An,\nLinhang Cai,\nand",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": ""
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": "83(25):66223–66262."
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "Yongjun Xu. 2021. Hierarchical self-supervised aug-",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": ""
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "mented knowledge distillation.\nIn Proceedings of the",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": "Xiaoheng Zhang and Yang Li. 2023. A cross-modality"
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "Thirtieth International Joint Conference on Artificial",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": "context fusion and semantic refinement network for"
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "Intelligence,\nIJCAI-21, pages 1217–1223.\nInterna-",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": "emotion recognition in conversation.\nIn Proceedings"
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "tional Joint Conferences on Artificial\nIntelligence",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": "of\nthe 61st Annual Meeting of\nthe Association for"
        },
        {
          "Ling Wang, Jiayu Hao, and Tie Hua Zhou. 2023. Ecg": "Organization. Main Track.",
          "Qu Yang, Mang Ye, and Bo Du. 2024. Emollm: Multi-": "Computational Linguistics (Volume 1: Long Papers),"
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "pages 13099–13110, Toronto, Canada. Association": "for Computational Linguistics.",
          "MELD.\nThe Multimodal EmotionLines Dataset": ""
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "",
          "MELD.\nThe Multimodal EmotionLines Dataset": "(MELD)\n(Poria et al., 2019a)\nis an extension of"
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "",
          "MELD.\nThe Multimodal EmotionLines Dataset": "the EmotionLines corpus (Hsu et al., 2018), con-"
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "Yazhou Zhang, Mengyao Wang, Youxi Wu, Prayag Ti-",
          "MELD.\nThe Multimodal EmotionLines Dataset": ""
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "",
          "MELD.\nThe Multimodal EmotionLines Dataset": "structed from the TV series Friends.\nIt contains"
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "wari, Qiuchi Li, Benyou Wang, and Jing Qin. 2025.",
          "MELD.\nThe Multimodal EmotionLines Dataset": ""
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "Dialoguellm: Context and emotion knowledge-tuned",
          "MELD.\nThe Multimodal EmotionLines Dataset": "1,433 multi-party conversations and 13,708 utter-"
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "large language models for emotion recognition in",
          "MELD.\nThe Multimodal EmotionLines Dataset": "ances.\nEach utterance is annotated with one of"
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "conversations. Neural Networks, 192:107901.",
          "MELD.\nThe Multimodal EmotionLines Dataset": ""
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "",
          "MELD.\nThe Multimodal EmotionLines Dataset": "seven emotion categories:\nanger,\ndisgust,\nfear,"
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "",
          "MELD.\nThe Multimodal EmotionLines Dataset": "joy, neutral, sadness, and surprise. Unlike dyadic"
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "Zhengxuan Zhang, Jianying Chen, Xuejie Liu, Weixing",
          "MELD.\nThe Multimodal EmotionLines Dataset": ""
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "",
          "MELD.\nThe Multimodal EmotionLines Dataset": "datasets, MELD captures the complexity of multi-"
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "Mai, and Qianhua Cai. 2024b.\n‘what’ and ‘where’",
          "MELD.\nThe Multimodal EmotionLines Dataset": ""
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "both matter: dual cross-modal graph convolutional",
          "MELD.\nThe Multimodal EmotionLines Dataset": "speaker\ninteractions, making it well-suited for"
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "networks for multimodal named entity recognition.",
          "MELD.\nThe Multimodal EmotionLines Dataset": "studying emotion recognition in multi-party con-"
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "International Journal of Machine Learning and Cy-",
          "MELD.\nThe Multimodal EmotionLines Dataset": ""
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "",
          "MELD.\nThe Multimodal EmotionLines Dataset": "versational settings."
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "bernetics, 15(6):2399–2409.",
          "MELD.\nThe Multimodal EmotionLines Dataset": ""
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "",
          "MELD.\nThe Multimodal EmotionLines Dataset": "CMU-MOSEI.\nThe\nCMU-MOSEI\ndataset"
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "Zhengxuan Zhang, Weixing Mai, Haoliang Xiong,",
          "MELD.\nThe Multimodal EmotionLines Dataset": ""
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "Chuhan Wu,\nand Yun Xue. 2023.\nA token-wise",
          "MELD.\nThe Multimodal EmotionLines Dataset": "(Bagher Zadeh et al., 2018) consists of 23,453"
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "graph-based framework for multimodal named entity",
          "MELD.\nThe Multimodal EmotionLines Dataset": ""
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "",
          "MELD.\nThe Multimodal EmotionLines Dataset": "sentence-level video segments\nfrom over 1,000"
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "recognition.\nIn 2023 IEEE International Conference",
          "MELD.\nThe Multimodal EmotionLines Dataset": ""
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "",
          "MELD.\nThe Multimodal EmotionLines Dataset": "speakers\ncovering\n250\ntopics,\ncollected\nfrom"
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "on Multimedia and Expo (ICME), pages 2153–2158.",
          "MELD.\nThe Multimodal EmotionLines Dataset": ""
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "",
          "MELD.\nThe Multimodal EmotionLines Dataset": "YouTube monologue videos.\nEach segment\nis"
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "Jinming Zhao, Tenggan Zhang, Jingwen Hu, Yuchen",
          "MELD.\nThe Multimodal EmotionLines Dataset": "annotated for sentiment on a 7-point Likert scale"
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "Liu, Qin Jin, Xinchao Wang, and Haizhou Li. 2022.",
          "MELD.\nThe Multimodal EmotionLines Dataset": "([-3:\nhighly negative,\n-2:\nnegative,\n-1: weakly"
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "M3ED: Multi-modal multi-scene multi-label emo-",
          "MELD.\nThe Multimodal EmotionLines Dataset": ""
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "",
          "MELD.\nThe Multimodal EmotionLines Dataset": "negative,\n0:\nneutral, +1:\nweakly positive, +2:"
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "tional dialogue database.\nIn Proceedings of the 60th",
          "MELD.\nThe Multimodal EmotionLines Dataset": ""
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "",
          "MELD.\nThe Multimodal EmotionLines Dataset": "positive, +3: highly positive]) and for the intensity"
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "Annual Meeting of the Association for Computational",
          "MELD.\nThe Multimodal EmotionLines Dataset": ""
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "",
          "MELD.\nThe Multimodal EmotionLines Dataset": "of\nsix Ekman\n(Ekman\net\nal.,\n1980)\nemotions:"
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "Linguistics (Volume 1: Long Papers), Dublin, Ireland.",
          "MELD.\nThe Multimodal EmotionLines Dataset": ""
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "Association for Computational Linguistics.",
          "MELD.\nThe Multimodal EmotionLines Dataset": "happiness,\nsadness,\nanger,\nfear,\ndisgust,\nand"
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "",
          "MELD.\nThe Multimodal EmotionLines Dataset": "surprise. The dataset provides aligned language,"
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "Xiaofei Zhu, Jiawei Cheng, Zhou Yang, Zhuo Chen,",
          "MELD.\nThe Multimodal EmotionLines Dataset": ""
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "",
          "MELD.\nThe Multimodal EmotionLines Dataset": "visual,\nand\nacoustic modalities, making\nit\na"
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "Qingyang Wang, and Jianfeng Yao. 2024. Cmath:",
          "MELD.\nThe Multimodal EmotionLines Dataset": ""
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "",
          "MELD.\nThe Multimodal EmotionLines Dataset": "large-scale benchmark for multimodal sentiment"
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "Cross-modality augmented transformer with hierar-",
          "MELD.\nThe Multimodal EmotionLines Dataset": ""
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "chical variational distillation for multimodal emo-",
          "MELD.\nThe Multimodal EmotionLines Dataset": "and emotion recognition."
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "arXiv preprint\ntion recognition in conversation.",
          "MELD.\nThe Multimodal EmotionLines Dataset": ""
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "arXiv:2411.10060.",
          "MELD.\nThe Multimodal EmotionLines Dataset": ""
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "",
          "MELD.\nThe Multimodal EmotionLines Dataset": "M3ED.\nThe M3ED dataset\n(Zhao et al., 2022)"
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "",
          "MELD.\nThe Multimodal EmotionLines Dataset": "consists of 990 dyadic dialogues and 24,449 utter-"
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "Shihao Zou, Xianying Huang, and Xudong Shen. 2023.",
          "MELD.\nThe Multimodal EmotionLines Dataset": ""
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "",
          "MELD.\nThe Multimodal EmotionLines Dataset": "ances collected from 56 Chinese TV series. Each"
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "Multimodal prompt\ntransformer with hybrid con-",
          "MELD.\nThe Multimodal EmotionLines Dataset": ""
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "trastive learning for emotion recognition in conver-",
          "MELD.\nThe Multimodal EmotionLines Dataset": "utterance is annotated with one or more of seven"
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "the 31st ACM Interna-\nsation.\nIn Proceedings of",
          "MELD.\nThe Multimodal EmotionLines Dataset": "emotion labels: happy, surprise, sad, disgust, anger,"
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "tional Conference on Multimedia, MM ’23, page",
          "MELD.\nThe Multimodal EmotionLines Dataset": ""
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "",
          "MELD.\nThe Multimodal EmotionLines Dataset": "fear, and neutral. The dataset covers text, audio,"
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "5994–6003, New York, NY, USA. Association for",
          "MELD.\nThe Multimodal EmotionLines Dataset": ""
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "",
          "MELD.\nThe Multimodal EmotionLines Dataset": "and visual modalities and features blended emo-"
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "Computing Machinery.",
          "MELD.\nThe Multimodal EmotionLines Dataset": ""
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "",
          "MELD.\nThe Multimodal EmotionLines Dataset": "tions and speaker metadata, making it\nthe first"
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "ShiHao Zou, Xianying Huang, XuDong Shen,\nand",
          "MELD.\nThe Multimodal EmotionLines Dataset": "large-scale multimodal emotional dialogue corpus"
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "Hankai Liu. 2022.\nImproving multimodal\nfusion",
          "MELD.\nThe Multimodal EmotionLines Dataset": ""
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "",
          "MELD.\nThe Multimodal EmotionLines Dataset": "in Chinese."
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "with main modal\ntransformer for emotion recogni-",
          "MELD.\nThe Multimodal EmotionLines Dataset": ""
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "tion in conversation.\nKnowledge-Based Systems,",
          "MELD.\nThe Multimodal EmotionLines Dataset": ""
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "",
          "MELD.\nThe Multimodal EmotionLines Dataset": "ACE.\nThe ACE dataset (Sasu et al., 2025) con-"
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "258:109978.",
          "MELD.\nThe Multimodal EmotionLines Dataset": ""
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "",
          "MELD.\nThe Multimodal EmotionLines Dataset": "tains 385 emotion-labeled dialogues and 6,162 ut-"
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "",
          "MELD.\nThe Multimodal EmotionLines Dataset": "terances in the Akan language, collected from 21"
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "A\nDatasets Details",
          "MELD.\nThe Multimodal EmotionLines Dataset": ""
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "",
          "MELD.\nThe Multimodal EmotionLines Dataset": "movie sources. It includes multimodal information"
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "IEMOCAP.\nThe IEMOCAP dataset (Busso et al.,",
          "MELD.\nThe Multimodal EmotionLines Dataset": "across text, audio, and visual modalities, and is"
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "2008) consists of dyadic conversation videos from",
          "MELD.\nThe Multimodal EmotionLines Dataset": "annotated with one of seven emotion categories:"
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "ten speakers, comprising 151 dialogues and 7,433",
          "MELD.\nThe Multimodal EmotionLines Dataset": "neutral, sadness, anger, fear, surprise, disgust, and"
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "utterances. Sessions 1 to 4 are used as the training",
          "MELD.\nThe Multimodal EmotionLines Dataset": "happiness. Word-level prosodic prominence is also"
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "set, while the last session is held out as the test",
          "MELD.\nThe Multimodal EmotionLines Dataset": "provided, making it\nthe first such dataset\nfor an"
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "set. Each utterance is annotated with one of six",
          "MELD.\nThe Multimodal EmotionLines Dataset": "African language. The dataset is gender-balanced,"
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "emotion labels: happy, sad, neutral, angry, excited,",
          "MELD.\nThe Multimodal EmotionLines Dataset": "featuring 308 speakers, and is split\ninto training,"
        },
        {
          "pages 13099–13110, Toronto, Canada. Association": "and frustrated.",
          "MELD.\nThe Multimodal EmotionLines Dataset": "validation, and test sets in a 7:1.5:1.5 ratio."
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MEmoR.\nThe MEmoR dataset\n(Shen\net\nal.,": "2020)\nconsists of 5,502 video clips\nand 8,536"
        },
        {
          "MEmoR.\nThe MEmoR dataset\n(Shen\net\nal.,": "person-level samples extracted from the sitcom The"
        },
        {
          "MEmoR.\nThe MEmoR dataset\n(Shen\net\nal.,": "Big Bang Theory, annotated with 14 fine-grained"
        },
        {
          "MEmoR.\nThe MEmoR dataset\n(Shen\net\nal.,": "emotions. Unlike most datasets focusing solely"
        },
        {
          "MEmoR.\nThe MEmoR dataset\n(Shen\net\nal.,": "on speakers, MEmoR includes both speakers and"
        },
        {
          "MEmoR.\nThe MEmoR dataset\n(Shen\net\nal.,": "non-speakers, even when modalities are partially"
        },
        {
          "MEmoR.\nThe MEmoR dataset\n(Shen\net\nal.,": "or completely missing. Each sample comprises a"
        },
        {
          "MEmoR.\nThe MEmoR dataset\n(Shen\net\nal.,": "target person, an emotion moment, and multimodal"
        },
        {
          "MEmoR.\nThe MEmoR dataset\n(Shen\net\nal.,": "inputs (text, audio, visual, and personality features),"
        },
        {
          "MEmoR.\nThe MEmoR dataset\n(Shen\net\nal.,": "making it a challenging benchmark for multimodal"
        },
        {
          "MEmoR.\nThe MEmoR dataset\n(Shen\net\nal.,": "emotion reasoning beyond direct recognition."
        },
        {
          "MEmoR.\nThe MEmoR dataset\n(Shen\net\nal.,": "AVEC.\nThe AVEC dataset (Schuller et al., 2012),"
        },
        {
          "MEmoR.\nThe MEmoR dataset\n(Shen\net\nal.,": "derived from the SEMAINE corpus (McKeown"
        },
        {
          "MEmoR.\nThe MEmoR dataset\n(Shen\net\nal.,": "et al., 2012), features human-agent interactions an-"
        },
        {
          "MEmoR.\nThe MEmoR dataset\n(Shen\net\nal.,": "notated with four continuous affective dimensions:"
        },
        {
          "MEmoR.\nThe MEmoR dataset\n(Shen\net\nal.,": "valence, arousal, expectancy, and power. While the"
        },
        {
          "MEmoR.\nThe MEmoR dataset\n(Shen\net\nal.,": "original labels are provided at 0.2-second intervals,"
        },
        {
          "MEmoR.\nThe MEmoR dataset\n(Shen\net\nal.,": "we aggregate them over each utterance to obtain"
        },
        {
          "MEmoR.\nThe MEmoR dataset\n(Shen\net\nal.,": "utterance-level annotations for emotion analysis."
        },
        {
          "MEmoR.\nThe MEmoR dataset\n(Shen\net\nal.,": "M-MELD.\nM-MELD (Ghosh et al., 2023) is a"
        },
        {
          "MEmoR.\nThe MEmoR dataset\n(Shen\net\nal.,": "multilingual extension of the MELD dataset, cre-"
        },
        {
          "MEmoR.\nThe MEmoR dataset\n(Shen\net\nal.,": "ated to support emotion recognition in conversa-"
        },
        {
          "MEmoR.\nThe MEmoR dataset\n(Shen\net\nal.,": "tions across different languages. While MELD is"
        },
        {
          "MEmoR.\nThe MEmoR dataset\n(Shen\net\nal.,": "an English-only multimodal dataset, M-MELD in-"
        },
        {
          "MEmoR.\nThe MEmoR dataset\n(Shen\net\nal.,": "cludes human-translated versions of the original"
        },
        {
          "MEmoR.\nThe MEmoR dataset\n(Shen\net\nal.,": "utterances in four additional\nlanguages: French,"
        },
        {
          "MEmoR.\nThe MEmoR dataset\n(Shen\net\nal.,": "Spanish, Greek,\nand Polish.\nThis multilingual"
        },
        {
          "MEmoR.\nThe MEmoR dataset\n(Shen\net\nal.,": "corpus retains the original multimodal structure"
        },
        {
          "MEmoR.\nThe MEmoR dataset\n(Shen\net\nal.,": "and emotion labels,\nenabling research in cross-"
        },
        {
          "MEmoR.\nThe MEmoR dataset\n(Shen\net\nal.,": "lingual and multimodal emotion analysis. By bal-"
        },
        {
          "MEmoR.\nThe MEmoR dataset\n(Shen\net\nal.,": "ancing high-resource and low-resource languages,"
        },
        {
          "MEmoR.\nThe MEmoR dataset\n(Shen\net\nal.,": "M-MELD offers a valuable benchmark for devel-"
        },
        {
          "MEmoR.\nThe MEmoR dataset\n(Shen\net\nal.,": "oping and evaluating multilingual ERC models."
        },
        {
          "MEmoR.\nThe MEmoR dataset\n(Shen\net\nal.,": "EmoryNLP.\nEmoryNLP (Zahiri and Choi, 2017)"
        },
        {
          "MEmoR.\nThe MEmoR dataset\n(Shen\net\nal.,": "is a textual emotion-labeled dataset derived from"
        },
        {
          "MEmoR.\nThe MEmoR dataset\n(Shen\net\nal.,": "the Friends TV series, containing over 12,000 utter-"
        },
        {
          "MEmoR.\nThe MEmoR dataset\n(Shen\net\nal.,": "ances across multi-party dialogues. Each utterance"
        },
        {
          "MEmoR.\nThe MEmoR dataset\n(Shen\net\nal.,": "is annotated with one of seven emotions: neutral,"
        },
        {
          "MEmoR.\nThe MEmoR dataset\n(Shen\net\nal.,": "joyful, peaceful, powerful, mad,\nsad, or scared,"
        },
        {
          "MEmoR.\nThe MEmoR dataset\n(Shen\net\nal.,": "offering a fine-grained resource for emotion recog-"
        },
        {
          "MEmoR.\nThe MEmoR dataset\n(Shen\net\nal.,": "nition in conversational settings."
        }
      ],
      "page": 18
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Sentiment analysis on a low-resource language dataset using multimodal representation learning and cross-lingual transfer learning",
      "authors": [
        "Aruna Gladys",
        "A Vetriselvi"
      ],
      "year": "2024",
      "venue": "Applied Soft Computing",
      "doi": "10.1016/j.asoc.2024.111553"
    },
    {
      "citation_id": "2",
      "title": "A systematic study on unimodal and multimodal human computer interface for emotion recognition",
      "authors": [
        "Akram Ahmad",
        "Vaishali Singh",
        "Kamal Upreti"
      ],
      "year": "2024",
      "venue": "Computing, Internet of Things and Data Analytics",
      "doi": "10.1007/978-3-031-53717-2_35"
    },
    {
      "citation_id": "3",
      "title": "2025a. Der-gcn: Dialog and event relation-aware graph convolutional neural network for multimodal dialog emotion recognition",
      "authors": [
        "Wei Ai",
        "Yuntao Shou",
        "Tao Meng",
        "Keqin Li"
      ],
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "doi": "10.1109/TNNLS.2024.3367940"
    },
    {
      "citation_id": "4",
      "title": "Revisiting multimodal emotion recognition in conversation from the perspective of graph spectrum",
      "authors": [
        "Wei Ai",
        "Fuchen Zhang",
        "Yuntao Shou",
        "Tao Meng",
        "Haowen Chen",
        "Keqin Li"
      ],
      "year": "2025",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": "10.1609/aaai.v39i11.33242"
    },
    {
      "citation_id": "5",
      "title": "Multimodal emotion detection with transfer learning",
      "authors": [
        "Amith Ananthram",
        "Karthik Kailash",
        "Jessica Saravanakumar",
        "Homayoon Huynh",
        "Beigi"
      ],
      "year": "2020",
      "venue": "Multimodal emotion detection with transfer learning",
      "arxiv": "arXiv:2011.07065"
    },
    {
      "citation_id": "6",
      "title": "Survey on multimodal approaches to emotion recognition",
      "authors": [
        "A Gladys",
        "V Vetriselvi"
      ],
      "year": "2023",
      "venue": "Neurocomputing",
      "doi": "10.1016/j.neucom.2023.126693"
    },
    {
      "citation_id": "7",
      "title": "Multimodal fusion for multimedia analysis: a survey",
      "authors": [
        "K Pradeep",
        "M Atrey",
        "Abdulmotaleb Anwar Hossain",
        "Mohan Saddik",
        "Kankanhalli"
      ],
      "year": "2010",
      "venue": "Multimedia Systems",
      "doi": "10.1007/s00530-010-0182-0"
    },
    {
      "citation_id": "8",
      "title": "Multimodal emotion recognition with deep learning: Advancements, challenges, and future directions",
      "authors": [
        "A Geetha",
        "T Mala",
        "D Priyanka"
      ],
      "year": "2024",
      "venue": "Information Fusion",
      "doi": "10.1016/j.inffus.2023.102218"
    },
    {
      "citation_id": "9",
      "title": "Emotion recognition from multimodal physiological signals for emotion aware healthcare systems",
      "authors": [
        "Deger Ayata",
        "Yusuf Yaslan",
        "Mustafa Kamasak"
      ],
      "year": "2020",
      "venue": "Journal of Medical and Biological Engineering",
      "doi": "10.1007/s40846-019-00505-7"
    },
    {
      "citation_id": "10",
      "title": "Multimodal language analysis in the wild: CMU-MOSEI dataset and interpretable dynamic fusion graph",
      "authors": [
        "Amirali Bagher Zadeh",
        "Paul Liang",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P18-1208"
    },
    {
      "citation_id": "11",
      "title": "Openface: An open source facial behavior analysis toolkit",
      "authors": [
        "Tadas Baltrušaitis",
        "Peter Robinson",
        "Louis-Philippe Morency"
      ],
      "year": "2016",
      "venue": "2016 IEEE Winter Conference on Applications of Computer Vision (WACV)",
      "doi": "10.1109/WACV.2016.7477553"
    },
    {
      "citation_id": "12",
      "title": "Toward zero-shot speech emotion recognition using llms in the absence of target data",
      "authors": [
        "Su Bo-Hao",
        "Shreya Upadhyay",
        "Lee Chi-Chun"
      ],
      "year": "2025",
      "venue": "ICASSP 2025 -2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
      "doi": "10.1109/ICASSP49660.2025.10889305"
    },
    {
      "citation_id": "13",
      "title": "Iemocap: interactive emotional dyadic motion capture database. Language Resources and Evaluation",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth Narayanan"
      ],
      "year": "2008",
      "venue": "Iemocap: interactive emotional dyadic motion capture database. Language Resources and Evaluation",
      "doi": "10.1007/s10579-008-9076-6"
    },
    {
      "citation_id": "14",
      "title": "Qwen2-audio technical report",
      "authors": [
        "Yunfei Chu",
        "Jin Xu",
        "Qian Yang",
        "Haojie Wei",
        "Xipin Wei",
        "Zhifang Guo",
        "Yichong Leng",
        "Yuanjun Lv",
        "Jinzheng He",
        "Junyang Lin"
      ],
      "year": "2024",
      "venue": "Qwen2-audio technical report",
      "arxiv": "arXiv:2407.10759"
    },
    {
      "citation_id": "15",
      "title": "Pankaj Wasnik, and Naoyuki Onoe. 2022. M2fnet: Multi-modal fusion network for emotion recognition in conversation",
      "authors": [
        "Purbayan Vishal Chudasama",
        "Ashish Kar",
        "Nirmesh Gudmalwar",
        "Shah"
      ],
      "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
      "doi": "10.1109/CVPRW56347.2022.00511"
    },
    {
      "citation_id": "16",
      "title": "Covarep -a collaborative voice analysis repository for speech technologies",
      "authors": [
        "Gilles Degottex",
        "John Kane",
        "Thomas Drugman",
        "Tuomo Raitio",
        "Stefan Scherer"
      ],
      "year": "2014",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "17",
      "title": "A survey of textual emotion recognition and its challenges",
      "authors": [
        "Jiawen Deng",
        "Fuji Ren"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2021.3053275"
    },
    {
      "citation_id": "18",
      "title": "Deep learning-based multi-head selfattention model for human epilepsy identification from eeg signal for biomedical traits",
      "authors": [
        "Ashit Kumar Dutta",
        "Mohan Raparthi",
        "Mahmood Alsaadi",
        "Mohammed Bhatt",
        "Sarath Babu Dodda",
        "G Prashant",
        "Mukta Sandhu",
        "Jagdish Chandra"
      ],
      "year": "2024",
      "venue": "Deep learning-based multi-head selfattention model for human epilepsy identification from eeg signal for biomedical traits",
      "doi": "10.1007/s11042-024-18918-1"
    },
    {
      "citation_id": "19",
      "title": "Llm supervised pre-training for multimodal emotion recognition in conversations",
      "authors": [
        "Soumya Dutta",
        "Sriram Ganapathy"
      ],
      "year": "2025",
      "venue": "Llm supervised pre-training for multimodal emotion recognition in conversations",
      "arxiv": "arXiv:2501.11468"
    },
    {
      "citation_id": "20",
      "title": "Facial signs of emotional experience",
      "authors": [
        "Paul Ekman",
        "Sonia Wallace V Freisen",
        "Ancoli"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology",
      "doi": "10.1037/h0077722"
    },
    {
      "citation_id": "21",
      "title": "Opensmile: the munich versatile and fast opensource audio feature extractor",
      "authors": [
        "Florian Eyben",
        "Martin Wöllmer",
        "Björn Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM International Conference on Multimedia, MM '10",
      "doi": "10.1145/1873951.1874246"
    },
    {
      "citation_id": "22",
      "title": "Cross-modal representation flattening for multi-modal domain generalization",
      "authors": [
        "Yunfeng Fan",
        "Wenchao Xu",
        "Haohao Wang",
        "Song Guo"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "23",
      "title": "Cross-modal context fusion and adaptive graph convolutional network for multimodal conversational emotion recognition",
      "authors": [
        "Junwei Feng",
        "Xueyan Fan"
      ],
      "year": "2025",
      "venue": "Cross-modal context fusion and adaptive graph convolutional network for multimodal conversational emotion recognition",
      "arxiv": "arXiv:2501.15063"
    },
    {
      "citation_id": "24",
      "title": "Hypergraph neural networks",
      "authors": [
        "Yifan Feng",
        "Haoxuan You",
        "Zizhao Zhang",
        "Rongrong Ji",
        "Yue Gao"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": "10.1609/aaai.v33i01.33013558"
    },
    {
      "citation_id": "25",
      "title": "Emotion recognition in conversations: A survey focusing on context, speaker dependencies, and fusion methods",
      "authors": [
        "Yao Fu",
        "Shaoyang Yuan",
        "Chi Zhang",
        "Juan Cao"
      ],
      "year": "2023",
      "venue": "Electronics",
      "doi": "10.3390/electronics12224714"
    },
    {
      "citation_id": "26",
      "title": "Ckerc: Joint large language models with commonsense knowledge for emotion recognition in conversation",
      "authors": [
        "Yumeng Fu"
      ],
      "year": "2024",
      "venue": "Ckerc: Joint large language models with commonsense knowledge for emotion recognition in conversation",
      "arxiv": "arXiv:2403.07260"
    },
    {
      "citation_id": "27",
      "title": "2025a. LaERC-S: Improving LLM-based emotion recognition in conversation with speaker characteristics",
      "authors": [
        "Yumeng Fu",
        "Junjie Wu",
        "Zhongjie Wang",
        "Meishan Zhang",
        "Lili Shan",
        "Yulin Wu",
        "Bingquan Liu"
      ],
      "venue": "Proceedings of the 31st International Conference on Computational Linguistics"
    },
    {
      "citation_id": "28",
      "title": "Bemerc: Behavior-aware mllm-based framework for multimodal emotion recognition in conversation",
      "authors": [
        "Yumeng Fu",
        "Junjie Wu",
        "Zhongjie Wang",
        "Meishan Zhang",
        "Yulin Wu",
        "Bingquan Liu"
      ],
      "year": "2025",
      "venue": "Bemerc: Behavior-aware mllm-based framework for multimodal emotion recognition in conversation",
      "arxiv": "arXiv:2503.23990"
    },
    {
      "citation_id": "29",
      "title": "A survey of dialogic emotion analysis: Developments, approaches and perspectives",
      "authors": [
        "Chenquan Gan",
        "Jiahao Zheng",
        "Qingyi Zhu",
        "Yang Cao",
        "Ye Zhu"
      ],
      "year": "2024",
      "venue": "Pattern Recognition",
      "doi": "10.1016/j.patcog.2024.110794"
    },
    {
      "citation_id": "30",
      "title": "Multimodal sentiment analysis: A systematic review of history, datasets, multimodal fusion methods, applications, challenges and future directions",
      "authors": [
        "Ankita Gandhi",
        "Kinjal Adhvaryu",
        "Soujanya Poria",
        "Erik Cambria",
        "Amir Hussain"
      ],
      "year": "2023",
      "venue": "Information Fusion",
      "doi": "10.1016/j.inffus.2022.09.025"
    },
    {
      "citation_id": "31",
      "title": "Enhanced experts with uncertainty-aware routing for multimodal sentiment analysis",
      "authors": [
        "Zixian Gao",
        "Disen Hu",
        "Xun Jiang",
        "Huimin Lu",
        "Heng Tao Shen",
        "Xing Xu"
      ],
      "year": "2024",
      "venue": "Proceedings of the 32nd ACM International Conference on Multimedia, MM 2024",
      "doi": "10.1145/3664647.3680949"
    },
    {
      "citation_id": "32",
      "title": "Multi-lingual speech emotion recognition: Investigating similarities between english and german languages",
      "authors": [
        "Devi Ghaayathri",
        "Kolluru Likhitha",
        "J Akshaya",
        "Rfj Gokul",
        "G Jyothish Lal"
      ],
      "year": "2024",
      "venue": "2024 International Conference on Advances in Computing, Communication and Applied Informatics (ACCAI)",
      "doi": "10.1109/ACCAI61061.2024.10601715"
    },
    {
      "citation_id": "33",
      "title": "COSMIC: COmmonSense knowledge for eMotion identification in conversations",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder"
      ],
      "year": "2020",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020",
      "doi": "10.18653/v1/2020.findings-emnlp.224"
    },
    {
      "citation_id": "34",
      "title": "Di-alogueGCN: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Soujanya Poria",
        "Niyati Chhaya",
        "Alexander Gelbukh"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
      "doi": "10.18653/v1/D19-1015"
    },
    {
      "citation_id": "35",
      "title": "M-meld: A multilingual multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Sreyan Ghosh",
        "Utkarsh Ramaneswaran",
        "Harshvardhan Tyagi",
        "Samden Srivastava",
        "S Lepcha",
        "Dinesh Sakshi",
        "Manocha"
      ],
      "year": "2023",
      "venue": "M-meld: A multilingual multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:2203.16799"
    },
    {
      "citation_id": "36",
      "title": "Eliciting rich positive emotions in dialogue generation",
      "authors": [
        "Ziwei Gong",
        "Min Qingkai",
        "Yue Zhang"
      ],
      "year": "2023",
      "venue": "Proceedings of the First Workshop on Social Influence in Conversations",
      "doi": "10.18653/v1/2023.sicon-1.1"
    },
    {
      "citation_id": "37",
      "title": "A mapping on current classifying categories of emotions used in multimodal models for emotion recognition",
      "authors": [
        "Ziwei Gong",
        "Muyin Yao",
        "Xinyi Hu",
        "Xiaoning Zhu",
        "Julia Hirschberg"
      ],
      "year": "2024",
      "venue": "Proceedings of the 18th Linguistic Annotation Workshop (LAW-XVIII)"
    },
    {
      "citation_id": "38",
      "title": "Speakeraware cognitive network with cross-modal attention for multimodal emotion recognition in conversation",
      "authors": [
        "Lili Guo",
        "Yikang Song",
        "Shifei Ding"
      ],
      "year": "2024",
      "venue": "Knowledge-Based Systems",
      "doi": "10.1016/j.knosys.2024.111969"
    },
    {
      "citation_id": "39",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Amir Zadeh",
        "Erik Cambria",
        "Louis-Philippe Morency",
        "Roger Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference of the North American Chapter",
      "doi": "10.18653/v1/N18-1193"
    },
    {
      "citation_id": "40",
      "title": "Efficient modality selection in multimodal learning",
      "authors": [
        "Yifei He",
        "Runxiang Cheng",
        "Gargi Balasubramaniam",
        "Hubert Yao-Hung",
        "Han Tsai",
        "Zhao"
      ],
      "year": "2024",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "41",
      "title": "Advances in multimodal emotion recognition based on brain-computer interfaces",
      "authors": [
        "Zhipeng He",
        "Zina Li",
        "Fuzhou Yang",
        "Lei Wang",
        "Jingcong Li",
        "Chengju Zhou",
        "Jiahui Pan"
      ],
      "year": "2020",
      "venue": "Brain Sciences",
      "doi": "10.3390/brainsci10100687"
    },
    {
      "citation_id": "42",
      "title": "Long short-term memory",
      "authors": [
        "Sepp Hochreiter",
        "Jürgen Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural Computation",
      "doi": "10.1162/neco.1997.9.8.1735"
    },
    {
      "citation_id": "43",
      "title": "Tfcnn-bigru with self-attention mechanism for automatic human emotion recognition using multi-channel eeg data",
      "authors": [
        "H Essam",
        "Asmaa Houssein",
        "Hammad",
        "Abdel Nagwan",
        "Manal Samee",
        "Abdelmgeid Abdullah Alohali",
        "Ali"
      ],
      "year": "2024",
      "venue": "Cluster Computing",
      "doi": "10.1007/s10586-024-04590-5"
    },
    {
      "citation_id": "44",
      "title": "Emotion-Lines: An emotion corpus of multi-party conversations",
      "authors": [
        "Chao-Chun",
        "Sheng-Yeh Hsu",
        "Chuan-Chun Chen",
        "Ting-Hao Kuo",
        "Lun-Wei Huang",
        "Ku"
      ],
      "year": "2018",
      "venue": "Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)"
    },
    {
      "citation_id": "45",
      "title": "2021a. Dia-logueCRN: Contextual reasoning networks for emotion recognition in conversations",
      "authors": [
        "Dou Hu",
        "Lingwei Wei",
        "Xiaoyong Huai"
      ],
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
      "doi": "10.18653/v1/2021.acl-long.547"
    },
    {
      "citation_id": "46",
      "title": "UniMSE: Towards unified multimodal sentiment analysis and emotion recognition",
      "authors": [
        "Guimin Hu",
        "Ting-En Lin",
        "Yi Zhao",
        "Guangming Lu",
        "Yuchuan Wu",
        "Yongbin Li"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/2022.emnlp-main.534"
    },
    {
      "citation_id": "47",
      "title": "2021b. MMGCN: Multimodal fusion via deep graph convolution network for emotion recognition in conversation",
      "authors": [
        "Jingwen Hu",
        "Yuchen Liu",
        "Jinming Zhao",
        "Qin Jin"
      ],
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
      "doi": "10.18653/v1/2021.acl-long.440"
    },
    {
      "citation_id": "48",
      "title": "Densely connected convolutional networks",
      "authors": [
        "Gao Huang",
        "Zhuang Liu",
        "Laurens Van Der Maaten",
        "Kilian Weinberger"
      ],
      "year": "2017",
      "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/cvpr.2017.243"
    },
    {
      "citation_id": "49",
      "title": "2024a. Dynamic hypergraph convolutional network for multimodal sentiment analysis",
      "authors": [
        "Jian Huang",
        "Yuanyuan Pu",
        "Dongming Zhou",
        "Jinde Cao",
        "Jinjing Gu",
        "Zhengpeng Zhao",
        "Dan Xu"
      ],
      "venue": "Neurocomputing",
      "doi": "10.1016/j.neucom.2023.126992"
    },
    {
      "citation_id": "50",
      "title": "Real-time application and effect evaluation of multimodal emotion recognition model in online learning",
      "authors": [
        "Ying Huang"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 10th International Conference on Computing and Data Engineering, ICCDE '24",
      "doi": "10.1145/3641181.3641183"
    },
    {
      "citation_id": "51",
      "title": "Mm-nodeformer: Node transformer multimodal fusion for emotion recognition in conversation",
      "authors": [
        "Zilong Huang",
        "Man-Wai Mak",
        "Kong Aik"
      ],
      "year": "2024",
      "venue": "Interspeech 2024",
      "doi": "10.21437/Interspeech.2024-538"
    },
    {
      "citation_id": "52",
      "title": "Fatmah Alanazi, Muhammad Waseem Iqbal, and M. Usman Ashraf. 2025. Low-resource mobilebert for emotion recognition in imbalanced text datasets mitigating challenges with limited resources",
      "authors": [
        "Muhammad Hussain",
        "Caikou Chen",
        "Sami Albouq",
        "Khlood Shinan"
      ],
      "venue": "PLOS ONE",
      "doi": "10.1371/journal.pone.0312867"
    },
    {
      "citation_id": "53",
      "title": "Enhanced multimodal emotion recognition in healthcare analytics: A deep learning based model-level fusion approach",
      "authors": [
        "Milon Md",
        "Sheikh Islam",
        "Fakhri Nooruddin",
        "Ghulam Karray",
        "Muhammad"
      ],
      "year": "2024",
      "venue": "Biomedical Signal Processing and Control",
      "doi": "10.1016/j.bspc.2024.106241"
    },
    {
      "citation_id": "54",
      "title": "Real-time emotion recognition via attention gated hierarchical memory network",
      "authors": [
        "Wenxiang Jiao",
        "Michael Lyu",
        "Irwin King"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": "10.1609/aaai.v34i05.6309"
    },
    {
      "citation_id": "55",
      "title": "Exploring contactless techniques in multimodal emotion recognition: insights into diverse applications, challenges, solutions, and prospects",
      "authors": [
        "Umair Ali Khan",
        "Qianru Xu",
        "Yang Liu",
        "Altti Lagstedt",
        "Ari Alamäki",
        "Janne Kauttonen"
      ],
      "year": "2024",
      "venue": "Multimedia Systems",
      "doi": "10.1007/s00530-024-01302-2"
    },
    {
      "citation_id": "56",
      "title": "Enhancing emotion recognition using multimodal fusion of physiological, environmental, personal data",
      "authors": [
        "Hakpyeong Kim",
        "Taehoon Hong"
      ],
      "year": "2024",
      "venue": "Expert Systems with Applications",
      "doi": "10.1016/j.eswa.2024.123723"
    },
    {
      "citation_id": "57",
      "title": "Convolutional neural networks for sentence classification",
      "authors": [
        "Yoon Kim"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
      "doi": "10.3115/v1/D14-1181"
    },
    {
      "citation_id": "58",
      "title": "Emotion analysis of twitter using opinion mining",
      "authors": [
        "Akshi Kumar",
        "Prakhar Dogra",
        "Vikrant Dabas"
      ],
      "year": "2015",
      "venue": "2015 Eighth International Conference on Contemporary Computing",
      "doi": "10.1109/ic3.2015.7346694"
    },
    {
      "citation_id": "59",
      "title": "Instructerc: Reforming emotion recognition in conversation with multi-task retrieval-augmented large language models",
      "authors": [
        "Shanglin Lei",
        "Guanting Dong",
        "Xiaoping Wang",
        "Keheng Wang",
        "Runqi Qiao",
        "Sirui Wang"
      ],
      "year": "2023",
      "venue": "Instructerc: Reforming emotion recognition in conversation with multi-task retrieval-augmented large language models",
      "arxiv": "arXiv:2309.11911"
    },
    {
      "citation_id": "60",
      "title": "2024a. Cfn-esa: A cross-modal fusion network with emotion-shift awareness for dialogue emotion recognition",
      "authors": [
        "Jiang Li",
        "Xiaoping Wang",
        "Yingjian Liu",
        "Zhigang Zeng"
      ],
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2024.3389453"
    },
    {
      "citation_id": "61",
      "title": "Graphmft: A graph network based multimodal fusion technique for emotion recognition in conversation",
      "authors": [
        "Jiang Li",
        "Xiaoping Wang",
        "Guoqing Lv",
        "Zhigang Zeng"
      ],
      "year": "2023",
      "venue": "Neurocomputing",
      "doi": "10.1016/j.neucom.2023.126427"
    },
    {
      "citation_id": "62",
      "title": "Tracing intricate cues in dialogue: Joint graph structure and sentiment dynamics for multimodal emotion recognition",
      "authors": [
        "Jiang Li",
        "Xiaoping Wang",
        "Zhigang Zeng"
      ],
      "year": "2024",
      "venue": "Tracing intricate cues in dialogue: Joint graph structure and sentiment dynamics for multimodal emotion recognition",
      "arxiv": "arXiv:2407.21536"
    },
    {
      "citation_id": "63",
      "title": "Multimodal emotion recognition in conversation based on hypergraphs",
      "authors": [
        "Jiaze Li",
        "Hongyan Mei",
        "Liyun Jia",
        "Xing Zhang"
      ],
      "year": "2023",
      "venue": "Electronics",
      "doi": "10.3390/electronics12224703"
    },
    {
      "citation_id": "64",
      "title": "Multimodal alignment and fusion: A survey",
      "authors": [
        "Songtao Li",
        "Hao Tang"
      ],
      "year": "2024",
      "venue": "Multimodal alignment and fusion: A survey",
      "arxiv": "arXiv:2411.17040"
    },
    {
      "citation_id": "65",
      "title": "Improving context understanding in multimodal large language models via multimodal composition learning",
      "authors": [
        "Wei Li",
        "Hehe Fan",
        "Yongkang Wong",
        "Yi Yang",
        "Mohan Kankanhalli"
      ],
      "year": "2024",
      "venue": "Proceedings of the 41st International Conference on Machine Learning"
    },
    {
      "citation_id": "66",
      "title": "2023c. Unisa: Unified generative framework for sentiment analysis",
      "authors": [
        "Zaijing Li",
        "Ting-En Lin",
        "Yuchuan Wu",
        "Meng Liu",
        "Fengxiao Tang",
        "Ming Zhao",
        "Yongbin Li"
      ],
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia, MM '23",
      "doi": "10.1145/3581783.3612336"
    },
    {
      "citation_id": "67",
      "title": "EmoCaps: Emotion capsule based model for conversational emotion recognition",
      "authors": [
        "Zaijing Li",
        "Fengxiao Tang",
        "Ming Zhao",
        "Yusen Zhu"
      ],
      "year": "2022",
      "venue": "Findings of the Association for Computational Linguistics: ACL 2022",
      "doi": "10.18653/v1/2022.findings-acl.126"
    },
    {
      "citation_id": "68",
      "title": "Gcnet: Graph completion network for incomplete multimodal learning in conversation",
      "authors": [
        "Zheng Lian",
        "Lan Chen",
        "Licai Sun",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "doi": "10.1109/TPAMI.2023.3234553"
    },
    {
      "citation_id": "69",
      "title": "Ctnet: Conversational transformer network for emotion recognition",
      "authors": [
        "Zheng Lian",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing",
      "doi": "10.1109/TASLP.2021.3049898"
    },
    {
      "citation_id": "70",
      "title": "2024a. Graph neural networks in eeg-based emotion recognition: a survey",
      "authors": [
        "Chenyu Liu",
        "Xinliang Zhou",
        "Yihao Wu",
        "Ruizhi Yang",
        "Zhongruo Wang",
        "Liming Zhai",
        "Ziyu Jia",
        "Yang Liu"
      ],
      "venue": "2024a. Graph neural networks in eeg-based emotion recognition: a survey",
      "arxiv": "arXiv:2402.01138"
    },
    {
      "citation_id": "71",
      "title": "2024b. Adaptive deep graph convolutional network for dialogical speech emotion recognition",
      "authors": [
        "Jiaxing Liu",
        "Sheng Wu",
        "Longbiao Wang",
        "Jianwu Dang"
      ],
      "venue": "Man-Machine Speech Communication",
      "doi": "10.1007/978-981-97-0601-3_21"
    },
    {
      "citation_id": "72",
      "title": "Revisiting graph contrastive learning from the perspective of graph spectrum",
      "authors": [
        "Nian Liu",
        "Xiao Wang",
        "Deyu Bo",
        "Chuan Shi",
        "Jian Pei"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "73",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach"
    },
    {
      "citation_id": "74",
      "title": "2024c. Smile upon the face but sadness in the eyes: Emotion recognition based on facial expressions and eye behaviors",
      "authors": [
        "Yuanyuan Liu",
        "Lin Wei",
        "Kejun Liu",
        "Yibing Zhan",
        "Zijing Chen",
        "Zhe Chen",
        "Shiguang Shan"
      ],
      "venue": "2024c. Smile upon the face but sadness in the eyes: Emotion recognition based on facial expressions and eye behaviors",
      "arxiv": "arXiv:2411.05879"
    },
    {
      "citation_id": "75",
      "title": "Hierarchical gating networks for sequential recommendation",
      "authors": [
        "Chen Ma",
        "Peng Kang",
        "Xue Liu"
      ],
      "year": "2019",
      "venue": "Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD '19",
      "doi": "10.1145/3292500.3330984"
    },
    {
      "citation_id": "76",
      "title": "2024a. A transformer-based model with self-distillation for multimodal emotion recognition in conversations",
      "authors": [
        "Hui Ma",
        "Jian Wang",
        "Hongfei Lin",
        "Bo Zhang",
        "Yijia Zhang",
        "Bo Xu"
      ],
      "venue": "IEEE Transactions on Multimedia",
      "doi": "10.1109/TMM.2023.3271019"
    },
    {
      "citation_id": "77",
      "title": "2024b. Learning modality knowledge alignment for cross-modality transfer",
      "authors": [
        "Wenxuan Ma",
        "Shuang Li",
        "Lincan Cai",
        "Jingxuan Kang"
      ],
      "venue": "Proceedings of the 41st International Conference on Machine Learning"
    },
    {
      "citation_id": "78",
      "title": "Multimodal boosting: Addressing noisy modalities and identifying modality contribution",
      "authors": [
        "Sijie Mai",
        "Ya Sun",
        "Aolin Xiong",
        "Ying Zeng",
        "Haifeng Hu"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Multimedia",
      "doi": "10.1109/TMM.2023.3306489"
    },
    {
      "citation_id": "79",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "Navonil Majumder",
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Rada Mihalcea",
        "Alexander Gelbukh",
        "Erik Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": "10.1609/aaai.v33i01.33016818"
    },
    {
      "citation_id": "80",
      "title": "DialogueTRM: Exploring multimodal emotional dynamics in a conversation",
      "authors": [
        "Yuzhao Mao",
        "Guang Liu",
        "Xiaojie Wang",
        "Weiguo Gao",
        "Xuan Li"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021",
      "doi": "10.18653/v1/2021.findings-emnlp.229"
    },
    {
      "citation_id": "81",
      "title": "Modselect: Automatic modality selection for synthetic-to-real domain generalization",
      "authors": [
        "Zdravko Marinov",
        "Alina Roitberg",
        "David Schneider",
        "Rainer Stiefelhagen"
      ],
      "year": "2023",
      "venue": "Computer Vision -ECCV 2022",
      "doi": "10.1007/978-3-031-25085-9_19"
    },
    {
      "citation_id": "82",
      "title": "",
      "authors": [
        "Workshops"
      ],
      "venue": ""
    },
    {
      "citation_id": "83",
      "title": "Eric Battenberg, and Oriol Nieto. 2015. librosa: Audio and music signal analysis in python",
      "authors": [
        "Brian Mcfee",
        "Colin Raffel",
        "Dawen Liang",
        "P Daniel",
        "Matt Ellis",
        "Mcvicar"
      ],
      "venue": "SciPy"
    },
    {
      "citation_id": "84",
      "title": "The semaine database: Annotated multimodal records of emotionally colored conversations between a person and a limited agent",
      "authors": [
        "Gary Mckeown",
        "Michel Valstar",
        "Roddy Cowie",
        "Maja Pantic",
        "Marc Schroder"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/T-AFFC.2011.20"
    },
    {
      "citation_id": "85",
      "title": "Emotion recognition framework using multiple modalities for an effective human-computer interaction",
      "authors": [
        "Anam Moin",
        "Farhan Aadil",
        "Zeeshan Ali",
        "Dongwann Kang"
      ],
      "year": "2023",
      "venue": "The Journal of Supercomputing",
      "doi": "10.1007/s11227-022-05026-w"
    },
    {
      "citation_id": "86",
      "title": "Multimodal emotion recognition using cross-modal attention and 1d convolutional neural networks",
      "authors": [
        "D Krishna",
        "Ankita Patil"
      ],
      "year": "2020",
      "venue": "Multimodal emotion recognition using cross-modal attention and 1d convolutional neural networks",
      "doi": "10.21437/Interspeech.2020-1190"
    },
    {
      "citation_id": "87",
      "title": "Conversation understanding using relational temporal graph neural networks with auxiliary cross-modality interaction",
      "authors": [
        "Cam-Van Thi Nguyen",
        "Anh-Tuan Mai",
        "The-Son Le",
        "Hai-Dang Kieu",
        "Duc-Trong Le"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/2023.emnlp-main.937"
    },
    {
      "citation_id": "88",
      "title": "Survey on emotional body gesture recognition",
      "authors": [
        "Fatemeh Noroozi",
        "Adrian Ciprian",
        "Dorota Corneanu",
        "Tomasz Kamińska",
        "Sergio Sapiński",
        "Gholamreza Escalera",
        "Anbarjafari"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2018.2874986"
    },
    {
      "citation_id": "89",
      "title": "A survey on deep learning for textual emotion analysis in social networks",
      "authors": [
        "Sancheng Peng",
        "Lihong Cao",
        "Yongmei Zhou",
        "Zhouhao Ouyang",
        "Aimin Yang",
        "Xinguang Li",
        "Weijia Jia",
        "Shui Yu"
      ],
      "year": "2022",
      "venue": "Digital Communications and Networks",
      "doi": "10.1016/j.dcan.2021.10.003"
    },
    {
      "citation_id": "90",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P17-1081"
    },
    {
      "citation_id": "91",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Gautam Naik"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P19-1050"
    },
    {
      "citation_id": "92",
      "title": "Emotion recognition in conversation: Research challenges, datasets, and recent advances",
      "authors": [
        "Soujanya Poria",
        "Navonil Majumder",
        "Rada Mihalcea",
        "Eduard Hovy"
      ],
      "year": "2019",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2019.2929050"
    },
    {
      "citation_id": "93",
      "title": "Zero-shot video emotion recognition via multimodal protagonist-aware transformer network",
      "authors": [
        "Fan Qi",
        "Xiaoshan Yang",
        "Changsheng Xu"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th ACM International Conference on Multimedia, MM '21",
      "doi": "10.1145/3474085.3475647"
    },
    {
      "citation_id": "94",
      "title": "Multimodal emotion recognition: A comprehensive review, trends, and challenges. WIREs Data Mining and Knowledge Discovery",
      "authors": [
        "Manju Priya",
        "Arthanarisamy Ramaswamy",
        "Suja Palaniswamy"
      ],
      "year": "2024",
      "venue": "Multimodal emotion recognition: A comprehensive review, trends, and challenges. WIREs Data Mining and Knowledge Discovery",
      "doi": "10.1002/widm.1563"
    },
    {
      "citation_id": "95",
      "title": "Sentence-BERT: Sentence embeddings using Siamese BERTnetworks",
      "authors": [
        "Nils Reimers",
        "Iryna Gurevych"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
      "doi": "10.18653/v1/D19-1410"
    },
    {
      "citation_id": "96",
      "title": "Akan cinematic emotions (ACE): A multimodal multi-party dataset for emotion recognition in movie dialogues",
      "authors": [
        "David Sasu",
        "Zehui Wu",
        "Ziwei Gong",
        "Run Chen",
        "Pengyuan Shi",
        "Lin Ai",
        "Julia Hirschberg",
        "Natalie Schluter"
      ],
      "year": "2025",
      "venue": "Findings of the Association for Computational Linguistics: ACL 2025",
      "doi": "10.18653/v1/2025.findings-acl.510"
    },
    {
      "citation_id": "97",
      "title": "The graph neural network model",
      "authors": [
        "Franco Scarselli",
        "Marco Gori",
        "Ah Chung Tsoi",
        "Markus Hagenbuchner",
        "Gabriele Monfardini"
      ],
      "year": "2009",
      "venue": "IEEE Transactions on Neural Networks",
      "doi": "10.1109/TNN.2008.2005605"
    },
    {
      "citation_id": "98",
      "title": "AVEC 2012: the continuous audio/visual emotion challenge -an introduction",
      "authors": [
        "W Björn",
        "Michel Schuller",
        "Roddy Valstar",
        "Maja Cowie",
        "Pantic"
      ],
      "year": "2012",
      "venue": "International Conference on Multimodal Interaction, ICMI '12",
      "doi": "10.1145/2388676.2388758"
    },
    {
      "citation_id": "99",
      "title": "Memor: A dataset for multimodal emotion reasoning in videos",
      "authors": [
        "Guangyao Shen",
        "Xin Wang",
        "Xuguang Duan",
        "Hongzhi Li",
        "Wenwu Zhu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia, MM '20",
      "doi": "10.1145/3394171.3413909"
    },
    {
      "citation_id": "100",
      "title": "MultiEMO: An attention-based correlation-aware multimodal fusion framework for emotion recognition in conversations",
      "authors": [
        "Tao Shi",
        "Shao-Lun Huang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2023.acl-long.824"
    },
    {
      "citation_id": "101",
      "title": "Efficient long-distance latent relation-aware graph neural network for multi-modal emotion recognition in conversations",
      "authors": [
        "Yuntao Shou",
        "Wei Ai",
        "Jiayi Du",
        "Tao Meng",
        "Haiyan Liu",
        "Nan Yin"
      ],
      "year": "2024",
      "venue": "Efficient long-distance latent relation-aware graph neural network for multi-modal emotion recognition in conversations",
      "arxiv": "arXiv:2407.00119"
    },
    {
      "citation_id": "102",
      "title": "Dynamic graph neural ODE network for multi-modal emotion recognition in conversation",
      "authors": [
        "Yuntao Shou",
        "Tao Meng",
        "Wei Ai",
        "Keqin Li"
      ],
      "year": "2025",
      "venue": "Proceedings of the 31st International Conference on Computational Linguistics"
    },
    {
      "citation_id": "103",
      "title": "Low-rank approximation for sparse attention in multimodal llms",
      "authors": [
        "Lin Song",
        "Yukang Chen",
        "Shuai Yang",
        "Xiaohan Ding",
        "Yixiao Ge",
        "Ying-Cong Chen",
        "Ying Shan"
      ],
      "year": "2024",
      "venue": "2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR52733.2024.01306"
    },
    {
      "citation_id": "104",
      "title": "Multimodal sentiment recognition driven by feature fusion strategy for social media comments on sudden natural disasters",
      "authors": [
        "Yanyuan Su",
        "Cuijuan Han",
        "Yaming Zhang",
        "Haiou Liu"
      ],
      "year": "2024",
      "venue": "Multimodal sentiment recognition driven by feature fusion strategy for social media comments on sudden natural disasters"
    },
    {
      "citation_id": "105",
      "title": "Toward a dialogue system using a large language model to recognize user emotions with a camera",
      "authors": [
        "Hiroki Tanioka",
        "Tetsushi Ueta",
        "Masahiko Sano"
      ],
      "year": "2024",
      "venue": "Toward a dialogue system using a large language model to recognize user emotions with a camera",
      "arxiv": "arXiv:2408.07982"
    },
    {
      "citation_id": "106",
      "title": "Learning Spatiotemporal Features with 3D Convolutional Networks",
      "authors": [
        "Du Tran",
        "Lubomir Bourdev",
        "Rob Fergus",
        "Lorenzo Torresani",
        "Manohar Paluri"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Computer Vision (ICCV)",
      "doi": "10.1109/ICCV.2015.510"
    },
    {
      "citation_id": "107",
      "title": "Text-centric alignment for multi-modality learning",
      "authors": [
        "Yun-Da Tsai",
        "Ting-Yu Yen",
        "Pei-Fu Guo",
        "Zhe-Yan Li",
        "Shou-De Lin"
      ],
      "year": "2024",
      "venue": "Text-centric alignment for multi-modality learning",
      "arxiv": "arXiv:2402.08086"
    },
    {
      "citation_id": "108",
      "title": "Multimodal emotion recognition using visual, vocal and physiological signals: A review",
      "authors": [
        "Gustave Udahemuka",
        "Karim Djouani",
        "Anish Kurien"
      ],
      "year": "2024",
      "venue": "Applied Sciences",
      "doi": "10.3390/app14178071"
    },
    {
      "citation_id": "109",
      "title": "Effective context modeling framework for emotion recognition in conversations",
      "authors": [
        "Tran Cuong",
        "Thanh Van",
        "Van Tran",
        "Truong Nguyen",
        "Hy Son"
      ],
      "year": "2025",
      "venue": "ICASSP 2025 -2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
      "doi": "10.1109/ICASSP49660.2025.10888112"
    },
    {
      "citation_id": "110",
      "title": "Representation learning with contrastive predictive coding",
      "authors": [
        "Aaron Van Den Oord",
        "Yazhe Li",
        "Oriol Vinyals"
      ],
      "year": "2019",
      "venue": "Representation learning with contrastive predictive coding",
      "arxiv": "arXiv:1807.03748"
    },
    {
      "citation_id": "111",
      "title": "Multimodal emotion recognition system for e-learning platform. Education and Information Technologies",
      "authors": [
        "Rk Vani",
        "Jayashree"
      ],
      "year": "2025",
      "venue": "Multimodal emotion recognition system for e-learning platform. Education and Information Technologies",
      "doi": "10.1007/s10639-024-13279-6"
    },
    {
      "citation_id": "112",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Ł Ukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "113",
      "title": "Multimodal emotional detection system for virtual educational environments: Integration into microsoft teams to improve student engagement",
      "authors": [
        "William Villegas-Ch",
        "Rommel Gutierrez",
        "Aracely Mera-Navarrete"
      ],
      "year": "2025",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2025.3546772"
    },
    {
      "citation_id": "114",
      "title": "Ecg multi-emotion recognition based on heart rate variability signal features mining",
      "authors": [
        "Ling Wang",
        "Jiayu Hao",
        "Tie Hua Zhou"
      ],
      "year": "2023",
      "venue": "Sensors",
      "doi": "10.3390/s23208636"
    },
    {
      "citation_id": "115",
      "title": "The dynamic nature of emotions in language learning context: Theory, method, and analysis",
      "authors": [
        "Peng Wang",
        "Lesya Ganushchak",
        "Camille Welie",
        "Roel Van Steensel"
      ],
      "year": "2024",
      "venue": "Educational Psychology Review",
      "doi": "10.1007/s10648-024-09946-2"
    },
    {
      "citation_id": "116",
      "title": "Finetuned language models are zero-shot learners",
      "authors": [
        "Jason Wei",
        "Maarten Bosma",
        "Y Vincent",
        "Kelvin Zhao",
        "Adams Guu",
        "Brian Yu",
        "Nan Lester",
        "Andrew Du",
        "Quoc V Dai",
        "Le"
      ],
      "year": "2021",
      "venue": "Finetuned language models are zero-shot learners",
      "arxiv": "arXiv:2109.01652"
    },
    {
      "citation_id": "117",
      "title": "Chain-of-thought prompting elicits reasoning in large language models",
      "authors": [
        "Jason Wei",
        "Xuezhi Wang",
        "Dale Schuurmans",
        "Maarten Bosma",
        "Fei Xia",
        "Ed Chi",
        "Denny Quoc V Le",
        "Zhou"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "118",
      "title": "The fair guiding principles for scientific data management and stewardship. Scientific Data",
      "authors": [
        "D Mark",
        "Michel Wilkinson",
        "I Dumontier",
        "Gabrielle Aalbersberg",
        "Myles Appleton",
        "Arie Axton",
        "Niklas Baak",
        "Jan-Willem Blomberg",
        "Luiz Boiten",
        "Silva Bonino Da",
        "Philip Santos",
        "Jildau Bourne",
        "Anthony Bouwman",
        "Tim Brookes",
        "Mercè Clark",
        "Ingrid Crosas",
        "Olivier Dillo",
        "Scott Dumon",
        "Chris Edmunds",
        "Richard Evelo",
        "Alejandra Finkers",
        "Alasdair Gonzalez-Beltran",
        "Paul Gray",
        "Carole Groth",
        "Jeffrey Goble",
        "Jaap Grethe",
        "Heringa",
        "A Peter",
        "Rob 't Hoen",
        "Tobias Hooft",
        "Ruben Kuhn",
        "Joost Kok",
        "Scott Kok",
        "Maryann Lusher",
        "Albert Martone",
        "Abel Mons",
        "Bengt Packer",
        "Persson"
      ],
      "year": "2016",
      "venue": "The fair guiding principles for scientific data management and stewardship. Scientific Data",
      "doi": "10.1038/sdata.2016.18"
    },
    {
      "citation_id": "119",
      "title": "Beyond silent letters: Amplifying LLMs in emotion recognition with vocal nuances",
      "authors": [
        "Zehui Wu",
        "Ziwei Gong",
        "Lin Ai",
        "Pengyuan Shi",
        "Kaan Donbekci",
        "Julia Hirschberg"
      ],
      "year": "2025",
      "venue": "Findings of the Association for Computational Linguistics: NAACL 2025"
    },
    {
      "citation_id": "120",
      "title": "Knowledge-interactive network with sentiment polarity intensity-aware multitask learning for emotion recognition in conversations",
      "authors": [
        "Yunhe Xie",
        "Kailai Yang",
        "Chengjie Sun",
        "Bingquan Liu",
        "Zhenzhou Ji"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021",
      "doi": "10.18653/v1/2021.findings-emnlp.245"
    },
    {
      "citation_id": "121",
      "title": "Hierarchical self-supervised augmented knowledge distillation",
      "authors": [
        "Chuanguang Yang",
        "Zhulin An",
        "Linhang Cai",
        "Yongjun Xu"
      ],
      "year": "2021",
      "venue": "Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21",
      "doi": "10.24963/ijcai.2021/168"
    },
    {
      "citation_id": "122",
      "title": "Emollm: Multimodal emotional understanding meets large language models",
      "authors": [
        "Qu Yang",
        "Mang Ye",
        "Bo Du"
      ],
      "year": "2024",
      "venue": "Emollm: Multimodal emotional understanding meets large language models",
      "arxiv": "arXiv:2406.16442"
    },
    {
      "citation_id": "123",
      "title": "Few-shot multimodal sentiment analysis based on multimodal probabilistic fusion prompts",
      "authors": [
        "Xiaocui Yang",
        "Shi Feng",
        "Daling Wang",
        "Yifei Zhang",
        "Soujanya Poria"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia, MM '23",
      "doi": "10.1145/3581783.3612181"
    },
    {
      "citation_id": "124",
      "title": "Mse-adapter: A lightweight plugin endowing llms with the capability to perform multimodal sentiment analysis and emotion recognition",
      "authors": [
        "Yang Yang",
        "Xunde Dong",
        "Yupeng Qiang"
      ],
      "year": "2025",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": "10.1609/aaai.v39i24.34755"
    },
    {
      "citation_id": "125",
      "title": "Multimodal emotion recognition with surgical and fabric masks",
      "authors": [
        "Ziqing Yang",
        "Katherine Nayan",
        "Zehao Fan",
        "Houwei Cao"
      ],
      "year": "2022",
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
      "doi": "10.1109/ICASSP43922.2022.9746414"
    },
    {
      "citation_id": "126",
      "title": "Fouriergnn: Rethinking multivariate time series forecasting from a pure graph perspective",
      "authors": [
        "Kun Yi",
        "Qi Zhang",
        "Wei Fan",
        "Hui He",
        "Liang Hu",
        "Pengyang Wang",
        "Ning An",
        "Longbing Cao",
        "Zhendong Niu"
      ],
      "year": "2023",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "127",
      "title": "Emotion detection on TV show transcripts with sequencebased convolutional neural networks",
      "authors": [
        "M Sayyed",
        "Jinho Zahiri",
        "Choi"
      ],
      "year": "2017",
      "venue": "Emotion detection on TV show transcripts with sequencebased convolutional neural networks"
    },
    {
      "citation_id": "128",
      "title": "Joint face detection and alignment using multitask cascaded convolutional networks",
      "authors": [
        "Kaipeng Zhang",
        "Zhanpeng Zhang",
        "Zhifeng Li",
        "Yu Qiao"
      ],
      "year": "2016",
      "venue": "IEEE Signal Processing Letters",
      "doi": "10.1109/LSP.2016.2603342"
    },
    {
      "citation_id": "129",
      "title": "Be your own teacher: Improve the performance of convolutional neural networks via self distillation",
      "authors": [
        "Linfeng Zhang",
        "Jiebo Song",
        "Anni Gao",
        "Jingwei Chen",
        "Chenglong Bao",
        "Kaisheng Ma"
      ],
      "year": "2019",
      "venue": "2019 IEEE/CVF International Conference on Computer Vision (ICCV)",
      "doi": "10.1109/ICCV.2019.00381"
    },
    {
      "citation_id": "130",
      "title": "Multimodal fusion on low-quality data: A comprehensive survey",
      "authors": [
        "Qingyang Zhang",
        "Yake Wei",
        "Zongbo Han",
        "Huazhu Fu",
        "Xi Peng",
        "Cheng Deng",
        "Qinghua Hu",
        "Cai Xu",
        "Jie Wen",
        "Di Hu"
      ],
      "venue": "Multimodal fusion on low-quality data: A comprehensive survey",
      "arxiv": "arXiv:2404.18947"
    },
    {
      "citation_id": "131",
      "title": "Survey of deep emotion recognition in dynamic data using facial, speech and textual cues",
      "authors": [
        "Tao Zhang",
        "Zhenhua Tan"
      ],
      "year": "2024",
      "venue": "Multimedia Tools and Applications",
      "doi": "10.1007/s11042-023-17944-9"
    },
    {
      "citation_id": "132",
      "title": "A cross-modality context fusion and semantic refinement network for emotion recognition in conversation",
      "authors": [
        "Xiaoheng Zhang",
        "Yang Li"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2023.acl-long.732"
    },
    {
      "citation_id": "133",
      "title": "Dialoguellm: Context and emotion knowledge-tuned large language models for emotion recognition in conversations",
      "authors": [
        "Yazhou Zhang",
        "Mengyao Wang",
        "Youxi Wu",
        "Prayag Tiwari",
        "Qiuchi Li",
        "Benyou Wang",
        "Jing Qin"
      ],
      "year": "2025",
      "venue": "Neural Networks",
      "doi": "10.1016/j.neunet.2025.107901"
    },
    {
      "citation_id": "134",
      "title": "2024b. 'what' and 'where' both matter: dual cross-modal graph convolutional networks for multimodal named entity recognition",
      "authors": [
        "Zhengxuan Zhang",
        "Jianying Chen",
        "Xuejie Liu",
        "Weixing Mai",
        "Qianhua Cai"
      ],
      "venue": "International Journal of Machine Learning and Cybernetics",
      "doi": "10.1007/s13042-023-02037-8"
    },
    {
      "citation_id": "135",
      "title": "A token-wise graph-based framework for multimodal named entity recognition",
      "authors": [
        "Zhengxuan Zhang",
        "Weixing Mai",
        "Haoliang Xiong",
        "Chuhan Wu",
        "Yun Xue"
      ],
      "year": "2023",
      "venue": "2023 IEEE International Conference on Multimedia and Expo (ICME)",
      "doi": "10.1109/ICME55011.2023.00368"
    },
    {
      "citation_id": "136",
      "title": "M3ED: Multi-modal multi-scene multi-label emotional dialogue database",
      "authors": [
        "Jinming Zhao",
        "Tenggan Zhang",
        "Jingwen Hu",
        "Yuchen Liu",
        "Qin Jin",
        "Xinchao Wang",
        "Haizhou Li"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2022.acl-long.391"
    },
    {
      "citation_id": "137",
      "title": "Cmath: Cross-modality augmented transformer with hierarchical variational distillation for multimodal emotion recognition in conversation",
      "authors": [
        "Xiaofei Zhu",
        "Jiawei Cheng",
        "Zhou Yang",
        "Zhuo Chen",
        "Qingyang Wang",
        "Jianfeng Yao"
      ],
      "year": "2024",
      "venue": "Cmath: Cross-modality augmented transformer with hierarchical variational distillation for multimodal emotion recognition in conversation",
      "arxiv": "arXiv:2411.10060"
    },
    {
      "citation_id": "138",
      "title": "Multimodal prompt transformer with hybrid contrastive learning for emotion recognition in conversation",
      "authors": [
        "Shihao Zou",
        "Xianying Huang",
        "Xudong Shen"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia, MM '23",
      "doi": "10.1145/3581783.3611805"
    },
    {
      "citation_id": "139",
      "title": "Improving multimodal fusion with main modal transformer for emotion recognition in conversation",
      "authors": [
        "Shihao Zou",
        "Xianying Huang",
        "Xudong Shen",
        "Hankai Liu"
      ],
      "year": "2022",
      "venue": "Knowledge-Based Systems",
      "doi": "10.1016/j.knosys.2022.109978"
    }
  ]
}