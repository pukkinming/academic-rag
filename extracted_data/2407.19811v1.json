{
  "paper_id": "2407.19811v1",
  "title": "Synthetic Thermal And Rgb Videos For Automatic Pain Assessment Utilizing A Vision-Mlp Architecture",
  "published": "2024-07-29T09:04:11Z",
  "authors": [
    "Stefanos Gkikas",
    "Manolis Tsiknakis"
  ],
  "keywords": [
    "Pain recognition",
    "deep learning",
    "GANs",
    "transformers",
    "multi-task learning",
    "data fusion"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Pain assessment is essential in developing optimal pain management protocols to alleviate suffering and prevent functional decline in patients. Consequently, reliable and accurate automatic pain assessment systems are essential for continuous and effective patient monitoring. This study presents synthetic thermal videos generated by Generative Adversarial Networks integrated into the pain recognition pipeline and evaluates their efficacy. A framework consisting of a Vision-MLP and a Transformer-based module is utilized, employing RGB and synthetic thermal videos in unimodal and multimodal settings. Experiments conducted on facial videos from the BioVid database demonstrate the effectiveness of synthetic thermal videos and underline the potential advantages of it.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Pain, as defined by the International Association for the Study of Pain (IASP), is \"an unpleasant sensory and emotional experience associated with actual or potential tissue damage, or described in terms of such damage\"  [1] . Additionally, Williams and Craig  [2]  state that pain encompasses emotional, cognitive, and social dimensions beyond physical aspects. Biologically, pain is a distasteful sensation that originates in the peripheral nervous system. It serves a vital function by activating sensory neurons, alerting the body to potential injury, and playing a critical role in recognizing and reacting to hazards.  [3] . Pain is a significant concern impacting individuals and social structures. Daily, people across all age groups suffer from pain resulting from accidents, illnesses, or as a part of medical treatment, making it the most common cause for seeking medical consultation. Both acute and chronic pain present clinical, economic, and social challenges. In addition to its immediate impact on a patient's life, pain is related to several adverse outcomes, including opioid consumption, drug misuse, addiction, deteriorating social relationships, and mental health issues  [4] . Effective pain assessment is crucial for early diagnosis, disease advancement monitoring, and treatment effectiveness, particularly in chronic pain management  [5] . Consequently, pain is referred to as \"the fifth vital sign\" in nursing literature  [6] . Objective measurement of pain is vital for providing appropriate care, especially for groups who cannot express their pain, such as infants, young children, people with mental health issues, and seniors. Numerous methods are utilized for pain assessment, including self-reporting, which remains the gold standard for determining pain presence and intensity through rating scales and questionnaires. Additionally, behavioral indicators such as facial expressions, vocalizations, and body movements are essential for evaluating pain.  [7] . Furthermore, physiological indicators such as electrocardiography, electromyography, skin conductance, and breathing rates provide significant insights into pain's physical effects  [5] . Although pain evaluation holds significant importance, it poses a substantial challenge to medical practitioners  [8] , particularly with patients who cannot communicate verbally. In senior patients, the situation becomes even more complex due to their decreased expressiveness or reluctance to share their pain experiences  [9] . Furthermore, extensive studies  [10] -  [12]  reveal distinct disparities in pain expression among various genders and age categories, underlining the intricacy of the pain assessment process.\n\nIn recent years, there has been a growing tendency in the field of affective computing research to incorporate thermal imaging techniques  [13] . The interest was sparked following findings in the literature that stress and cognitive load significantly impact skin temperature  [14] . This is due to the role of the autonomic nervous system (ANS) in controlling physiological signals like heart rate, respiration rate, blood perfusion, and body temperature, which are indicative of human emotions and affects  [13] . Additionally, muscle contractions influence facial temperature by transferring heat to the facial skin  [15] . Therefore, thermal imaging is a promising method for measuring transient facial temperatures  [16] . The authors in  [17]  examined thermal imaging and facial action units to assess emotions, including frustration, boredom, and enjoyment. The multimodal approach demonstrated the highest accuracy. Thermal imaging has been explored in a relatively small number of studies within the field of pain research. In  [18] , the authors observed that facial temperature rises following a painful stimulus, indicating that thermal cameras could serve as valuable tools for pain monitoring. In  [19] , a pain dataset comprising RGB, thermal, and depth videos was introduced. The findings demonstrated that the RGB modality marginally surpassed the others in performance while integrating all modalities led to superior results.\n\nThis study introduces the generation of synthetic thermal videos through generative adversarial networks (GANs), which are used in unimodal and multimodal settings combined with the RGB video modality. The foundation of the automatic pain assessment pipeline is a framework that integrates a Vision Multilayer Perceptron (MLP) model with a transformer-based module. The primary contributions of our research include:\n\n(1) generating synthetic thermal videos to supplement pain assessment as an additional vision modality, (2) evaluating the effectiveness of RGB and synthetic thermal videos as independent modalities, (3) exploring the effectiveness of the thermalrelated information, and (4) analyzing the performance and application of the newly introduced Vision-MLP architectures.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work",
      "text": "Recent developments have seen a range of innovative methods to assess pain levels from video data. Werner et al.  [20]  focused on domain-specific features, using facial action markers with a deep random forest (RF) classifier, and proposed a 3D distance computation method among facial points while in  [21] , an optical flow method was introduced to track facial points and capture expression changes across frames. The dynamic aspects of pain were addressed by developing long short-term memory networks combined with sparse coding (SLSTM)  [22] . Tavakolian et al.  [23]  utilized 3D convolutional neural networks (CNNs) with varied temporal depths to analyze short-, mid-, and long-term facial expressions. In  [24] , the authors leverage the temporal aspect of videos by encoding frames into motion history and optical flow images, which were then analyzed using a combination of CNN and bidirectional LSTM (biLSTM). Another method encoded videos into single RGB images through statistical spatiotemporal distillation (SSD) and trained a Siamese network in a self-supervised manner  [25] . In  [26]  the authors implemented a multi-stream CNN for feature extraction from different facial regions, applying learned weights to emphasize the significance of each region's features in expressing pain. Further research  [27]  identified that specific frames more clearly displayed pain expressions and developed a framework using CNNs, gated recurrent units (GRUs), and attention saliency maps, assigning weights to each frame's influence on overall pain intensity. A novel approach by Huang et al.  [28]  was introduced by extracting simulated heart rate data from video content utilizing a 3D CNN, demonstrating strong results in binary and multiclass classification scenarios. Finally, in the studies  [29] ,  [30] , transformer-based frameworks were proposed, yielding promising results with high efficiency.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Methodology",
      "text": "This section describes the process of generating synthetic thermal videos, the architecture of the proposed automatic pain assessment framework, the developed augmentation techniques, the pre-processing methods, and the pre-training strategy for the modules.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Synthetic Thermal Videos",
      "text": "An image-to-image translation (I2I) approach has been developed for generating synthetic thermal videos. I2I generative models aim to map distinct image domains by learning the intrinsic data distributions of both domains. In this case, the source domain consists of RGB images, while thermal images represent the target domain. In this study, conditional generative adversarial networks (cGANs)  [31]  were developed and trained in supervised settings with aligned image pairs. Fig.  1  illustrates a high-level overview of the proposed method. The generator G generates realistic-looking images, while discriminator D aims to distinguish authentic images from synthetic ones via the following minimax game:\n\nwhere the objective function L cGAN pG, Dq can be expressed as:\n\nwhere x represents the real data, y signifies the target data, and z denotes the random noise vector. The G aims to minimize the objective function, while the D functions adversarially, trying to maximize it. Furthermore, we included the Wasserstein gradient penalty (WGAN-GP)  [32]  to increase the training stability. The final objective is described as:\n\nwhere λ denotes the penalty coefficient. Regarding the architecture, in the proposed methodology, inspired by  [33] , the G is structured into 3 distinct modules: an encoder, which comprises 2 convolutional layers downsampling the input; an intermediate ResNet module, consisting of 9 residual blocks, each consisting with 2 convolutional layers; and a decoder, upsampling the feature maps to the final resolution (i.e., 256 ˆ256) for the synthetic sample. The D founded on  [34]  is a pixel-level PatchGAN discriminator using 1 ˆ1 kernels consisting of 2 convolutional layers.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Framework Architecture",
      "text": "The proposed framework comprises two main modules: a Vision-MLP model that functions as a spatial embedding extractor for individual video frames and a transformer-based model that serves as a temporal module, utilizing the embedded representations of the videos for temporal analysis and final pain assessment. Fig.  2  illustrates the modules and their primary building blocks.\n\n1) Vision-MLP: MLP-like models represent a newly introduced type of vision models, serving as alternatives to traditional Convolutional Neural Networks (CNNs) and Vision Transformers (ViT). They are characterized by simple architectures consisting of fully connected layers coupled with activation functions. They embody lesser inductive bias and are based on basic matrix multiplication routines. Our approach is founded on the principles of  [35]  introducing the Vision-MLP and  [36]  incorporating a wave representation for the patches (also referred to as tokens). Each video frame is initially divided into n non-overlapping tokens F m \" rf m,1 , f m,2 , . . . , f m,n s P R nˆpˆpˆ3 , where p specifies the resolution of each token, i.e., 16 ˆ16 pixels, and 3 represents the number of color channels. Each token is then linearly projected into a dimension d \" 768 before being fed into the Vision-MLP (refer to Fig.  2a ). The first main sub-module is the so-called Channel-Mixer (Fig.  2c ), which operates on each token f j independently and allows communication between different channels and is formulated as:\n\nwhere W c denotes the weight matrix with learnable parameters, and j \" 1, 2, . . . , n. Next, the second main sub-module, Token-Mixer (Fig.  2b ), allows communication between different tokens, enabling feature extraction from different spatial locations. Typically, in MLP-based models, the token-mixers formulated as:\n\nwhere W t denotes the corresponding weight matrix for the tokens, and the d represents element-wise multiplication.\n\nOur proposed approach transforms the tokens into wave-like representations to modulate the relationship between tokens and weights dynamically according to their semantic content.\n\nIn order to represent a token f j as wave fj through a wave function, amplitude and phase information are needed:\n\nHere, i denotes the imaginary unit satisfying i 2 \" ´1. The term |f j | represents the amplitude of the signal. The function e iθj is a periodic function, and θ j symbolizes the phase of the signal.\n\nThe amplitude |f j | can be likened to the real-valued feature found in conventional models, with the notable distinction being the application of the absolute value operation. In the practical implementation, the absolute value operation is omitted and replaced with 4 for simplicity. The phase θ j for each token reflects its position within a wave's cycle and can thus be described using fixed parameters, which are learnable during the training phase. Consequently, 4 is also utilized for the phase estimation. Given that 6 characterizes a wave within the complex domain, the Euler formula facilitates embedding tokens within the neural network architecture:\n\nCombining 5 and 7, a token is represented as:\n\nwhere W t , W c and W i are learnable weight matrices. The process described, which pertains to wave-like representations, unfolds within the Token-Mixer, particularly in the Wave-Block.\n\nThe Token-Mixer architecture comprises three blocks: two Wave-Blocks and one Channel-Mixer operating in parallel. The Vision-MLP module is structured into four stages. Each stage comprises a sequence consisting of a Token-Mixer and a Channel-Mixer block, with a normalization layer preceding each. The depth of parallel blocks in each stage is 3, 4, 18, and 3, respectively. This structure facilitates extracting hierarchical embeddings with corresponding dimensions across stages 64, 128, 320, and 100.\n\n2) Fusion: For each input frame, the Vision-MLP extracts an embedding with a dimensionality of d \" 100. Subsequently, the embeddings derived from the respective frames of a particular video are concatenated to create a unified embedding representation of the original video:\n\nwhere m denotes the number of frames in a video, and N represents the dimensionality of the final embedding. Subsequently, the embeddings derived from RGB and synthetic thermal videos are integrated through a weighted fusion process:\n\nThe fusion process is founded on combining the corresponding embeddings, utilizing learned weights w 1 and w 2 , which modulate the contributions of the RGB and thermal embeddings, respectively. The weighted addition provides an optimized integration, reflecting the importance of each modality in the final fused representation V F used .\n\n3) Transformer: The fused embeddings are subsequently fed into a transformer-based module comprising self-attention and cross-attention blocks (Fig.  2e ). The self-attention process is represented as follows:\n\nAttentionpQ, K, V q \" sof tmax\n\nHere, Q P R M ˆC , K P R M ˆC , and V P R M ˆC represent the Query, Key, and Value matrices, respectively, where M denotes the input dimension, and C the channel dimension. Similarly, the cross-attention mechanism employs a dot product operation, but the Q instead of M ˆC is N ˆC, where N ă M offers a computational cost reduction. Each self and cross-attention block incorporates 1 and 8 attention heads, respectively, while 4 parallel blocks comprise the whole Transformer module. The resulting output embeddings, with a dimensionality of 340, are employed to complete the final pain assessment through a fully connected neural network.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Augmentation Methods",
      "text": "Two augmentation techniques have been implemented within the framework. First, the so-called Basic is employed, integrating polarity inversion with noise addition. This method transforms the original input embedding by reversing the polarity of data elements and adding random noise from a Gaussian distribution, creating variability and perturbations. Second, the Masking involves applying zero-valued masks to the embeddings, nullifying segments of the vectors. The dimensions of the masks are randomly determined, spanning 10% to 50% of the embedding's total dimensions, and they are positioned at random locations within the embeddings.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "D. Pre-Processing",
      "text": "The pre-processing involved face detection to isolate the facial region. The MTCNN face detector  [37]  was employed, which utilizes multitask cascaded convolutional neural networks for predicting faces and landmarks. It is important to note that the face detector was applied only to the individual RGB frames, and the coordinates of the detected face were applied to the corresponding synthetic thermal frames. The resolution of all frames was set at 224 ˆ224 pixels.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "E. Pre-Training",
      "text": "For the I2I approach, the SpeakingFaces  [38]  dataset was utilized to train the proposed GAN model for translating the RGB to synthetic thermal videos. In addition, prior to the automatic pain assessment training process, the Vision-MLP and Transformer modules were pre-trained. The Vision-MLP underwent a three-stage pre-training strategy: initially, it was trained on DigiFace-1M  [39]  to learn basic facial features. Subsequently, it was trained on AffectNet  [40]  and RAF Face Database basic  [41]  to learn features related to basic emotions through multi-task learning. Finally, the Compound Facial Expressions of Emotions Database  [42]  and the RAF Face Database compound  [41]  were utilized to learn features of compound emotions in a similar multi-task setting. The multitask learning process is described as:\n\nwhere L S is the loss for the corresponding task related to different datasets, and w represents the learned weights that drive the learning process in minimizing the combined loss L total , considering all the individual losses. The Transformer  was pre-trained only on the DigiFace-1M  [39] , where the input images were flattened into 1D vectors due to its architectural design. Table  I  details the datasets used in the pre-training procedure.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Iv. Experimental Evaluation & Results",
      "text": "The BioVid Heat Pain Database  [43] , was utilized to evaluate the proposed framework. It comprises facial videos, electrocardiogram, electromyogram, and skin conductance levels from 87 healthy individuals. The experimental design of the dataset utilized a thermode to induce pain in the participants' right arm, resulting in five distinct intensity levels: no pain (NP), mild pain (P 1 ), moderate pain (P 2 ), severe pain (P 3 ), and very severe pain (P 4 ). Each participant was exposed to each level of pain intensity 20 times, resulting in 100 data samples for each modality and 1740 data samples per class. We utilized the videos (5 ˆ1740 \" 8700) from Part A of BioVid in this study. The pain assessment experiments were structured in binary and multi-level classification settings, evaluating each modality individually and in combination. In binary classification, the task was to distinguish between No Pain (NP) and very severe pain (P 4 ). In contrast, multi-level classification (MC) involves classifying all pain levels within the dataset. For evaluation, the leave-one-subject-out (LOSO) cross-validation method was adopted, and performance was assessed based on the accuracy metric. Table  II  presents the framework's training details related to the automatic pain assessment task, and outlines the number of parameters and the computational cost in terms of floatingpoint operations (FLOPS) for each module.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Rgb Videos",
      "text": "In the RGB video modality context, we observed an accuracy of 69.37% for the binary classification task (NP vs. P 4 ) and 30.23% for the multi-class classification (MC). Upon intensifying the Masking augmentation method to encompass 20 ´50% of the input embeddings, there was a modest improvement of 0.89% in accuracy for the binary task. In contrast, a decrement was observed in the multi-class task. Subsequent extension of training to 300 epochs, 30 ´50% for the Masking method and 90% probability for both the augmentation methods yielded accuracies of 70.05% and 30.02% for the binary and multi-class tasks, respectively, translating to an average increment marginally below 0.5%. Table  III  presents the classification results.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Synthetic Thermal Videos",
      "text": "In the experiments conducted with the synthetic thermal modality under identical experimental conditions, initial accuracies were recorded at 69.97% for the binary task and 30.04% for the multi-class task. An increase in the intensity of the masking method resulted in modest accuracy improvements of 0.23% and 0.46% for the binary and multi-class tasks, respectively. Subsequently, final accuracy measurements were 70.69% for the binary task and 29.60% for the multi-class task, culminating in an average increase of 0.28%. This difference may arise from the challenge of discerning subtle facial changes associated with low-level pain and accompaniment by further corruption from heavier augmentation, which results in diminished performances. The corresponding results are summarized in Table  IV .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Additional Analysis On Rgb & Synthetic Thermal Videos",
      "text": "The findings from IV-A and IV-B revealed a notable circumstance where the performance metrics for the RGB and synthetic thermal modalities are remarkably similar. Specifically, the highest recorded accuracies for the RGB modality were 70.26% and 30.23% for the NP vs. P4 and MC tasks, respectively. Correspondingly, the peak accuracies for the synthetic thermal modality were 70.69% and 30.50%. On average, the performances from the thermal videos are approximately 1% superior to those of the RGB modality. This outcome was unexpected, given that the synthetic modality was initially presumed to be less effective than the original. This prompted an exploration into the reason synthetic modalities exhibit comparable or superior performance to the original RGB modality. A primary question was regarding the richness and effectiveness of the thermal-related information incorporated in the synthetic videos. The hypothesis suggested that reducing facial expressions in the thermal videos could allow a more explicit assessment of the thermal information. Gaussian blurring was progressively applied to RGB and synthetic thermal videos (refer to Fig.  3 ), with kernel sizes k incrementally adjusted from 0 to 191. Similar, albeit less time-intensive, experiments to IV-A, IV-B were conducted.\n\nTable  V  shows that with a kernel size of k \" 0, the performance disparity of 0.47% (favoring the thermal modality) aligns with prior experimental outcomes. As blurring intensifies to k \" 41, this discrepancy marginally increases to 0.49%. Notably, at k \" 91, the divergence expands to 2.13% and intensifies to 5.90% when the blur peaks at k \" 191 (heavily blurred). The classification performances demonstrated that by diminishing the visibility of facial expressions through blurring, the synthetic thermal videos resulted in superior performance compared to the RGB, with figures of 66.24% over 60.34%. Additionally, as the kernel size increased from k \" 0 to k \" 191, the decline in accuracy rates for the synthetic thermal and RGB modalities was 1.81% and 7.13%, respectively. This suggests that the residual information in the synthetic modality, essentially the visually represented facial temperature, remains intact or minimally influenced. Fig.  4  depicts the embedding distribution for the RGB and synthetic thermal modality for k \" 0 and k \" 191. Although the separation of the data points is not clear, we observe a distinct difference in the distribution. For k \" 191, the RGB embeddings are centralized and probably overlap, and a plethora of points are notably spread away from the central mass without a clear pattern. Respectively, the data points are much more uniformly spreading for the synthetic modality, suggesting potentially better differentiation between classes.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "D. Fusion",
      "text": "Three fusion methods were assessed in the context of multimodal analysis for RGB and synthetic thermal videos. The approach outlined in 11 was initially applied, utilizing learned weights w 1 and w 2 to scale the respective modalities. Additionally, a second method was employed where a third weight, w 3 , was introduced, resulting in w 3 ¨pw 1 ¨VRGB `w2 ¨VT hermal q.   Lastly, a method without learned weights was explored, directly adding the embedding vectors from both modalities. Table  VI  presents the corresponding results. The absence of weights resulted in 64.92% and 26.40% accuracy for the binary and multi-class tasks, respectively. The integration of the three weights resulted in a decrease of 0.5% in accuracy for both tasks, whereas the application of weights w 1 and w 2 yielded the highest performance, with accuracies reaching 65.08% and 26.50% for the binary and multi-class tasks, respectively. By applying weights w 1 and w 2 and increasing the training period from 100 to 300 epochs while maintaining consistent augmentation settings, accuracies of 69.50% and 29.80% were achieved for the binary and multi-class tasks, respectively. Further extending the training time to 500 epochs without encountering any overfitting phenomena improved performance, reaching accuracies of 71.03% and 30.70% for the respective tasks.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "V. Comparison With Existing Methods",
      "text": "This section compares the proposed method with other existing approaches in the literature. The evaluation utilizes Part A of the BioVid dataset, involving all 87 subjects, and follows the same validation protocol, LOSO cross-validation. Table  VII  presents the corresponding results. The proposed visionbased method, utilizing RGB and synthetic thermal modalities, demonstrated performances comparable to or exceeding that of previous methods. Compared to the findings reported in studies  [21] ,  [22] ,  [24] ,  [25] , improved accuracy was attained in binary and multi-level tasks. It is noted that the authors in  [20]  reported accuracies of 72.40% and 30.80%, showing an improvement of 1.37% and 0.10% over our results. In the study  [29] , the authors achieved the highest reported results, employing a transformer-based architecture.\n\nFurthermore, in Table  VIII , we compare our findings with those from the study  [19] , in which the authors introduce the MIntPAIN dataset, including both RGB and thermal videos for automatic pain assessment across five intensity levels. We observe that the accuracy of the RGB and thermal modalities is particularly similar, at 18.55% and 18.33%, respectively. This outcome mirrors our findings, where performance between the two modalities-RGB and synthetic thermal was similarly aligned. By fusing the modalities, the authors achieved an  This contrasts with our results, where the increase was marginal. However, it is important to note that the performances of the unimodal approaches in  [19]  were below the guess prediction threshold of 20%, and only through the fusion of these modalities did the performance surpass it.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "This study explored the generation of synthetic thermal imagery from GAN models to evaluate its effectiveness in the context of automatic pain assessment. Furthermore, a novel framework based on Vision-MLP was introduced, complimented by a Transformer module serving as the core of the assessment system. The conducted experiments underscored the efficacy of the synthetic thermal modality, showcasing performances comparable to or surpassing those of the original RGB modality. Moreover, this study examined the underlying factors contributing to this effectiveness, particularly focusing on the role of temperature color representations. Additionally, the integration of the two vision modalities was analyzed using various fusion techniques. It should be emphasized that further optimization and experimentation, particularly with the multimodal approach, have the capacity to yield enhanced results. We believe that the generation and integration of synthetic modalities, such as thermal imagery, in an automatic pain assessment framework holds significant potential, and additional exploration and research are needed.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Ethical Impact Statement",
      "text": "This research employed the BioVid Heat Pain Database  [43]  to evaluate the proposed methods. The data were recorded according to the ethical guidelines of Helsinki (ethics committee: 196/10-UBB/bal). Prior to commencing data collection, each participant's pain threshold (where sensation transitions from heat to pain) and tolerance threshold (the moment when pain becomes unbearable) were determined. The facial images presented in this study are from participants who have consented to their use for illustrative purposes within a scientific research context. This study aims to introduce a pain assessment framework designed to facilitate continuous patient monitoring while reducing human biases. However, it is essential to recognize that real-world applications, especially in clinical settings, might present challenges, necessitating further experimentation and comprehensive evaluation through clinical trials before deployment.\n\nIn addition, this study utilized the SpeakingFaces  [38]  dataset for the image-to-image translation process. The data was collected according to the ethical guidelines of the Declaration of Helsinki, and with the approval from the Institutional Research Ethics Committee at Nazarbayev University. All participants were volunteers who were fully informed about the data collection procedures and the intended use of identifiable images, which will be distributed as part of a dataset. Each participant provided their permission by signing informed consent forms.\n\nFurthermore, several datasets were utilized to pretrain the proposed pain assessment framework. The DigiFace-1M  [39]  is a synthetic dataset where 511 initial face scans were obtained with consent and employed to build a parametric face geometry and texture library model. All the identities and samples were generated from these source data. The AffectNet  [40]  dataset is compiled using search engine queries. The original paper does not explicitly detail ethical compliance measures such as adherence to the Declaration of Helsinki or informed consent procedures. The original paper of Compound FEE-DB  [42]  does not mention ethical compliance measures, but only that the subjects were recruited from the Ohio State University area and received a monetary reward for participating. The RAF-DB  [41]  dataset was compiled using the Flickr image hosting service. Although Flickr hosts both public and privately shared images, the authors do not explicitly mention the type of the downloaded images.",
      "page_start": 8,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: illustrates",
      "page": 2
    },
    {
      "caption": "Figure 1: Overview of the pipeline for generating synthetic thermal",
      "page": 2
    },
    {
      "caption": "Figure 2: Schematic overview of the proposed framework for automatic pain assessment, detailing its modules and their primary",
      "page": 3
    },
    {
      "caption": "Figure 2: illustrates the modules and their primary",
      "page": 3
    },
    {
      "caption": "Figure 2: b), allows communication between different",
      "page": 3
    },
    {
      "caption": "Figure 2: e). The self-attention process",
      "page": 4
    },
    {
      "caption": "Figure 3: ), with kernel sizes k incrementally adjusted",
      "page": 6
    },
    {
      "caption": "Figure 4: depicts the embedding",
      "page": 6
    },
    {
      "caption": "Figure 3: Progressive blurring of RGB and synthetic thermal facial",
      "page": 6
    },
    {
      "caption": "Figure 4: 3D embedding space distributions of NP (no pain) and",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "gkikas@ics.forth.gr": "Abstract—Pain assessment\nis essential\nin developing optimal",
          "tsiknaki@ics.forth.gr": "providing appropriate care, especially for groups who cannot"
        },
        {
          "gkikas@ics.forth.gr": "pain management protocols\nto alleviate suffering and prevent",
          "tsiknaki@ics.forth.gr": ""
        },
        {
          "gkikas@ics.forth.gr": "",
          "tsiknaki@ics.forth.gr": "express\ntheir pain,\nsuch as\ninfants, young children, people"
        },
        {
          "gkikas@ics.forth.gr": "functional decline in patients. Consequently, reliable and accurate",
          "tsiknaki@ics.forth.gr": ""
        },
        {
          "gkikas@ics.forth.gr": "",
          "tsiknaki@ics.forth.gr": "with mental health issues, and seniors. Numerous methods are"
        },
        {
          "gkikas@ics.forth.gr": "automatic pain assessment systems are essential\nfor continuous",
          "tsiknaki@ics.forth.gr": ""
        },
        {
          "gkikas@ics.forth.gr": "",
          "tsiknaki@ics.forth.gr": "utilized for pain assessment,\nincluding self-reporting, which"
        },
        {
          "gkikas@ics.forth.gr": "and effective patient monitoring. This study presents synthetic",
          "tsiknaki@ics.forth.gr": ""
        },
        {
          "gkikas@ics.forth.gr": "thermal\nvideos\ngenerated by Generative Adversarial Networks",
          "tsiknaki@ics.forth.gr": "remains the gold standard for determining pain presence and"
        },
        {
          "gkikas@ics.forth.gr": "integrated\ninto\nthe\npain\nrecognition\npipeline\nand\nevaluates",
          "tsiknaki@ics.forth.gr": "intensity through rating scales and questionnaires. Additionally,"
        },
        {
          "gkikas@ics.forth.gr": "their\nefficacy. A framework consisting\nof\na Vision-MLP and",
          "tsiknaki@ics.forth.gr": ""
        },
        {
          "gkikas@ics.forth.gr": "",
          "tsiknaki@ics.forth.gr": "behavioral\nindicators such as facial expressions, vocalizations,"
        },
        {
          "gkikas@ics.forth.gr": "a Transformer-based module\nis utilized,\nemploying RGB and",
          "tsiknaki@ics.forth.gr": ""
        },
        {
          "gkikas@ics.forth.gr": "",
          "tsiknaki@ics.forth.gr": "and body movements are essential\nfor evaluating pain.\n[7]."
        },
        {
          "gkikas@ics.forth.gr": "synthetic thermal videos\nin unimodal and multimodal\nsettings.",
          "tsiknaki@ics.forth.gr": ""
        },
        {
          "gkikas@ics.forth.gr": "",
          "tsiknaki@ics.forth.gr": "Furthermore, physiological\nindicators\nsuch as electrocardio-"
        },
        {
          "gkikas@ics.forth.gr": "Experiments conducted on facial videos from the BioVid database",
          "tsiknaki@ics.forth.gr": ""
        },
        {
          "gkikas@ics.forth.gr": "demonstrate\nthe\neffectiveness of\nsynthetic\nthermal videos and",
          "tsiknaki@ics.forth.gr": "graphy, electromyography,\nskin conductance, and breathing"
        },
        {
          "gkikas@ics.forth.gr": "underline the potential advantages of\nit.",
          "tsiknaki@ics.forth.gr": "rates provide significant\ninsights into pain’s physical effects"
        },
        {
          "gkikas@ics.forth.gr": "Index Terms—Pain recognition, deep learning, GANs,\ntrans-",
          "tsiknaki@ics.forth.gr": ""
        },
        {
          "gkikas@ics.forth.gr": "",
          "tsiknaki@ics.forth.gr": "[5]. Although pain evaluation holds\nsignificant\nimportance,"
        },
        {
          "gkikas@ics.forth.gr": "formers, multi-task learning, data fusion",
          "tsiknaki@ics.forth.gr": ""
        },
        {
          "gkikas@ics.forth.gr": "",
          "tsiknaki@ics.forth.gr": "it poses a substantial challenge to medical practitioners\n[8],"
        },
        {
          "gkikas@ics.forth.gr": "",
          "tsiknaki@ics.forth.gr": "particularly with patients who cannot communicate verbally. In"
        },
        {
          "gkikas@ics.forth.gr": "I.\nINTRODUCTION",
          "tsiknaki@ics.forth.gr": ""
        },
        {
          "gkikas@ics.forth.gr": "",
          "tsiknaki@ics.forth.gr": "senior patients,\nthe situation becomes even more complex due"
        },
        {
          "gkikas@ics.forth.gr": "Pain, as defined by the International Association for\nthe",
          "tsiknaki@ics.forth.gr": "to their decreased expressiveness or\nreluctance to share their"
        },
        {
          "gkikas@ics.forth.gr": "Study of Pain (IASP),\nis “an unpleasant sensory and emotional",
          "tsiknaki@ics.forth.gr": "pain experiences [9]. Furthermore, extensive studies [10]–[12]"
        },
        {
          "gkikas@ics.forth.gr": "experience associated with actual or potential\ntissue damage,",
          "tsiknaki@ics.forth.gr": ""
        },
        {
          "gkikas@ics.forth.gr": "",
          "tsiknaki@ics.forth.gr": "reveal distinct disparities\nin pain expression among various"
        },
        {
          "gkikas@ics.forth.gr": "or\ndescribed\nin\nterms\nof\nsuch\ndamage” [1]. Additionally,",
          "tsiknaki@ics.forth.gr": ""
        },
        {
          "gkikas@ics.forth.gr": "",
          "tsiknaki@ics.forth.gr": "genders and age categories, underlining the intricacy of\nthe"
        },
        {
          "gkikas@ics.forth.gr": "Williams and Craig [2] state that pain encompasses emotional,",
          "tsiknaki@ics.forth.gr": "pain assessment process."
        },
        {
          "gkikas@ics.forth.gr": "cognitive,\nand\nsocial\ndimensions\nbeyond\nphysical\naspects.",
          "tsiknaki@ics.forth.gr": "In recent years, there has been a growing tendency in the field"
        },
        {
          "gkikas@ics.forth.gr": "Biologically, pain is\na distasteful\nsensation that originates",
          "tsiknaki@ics.forth.gr": "of affective computing research to incorporate thermal\nimaging"
        },
        {
          "gkikas@ics.forth.gr": "in the peripheral nervous\nsystem.\nIt\nserves a vital\nfunction",
          "tsiknaki@ics.forth.gr": "techniques [13]. The interest was sparked following findings"
        },
        {
          "gkikas@ics.forth.gr": "by activating sensory neurons, alerting the body to potential",
          "tsiknaki@ics.forth.gr": "in the\nliterature\nthat\nstress\nand cognitive\nload significantly"
        },
        {
          "gkikas@ics.forth.gr": "injury, and playing a critical role in recognizing and reacting to",
          "tsiknaki@ics.forth.gr": "impact skin temperature [14]. This is due to the role of\nthe"
        },
        {
          "gkikas@ics.forth.gr": "hazards. [3]. Pain is a significant concern impacting individuals",
          "tsiknaki@ics.forth.gr": "autonomic nervous system (ANS)\nin controlling physiological"
        },
        {
          "gkikas@ics.forth.gr": "and social structures. Daily, people across all age groups suffer",
          "tsiknaki@ics.forth.gr": "signals like heart\nrate,\nrespiration rate, blood perfusion, and"
        },
        {
          "gkikas@ics.forth.gr": "from pain resulting from accidents,\nillnesses, or\nas\na part",
          "tsiknaki@ics.forth.gr": "body temperature, which are indicative of human emotions"
        },
        {
          "gkikas@ics.forth.gr": "of medical\ntreatment, making it\nthe most common cause for",
          "tsiknaki@ics.forth.gr": "and affects [13]. Additionally, muscle contractions influence"
        },
        {
          "gkikas@ics.forth.gr": "seeking medical\nconsultation. Both acute\nand chronic pain",
          "tsiknaki@ics.forth.gr": "facial\ntemperature\nby\ntransferring\nheat\nto\nthe\nfacial\nskin"
        },
        {
          "gkikas@ics.forth.gr": "present clinical, economic, and social challenges.\nIn addition",
          "tsiknaki@ics.forth.gr": "[15]. Therefore,\nthermal\nimaging is a promising method for"
        },
        {
          "gkikas@ics.forth.gr": "to its\nimmediate\nimpact on a patient’s\nlife, pain is\nrelated",
          "tsiknaki@ics.forth.gr": "measuring transient\nfacial\ntemperatures [16]. The authors in"
        },
        {
          "gkikas@ics.forth.gr": "to several adverse outcomes,\nincluding opioid consumption,",
          "tsiknaki@ics.forth.gr": "[17]\nexamined\nthermal\nimaging\nand\nfacial\naction\nunits\nto"
        },
        {
          "gkikas@ics.forth.gr": "drug misuse, addiction, deteriorating social\nrelationships, and",
          "tsiknaki@ics.forth.gr": "assess emotions, including frustration, boredom, and enjoyment."
        },
        {
          "gkikas@ics.forth.gr": "mental health issues [4]. Effective pain assessment is crucial for",
          "tsiknaki@ics.forth.gr": "The multimodal approach demonstrated the highest accuracy."
        },
        {
          "gkikas@ics.forth.gr": "early diagnosis, disease advancement monitoring, and treatment",
          "tsiknaki@ics.forth.gr": "Thermal\nimaging\nhas\nbeen\nexplored\nin\na\nrelatively\nsmall"
        },
        {
          "gkikas@ics.forth.gr": "effectiveness, particularly in chronic pain management\n[5].",
          "tsiknaki@ics.forth.gr": "number of studies within the field of pain research.\nIn [18],"
        },
        {
          "gkikas@ics.forth.gr": "Consequently, pain is referred to as “the fifth vital sign”\nin",
          "tsiknaki@ics.forth.gr": "the authors observed that\nfacial\ntemperature rises following a"
        },
        {
          "gkikas@ics.forth.gr": "nursing literature [6]. Objective measurement of pain is vital for",
          "tsiknaki@ics.forth.gr": "painful stimulus,\nindicating that\nthermal cameras could serve"
        },
        {
          "gkikas@ics.forth.gr": "",
          "tsiknaki@ics.forth.gr": "as valuable tools for pain monitoring.\nIn [19], a pain dataset"
        },
        {
          "gkikas@ics.forth.gr": "the Computational\n:: Corresponding Author, ;: Affiliated Researcher at",
          "tsiknaki@ics.forth.gr": ""
        },
        {
          "gkikas@ics.forth.gr": "",
          "tsiknaki@ics.forth.gr": "comprising RGB,\nthermal, and depth videos was introduced."
        },
        {
          "gkikas@ics.forth.gr": "Biomedicine Laboratory of\nthe Foundation for Research and Technology",
          "tsiknaki@ics.forth.gr": ""
        },
        {
          "gkikas@ics.forth.gr": "(FORTH), Heraklion, Greece",
          "tsiknaki@ics.forth.gr": "The findings demonstrated that\nthe RGB modality marginally"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "surpassed\nthe\nothers\nin\nperformance while\nintegrating\nall": "modalities led to superior\nresults.",
          "A.\nSynthetic Thermal Videos": ""
        },
        {
          "surpassed\nthe\nothers\nin\nperformance while\nintegrating\nall": "",
          "A.\nSynthetic Thermal Videos": "An image-to-image translation (I2I) approach has been de-"
        },
        {
          "surpassed\nthe\nothers\nin\nperformance while\nintegrating\nall": "This study introduces the generation of synthetic thermal",
          "A.\nSynthetic Thermal Videos": ""
        },
        {
          "surpassed\nthe\nothers\nin\nperformance while\nintegrating\nall": "",
          "A.\nSynthetic Thermal Videos": "veloped for generating synthetic thermal videos.\nI2I generative"
        },
        {
          "surpassed\nthe\nothers\nin\nperformance while\nintegrating\nall": "videos through generative adversarial networks (GANs), which",
          "A.\nSynthetic Thermal Videos": ""
        },
        {
          "surpassed\nthe\nothers\nin\nperformance while\nintegrating\nall": "",
          "A.\nSynthetic Thermal Videos": "models aim to map distinct\nimage domains by learning the"
        },
        {
          "surpassed\nthe\nothers\nin\nperformance while\nintegrating\nall": "are used in unimodal and multimodal settings combined with",
          "A.\nSynthetic Thermal Videos": ""
        },
        {
          "surpassed\nthe\nothers\nin\nperformance while\nintegrating\nall": "",
          "A.\nSynthetic Thermal Videos": "intrinsic data distributions of both domains.\nIn this case,\nthe"
        },
        {
          "surpassed\nthe\nothers\nin\nperformance while\nintegrating\nall": "the RGB video modality. The foundation of the automatic pain",
          "A.\nSynthetic Thermal Videos": ""
        },
        {
          "surpassed\nthe\nothers\nin\nperformance while\nintegrating\nall": "",
          "A.\nSynthetic Thermal Videos": "source domain consists of RGB images, while thermal\nimages"
        },
        {
          "surpassed\nthe\nothers\nin\nperformance while\nintegrating\nall": "assessment pipeline is a framework that\nintegrates a Vision",
          "A.\nSynthetic Thermal Videos": ""
        },
        {
          "surpassed\nthe\nothers\nin\nperformance while\nintegrating\nall": "",
          "A.\nSynthetic Thermal Videos": "represent the target domain. In this study, conditional generative"
        },
        {
          "surpassed\nthe\nothers\nin\nperformance while\nintegrating\nall": "Multilayer Perceptron (MLP) model with a transformer-based",
          "A.\nSynthetic Thermal Videos": ""
        },
        {
          "surpassed\nthe\nothers\nin\nperformance while\nintegrating\nall": "",
          "A.\nSynthetic Thermal Videos": "adversarial networks (cGANs) [31] were developed and trained"
        },
        {
          "surpassed\nthe\nothers\nin\nperformance while\nintegrating\nall": "module. The primary contributions of our\nresearch include:",
          "A.\nSynthetic Thermal Videos": ""
        },
        {
          "surpassed\nthe\nothers\nin\nperformance while\nintegrating\nall": "",
          "A.\nSynthetic Thermal Videos": "in supervised settings with aligned image pairs. Fig. 1 illustrates"
        },
        {
          "surpassed\nthe\nothers\nin\nperformance while\nintegrating\nall": "(1) generating synthetic thermal videos\nto supplement pain",
          "A.\nSynthetic Thermal Videos": ""
        },
        {
          "surpassed\nthe\nothers\nin\nperformance while\nintegrating\nall": "",
          "A.\nSynthetic Thermal Videos": "a high-level overview of\nthe proposed method. The generator"
        },
        {
          "surpassed\nthe\nothers\nin\nperformance while\nintegrating\nall": "assessment as an additional vision modality,\n(2) evaluating the",
          "A.\nSynthetic Thermal Videos": ""
        },
        {
          "surpassed\nthe\nothers\nin\nperformance while\nintegrating\nall": "",
          "A.\nSynthetic Thermal Videos": "G generates realistic-looking images, while discriminator D"
        },
        {
          "surpassed\nthe\nothers\nin\nperformance while\nintegrating\nall": "effectiveness of RGB and synthetic thermal videos as indepen-",
          "A.\nSynthetic Thermal Videos": ""
        },
        {
          "surpassed\nthe\nothers\nin\nperformance while\nintegrating\nall": "",
          "A.\nSynthetic Thermal Videos": "aims to distinguish authentic images from synthetic ones via"
        },
        {
          "surpassed\nthe\nothers\nin\nperformance while\nintegrating\nall": "dent modalities,\n(3) exploring the effectiveness of\nthe thermal-",
          "A.\nSynthetic Thermal Videos": ""
        },
        {
          "surpassed\nthe\nothers\nin\nperformance while\nintegrating\nall": "",
          "A.\nSynthetic Thermal Videos": "the following minimax game:"
        },
        {
          "surpassed\nthe\nothers\nin\nperformance while\nintegrating\nall": "related information, and (4) analyzing the performance and",
          "A.\nSynthetic Thermal Videos": ""
        },
        {
          "surpassed\nthe\nothers\nin\nperformance while\nintegrating\nall": "application of\nthe newly introduced Vision-MLP architectures.",
          "A.\nSynthetic Thermal Videos": "min\nmax\n(1)\nLcGANpG, Dq,"
        },
        {
          "surpassed\nthe\nothers\nin\nperformance while\nintegrating\nall": "",
          "A.\nSynthetic Thermal Videos": "G\nD"
        },
        {
          "surpassed\nthe\nothers\nin\nperformance while\nintegrating\nall": "II. RELATED WORK",
          "A.\nSynthetic Thermal Videos": "where the objective function LcGANpG, Dq can be expressed"
        },
        {
          "surpassed\nthe\nothers\nin\nperformance while\nintegrating\nall": "",
          "A.\nSynthetic Thermal Videos": "as:"
        },
        {
          "surpassed\nthe\nothers\nin\nperformance while\nintegrating\nall": "Recent\ndevelopments\nhave\nseen\na\nrange\nof\ninnovative",
          "A.\nSynthetic Thermal Videos": ""
        },
        {
          "surpassed\nthe\nothers\nin\nperformance while\nintegrating\nall": "methods to assess pain levels from video data. Werner et al. [20]",
          "A.\nSynthetic Thermal Videos": "(2)\nEx,yrlog Dpx, yqs ` Ex,zrlogp1 ´ Dpx, Gpx, zqqqs,"
        },
        {
          "surpassed\nthe\nothers\nin\nperformance while\nintegrating\nall": "focused on domain-specific features, using facial action markers",
          "A.\nSynthetic Thermal Videos": ""
        },
        {
          "surpassed\nthe\nothers\nin\nperformance while\nintegrating\nall": "",
          "A.\nSynthetic Thermal Videos": "where x represents the real data, y signifies the target data, and"
        },
        {
          "surpassed\nthe\nothers\nin\nperformance while\nintegrating\nall": "with a deep random forest\n(RF) classifier, and proposed a 3D",
          "A.\nSynthetic Thermal Videos": ""
        },
        {
          "surpassed\nthe\nothers\nin\nperformance while\nintegrating\nall": "",
          "A.\nSynthetic Thermal Videos": "z denotes the random noise vector. The G aims to minimize the"
        },
        {
          "surpassed\nthe\nothers\nin\nperformance while\nintegrating\nall": "distance computation method among facial points while in [21],",
          "A.\nSynthetic Thermal Videos": ""
        },
        {
          "surpassed\nthe\nothers\nin\nperformance while\nintegrating\nall": "",
          "A.\nSynthetic Thermal Videos": "objective function, while the D functions adversarially,\ntrying"
        },
        {
          "surpassed\nthe\nothers\nin\nperformance while\nintegrating\nall": "an optical flow method was introduced to track facial points",
          "A.\nSynthetic Thermal Videos": ""
        },
        {
          "surpassed\nthe\nothers\nin\nperformance while\nintegrating\nall": "",
          "A.\nSynthetic Thermal Videos": "to maximize\nit. Furthermore, we\nincluded the Wasserstein"
        },
        {
          "surpassed\nthe\nothers\nin\nperformance while\nintegrating\nall": "and capture expression changes across frames. The dynamic",
          "A.\nSynthetic Thermal Videos": ""
        },
        {
          "surpassed\nthe\nothers\nin\nperformance while\nintegrating\nall": "",
          "A.\nSynthetic Thermal Videos": "gradient penalty (WGAN-GP)\n[32]\nto increase the training"
        },
        {
          "surpassed\nthe\nothers\nin\nperformance while\nintegrating\nall": "aspects of pain were addressed by developing long short-term",
          "A.\nSynthetic Thermal Videos": ""
        },
        {
          "surpassed\nthe\nothers\nin\nperformance while\nintegrating\nall": "",
          "A.\nSynthetic Thermal Videos": "stability. The final objective is described as:"
        },
        {
          "surpassed\nthe\nothers\nin\nperformance while\nintegrating\nall": "memory networks\ncombined with sparse\ncoding (SLSTM)",
          "A.\nSynthetic Thermal Videos": ""
        },
        {
          "surpassed\nthe\nothers\nin\nperformance while\nintegrating\nall": "[22]. Tavakolian et al.\n[23] utilized 3D convolutional neural",
          "A.\nSynthetic Thermal Videos": ""
        },
        {
          "surpassed\nthe\nothers\nin\nperformance while\nintegrating\nall": "",
          "A.\nSynthetic Thermal Videos": "(3)\nLcGANpG, Dq ` λEˆx,yrp}∇ˆxDpˆx, yq}2 ´ 1q2s,"
        },
        {
          "surpassed\nthe\nothers\nin\nperformance while\nintegrating\nall": "networks (CNNs) with varied temporal depths to analyze short-,",
          "A.\nSynthetic Thermal Videos": ""
        },
        {
          "surpassed\nthe\nothers\nin\nperformance while\nintegrating\nall": "mid-, and long-term facial expressions.\nIn [24],\nthe authors",
          "A.\nSynthetic Thermal Videos": "where λ denotes the penalty coefficient. Regarding the architec-"
        },
        {
          "surpassed\nthe\nothers\nin\nperformance while\nintegrating\nall": "leverage the temporal aspect of videos by encoding frames",
          "A.\nSynthetic Thermal Videos": "ture,\nin the proposed methodology,\ninspired by [33],\nthe G is"
        },
        {
          "surpassed\nthe\nothers\nin\nperformance while\nintegrating\nall": "into motion history and optical flow images, which were then",
          "A.\nSynthetic Thermal Videos": "structured into 3 distinct modules: an encoder, which comprises"
        },
        {
          "surpassed\nthe\nothers\nin\nperformance while\nintegrating\nall": "analyzed using a combination of CNN and bidirectional LSTM",
          "A.\nSynthetic Thermal Videos": "2 convolutional\nlayers downsampling the input; an intermediate"
        },
        {
          "surpassed\nthe\nothers\nin\nperformance while\nintegrating\nall": "(biLSTM). Another method encoded videos into single RGB",
          "A.\nSynthetic Thermal Videos": "ResNet module, consisting of 9 residual blocks, each consisting"
        },
        {
          "surpassed\nthe\nothers\nin\nperformance while\nintegrating\nall": "images\nthrough statistical\nspatiotemporal distillation (SSD)",
          "A.\nSynthetic Thermal Videos": "with 2 convolutional\nlayers; and a decoder, upsampling the"
        },
        {
          "surpassed\nthe\nothers\nin\nperformance while\nintegrating\nall": "and trained a Siamese network in a self-supervised manner",
          "A.\nSynthetic Thermal Videos": "feature maps to the final\nfor\nthe\nresolution (i.e., 256 ˆ 256)"
        },
        {
          "surpassed\nthe\nothers\nin\nperformance while\nintegrating\nall": "[25].\nIn [26]\nthe authors\nimplemented a multi-stream CNN",
          "A.\nSynthetic Thermal Videos": "synthetic\nsample. The D founded on [34]\nis\na pixel-level"
        },
        {
          "surpassed\nthe\nothers\nin\nperformance while\nintegrating\nall": "for\nfeature extraction from different\nfacial\nregions, applying",
          "A.\nSynthetic Thermal Videos": "PatchGAN discriminator using 1 ˆ 1 kernels consisting of 2"
        },
        {
          "surpassed\nthe\nothers\nin\nperformance while\nintegrating\nall": "learned weights to emphasize the significance of each region’s",
          "A.\nSynthetic Thermal Videos": "convolutional\nlayers."
        },
        {
          "surpassed\nthe\nothers\nin\nperformance while\nintegrating\nall": "features\nin expressing pain. Further\nresearch [27]\nidentified",
          "A.\nSynthetic Thermal Videos": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Cross-Attention": "",
          "1x": ""
        },
        {
          "Cross-Attention": "FCN",
          "1x": ""
        },
        {
          "Cross-Attention": "",
          "1x": ""
        },
        {
          "Cross-Attention": "Self-Attention",
          "1x": "8x"
        },
        {
          "Cross-Attention": "FCN",
          "1x": ""
        },
        {
          "Cross-Attention": "",
          "1x": ""
        },
        {
          "Cross-Attention": "",
          "1x": ""
        },
        {
          "Cross-Attention": "Self-Attention",
          "1x": "8x"
        },
        {
          "Cross-Attention": "",
          "1x": ""
        },
        {
          "Cross-Attention": "FCN",
          "1x": ""
        },
        {
          "Cross-Attention": "",
          "1x": ""
        },
        {
          "Cross-Attention": "",
          "1x": ""
        },
        {
          "Cross-Attention": "Self-Attention",
          "1x": "8x"
        },
        {
          "Cross-Attention": "FCN",
          "1x": ""
        },
        {
          "Cross-Attention": "",
          "1x": ""
        },
        {
          "Cross-Attention": "",
          "1x": ""
        },
        {
          "Cross-Attention": "",
          "1x": ""
        },
        {
          "Cross-Attention": "",
          "1x": ""
        },
        {
          "Cross-Attention": "Pain Assessment",
          "1x": ""
        },
        {
          "Cross-Attention": "",
          "1x": ""
        },
        {
          "Cross-Attention": "Transformer",
          "1x": ""
        },
        {
          "Cross-Attention": "",
          "1x": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MLP": "Channel-Mixer\nMLP"
        },
        {
          "MLP": "Pain Assessment"
        },
        {
          "MLP": "MLP\nInput Image"
        },
        {
          "MLP": "Transformer\nToken-Mixer"
        },
        {
          "MLP": "Vision-MLP\nChannel-Mixer"
        },
        {
          "MLP": "Fig. 2: Schematic overview of\nthe proposed framework for automatic pain assessment, detailing its modules and their primary"
        },
        {
          "MLP": "components:\n(a) The Vision-MLP module,\nresponsible for extracting embeddings from individual\nframes.\n(b) The Token-Mixer,"
        },
        {
          "MLP": "another major\nsub-module of Vision-MLP, creates\nthe wave representation for\nthe tokens.\n(c) The Channel-Mixer, a key"
        },
        {
          "MLP": "sub-module within Vision-MLP.\n(c) The MLP, an integral part of\nthe Channel-Mixer.\n(e) The fusion process combining RGB"
        },
        {
          "MLP": "and synthetic thermal embeddings,\nfollowed by the Transformer module, which performs the final pain assessment."
        },
        {
          "MLP": "B. Framework Architecture\ncommunication between different channels and is formulated"
        },
        {
          "MLP": "as:"
        },
        {
          "MLP": "The\nproposed\nframework\ncomprises\ntwo main modules:"
        },
        {
          "MLP": "(4)\nChannel-Mixerpfj, W cq “ W cfj"
        },
        {
          "MLP": "a Vision-MLP model\nthat\nfunctions as a spatial embedding"
        },
        {
          "MLP": "extractor\nfor\nindividual video frames and a transformer-based"
        },
        {
          "MLP": "where W c denotes the weight matrix with learnable parameters,"
        },
        {
          "MLP": "model that serves as a temporal module, utilizing the embedded"
        },
        {
          "MLP": "the second main sub-module, Token-\nand j “ 1, 2, . . . , n. Next,"
        },
        {
          "MLP": "representations of\nthe videos for\ntemporal analysis and final"
        },
        {
          "MLP": "Mixer\n(Fig.\n2b),\nallows\ncommunication\nbetween\ndifferent"
        },
        {
          "MLP": "pain assessment. Fig. 2 illustrates the modules and their primary"
        },
        {
          "MLP": "tokens,\nenabling\nfeature\nextraction\nfrom different\nspatial"
        },
        {
          "MLP": "building blocks."
        },
        {
          "MLP": "locations. Typically,\nin MLP-based models,\nthe token-mixers"
        },
        {
          "MLP": "1) Vision-MLP: MLP-like models\nrepresent\na newly in-"
        },
        {
          "MLP": "formulated as:"
        },
        {
          "MLP": "ÿ"
        },
        {
          "MLP": "troduced type of vision models,\nserving as\nalternatives\nto"
        },
        {
          "MLP": "W t\n(5)\nToken-MixerpF, W tqj “\njk d fk,"
        },
        {
          "MLP": "traditional Convolutional Neural Networks\n(CNNs) and Vi-"
        },
        {
          "MLP": "k"
        },
        {
          "MLP": "sion Transformers\n(ViT). They are characterized by simple"
        },
        {
          "MLP": "where W t denotes\nthe corresponding weight matrix for\nthe\narchitectures consisting of\nfully connected layers coupled with"
        },
        {
          "MLP": "tokens,\nand\nelement-wise multiplication.\nthe d represents\nactivation functions. They embody lesser inductive bias and are"
        },
        {
          "MLP": "Our proposed approach transforms the tokens into wave-like\nbased on basic matrix multiplication routines. Our approach is"
        },
        {
          "MLP": "representations to modulate the relationship between tokens\nfounded on the principles of\n[35]\nintroducing the Vision-MLP"
        },
        {
          "MLP": "and weights dynamically according to their semantic content.\nand [36]\nincorporating a wave representation for\nthe patches"
        },
        {
          "MLP": "˜"
        },
        {
          "MLP": "In order\nas wave\nthrough a wave\n(also referred to as tokens). Each video frame is initially divided\nto represent a token fj\nfj"
        },
        {
          "MLP": "function, amplitude and phase information are needed:\ninto n non-overlapping tokens Fm “ rfm,1, fm,2, . . . , fm,ns P"
        },
        {
          "MLP": "i.e.,\nRnˆpˆpˆ3, where p specifies the resolution of each token,"
        },
        {
          "MLP": "(6)\nfj “ |fj| d eiθj ."
        },
        {
          "MLP": "16 ˆ 16 pixels, and 3 represents the number of color channels."
        },
        {
          "MLP": "Each token is then linearly projected into a dimension d “ 768\nHere, i denotes the imaginary unit satisfying i2 “ ´1. The term"
        },
        {
          "MLP": "before being fed into the Vision-MLP (refer\nto Fig. 2a). The\nis\n|fj| represents the amplitude of the signal. The function eiθj"
        },
        {
          "MLP": "first main sub-module is the so-called Channel-Mixer (Fig. 2c),\nsymbolizes the phase of\nthe signal.\na periodic function, and θj"
        },
        {
          "MLP": "independently and allows\nwhich operates on each token fj\nThe amplitude |fj| can be likened to the real-valued feature"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "found in conventional models, with the notable distinction being": "the application of the absolute value operation.\nIn the practical",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": "Query, Key, and Value matrices, respectively, where M denotes"
        },
        {
          "found in conventional models, with the notable distinction being": "implementation,\nthe absolute value operation is omitted and",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": "the input dimension, and C the channel dimension. Similarly,"
        },
        {
          "found in conventional models, with the notable distinction being": "for each token\nreplaced with 4 for simplicity. The phase θj",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": "the cross-attention mechanism employs a dot product operation,"
        },
        {
          "found in conventional models, with the notable distinction being": "reflects\nits position within a wave’s cycle and can thus be",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": "but\nthe Q instead of M ˆ C is N ˆ C, where N ă M offers"
        },
        {
          "found in conventional models, with the notable distinction being": "described using fixed parameters, which are learnable during",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": "a computational cost\nreduction. Each self and cross-attention"
        },
        {
          "found in conventional models, with the notable distinction being": "the training phase. Consequently, 4 is\nalso utilized for\nthe",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": "block incorporates 1 and 8 attention heads,\nrespectively, while"
        },
        {
          "found in conventional models, with the notable distinction being": "phase estimation. Given that 6 characterizes a wave within",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": "4 parallel blocks comprise the whole Transformer module. The"
        },
        {
          "found in conventional models, with the notable distinction being": "the complex domain,\nthe Euler\nformula facilitates embedding",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": "resulting output embeddings, with a dimensionality of 340, are"
        },
        {
          "found in conventional models, with the notable distinction being": "tokens within the neural network architecture:",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": "employed to complete the final pain assessment\nthrough a fully"
        },
        {
          "found in conventional models, with the notable distinction being": "",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": "connected neural network."
        },
        {
          "found in conventional models, with the notable distinction being": "˜",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": ""
        },
        {
          "found in conventional models, with the notable distinction being": "(7)\nfj “ |fj| d cos θj ` i|fj| d sin θj.",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": ""
        },
        {
          "found in conventional models, with the notable distinction being": "",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": "C. Augmentation Methods"
        },
        {
          "found in conventional models, with the notable distinction being": "Combining 5 and 7, a token is represented as:",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": ""
        },
        {
          "found in conventional models, with the notable distinction being": "ÿ",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": "Two augmentation techniques have been implemented within"
        },
        {
          "found in conventional models, with the notable distinction being": "W t",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": "the framework. First,\nthe so-called Basic is employed,\ninte-"
        },
        {
          "found in conventional models, with the notable distinction being": "(8)\nfj “\njkfk d cos θk ` W i\njkfk d sin θk",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": ""
        },
        {
          "found in conventional models, with the notable distinction being": "ÿ",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": "grating polarity inversion with noise addition. This method"
        },
        {
          "found in conventional models, with the notable distinction being": "W t",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": "transforms\nthe\noriginal\ninput\nembedding\nby\nreversing\nthe"
        },
        {
          "found in conventional models, with the notable distinction being": "(9)\nùñ\njkfk d cospW cfkq ` W i\njkfk d sinpW cfkq",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": ""
        },
        {
          "found in conventional models, with the notable distinction being": "",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": "polarity of data elements and adding random noise from a"
        },
        {
          "found in conventional models, with the notable distinction being": "where W t, W c and W i are learnable weight matrices. The",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": "Gaussian distribution, creating variability and perturbations."
        },
        {
          "found in conventional models, with the notable distinction being": "process described, which pertains to wave-like representations,",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": "Second,\nthe Masking involves\napplying zero-valued masks"
        },
        {
          "found in conventional models, with the notable distinction being": "unfolds within the Token-Mixer, particularly in the Wave-Block.",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": "to the embeddings, nullifying segments of\nthe vectors. The"
        },
        {
          "found in conventional models, with the notable distinction being": "The Token-Mixer\narchitecture\ncomprises\nthree blocks:\ntwo",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": "dimensions of\nthe masks are randomly determined, spanning"
        },
        {
          "found in conventional models, with the notable distinction being": "Wave-Blocks\nand\none Channel-Mixer\noperating\nin\nparallel.",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": "10% to 50% of the embedding’s total dimensions, and they are"
        },
        {
          "found in conventional models, with the notable distinction being": "The Vision-MLP module is structured into four stages. Each",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": "positioned at\nrandom locations within the embeddings."
        },
        {
          "found in conventional models, with the notable distinction being": "stage comprises a sequence consisting of a Token-Mixer and",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": ""
        },
        {
          "found in conventional models, with the notable distinction being": "",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": "D. Pre-processing"
        },
        {
          "found in conventional models, with the notable distinction being": "a Channel-Mixer block, with a normalization layer preceding",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": ""
        },
        {
          "found in conventional models, with the notable distinction being": "each. The depth of parallel blocks in each stage is 3, 4, 18, and",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": "The pre-processing involved face detection to isolate the"
        },
        {
          "found in conventional models, with the notable distinction being": "3,\nrespectively. This structure facilitates extracting hierarchical",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": "facial\nregion. The MTCNN face detector\n[37] was employed,"
        },
        {
          "found in conventional models, with the notable distinction being": "embeddings with corresponding dimensions across stages 64,",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": "which utilizes multitask cascaded convolutional neural networks"
        },
        {
          "found in conventional models, with the notable distinction being": "128, 320, and 100.",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": "for predicting faces and landmarks.\nIt\nis important\nto note that"
        },
        {
          "found in conventional models, with the notable distinction being": "2) Fusion: For each input\nframe,\nthe Vision-MLP extracts",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": "the\nface detector was\napplied only to the\nindividual RGB"
        },
        {
          "found in conventional models, with the notable distinction being": "an embedding with a dimensionality of d “ 100. Subsequently,",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": "frames, and the coordinates of\nthe detected face were applied"
        },
        {
          "found in conventional models, with the notable distinction being": "the\nembeddings\nderived\nfrom the\nrespective\nframes\nof\na",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": "to the corresponding synthetic thermal\nframes. The resolution"
        },
        {
          "found in conventional models, with the notable distinction being": "particular video are concatenated to create a unified embedding",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": "of all\nframes was set at 224 ˆ 224 pixels."
        },
        {
          "found in conventional models, with the notable distinction being": "representation of\nthe original video:",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": ""
        },
        {
          "found in conventional models, with the notable distinction being": "",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": "E. Pre-training"
        },
        {
          "found in conventional models, with the notable distinction being": "(10)\nVD “ rd1}d2} ¨ ¨ ¨ }dms,\nVD P RN ,",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": ""
        },
        {
          "found in conventional models, with the notable distinction being": "",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": "For\nthe I2I approach,\nthe SpeakingFaces [38] dataset was"
        },
        {
          "found in conventional models, with the notable distinction being": "",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": "utilized to train the proposed GAN model\nfor\ntranslating the"
        },
        {
          "found in conventional models, with the notable distinction being": "where m denotes the number of frames in a video, and N repre-",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": ""
        },
        {
          "found in conventional models, with the notable distinction being": "",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": "RGB to synthetic\nthermal videos.\nIn addition, prior\nto the"
        },
        {
          "found in conventional models, with the notable distinction being": "sents the dimensionality of\nthe final embedding. Subsequently,",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": ""
        },
        {
          "found in conventional models, with the notable distinction being": "",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": "automatic pain assessment\ntraining process,\nthe Vision-MLP"
        },
        {
          "found in conventional models, with the notable distinction being": "the embeddings derived from RGB and synthetic thermal videos",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": ""
        },
        {
          "found in conventional models, with the notable distinction being": "",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": "and Transformer modules were pre-trained. The Vision-MLP"
        },
        {
          "found in conventional models, with the notable distinction being": "are integrated through a weighted fusion process:",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": ""
        },
        {
          "found in conventional models, with the notable distinction being": "",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": "underwent a three-stage pre-training strategy:\ninitially,\nit was"
        },
        {
          "found in conventional models, with the notable distinction being": "(11)\nVF used “ w1 ¨ VRGB ` w2 ¨ VT hermal,\nVF used P RN .",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": "trained on DigiFace-1M [39]\nto learn basic\nfacial\nfeatures."
        },
        {
          "found in conventional models, with the notable distinction being": "",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": "Subsequently,\nit was trained on AffectNet\n[40] and RAF Face"
        },
        {
          "found in conventional models, with the notable distinction being": "The fusion process is founded on combining the corresponding",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": ""
        },
        {
          "found in conventional models, with the notable distinction being": "",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": "Database basic [41] to learn features related to basic emotions"
        },
        {
          "found in conventional models, with the notable distinction being": "embeddings,\nutilizing\nlearned weights w1\nand w2, which",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": ""
        },
        {
          "found in conventional models, with the notable distinction being": "",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": "through multi-task learning. Finally,\nthe Compound Facial"
        },
        {
          "found in conventional models, with the notable distinction being": "modulate the contributions of the RGB and thermal embeddings,",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": ""
        },
        {
          "found in conventional models, with the notable distinction being": "",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": "Expressions of Emotions Database [42] and the RAF Face"
        },
        {
          "found in conventional models, with the notable distinction being": "respectively. The weighted addition provides\nan optimized",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": ""
        },
        {
          "found in conventional models, with the notable distinction being": "",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": "Database compound [41] were utilized to learn features of"
        },
        {
          "found in conventional models, with the notable distinction being": "integration,\nreflecting the importance of each modality in the",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": ""
        },
        {
          "found in conventional models, with the notable distinction being": "",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": "compound emotions in a similar multi-task setting. The multi-"
        },
        {
          "found in conventional models, with the notable distinction being": "final\nfused representation VF used.",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": ""
        },
        {
          "found in conventional models, with the notable distinction being": "",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": "task learning process is described as:"
        },
        {
          "found in conventional models, with the notable distinction being": "3) Transformer: The fused embeddings are subsequently",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": ""
        },
        {
          "found in conventional models, with the notable distinction being": "",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": "(13)\nLtotal “ rew1LS1 ` w1s ` rew2LS2 ` w2s,"
        },
        {
          "found in conventional models, with the notable distinction being": "fed into a transformer-based module comprising self-attention",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": ""
        },
        {
          "found in conventional models, with the notable distinction being": "and cross-attention blocks (Fig. 2e). The self-attention process",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": ""
        },
        {
          "found in conventional models, with the notable distinction being": "",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": "is\nthe loss\nfor\nthe corresponding task related to\nwhere LS"
        },
        {
          "found in conventional models, with the notable distinction being": "is represented as follows:",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": ""
        },
        {
          "found in conventional models, with the notable distinction being": "",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": "different datasets, and w represents the learned weights that"
        },
        {
          "found in conventional models, with the notable distinction being": "ˆ\n˙",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": ""
        },
        {
          "found in conventional models, with the notable distinction being": "QK T",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": "drive the learning process in minimizing the combined loss"
        },
        {
          "found in conventional models, with the notable distinction being": "V\n.\n?\n(12)\nAttentionpQ, K, V q “ sof tmax",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": ""
        },
        {
          "found in conventional models, with the notable distinction being": "",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": "the individual\nlosses. The Transformer\nLtotal, considering all"
        },
        {
          "found in conventional models, with the notable distinction being": "dk",
          "the\nHere, Q P RM ˆC, K P RM ˆC, and V P RM ˆC represent": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE I: Datasets utilized for": "framework.",
          "the pretraining process of": "",
          "the": "",
          "TABLE III: Classification results utilizing the RGB video": "reported on accuracy %."
        },
        {
          "TABLE I: Datasets utilized for": "Dataset",
          "the pretraining process of": "",
          "the": "",
          "TABLE III: Classification results utilizing the RGB video": "Augmentations\nTask"
        },
        {
          "TABLE I: Datasets utilized for": "",
          "the pretraining process of": "",
          "the": "",
          "TABLE III: Classification results utilizing the RGB video": ""
        },
        {
          "TABLE I: Datasets utilized for": "",
          "the pretraining process of": "",
          "the": "",
          "TABLE III: Classification results utilizing the RGB video": "Basic\nMasking\nP(Aug)\nMC\nNP vs P4"
        },
        {
          "TABLE I: Datasets utilized for": "SpeakingFaces [38]",
          "the pretraining process of": "",
          "the": "",
          "TABLE III: Classification results utilizing the RGB video": ""
        },
        {
          "TABLE I: Datasets utilized for": "",
          "the pretraining process of": "",
          "the": "",
          "TABLE III: Classification results utilizing the RGB video": ""
        },
        {
          "TABLE I: Datasets utilized for": "DigiFace-1M [39]",
          "the pretraining process of": "",
          "the": "",
          "TABLE III: Classification results utilizing the RGB video": ""
        },
        {
          "TABLE I: Datasets utilized for": "",
          "the pretraining process of": "",
          "the": "",
          "TABLE III: Classification results utilizing the RGB video": "✓"
        },
        {
          "TABLE I: Datasets utilized for": "",
          "the pretraining process of": "",
          "the": "",
          "TABLE III: Classification results utilizing the RGB video": "30.23\n10-20\n0.7\n69.37"
        },
        {
          "TABLE I: Datasets utilized for": "AffectNet",
          "the pretraining process of": "",
          "the": "",
          "TABLE III: Classification results utilizing the RGB video": ""
        },
        {
          "TABLE I: Datasets utilized for": "",
          "the pretraining process of": "",
          "the": "",
          "TABLE III: Classification results utilizing the RGB video": "✓"
        },
        {
          "TABLE I: Datasets utilized for": "",
          "the pretraining process of": "",
          "the": "",
          "TABLE III: Classification results utilizing the RGB video": "70.26\n20-50\n0.7\n28.50"
        },
        {
          "TABLE I: Datasets utilized for": "Compound FEE-DB [42]",
          "the pretraining process of": "",
          "the": "",
          "TABLE III: Classification results utilizing the RGB video": ""
        },
        {
          "TABLE I: Datasets utilized for": "",
          "the pretraining process of": "",
          "the": "",
          "TABLE III: Classification results utilizing the RGB video": "✓"
        },
        {
          "TABLE I: Datasets utilized for": "",
          "the pretraining process of": "",
          "the": "",
          "TABLE III: Classification results utilizing the RGB video": "30-50\n0.9\n70.05\n30.02"
        },
        {
          "TABLE I: Datasets utilized for": "RAF-DB basic [41]",
          "the pretraining process of": "",
          "the": "",
          "TABLE III: Classification results utilizing the RGB video": ""
        },
        {
          "TABLE I: Datasets utilized for": "",
          "the pretraining process of": "",
          "the": "",
          "TABLE III: Classification results utilizing the RGB video": ""
        },
        {
          "TABLE I: Datasets utilized for": "",
          "the pretraining process of": "",
          "the": "",
          "TABLE III: Classification results utilizing the RGB video": "Masking: indicates the percentage of the input embedding to which zero-value masking"
        },
        {
          "TABLE I: Datasets utilized for": "RAF-DB compound [41]",
          "the pretraining process of": "",
          "the": "",
          "TABLE III: Classification results utilizing the RGB video": ""
        },
        {
          "TABLE I: Datasets utilized for": "",
          "the pretraining process of": "",
          "the": "",
          "TABLE III: Classification results utilizing the RGB video": ""
        },
        {
          "TABLE I: Datasets utilized for": "",
          "the pretraining process of": "",
          "the": "",
          "TABLE III: Classification results utilizing the RGB video": "P(Aug):\nrepresents the probability of applying the augmentation methods"
        },
        {
          "TABLE I: Datasets utilized for": ":",
          "the pretraining process of": ":",
          "the": "",
          "TABLE III: Classification results utilizing the RGB video": "NP: No Pain\nMC: multiclass pain level\nP4: Very Severe Pain"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE IV: Classification results utilizing the synthetic thermal": ""
        },
        {
          "TABLE IV: Classification results utilizing the synthetic thermal": "reported on accuracy %."
        },
        {
          "TABLE IV: Classification results utilizing the synthetic thermal": ""
        },
        {
          "TABLE IV: Classification results utilizing the synthetic thermal": "Augmentations"
        },
        {
          "TABLE IV: Classification results utilizing the synthetic thermal": ""
        },
        {
          "TABLE IV: Classification results utilizing the synthetic thermal": ""
        },
        {
          "TABLE IV: Classification results utilizing the synthetic thermal": "Masking\nP(Aug)"
        },
        {
          "TABLE IV: Classification results utilizing the synthetic thermal": ""
        },
        {
          "TABLE IV: Classification results utilizing the synthetic thermal": ""
        },
        {
          "TABLE IV: Classification results utilizing the synthetic thermal": "10-20\n0.7"
        },
        {
          "TABLE IV: Classification results utilizing the synthetic thermal": ""
        },
        {
          "TABLE IV: Classification results utilizing the synthetic thermal": "20-50\n0.7"
        },
        {
          "TABLE IV: Classification results utilizing the synthetic thermal": ""
        },
        {
          "TABLE IV: Classification results utilizing the synthetic thermal": "30-50\n0.9"
        },
        {
          "TABLE IV: Classification results utilizing the synthetic thermal": ""
        },
        {
          "TABLE IV: Classification results utilizing the synthetic thermal": ""
        },
        {
          "TABLE IV: Classification results utilizing the synthetic thermal": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": "The\nfindings\nfrom IV-A and\nIV-B revealed\na\nnotable"
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": "circumstance where\nthe performance metrics\nfor\nthe RGB"
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": "and\nsynthetic\nthermal modalities\nare\nremarkably\nsimilar."
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": "Specifically,\nthe\nhighest\nrecorded\naccuracies\nfor\nthe RGB"
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": "modality were 70.26% and 30.23% for\nthe NP vs. P4 and"
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": "MC tasks,\nrespectively. Correspondingly,\nthe peak accuracies"
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": "for\nthe synthetic thermal modality were 70.69% and 30.50%."
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": "On average,\nthe performances\nfrom the thermal videos are"
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": "approximately 1% superior to those of the RGB modality. This"
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": "outcome was unexpected, given that\nthe synthetic modality was"
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": "initially presumed to be less effective than the original. This"
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": ""
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": "prompted an exploration into the reason synthetic modalities"
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": ""
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": "exhibit comparable or superior performance to the original RGB"
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": ""
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": "modality. A primary question was regarding the richness and"
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": ""
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": "effectiveness of\nthe thermal-related information incorporated"
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": "in the synthetic videos. The hypothesis suggested that reducing"
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": "facial expressions in the thermal videos could allow a more ex-"
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": "plicit assessment of the thermal\ninformation. Gaussian blurring"
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": "was progressively applied to RGB and synthetic thermal videos"
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": "(refer\nto Fig. 3), with kernel sizes k incrementally adjusted"
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": "from 0 to 191. Similar, albeit\nless time-intensive, experiments"
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": "to IV-A,\nIV-B were conducted."
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": "Table V shows\nthat with\na\nkernel\nsize\nof\nthe\nk “ 0,"
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": "performance disparity of 0.47% (favoring the thermal modality)"
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": "aligns with prior experimental outcomes. As blurring intensifies"
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": "this discrepancy marginally increases\nto 0.49%.\nto k “ 41,"
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": "the divergence expands\nto 2.13% and\nNotably, at k “ 91,"
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": "intensifies to 5.90% when the blur peaks at k “ 191 (heavily"
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": "blurred). The classification performances demonstrated that by"
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": "diminishing the visibility of facial expressions through blurring,"
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": "the synthetic thermal videos resulted in superior performance"
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": "compared to the RGB, with figures of 66.24% over 60.34%."
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": "Additionally,\nas\nthe kernel\nsize\nto\nincreased from k “ 0"
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": "the decline in accuracy rates for the synthetic thermal\nk “ 191,"
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": "and RGB modalities was 1.81% and 7.13%,\nrespectively. This"
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": "suggests that\nthe residual\ninformation in the synthetic modality,"
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": ""
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": "essentially the visually represented facial\ntemperature,\nremains"
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": ""
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": "intact or minimally influenced. Fig. 4 depicts the embedding"
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": ""
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": "distribution for\nthe RGB and synthetic thermal modality for"
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": "k “ 0 and k “ 191. Although the separation of the data points"
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": ""
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": "is not clear, we observe a distinct difference in the distribution."
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": ""
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": "For k “ 191, the RGB embeddings are centralized and probably"
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": "overlap, and a plethora of points are notably spread away from"
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": ""
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": "the central mass without a clear pattern. Respectively,\nthe data"
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": ""
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": "points are much more uniformly spreading for\nthe synthetic"
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": "modality, suggesting potentially better differentiation between"
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": ""
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": "classes."
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": ""
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": ""
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": "D. Fusion"
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": ""
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": "Three\nfusion methods were\nassessed\nin\nthe\ncontext\nof"
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": ""
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": "multimodal analysis for RGB and synthetic thermal videos. The"
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": ""
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": "approach outlined in 11 was initially applied, utilizing learned"
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": "weights w1 and w2 to scale the respective modalities. Addition-"
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": "ally, a second method was employed where a third weight, w3,"
        },
        {
          "C. Additional Analysis on RGB & Synthetic Thermal Videos": "was introduced, resulting in w3 ¨ pw1 ¨ VRGB ` w2 ¨ VT hermalq."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE VI: Classification results utilizing the fusion of RGB": "",
          "TABLE VII: Comparison of studies that utilized BioVid, videos,": ""
        },
        {
          "TABLE VI: Classification results utilizing the fusion of RGB": "",
          "TABLE VII: Comparison of studies that utilized BioVid, videos,": ""
        },
        {
          "TABLE VI: Classification results utilizing the fusion of RGB": "Fusion",
          "TABLE VII: Comparison of studies that utilized BioVid, videos,": ""
        },
        {
          "TABLE VI: Classification results utilizing the fusion of RGB": "",
          "TABLE VII: Comparison of studies that utilized BioVid, videos,": "Method"
        },
        {
          "TABLE VI: Classification results utilizing the fusion of RGB": "weights",
          "TABLE VII: Comparison of studies that utilized BioVid, videos,": ""
        },
        {
          "TABLE VI: Classification results utilizing the fusion of RGB": "",
          "TABLE VII: Comparison of studies that utilized BioVid, videos,": ""
        },
        {
          "TABLE VI: Classification results utilizing the fusion of RGB": "–",
          "TABLE VII: Comparison of studies that utilized BioVid, videos,": "Deep RF"
        },
        {
          "TABLE VI: Classification results utilizing the fusion of RGB": "W2",
          "TABLE VII: Comparison of studies that utilized BioVid, videos,": "RF"
        },
        {
          "TABLE VI: Classification results utilizing the fusion of RGB": "W3",
          "TABLE VII: Comparison of studies that utilized BioVid, videos,": "SLSTM"
        },
        {
          "TABLE VI: Classification results utilizing the fusion of RGB": "W2",
          "TABLE VII: Comparison of studies that utilized BioVid, videos,": "2D CNN, biLSTM"
        },
        {
          "TABLE VI: Classification results utilizing the fusion of RGB": "W2",
          "TABLE VII: Comparison of studies that utilized BioVid, videos,": "2D CNN"
        },
        {
          "TABLE VI: Classification results utilizing the fusion of RGB": "",
          "TABLE VII: Comparison of studies that utilized BioVid, videos,": "Vision-Transformer"
        },
        {
          "TABLE VI: Classification results utilizing the fusion of RGB": "[w1,w2]",
          "TABLE VII: Comparison of studies that utilized BioVid, videos,": ""
        },
        {
          "TABLE VI: Classification results utilizing the fusion of RGB": "",
          "TABLE VII: Comparison of studies that utilized BioVid, videos,": "Vision-MLP"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Lastly, a method without learned weights was explored, directly": "adding the embedding vectors from both modalities. Table VI",
          "TABLE VIII: Comparison with the MIntPAIN dataset, reported": "on accuracy%."
        },
        {
          "Lastly, a method without learned weights was explored, directly": "presents\nthe corresponding results. The absence of weights",
          "TABLE VIII: Comparison with the MIntPAIN dataset, reported": ""
        },
        {
          "Lastly, a method without learned weights was explored, directly": "resulted in 64.92% and 26.40% accuracy for\nthe binary and",
          "TABLE VIII: Comparison with the MIntPAIN dataset, reported": ""
        },
        {
          "Lastly, a method without learned weights was explored, directly": "",
          "TABLE VIII: Comparison with the MIntPAIN dataset, reported": "Task"
        },
        {
          "Lastly, a method without learned weights was explored, directly": "multi-class\ntasks,\nrespectively. The\nintegration of\nthe\nthree",
          "TABLE VIII: Comparison with the MIntPAIN dataset, reported": "Study\nDataset\nModality"
        },
        {
          "Lastly, a method without learned weights was explored, directly": "",
          "TABLE VIII: Comparison with the MIntPAIN dataset, reported": "MC"
        },
        {
          "Lastly, a method without learned weights was explored, directly": "weights resulted in a decrease of 0.5% in accuracy for both",
          "TABLE VIII: Comparison with the MIntPAIN dataset, reported": ""
        },
        {
          "Lastly, a method without learned weights was explored, directly": "tasks, whereas the application of weights w1 and w2 yielded",
          "TABLE VIII: Comparison with the MIntPAIN dataset, reported": "RGB\n18.55"
        },
        {
          "Lastly, a method without learned weights was explored, directly": "",
          "TABLE VIII: Comparison with the MIntPAIN dataset, reported": "Haque et al.\n[19]\nMIntPAIN\nThermal˝\n18.33"
        },
        {
          "Lastly, a method without learned weights was explored, directly": "the highest performance, with accuracies\nreaching 65.08%",
          "TABLE VIII: Comparison with the MIntPAIN dataset, reported": ""
        },
        {
          "Lastly, a method without learned weights was explored, directly": "",
          "TABLE VIII: Comparison with the MIntPAIN dataset, reported": "Fusion\n30.77"
        },
        {
          "Lastly, a method without learned weights was explored, directly": "and 26.50% for\nthe binary and multi-class tasks,\nrespectively.",
          "TABLE VIII: Comparison with the MIntPAIN dataset, reported": ""
        },
        {
          "Lastly, a method without learned weights was explored, directly": "",
          "TABLE VIII: Comparison with the MIntPAIN dataset, reported": "RGB\n30.02"
        },
        {
          "Lastly, a method without learned weights was explored, directly": "By applying weights w1 and w2 and increasing the training",
          "TABLE VIII: Comparison with the MIntPAIN dataset, reported": ""
        },
        {
          "Lastly, a method without learned weights was explored, directly": "",
          "TABLE VIII: Comparison with the MIntPAIN dataset, reported": "Our\nBioVid\nThermal‹\n29.69"
        },
        {
          "Lastly, a method without learned weights was explored, directly": "period from 100 to 300 epochs while maintaining consistent",
          "TABLE VIII: Comparison with the MIntPAIN dataset, reported": "Fusion\n30.70"
        },
        {
          "Lastly, a method without learned weights was explored, directly": "augmentation settings, accuracies of 69.50% and 29.80% were",
          "TABLE VIII: Comparison with the MIntPAIN dataset, reported": ""
        },
        {
          "Lastly, a method without learned weights was explored, directly": "",
          "TABLE VIII: Comparison with the MIntPAIN dataset, reported": "˝:real ‹: synthetic"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "Pain, vol. 157, no. 11, pp. 2420–2423, nov 2016."
        },
        {
          "ETHICAL IMPACT STATEMENT": "This research employed the BioVid Heat Pain Database [43]",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "[3] K. S and T. RS, “Neuroanatomy and neuropsychology of pain,” Cureus,"
        },
        {
          "ETHICAL IMPACT STATEMENT": "to evaluate the proposed methods. The data were recorded",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "vol. 9, no. 10, oct 2017."
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "in\nP. Dinakar and A. M. Stillman, “Pathogenesis of pain,” Seminars\n[4]"
        },
        {
          "ETHICAL IMPACT STATEMENT": "according to the ethical guidelines of Helsinki\n(ethics commit-",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "Pediatric Neurology, vol. 23, no. 3, pp. 201–208, aug 2016."
        },
        {
          "ETHICAL IMPACT STATEMENT": "tee: 196/10-UBB/bal). Prior\nto commencing data collection,",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[5]"
        },
        {
          "ETHICAL IMPACT STATEMENT": "each participant’s pain threshold (where sensation transitions",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "deep learning methods: A systematic review,” Computer Methods and"
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "Programs in Biomedicine, vol. 231, p. 107365, 2023."
        },
        {
          "ETHICAL IMPACT STATEMENT": "from heat\nto\npain)\nand\ntolerance\nthreshold\n(the moment",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "[6]\nL. A. Joel, “The fifth vital sign: pain,” AJN The American Journal of"
        },
        {
          "ETHICAL IMPACT STATEMENT": "when pain becomes unbearable) were determined. The facial",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "Nursing, vol. 99, no. 2, p. 9, 1999."
        },
        {
          "ETHICAL IMPACT STATEMENT": "images\npresented\nin\nthis\nstudy\nare\nfrom participants who",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "[7] R. Fernandez Rojas, N. Brown, G. Waddington, and R. Goecke, “A"
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "systematic review of neurophysiological sensing for\nthe assessment of"
        },
        {
          "ETHICAL IMPACT STATEMENT": "have consented to their use for\nillustrative purposes within",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "acute pain,” NPJ Digital Medicine, vol. 6, no. 1, p. 76, 2023."
        },
        {
          "ETHICAL IMPACT STATEMENT": "a scientific research context. This study aims to introduce a",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "[8]\nS. A. H. Aqajari, R. Cao, E. Kasaeyan Naeini, M.-D. Calderon, K. Zheng,"
        },
        {
          "ETHICAL IMPACT STATEMENT": "pain assessment\nframework designed to facilitate continuous",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "N. Dutt, P. Liljeberg, S. Salanter¨a, A. M. Nelson, and A. M. Rahmani,"
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "“Pain\nassessment\ntool with\nelectrodermal\nactivity\nfor\npostoperative"
        },
        {
          "ETHICAL IMPACT STATEMENT": "patient monitoring while reducing human biases. However,\nit\nis",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "patients: method validation study,” JMIR mHealth and uHealth, vol. 9,"
        },
        {
          "ETHICAL IMPACT STATEMENT": "essential\nto recognize that real-world applications, especially in",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "no. 5, p. e25258, 2021."
        },
        {
          "ETHICAL IMPACT STATEMENT": "clinical settings, might present challenges, necessitating further",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "[9] H. H. Yong, S. J. Gibson, D. J. Horne, and R. D. Helme, “Development"
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "of a pain attitudes questionnaire to assess\nstoicism and cautiousness"
        },
        {
          "ETHICAL IMPACT STATEMENT": "experimentation and comprehensive evaluation through clinical",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "for possible age differences.” The journals of gerontology. Series B,"
        },
        {
          "ETHICAL IMPACT STATEMENT": "trials before deployment.",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "Psychological sciences and social sciences, vol. 56, no. 5, pp. P279–84,"
        },
        {
          "ETHICAL IMPACT STATEMENT": "In addition, this study utilized the SpeakingFaces [38] dataset",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "sep 2001."
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "E. J. Bartley and R. B. Fillingim, “Sex differences in pain: a brief review\n[10]"
        },
        {
          "ETHICAL IMPACT STATEMENT": "for\nthe\nimage-to-image\ntranslation\nprocess. The\ndata was",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "of clinical and experimental findings,” British journal of anaesthesia,"
        },
        {
          "ETHICAL IMPACT STATEMENT": "collected according to the ethical guidelines of\nthe Declaration",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "vol. 111, no. 1, pp. 52–58,\njul 2013."
        },
        {
          "ETHICAL IMPACT STATEMENT": "of Helsinki,\nand with\nthe\napproval\nfrom the\nInstitutional",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "[11]\nS. Gkikas., C. Chatzaki., E.\nPavlidou.,\nF. Verigou., K. Kalkanis.,"
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "and M. Tsiknakis.,\n“Automatic\npain\nintensity\nestimation\nbased\non"
        },
        {
          "ETHICAL IMPACT STATEMENT": "Research Ethics Committee\nat Nazarbayev University. All",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "electrocardiogram and demographic factors.”\nSciTePress, 2022, pp."
        },
        {
          "ETHICAL IMPACT STATEMENT": "participants were volunteers who were fully informed about\nthe",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "155–162."
        },
        {
          "ETHICAL IMPACT STATEMENT": "data collection procedures and the intended use of\nidentifiable",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "[12]\nS. Gkikas, C. Chatzaki, and M. Tsiknakis, “Multi-task neural networks"
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "for pain intensity estimation using electrocardiogram and demographic"
        },
        {
          "ETHICAL IMPACT STATEMENT": "images, which will be distributed as part of a dataset. Each",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "factors,” in Information and Communication Technologies for Ageing"
        },
        {
          "ETHICAL IMPACT STATEMENT": "participant\nprovided\ntheir\npermission\nby\nsigning\ninformed",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "Well and e-Health.\nSpringer Nature Switzerland, 2023, pp. 324–337."
        },
        {
          "ETHICAL IMPACT STATEMENT": "consent\nforms.",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "[13] M. M. Al Qudah, A. S. Mohamed, and S. L. Lutfi, “Affective state"
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "recognition using thermal-based imaging: A survey.” Computer Systems"
        },
        {
          "ETHICAL IMPACT STATEMENT": "Furthermore, several datasets were utilized to pretrain the",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "Science & Engineering, vol. 37, no. 1, 2021."
        },
        {
          "ETHICAL IMPACT STATEMENT": "proposed pain assessment framework. The DigiFace-1M [39] is",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "[14]\nS.\nIoannou, V. Gallese, and A. Merla, “Thermal\ninfrared imaging in"
        },
        {
          "ETHICAL IMPACT STATEMENT": "a synthetic dataset where 511 initial\nface scans were obtained",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "psychophysiology: Potentialities and limits,” Psychophysiology, vol. 51,"
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "no. 10, pp. 951–963, 2014."
        },
        {
          "ETHICAL IMPACT STATEMENT": "with consent and employed to build a parametric face geometry",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "[15]\nS. Jarlier, D. Grandjean, S. Delplanque, K. N’Diaye,\nI. Cayeux, M.\nI."
        },
        {
          "ETHICAL IMPACT STATEMENT": "and texture library model. All\nthe identities and samples were",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "Velazco, D.\nSander,\nP. Vuilleumier,\nand K. R.\nScherer,\n“Thermal"
        },
        {
          "ETHICAL IMPACT STATEMENT": "generated from these source data. The AffectNet\n[40] dataset",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "analysis of\nfacial muscles contractions,” IEEE Transactions on Affective"
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "Computing, vol. 2, no. 1, pp. 2–9, 2011."
        },
        {
          "ETHICAL IMPACT STATEMENT": "is compiled using search engine queries. The original paper",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "[16] A. Merla, L. Di Donato, P. Rossini, and G. Romani, “Emotion detection"
        },
        {
          "ETHICAL IMPACT STATEMENT": "does not explicitly detail ethical compliance measures such as",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "through functional infrared imaging: preliminary results,” Biomedizinische"
        },
        {
          "ETHICAL IMPACT STATEMENT": "adherence to the Declaration of Helsinki or\ninformed consent",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "Technick, vol. 48, no. 2, pp. 284–286, 2004."
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "[17] Y. Mohamed, A. G¨uneysu, S. Lemaignan, and I. Leite, “Multi-modal"
        },
        {
          "ETHICAL IMPACT STATEMENT": "procedures. The original paper of Compound FEE-DB [42]",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "affect detection using thermal and optical\nimaging in a gamified robotic"
        },
        {
          "ETHICAL IMPACT STATEMENT": "does not mention ethical compliance measures, but only that",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "exercise,” International Journal of Social Robotics, Oct 2023."
        },
        {
          "ETHICAL IMPACT STATEMENT": "the subjects were recruited from the Ohio State University",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "¨\n[18] V. K. Erel and H. S.\nOzkan, “Thermal camera as a pain monitor,” Journal"
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "of Pain Research, vol. 10, pp. 2827–2832, 2017."
        },
        {
          "ETHICAL IMPACT STATEMENT": "area and received a monetary reward for participating. The",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "[19] M. A. Haque, R. B. Bautista, F. Noroozi, K. Kulkarni, C. B. Laursen,"
        },
        {
          "ETHICAL IMPACT STATEMENT": "RAF-DB [41] dataset was compiled using the Flickr\nimage",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "R.\nIrani, M. Bellantonio, S. Escalera, G. Anbarjafari, K. Nasrollahi,"
        },
        {
          "ETHICAL IMPACT STATEMENT": "hosting service. Although Flickr hosts both public and privately",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "O. K. Andersen, E. G. Spaich, and T. B. Moeslund, “Deep multimodal"
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "pain recognition: A database and comparison of spatio-temporal visual"
        },
        {
          "ETHICAL IMPACT STATEMENT": "shared images,\nthe authors do not explicitly mention the type",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "modalities,” in 2018 13th IEEE International Conference on Automatic"
        },
        {
          "ETHICAL IMPACT STATEMENT": "of\nthe downloaded images.",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "Face & Gesture Recognition (FG 2018), 2018, pp. 250–257."
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "[20]\nP. Werner, A. Al-Hamadi, K. Limbrecht-Ecklundt, S. Walter, S. Gruss, and"
        },
        {
          "ETHICAL IMPACT STATEMENT": "ACKNOWLEDGEMENT",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "H. C. Traue, “Automatic pain assessment with facial activity descriptors,”"
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "IEEE Transactions on Affective Computing, vol. 8, no. 3, pp. 286–299,"
        },
        {
          "ETHICAL IMPACT STATEMENT": "Research supported by the ODIN project\nthat has received",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "2016."
        },
        {
          "ETHICAL IMPACT STATEMENT": "funding from the European Union’s Horizon 2020 research and",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "[21]\nP. Werner, A. Al-Hamadi, and S. Walter, “Analysis of facial expressiveness"
        },
        {
          "ETHICAL IMPACT STATEMENT": "innovation program under grant agreement No 101017331.",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "during experimentally induced heat pain,” in 2017 Seventh International"
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "Conference on Affective Computing and Intelligent Interaction Workshops"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "Pain, vol. 157, no. 11, pp. 2420–2423, nov 2016."
        },
        {
          "ETHICAL IMPACT STATEMENT": "This research employed the BioVid Heat Pain Database [43]",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "[3] K. S and T. RS, “Neuroanatomy and neuropsychology of pain,” Cureus,"
        },
        {
          "ETHICAL IMPACT STATEMENT": "to evaluate the proposed methods. The data were recorded",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "vol. 9, no. 10, oct 2017."
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "in\nP. Dinakar and A. M. Stillman, “Pathogenesis of pain,” Seminars\n[4]"
        },
        {
          "ETHICAL IMPACT STATEMENT": "according to the ethical guidelines of Helsinki\n(ethics commit-",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "Pediatric Neurology, vol. 23, no. 3, pp. 201–208, aug 2016."
        },
        {
          "ETHICAL IMPACT STATEMENT": "tee: 196/10-UBB/bal). Prior\nto commencing data collection,",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[5]"
        },
        {
          "ETHICAL IMPACT STATEMENT": "each participant’s pain threshold (where sensation transitions",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "deep learning methods: A systematic review,” Computer Methods and"
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "Programs in Biomedicine, vol. 231, p. 107365, 2023."
        },
        {
          "ETHICAL IMPACT STATEMENT": "from heat\nto\npain)\nand\ntolerance\nthreshold\n(the moment",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "[6]\nL. A. Joel, “The fifth vital sign: pain,” AJN The American Journal of"
        },
        {
          "ETHICAL IMPACT STATEMENT": "when pain becomes unbearable) were determined. The facial",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "Nursing, vol. 99, no. 2, p. 9, 1999."
        },
        {
          "ETHICAL IMPACT STATEMENT": "images\npresented\nin\nthis\nstudy\nare\nfrom participants who",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "[7] R. Fernandez Rojas, N. Brown, G. Waddington, and R. Goecke, “A"
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "systematic review of neurophysiological sensing for\nthe assessment of"
        },
        {
          "ETHICAL IMPACT STATEMENT": "have consented to their use for\nillustrative purposes within",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "acute pain,” NPJ Digital Medicine, vol. 6, no. 1, p. 76, 2023."
        },
        {
          "ETHICAL IMPACT STATEMENT": "a scientific research context. This study aims to introduce a",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "[8]\nS. A. H. Aqajari, R. Cao, E. Kasaeyan Naeini, M.-D. Calderon, K. Zheng,"
        },
        {
          "ETHICAL IMPACT STATEMENT": "pain assessment\nframework designed to facilitate continuous",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "N. Dutt, P. Liljeberg, S. Salanter¨a, A. M. Nelson, and A. M. Rahmani,"
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "“Pain\nassessment\ntool with\nelectrodermal\nactivity\nfor\npostoperative"
        },
        {
          "ETHICAL IMPACT STATEMENT": "patient monitoring while reducing human biases. However,\nit\nis",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "patients: method validation study,” JMIR mHealth and uHealth, vol. 9,"
        },
        {
          "ETHICAL IMPACT STATEMENT": "essential\nto recognize that real-world applications, especially in",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "no. 5, p. e25258, 2021."
        },
        {
          "ETHICAL IMPACT STATEMENT": "clinical settings, might present challenges, necessitating further",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "[9] H. H. Yong, S. J. Gibson, D. J. Horne, and R. D. Helme, “Development"
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "of a pain attitudes questionnaire to assess\nstoicism and cautiousness"
        },
        {
          "ETHICAL IMPACT STATEMENT": "experimentation and comprehensive evaluation through clinical",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "for possible age differences.” The journals of gerontology. Series B,"
        },
        {
          "ETHICAL IMPACT STATEMENT": "trials before deployment.",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "Psychological sciences and social sciences, vol. 56, no. 5, pp. P279–84,"
        },
        {
          "ETHICAL IMPACT STATEMENT": "In addition, this study utilized the SpeakingFaces [38] dataset",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "sep 2001."
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "E. J. Bartley and R. B. Fillingim, “Sex differences in pain: a brief review\n[10]"
        },
        {
          "ETHICAL IMPACT STATEMENT": "for\nthe\nimage-to-image\ntranslation\nprocess. The\ndata was",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "of clinical and experimental findings,” British journal of anaesthesia,"
        },
        {
          "ETHICAL IMPACT STATEMENT": "collected according to the ethical guidelines of\nthe Declaration",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "vol. 111, no. 1, pp. 52–58,\njul 2013."
        },
        {
          "ETHICAL IMPACT STATEMENT": "of Helsinki,\nand with\nthe\napproval\nfrom the\nInstitutional",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "[11]\nS. Gkikas., C. Chatzaki., E.\nPavlidou.,\nF. Verigou., K. Kalkanis.,"
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "and M. Tsiknakis.,\n“Automatic\npain\nintensity\nestimation\nbased\non"
        },
        {
          "ETHICAL IMPACT STATEMENT": "Research Ethics Committee\nat Nazarbayev University. All",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "electrocardiogram and demographic factors.”\nSciTePress, 2022, pp."
        },
        {
          "ETHICAL IMPACT STATEMENT": "participants were volunteers who were fully informed about\nthe",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "155–162."
        },
        {
          "ETHICAL IMPACT STATEMENT": "data collection procedures and the intended use of\nidentifiable",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "[12]\nS. Gkikas, C. Chatzaki, and M. Tsiknakis, “Multi-task neural networks"
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "for pain intensity estimation using electrocardiogram and demographic"
        },
        {
          "ETHICAL IMPACT STATEMENT": "images, which will be distributed as part of a dataset. Each",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "factors,” in Information and Communication Technologies for Ageing"
        },
        {
          "ETHICAL IMPACT STATEMENT": "participant\nprovided\ntheir\npermission\nby\nsigning\ninformed",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "Well and e-Health.\nSpringer Nature Switzerland, 2023, pp. 324–337."
        },
        {
          "ETHICAL IMPACT STATEMENT": "consent\nforms.",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "[13] M. M. Al Qudah, A. S. Mohamed, and S. L. Lutfi, “Affective state"
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "recognition using thermal-based imaging: A survey.” Computer Systems"
        },
        {
          "ETHICAL IMPACT STATEMENT": "Furthermore, several datasets were utilized to pretrain the",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "Science & Engineering, vol. 37, no. 1, 2021."
        },
        {
          "ETHICAL IMPACT STATEMENT": "proposed pain assessment framework. The DigiFace-1M [39] is",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "[14]\nS.\nIoannou, V. Gallese, and A. Merla, “Thermal\ninfrared imaging in"
        },
        {
          "ETHICAL IMPACT STATEMENT": "a synthetic dataset where 511 initial\nface scans were obtained",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "psychophysiology: Potentialities and limits,” Psychophysiology, vol. 51,"
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "no. 10, pp. 951–963, 2014."
        },
        {
          "ETHICAL IMPACT STATEMENT": "with consent and employed to build a parametric face geometry",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "[15]\nS. Jarlier, D. Grandjean, S. Delplanque, K. N’Diaye,\nI. Cayeux, M.\nI."
        },
        {
          "ETHICAL IMPACT STATEMENT": "and texture library model. All\nthe identities and samples were",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "Velazco, D.\nSander,\nP. Vuilleumier,\nand K. R.\nScherer,\n“Thermal"
        },
        {
          "ETHICAL IMPACT STATEMENT": "generated from these source data. The AffectNet\n[40] dataset",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "analysis of\nfacial muscles contractions,” IEEE Transactions on Affective"
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "Computing, vol. 2, no. 1, pp. 2–9, 2011."
        },
        {
          "ETHICAL IMPACT STATEMENT": "is compiled using search engine queries. The original paper",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "[16] A. Merla, L. Di Donato, P. Rossini, and G. Romani, “Emotion detection"
        },
        {
          "ETHICAL IMPACT STATEMENT": "does not explicitly detail ethical compliance measures such as",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "through functional infrared imaging: preliminary results,” Biomedizinische"
        },
        {
          "ETHICAL IMPACT STATEMENT": "adherence to the Declaration of Helsinki or\ninformed consent",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "Technick, vol. 48, no. 2, pp. 284–286, 2004."
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "[17] Y. Mohamed, A. G¨uneysu, S. Lemaignan, and I. Leite, “Multi-modal"
        },
        {
          "ETHICAL IMPACT STATEMENT": "procedures. The original paper of Compound FEE-DB [42]",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "affect detection using thermal and optical\nimaging in a gamified robotic"
        },
        {
          "ETHICAL IMPACT STATEMENT": "does not mention ethical compliance measures, but only that",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "exercise,” International Journal of Social Robotics, Oct 2023."
        },
        {
          "ETHICAL IMPACT STATEMENT": "the subjects were recruited from the Ohio State University",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "¨\n[18] V. K. Erel and H. S.\nOzkan, “Thermal camera as a pain monitor,” Journal"
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "of Pain Research, vol. 10, pp. 2827–2832, 2017."
        },
        {
          "ETHICAL IMPACT STATEMENT": "area and received a monetary reward for participating. The",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "[19] M. A. Haque, R. B. Bautista, F. Noroozi, K. Kulkarni, C. B. Laursen,"
        },
        {
          "ETHICAL IMPACT STATEMENT": "RAF-DB [41] dataset was compiled using the Flickr\nimage",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "R.\nIrani, M. Bellantonio, S. Escalera, G. Anbarjafari, K. Nasrollahi,"
        },
        {
          "ETHICAL IMPACT STATEMENT": "hosting service. Although Flickr hosts both public and privately",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "O. K. Andersen, E. G. Spaich, and T. B. Moeslund, “Deep multimodal"
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "pain recognition: A database and comparison of spatio-temporal visual"
        },
        {
          "ETHICAL IMPACT STATEMENT": "shared images,\nthe authors do not explicitly mention the type",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "modalities,” in 2018 13th IEEE International Conference on Automatic"
        },
        {
          "ETHICAL IMPACT STATEMENT": "of\nthe downloaded images.",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "Face & Gesture Recognition (FG 2018), 2018, pp. 250–257."
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "[20]\nP. Werner, A. Al-Hamadi, K. Limbrecht-Ecklundt, S. Walter, S. Gruss, and"
        },
        {
          "ETHICAL IMPACT STATEMENT": "ACKNOWLEDGEMENT",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "H. C. Traue, “Automatic pain assessment with facial activity descriptors,”"
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "IEEE Transactions on Affective Computing, vol. 8, no. 3, pp. 286–299,"
        },
        {
          "ETHICAL IMPACT STATEMENT": "Research supported by the ODIN project\nthat has received",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "2016."
        },
        {
          "ETHICAL IMPACT STATEMENT": "funding from the European Union’s Horizon 2020 research and",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "[21]\nP. Werner, A. Al-Hamadi, and S. Walter, “Analysis of facial expressiveness"
        },
        {
          "ETHICAL IMPACT STATEMENT": "innovation program under grant agreement No 101017331.",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "during experimentally induced heat pain,” in 2017 Seventh International"
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "Conference on Affective Computing and Intelligent Interaction Workshops"
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "and Demos (ACIIW), 2017, pp. 176–180."
        },
        {
          "ETHICAL IMPACT STATEMENT": "REFERENCES",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "[22] R. Zhi and M. Wan, “Dynamic facial expression feature learning based"
        },
        {
          "ETHICAL IMPACT STATEMENT": "S. N. Raja, D. B. Carr, M. Cohen, N. B. Finnerup, H. Flor, S. Gibson, F. J.\n[1]",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "International\non sparse rnn,” in Proceedings of 2019 IEEE 8th Joint"
        },
        {
          "ETHICAL IMPACT STATEMENT": "Keefe, J. S. Mogil, M. Ringkamp, K. A. Sluka, X.-J. Song, B. Stevens,",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "Information Technology and Artificial\nIntelligence Conference,\nITAIC"
        },
        {
          "ETHICAL IMPACT STATEMENT": "M. D. Sullivan, P. R. Tutelman, T. Ushida, and K. Vader, “The revised",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "2019.\nInstitute of Electrical and Electronics Engineers Inc., may 2019,"
        },
        {
          "ETHICAL IMPACT STATEMENT": "international association for the study of pain definition of pain: concepts,",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "pp. 1373–1377."
        },
        {
          "ETHICAL IMPACT STATEMENT": "challenges, and compromises,” Pain, vol. 161, no. 9, p. 1976—1982,",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "[23] M. Tavakolian and A. Hadid, “A spatiotemporal convolutional neural"
        },
        {
          "ETHICAL IMPACT STATEMENT": "September 2020.",
          "[2] A. C. d. C. Williams and K. D. Craig, “Updating the definition of pain.”": "network for automatic pain intensity estimation from facial dynamics,”"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "funding from the European Union’s Horizon 2020 research and": "innovation program under grant agreement No 101017331.",
          "[21]\nP. Werner, A. Al-Hamadi, and S. Walter, “Analysis of facial expressiveness": "during experimentally induced heat pain,” in 2017 Seventh International"
        },
        {
          "funding from the European Union’s Horizon 2020 research and": "",
          "[21]\nP. Werner, A. Al-Hamadi, and S. Walter, “Analysis of facial expressiveness": "Conference on Affective Computing and Intelligent Interaction Workshops"
        },
        {
          "funding from the European Union’s Horizon 2020 research and": "",
          "[21]\nP. Werner, A. Al-Hamadi, and S. Walter, “Analysis of facial expressiveness": "and Demos (ACIIW), 2017, pp. 176–180."
        },
        {
          "funding from the European Union’s Horizon 2020 research and": "REFERENCES",
          "[21]\nP. Werner, A. Al-Hamadi, and S. Walter, “Analysis of facial expressiveness": ""
        },
        {
          "funding from the European Union’s Horizon 2020 research and": "",
          "[21]\nP. Werner, A. Al-Hamadi, and S. Walter, “Analysis of facial expressiveness": "[22] R. Zhi and M. Wan, “Dynamic facial expression feature learning based"
        },
        {
          "funding from the European Union’s Horizon 2020 research and": "S. N. Raja, D. B. Carr, M. Cohen, N. B. Finnerup, H. Flor, S. Gibson, F. J.\n[1]",
          "[21]\nP. Werner, A. Al-Hamadi, and S. Walter, “Analysis of facial expressiveness": "International\non sparse rnn,” in Proceedings of 2019 IEEE 8th Joint"
        },
        {
          "funding from the European Union’s Horizon 2020 research and": "Keefe, J. S. Mogil, M. Ringkamp, K. A. Sluka, X.-J. Song, B. Stevens,",
          "[21]\nP. Werner, A. Al-Hamadi, and S. Walter, “Analysis of facial expressiveness": "Information Technology and Artificial\nIntelligence Conference,\nITAIC"
        },
        {
          "funding from the European Union’s Horizon 2020 research and": "M. D. Sullivan, P. R. Tutelman, T. Ushida, and K. Vader, “The revised",
          "[21]\nP. Werner, A. Al-Hamadi, and S. Walter, “Analysis of facial expressiveness": "2019.\nInstitute of Electrical and Electronics Engineers Inc., may 2019,"
        },
        {
          "funding from the European Union’s Horizon 2020 research and": "international association for the study of pain definition of pain: concepts,",
          "[21]\nP. Werner, A. Al-Hamadi, and S. Walter, “Analysis of facial expressiveness": "pp. 1373–1377."
        },
        {
          "funding from the European Union’s Horizon 2020 research and": "challenges, and compromises,” Pain, vol. 161, no. 9, p. 1976—1982,",
          "[21]\nP. Werner, A. Al-Hamadi, and S. Walter, “Analysis of facial expressiveness": "[23] M. Tavakolian and A. Hadid, “A spatiotemporal convolutional neural"
        },
        {
          "funding from the European Union’s Horizon 2020 research and": "September 2020.",
          "[21]\nP. Werner, A. Al-Hamadi, and S. Walter, “Analysis of facial expressiveness": "network for automatic pain intensity estimation from facial dynamics,”"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Conference on Computer Vision and Pattern Recognition (CVPR), 2022,": ""
        },
        {
          "Conference on Computer Vision and Pattern Recognition (CVPR), 2022,": "pp. 10 925–10 934."
        },
        {
          "Conference on Computer Vision and Pattern Recognition (CVPR), 2022,": "[37] K. Zhang, Z. Zhang, Z. Li,\nand Y. Qiao,\n“Joint"
        },
        {
          "Conference on Computer Vision and Pattern Recognition (CVPR), 2022,": "alignment using multitask cascaded convolutional networks,” IEEE signal"
        },
        {
          "Conference on Computer Vision and Pattern Recognition (CVPR), 2022,": ""
        },
        {
          "Conference on Computer Vision and Pattern Recognition (CVPR), 2022,": ""
        },
        {
          "Conference on Computer Vision and Pattern Recognition (CVPR), 2022,": "processing letters, vol. 23, no. 10, pp. 1499–1503, 2016."
        },
        {
          "Conference on Computer Vision and Pattern Recognition (CVPR), 2022,": ""
        },
        {
          "Conference on Computer Vision and Pattern Recognition (CVPR), 2022,": ""
        },
        {
          "Conference on Computer Vision and Pattern Recognition (CVPR), 2022,": "[38] M. Abdrakhmanova, A. Kuzdeuov, S. Jarju, Y. Khassanov, M. Lewis, and"
        },
        {
          "Conference on Computer Vision and Pattern Recognition (CVPR), 2022,": "H. A. Varol, “Speakingfaces: A large-scale multimodal dataset of voice"
        },
        {
          "Conference on Computer Vision and Pattern Recognition (CVPR), 2022,": "commands with visual and thermal video streams,” Sensors, vol. 21,"
        },
        {
          "Conference on Computer Vision and Pattern Recognition (CVPR), 2022,": ""
        },
        {
          "Conference on Computer Vision and Pattern Recognition (CVPR), 2022,": "no. 10, 2021."
        },
        {
          "Conference on Computer Vision and Pattern Recognition (CVPR), 2022,": ""
        },
        {
          "Conference on Computer Vision and Pattern Recognition (CVPR), 2022,": "[39] G. Bae, M. de La Gorce, T. Baltruˇsaitis, C. Hewitt, D. Chen, J. Valentin,"
        },
        {
          "Conference on Computer Vision and Pattern Recognition (CVPR), 2022,": ""
        },
        {
          "Conference on Computer Vision and Pattern Recognition (CVPR), 2022,": "R. Cipolla, and J. Shen, “Digiface-1m: 1 million digital"
        },
        {
          "Conference on Computer Vision and Pattern Recognition (CVPR), 2022,": ""
        },
        {
          "Conference on Computer Vision and Pattern Recognition (CVPR), 2022,": "face recognition,” in Proceedings of"
        },
        {
          "Conference on Computer Vision and Pattern Recognition (CVPR), 2022,": "Applications of Computer Vision (WACV), January 2023, pp. 3526–3535."
        },
        {
          "Conference on Computer Vision and Pattern Recognition (CVPR), 2022,": ""
        },
        {
          "Conference on Computer Vision and Pattern Recognition (CVPR), 2022,": "[40] A. Mollahosseini, B. Hasani, and M. H. Mahoor, “Affectnet: A database"
        },
        {
          "Conference on Computer Vision and Pattern Recognition (CVPR), 2022,": "for\nfacial expression, valence, and arousal computing in the wild,” IEEE"
        },
        {
          "Conference on Computer Vision and Pattern Recognition (CVPR), 2022,": ""
        },
        {
          "Conference on Computer Vision and Pattern Recognition (CVPR), 2022,": ""
        },
        {
          "Conference on Computer Vision and Pattern Recognition (CVPR), 2022,": "Transactions on Affective Computing, vol. 10, no. 1, pp. 18–31, 2019."
        },
        {
          "Conference on Computer Vision and Pattern Recognition (CVPR), 2022,": ""
        },
        {
          "Conference on Computer Vision and Pattern Recognition (CVPR), 2022,": "[41]\nS. Li, W. Deng, and J. Du, “Reliable crowdsourcing and deep locality-"
        },
        {
          "Conference on Computer Vision and Pattern Recognition (CVPR), 2022,": ""
        },
        {
          "Conference on Computer Vision and Pattern Recognition (CVPR), 2022,": "preserving learning for expression recognition in the wild,” in Proceedings"
        },
        {
          "Conference on Computer Vision and Pattern Recognition (CVPR), 2022,": "of\nthe IEEE Conference on Computer Vision and Pattern Recognition"
        },
        {
          "Conference on Computer Vision and Pattern Recognition (CVPR), 2022,": "(CVPR), July 2017."
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "1425, oct 2019.",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": "P. Werner, A. Al-Hamadi, A. O. Andrade, and G. M. D. Silva, “The"
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "[24]\nP. Thiam, H. A. Kestler, and F. Schwenker, “Two-stream attention network",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": "biovid heat pain database: Data for\nthe advancement and systematic"
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "for pain recognition from video sequences,” Sensors (Switzerland), vol. 20,",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": "validation of an automated pain recognition,” 2013, pp. 128–131."
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "no. 3, p. 839,\nfeb 2020.",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": ""
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "[25] M. Tavakolian, M. Bordallo Lopez, and L. Liu, “Self-supervised pain",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": "APPENDIX"
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "intensity estimation from facial videos via\nstatistical\nspatiotemporal",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": ""
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": "Supplementary Metrics"
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "distillation,” Pattern Recognition Letters, vol. 140, pp. 26–33, 2020.",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": ""
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "[26] D. Huang, Z. Xia, L. Li, K. Wang,\nand X. Feng,\n“Pain-awareness",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": ""
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "multistream convolutional neural network for pain estimation,” Journal",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": ""
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": "TABLE IX: Classification results utilizing the RGB video"
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "of Electronic Imaging, vol. 28, no. 04, p. 1, 2019.",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": ""
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": "modality,\nreported on recall and F1 score."
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "[27] D. Huang, Z. Xia, J. Mwesigye, and X. Feng, “Pain-attentive network: a",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": ""
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "deep spatio-temporal attention model\nfor pain estimation,” Multimedia",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": ""
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "Tools and Applications, vol. 79, no. 37-38, pp. 28 329–28 354, 2020.",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": ""
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": "Augmentations\nTask"
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "[28] D. Huang, X. Feng, H. Zhang, Z. Yu,\nJ. Peng, G. Zhao, and Z. Xia,",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": ""
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": "Epochs\nMetric"
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "“Spatio-temporal pain estimation network with measuring pseudo heart",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": ""
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": "NP vs P4"
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "rate gain,” IEEE Transactions on Multimedia, vol. 24, pp. 3300–3313,",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": ""
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "2022.",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": "Recall\n71.29\n29.61"
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": "✓\n200\n30-50\n0.9"
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "[29]\nS. Gkikas\nand M. Tsiknakis,\n“A full\ntransformer-based\nframework",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": ""
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": "F1\n68.53\n27.22"
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "2023\n45th Annual\nfor\nautomatic\npain\nestimation\nusing\nvideos,”\nin",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": ""
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": "Recall\n71.93\n24.43"
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "International Conference of the IEEE Engineering in Medicine & Biology",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": "✓\n200\n30-50\n0.9"
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": "F1\n69.61\n23.78"
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "Society (EMBC), 2023, pp. 1–6.",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": ""
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "[30]\nS. Gkikas, N. S. Tachos, S. Andreadis, V. C. Pezoulas, D. Zaridis,",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": "Recall\n71.34\n30.64"
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": "✓\n300\n30-50\n0.9"
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "G. Gkois, A. Matonaki, T. G. Stavropoulos, and D.\nI. Fotiadis, “Mul-",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": "F1\n69.65\n26.12"
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "timodal automatic assessment of acute pain through facial videos and",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": ""
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "heart\nrate signals utilizing transformer-based architectures,” Frontiers in",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": ""
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "Pain Research, vol. 5, 2024.",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": ""
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "[31] M. Mirza and S. Osindero, “Conditional generative adversarial nets,”",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": ""
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "arXiv preprint arXiv:1411.1784, 2014.",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": ""
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": "TABLE X: Classification results utilizing the synthetic thermal"
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "[32]\nI. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville,",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": ""
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "Informa-\n“Improved training of wasserstein gans,” in Advances in Neural",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": "video modality,\nreported on recall and F1 score."
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "tion Processing Systems, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach,",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": ""
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "R. Fergus, S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": ""
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": "Augmentations\nTask"
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "Associates,\nInc., 2017.",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": ""
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": "Epochs\nMetric"
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "[33]\nT.-C. Wang, M.-Y. Liu, J.-Y. Zhu, A. Tao, J. Kautz, and B. Catanzaro,",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": ""
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": "Basic Masking\nP(Aug)\nMC\nNP vs P4"
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "“High-resolution image synthesis and semantic manipulation with condi-",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": ""
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "the IEEE Conference on Computer Vision\ntional gans,” in Proceedings of",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": ""
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": "Recall\n72.04\n28.80"
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "and Pattern Recognition (CVPR), June 2018.",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": "✓\n200\n30-50\n0.9"
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": "F1\n69.16\n26.45"
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, “Image-to-image translation\n[34]",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": ""
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "the\nIEEE\nwith conditional\nadversarial networks,”\nin Proceedings of",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": "Recall\n72.18\n30.89"
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": "✓\n200\n30-50\n0.9"
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "conference on computer vision and pattern recognition, 2017, pp. 1125–",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": "F1\n69.44\n26.45"
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "1134.",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": ""
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": "Recall\n72.52\n24.96"
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": "✓\n300\n30-50\n0.9"
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "[35]\nI. O.\nTolstikhin, N. Houlsby, A. Kolesnikov,\nL. Beyer, X.\nZhai,",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": ""
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": "F1\n70.01\n23.43"
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "T. Unterthiner, J. Yung, A. Steiner, D. Keysers, J. Uszkoreit, M. Lucic,",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": ""
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "and A. Dosovitskiy, “Mlp-mixer: An all-mlp architecture for vision,”",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": ""
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "in Neural\nin Advances\nInformation Processing Systems, M. Ranzato,",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": ""
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, Eds., vol. 34.",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": ""
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "Curran Associates,\nInc., 2021, pp. 24 261–24 272.",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": ""
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "[36] Y. Tang, K. Han,\nJ. Guo, C. Xu, Y. Li, C. Xu,\nand Y. Wang,\n“An",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": "TABLE XI: Classification results utilizing the fusion of RGB"
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "image patch is a wave: Phase-aware vision mlp,” in 2022 IEEE/CVF",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": "& synthetic thermal video modality,\nreported on recall and F1"
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "Conference on Computer Vision and Pattern Recognition (CVPR), 2022,",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": ""
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": "score."
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "pp. 10 925–10 934.",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": ""
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "[37] K. Zhang, Z. Zhang, Z. Li,\nand Y. Qiao,\n“Joint\nface detection and",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": ""
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "alignment using multitask cascaded convolutional networks,” IEEE signal",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": ""
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": "Augmentations\nTask"
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": "Fusion"
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "processing letters, vol. 23, no. 10, pp. 1499–1503, 2016.",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": ""
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": "Metric\nEpochs"
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": "weights"
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "[38] M. Abdrakhmanova, A. Kuzdeuov, S. Jarju, Y. Khassanov, M. Lewis, and",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": "Basic Masking P(Aug)\nMC\nNP vs P4"
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "H. A. Varol, “Speakingfaces: A large-scale multimodal dataset of voice",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": ""
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "commands with visual and thermal video streams,” Sensors, vol. 21,",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": "21.68\nRecall\n67.05"
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": "✓\n100\n–\n30-50\n0.9"
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "no. 10, 2021.",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": ""
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": "F1\n62.96\n18.29"
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "[39] G. Bae, M. de La Gorce, T. Baltruˇsaitis, C. Hewitt, D. Chen, J. Valentin,",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": ""
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": "21.69\nRecall\n68.72"
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "R. Cipolla, and J. Shen, “Digiface-1m: 1 million digital\nface images for",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": "✓\n100\nW2\n30-50\n0.9"
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": "F1\n62.98\n19.35"
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "the IEEE/CVF Winter Conference on\nface recognition,” in Proceedings of",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": ""
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "Applications of Computer Vision (WACV), January 2023, pp. 3526–3535.",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": "Recall\n66.12\n23.12"
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": "✓\n100\nW3\n30-50\n0.9"
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "[40] A. Mollahosseini, B. Hasani, and M. H. Mahoor, “Affectnet: A database",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": "F1\n59.72\n19.67"
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "for\nfacial expression, valence, and arousal computing in the wild,” IEEE",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": ""
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": "Recall\n71.40\n26.39"
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": "✓\n300\nW2\n30-50\n0.9"
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "Transactions on Affective Computing, vol. 10, no. 1, pp. 18–31, 2019.",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": ""
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": "F1\n68.82\n26.18"
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "[41]\nS. Li, W. Deng, and J. Du, “Reliable crowdsourcing and deep locality-",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": ""
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": "Recall\n73.20\n29.69"
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "preserving learning for expression recognition in the wild,” in Proceedings",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": "✓\n500\nW2\n10-20\n0.7"
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "of\nthe IEEE Conference on Computer Vision and Pattern Recognition",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": "F1\n70.30\n27.84"
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "(CVPR), July 2017.",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": ""
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "[42]\nS. Du, Y. Tao, and A. M. Martinez, “Compound facial expressions of",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": ""
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "emotion,” Proceedings of\nthe National Academy of Sciences, vol. 111,",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": ""
        },
        {
          "International Journal of Computer Vision, vol. 127, no. 10, pp. 1413–": "no. 15, pp. E1454–E1462, 2014.",
          "S. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,\n[43]": ""
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "The revised international association for the study of pain definition of pain: concepts, challenges, and compromises",
      "authors": [
        "S Raja",
        "D Carr",
        "M Cohen",
        "N Finnerup",
        "H Flor",
        "S Gibson",
        "F Keefe",
        "J Mogil",
        "M Ringkamp",
        "K Sluka",
        "X.-J Song",
        "B Stevens",
        "M Sullivan",
        "P Tutelman",
        "T Ushida",
        "K Vader"
      ],
      "year": "2020",
      "venue": "Pain"
    },
    {
      "citation_id": "2",
      "title": "Updating the definition of pain",
      "authors": [
        "A Williams",
        "K Craig"
      ],
      "year": "2016",
      "venue": "Pain"
    },
    {
      "citation_id": "3",
      "title": "Neuroanatomy and neuropsychology of pain",
      "authors": [
        "T Rs"
      ],
      "year": "2017",
      "venue": "Cureus"
    },
    {
      "citation_id": "4",
      "title": "Pathogenesis of pain",
      "authors": [
        "P Dinakar",
        "A Stillman"
      ],
      "year": "2016",
      "venue": "Seminars in Pediatric Neurology"
    },
    {
      "citation_id": "5",
      "title": "Automatic assessment of pain based on deep learning methods: A systematic review",
      "authors": [
        "S Gkikas",
        "M Tsiknakis"
      ],
      "year": "2023",
      "venue": "Computer Methods and Programs in Biomedicine"
    },
    {
      "citation_id": "6",
      "title": "The fifth vital sign: pain",
      "authors": [
        "L Joel"
      ],
      "year": "1999",
      "venue": "AJN The American Journal of Nursing"
    },
    {
      "citation_id": "7",
      "title": "A systematic review of neurophysiological sensing for the assessment of acute pain",
      "authors": [
        "R Rojas",
        "N Brown",
        "G Waddington",
        "R Goecke"
      ],
      "year": "2023",
      "venue": "NPJ Digital Medicine"
    },
    {
      "citation_id": "8",
      "title": "Pain assessment tool with electrodermal activity for postoperative patients: method validation study",
      "authors": [
        "S Aqajari",
        "R Cao",
        "E Kasaeyan",
        "M.-D Naeini",
        "K Calderon",
        "N Zheng",
        "P Dutt",
        "S Liljeberg",
        "A Salanterä",
        "A Nelson",
        "Rahmani"
      ],
      "year": "2021",
      "venue": "JMIR mHealth and uHealth"
    },
    {
      "citation_id": "9",
      "title": "Development of a pain attitudes questionnaire to assess stoicism and cautiousness for possible age differences",
      "authors": [
        "H Yong",
        "S Gibson",
        "D Horne",
        "R Helme"
      ],
      "year": "2001",
      "venue": "The journals of gerontology. Series B, Psychological sciences and social sciences"
    },
    {
      "citation_id": "10",
      "title": "Sex differences in pain: a brief review of clinical and experimental findings",
      "authors": [
        "E Bartley",
        "R Fillingim"
      ],
      "year": "2013",
      "venue": "British journal of anaesthesia"
    },
    {
      "citation_id": "11",
      "title": "Automatic pain intensity estimation based on electrocardiogram and demographic factors",
      "authors": [
        "S Gkikas",
        "C Chatzaki",
        "E Pavlidou",
        "F Verigou",
        "K Kalkanis",
        "M Tsiknakis"
      ],
      "year": "2022",
      "venue": "Automatic pain intensity estimation based on electrocardiogram and demographic factors"
    },
    {
      "citation_id": "12",
      "title": "Multi-task neural networks for pain intensity estimation using electrocardiogram and demographic factors,\" in Information and Communication Technologies for Ageing Well and e-Health",
      "authors": [
        "S Gkikas",
        "C Chatzaki",
        "M Tsiknakis"
      ],
      "year": "2023",
      "venue": "Multi-task neural networks for pain intensity estimation using electrocardiogram and demographic factors,\" in Information and Communication Technologies for Ageing Well and e-Health"
    },
    {
      "citation_id": "13",
      "title": "Affective state recognition using thermal-based imaging: A survey",
      "authors": [
        "M Al Qudah",
        "A Mohamed",
        "S Lutfi"
      ],
      "year": "2021",
      "venue": "Computer Systems Science & Engineering"
    },
    {
      "citation_id": "14",
      "title": "Thermal infrared imaging in psychophysiology: Potentialities and limits",
      "authors": [
        "S Ioannou",
        "V Gallese",
        "A Merla"
      ],
      "year": "2014",
      "venue": "Psychophysiology"
    },
    {
      "citation_id": "15",
      "title": "Thermal analysis of facial muscles contractions",
      "authors": [
        "S Jarlier",
        "D Grandjean",
        "S Delplanque",
        "K N'diaye",
        "I Cayeux",
        "M Velazco",
        "D Sander",
        "P Vuilleumier",
        "K Scherer"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "16",
      "title": "Emotion detection through functional infrared imaging: preliminary results",
      "authors": [
        "A Merla",
        "L Donato",
        "P Rossini",
        "G Romani"
      ],
      "year": "2004",
      "venue": "Biomedizinische Technick"
    },
    {
      "citation_id": "17",
      "title": "Multi-modal affect detection using thermal and optical imaging in a gamified robotic exercise",
      "authors": [
        "Y Mohamed",
        "A Güneysu",
        "S Lemaignan",
        "I Leite"
      ],
      "year": "2023",
      "venue": "International Journal of Social Robotics"
    },
    {
      "citation_id": "18",
      "title": "Thermal camera as a pain monitor",
      "authors": [
        "V Erel",
        "H Özkan"
      ],
      "year": "2017",
      "venue": "Journal of Pain Research"
    },
    {
      "citation_id": "19",
      "title": "Deep multimodal pain recognition: A database and comparison of spatio-temporal visual modalities",
      "authors": [
        "M Haque",
        "R Bautista",
        "F Noroozi",
        "K Kulkarni",
        "C Laursen",
        "R Irani",
        "M Bellantonio",
        "S Escalera",
        "G Anbarjafari",
        "K Nasrollahi",
        "O Andersen",
        "E Spaich",
        "T Moeslund"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)"
    },
    {
      "citation_id": "20",
      "title": "Automatic pain assessment with facial activity descriptors",
      "authors": [
        "P Werner",
        "A Al-Hamadi",
        "K Limbrecht-Ecklundt",
        "S Walter",
        "S Gruss",
        "H Traue"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "21",
      "title": "Analysis of facial expressiveness during experimentally induced heat pain",
      "authors": [
        "P Werner",
        "A Al-Hamadi",
        "S Walter"
      ],
      "year": "2017",
      "venue": "2017 Seventh International Conference on Affective Computing and Intelligent Interaction Workshops and Demos"
    },
    {
      "citation_id": "22",
      "title": "Dynamic facial expression feature learning based on sparse rnn",
      "authors": [
        "R Zhi",
        "M Wan"
      ],
      "year": "2019",
      "venue": "Proceedings of 2019 IEEE 8th Joint International Information Technology and Artificial Intelligence Conference, ITAIC 2019"
    },
    {
      "citation_id": "23",
      "title": "A spatiotemporal convolutional neural network for automatic pain intensity estimation from facial dynamics",
      "authors": [
        "M Tavakolian",
        "A Hadid"
      ],
      "year": "2019",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "24",
      "title": "Two-stream attention network for pain recognition from video sequences",
      "authors": [
        "P Thiam",
        "H Kestler",
        "F Schwenker"
      ],
      "year": "2020",
      "venue": "Sensors (Switzerland)"
    },
    {
      "citation_id": "25",
      "title": "Self-supervised pain intensity estimation from facial videos via statistical spatiotemporal distillation",
      "authors": [
        "M Tavakolian",
        "M Lopez",
        "L Liu"
      ],
      "year": "2020",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "26",
      "title": "Pain-awareness multistream convolutional neural network for pain estimation",
      "authors": [
        "D Huang",
        "Z Xia",
        "L Li",
        "K Wang",
        "X Feng"
      ],
      "year": "2019",
      "venue": "Journal of Electronic Imaging"
    },
    {
      "citation_id": "27",
      "title": "Pain-attentive network: a deep spatio-temporal attention model for pain estimation",
      "authors": [
        "D Huang",
        "Z Xia",
        "J Mwesigye",
        "X Feng"
      ],
      "year": "2020",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "28",
      "title": "Spatio-temporal pain estimation network with measuring pseudo heart rate gain",
      "authors": [
        "D Huang",
        "X Feng",
        "H Zhang",
        "Z Yu",
        "J Peng",
        "G Zhao",
        "Z Xia"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "29",
      "title": "A full transformer-based framework for automatic pain estimation using videos",
      "authors": [
        "S Gkikas",
        "M Tsiknakis"
      ],
      "year": "2023",
      "venue": "2023 45th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)"
    },
    {
      "citation_id": "30",
      "title": "Multimodal automatic assessment of acute pain through facial videos and heart rate signals utilizing transformer-based architectures",
      "authors": [
        "S Gkikas",
        "N Tachos",
        "S Andreadis",
        "V Pezoulas",
        "D Zaridis",
        "G Gkois",
        "A Matonaki",
        "T Stavropoulos",
        "D Fotiadis"
      ],
      "year": "2024",
      "venue": "Frontiers in Pain Research"
    },
    {
      "citation_id": "31",
      "title": "Conditional generative adversarial nets",
      "authors": [
        "M Mirza",
        "S Osindero"
      ],
      "year": "2014",
      "venue": "Conditional generative adversarial nets",
      "arxiv": "arXiv:1411.1784"
    },
    {
      "citation_id": "32",
      "title": "Improved training of wasserstein gans",
      "authors": [
        "I Gulrajani",
        "F Ahmed",
        "M Arjovsky",
        "V Dumoulin",
        "A Courville"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems, I. Guyon, U. V. Luxburg"
    },
    {
      "citation_id": "33",
      "title": "High-resolution image synthesis and semantic manipulation with conditional gans",
      "authors": [
        "T.-C Wang",
        "M.-Y Liu",
        "J.-Y Zhu",
        "A Tao",
        "J Kautz",
        "B Catanzaro"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "34",
      "title": "Image-to-image translation with conditional adversarial networks",
      "authors": [
        "P Isola",
        "J.-Y Zhu",
        "T Zhou",
        "A Efros"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "35",
      "title": "Mlp-mixer: An all-mlp architecture for vision",
      "authors": [
        "I Tolstikhin",
        "N Houlsby",
        "A Kolesnikov",
        "L Beyer",
        "X Zhai",
        "T Unterthiner",
        "J Yung",
        "A Steiner",
        "D Keysers",
        "J Uszkoreit",
        "M Lucic",
        "A Dosovitskiy"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "36",
      "title": "An image patch is a wave: Phase-aware vision mlp",
      "authors": [
        "Y Tang",
        "K Han",
        "J Guo",
        "C Xu",
        "Y Li",
        "C Xu",
        "Y Wang"
      ],
      "year": "2022",
      "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "37",
      "title": "Joint face detection and alignment using multitask cascaded convolutional networks",
      "authors": [
        "K Zhang",
        "Z Zhang",
        "Z Li",
        "Y Qiao"
      ],
      "year": "2016",
      "venue": "IEEE signal processing letters"
    },
    {
      "citation_id": "38",
      "title": "Speakingfaces: A large-scale multimodal dataset of voice commands with visual and thermal video streams",
      "authors": [
        "M Abdrakhmanova",
        "A Kuzdeuov",
        "S Jarju",
        "Y Khassanov",
        "M Lewis",
        "H Varol"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "39",
      "title": "Digiface-1m: 1 million digital face images for face recognition",
      "authors": [
        "G Bae",
        "M De La Gorce",
        "T Baltrušaitis",
        "C Hewitt",
        "D Chen",
        "J Valentin",
        "R Cipolla",
        "J Shen"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)"
    },
    {
      "citation_id": "40",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "41",
      "title": "Reliable crowdsourcing and deep localitypreserving learning for expression recognition in the wild",
      "authors": [
        "S Li",
        "W Deng",
        "J Du"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "42",
      "title": "Compound facial expressions of emotion",
      "authors": [
        "S Du",
        "Y Tao",
        "A Martinez"
      ],
      "year": "2014",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "43",
      "title": "The biovid heat pain database: Data for the advancement and systematic validation of an automated pain recognition",
      "authors": [
        "S Walter",
        "S Gruss",
        "H Ehleiter",
        "J Tan",
        "H Traue",
        "S Crawcour",
        "P Werner",
        "A Al-Hamadi",
        "A Andrade",
        "G Silva"
      ],
      "year": "2013",
      "venue": "The biovid heat pain database: Data for the advancement and systematic validation of an automated pain recognition"
    }
  ]
}