{
  "paper_id": "2208.07589v2",
  "title": "Efficient Multimodal Transformer With Dual-Level Feature Restoration For Robust Multimodal Sentiment Analysis",
  "published": "2022-08-16T08:02:30Z",
  "authors": [
    "Licai Sun",
    "Zheng Lian",
    "Bin Liu",
    "Jianhua Tao"
  ],
  "keywords": [
    "Multimodal sentiment analysis",
    "unaligned and incomplete data",
    "efficient multimodal Transformer",
    "dual-level feature restoration",
    "robustness"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "With the proliferation of user-generated online videos, Multimodal Sentiment Analysis (MSA) has attracted increasing attention recently. Despite significant progress, there are still two major challenges on the way towards robust MSA: 1) inefficiency when modeling cross-modal interactions in unaligned multimodal data; and 2) vulnerability to random modality feature missing which typically occurs in realistic settings. In this paper, we propose a generic and unified framework to address them, named Efficient Multimodal Transformer with Dual-Level Feature Restoration (EMT-DLFR). Concretely, EMT employs utterance-level representations from each modality as the global multimodal context to interact with local unimodal features and mutually promote each other. It not only avoids the quadratic scaling cost of previous local-local cross-modal interaction methods but also leads to better performance. To improve model robustness in the incomplete modality setting, on the one hand, DLFR performs low-level feature reconstruction to implicitly encourage the model to learn semantic information from incomplete data. On the other hand, it innovatively regards complete and incomplete data as two different views of one sample and utilizes siamese representation learning to explicitly attract their high-level representations. Comprehensive experiments on three popular datasets demonstrate that our method achieves superior performance in both complete and incomplete modality settings.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "M ULTIMODAL Sentiment Analysis (MSA), which lever- ages multimodal signals to achieve an affective understanding of user-generated video, has become an active research area due to its wide applications in marketing management  [1] ,  [2] , social media analysis  [3] ,  [4] , and humancomputer interaction  [5] ,  [6] , to name a few. It mainly involves sequential data of three common modalities, i.e., audio (acoustic behaviors), vision (facial expressions), and text (spoken words). These different types of data provide us with abundant information to make a thorough understanding of human sentiment. Nevertheless, it remains challenging to efficiently fuse the heterogeneous sequential features in practical applications. The first issue is that multimodal sequences usually exhibit unaligned nature as different modalities typically have variable sampling rates. In addition, they often suffer from random modality feature missing (i.e., incomplete) due to many inevitable factors in real-world scenarios. For instance, the speech may be temporarily corrupted by background noise or sensor failure. The speaker's face could occasionally miss because of occlusion and motion. Some spoken words are probably unavailable owing to automatic speech recognition errors.\n\nThe earlier studies address the unaligned issue by manually performing forced word-level alignment before model training  [7] ,  [8] ,  [9] ,  [10] ,  [11] . However, the manual alignment process requires domain expert knowledge and is not always feasible in the real-world deployment of MSA models. Recently, Multimodal Transformer (MulT)  [12]  has been proposed to directly model cross-modal correlations in unaligned multimodal sequences. It utilizes directional pairwise cross-modal attention to attend to dense (i.e., local-local) interactions across distinct time steps between two involved modalities. Although MulT can address the unaligned issue, it is not efficient to conduct multimodal fusion in a pairwise manner. Therefore, Lv et al.  [13]  propose the Progressive Modality Reinforcement (PMR), which introduces a message hub to communicate with each modality. The message hub can send common messages to each modality and it can also collect information from them. In this way, PMR avoids the inefficient pairwise communication of two modalities in MulT. Unfortunately, both PMR and MulT suffer from quadratic computational complexity over the involved modalities, as they focus on modeling the local-local cross-modal dependencies across all modalities (see details in Section 3.3).\n\nMoreover, random modality feature missing which often occurs in realistic settings exacerbates the problem of fusing unaligned multimodal sequences. A simple method to tackle the incomplete issue is to perform zero, average, or nearest-neighbor imputation during model inference. However, there is a large domain gap between the ground truth and the imputed one. Thus, the model performance typically degrades severely using these naive imputations. Recently, several modality translation-based methods  [11] ,  [14] ,  [15]  have been proposed to learn robust joint multimodal representations that retain maximal information from all modalities. While appealing, these methods are developed to handle the entire loss of one or more modalities which is less likely to happen in practice. Besides, most of them are only applicable to aligned multimodal inputs. More recently, Yuan et al.  [16]  introduce the Transformerbased Feature Reconstruction Network (TFR-Net) to cope with random modality feature missing. It expects the model to implicitly learn semantic features from incomplete multimodal sequences by reconstructing the missing parts. Though achieving promising results, TFR-Net has the risk of learning trivial solutions as the model may find a shortcut instead of inferring semantics to solve the reconstruction task  [17] . Besides, it builds on MulT and thus also has a quadratic scaling cost with the involved modalities.\n\nTo efficiently fuse unaligned multimodal sequences in both complete and incomplete modality settings, we propose a generic and unified framework in this paper, named Efficient Multimodal Transformer with Dual-Level Feature Restoration (EMT-DLFR). In contrast to MulT and PMR, the Efficient Multimodal Transformer (EMT) explores less dense (i.e., global-local) cross-modal interactions, which is mainly motivated by the observation that a large amount of redundancy exists in unaligned multimodal sequences (especially for the audio and video modalities which have high sampling rates). Inspired by the bottleneck mechanism in Transformers  [18] ,  [19] ,  [20] , EMT utilizes utterance-level representations from each modality as the global multimodal context to interact with local unimodal features. On the one side, local unimodal features can be efficiently reinforced by the global multimodal context via cross-modal attention. In turn, the global multimodal context can update itself by extracting useful information from local unimodal features through symmetric cross-modal attention. By stacking multiple layers, the global multimodal context and local unimodal features can mutually promote each other and refine themselves progressively. Thanks to the introduction of the global multimodal context, EMT not only practically has linear computational complexity over the involved modalities but also leads to performance gains. Furthermore, we introduce hierarchical parameter sharing for EMT to increase parameter efficiency and ease model training.\n\nTo promote model robustness to random modality feature missing, we randomly mask the input feature sequences of complete data to mimic real-world scenarios and utilize the Dual-Level Feature Restoration (DLFR) built upon EMT to achieve robust representation learning from incomplete multimodal data. On the one hand, DLFR follows TFR-Net and tries to reconstruct the missing parts by exploiting available intra-and inter-modal clues using multiple stacked layers in EMT, i.e, the Low-Level Feature Restoration (LLFR), or more specifically, low-level feature reconstruction. In this way, LLFR implicitly encourages the model to learn semantic information from incomplete multimodal sequences. On the other hand, inspired by recent advances in self-supervised representation learning  [21] ,  [22] ,  [23] , we originally regard incomplete and complete sequences as two different views of one sample, and utilize siamese representation learning to explicitly attract highlevel representations of incomplete and complete views in the latent space, i.e., High-Level Feature Restoration (HLFR), or more specifically, high-level feature attraction. Compare with LLFR, HLFR is more direct and thus more effective. Nevertheless, they are complementary to each other. Therefore, the combination of LLFR and HLFR can be a unified framework for robust MSA.\n\nTo verify the effectiveness of the proposed method, we conduct comprehensive experiments on three widely used MSA benchmark datasets, including CMU-MOSI  [24] , CMU-MOSEI  [25] , and CH-SIMS  [26] . The results show that our proposed method outperforms previous state-ofthe-art methods in both incomplete and complete modality settings. To summarize, the main contributions of this paper are as follows:",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "•",
      "text": "We propose EMT, an Efficient Multimodal Transformer to achieve effective and efficient fusion of unaligned multimodal data. It not only avoids the quadratic scaling cost of previous dense cross-modal interaction methods but also achieves better performance than them.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "•",
      "text": "We propose DLFR, a Dual-Level Feature Restoration method to improve model robustness to random modality feature missing which typically occurs in real-world scenarios. It combines both implicit lowlevel feature reconstruction and explicit high-level feature attraction to realize robust representation learning from incomplete multimodal data.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "•",
      "text": "Extensive experiments on three popular MSA benchmark datasets demonstrate that EMT and EMT-DLFR achieve state-of-the-art performance in the complete and incomplete modality settings, respectively. 1",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Msa In The Complete Modality Setting",
      "text": "Most works in MSA assume a complete modality setting and they are centered around developing various methods to fuse heterogeneous features from different modalities. Generally, they conduct multimodal fusion at two different levels: 1) utterance level, and 2) element level. The utterance-level fusion methods mainly include simple concatenation  [27] ,  [28] ,  [29] ,  [30] , attention  [7] ,  [31] ,  [32] , and tensor-based fusion  [33] ,  [34] ,  [35] ,  [36] . Although these methods obtain promising results, they ignore the finegrained cross-modal interactions. Therefore, lots of methods have been proposed to perform element-level fusion on manually aligned multimodal sequences, including recurrent methods  [8] ,  [9] ,  [37] , attention-based methods  [10] ,  [38] , and multimodal-aware word embeddings  [39] ,  [40] . Unfortunately, the manual alignment process requires domain expert knowledge and is not always feasible in realworld applications. Motivated by the great success of Transformer  [41]  in various fields of deep learning, Transformerbased methods  [12] ,  [13] ,  [16] ,  [42] ,  [43] ,  [44] ,  [45] ,  [46]  have attracted increasing attention in recent years as they can directly perform multimodal fusion on unaligned multimodal sequences. For instance, MulT  [12]  utilizes directional pairwise cross-modal attention to attend to dense interactions between different modalities. PMR  [13]  further introduces a message hub to explore multi-way interactions in a single cross-modal attention module. TFR-Net  [16]  adds an extra intra-modal Transformer in parallel with the cross-modal Transformer in MulT to achieve simultaneous modeling of intra-modal and cross-modal interactions. Although achieving encouraging results, they all have quadratic computational complexity over the involved modalities. In contrast, due to the introduction of the global multimodal context, our proposed method enjoys the linear scaling cost in practice and even has better performance.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Msa In The Incomplete Modality Setting",
      "text": "Compared with numerous methods in the above setting, there are only a few studies focusing on improving model robustness in the incomplete setting, despite its critical role in enabling reliable deployment in the wild. Parthasarathy et al.  [47]  propose a strategy to cope with incomplete sequences by randomly ablating input features during training. Similarly, Hazarika et al.  [48]  utilize modalityperturbation (i.e., removing or masking modalities) training to reduce the sensitivity of the model to the missing language modality. Liang et al.  [49]  present a tensor rank regularization method to learn multimodal representations from aligned imperfect time series data. Pham et al.  [11]  utilize sequential cyclic translations to learn robust joint representations that may not require all modalities as input at inference time. Tang et al.  [15]  further explore pairwise bidirectional modality translations. Moreover, Zhao et al.  [14]  employ data augmentation and cyclic translations based on cascaded residual autoencoder  [50]  to tackle the uncertain missing modality issue. However, these modality translation-based methods are developed to handle the entire loss of one or more modalities which is less likely to happen in real-world scenarios. Besides, most of them only accept aligned sequences as inputs. Recently, Yuan et al.  [16]  propose to reconstruct low-level missing features to implicitly force the model to learn semantic features from incomplete multimodal sequences. Lian et al.  [51]  also introduce low-level feature reconstruction for incomplete multimodal learning in conversations. Compared with these two methods, our proposed method additionally utilizes explicit high-level feature attraction to improve model robustness. Moreover, our experiments demonstrate that explicit high-level feature attraction is superior to implicit low-level feature reconstruction and they work best when combined with each other.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Method",
      "text": "In this section, we describe the proposed Efficient Multimodal Transformer with Dual-Level Feature Restoration (EMT-DLFR) for robust MSA, with its whole pipeline in the incomplete modality setting shown in Fig.  1 . In this setting, we use the ground-truth complete view to achieve robust representation learning from incomplete multimodal sequences. Specifically, for both views, we first utilize modality-specific encoders to obtain utterance-level (i.e., global) and element-level (i.e., local) intra-modal features from different modality inputs. Then the Efficient Multimodal Transformer (EMT) is employed to effectively and efficiently capture useful cross-modal interactions between the global multimodal context and local unimodal features. After that, utterance-level intra-and inter-modal features are combined to get the final sentiment intensity prediction. To promote model robustness to random modality feature missing, the Dual-Level Feature Restoration (DLFR) conducts high-level feature attraction and low-level feature reconstruction simultaneously. The former explicitly attracts high-level intra-and inter-modal representations of two views in the latent space, while the latter reconstructs lowlevel modality inputs from complete view to implicitly urge the model to learn semantic information. In the complete modality setting, the problem degenerates into a simple case. Thus, we do not perform DLFR and only the original prediction loss is used in this setting.\n\nIn the following parts, we begin by giving a problem definition. Then we elaborate on the four main modules in EMT-DLFR: unimodal feature encoder (Section 3.2), EMT (Section 3.3), prediction module (Section 3.4), and DLFR (Section 3.5). Finally, we present the overall loss function for model training in two modality settings (Section 3.6).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Problem Definition",
      "text": "The task of MSA is to predict the sentiment intensity of the speaker in the video. Typically, three modalities are involved in MSA, i.e., text or language (l), audio (a), and vision (v). We denote the complete input feature sequences as Xm ∈ R Tm×fm , where T m is sequence length and f m is the feature dimension of modality m ∈ {l, a, v}. Specifically, Xa and Xv are shallow features extracted by open- source tools, while Xl is the raw text tokens output by the BERT  [52]  tokenizer. To mimic random modality feature missing in real-world scenarios, we randomly mask the complete sequence Xm to obtain the incomplete sequence Xm = F ( Xm , g m ) ∈ R Tm×fm , where F (•) is the mask function, g m ∈ {0, 1} Tm is a random temporal mask which indicates the positions to mask. For the audio and video modality, the mask function replaces the original feature vector in the masked position with a zero vector. For the text modality, it replaces the original token with the [UNK] token in BERT vocabulary  [16] . Our goal is to develop a robust model which can efficiently integrate all the available multimodal information to make an accurate prediction of sentiment intensity score y ∈ R in both complete and incomplete modality settings. Note that, when we do not use the diacritical mark˚or˜(e.g., X), it indicates that both views can be applied.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Unimodal Feature Encoder",
      "text": "Following previous works  [16] ,  [29] ,  [32] , we use the powerful BERT  [52]   [53] to capture the temporal dependencies in the feature sequence, i.e.,\n\nwhere H m ∈ R Tm×dm , m ∈ {l, a, v}. Since the [CLS] token in BERT aggregates the information from all tokens, we use its embedding as the utterance-level representation of the text modality h l ∈ R d l . For h a ∈ R da and h v ∈ R dv , we simply use the feature of the last time step in H a and H v . Finally, we use several linear layers to project H l , H a , H v , h l , h a , and h v to the same dimension d to facilitate subsequent multimodal fusion.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Efficient Multimodal Transformer",
      "text": "In this part, we first give a preliminary introduction. Then we introduce the basic building block of EMT, named Mutual Promotion Unit (MPU). Based on MPU, we elaborate on three multimodal fusion strategies for modeling crossmodal interactions. The first two are inspired by two predominant models (i.e., MulT  [12]  and PMR  [13] ) in MSA. However, both of them suffer from quadratic computational complexity over the involved modalities. The third practically has a linear scaling cost and thus is adopted in EMT by default. Finally, we present hierarchical parameter sharing for EMT to further improve parameter efficiency.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Preliminary",
      "text": "Self-attention (SA) is the core component in Transformer. It allows modeling of global dependencies in a sequence via scaled dot-product attention  [41] . For the input sequence H t ∈ R Tt×d , we define the Querys as\n\nThen SA can be formulated as follows:\n\nCross-modal attention (CA) involves two modalities. The Querys are from the target modality t, while the Keys and Values are from the source modality s, i.e.,\n\nIn this way, CA can provide a latent adaptation from modality s to t:\n\nwhich is a good way to fuse cross-modal information  [12] . Note that, for simplicity, we only present the formulation of single-head attention. In practice, we use multi-head SA or CA (i.e., MHSA or MHCA) to allow the model to attend to information from different feature subspaces  [41] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Mutual Promotion Unit",
      "text": "The architecture of MPU is shown in Fig.  2 . It mainly utilizes symmetric cross-modal attention to explore inherent correlations between elements across two input feature sequences. In this way, MPU enables useful information exchange between two sequences and thus they can mutually promote each other. To allow further information integration, selfattention is employed to model temporal dependencies in each feature sequence. Formally, MPU takes two sequences H m and H n as inputs and outputs their mutually promoted ones H m→n and H n→m :\n\nwhere → and ↔ indicates the direction of information flow. In specific, the calculation of M P U n→m (H m , H n ) is as follows:\n\nwhere LN denotes layer normalization  [54] , FFN is the position-wise feed-forward network in Transformer.\n\nMPU m→n (H n , H m ) can be formulated in a similar way.\n\nConsidering that the computational time complexity of",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Multimodal Fusion Strategy",
      "text": "Built upon MPU, we now describe three multimodal fusion strategies in a progressive manner and give a computational complexity analysis for each of them (summarized in Table  1 ). Note that the third fusion strategy (Fig.  1 ) is adopted in EMT by default and we leave their empirical comparisons in the ablation study (Section 5.2). For simplicity, we only present the information flow from layer i to i + 1 in an EMT with L layers. We suppose that the input feature sequence is\n\nwhere m ∈ {l, a, v} and PE m ∈ R Tm×d is the sinusoidal position embedding in the vanilla Transformer.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Table 1",
      "text": "Computational complexity analysis of three multimodal fusion strategies. M is the number of involved modalities, T is the temporal length of the input feature sequence. For simplicity, we assume aligned multimodal inputs. The detailed time complexity for unaligned inputs can refer to the main text.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Fusion Strategy Space Complexity Time Complexity",
      "text": "• One-to-One Local-Local Fusion. This fusion strategy shares the same spirit with MulT  [12] . The core is to explore one-to-one local-local (OOLL) cross-modal interactions across all modalities by applying MPU to each paired modalities, i.e.,\n\na→l , H\n\nWhen there are M modalities involved in the fusion process, the number of required MPUs in each layer is\n\n). If all modalities are aligned (i.e., T m = T, ∀m), the time complexity degenerates into O(M 2 T 2 ). Thus, OOLL suffers from quadratic space and time complexity over the involved modalities.\n\n• One-to-All Local-Local Fusion. Since it is inefficient to fuse multiple modalities in a one-to-one manner, this strategy utilizes the common message introduced in PMR  [13]  to explore one-to-all local-local (OALL) cross-modal interactions in a single MPU. The common message C is initialized by temporal concatenation of unimodal feature sequences, i.e.,\n\nDue to the introduction of the common message, OALL only requires M MPUs in each layer. After conducting a similar complexity analysis, we can get the time complexity of OALL:\n\n2 ), which is greater than that of OOLL. In the modality-aligned setting, it degenerates into O(M 2 T 2 ), which indicates that this strategy also has a quadratic scaling cost.\n\n• One-to-All Global-Local Fusion. The above analysis indicates that both OOLL and OALL have quadratic computational complexity over the involved modalities. We observe that this issue results from the fact that they both explore local-local cross-modal interactions across all modalities. We believe that it is neither efficient nor necessary to perform multimodal fusion in a local-local fashion. On the one hand, a large amount of redundancy exists in the unimodal feature sequence, especially for the audio and video modality which has high sampling rates. On the other hand, too many local features in one modality (or the common message) may distract the attention of another modality during their interactions because the latter could not 'see' the global (i.e., summarized) information of the former. Moreover, it could increase the risk of overfitting spurious crossmodal correlation signals. Therefore, we believe that the utterance-level representations from each modality can substitute for the common message in OALL and serves as the global multimodal context G to interact with local unimodal features, i.e.,\n\nwhere\n\n, and it degenerates into O(M T 2 ) in the modality-aligned setting. Thus, the default OAGL fusion strategy in EMT not only has linear space complexity but also enjoys linear computations over the involved modalities.\n\nFinally, we briefly introduce the pooling layer (Fig.  1 ) for aggregating promoted information from different modalities to facilitate subsequent fusion. Specifically, we utilize an attention-based pooling layer to implement it. Taking OAGL as an example, we have three promoted global multimodal contexts, i.e., G\n\na→g , and\n\nv→g . The new global multimodal context can be obtained as follows:\n\nwhere\n\nv→g ), v, W , and b are learnable parameters. We also develop other two pooling methods, including average pooling and MLP-based pooling. However, they have inferior performance (Section 5.2).",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Hierarchical Parameter Sharing",
      "text": "Inspired by  [19] ,  [55] , we propose hierarchical parameter sharing to further improve model parameter efficiency. Concretely, there are three levels to share parameters in EMT, including MPU-level sharing, modality-level sharing, and layer-level sharing. The first level shares the parameters of two directional sub-MPUs in an MPU, i.e., MPU n→m and MPU m→n in Equation  4 . The second level shares MPUs across modalities, i.e., MPU [i]  m↔g (m ∈ {l, a, v}) in Equation  8 . And the last one shares parameters across layers. We show that the hierarchical parameter sharing strategy not only improves parameter efficiency but also can make the optimization of EMT easier in Section 5.2.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Prediction Module",
      "text": "After multimodal fusion, we flatten the temporal dimension of the final global multimodal context (i.e., G [L] ) in EMT to get utterance-level inter-modal representation g ∈ R 3d . We then concatenate g and utterance-level intra-modal representations {h m } (m ∈ {l, a, v}) and finally pass them through a multi-layer perceptron (MLP) to get the sentiment prediction y . Following previous works  [16] ,  [29] , we employ L1 loss as the prediction loss, i.e., L task = |y -y | (10)",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Dual-Level Feature Restoration",
      "text": "To improve model robustness in the incomplete modality setting, we utilize the complete view to perform the duallevel feature restoration (DLFR), including implicit low-level feature reconstruction and explicit high-level feature attraction. Note that, the complete view is only used to guide representation learning during training, and it will not be available at the test stage.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Low-Level Feature Reconstruction",
      "text": "Reconstruction-based training objective (e.g., masked autoencoding) has achieved great success for representation learning in the fields of computer vision  [56] ,  [57] ,  [58] , natural language processing  [52] ,  [59] , and speech signal processing  [60] ,  [61] . Motivated by this, we leverage lowlevel feature restoration (LLFR), i.e., low-level feature reconstruction, to implicitly encourage the model to learn semantic representations from incomplete multimodal inputs.\n\nAfter multimodal fusion via EMT, the incomplete sequence Xm is mapped to a latent sequence Zm = H[L] m that has sufficient awareness of both global inter-modal and local intra-modal information. Thus, we pass Zm through a simple MLP-based decoder r(•) to reconstruct the complete sequence Xm . Note that, since reconstructing raw text to- kens will waste a large amount of model capacity 2 , we use the output embedding of the BERT model Hl (instead of raw text token sequence Xl ) as the reconstruction target of the text modality. Following  [16] , we employ the smooth L1 loss to evaluate the reconstruction quality, i.e.,\n\nwhere,\n\nand the temporal mask g m (m ∈ {l, a, v}) is used to exclude the loss from unmasked positions.\n\n2. Reconstructing raw text tokens from text representations will need a very big projection matrix W ∈ R 30522×768 with about 23.4M parameters (30522 is the vocabulary size of BERT, 768 is the dimension of text representations).",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Projector Predictor",
      "text": "Projector Predictor Similarity Similarity Weight Sharing Fig.  3 . The illustration of symmetric SimSiam loss.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "High-Level Feature Attraction",
      "text": "As a handcrafted pretext task, low-level feature reconstruction could fail to encourage the model to learn semantic information, as the model may find a shortcut to solve this task (i.e., only utilize local neighbor information to accomplish this task instead of inferring global semantics)  [17] ,  [58] ,  [62] . To this end, we further utilize siamese representation learning to explicitly attract high-level representations of complete and incomplete views in the latent space. As shown in Fig.  1 , except for Xm , we also pass Xm through the model to get utterance-level inter-modal representation g and intra-modal representations { hm } of complete view.\n\nThe straightforward way to perform feature attraction is to maximize the cosine similarity of representations from two views. However, this method has the risk of collapsing as it may overwhelm the task loss and the model encodes both complete and incomplete inputs to constant vectors  [23] ,  [63] . Therefore, we employ a recently proposed selfsupervised representation learning framework, SimSiam  [23] , to avoid possible collapsing solutions. As shown in Fig.  3 , SimSiam consists of an MLP-based projector p(•) and an MLP-based predictor q(•). The input representations of two views are first processed by the projector. Then the predictor maps the intermediate representation of one view to that of the other view. Besides, a stop-gradient operation sg(•) is used to avoid trivial solutions. Formally, we define the symmetric SimSiam loss of utterance-level intra-and intermodal representations as follows:\n\nwhere,\n\nis the negative cosine similarity function, and m ∈ {l, a, v}.\n\nTo ensure the quality of intra-and inter-modal representations from the complete view, we also send { hm } and g to the prediction module and add the corresponding prediction loss in the attraction loss. Thus, the total feature attraction loss is as follows:",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Overall Loss Function",
      "text": "In the incomplete modality setting, the task loss is the prediction loss of incomplete view, i.e., L task = |y-ỹ |. Thus, the overall loss function can be formulated as follows:\n\nwhere λ 1 and λ 2 ∈ R are the weights that balance the contribution of dual-level feature restoration to L.\n\nIn the complete modality setting, it is not necessary to perform dual-level feature restoration. Therefore, the overall loss function in this setting is L = L task = |y -ẙ |.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Datasets",
      "text": "In this paper, we conduct extensive experiments on three commonly used benchmark datasets in MSA. We give a brief introduction to each of them and summarize their basic statistics in Table  2 .\n\nCMU-MOSI. The CMU-MOSI dataset  [24]  is a widely used MSA benchmark dataset in English. It is a collection of YouTube monologues in which the speakers share their opinions on a wide range of subjects (such as movies). CMU-MOSI consists of a total of 2,199 utterance-level video segments from 93 videos of 89 distinct speakers. Each video segment is manually annotated with a sentiment intensity score, which is defined from -3 (strongly negative) to 3 (strongly positive).\n\nCMU-MOSEI. The CMU-MOSEI dataset  [25]  is the next generation of CMU-MOSI. Compared with CMU-MOSI, it has much larger training samples and more variations in speakers and video topics. Specifically, CMU-MOSEI contains 23,453 manually annotated utterance-level video segments from 1,000 distinct speakers and 250 different topics.\n\nCH-SIMS. The CH-SIMS dataset  [26]  is a Chinese MSA benchmark dataset. Compared with the above two datasets, it has both multimodal and unimodal annotations. Nevertheless, we only use the former in this paper. CH-SIMS consists of 2,281 utterance-level video segments from 60 videos whose types span from movies, TV series, and variety shows. Each video segment is manually annotated with a sentiment intensity score defined from -1 (strongly negative) to 1 (strongly positive).",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "Since sentiment intensity prediction is primarily a regression task, the typically adopted evaluation metrics are mean absolute error (MAE) and Pearson correlation coefficient (Corr). Researchers also convert the continuous score into different discrete categories and report classification accuracy. Following previous works  [12] ,  [16] ,  [25] ,  [32] , we report seven-class accuracy (Acc-7), five-class accuracy (Acc-5), binary accuracy (Acc-2), and F1-score on CMU-MOSI and CMU-MOSEI. Note that, there are two distinct methods for the binary formulation, i.e., negative/non-negative  [25] , and negative/positive  [12] . Thus, we report the Acc-2 and F1score of each method and use a segmentation marker -/-to differentiate them, where the left score is for negative/nonnegative and the right score is for negative/positive. On CH-SIMS, following  [26] , we report five-class accuracy (Acc-5), three-class accuracy (Acc-3), and binary accuracy (Acc-2). For all metrics but MAE, higher values indicate better performance.\n\nTo evaluate the model's overall performance under various missing rates in the incomplete modality setting, we follow  [16]  to compute the Area Under Indicators Line Chart (AUILC) for each of the above metrics. Suppose that the model performance on a metric under increasing missing rates {p 0 , p 2 , ..., p t } is {v 0 , v 2 , ..., v t }, the AUILC of this metric is defined as follows:\n\nTo be consistent with  [16] , we evaluate the model under the missing rates of {0.1, 0.2, ..., 1.0} on CMU-MOSI and CMU-MOSEI, and under the missing rates of {0.1, 0.2, ..., 0.5} on the CH-SIMS dataset.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Feature Extraction",
      "text": "To make fair comparisons, we use the official unaligned features which are provided by the corresponding benchmark datasets and adopted by top-performing MSA methods.\n\nText Modality. Transformer-based pre-trained language models have achieved state-of-the-art performances on a wide range of tasks in natural language processing. In agreement with recent works  [16] ,  [29] ,  [30] , we employ the pre-trained BERT model from the open-source Transformers library  [64]  to encode raw text. Specifically, we use bert-baseuncased model for CMU-MOSI and CMU-MOSEI and bertbase-chinese model for CH-SIMS.\n\nAudio Modality. For CMU-MOSI and CMU-MOSEI, COVAREP  [65]  acoustic analysis framework is utilized to extract the low-level acoustic features, which mainly consists of pitch, glottal source parameters, and 12 Mel-frequency cepstral coefficients (MFCCs). For CH-SIMS, an audio and music analysis Python package, Librosa  [66] , is used to extract logarithmic fundamental frequency, 12 Constant-Q chromatograms, and 20 MFCCs.\n\nVision Modality. For CMU-MOSI and CMU-MOSEI, Facet 3 is employed to extract 35 facial action units, which record facial muscle movements related to emotions. For CH-SIMS, OpenFace 2.0  [67]  facial behavior analysis toolkit is used to extract 17 facial action units, 68 facial landmarks, and several features related to the head and eyes.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Implementation Details",
      "text": "We implement the proposed model using the PyTorch  [68]  framework. To train the model, we utilize an Adam  [69]  optimizer and adopt an early-stopping strategy with the patience of 8 epochs. For the hyper-parameters tuning, we perform a random search. The detailed configurations on CMU-MOSI, CMU-MOSEI, and CH-SIMS are summarized in Table  3 . In the complete modality setting, aligned with  [29] ,  [30] , we run the model five times and report average results for each evaluation metric. While in the incomplete modality setting, we follow  [16]  and run the model three times to ensure a fair comparison.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Baselines",
      "text": "To comprehensively evaluate the performance of our proposed method in incomplete and complete settings, we consider both general MSA approaches and those which are specially designed to deal with random modality feature missing as our baselines. TFN. Tensor Fusion Network (TFN)  [33]  introduces a three-fold Cartesian product-based tensor fusion layer to explicitly model intra-modality and inter-modality dynamics in an end-to-end manner.\n\nLMF. Low-rank Multimodal Fusion (LMF)  [34]  leverages modality-specific low-rank factors to compute tensor-based multimodal representations, which makes tensor fusion more efficient.\n\nMulT. Multimodal Transformer (MulT)  [12]  utilizes directional pairwise cross-modal attention to capture intermodal correlations in unaligned multimodal sequences.\n\nMISA. This method  [32]  factorizes modalities into Modality-Invariant and -Specific Representations (MISA) using a combination of specially designed losses and then performs multimodal fusion on these representations.\n\nSelf-MM. Self-Supervised Multi-task Multimodal (Self-MM) sentiment analysis network  [29]  designs a unimodal label generation module based on self-supervised learning to explore unimodal supervision.\n\nMMIM. MultiModal InfoMax (MMIM)  [30]  proposes a hierarchical mutual information maximization framework to guide the model to learn shared representations from all modalities.\n\nAMML. Adaptive Multimodal Meta-Learning (AMML)  [70]  introduces a meta-learning-based method to learn better unimodal representations and then adapt them for subsequent multimodal fusion. TFR-Net. Transformer-based Feature Reconstruction Network (TFR-Net)  [16]  employs intra-and inter-modal attention and a feature reconstruction module to deal with random modality feature missing in unaligned multimodal sequences.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Comparison To State-Of-The-Art",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Complete Modality Setting",
      "text": "First, we compare the proposed EMT with top-performing MSA methods in the complete modality setting. Table  4  shows the results on CMU-MOSI and CMU-MOSEI. Since the baseline papers do not have a common evaluation setting and only report results on partial metrics, we present both the results from original papers and those reproduced by ourselves under the same setting (they are generally comparable). For fair and full comparisons, we mainly compare ours with the reproduced one. From Table  4 , we first observe that our EMT shows superior performance over previous state-of-the-art Transformer-based methods (i.e., TFR-Net, and MulT). Specifically, it surpasses the best-performing TFR-Net by 0.016 MAE on CMU-MOSI and 2.2% Acc-7 on CMU-MOSEI, and outperforms MulT by 0.141 MAE on CMU-MOSI and 0.043 Corr on CMU-MOSEI. We can attribute these encouraging results to the effective yet efficient global-local cross-modal interaction modeling in EMT, because both two baseline methods focus on capturing pairwise local-local cross-modal interactions, which not only suffer from a large amount of redundancy but also increase the risk of overfitting. In addition to Transformer-based methods, EMT also surpasses several recent state-of-theart non-Transformer-based methods (e.g., AMML, MMIM, and Self-MM), setting new records on most metrics. Finally, we present the results on the Chinese MSA dataset CH-SIMS in Table  5 . It can be seen that EMT achieves the best performance on all metrics. For instance, it outperforms the second performer Self-MM by 0.015 MAE, 0.022 Corr, and 1.5% Acc-2. In summary, the above results demonstrate the effectiveness of EMT in the complete modality setting.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Incomplete Modality Setting",
      "text": "Next, we concentrate on evaluating the robustness of our EMT-DLFR under various missing rates. Note that, to generate incomplete views, we apply the same missing rate to three modalities during both the training and test stages. Besides, we independently generate the random temporal mask for each modality. The performance comparison on CMU-MOSI, CMU-MOSEI, and CH-SIMS are shown in Fig.  4, 5,  and 6 , respectively. For simplicity, we only show four representative metrics (i.e., MAE, Corr, Acc-7/Acc-5, and Acc-2) for each dataset. Note that the Acc-2 on CMU-MOSI and CMU-MOSEI means the binary accuracy calculated on negative/positive samples. From the three figures, we have the following observations: (1) In general, with the increase in missing rates, the performances of all methods decrease quasi-linearly. It indicates that random modality feature missing in unaligned multimodal sequences will moderately degrade the model's performance and this issue needs to be carefully addressed in the real-world deployment of MSA models. (2) EMT-DLFR outperforms all other compared methods in most cases on three datasets, especially when the missing rate is relatively high, thus clearly illustrating the robustness of our proposed method in the incomplete modality setting. To quantitatively evaluate the overall performance of different methods, we follow  [16]  to calculate the AUILC of each evaluation metric, as stated in Section 4.2. The results on CMU-MOSI and CMU-MOSEI are shown in Table  6 . Similar to the complete case, we report both original and reproduced results, and we mainly compare ours with the latter. On CMU-MOSI, we can find that the state-ofthe-art TFR-Net which is specialized to deal with random modality feature missing outperforms other general MSA methods in most cases. This indicates that low-level feature reconstruction adopted by TFR-Net can indeed force the model to learn certain semantic information from incomplete multimodal sequences. Nevertheless, our EMT-DLFR which utilizes both low-level feature reconstruction and high-level feature attraction improves both TFR-Net and other methods by a large margin on almost all metrics. To be specific, it outperforms the second performer by 0.050 MAE, 0.034 Corr, 4.7% Acc-7, 5.1% Acc-5, and 3.9%/4.1% F1, which amply demonstrates the superiority of the proposed method. On CMU-MOSEI, we observe that the difference between different methods is smaller than that on CMU-MOSI (also shown in Fig.  5 ), which could be partly ascribed to much larger training samples in this dataset. Our EMT-DLFR still achieves 0.020 and 0.032 higher performance on MAE and Corr than the second performer. We further present the results on CH-SIMS in Table  7 . The proposed  TABLE 6  Overall performance comparison on CMU-MOSI and CMU-MOSEI in the incomplete modality setting. The reported result is the AUILC of each evaluation metric, which is calculated under the missing rates of {0.1, 0.2, ..., 1.0}. ♦ : results from  [16] . All other results are reproduced using publicly available source codes and original hyper-parameters under the same setting. We run each model three times and report average results.\n\nFor all metrics but MAE, higher values indicate better performance.",
      "page_start": 9,
      "page_end": 11
    },
    {
      "section_name": "Cmu-Mosi",
      "text": "",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Table 7",
      "text": "Overall performance comparison on CH-SIMS in the incomplete modality setting. The reported results are the AUILC of each evaluation metric, which are calculated under the missing rates of {0.1, 0.2, ..., 0.5}. ♦ : results from  [16] . All other results are reproduced using publicly available source codes and original hyper-parameters under the same setting. We run each model three times and report average results. For all metrics but MAE, higher values indicate better performance. method also achieves superior performance, outperforming the previous state-of-art by 0.015 MAE and 0.025 Corr. To summarize, the above results verify the robustness of our proposed method in the incomplete modality setting.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Ablation Study",
      "text": "To further investigate the influence of each component in our method, we conduct comprehensive ablation experiments on CMU-MOSI in the incomplete modality setting.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Multimodal Fusion Strategy",
      "text": "We first validate the effectiveness and efficiency of the proposed multimodal fusion strategy. We compare the default OAGL fusion strategy adopted by EMT with two baseline fusion strategies (i.e., OOLL and OALL) inspired by previous methods as stated in Section 3.3. We show the performance of three fusion strategies in Fig.  7 . We can find that OAGL outperforms OOLL and OALL, which verifies the feasibility and effectiveness of utilizing utterance-level representations from each modality as the global multimodal context to interact with local unimodal features. We think that the superiority of OAGL mainly comes from the simplification of cross-modal interaction modeling. In comparison to the local-local version in OOLL and OALL, it not only minimizes redundant information but also encourages the model to concentrate exclusively on prominent cross-modal correlation patterns, thus making it less likely to overfit the spurious ones. Additionally, inspired by  [13] , for OALL and OAGL, we develop their serial variants by replacing H m in the last row of Equation 4 with H n→m (i.e., the promoted H m ) in the second row. From Fig.  7 , we observe that the serial variant performs a bit worse than the default one for both OALL and OAGL, which justifies the parallel design choice of two fusion strategies. We believe this might be due to the loss of original unimodal information brought by the too-fast feature update in the serial implementation.\n\nTo further illustrate the efficiency of OAGL over OOLL and OALL, we give a practical complexity comparison of them. Since there are three input modalities, these three strategies have the same space complexity (as they all need 3 MPUs in each fusion layer). Thus, we mainly evaluate the time complexity in this paper. Nevertheless, it is worth noting that OAGL has linear scalability of MPUs when more modalities are involved in the fusion process. In Table  8 , we compare three fusion strategies in terms of the amount of of the model. For the number of parameters, we do not count the networks for feature reconstruction and attraction because they will not be used during inference. We run each experiment five times using a batch size of 32 on a single Tesla V100 GPU (32GB) and report the mean value for each metric. In addition to OOLL and OALL, we also provide metrics of two state-of-the-art multimodal Transformers as references, including MulT and TFR-Net.\n\nFrom Table  8 , we can observe that OAGL has the least MACs among the three fusion strategies (5× less than OALL and 2× less than OOLL). This is expected since OAGL enjoys linear computational complexity over the involved modalities in practice while both OOLL and OALL have a quadratic scaling cost. Compared with OOLL, OALL which utilizes the common message (i.e., local multimodal context) to explore multi-way cross-modal interactions achieves better performance (as shown in Fig.  7 ) but at the expense of a much larger amount of computation (especially the GPU memory usage). In contrast, thanks to the introduction of the global multimodal context, our OAGL not only has the best performance but also has the least computation. We also notice that the improvement in training speed is less significant than that of MACs. This is owing to the dominant role of the text modality feature encoder (i.e., BERT) in the overall computational overhead. Finally, the state-of-the-art TFR-Net and MulT, which utilize similar strategies to OOLL for multimodal fusion, also have much larger computational costs and more parameters than our OAGL. To summarize, the above quantitative results demonstrate the efficiency of OAGL over traditional local-local methods for cross-modal interaction modeling.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Hierarchical Parameter Sharing For Emt",
      "text": "To verify the role of hierarchical parameter sharing for EMT, we evaluate different combinations of three-level (i.e., MPUlevel, modality-level, and layer-level) parameter sharing strategies in Table  9 . Surprisingly, we find that all methods have comparable performance and the full (i.e., notshared) model is even slightly inferior to several parametersharing variants. Moreover, the all-shared model achieves decent results only with negligible performance degradation on MAE when compared to the full model. These results indicate that EMT is more difficult to train when there are too many parameters, and parameter sharing can act as a regularization constraint to alleviate this problem to some extent. Another benefit of parameter sharing is the big improvement in parameter efficiency. Note that, compared with the not-shared case, sharing all parameters in EMT leads to a reduction of 6M parameters. Since the text BERT encoder has 109.5M parameters, there are only less than 1M (actually, 0.5M) parameters in EMT for the all-shared model.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Pooling Layer In Emt",
      "text": "In this part, we ablate the design choice of the pooling layer in EMT. The pooling layer in EMT is used to aggregate promoted information from different modalities. We compare the default attention pooling with two variants, i.e., average pooling and MLP-based pooling. The results are shown in Fig.  8 . As a non-parametric method, average pooling is slightly inferior to attention pooling since it can not make aware of the contributions of different modalities. MLP-based pooling directly maps multiple promoted features into a single one using a fully connected layer. Thus, compared with attention-pooling, it has much more parameters. However, MLP-based pooling achieves the worst performance among the three methods. We guess this could be caused by the overfitting problem considering that MSA datasets are relatively small.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Dual-Level Feature Restoration",
      "text": "We then investigate the contributions of low-level feature reconstruction and high-level feature attraction to the improvement of model robustness. We compare our full method EMT-DLFR with the following three variants:\n\n• EMT-LLFR: only use low-level feature reconstruction by removing L attra in Equation  16 .\n\n• EMT-HLFR: only use high-level feature attraction by removing L recon in Equation  16 . • EMT: use neither low-level feature reconstruction nor high-level feature attraction by removing both L recon and L attra in Equation  16 .\n\nFig.  9  shows the results of different methods. We can observe that EMT-LLFR and EMT-HLFR outperform vanilla EMT, which indicates both low-level feature reconstruction and high-level feature attraction can improve model robustness to random modality feature missing. Besides, high-level feature attraction is more crucial than low-level feature reconstruction considering that EMT-LLFR significantly underperforms EMT-HLFR by 0.022 MAE, 0.034 Corr, 2.0% Acc-7, and 0.9% Acc-2. This is conceptually intuitive since implicit low-level feature reconstruction might not be sufficient to force the model to infer high-level semantics, as it could find a shortcut (i.e., only using local neighbor information) to accomplish the reconstruction task. In contrast, directly performing explicit attraction of high-level features from the complete and incomplete view in the latent space is more beneficial and effective. Moreover, our full method EMT-DLFR achieves the best performance among all methods, which suggests that implicit low-level feature reconstruction and explicit high-level feature attraction are complementary to each other. Therefore, the combination of these two feature restoration mechanisms can be a unified framework for robust multimodal sentiment analysis. For reference, we also present the result of state-of-the-art TFR-Net in Fig.  9 . As expected, both EMT-LLFR and EMT-HLFR have better performance than TFR-Net. The encouraging thing is that even vanilla EMT slightly outperforms TFR-Net on several metrics (e.g., MAE, Acc-7, and Acc-2), which once again demonstrates the superiority of EMT over traditional local-local cross-modal fusion methods.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Sensitivity Of Loss Weights",
      "text": "After ablating the contribution of two feature restoration methods, we investigate the sensitivity of their corresponding weights in the loss function, i.e., λ 1 for low-level feature reconstruction and λ 2 for high-level feature attraction in Equation  16 . We try several values for each weight parameter in the range of 0.5 to 2.0 with a step of 0.5. The results of different combinations of λ 1 and λ 2 are shown in Fig.  10 . We mainly have the following observations: 1) All weight combinations have comparable performance, which suggests that our proposed method is insensitive to the values of λ 1 and λ 2 . 2) Too large weights may hurt the performance. 3) When λ 1 = 1.0 and λ 2 = 1.0, the model achieves the best performance. Thus, we use them as the default loss weights on this dataset.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Simsiam For High-Level Feature Attraction",
      "text": "Finally, we analyze the impact of SimSiam in siamese representation learning for explicit high-level feature attraction.\n\nThe alternative to SimSiam is to simply minimize the negative cosine similarity of utterance-level intra-and intermodal representations between complete and incomplete views. We present the results of two methods in Fig.  11 . We observe that the performance generally decreases when SimSiam is not used in siamese representation learning, which verifies the role of SimSiam in preventing the model from learning collapsed representations by virtue of its special architecture designs.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Visualization Analysis",
      "text": "To have a deeper understanding of how our model works when modeling unaligned multimodal sequences in both complete and incomplete modality settings, we empirically investigate the signals EMT captures by visualizing the cross-modal attention weights. Fig.  12",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Global Multimodal Context",
      "text": "And they were all just really bland and forget ##table of 2 to halve them. We also show the key video frames, audio waveform, text tokens from BERT, and alignment information for better interpretation. It should be noted that, due to the segmentation error, the first word And was not said by the speaker though it appears in the transcription provided by the dataset.\n\nAs shown in Fig.  12  (a), we find that our model captures meaningful global-local cross-modal interactions in the complete modality setting. Specifically, the global multimodal context pays its most attention to the visual segments where a disappointed facial expression occurs. For the audio modality, it learns to attend to the emphasized intervals in the acoustic sequence. While for the high-level text modality, we can see that sentiment words (e.g., bland and forgettable) are received stronger attention from the global multimodal context. These interesting observations qualitatively demonstrate the effectiveness of EMT for cross-modal interaction modeling. Finally, our model predicts a negative sentiment score of -2.1 for this sample, which is very close to the ground-truth label -2.0.\n\nIn the incomplete modality setting, we use a missing rate of 0.5 to generate incomplete multimodal sequences. The missing audio and visual features are padded with zeros. The missing text tokens are replaced by the unknown token [UNK] in BERT. In Fig.  12  (b), we use blur background and mosaic to indicate the missing vision and audio modality features. It should be noted that they might not coincide with the real-generated temporal masks since they are only used for illustrative purposes. From Fig.  12  (b), we observe that, although the attention matrices are more scattered due to the influence of random modality feature missing, our model still attends to those useful cross-modal signals captured in the complete modality setting. Notably, for the text modality, we find that the model pays more attention to the available token forget as expected but also gives partial attention to the missing tokens (e.g., bland and ##table). Under such a moderate missing rate, our model prediction remains a negative score of -1.0. These results once again verify the efficacy of the proposed dual-level feature restoration to the improvement of model robustness.\n\nIn addition, we also visualize the local-local cross-modal attention weights to empirically investigate the differences between the proposed global-local fusion strategy (i.e., OAGL) and the previous local-local one (e.g., OOLL and OALL). For simplicity, we only show OOLL in Fig.  13  as we have similar observations for OALL. We find that, although OOLL can capture a part of meaningful crossmodal correlation signals as OAGL, its attention matrices are typically low-rank (especially for the upper part in Fig.  13 ), which demonstrates that a large amount of redundancy exists in local-local cross-modal interactions and it can be greatly reduced via the global-local interactions in OAGL to achieve efficient multimodal fusion. This empirical finding is also consistent with that in the previous study  [12] . Moreover, we notice that OOLL attends to more irrelevant crossmodal information than OAGL (such as the relatively higher attention to audio and vision modalities when saying forgettable ), implying that the local-local fusion strategy could also increase the risk of overfitting the spurious correlations in unaligned multimodal data.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we have presented a generic and unified framework, named Efficient Multimodal Transformer with Dual-Level Feature Restoration (EMT-DLFR), for efficient and robust multimodal sentiment analysis. At the heart of EMT is the introduction of the global multimodal context, which enables effective and efficient exploration of global-local cross-modal interactions. It not only avoids the quadratic scaling cost of previous local-local cross-modal interaction modeling methods but also leads to performance gains. Furthermore, we utilize hierarchical parameter sharing to improve parameter efficiency and ease model training. To cope with random modality feature missing which typically occurs in realistic settings, DLFR employs both implicit low-level feature reconstruction and explicit high-level feature attraction to achieve robust representation learning from incomplete multimodal data. We find that, although the former is more effective than the latter, these two strategies are complementary to each other and thus can be combined to achieve better performance. Finally, extensive experiments on three datasets show that the proposed method achieves state-of-the-art performance in both complete and incomplete modality settings. In addition, empirical visualization analysis also demonstrates that our model can capture interpretable and robust cross-modal correlation signals for reliable sentiment prediction.\n\nIn future work, we hope to apply EMT-DLFR to the entire modality missing setting, noisy modality setting, and even more challenging noise-missing-mixed setting.\n\nIt's also interesting to explore the effectiveness of the proposed method on different multimodal learning tasks and datasets. Besides, since we do not take into consideration the importance of each modality, it would be helpful to explicitly incorporate it into EMT-DLFR to further improve performance. Finally, although our EMT enjoys a linear scaling cost over the involved modalities, it still suffers from quadratic complexity with respect to the input sequence length. Thus, how to further reduce the complexity from O(M T 2 ) to O(M T ) remains an attractive research direction.",
      "page_start": 15,
      "page_end": 16
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The overall architecture of EMT-DLFR, which mainly consists of the Efﬁcient Multimodal Transformer (EMT) and Dual-Level Feature",
      "page": 4
    },
    {
      "caption": "Figure 2: It mainly utilizes",
      "page": 4
    },
    {
      "caption": "Figure 2: The architecture of Mutual Promotion Unit (MPU).",
      "page": 5
    },
    {
      "caption": "Figure 1: ) is adopted in",
      "page": 5
    },
    {
      "caption": "Figure 3: The illustration of symmetric SimSiam loss.",
      "page": 7
    },
    {
      "caption": "Figure 1: , except for ˜Xm, we also pass ˚",
      "page": 7
    },
    {
      "caption": "Figure 3: , SimSiam consists of an MLP-based projector p(·) and an",
      "page": 7
    },
    {
      "caption": "Figure 4: , 5, and 6, respectively. For simplicity, we only show four",
      "page": 9
    },
    {
      "caption": "Figure 4: Performance comparison under various missing rates on CMU-MOSI.",
      "page": 10
    },
    {
      "caption": "Figure 5: Performance comparison under various missing rates on CMU-MOSEI.",
      "page": 10
    },
    {
      "caption": "Figure 6: Performance comparison under various missing rates on CH-SIMS.",
      "page": 10
    },
    {
      "caption": "Figure 5: ), which could be partly ascribed",
      "page": 10
    },
    {
      "caption": "Figure 7: We can ﬁnd that",
      "page": 11
    },
    {
      "caption": "Figure 7: Ablation study of multimodal fusion strategy on CMU-MOSI in the",
      "page": 11
    },
    {
      "caption": "Figure 7: , we observe that the",
      "page": 11
    },
    {
      "caption": "Figure 7: ) but at the expense of",
      "page": 12
    },
    {
      "caption": "Figure 8: Ablation study of pooling layer in EMT on CMU-MOSI in the",
      "page": 12
    },
    {
      "caption": "Figure 8: As a non-parametric method, average",
      "page": 12
    },
    {
      "caption": "Figure 9: Ablation study of loss function on CMU-MOSI in the incomplete",
      "page": 13
    },
    {
      "caption": "Figure 9: shows the results of different methods. We can",
      "page": 13
    },
    {
      "caption": "Figure 9: As expected, both EMT-LLFR and EMT-HLFR",
      "page": 13
    },
    {
      "caption": "Figure 10: We mainly have the following observations: 1) All",
      "page": 13
    },
    {
      "caption": "Figure 10: Ablation study of loss weights on CMU-MOSI in the incomplete",
      "page": 13
    },
    {
      "caption": "Figure 11: Ablation study of SimSiam for high-level feature attraction on",
      "page": 13
    },
    {
      "caption": "Figure 11: We observe that the performance generally decreases when",
      "page": 13
    },
    {
      "caption": "Figure 12: presents the global-",
      "page": 13
    },
    {
      "caption": "Figure 12: Visualization of global-local cross-modal attention weights from EMT in the complete and incomplete (under a missing rate of 0.5) modality",
      "page": 14
    },
    {
      "caption": "Figure 12: (a), we ﬁnd that our model cap-",
      "page": 14
    },
    {
      "caption": "Figure 12: (b), we use blur back-",
      "page": 14
    },
    {
      "caption": "Figure 13: as we have similar observations for OALL. We ﬁnd that,",
      "page": 14
    },
    {
      "caption": "Figure 13: ), which demonstrates that a large amount of redundancy",
      "page": 14
    },
    {
      "caption": "Figure 13: Visualization of local-local cross-modal attention weights from",
      "page": 15
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Low-Level Reconstruction": "High-Level Attraction\nor"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dual-Level Feature Restoration (DLFR)\nLSTM\nEMT\nBERT\nLSTM\nComplete View\n⊕\n⊕\n⊕": ""
        },
        {
          "Dual-Level Feature Restoration (DLFR)\nLSTM\nEMT\nBERT\nLSTM\nComplete View\n⊕\n⊕\n⊕": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 7: The proposed",
      "data": [
        {
          "TFN\nLMF": "MulT\nMISA\nSelf-MM"
        },
        {
          "TFN\nLMF": "MMIM\nTFR-Net\nEMT-DLFR"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 7: The proposed",
      "data": [
        {
          "TFN\nLMF\nMulT": "MISA\nSelf-MM\nMMIM\nTFR-Net\nEMT-DLFR"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 7: The proposed",
      "data": [
        {
          "TFN": "LMF\nMulT\nMISA\nSelf-MM\nMMIM"
        },
        {
          "TFN": "TFR-Net\nEMT-DLFR"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 7: The proposed",
      "data": [
        {
          "TFN": "LMF\nMulT\nMISA\nSelf-MM\nMMIM\nTFR-Net\nEMT-DLFR"
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A survey of multimodal sentiment analysis",
      "authors": [
        "M Soleymani",
        "D Garcia",
        "B Jou",
        "B Schuller",
        "S.-F Chang",
        "M Pantic"
      ],
      "year": "2017",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "2",
      "title": "Beneath the tip of the iceberg: Current challenges and new directions in sentiment analysis research",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "R Mihalcea"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "3",
      "title": "Computational media intelligence: Humancentered machine analysis of media",
      "authors": [
        "K Somandepalli",
        "T Guha",
        "V Martinez",
        "N Kumar",
        "H Adam",
        "S Narayanan"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "4",
      "title": "The multimodal sentiment analysis in car reviews (muse-car) dataset: Collection, insights and improvements",
      "authors": [
        "L Stappen",
        "A Baird",
        "L Schumann",
        "S Bjorn"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "5",
      "title": "Affective computing and sentiment analysis",
      "authors": [
        "E Cambria",
        "D Das",
        "S Bandyopadhyay",
        "A Feraco"
      ],
      "year": "2017",
      "venue": "A practical guide to sentiment analysis"
    },
    {
      "citation_id": "6",
      "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
      "authors": [
        "S Poria",
        "E Cambria",
        "R Bajpai",
        "A Hussain"
      ],
      "year": "2017",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "7",
      "title": "Context-dependent sentiment analysis in usergenerated videos",
      "authors": [
        "S Poria",
        "E Cambria",
        "D Hazarika",
        "N Majumder",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th annual meeting of the association for computational linguistics"
    },
    {
      "citation_id": "8",
      "title": "Memory fusion network for multi-view sequential learning",
      "authors": [
        "A Zadeh",
        "P Liang",
        "N Mazumder",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "9",
      "title": "Multi-attention recurrent network for human communication comprehension",
      "authors": [
        "A Zadeh",
        "P Liang",
        "S Poria",
        "P Vij",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Thirty-Second AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "10",
      "title": "Multimodal affective analysis using hierarchical attention strategy with wordlevel alignment",
      "authors": [
        "Y Gu",
        "K Yang",
        "S Fu",
        "S Chen",
        "X Li",
        "I Marsic"
      ],
      "year": "2018",
      "venue": "Proceedings of the conference. Association for Computational Linguistics. Meeting"
    },
    {
      "citation_id": "11",
      "title": "Found in translation: Learning robust joint representations by cyclic translations between modalities",
      "authors": [
        "H Pham",
        "P Liang",
        "T Manzini",
        "L.-P Morency",
        "B Óczos"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "12",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Y.-H Tsai",
        "S Bai",
        "P Liang",
        "J Kolter",
        "L.-P Morency",
        "R Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "13",
      "title": "Progressive modality reinforcement for human multimodal emotion recognition from unaligned multimodal sequences",
      "authors": [
        "F Lv",
        "X Chen",
        "Y Huang",
        "L Duan",
        "G Lin"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "14",
      "title": "Missing modality imagination network for emotion recognition with uncertain missing modalities",
      "authors": [
        "J Zhao",
        "R Li",
        "Q Jin"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "15",
      "title": "Ctfn: Hierarchical learning for multimodal sentiment analysis using coupled-translation fusion network",
      "authors": [
        "J Tang",
        "K Li",
        "X Jin",
        "A Cichocki",
        "Q Zhao",
        "W Kong"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "16",
      "title": "Transformer-based feature reconstruction network for robust multimodal sentiment analysis",
      "authors": [
        "Z Yuan",
        "W Li",
        "H Xu",
        "W Yu"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "17",
      "title": "Self-supervised visual feature learning with deep neural networks: A survey",
      "authors": [
        "L Jing",
        "Y Tian"
      ],
      "year": "2020",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "18",
      "title": "Set transformer: A framework for attention-based permutationinvariant neural networks",
      "authors": [
        "J Lee",
        "Y Lee",
        "J Kim",
        "A Kosiorek",
        "S Choi",
        "Y Teh"
      ],
      "year": "2019",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "19",
      "title": "Perceiver: General perception with iterative attention",
      "authors": [
        "A Jaegle",
        "F Gimeno",
        "A Brock",
        "O Vinyals",
        "A Zisserman",
        "J Carreira"
      ],
      "year": "2021",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "20",
      "title": "Attention bottlenecks for multimodal fusion",
      "authors": [
        "A Nagrani",
        "S Yang",
        "A Arnab",
        "A Jansen",
        "C Schmid",
        "C Sun"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "21",
      "title": "Momentum contrast for unsupervised visual representation learning",
      "authors": [
        "K He",
        "H Fan",
        "Y Wu",
        "S Xie",
        "R Girshick"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "22",
      "title": "A simple framework for contrastive learning of visual representations",
      "authors": [
        "T Chen",
        "S Kornblith",
        "M Norouzi",
        "G Hinton"
      ],
      "year": "2020",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "23",
      "title": "Exploring simple siamese representation learning",
      "authors": [
        "X Chen",
        "K He"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "24",
      "title": "Multimodal sentiment intensity analysis in videos: Facial gestures and verbal messages",
      "authors": [
        "A Zadeh",
        "R Zellers",
        "E Pincus",
        "L.-P Morency"
      ],
      "year": "2016",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "25",
      "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "A Zadeh",
        "P Pu"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)"
    },
    {
      "citation_id": "26",
      "title": "Ch-sims: A chinese multimodal sentiment analysis dataset with fine-grained annotation of modality",
      "authors": [
        "W Yu",
        "H Xu",
        "F Meng",
        "Y Zhu",
        "Y Ma",
        "J Wu",
        "J Zou",
        "K Yang"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "27",
      "title": "Towards multimodal sentiment analysis: Harvesting opinions from the web",
      "authors": [
        "L.-P Morency",
        "R Mihalcea",
        "P Doshi"
      ],
      "year": "2011",
      "venue": "Proceedings of the 13th international conference on multimodal interfaces"
    },
    {
      "citation_id": "28",
      "title": "Utterance-level multimodal sentiment analysis",
      "authors": [
        "V Pérez-Rosas",
        "R Mihalcea",
        "L.-P Morency"
      ],
      "year": "2013",
      "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "29",
      "title": "Learning modality-specific representations with self-supervised multi-task learning for multimodal sentiment analysis",
      "authors": [
        "W Yu",
        "H Xu",
        "Z Yuan",
        "J Wu"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "30",
      "title": "Improving multimodal fusion with hierarchical mutual information maximization for multimodal sentiment analysis",
      "authors": [
        "W Han",
        "H Chen",
        "S Poria"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "31",
      "title": "Conversational emotion analysis via attention mechanisms",
      "authors": [
        "Z Lian",
        "J Tao",
        "B Liu",
        "J Huang"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "32",
      "title": "Misa: Modalityinvariant and-specific representations for multimodal sentiment analysis",
      "authors": [
        "D Hazarika",
        "R Zimmermann",
        "S Poria"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "33",
      "title": "Tensor fusion network for multimodal sentiment analysis",
      "authors": [
        "A Zadeh",
        "M Chen",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "34",
      "title": "Efficient low-rank multimodal fusion with modality-specific factors",
      "authors": [
        "Z Liu",
        "Y Shen",
        "V Lakshminarasimhan",
        "P Liang",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "35",
      "title": "Divide, conquer and combine: Hierarchical feature fusion network with local and global perspectives for multimodal affective computing",
      "authors": [
        "S Mai",
        "H Hu",
        "S Xing"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "36",
      "title": "Dual low-rank multimodal fusion",
      "authors": [
        "T Jin",
        "S Huang",
        "Y Li",
        "Z Zhang"
      ],
      "venue": "Dual low-rank multimodal fusion"
    },
    {
      "citation_id": "37",
      "title": "Extending long short-term memory for multi-view structured learning",
      "authors": [
        "S Rajagopalan",
        "L.-P Morency",
        "T Baltrusaitis",
        "R Goecke"
      ],
      "year": "2016",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "38",
      "title": "Multimodal sentiment analysis with word-level fusion and reinforcement learning",
      "authors": [
        "M Chen",
        "S Wang",
        "P Liang",
        "T Baltrušaitis",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 19th ACM international conference on multimodal interaction"
    },
    {
      "citation_id": "39",
      "title": "Words can shift: Dynamically adjusting word representations using nonverbal behaviors",
      "authors": [
        "Y Wang",
        "Y Shen",
        "Z Liu",
        "P Liang",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "40",
      "title": "Integrating multimodal information in large pretrained transformers",
      "authors": [
        "W Rahman",
        "M Hasan",
        "S Lee",
        "A Zadeh",
        "C Mao",
        "L.-P Morency",
        "E Hoque"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "41",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "42",
      "title": "Factorized multimodal transformer for multimodal sequential learning",
      "authors": [
        "A Zadeh",
        "C Mao",
        "K Shi",
        "Y Zhang",
        "P Liang",
        "S Poria",
        "L.-P Morency"
      ],
      "year": "2019",
      "venue": "Factorized multimodal transformer for multimodal sequential learning",
      "arxiv": "arXiv:1911.09826"
    },
    {
      "citation_id": "43",
      "title": "Attention is not enough: Mitigating the distribution discrepancy in asynchronous multimodal sequence fusion",
      "authors": [
        "T Liang",
        "G Lin",
        "L Feng",
        "Y Zhang",
        "F Lv"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "44",
      "title": "Multimodal cross-and selfattention network for speech emotion recognition",
      "authors": [
        "L Sun",
        "B Liu",
        "J Tao",
        "Z Lian"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "45",
      "title": "Ctnet: Conversational transformer network for emotion recognition",
      "authors": [
        "Z Lian",
        "B Liu",
        "J Tao"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "46",
      "title": "Is cross-attention preferable to self-attention for multi-modal emotion recognition?",
      "authors": [
        "V Rajan",
        "A Brutti",
        "A Cavallaro"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "47",
      "title": "Training strategies to handle missing modalities for audio-visual expression recognition",
      "authors": [
        "S Parthasarathy",
        "S Sundaram"
      ],
      "year": "2020",
      "venue": "Companion Publication of the 2020 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "48",
      "title": "Analyzing modality robustness in multimodal sentiment analysis",
      "authors": [
        "D Hazarika",
        "Y Li",
        "B Cheng",
        "S Zhao",
        "R Zimmermann",
        "S Poria"
      ],
      "year": "2022",
      "venue": "Analyzing modality robustness in multimodal sentiment analysis",
      "arxiv": "arXiv:2205.15465"
    },
    {
      "citation_id": "49",
      "title": "Learning representations from imperfect time series data via tensor rank regularization",
      "authors": [
        "P Liang",
        "Z Liu",
        "Y.-H Tsai",
        "Q Zhao",
        "R Salakhutdinov",
        "L.-P Morency"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "50",
      "title": "Missing modalities imputation via cascaded residual autoencoder",
      "authors": [
        "L Tran",
        "X Liu",
        "J Zhou",
        "R Jin"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "51",
      "title": "Gcnet: Graph completion network for incomplete multimodal learning in conversation",
      "authors": [
        "Z Lian",
        "L Chen",
        "L Sun",
        "B Liu",
        "J Tao"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "52",
      "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "53",
      "title": "Long short-term memory",
      "authors": [
        "S Hochreiter",
        "J Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural computation"
    },
    {
      "citation_id": "54",
      "title": "Layer normalization",
      "authors": [
        "J Ba",
        "J Kiros",
        "G Hinton"
      ],
      "year": "2016",
      "venue": "Layer normalization",
      "arxiv": "arXiv:1607.06450"
    },
    {
      "citation_id": "55",
      "title": "Albert: A lite bert for self-supervised learning of language representations",
      "authors": [
        "Z Lan",
        "M Chen",
        "S Goodman",
        "K Gimpel",
        "P Sharma",
        "R Soricut"
      ],
      "year": "2019",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "56",
      "title": "Extracting and composing robust features with denoising autoencoders",
      "authors": [
        "P Vincent",
        "H Larochelle",
        "Y Bengio",
        "P.-A Manzagol"
      ],
      "year": "2008",
      "venue": "Proceedings of the 25th international conference on Machine learning"
    },
    {
      "citation_id": "57",
      "title": "Context encoders: Feature learning by inpainting",
      "authors": [
        "D Pathak",
        "P Krahenbuhl",
        "J Donahue",
        "T Darrell",
        "A Efros"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "58",
      "title": "Masked autoencoders are scalable vision learners",
      "authors": [
        "K He",
        "X Chen",
        "S Xie",
        "Y Li",
        "P Dollár",
        "R Girshick"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "59",
      "title": "Improving language understanding by generative pre-training",
      "authors": [
        "A Radford",
        "K Narasimhan",
        "T Salimans"
      ],
      "year": "2018",
      "venue": "Improving language understanding by generative pre-training"
    },
    {
      "citation_id": "60",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "61",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "62",
      "title": "Beit: Bert pre-training of image transformers",
      "authors": [
        "H Bao",
        "L Dong",
        "F Wei"
      ],
      "year": "2021",
      "venue": "Beit: Bert pre-training of image transformers",
      "arxiv": "arXiv:2106.08254"
    },
    {
      "citation_id": "63",
      "title": "Bootstrap your own latent-a new approach to self-supervised learning",
      "authors": [
        "J.-B Grill",
        "F Strub",
        "F Altché",
        "C Tallec",
        "P Richemond",
        "E Buchatskaya",
        "C Doersch",
        "B Avila Pires",
        "Z Guo",
        "M Azar"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "64",
      "title": "Transformers: State-of-the-art natural language processing",
      "authors": [
        "T Wolf",
        "L Debut",
        "V Sanh",
        "J Chaumond",
        "C Delangue",
        "A Moi",
        "P Cistac",
        "T Rault",
        "R Louf",
        "M Funtowicz"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations"
    },
    {
      "citation_id": "65",
      "title": "Covarep-a collaborative voice analysis repository for speech technologies",
      "authors": [
        "G Degottex",
        "J Kane",
        "T Drugman",
        "T Raitio",
        "S Scherer"
      ],
      "year": "2014",
      "venue": "2014 ieee international conference on acoustics, speech and signal processing (icassp)"
    },
    {
      "citation_id": "66",
      "title": "librosa: Audio and music signal analysis in python",
      "authors": [
        "B Mcfee",
        "C Raffel",
        "D Liang",
        "D Ellis",
        "M Mcvicar",
        "E Battenberg",
        "O Nieto"
      ],
      "year": "2015",
      "venue": "Proceedings of the 14th python in science conference"
    },
    {
      "citation_id": "67",
      "title": "Openface 2.0: Facial behavior analysis toolkit",
      "authors": [
        "T Baltrusaitis",
        "A Zadeh",
        "Y Lim",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE international conference on automatic face & gesture recognition"
    },
    {
      "citation_id": "68",
      "title": "Pytorch: An imperative style, high-performance deep learning library",
      "authors": [
        "A Paszke",
        "S Gross",
        "F Massa",
        "A Lerer",
        "J Bradbury",
        "G Chanan",
        "T Killeen",
        "Z Lin",
        "N Gimelshein",
        "L Antiga"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "69",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "70",
      "title": "Learning to learn better unimodal representations via adaptive multimodal meta-learning",
      "authors": [
        "Y Sun",
        "S Mai",
        "H Hu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "71",
      "title": "M-sena: An integrated platform for multimodal sentiment analysis",
      "authors": [
        "H Mao",
        "Z Yuan",
        "H Xu",
        "W Yu",
        "Y Liu",
        "K Gao"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations"
    }
  ]
}