{
  "paper_id": "2312.06466v1",
  "title": "Towards Domain-Specific Cross-Corpus Speech Emotion Recognition Approach",
  "published": "2023-12-11T15:53:57Z",
  "authors": [
    "Yan Zhao",
    "Yuan Zong",
    "Hailun Lian",
    "Cheng Lu",
    "Jingang Shi",
    "Wenming Zheng"
  ],
  "keywords": [
    "Cross-corpus speech emotion recognition",
    "speech emotion recognition",
    "transfer subspace learning",
    "domain adaptation",
    "domain-specific knowledge"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Cross-corpus speech emotion recognition (SER) poses a challenge due to feature distribution mismatch, potentially degrading the performance of established SER methods. In this paper, we tackle this challenge by proposing a novel transfer subspace learning method called acoustic knowledgeguided transfer linear regression (AKTLR). Unlike existing approaches, which often overlook domain-specific knowledge related to SER and simply treat cross-corpus SER as a generic transfer learning task, our AKITR method is built upon a well-designed acoustic knowledge-guided dual sparsity constraint mechanism. This mechanism emphasizes the potential of minimalistic acoustic parameter feature sets to alleviate classifier overadaptation, which is empirically validated acoustic knowledge in SER, enabling superior generalization in cross-corpus SER tasks compared to using large feature sets. Through this mechanism, we extend a simple transfer linear regression model to AKTLR. This extension harnesses its full capability to seek emotiondiscriminative and corpus-invariant features from established acoustic parameter feature sets used for describing speech signals across two scales: contributive acoustic parameter groups and constituent elements within each contributive group. Our proposed method is evaluated through extensive cross-corpus SER experiments on three widely-used speech emotion corpora: EmoDB, eNTERFACE, and CASIA. The results confirm the effectiveness and superior performance of our method, outperforming recent state-of-the-art transfer subspace learning and deep transfer learning-based cross-corpus SER methods. Furthermore, our work provides experimental evidence supporting the feasibility and superiority of incorporating domain-specific knowledge into the transfer learning model to address cross-corpus SER tasks.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "S PEECH plays a crucial role in human daily communica- tion, serving as a natural means for individuals to express their emotions such as Happiness, Fear, and Sadness. As a result, the research of speech emotion recognition (SER)  [1] ,  [2] ,  [3] , which seeks to empower computers to automatically understand emotional states from speech signals, holds significant practical value. Over the past few decades, SER has garnered substantial attention within the communities of human-computer interaction, affective computing, and signal processing, leading to the development of numerous wellperforming SER methods  [4] ,  [5] ,  [6] ,  [7] ,  [8] ,  [9] .\n\nHowever, it is important to note that most established SER methods, including those mentioned above, primarily focus on an ideal scenario where the training and testing speech signals belong to the same speech emotion corpus. In practical situations, the testing speech signals may differ significantly from the training speech signals, exhibiting variations in numerous factors, such as languages, recording equipment, and environmental conditions. This gives rise to a challenging but intriguing task known as cross-corpus SER  [10]  within the field of SER. In cross-corpus SER tasks, the training and testing speech signals originate from different speech emotion corpora and can be referred to as the source and target signals, respectively. Moreover, while we have access to ground truth emotion labels for the source speech samples, the target speech emotion corpus remains entirely unlabeled.\n\nIn the early stages, the research of cross-corpus SER mostly focus on feature engineering, aiming to enhance the corpusinvariant ability of acoustic parameter feature sets used to describe speech signals. For example, in the work of  [10] , three feature normalization schemes, including corpus normalization, speaker normalization, and speaker-corpus normalization, are designed to address feature distribution mismatches between source and target speech emotion corpora. Subsequently, Parlak et al.  [11]  attempt to use numerous feature selectors, such as linear forward selection, to seek high-quality speech features that are robust to corpus variance from existing comprehensive acoustic feature sets. In recent years, inspired by the tremendous success of transfer learning in various crossdomain recognition tasks  [12] ,  [13] , researchers have shifted their focus to the development of transfer learning methods for cross-corpus SER. These methods have achieved promising performance in recognizing emotions in speech signals across different corpora, marking a significant advancement in this field.\n\nBroadly, current transfer learning-based cross-corpus SER methods can be classfied into two types, including Transfer Subspace Learning and Deep Transfer Learning:\n\n(1) Transfer subspace learning-based cross-corpus SER methods typically begin by using a set of acoustic low-level descriptors (LLDs), such as fundamental frequency (F0) and Mel-frequency cepstral coefficients (MFCC), along with their associated functions, such as maximal and mean values, to describe the source and target speech signals. Subsequently, a transfer subspace learning model is developed to mitigate the distribution mismatch between the two feature sets. One early method can be traced back to the work of  [14] , in which Hassan et al. extend the support vector machine (SVM)  [15]  to an importance-weighted SVM (IW-SVM) for cross-corpus SER. IW-SVM incorporates three different transfer subspace learning models: kernel mean matching (KMM)  [16] , unconstrained least-squares importance fitting (uLSIF)  [17] , and Kullback-Leibler importance estimation procedure (KLIEP)  [18] . It is hence enabled to learn a set of weights for the source speech samples, ensuring that the weighted source speech feature sets align with the distribution of target speech feature sets. Another notable work is the transfer non-negative matrix factorization (TNNMF) models designed by Song et al.  [19] . These models integrate the maximum mean discrepancy (MMD)  [20]  to measure and minimize the discrepancies between the source and target speech feature distributions. Following this work, Luo et al.  [21]  further advance TNNMF model by jointly reducing the marginal and class-aware conditional feature distribution gaps between the two different speech sample sets.\n\n(2) In contrast to transfer subspace learning, deep transfer learning methods often utilize the speech spectrums of the original speech signals as input for deep neural networks, harnessing their powerful nonlinear representation capabilities to learn emotion-discriminative and corpus-invariant features. Parry et al.  [22]  examine the generalization capacity of deep neural networks for cross-corpus SER across six different speech emotion corpora. Their experimental results demonstrate that convolutional neural networks (CNNs)  [23]  exhibit superior generalisation capabilities compared to recurrent neural networks (RNNs)  [24] . Insipired by this observation, Zhao et al.  [25]  propose deep transductive transfer regression networks (DTTRN) based on CNN architectures. A key contribution of DTTRN is the incorporation of additional finegrained emotion class-aware conditional MMD, which aids in better bridging the distribution gap between learned source and target features compared to the original MMD. Additionally, Zhao et al.  [26]  introduce another CNN-based deep transfer learning method called deep implicit distribution alignment neural networks (DIDAN). DIDAN performs implicit distribution alignment for source and target speech corpora by replacing the minimization of MMD with sparsely reconstructing target samples using source samples. More recently, domainadversarial learning-based models  [27] ,  [28] ,  [29]  have been developed to learn more generalized representations of speech signals for cross-corpus SER. The key concept behind these methods are the introduction of an additional domain (corpus) classifier, which enables the deep neural networks to learn the generalized features to describe speech signals, regardless of their corpus sources.\n\nWhile both transfer subspace learning and deep transfer learning methods have demonstrated success in addressing the challenge of cross-corpus SER, it is worth noting that these methods often approach cross-corpus SER as a generic transfer learning task. This means that most of these methods focus primarily on developing transfer learning models without specifically considering the valuable acoustic knowledge inherent to SER. As a result, these transfer learning methods can be applied to other cross-domain recognition tasks without making significant modifications. According to the \"No Free Lunch Theorem\"  [30] , it is established that \"There is no universal learning algorithm that can provide the best solution for every problem. Each algorithm has its strengths and weaknesses, and its performance is highly dependent on the specific problem domain and data distribution.\" From this perspective, it can be argued that they may not offer ultimately satisfactory solutions for cross-corpus SER. In other words, incorporating domain-specific knowledge from SER to guide the design of transfer learning models could potentially lead to even better performance compared to the generic transfer learning models when dealing with cross-corpus SER. Therefore, our goal in this paper is to develop a domain-specific transfer learning approach for cross-corpus SER. Specifically, we propose a novel transfer subspace learning method called acoustic knowledgeguided transfer linear regression (AKTLR).\n\nThe basic concept behind AKTLR comes from the empirically validated acoustic knowledge about the acoustic parameter feature sets designed for describing speech signals and their cross-corpus recognition performance evaluation within SER  [31] ,  [11] ,  [32] . These works inform us that selectively minimalistic high-quality acoustic parameters are more capable of exhibiting superior generalization ability to variance in speech emotion corpus.Therefore, selecting these acoustic parameters may enable the transfer subspace learning models to achieve more promising recognition performance in crosscorpus SER tasks compared to directly using larger feature sets comprising comprehensive acoustic parameters. This insight motivates us to introduce an acoustic knowledge-guided dual sparsity constraint mechanism, illustrated in Fig.  1 , to develop AKTLR model for cross-corpus SER. As depicted in Fig.  1 , this mechanism equips the AKTLR model to proficiently discern emotion-discriminative and corpus-invariant features from established acoustic parameter feature sets at both coarsegrained and fine-grained scales. Specifically, it begins by measuring the contribution scores of different acoustic LLDs, and subsequently selects truly contributive derived features from each of LLD groups with high contribution scores.\n\nTo evaluate the effectiveness of AKTLR, we conduct extensive cross-corpus SER experiments using three widely-used speech emotion corpora: EmoDB  [33] , eNTERFACE  [34] , and CASIA  [35] . The experimental results demonstrate that our AKTLR outperforms recent state-of-the-art transfer subspace learning and deep transfer learning-based cross-corpus SER methods, showcasing the effectiveness of incorporating domain-specific knowledge into transfer subspace learning for cross-corpus SER. In summary, this paper makes three primary contributions:\n\n1) We propose AKTLR, a novel transfer subspace learning method inspired by empirically verified acoustic knowledge, making it the first work to propose a domainspecific approach for cross-corpus SER. 2) We introduce an acoustic knowledge-guided dual sparsity constraint mechanism to guide the design of AK-TLR. This mechanism enables AKTLR to effectively seek emotion-discriminative and corpus-invariant features from established acoustic parameter feature sets, operating at two different scales, for cross-corpus SER. 3) We perform extensive cross-corpus SER experiments using three widely-used speech emotion corpora to assess the effectiveness of AKTLR. The experimental results demonstrate the superior performance of AKTLR in addressing the challenge of cross-corpus SER. The subsequent sections of this paper are structured as follows: Section II provides detailed explanations of the proposed AKTLR method. In Section III, we evaluate the performance of the AKTLR method in tackling the challenge of crosscorpus SER. Finally, the paper is concluded in Section IV.",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "Ii. Proposed Method",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Notations",
      "text": "In this section, we will provide a detailed description of the proposed AKTLR model and demonsrtate how to utilize this model to address cross-corpus SER tasks. Before delving into the model specifics, let us establish a set of notations necessary for constructing the model. Suppose we have a source speech emotion corpus comprising N s samples, with its feature matrix denoted as\n\nHere, N s represents the dimension of acoustic parameter feature vector and d represents the feature dimension. X s i ∈ R di×Ns represents the features derived from a LLD group (one specific LLD or more closely-related LLDs), such as MFCC or energy-based features, within G LLD groups used to design acoustic parameter features for describing speech signals. The corresponding emotion label matrix of the source speech samples is expressed as\n\n] T is a one-hot vector associated with the j th speech sample. The k th entry in y s j is set as 1 if the corresponding speech sample expresses the k th emotion within emotion set {1, • • • , C}, and 0 otherwise. Similarly, the target speech feature matrix can be denoted as\n\n, where N t is number of samples in the target speech emotion corpus.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Aktlr Model",
      "text": "As previously described and illustrated in Fig.  1 , our AKTLR method is designed based on a simple transfer linear regression model and an acoustic knowledge-guided dual sparsity constraint mechanism. This design enables the model to effectively seek high-quality speech features that are emotion-discriminative and corpus-invariant at two scales within a comprehensive feature set consisting of various acoustic parameters and their derived features. This facilitates the connection of emotions expressed in speech signals from different corpora. To achieve this, we design the following optimization problem for AKTLR:\n\nIn Eq.(  1 ), L tlr and L ds represent the loss functions corresponding to a simple Transfer Linear Regression model and newly designed Acoustic Knowledge-guided Dual Sparsity Constraint Mechanism, respectively. The parameter µ serves as a trade-off parameter that controls the balance between these two functions. It is important to note that P ∈ R C×d is the regression coefficient matrix to be learned in AKTLR, and\n\nT is a contribution score vector and also a model parameter of AKTLR. Each entry in this vector, α i , is a non-negative value and measures the contribution of its corresponding acoustic parameter feature derived from a LLD group, in recognizing emotions across speech corpora. In what follows, we describe the details of the key loss functions in AKTLR.\n\n1) Loss Function for Transfer Linear Regression: The loss function corresponding to transfer linear regression, denoted as L tlr , can be formulated as follows:\n\nwhere\n\nNt is the mean difference between the source and target speech feature vectors associated with the i th LLD group, and λ 1 is the trade-off parameter.\n\nThe loss function L tlr consists of two main terms. The first term, ∥Y s -\n\nF , represents a weighted linear regression function that establishes the relationship between the source speech feature sets and their ground truth emotion labels. Minimizing this term enables the proposed AKTLR to seek a subspace to distinguish different emotions expressed in speech signals. The second term, ∥ G i=1 α i P i ∆x st i ∥ 2 , measures the distribution gap between the source and target speech feature sets in such subspace using the one-order statistical moment, the mean value. Minimizing this term encourages the source and target feature sets to have similar feature distributions in such subspace. Thus, the proposed AKTLR is also applicable to distinguish emotions expressed in target speech signals.\n\n2) Loss Function for Acoustic Knowledge-guided Dual Sparsity Constraint Mechanism: The loss function for the acoustic knowledge-guided dual sparsity constraint mechanism is designed as follows:\n\nHere, τ is the trade-off parameter. This loss function consists of two major terms: the l 1 norm with respect to α and the l 2,1 norm with respect to P i . Minimizing this loss function enforces the proposed AKTLR to learn a nonnegative sparse α and column-sparse P i . The non-negative sparse α allows the AKTLR model to measure the specific contributions of different acoustic parameter features at a coarse-grained scale of LLD group, suppressing the lesscontributive ones, while highlighting highly-contributive ones. Additionally, the column-sparse P i further enhance AKTLR by performing fine-grained feature selection to suppress lowquality acoustic parameter features derived from LLD groups with high contribution scores.\n\n3) Optimization Problem of AKTLR: By incorporating the formulations of the two loss functions as shown in Eqs.(  2 ) and (3) into Eq.(  1 ), we can derive the ultimate optimization problem for training the proposed AKTLR models, which is expressed as follows:\n\nwhere λ 1 , λ 2 = µ, and λ 3 = µ×τ are the trade-off parameters that control the balance among the key terms in the total loss function of AKTLR.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Optimization Of Aktlr",
      "text": "The optimization problem for training AKTLR, as shown in Eq.(  4 ), can be effectively addressed using the alternated direction method (ADM)  [37] . Specifically, the optimal parameters in AKTLR, represented by Pi and α, can be obtained through the following iterative steps:\n\n(1) Fix P i and Update α: In this step, the optimization problem becomes one with respect to α, which can be formulated as follows:\n\nwhere 0 ∈ R C×1 is a vector of all zero values. Then, the optimization problem in Eq.(  5 ) can be rewritten as:\n\nAlgorithm 1 Updating Procedures for Learning the Optimal P in Eq.(  8 ).\n\n(1) Fix P, T, κ, and Minimize L w.r.t. Q: This step is equivalent to solving the following optimization problem:\n\nNote that this optimization problem has a closed-form solution, which can be expressed as:\n\nwhere I is a d-by-d identity matrix.\n\n(2) Fix Q, T, κ, and Minimize L w.r.t. P: In this step, we are required to solve the following optimization problem:\n\nwhich can be reformulated as follows:\n\nAccording to Lemma 4.1 shown in the work of  [36] , the optimal solution to the above optimization problem is\n\nwhere p i , q i , and t i are the i th column of P, Q, and T, respectively.\n\n(3) Update T and κ:\n\nwhere ρ > 1 and κ max is the preset maximal value for κ.\n\n(4) Check Convergence:\n\nwhere ϵ is the machine epsilon value.\n\nSubsequently, let z i = F latten(P i Xi ) (i = {1, • • • , G}) and y = F latten(Y), where F latten(•) is an operation that reshapes a matrix into a vector column by column. We are thus able to further restate the objective function in Eq.(  6 ) as the following formulation:\n\nwhere\n\nIt is apparent that Eq.(  7 ) represents a standard non-negative LASSO problem, and we utilize the SLEP package  [38]  to solve it.\n\n(2) Fix α and Update P i : The optimization problem in this step can be formulated as follows:\n\nwhere\n\nWe use the inexact augmented Lagrangian multiplier (IALM)  [39]  to learn the optimal P i .\n\nTo be specific, an additional variable, Q satisifying P = Q, is introduced to first convert the original unconstrained optimization problem in Eq.(  8 ) to a constrained one, which can be expressed as follows:\n\nSubsequently, we are able to obtain the Lagrangian function for Eq.(  9 ), which is formulated as follows:\n\nwhere T is the Lagrangian multiplier matrix, T r(•) represents the trace of a square matrix, and κ is a relaxation factor. Finally, the optimal solution of P can be obtained by iteratively minimizing the Lagrangian function in Eq.(  10 ) with respect to one of variables while fixing the others. The detailed updating procedures are summarized in Algorithm 1.\n\n(3) Check Convergence: the value of objective function is less than the machine epsilon value ϵ or that the iteration reaches the preset maximal number.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "D. Prediction Of Emotion Labels For Target Speech Signals",
      "text": "Once we have obtained the optimal solution, Pi and α, for AKTLR, we can easily predict the emotion labels of the target speech signals. Let\n\n] T be the feature vector of a target speech sample. We first predict its emotion label vector ŷt by solving the following optimization problem:\n\nThis is a standard quadratic programming problem and can be effectively solved using the interior point method. Then, based on ŷt , the emotion label of its corresponding target speech signal can be determined as the j th emotion, which satisfies the following criterion:\n\nwhere ŷt (j) represents the j th entry in the predcted emotion label vector ŷt .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Iii. Experiments A. Experiment Setup",
      "text": "In this section, we evaluate the performance of the proposed AKTLR method through extensive cross-corpus SER experiments. We provide details of our experiment setup, including: 1) Speech Emotion Corpora, 2) Experimental Protocol, 3) Performance Metric, and 4) Comparison Methods and Implementation Details.\n\n1) Speech Emotion Corpora: We utilize three publicly available speech emotion corpora in our experiments. Here is a brief overview of these corpora:\n\nEmoDB  [33] : This German speech emotion corpus consists of 535 speech samples. Each sample corresponds to a sentence uttered in German under one of seven emotional states (Anger, Boredom, Disgust, Fear, Happiness, Neutral, and Sadness) by one of 10 professional German actresses/actors (five actresses and five actors).\n\neNTERFACE  [34] : Unlike EmoDB, eNTERFACE is a bimodal emotion database containing 1,257 video clips with both speech and facial expressions. Each video clip is labeled with one of six basic emotions (Anger, Disgust, Fear, Happiness, Sadness, and Surprise). For the design of our cross-corpus SER tasks, only the speech data is used.\n\nCASIA  [35] : This is a large-scale Chinese speech emotion corpus comprising 9,600 speech samples. In our experiments, we utilize its freely released version, which includes 1,200 speech samples from four speakers (two females and two males), with each speech sample conveying one of six different emotions (Anger, Fear, Happiness, Neutral, Sadness, and Surprise).\n\n2) Experimental Protocol: We used the aforementioned three speech emotion corpora to create six cross-corpus SER tasks:\n\nHere, B, E, and C represent EmoDB, eNTERFACE, and CASIA, respectively. The corpora listed on either side of the arrow indicate the source and target speech emotion corpora in their respective cross-corpus SER tasks. It is important to note that due to inconsistencies in emotion labels across the three speech emotion corpora, only speech samples with matching emotion labels are chosen for their corresponding tasks. For a more comprehensive understanding of these crosscorpus SER tasks, detailed data composition for all the speech emotion corpora is presented in Table  I .\n\n3) Performance Metric: We have chosen the unweighted average recall (UAR)  [10]  as the performance metric for our experiments. UAR is computed by averaging the accuracy across the total number of emotion classes. It is calculated using the formula\n\nHere, C is the number of total emotion classes involved in the cross-corpus SER task, and N p i and N g i represent the number of samples predicted as the i th emotion and the actual number of i th emotion samples, respectively.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "4) Comparison Methods And Implementation Details:",
      "text": "To highlight the effectiveness and superior performance of our AKTLR method in addressing the challenge of cross-corpus SER, we compare it with five recent state-of-the-art (SOTA) Transfer Subspace Learning methods and six SOTA Deep Transfer Learning methods. The methods included in the comparison and their implementation details are as follows:\n\nTransfer Subspace Learning Methods include transfer component analysis (TCA)  [40] , geodesic flow kernel (GFK)  [41] , subspace alignment (SA)  [42] , domain-adaptive subspace learning (DoSL)  [43] , and joint distribution adaptive regression (JDAR)  [44] . In these methods, two widely-used acoustic parameter feature sets, namely INTERSPEECH 2009 Emotion Challenge (IS09)  [45]  and the extended Geneva minimalistic acoustic parameter set (eGeMAPS)  [32] , are utilized to describe speech signals. Both feature sets consist of low-level descriptors (LLDs) such as F0 and MFCC through typical statistical functions. The openSMILE toolkit  [46]  is used to extract these feature sets from the speech signals. For the experiments, linear support vector machine (SVM)  [47]  is used as the classifier for all subspace learning methods without classification ability, including TCA, GFK, and SA. Additionally, the results of directly using SVM to conduct all cross-corpus SER experiments are included as the baseline.\n\nSince emotion label information is unavailable in the tasks of cross-corpus SER, we follow the tradition of transfer learning evaluation. Therefore, we report the best results of the five transfer subspace learning methods by searching their hyperparameters from a given interval. Specifically, TCA, GFK, and SA aim to learn a d-dimensional common subspace for both source and target speech samples, where d is set within a predetermined parameter interval, [1 : d max ], and d max represents the number of elements in the acoustic parameter set used in the experiments. DoSL and JDAR require setting two trade-off parameters, λ and µ, which control the balance between the sparsity and feature distribution elimination terms and the original regression loss function. In the experiments, λ and µ are determined by searching within the range of  [1 : 100] .\n\nDeep Transfer Learning Methods including deep adaptation network (DAN)  [48] , joint adaptation network (JAN)  [49] , deep subdomain adaptation network (DSAN)  [50] , domainadversarial neural network (DANN)  [51] , conditional domain adversarial network (CDAN)  [52] , and DIDAN  [26] , are utilized in the comparison experiments. The speech signals are first tranformed into the Mel-spectrograms and then resized to 224 × 224 pixels, serving as the input for deep neural networks. In this comparison, VGG-11  [53]  is chosen as the CNN backbone of all the deep transfer learning methods, and its experimental results are included as the baseline. The optimizer, learning rate, weight decay, and batch size are set as SGD, 1e -2 , 5e -4 , and 32, respectively, for the VGG-11 and comparison deep transfer learning methods. The trade-off parameter settings for all deep transfer learning methods are as follows:\n\nDAN, JAN, DSAN, DANN, and CDAN have a trade-off parameter λ in their loss functions, which balances the original loss function and the feature distribution alleviation term. In the experiments, λ is searched within the parameter interval [0.0001 : 0.0001 : 0.001, 0.002 : 0.001 : 0.01, 0.02 : 0.01 : 0.1, 0.2 : 0.1 : 1, 2, 5, 10, 100]. Besides λ, DIDAN has an additional trade-off parameter, α, which controls the sparsity of its learned reconstruction coefficient matrix. For DIDAN, λ and α are also searched within the same intervals as the other five deep transfer learning methods: [0.0001 : 0.0001 :",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Feature Set",
      "text": "LLD Groups IS09 ZCR  (12) , ∆ZCR (12), F0 (12), ∆F0 (12), RMS Energy  (12) , ∆RMS Energy (12), HNR  (12) , ∆HNR\n\n, MFCC (144), ∆MFCC (144) eGeMAPS F0  (18) , Loudness (  16 ), Spectral Flux (5), Formant  (18) , Hammarberg Index (3), MFCC  (16) , Spectral Slope (\n\n0.001, 0.002 : 0.001 : 0.01, 0.02 : 0.01 : 0.1, 0.2 : 0.1 : 1, 2, 5, 10, 100].\n\nOur AKTLR has three trade-off parameters: λ 1 and λ 3 . In our experments, we conduct a search for λ 1 and λ 3 in the parameter interval of [1 : 100], while λ 2 is searched winthin the range of [0.1 : 0.1 : 1]. Additionally, we divide both IS09 and eGeMAPS feature sets into 10 LLD groups based on the acoustic parameter type. For further details, please refer to Table  II .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "B. Comparison With State-Of-The-Art Cross-Corpus Ser Methods",
      "text": "The experimental results for all transfer learning methods are presented in Table  III . Several noteworthy observations can be made from this table : \n(1) It is evident from Table  III  that both transfer subspace learning and deep transfer learning methods exhibit promising performance improvements compared to their respective baseline methods (SVM or VGG-11) in all six cross-corpus SER tasks. Particularly interesting is the consistent enhancement observed in transfer subspace methods, regardless of the choice of acoustic parameter feature sets (IS09 or eGeMAPS) used to describe speech signals. In summary, our experimental results strongly indicate the potential of transfer learning as a promising approach to effectively address the challenge of cross-corpus SER.\n\n(2) The performance comparison of transfer subspace learning methods using the IS09 (16 LLDs yielding 384 features) and eGeMAPS feature sets (five meticulously chosen LLDs yielding 88 features) reveals that the eGeMAPS feature set significantly improves cross-corpus SER performance compared to IS09. This finding underscores the importance of selecting minimalistic high-quality acoustic parameters capable of exhibiting superior generalization ability to corpus invariance when employing transfer subspace learning methods to address cross-corpus SER tasks. Our results provide additional experimental evidence to support this established knowledge in SER  [31] ,  [11] ,  [32] , which motivates the design of our AKTLR method.\n\n(3) As shown in the table, our AKTLR, utilizing the eGeMAPS feature set, achieves the highest UAR among all transfer learning methods, averaging a UAR of 42.12% across the six cross-corpus SER tasks. Furthermore, our AKTLR outperforms all other methods in two out of the six tasks, namely B → C and C → B. While AKTLR may not achieve the best performance in the remaining four tasks, it still demonstrates a very competitive performance compared to all other transfer learning methods. In summary, these observations highlight the superior performance of our AKTLR method in addressing the challenge of cross-corpus SER, surpassing both recent SOTA transfer subspace learning and deep transfer learning methods. This also demonstrates the feasibility and superiority of incorporating acoustic knowledge to develop a domainspecific cross-corpus SER approach for dealing with crosscorpus SER tasks.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "C. A Deeper Look At The Proposed Aktlr Method",
      "text": "This section aims to provide a comprehensive understanding of the proposed AKTLR method. We will address three key",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "#Lld Groups Details Of Lld Groups",
      "text": "4 Groups Frequency  (30) , Energy  (20) , Spectral  (37) , Equivalent Sound Level  (1)  13 Groups F0  (10) , Jitter (2), Formant  (18) , Spectral Slope  (6) , MFCC  (16) , Alpha Ratio (3), Shimmer (2), Hammarberg (3), HNR (2), Harmonic Difference (4), Spectral Flux (5), MFCC  (16) , Londness  (16) , Equivalent Sound Level  (1)  questions to delve into AKTLR: 1) Does AKTLR truly benefit from the incorporation of the selected acoustic knowledge?\n\n2) What can AKTLR learn guided by the selected acoustic knowledge? 3) How does the performance of AKTLR vary with changes in the trade-off parameter?. To answer these questions, we will conduct additional cross-corpus SER experiments using AKTLR, with the aim of offering comprehensive insights into its effectiveness and advantages.   Firstly, it is evident that our AKTLR models, which adopt different LLD group settings, achieve better performance in terms of UAR compared to AKTLR without setting LLD groups. This observation demonstrates the feasibility and superiority of the concept behind our proposed AKTLR, i.e., \"selecting these acoustic parameters may enable the transfer subspace learning models to achieve more promising recognition performance in cross-corpus SER tasks compared to directly using larger feature sets comprising comprehensive acoustic parameters\". Guided by this acoustic knowledge, AKTLR divides the acoustic parameter feature set into different LLD groups and measures their contribution scores, ensuring the learning of both emotion-discriminative and corpus-invariant features.\n\nSecondly, it is worth noting that the AKTLR models with 10 and 13 groups perform worse than AKTLR without without setting LLD groups in the task of E → C. We believe that this is mainly due to the use of an excessive LLD groups in these cases. By comparing the different groups used for various cross-corpus SER tasks, it becomes apparent that the overall performance of AKTLR decreases with an increase in the number of groups. This supports our previous supposition.\n\nIn other words, determining a suitable LLD group setting remains an open question for our AKTLR method in tackling the challenge of cross-corpus SER.\n\n2) What Can AKTLR Learn When Guided by the Selected Acoustic Knowledge?: Our proposed AKTLR benefits from the incorporation of established acoustic knowledge into its design. By dividing the acoustic parameter feature set into different LLD groups and measuring their contribution scores, AKTLR model is more capable of seeking a minimalistic highquality features that are emotion-discriminative features and corpus-invariant. This approach inspires us to explore what AKTLR can learn when guided by the utilization of acoustic knowledge. To this end, we present a set of bar charts in Fig.  2 , illustrating the α i values learned by AKTLR when utilizing the eGeMAPS feature set with different LLD groups to address three representative cross-corpus SER experiments in Table  V .\n\nThe findings from Fig.  2  are quite intriguing. Firstly, it is evident that different LLD groups exhibit varying contributions when addressing cross-corpus SER tasks. Specifically, in five out of the nine cross-corpus SER experiments, certain acoustic parameters (corresponding to 0-valued α i ) show negligible contribution in distinguishing emotions across speech corpora. These observations provide experimental evidence that supports selected acoustic knowledge guiding the design of the proposed AKTLR  [31] ,  [11] ,  [32] . This implies that selecting minimalistic high-quality acoustic parameters is necessary and sufficient for dealing with the cross-corpus SER tasks.\n\nSecondly, upon further examination of the contributive LLD groups, it becomes apparent that the contributions of several acoustic parameters vary across different cross-corpus SER tasks, exhibiting high scores in some tasks while low scores in others. This suggests that there are no consistently highlycontributive acoustic parameters for all the cross-corpus SER tasks. However, it is interesting to note the presence of several \"stable\" (varied but consistently contributive) emotiondiscriminative and corpus-invariant acoustic parameters, such as MFCC, which consistently exhibit a satisfactory learned score. This insight inspires us to consider the possibility of testing and selecting acoustic parameters to develop a general minimalistic acoustic parameter feature set consisting of highquality elements that are consistently emotion-discriminative and corpus-invariant. Such a set could potentially enhance the performance of transfer learning methods in addressing the challenge of cross-corpus SER.\n\n3) How Trade-off Parameters Affect the Performance of AKTLR?: In Eq.(  4 ), our AKTLR requires to set three trade-off parameters: λ 1 , λ 2 , and λ 3 . This raises the question of how the choice of these trade-off parameters affect the performance of AKTLR in addressing the challenge of cross-corpus SER. The results are illustrated in Figure  3 . From this figure, it is evident that the performance of our AKTLR varies slightly with respect to the choice of λ 1 and λ 2 across all three crosscorpus SER tasks. However, in the case of λ 3 , although the performance of AKTLR appears to be sensitive to changes in its value, AKTLR consistently performs within an acceptable range around the fixed value used in the experiments. In summary, we can conclude that the performance of our AKTLR is generally less sensitive to the choice of its associated trade-off parameters.\n\nIV. CONCLUSION In this paper, we have addressed the challenge of crosscorpus SER from a new perspective by introducing a novel transfer subspace learning method called AKTLR. The primary contribution of AKTLR lies in its acoustic knowledgeguided dual sparsity constraint mechanism, which enables more effective learning of emotion-discriminative and corpusinvariant features at two different scales: acoustic parameter and feature. Compared with existing transfer subspace learning-based cross-corpus SER methods, AKTLR is the first domain-specific approach designed specifically under the guidance of established acoustic knowledge for cross-corpus SER. To evaluate the effectiveness of AKTLR, we conduct extensive cross-corpus SER experiments using three widelyused speech emotion corpora. The results demonstrate that AKTLR outperforms current SOTA transfer subspace learning and deep transfer learning-based cross-corpus SER methods. This confirms the efficacy and feasibility of leveraging acoustic knowledge to develop domain-specific transfer learning methods for cross-corpus SER.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Acoustic Knowledge-Guided Dual Sparsity Constraint Mechanism:",
      "page": 2
    },
    {
      "caption": "Figure 1: , to develop",
      "page": 2
    },
    {
      "caption": "Figure 2: The bar charts for the learned α by AKTLR, depicting the specific contributions of their corresponding acoustic parameter derived features for",
      "page": 8
    },
    {
      "caption": "Figure 3: The experimental results of trade-off parameter sensitivity analysis for our proposed AKTLR in addressing the tasks of cross-corpus SER, where (a),",
      "page": 9
    },
    {
      "caption": "Figure 2: are quite intriguing. Firstly, it is",
      "page": 9
    },
    {
      "caption": "Figure 3: From this figure, it",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Sample Number": "",
          "Anger": "Fear",
          "127": "69",
          "211": "211",
          "200": "200"
        },
        {
          "Sample Number": "",
          "Anger": "Disgust",
          "127": "-",
          "211": "-",
          "200": "-"
        },
        {
          "Sample Number": "",
          "Anger": "Happiness",
          "127": "71",
          "211": "208",
          "200": "200"
        },
        {
          "Sample Number": "",
          "Anger": "Neutral",
          "127": "79",
          "211": "-",
          "200": "-"
        },
        {
          "Sample Number": "",
          "Anger": "Sadness",
          "127": "62",
          "211": "211",
          "200": "200"
        },
        {
          "Sample Number": "",
          "Anger": "Surprise",
          "127": "-",
          "211": "211",
          "200": "200"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "IS09": "eGeMAPS",
          "ZCR (12), ∆ZCR (12), F0 (12), ∆F0 (12),\nRMS Energy (12), ∆RMS Energy (12), HNR (12),\n∆HNR (12), MFCC (144), ∆MFCC (144)": "F0 (18), Loudness (16), Spectral Flux (5),\nFormant\n(18), Hammarberg Index (3), MFCC (16),\nSpectral Slope (6), Alpha Ratio (3), HNR (2),\nEquivalent Sound Level\n(1)"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Subspace Learning\n(IS09 Feature Set)": "Subspace Learning\n(eGeMAPS Feautre Set)",
          "SVM\nTCA\nGFK\nSA\nDoSL\nJDAR": "SVM\nTCA\nGFK\nSA\nDoSL\nJDAR",
          "28.93\n30.73\n32.40\n33.50\n36.29\n37.10": "25.65\n31.09\n30.08\n32.18\n30.81\n31.41",
          "23.58\n45.16\n45.42\n45.78\n39.84\n40.78": "32.58\n37.43\n35.79\n39.37\n40.71\n45.19",
          "29.60\n33.40\n35.60\n36.90\n34.60\n33.10": "33.50\n42.90\n40.00\n38.80\n39.30\n42.30",
          "35.01\n45.82\n51.19\n48.48\n46.14\n47.34": "51.84\n53.43\n50.79\n53.20\n52.21\n56.14",
          "26.10\n31.80\n32.90\n32.80\n30.90\n32.40": "36.40\n41.10\n39.20\n37.00\n39.10\n38.40",
          "25.14\n34.12\n29.54\n32.71\n31.69\n31.50": "34.79\n35.90\n34.48\n35.43\n34.27\n33.62",
          "28.06\n36.84\n37.84\n38.36\n36.58\n37.04": "35.96\n40.31\n38.39\n39.33\n39.40\n41.18"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4 Groups": "13 Groups",
          "Frequency (30), Energy (20),\nSpectral\n(37), Equivalent Sound Level\n(1)": "F0 (10), Jitter\n(2), Formant\n(18), Spectral Slope (6),\nMFCC (16), Alpha Ratio (3), Shimmer\n(2),\nHammarberg (3), HNR (2), Harmonic Difference (4),\nSpectral Flux (5), MFCC (16), Londness (16),\nEquivalent Sound Level\n(1)"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Speech emotion recognition: Emotional models, databases, features, preprocessing methods, supporting modalities, and classifiers",
      "authors": [
        "M Akc",
        "K Oguz"
      ],
      "year": "2020",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "2",
      "title": "A systematic literature review of speech emotion recognition approaches",
      "authors": [
        "Y Singh",
        "S Goel"
      ],
      "year": "2022",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "3",
      "title": "Dawn of the transformer era in speech emotion recognition: closing the valence gap",
      "authors": [
        "J Wagner",
        "A Triantafyllopoulos",
        "H Wierstorf",
        "M Schmitt",
        "F Burkhardt",
        "F Eyben",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "4",
      "title": "Speech emotion recognition using hidden markov models",
      "authors": [
        "T Nwe",
        "S Foo",
        "L Silva"
      ],
      "year": "2003",
      "venue": "Speech communication"
    },
    {
      "citation_id": "5",
      "title": "Speech emotion recognition using cnn",
      "authors": [
        "Z Huang",
        "M Dong",
        "Q Mao",
        "Y Zhan"
      ],
      "year": "2014",
      "venue": "Proceedings of the 22nd ACM international conference on Multimedia"
    },
    {
      "citation_id": "6",
      "title": "Evaluating deep learning architectures for speech emotion recognition",
      "authors": [
        "H Fayek",
        "M Lech",
        "L Cavedon"
      ],
      "year": "2017",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "7",
      "title": "Speech emotion recognition via an attentive time-frequency neural network",
      "authors": [
        "C Lu",
        "W Zheng",
        "H Lian",
        "Y Zong",
        "C Tang",
        "S Li",
        "Y Zhao"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Computational Social Systems"
    },
    {
      "citation_id": "8",
      "title": "Domain invariant feature learning for speaker-independent speech emotion recognition",
      "authors": [
        "C Lu",
        "Y Zong",
        "W Zheng",
        "Y Li",
        "C Tang",
        "B Schuller"
      ],
      "year": "2022",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "9",
      "title": "Spontaneous speech emotion recognition using multiscale deep convolutional lstm",
      "authors": [
        "S Zhang",
        "X Zhao",
        "Q Tian"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "10",
      "title": "Cross-corpus acoustic emotion recognition: Variances and strategies",
      "authors": [
        "B Schuller",
        "B Vlasenko",
        "F Eyben",
        "M Wöllmer",
        "A Stuhlsatz",
        "A Wendemuth",
        "G Rigoll"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "11",
      "title": "A cross-corpus experiment in speech emotion recognition",
      "authors": [
        "C Parlak",
        "B Diri",
        "F Gürgen"
      ],
      "year": "2014",
      "venue": "SLAM@ INTERSPEECH"
    },
    {
      "citation_id": "12",
      "title": "A survey on transfer learning",
      "authors": [
        "S Pan",
        "Q Yang"
      ],
      "year": "2009",
      "venue": "IEEE Transactions on knowledge and data engineering"
    },
    {
      "citation_id": "13",
      "title": "A decade survey of transfer learning",
      "authors": [
        "S Niu",
        "Y Liu",
        "J Wang",
        "H Song"
      ],
      "year": "2010",
      "venue": "A decade survey of transfer learning"
    },
    {
      "citation_id": "14",
      "title": "On acoustic emotion recognition: compensating for covariate shift",
      "authors": [
        "A Hassan",
        "R Damper",
        "M Niranjan"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "15",
      "title": "Support-vector networks",
      "authors": [
        "C Cortes",
        "V Vapnik"
      ],
      "year": "1995",
      "venue": "Machine learning"
    },
    {
      "citation_id": "16",
      "title": "Covariate shift by kernel mean matching",
      "authors": [
        "A Gretton",
        "A Smola",
        "J Huang",
        "M Schmittfull",
        "K Borgwardt",
        "B Schölkopf"
      ],
      "year": "2009",
      "venue": "Dataset shift in machine learning"
    },
    {
      "citation_id": "17",
      "title": "A least-squares approach to direct importance estimation",
      "authors": [
        "T Kanamori",
        "S Hido",
        "M Sugiyama"
      ],
      "year": "2009",
      "venue": "The Journal of Machine Learning Research"
    },
    {
      "citation_id": "18",
      "title": "Direct density ratio estimation for large-scale covariate shift adaptation",
      "authors": [
        "Y Tsuboi",
        "H Kashima",
        "S Hido",
        "S Bickel",
        "M Sugiyama"
      ],
      "year": "2009",
      "venue": "Journal of Information Processing"
    },
    {
      "citation_id": "19",
      "title": "Crosscorpus speech emotion recognition based on transfer non-negative matrix factorization",
      "authors": [
        "P Song",
        "W Zheng",
        "S Ou",
        "X Zhang",
        "Y Jin",
        "J Liu",
        "Y Yu"
      ],
      "year": "2016",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "20",
      "title": "Integrating structured biological data by kernel maximum mean discrepancy",
      "authors": [
        "K Borgwardt",
        "A Gretton",
        "M Rasch",
        "H.-P Kriegel",
        "B Schölkopf",
        "A Smola"
      ],
      "year": "2006",
      "venue": "Bioinformatics"
    },
    {
      "citation_id": "21",
      "title": "Nonnegative matrix factorization based transfer subspace learning for cross-corpus speech emotion recognition",
      "authors": [
        "H Luo",
        "J Han"
      ],
      "year": "2020",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "22",
      "title": "Analysis of deep learning architectures for cross-corpus speech emotion recognition",
      "authors": [
        "J Parry",
        "D Palaz",
        "G Clarke",
        "P Lecomte",
        "R Mead",
        "M Berger",
        "G Hofer"
      ],
      "year": "2019",
      "venue": "Interspeech"
    },
    {
      "citation_id": "23",
      "title": "Imagenet classification with deep convolutional neural networks",
      "authors": [
        "A Krizhevsky",
        "I Sutskever",
        "G Hinton"
      ],
      "year": "2012",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "24",
      "title": "Long short-term memory",
      "authors": [
        "S Hochreiter",
        "J Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural computation"
    },
    {
      "citation_id": "25",
      "title": "Deep transductive transfer regression network for cross-corpus speech emotion recognition",
      "authors": [
        "Y Zhao",
        "J Wang",
        "R Ye",
        "Y Zong",
        "W Zheng",
        "L Zhao"
      ],
      "year": "2022",
      "venue": "Proceedings of the INTERSPEECH"
    },
    {
      "citation_id": "26",
      "title": "Deep implicit distribution alignment networks for cross-corpus speech emotion recognition",
      "authors": [
        "Y Zhao",
        "J Wang",
        "Y Zong",
        "W Zheng",
        "H Lian",
        "L Zhao"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "27",
      "title": "Improving cross-corpus speech emotion recognition with adversarial discriminative domain generalization (addog)",
      "authors": [
        "J Gideon",
        "M Mcinnis",
        "E Provost"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "28",
      "title": "Domain-invariant feature learning for cross corpus speech emotion recognition",
      "authors": [
        "Y Gao",
        "S Okada",
        "L Wang",
        "J Liu",
        "J Dang"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "29",
      "title": "Adversarial domain generalized transformer for cross-corpus speech emotion recognition",
      "authors": [
        "Y Gao",
        "L Wang",
        "J Liu",
        "J Dang",
        "S Okada"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "30",
      "title": "No free lunch theorems for optimization",
      "authors": [
        "D Wolpert",
        "W Macready"
      ],
      "year": "1997",
      "venue": "IEEE transactions on evolutionary computation"
    },
    {
      "citation_id": "31",
      "title": "Emotions and speech: Some acoustical correlates",
      "authors": [
        "C Williams",
        "K Stevens"
      ],
      "year": "1972",
      "venue": "The journal of the acoustical society of America"
    },
    {
      "citation_id": "32",
      "title": "The geneva minimalistic acoustic parameter set (gemaps) for voice research and affective computing",
      "authors": [
        "F Eyben",
        "K Scherer",
        "B Schuller",
        "J Sundberg",
        "E André",
        "C Busso",
        "L Devillers",
        "J Epps",
        "P Laukka",
        "S Narayanan"
      ],
      "year": "2015",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "33",
      "title": "A database of german emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "Interspeech"
    },
    {
      "citation_id": "34",
      "title": "The enterface'05 audiovisual emotion database",
      "authors": [
        "O Martin",
        "I Kotsia",
        "B Macq",
        "I Pitas"
      ],
      "year": "2006",
      "venue": "22nd International Conference on Data Engineering Workshops (ICDEW'06)"
    },
    {
      "citation_id": "35",
      "title": "Design of speech corpus for mandarin text to speech",
      "authors": [
        "J Zhang",
        "H Jia"
      ],
      "year": "2008",
      "venue": "The blizzard challenge 2008 workshop"
    },
    {
      "citation_id": "36",
      "title": "Robust recovery of subspace structures by low-rank representation",
      "authors": [
        "G Liu",
        "Z Lin",
        "S Yan",
        "J Sun",
        "Y Yu",
        "Y Ma"
      ],
      "year": "2012",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "37",
      "title": "Multi-view facial expression recognition based on group sparse reduced-rank regression",
      "authors": [
        "W Zheng"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "38",
      "title": "Slep: Sparse learning with efficient projections",
      "authors": [
        "J Liu",
        "S Ji",
        "J Ye"
      ],
      "year": "2009",
      "venue": "Slep: Sparse learning with efficient projections"
    },
    {
      "citation_id": "39",
      "title": "The augmented lagrange multiplier method for exact recovery of corrupted low-rank matrices",
      "authors": [
        "Z Lin",
        "M Chen",
        "Y Ma"
      ],
      "year": "2010",
      "venue": "The augmented lagrange multiplier method for exact recovery of corrupted low-rank matrices",
      "arxiv": "arXiv:1009.5055"
    },
    {
      "citation_id": "40",
      "title": "Domain adaptation via transfer component analysis",
      "authors": [
        "S Pan",
        "I Tsang",
        "J Kwok",
        "Q Yang"
      ],
      "year": "2010",
      "venue": "IEEE transactions on neural networks"
    },
    {
      "citation_id": "41",
      "title": "Geodesic flow kernel for unsupervised domain adaptation",
      "authors": [
        "B Gong",
        "Y Shi",
        "F Sha",
        "K Grauman"
      ],
      "year": "2012",
      "venue": "Geodesic flow kernel for unsupervised domain adaptation"
    },
    {
      "citation_id": "42",
      "title": "Unsupervised visual domain adaptation using subspace alignment",
      "authors": [
        "B Fernando",
        "A Habrard",
        "M Sebban",
        "T Tuytelaars"
      ],
      "year": "2013",
      "venue": "Proceedings of the IEEE international conference on computer vision"
    },
    {
      "citation_id": "43",
      "title": "Unsupervised cross-corpus speech emotion recognition using domainadaptive subspace learning",
      "authors": [
        "N Liu",
        "Y Zong",
        "B Zhang",
        "L Liu",
        "J Chen",
        "G Zhao",
        "J Zhu"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "44",
      "title": "Cross-corpus speech emotion recognition using joint distribution adaptive regression",
      "authors": [
        "J Zhang",
        "L Jiang",
        "Y Zong",
        "W Zheng",
        "L Zhao"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "45",
      "title": "The interspeech 2009 emotion challenge",
      "authors": [
        "B Schuller",
        "S Steidl",
        "A Batliner"
      ],
      "year": "2009",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "46",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "47",
      "title": "Libsvm: a library for support vector machines",
      "authors": [
        "C.-C Chang",
        "C.-J Lin"
      ],
      "year": "2011",
      "venue": "ACM transactions on intelligent systems and technology (TIST)"
    },
    {
      "citation_id": "48",
      "title": "Learning transferable features with deep adaptation networks",
      "authors": [
        "M Long",
        "Y Cao",
        "J Wang",
        "M Jordan"
      ],
      "year": "2015",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "49",
      "title": "Deep transfer learning with joint adaptation networks",
      "authors": [
        "M Long",
        "H Zhu",
        "J Wang",
        "M Jordan"
      ],
      "year": "2017",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "50",
      "title": "Deep subdomain adaptation network for image classification",
      "authors": [
        "Y Zhu",
        "F Zhuang",
        "J Wang",
        "G Ke",
        "J Chen",
        "J Bian",
        "H Xiong",
        "Q He"
      ],
      "year": "2020",
      "venue": "IEEE transactions on neural networks and learning systems"
    },
    {
      "citation_id": "51",
      "title": "Domain-adversarial neural networks",
      "authors": [
        "H Ajakan",
        "P Germain",
        "H Larochelle",
        "F Laviolette",
        "M Marchand"
      ],
      "year": "2014",
      "venue": "Domain-adversarial neural networks",
      "arxiv": "arXiv:1412.4446"
    },
    {
      "citation_id": "52",
      "title": "Conditional adversarial domain adaptation",
      "authors": [
        "M Long",
        "Z Cao",
        "J Wang",
        "M Jordan"
      ],
      "year": "2018",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "53",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for large-scale image recognition",
      "arxiv": "arXiv:1409.1556"
    }
  ]
}