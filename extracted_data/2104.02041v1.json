{
  "paper_id": "2104.02041v1",
  "title": "Exploring Transformers In Emotion Recognition: A Comparison Of Bert, Distillbert, Roberta, Xlnet And Electra",
  "published": "2021-04-05T17:46:10Z",
  "authors": [
    "Diogo Cortiz"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This paper investigates how Natural Language Understanding (NLU) could be applied in Emotion Recognition, a specific task in affective computing. We finetuned different transformers language models (BERT, DistilBERT, RoBERTa, XLNet, and ELECTRA) using a fine-grained emotion dataset and evaluating them in terms of performance (f1-score) and time to complete.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "One of the main challenges of affective computing involving the Natural Language Understanding (NLU) area is recognizing emotions. Some projects seek to understand emotions through facial expressions, body movement, and blood pressure. In this paper, we will delimit the scope for recognizing emotional expressions in texts.\n\nWe have seen different architectures emerging to deal with challenges in NLU tasks, such as RNNs, LSTM, BiLSTM. More recently, the Transformers architecture  (Vaswani et al., 2017)  has shown to be an appropriate alternative for better results in NLU tasks.\n\nTransformers' language models have shown better language understanding abilities to achieve state-of-the-art results in many different tasks. Our goal is to investigate the performance of different transformer language models for Emotion Recognition when using a dataset of fine-grained emotions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "This section reviews five articles that propose different approaches, methods, or techniques for recognizing emotions in texts. One study  (Polignano et al., 2019)  proposed an approach based on deep neural network. In this case, the researchers demonstrated that an architecture composed of BiLSTM, CNN, and self-attention demonstrated promising results in different datasets. They also took the opportunity to compare three pre-trained word-embeddings (Google word embeddings, GloVe, and FastText) for word encoding.\n\nThe authors argued the model is based on the synergy of two deep learning approaches for classification: long-short-term memory networks (LSTM) in their bi-directional variation (BiLSTM) and the convolutional neural networks (CNN) mediated by approach max pooling. However, the architectural difference includes a self-attention layer after BiLSTM so that the model captures distant relationships between words, with different weights according to their contribution to the classification task.\n\nFor embedding layers, the authors argue that training a word embedding directly on the application domain's \"training data\" can impact the model's generalization ability. If a new sentence is used as an entry for classification, words are missing leaving the task difficult to complete successfully.\n\nTo overcome this challenge, the authors used transfer learning practice, using word embeddings already pre-trained in different domains. With that, they could reduce the computational cost while covering a wider range of terms regardless of domains. They applied GoogleEmb, GloVe, and FastText in different experiments and compared them. FastText has shown very significance for highlighting the better performances on all the three datasets evaluated.\n\nAuthors used three datasets: ISEAR  (Scherer and Wallbott, 1994) , which contains annotation for seven emotions (joy, fear, anger, sadness, disgust, shame, and guilt); SemEval 2018 task 1, which contains annotations only for joy, fear, anger, and sadness; and SemEval 2019 task 3, which contains three specifically annotated emotions: happy, sad, angry and an 'other' class that includes all others not annotated possible emotions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Model",
      "text": "Embeddings Macro-F1 The model's results combining BiLSTM, CNN, and Self-attention demonstrated the effectiveness of this approach compared to baseline models and some other deep neural approaches. The authors also point out that the choice of encoding the textual input through word embedding can influence the behavior and the final result of the model. The results showed that FastText could bring better results to the task of classifying emotional sentences.  Batbaatar et al. (2019)  argued that several previous research pieces adopted word embedding vectors that could represent rich semantic/syntactic information but had difficulty identifying emotional relationships between words. Some word embeddings with emotional information have been proposed, but it also requires semantic and syntactic information. Those models' approaches cannot encode and learn semantic and emotional relationships efficiently in a short text.\n\nTo overcome this challenge, the authors proposed a new neural network architecture (called semantic-emotion neural network -SENN) that combines two sub-networks to capture semantic and emotional information. The first network consists of a BiLSTM to capture semantic information and map it into semantic-sentence space; the second network is a CNN to capture emotional information and map it to an emotionsentence space.\n\nAccording to the authors, CNN is supposedly suitable for extracting position invariant features (emotion information), while BiLSTM is indicated to deal with a semantic sequence in a sentence (contextual information). The combination of the final representations can be helpful for the recognition of emotions. The study used different pre-trained general word embeddings (Word2Vec, GloVE, and FastText) and an emotion-enriched word embedding (EWE).\n\nThe authors explored different available datasets (from distinct domains: social media, fairy tale, etc.). We highlight the ISEAR dataset for this literature review because it is the common dataset used by other papers we discuss. It is important to note that the authors made some kind of transformation in the data that was not very clear in the paper. The original dataset contains 7,666 annotated sentences, but the authors used only 5,241 sentences in the SENN model experiment.\n\nThe results showed that the SENN model performed better than baseline models and other state-of-the-art deep learning models to classify datasets based on Paul Ekman's six categories of emotions (using different datasets). Specifically, for the ISEAR dataset, the performance of the SENN model is highlighted above: Even though GloVe combined with EWE performed slightly better for ISEAR, the authors observed that among all variants of the SENN model, the one that used FastText performed better than Word2Vec and Glove. The authors argue that a possible explanation is that FastText word embeddings capture the meaning of smaller words and allow embeddings to understand suffixes and prefixes, facilitating the processing of rare words and out-of-vocabulary. Based on the results, we may discuss that using a specific embedding for emotions can improve emotional sentence classification tasks.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Model Embeddings Macro-F1",
      "text": "In a recent publication  (Adoma et al., 2020) , there was a comparative analysis of different Transformer-based models (BERT, RoBERTa, DistilBert, and XLNet) in an emotion recognition task, using the ISEAR dataset Authors argued the difficulty of identifying the most appropriate embeddings for extracting long sequence relationships and capturing lightning information in a specific scenario (such as emotion recognition), can be overcome with the Transformers approach and language models based on transformers.\n\nThis position appears to agree with the results of the previously reviewed papers. They noticed that embeddings directly influence the model's behavior and that task-related embeddings (as was the case with emotion-enriched word embedding -EWE) can bring better results.\n\nThe purpose of the paper is to analyze the effectiveness of the BERT, RoBERTa, DistilBERT, and XLNET models in recognizing emotions using the ISEAR dataset. Each model's experiments and results are discussed comparatively according to accuracy, precision, recall, and F1-score for each of the classes of emotions in the dataset.\n\nUnlike the other reviewed articles, the authors detailed the data pre-processing. They removed records in which there was a column of emotions, but the sentence was missing. Thus, the dataset used in the experiment reduced in size, from 7666 sentences to 7589 sentences. The authors also stated that they removed special characters, double spaces, tags, and irregular expressions because they could negatively affect the models' performance. Stop words have also been removed. The dataset was divided into 80% of the data for training and 20% for testing purposes.\n\nAll models were trained using the same hyperparameters. The results showed RoBERTa as the best model (accuracy of 0.743), followed by XLNet (accuracy of 0.729). BERT achieved the third position (accuracy of 0.700) while DistillBERT holds the last position (accuracy of 0.669). In Table  3 , we present the Macro-F1 for each class.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Model",
      "text": "Macro-F1 The authors concluded that RoBERTa pretrained model outperforms the other pre-trained models. They also discuss that even though DistilBERT was the least accurate model, it was the fastest model while XLNet was the slowest. The decreasing order of computational resources is given as XLNet, BERT, RoBERTa, DistilBERT.\n\nFinally, the authors conclude that pre-trained models based on transformers prove to be effective in detecting emotions in texts, with RoBERTa presenting the best metrics for accuracy and macro-f1. This work is important to show the potential use of pre-trained models in specific scenarios (as in recognizing emotions), just finetuning the model.\n\nThe papers reviewed so far reflect a common characteristic in NLP projects that deal with emotions: the assumption that emotions are structured in just six basic emotions. Most datasets available (such as the ISEAR dataset) are structured based on this theoretical position ranging from 5 to 6 classes of emotions (on average).\n\nHowever, there is a debate in affective science, neuroscience, psychology, and philosophy about what an emotion is and how many there are. We can divide the positions into two approaches: basic emotion theory (BET) and constructivism. BET assumes that only a few discrete emotions are separated by clear boundaries and that categories are universal. On the other hand, constructivism argues that emotions are constructions based on valence and arousal, that people put different interpretations for affection from an individual perspective.\n\nThere is an alternative perspective: semantic space theory  (Cowen and Keltner, 2021) . The authors describe this theory as a computational approach that explores broad natural stimuli and open-ended statistical techniques to capture systematic variation in behaviors related to emotions.\n\nThe authors argued that there are more than 25 distinct emotional experience varieties with different backgrounds and expressions. These emotions are highly dimensional, categorical, and can be blended. Semantic space theory is a promising proposal and is arousing interest in research in affective science. In this sense, it is also essential to investigate their possible interaction in emotion recognition projects using NLU approaches.\n\nRecently,  Demszky et al. (2020)  published a new paper where they introduce GoEmotion, a manually annotated dataset of 58 thousand Reddit Comments (in English), labeled for 27 emotions categories (based on semantic space theory).\n\nThe authors argue that in contrast to Ekman's taxonomy, which includes only a positive emotion (joy), this new taxonomy proposal includes a more significant number of positive, negative, and ambiguous emotion categories, which may be suitable for comprehension tasks that require an understanding of emotional expressions in conversations, such as analyzing feedback sent by a customer or interacting with chatbots.\n\nTo create this fine-grained dataset, the authors used a data dump that contains comments from 2005 to January 2019. They selected subreddits with more than 10k comments and applied a series of data curation (which is not specified in the original paper) to ensure the data does not reinforce language bias. They removed subreddits based on \"not safe for work\" public list. They kept vulgar comments because they could include negative emotions. They also used a pilot model (trained with 2.2K annotated examples) to exclude subreddits that consist of more than 30% of neutral comments or less than 20% negative, positive, or ambiguous comments. They also used a pilot model to balance the emotions in the dataset.\n\nThe annotation process consisted of three raters for each example. When no raters agree on at least one emotion label, they assigned two additional raters. All raters were native English speakers from India, but the authors did not detail their background (education, profession, and so on).\n\nThe authors used the dataset to train two models: a baseline model using a BiLSTM and a BERT-base  (Devlin et al., 2018)  for the final experiment. For the BERT model, they added a dense output layer on top of the pre-trained model for finetuning, with a sigmoid cross-entropy loss function to support multi-label classification.\n\nThey kept most of the hyperparameters presented by the original paper. They only change the batch size (to 16) and learning rate (to 5e-5). They also found that training for at least four epochs is indicated for this dataset, but training for more epochs results in overfitting.\n\nThe performance of the best model, BERT, on the test set, achieved an average F1-score of .46 (std=.19) for the full taxonomy (27 emotions). The BiLSTM model performed significantly worse than BERT, obtaining an average F1-score of .41 for the full taxonomy.\n\nAuthors suggest that it is a promising approach integrating affective science and NLU, but there is much room for improvement. In this sense, we investigated the performance of different transformers-based language with this dataset. A similar approach to the one driven by  (Adoma et al., 2020) , but using a fine-grained dataset of emotions",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Experimental Protocol",
      "text": "One of the many challenges of NLU is to identify the best embeddings to extract specific semantic information (for example, emotions) and identify different techniques to deal with long-term dependency. In recent years, we have seen different architectures emerging to deal with these challenges, such as RNNs, LSTM, BiLSTM. More recently, the Transformers architecture  (Vaswani et al., 2017)  has shown to be alternatives in addressing these limitations.\n\nMore recently, transformers' language models have shown better language understanding abilities to achieve state-of-the-art results in many different NLP and NLU tasks.\n\nIt is important to highlight that different implementations of language models have emerged from the original idea of the Transformer architecture. We hypothesize that different models can present different performance (f1-score) and time to complete in the task of recognizing emotions.\n\nBERT was one of the first models based on Transformers to achieve great results in many NLP tasks. However, we assume that more recent Transformers language models could be more efficient than BERT. Thus, our goal is to investigate different transformer language models' behavior in this type of task.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Data",
      "text": "In this research, the dataset we will use is GoEmotion, released by  Demszky et al. (2020) . It is the largest manually annotated dataset of 58 thousand Reddit Comments (in English), labeled for 27 emotion categories (based on semantic space theory) or neutral.\n\nOther annotated datasets are available for emotion recognition tasks, but they mainly rely on basic emotion theory from Paul Ekman, which comprises only six basic emotions. For this reason, we decided to use GoEmotion and investigate how different transformer language models can handle a dataset of fine-grained emotions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Metrics",
      "text": "We will compute three standard metrics to measure the model's performance: precision, recall, and F1score for each class. We will use F1-score (macro) as used in the original paper of GoEmotion  (Demszky et al., 2020)  to compare the performance among all the models. We will also measure the time taken for each model to run to completion on training and evaluation.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Models",
      "text": "In the original paper,  Demszky et al. (2020)  used a BiLSTM as baseline model. The BiLSTM model performed achieved an average F1-score of 0.41. They also used a BERT implementation as the main model.\n\nGoogle released the Bidirectional Encoder Representations from Transformers (BERT) pretrained language model  (Devlin et al., 2018)  in 2018. It is considered the milestone of a rediscovery of the NLP area for improving performance in different natural language tasks. In the GoEmotion original paper, the BERT model performed significantly better than BiLSTM, with an average F1-score of 0.46 against an average F1score of 0.41 from the baseline model.\n\nIn this research, we will assume BERT as our baseline model. Despite the excellent results in different benchmarks, this is a model that has some limitations. Since the release of BERT, different models were proposed to address some BERT limitations. For this reason, we will investigate the performance of four recent transformer language models (DistilBert, RoBERTa, XLNet, and ELECTRA) in the Emotion Recognition task. We will assume BERT as our baseline model.\n\nDistilBERT is a distilled version of BERT, which is smaller, faster, cheaper, and lighter. This model is inspired by the Knowledge distillation approach. It is a compression technique to train a small model to reproduce a larger model's behavior (that is the reason it is also called teacher-student learning). Using this technique reduces the size of a BERT model by 40%, while 97% of its language capabilities are kept. The model is also 60% faster  (Sanh et al., 2019) .\n\nRoBERTa (Robustly optimized BERT pretraining Approach) is a model developed by a Facebook team to improve BERT implementation. The researchers proposed some changes to the original model  (Liu et al., 2019) .\n\nFirst, they used a larger dataset. BERT was trained on a combination of BookCopus plus English Wikipedia text, which totals 16GB of text.\n\nRoBERTa was trained with those two corpora plus three corpora from different domains: CC-News, OpenWeb Text, and Stories, which totals 160GB of text.\n\nThey also proposed improvements to the model design. For the training procedure, instead of using Next Sentence Prediction (NSP) task from BERT's pre-training, they introduced dynamic masking so that the masked token changes during the training epochs. They also trained the model on longer sequences  (Liu et al., 2019) .\n\nXLNet  (Yang et al., 2019 ) is a large bidirectional transformer that uses improved training methodology, larger data, and more computational power. To improve the training, XLNet developers introduced permutation language modeling. In contrast to BERT, which predicts only the masked 15% tokens (Masked tokens), XLNET predicts all tokens but in random order. XLNet was trained with over 130 GB of textual data. In addition to BERT's two datasets, they included three more corpora: Giga5, ClueWeb, and Common Crawl. XLNet outperformed BERT on 20 tasks, such as question answering, natural language inference, sentiment analysis, and so on.\n\nELECTRA is a model which training process does not rely on masked language such as BERT or RoBERTa. Instead of masking the input and predicting it, the ELECTRA approach replaces some tokens with plausible alternatives generated by a small generator network. Then, the model does not predict the original identities of the token replaced, and it trains a discriminative model to predict whether a generator sample replaced tokens in the input. The authors argued that this approach works well at scale. The model had the same performance as RoBERTa and XLNet but using less than ¼ of the computing resources. The model also outperformed them when the same computing amount was available  (Clark et al., 2020) .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Parameters Settings",
      "text": "When finetuning all Pre-trained transformers language model, we keep most of the hyperparameters set by  Devlin et al. (2019)  intact and only change the batch size and learning rate, based on the settings proposed by  Demszky et al. (2020) . We set the learning rate to 5e-5 and batch size to 16. We train the model for four epochs. The threshold to set a result as positive was 0.30. Parameter settings for all models are detailed:\n\n• Learning rate: 5e-5 • Batch Size: 16 • Epochs: 4 • Threshold: 0.30 All models were implemented using the huggingface library. The training process used the same computing environment (Tesla T4 GPU).",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Results",
      "text": "Table  4  summarizes our four models' performance (DistiBERT, RoBERTa, XLNet, and Electra) plus our BERT Baseline based on the original paper of GoEMotion  (Demszky et al., 2020) .\n\nThe model that achieved the highest f1-score (macro-average) was RoBERTa with .49 (std=.23). The model obtained the best result for 14 out of a total of 28 classes. The model did not achieve the worst performance for any class of emotions.\n\nTwo models achieved the second-highest f1score (macro-average) but with different standard deviations. DistilBERT obtained the result of .48 (std = .21) while XLNet obtained the result of .48 (std = .23). The DistilBERT model achieved the best results for six classes of emotions (annoyance, gratitude, joy, realization, optimism, sadness) and the worst result for one class (love). T XLNET model had the best result for nine classes of emotions (anger, approval, caring, confusion, curiosity, fear, gratitude, nervousness, remorse) and two worst results (amusement and pride). It is important to note that only the BERT and DistilBERT models achieved results for the pride class  (.36 and .22, respectively) , while the other models had an f1-score of zero.\n\nELECTRA was the worst model for the GoEmotion dataset. It achieved F1-score of .33 (std=.30). The model achieved the worst performance for 18 out of 28 classes. It is important to highlight the model had an F1-score of .0 for nine classes (caring, desire, disappointment, disgust, embarrassment, excitement, grief, nervousness, pride, realization, and relief).\n\nHowever, along with BERT, ELECTRA achieved the best performance for the neutral class. Table  5  summarizes the time to complete training (4 epochs) and evaluation in the test set. The hyperparameters settings were the same for each model (learning rate and batch size), and they all ran in the same computing environment (Tesla T4 GPU).  XLNET was the model that took the longest to train, followed by RoBERTa, the model that achieved the best F1-score. ELECTRA was the fastest model of all, taking just 13 minutes in the training phase. However, it was the model with the worst performance. DistilBERT appears as an interesting model. It was a quick model to train (compared to the others), achieved the same F1score as larger models (XLNet), and surpassed BERT.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Conclusion",
      "text": "Emotion Recognition in texts is an essential task in the field of affective computing. We investigated different language models' performance when using GoEmotion, a large manually annotated dataset for fine-grained emotion.\n\nWe used BERT as our baseline model and compared it with four additional transformersbased models (DistillBERT, RoBERTa, XLNet, and ELECTRA). Except for the ELECTRA model, which had the worst F1-score (.33), the other models had more similar results. RoBERTa achieved the best F1-score (.49), followed by DistillBERT (.48), XLNet (.48), and then BERT (.46). However, when we look at the metric of computational cost and time to complete, we argue that the DistillBERT model is the most efficient for this type of task.\n\nThe results show us that we still have much room to improve both models and create datasets for fine-grained emotions.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": "dcortiz@pucsp.br"
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": "researchers \ndemonstrated \nthat \nan \narchitecture"
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": "composed  of  BiLSTM,  CNN,  and  self-attention"
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": "demonstrated \npromising \nresults \nin \ndifferent"
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": "datasets.  They \nalso \ntook \nthe \nopportunity \nto"
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": "compare \nthree \npre-trained \nword-embeddings"
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": "(Google word embeddings, GloVe, and FastText)"
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": "for word encoding."
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": "The authors argued the model is based on the"
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": "synergy  of \ntwo  deep \nlearning  approaches \nfor"
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": ""
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": "classification:  long-short-term  memory  networks"
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": ""
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": "(LSTM) in their bi-directional variation (BiLSTM)"
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": ""
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": "and \nthe  convolutional  neural  networks \n(CNN)"
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": ""
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": "mediated by approach max pooling. However, the"
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": ""
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": "architectural  difference  includes  a  self-attention"
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": "layer  after  BiLSTM  so  that  the  model  captures"
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": ""
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": "distant relationships between words, with different"
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": "weights  according \nto \ntheir  contribution \nto \nthe"
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": ""
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": "classification task."
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": ""
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": "For  embedding  layers,  the  authors  argue  that"
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": ""
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": "training \na  word \nembedding \ndirectly \non \nthe"
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": ""
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": "application domain's \"training data\" can impact the"
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": ""
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": "model's generalization ability. If a new sentence is"
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": ""
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": "used  as  an  entry \nfor  classification,  words  are"
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": ""
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": "missing \nleaving \nthe \ntask  difficult \nto  complete"
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": ""
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": "successfully."
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": ""
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": "To  overcome  this  challenge,  the  authors  used"
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": ""
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": "transfer learning practice, using word embeddings"
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": ""
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": "already pre-trained in different domains. With that,"
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": ""
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": "they  could  reduce  the  computational  cost  while"
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": ""
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": "covering  a  wider  range  of \nterms  regardless  of"
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": ""
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": "domains.  They  applied  GoogleEmb,  GloVe,  and"
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": ""
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": "FastText  in  different  experiments  and  compared"
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": ""
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": "them.  FastText  has  shown  very  significance  for"
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": ""
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": "highlighting  the  better  performances  on  all  the"
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": ""
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": "three datasets evaluated."
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": ""
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": "Authors  used  three  datasets:  ISEAR  (Scherer"
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": ""
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": "and Wallbott, 1994), which contains annotation for"
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": "seven emotions (joy, fear, anger, sadness, disgust,"
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": ""
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": "shame,  and  guilt);  SemEval  2018  task  1,  which"
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": "contains annotations only for joy, fear, anger, and"
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": "sadness; and SemEval 2019 task 3, which contains"
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": "three specifically annotated emotions: happy, sad,"
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": "angry  and  an  'other'  class  that  includes  all  others"
        },
        {
          "Pontifical Catholic University of São Paulo (PUC-SP)": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table 1: Comparison of BiLSTM+CNN+Self- 5,241 sentences in the SENN model experiment.",
      "data": [
        {
          "not annotated possible emotions.": "",
          "The \nauthors \nexplored \ndifferent \navailable": "datasets (from distinct domains: social media, fairy"
        },
        {
          "not annotated possible emotions.": "Model \nEmbeddings  Macro-F1",
          "The \nauthors \nexplored \ndifferent \navailable": "tale, etc.). We highlight the ISEAR dataset for this"
        },
        {
          "not annotated possible emotions.": "0.55 \n-",
          "The \nauthors \nexplored \ndifferent \navailable": "literature review because it is the common dataset"
        },
        {
          "not annotated possible emotions.": "SVM",
          "The \nauthors \nexplored \ndifferent \navailable": ""
        },
        {
          "not annotated possible emotions.": "",
          "The \nauthors \nexplored \ndifferent \navailable": "used by other papers we discuss. It is important to"
        },
        {
          "not annotated possible emotions.": "Random Forest \n- \n0.49",
          "The \nauthors \nexplored \ndifferent \navailable": ""
        },
        {
          "not annotated possible emotions.": "",
          "The \nauthors \nexplored \ndifferent \navailable": "note \nthat \nthe \nauthors  made \nsome \nkind \nof"
        },
        {
          "not annotated possible emotions.": "GoobleEmb \n0.62",
          "The \nauthors \nexplored \ndifferent \navailable": ""
        },
        {
          "not annotated possible emotions.": "BiLSTM+CNN+",
          "The \nauthors \nexplored \ndifferent \navailable": "transformation in the data that was not very clear"
        },
        {
          "not annotated possible emotions.": "GloVE \n0.62",
          "The \nauthors \nexplored \ndifferent \navailable": ""
        },
        {
          "not annotated possible emotions.": "Self-Attention",
          "The \nauthors \nexplored \ndifferent \navailable": "in  the  paper.  The  original  dataset  contains  7,666"
        },
        {
          "not annotated possible emotions.": "FastText \n0.63",
          "The \nauthors \nexplored \ndifferent \navailable": ""
        },
        {
          "not annotated possible emotions.": "",
          "The \nauthors \nexplored \ndifferent \navailable": "annotated  sentences,  but  the  authors  used  only"
        },
        {
          "not annotated possible emotions.": "Table 1:  Comparison of BiLSTM+CNN+Self-",
          "The \nauthors \nexplored \ndifferent \navailable": "5,241 sentences in the SENN model experiment."
        },
        {
          "not annotated possible emotions.": "Attention models for ISEAR",
          "The \nauthors \nexplored \ndifferent \navailable": "The \nresults  showed \nthat \nthe  SENN  model"
        },
        {
          "not annotated possible emotions.": "",
          "The \nauthors \nexplored \ndifferent \navailable": "performed  better  than  baseline  models  and  other"
        },
        {
          "not annotated possible emotions.": "The model's results combining BiLSTM, CNN,",
          "The \nauthors \nexplored \ndifferent \navailable": ""
        },
        {
          "not annotated possible emotions.": "",
          "The \nauthors \nexplored \ndifferent \navailable": "state-of-the-art  deep  learning  models  to  classify"
        },
        {
          "not annotated possible emotions.": "and  Self-attention  demonstrated  the  effectiveness",
          "The \nauthors \nexplored \ndifferent \navailable": ""
        },
        {
          "not annotated possible emotions.": "",
          "The \nauthors \nexplored \ndifferent \navailable": "datasets  based  on  Paul Ekman's  six  categories  of"
        },
        {
          "not annotated possible emotions.": "of this approach compared to baseline models and",
          "The \nauthors \nexplored \ndifferent \navailable": ""
        },
        {
          "not annotated possible emotions.": "",
          "The \nauthors \nexplored \ndifferent \navailable": "emotions  (using  different  datasets).  Specifically,"
        },
        {
          "not annotated possible emotions.": "some  other  deep  neural  approaches.  The  authors",
          "The \nauthors \nexplored \ndifferent \navailable": ""
        },
        {
          "not annotated possible emotions.": "",
          "The \nauthors \nexplored \ndifferent \navailable": "for  the  ISEAR  dataset,  the  performance  of  the"
        },
        {
          "not annotated possible emotions.": "also  point  out  that  the  choice  of  encoding  the",
          "The \nauthors \nexplored \ndifferent \navailable": ""
        },
        {
          "not annotated possible emotions.": "",
          "The \nauthors \nexplored \ndifferent \navailable": "SENN model is highlighted above:"
        },
        {
          "not annotated possible emotions.": "textual \ninput \nthrough  word \nembedding \ncan",
          "The \nauthors \nexplored \ndifferent \navailable": ""
        },
        {
          "not annotated possible emotions.": "influence  the  behavior  and  the  final  result  of  the",
          "The \nauthors \nexplored \ndifferent \navailable": ""
        },
        {
          "not annotated possible emotions.": "",
          "The \nauthors \nexplored \ndifferent \navailable": "Model  Embeddings \nMacro-F1"
        },
        {
          "not annotated possible emotions.": "model.  The  results  showed  that  FastText  could",
          "The \nauthors \nexplored \ndifferent \navailable": ""
        },
        {
          "not annotated possible emotions.": "",
          "The \nauthors \nexplored \ndifferent \navailable": "Word2Vec + EWE \n0.737"
        },
        {
          "not annotated possible emotions.": "bring  better \nresults \nto \nthe \ntask  of  classifying",
          "The \nauthors \nexplored \ndifferent \navailable": ""
        },
        {
          "not annotated possible emotions.": "",
          "The \nauthors \nexplored \ndifferent \navailable": "SENN \nGloVe + EWE \n0.746"
        },
        {
          "not annotated possible emotions.": "emotional sentences.",
          "The \nauthors \nexplored \ndifferent \navailable": ""
        },
        {
          "not annotated possible emotions.": "",
          "The \nauthors \nexplored \ndifferent \navailable": "FastText + EWE \n0.745"
        },
        {
          "not annotated possible emotions.": "Batbaatar  et  al. \n(2019)  argued \nthat  several",
          "The \nauthors \nexplored \ndifferent \navailable": ""
        },
        {
          "not annotated possible emotions.": "previous research pieces adopted word embedding",
          "The \nauthors \nexplored \ndifferent \navailable": "Table 2:  Comparison SENN models for ISEAR"
        },
        {
          "not annotated possible emotions.": "vectors that could represent rich semantic/syntactic",
          "The \nauthors \nexplored \ndifferent \navailable": ""
        },
        {
          "not annotated possible emotions.": "",
          "The \nauthors \nexplored \ndifferent \navailable": "Even \nthough  GloVe  combined  with  EWE"
        },
        {
          "not annotated possible emotions.": "information \nbut \nhad \ndifficulty \nidentifying",
          "The \nauthors \nexplored \ndifferent \navailable": ""
        },
        {
          "not annotated possible emotions.": "",
          "The \nauthors \nexplored \ndifferent \navailable": "performed  slightly  better  for  ISEAR,  the  authors"
        },
        {
          "not annotated possible emotions.": "emotional \nrelationships  between  words.  Some",
          "The \nauthors \nexplored \ndifferent \navailable": ""
        },
        {
          "not annotated possible emotions.": "",
          "The \nauthors \nexplored \ndifferent \navailable": "observed  that  among  all  variants  of  the  SENN"
        },
        {
          "not annotated possible emotions.": "word  embeddings  with  emotional \ninformation",
          "The \nauthors \nexplored \ndifferent \navailable": ""
        },
        {
          "not annotated possible emotions.": "",
          "The \nauthors \nexplored \ndifferent \navailable": "model, the one that used FastText performed better"
        },
        {
          "not annotated possible emotions.": "have been proposed, but it also requires semantic",
          "The \nauthors \nexplored \ndifferent \navailable": ""
        },
        {
          "not annotated possible emotions.": "",
          "The \nauthors \nexplored \ndifferent \navailable": "than Word2Vec and Glove. The authors argue that"
        },
        {
          "not annotated possible emotions.": "and \nsyntactic \ninformation. \nThose \nmodels'",
          "The \nauthors \nexplored \ndifferent \navailable": ""
        },
        {
          "not annotated possible emotions.": "",
          "The \nauthors \nexplored \ndifferent \navailable": "a  possible  explanation \nis \nthat  FastText  word"
        },
        {
          "not annotated possible emotions.": "approaches cannot encode and learn semantic and",
          "The \nauthors \nexplored \ndifferent \navailable": ""
        },
        {
          "not annotated possible emotions.": "",
          "The \nauthors \nexplored \ndifferent \navailable": "embeddings capture the meaning of smaller words"
        },
        {
          "not annotated possible emotions.": "emotional relationships efficiently in a short text.",
          "The \nauthors \nexplored \ndifferent \navailable": ""
        },
        {
          "not annotated possible emotions.": "",
          "The \nauthors \nexplored \ndifferent \navailable": "and allow embeddings to understand suffixes and"
        },
        {
          "not annotated possible emotions.": "To \novercome \nthis \nchallenge, \nthe \nauthors",
          "The \nauthors \nexplored \ndifferent \navailable": ""
        },
        {
          "not annotated possible emotions.": "",
          "The \nauthors \nexplored \ndifferent \navailable": "prefixes, facilitating the processing of rare words"
        },
        {
          "not annotated possible emotions.": "proposed a new neural network architecture (called",
          "The \nauthors \nexplored \ndifferent \navailable": ""
        },
        {
          "not annotated possible emotions.": "",
          "The \nauthors \nexplored \ndifferent \navailable": "and  out-of-vocabulary.  Based  on  the  results,  we"
        },
        {
          "not annotated possible emotions.": "semantic-emotion  neural  network  -  SENN)  that",
          "The \nauthors \nexplored \ndifferent \navailable": "may  discuss  that  using  a  specific  embedding  for"
        },
        {
          "not annotated possible emotions.": "combines  two  sub-networks  to  capture  semantic",
          "The \nauthors \nexplored \ndifferent \navailable": "emotions \ncan \nimprove \nemotional \nsentence"
        },
        {
          "not annotated possible emotions.": "and  emotional \ninformation.  The \nfirst  network",
          "The \nauthors \nexplored \ndifferent \navailable": "classification tasks."
        },
        {
          "not annotated possible emotions.": "consists \nof \na  BiLSTM \nto \ncapture \nsemantic",
          "The \nauthors \nexplored \ndifferent \navailable": "In  a  recent  publication  (Adoma  et  al.,  2020),"
        },
        {
          "not annotated possible emotions.": "information  and  map \nit \ninto  semantic-sentence",
          "The \nauthors \nexplored \ndifferent \navailable": "there  was  a  comparative  analysis  of  different"
        },
        {
          "not annotated possible emotions.": "space;  the  second  network  is  a  CNN  to  capture",
          "The \nauthors \nexplored \ndifferent \navailable": "Transformer-based  models \n(BERT,  RoBERTa,"
        },
        {
          "not annotated possible emotions.": "",
          "The \nauthors \nexplored \ndifferent \navailable": "DistilBert, and XLNet) in an emotion recognition"
        },
        {
          "not annotated possible emotions.": "emotional information and map it to an emotion-",
          "The \nauthors \nexplored \ndifferent \navailable": ""
        },
        {
          "not annotated possible emotions.": "",
          "The \nauthors \nexplored \ndifferent \navailable": "task, using the ISEAR dataset Authors argued the"
        },
        {
          "not annotated possible emotions.": "sentence space.",
          "The \nauthors \nexplored \ndifferent \navailable": ""
        },
        {
          "not annotated possible emotions.": "",
          "The \nauthors \nexplored \ndifferent \navailable": "difficulty  of \nidentifying \nthe  most \nappropriate"
        },
        {
          "not annotated possible emotions.": "According  to  the  authors,  CNN  is  supposedly",
          "The \nauthors \nexplored \ndifferent \navailable": ""
        },
        {
          "not annotated possible emotions.": "",
          "The \nauthors \nexplored \ndifferent \navailable": "embeddings \nfor \nextracting \nlong \nsequence"
        },
        {
          "not annotated possible emotions.": "suitable  for  extracting  position  invariant  features",
          "The \nauthors \nexplored \ndifferent \navailable": ""
        },
        {
          "not annotated possible emotions.": "",
          "The \nauthors \nexplored \ndifferent \navailable": "relationships  and  capturing  lightning  information"
        },
        {
          "not annotated possible emotions.": "(emotion information), while BiLSTM is indicated",
          "The \nauthors \nexplored \ndifferent \navailable": ""
        },
        {
          "not annotated possible emotions.": "",
          "The \nauthors \nexplored \ndifferent \navailable": "in \na \nspecific \nscenario \n(such \nas \nemotion"
        },
        {
          "not annotated possible emotions.": "to  deal  with  a  semantic  sequence  in  a  sentence",
          "The \nauthors \nexplored \ndifferent \navailable": ""
        },
        {
          "not annotated possible emotions.": "",
          "The \nauthors \nexplored \ndifferent \navailable": "recognition), \ncan \nbe \novercome \nwith \nthe"
        },
        {
          "not annotated possible emotions.": "(contextual  information). The  combination  of  the",
          "The \nauthors \nexplored \ndifferent \navailable": ""
        },
        {
          "not annotated possible emotions.": "",
          "The \nauthors \nexplored \ndifferent \navailable": "Transformers approach and language models based"
        },
        {
          "not annotated possible emotions.": "final \nrepresentations \ncan \nbe \nhelpful \nfor \nthe",
          "The \nauthors \nexplored \ndifferent \navailable": ""
        },
        {
          "not annotated possible emotions.": "",
          "The \nauthors \nexplored \ndifferent \navailable": "on transformers."
        },
        {
          "not annotated possible emotions.": "recognition of emotions. The study used different",
          "The \nauthors \nexplored \ndifferent \navailable": ""
        },
        {
          "not annotated possible emotions.": "",
          "The \nauthors \nexplored \ndifferent \navailable": "This position appears to agree with the results"
        },
        {
          "not annotated possible emotions.": "pre-trained general word embeddings (Word2Vec,",
          "The \nauthors \nexplored \ndifferent \navailable": ""
        },
        {
          "not annotated possible emotions.": "",
          "The \nauthors \nexplored \ndifferent \navailable": "of  the  previously  reviewed  papers.  They  noticed"
        },
        {
          "not annotated possible emotions.": "GloVE,  and  FastText)  and  an  emotion-enriched",
          "The \nauthors \nexplored \ndifferent \navailable": ""
        },
        {
          "not annotated possible emotions.": "",
          "The \nauthors \nexplored \ndifferent \navailable": "that  embeddings  directly \ninfluence \nthe  model's"
        },
        {
          "not annotated possible emotions.": "word embedding (EWE).",
          "The \nauthors \nexplored \ndifferent \navailable": ""
        },
        {
          "not annotated possible emotions.": "",
          "The \nauthors \nexplored \ndifferent \navailable": "behavior and that task-related embeddings (as was"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 3: , we present the Macro-F1 for",
      "data": [
        {
          "the case with emotion-enriched word embedding -": "EWE) can bring better results.",
          "The  papers  reviewed  so  far  reflect  a  common": "characteristic \nin  NLP  projects \nthat  deal  with"
        },
        {
          "the case with emotion-enriched word embedding -": "The  purpose  of  the  paper  is  to  analyze  the",
          "The  papers  reviewed  so  far  reflect  a  common": "emotions: \nthe \nassumption \nthat \nemotions \nare"
        },
        {
          "the case with emotion-enriched word embedding -": "effectiveness of the BERT, RoBERTa, DistilBERT,",
          "The  papers  reviewed  so  far  reflect  a  common": "structured in just six basic emotions. Most datasets"
        },
        {
          "the case with emotion-enriched word embedding -": "and XLNET models in recognizing emotions using",
          "The  papers  reviewed  so  far  reflect  a  common": ""
        },
        {
          "the case with emotion-enriched word embedding -": "",
          "The  papers  reviewed  so  far  reflect  a  common": "available \n(such \nas \nthe \nISEAR \ndataset) \nare"
        },
        {
          "the case with emotion-enriched word embedding -": "the ISEAR dataset. Each model's experiments and",
          "The  papers  reviewed  so  far  reflect  a  common": ""
        },
        {
          "the case with emotion-enriched word embedding -": "",
          "The  papers  reviewed  so  far  reflect  a  common": "structured \nbased \non \nthis \ntheoretical \nposition"
        },
        {
          "the case with emotion-enriched word embedding -": "results  are  discussed  comparatively  according  to",
          "The  papers  reviewed  so  far  reflect  a  common": ""
        },
        {
          "the case with emotion-enriched word embedding -": "",
          "The  papers  reviewed  so  far  reflect  a  common": "ranging \nfrom  5 \nto  6  classes  of  emotions \n(on"
        },
        {
          "the case with emotion-enriched word embedding -": "accuracy, precision, recall, and F1-score for each",
          "The  papers  reviewed  so  far  reflect  a  common": ""
        },
        {
          "the case with emotion-enriched word embedding -": "",
          "The  papers  reviewed  so  far  reflect  a  common": "average)."
        },
        {
          "the case with emotion-enriched word embedding -": "of the classes of emotions in the dataset.",
          "The  papers  reviewed  so  far  reflect  a  common": ""
        },
        {
          "the case with emotion-enriched word embedding -": "",
          "The  papers  reviewed  so  far  reflect  a  common": "However, there is a debate in affective science,"
        },
        {
          "the case with emotion-enriched word embedding -": "Unlike the other reviewed articles, the authors",
          "The  papers  reviewed  so  far  reflect  a  common": ""
        },
        {
          "the case with emotion-enriched word embedding -": "",
          "The  papers  reviewed  so  far  reflect  a  common": "neuroscience,  psychology,  and  philosophy  about"
        },
        {
          "the case with emotion-enriched word embedding -": "detailed  the  data  pre-processing.  They  removed",
          "The  papers  reviewed  so  far  reflect  a  common": ""
        },
        {
          "the case with emotion-enriched word embedding -": "",
          "The  papers  reviewed  so  far  reflect  a  common": "what an emotion is and how many there are. We"
        },
        {
          "the case with emotion-enriched word embedding -": "records in which there was a column of emotions,",
          "The  papers  reviewed  so  far  reflect  a  common": ""
        },
        {
          "the case with emotion-enriched word embedding -": "",
          "The  papers  reviewed  so  far  reflect  a  common": "can divide the positions into two approaches: basic"
        },
        {
          "the case with emotion-enriched word embedding -": "but  the  sentence  was  missing.  Thus,  the  dataset",
          "The  papers  reviewed  so  far  reflect  a  common": ""
        },
        {
          "the case with emotion-enriched word embedding -": "",
          "The  papers  reviewed  so  far  reflect  a  common": "emotion  theory  (BET)  and  constructivism.  BET"
        },
        {
          "the case with emotion-enriched word embedding -": "used in the experiment reduced in size, from 7666",
          "The  papers  reviewed  so  far  reflect  a  common": ""
        },
        {
          "the case with emotion-enriched word embedding -": "",
          "The  papers  reviewed  so  far  reflect  a  common": "assumes  that  only  a  few  discrete  emotions  are"
        },
        {
          "the case with emotion-enriched word embedding -": "sentences \nto  7589  sentences.  The  authors  also",
          "The  papers  reviewed  so  far  reflect  a  common": ""
        },
        {
          "the case with emotion-enriched word embedding -": "",
          "The  papers  reviewed  so  far  reflect  a  common": "separated by clear boundaries and that categories"
        },
        {
          "the case with emotion-enriched word embedding -": "stated that they removed special characters, double",
          "The  papers  reviewed  so  far  reflect  a  common": ""
        },
        {
          "the case with emotion-enriched word embedding -": "",
          "The  papers  reviewed  so  far  reflect  a  common": "are  universal.  On  the  other  hand,  constructivism"
        },
        {
          "the case with emotion-enriched word embedding -": "spaces, \ntags,  and \nirregular  expressions  because",
          "The  papers  reviewed  so  far  reflect  a  common": ""
        },
        {
          "the case with emotion-enriched word embedding -": "",
          "The  papers  reviewed  so  far  reflect  a  common": "argues  that  emotions  are  constructions  based  on"
        },
        {
          "the case with emotion-enriched word embedding -": "they \ncould \nnegatively \naffect \nthe \nmodels'",
          "The  papers  reviewed  so  far  reflect  a  common": ""
        },
        {
          "the case with emotion-enriched word embedding -": "performance. Stop words have also been removed.",
          "The  papers  reviewed  so  far  reflect  a  common": "valence  and  arousal, \nthat  people  put  different"
        },
        {
          "the case with emotion-enriched word embedding -": "The dataset was divided into 80% of the data for",
          "The  papers  reviewed  so  far  reflect  a  common": "interpretations  for  affection  from  an \nindividual"
        },
        {
          "the case with emotion-enriched word embedding -": "training and 20% for testing purposes.",
          "The  papers  reviewed  so  far  reflect  a  common": "perspective."
        },
        {
          "the case with emotion-enriched word embedding -": "All  models  were \ntrained  using \nthe \nsame",
          "The  papers  reviewed  so  far  reflect  a  common": "There  is  an  alternative  perspective:  semantic"
        },
        {
          "the case with emotion-enriched word embedding -": "hyperparameters. The results showed RoBERTa as",
          "The  papers  reviewed  so  far  reflect  a  common": "space \ntheory  (Cowen  and  Keltner,  2021).  The"
        },
        {
          "the case with emotion-enriched word embedding -": "the  best  model  (accuracy  of  0.743),  followed  by",
          "The  papers  reviewed  so  far  reflect  a  common": "authors  describe  this  theory  as  a  computational"
        },
        {
          "the case with emotion-enriched word embedding -": "XLNet  (accuracy  of  0.729).  BERT  achieved  the",
          "The  papers  reviewed  so  far  reflect  a  common": "approach  that  explores  broad  natural  stimuli  and"
        },
        {
          "the case with emotion-enriched word embedding -": "third \nposition \n(accuracy \nof \n0.700) \nwhile",
          "The  papers  reviewed  so  far  reflect  a  common": ""
        },
        {
          "the case with emotion-enriched word embedding -": "",
          "The  papers  reviewed  so  far  reflect  a  common": "open-ended \nstatistical \ntechniques \nto \ncapture"
        },
        {
          "the case with emotion-enriched word embedding -": "DistillBERT  holds  the  last  position  (accuracy  of",
          "The  papers  reviewed  so  far  reflect  a  common": ""
        },
        {
          "the case with emotion-enriched word embedding -": "",
          "The  papers  reviewed  so  far  reflect  a  common": "systematic \nvariation \nin \nbehaviors \nrelated \nto"
        },
        {
          "the case with emotion-enriched word embedding -": "0.669).  In  Table  3,  we  present  the  Macro-F1  for",
          "The  papers  reviewed  so  far  reflect  a  common": ""
        },
        {
          "the case with emotion-enriched word embedding -": "",
          "The  papers  reviewed  so  far  reflect  a  common": "emotions."
        },
        {
          "the case with emotion-enriched word embedding -": "each class.",
          "The  papers  reviewed  so  far  reflect  a  common": ""
        },
        {
          "the case with emotion-enriched word embedding -": "",
          "The  papers  reviewed  so  far  reflect  a  common": "The authors argued that there are more than 25"
        },
        {
          "the case with emotion-enriched word embedding -": "",
          "The  papers  reviewed  so  far  reflect  a  common": "distinct \nemotional \nexperience \nvarieties  with"
        },
        {
          "the case with emotion-enriched word embedding -": "Model \nMacro-F1",
          "The  papers  reviewed  so  far  reflect  a  common": ""
        },
        {
          "the case with emotion-enriched word embedding -": "",
          "The  papers  reviewed  so  far  reflect  a  common": "different  backgrounds \nand \nexpressions.  These"
        },
        {
          "the case with emotion-enriched word embedding -": "BERT \n0.702",
          "The  papers  reviewed  so  far  reflect  a  common": ""
        },
        {
          "the case with emotion-enriched word embedding -": "",
          "The  papers  reviewed  so  far  reflect  a  common": "emotions are highly dimensional, categorical, and"
        },
        {
          "the case with emotion-enriched word embedding -": "RoBERTa \n0.742",
          "The  papers  reviewed  so  far  reflect  a  common": ""
        },
        {
          "the case with emotion-enriched word embedding -": "",
          "The  papers  reviewed  so  far  reflect  a  common": "can  be  blended.  Semantic \nspace \ntheory \nis  a"
        },
        {
          "the case with emotion-enriched word embedding -": "DistilBERT \n0.693",
          "The  papers  reviewed  so  far  reflect  a  common": ""
        },
        {
          "the case with emotion-enriched word embedding -": "",
          "The  papers  reviewed  so  far  reflect  a  common": "promising  proposal  and \nis  arousing \ninterest \nin"
        },
        {
          "the case with emotion-enriched word embedding -": "XLNet \n0.731",
          "The  papers  reviewed  so  far  reflect  a  common": ""
        },
        {
          "the case with emotion-enriched word embedding -": "",
          "The  papers  reviewed  so  far  reflect  a  common": "research in affective science. In this sense, it is also"
        },
        {
          "the case with emotion-enriched word embedding -": "Table 3:  Comparison of Transformers models for",
          "The  papers  reviewed  so  far  reflect  a  common": "essential to investigate their possible interaction in"
        },
        {
          "the case with emotion-enriched word embedding -": "ISEAR",
          "The  papers  reviewed  so  far  reflect  a  common": "emotion \nrecognition \nprojects \nusing \nNLU"
        },
        {
          "the case with emotion-enriched word embedding -": "",
          "The  papers  reviewed  so  far  reflect  a  common": "approaches."
        },
        {
          "the case with emotion-enriched word embedding -": "The  authors  concluded \nthat  RoBERTa  pre-",
          "The  papers  reviewed  so  far  reflect  a  common": ""
        },
        {
          "the case with emotion-enriched word embedding -": "",
          "The  papers  reviewed  so  far  reflect  a  common": "Recently,  Demszky  et  al.  (2020)  published  a"
        },
        {
          "the case with emotion-enriched word embedding -": "trained  model  outperforms  the  other  pre-trained",
          "The  papers  reviewed  so  far  reflect  a  common": ""
        },
        {
          "the case with emotion-enriched word embedding -": "",
          "The  papers  reviewed  so  far  reflect  a  common": "new  paper  where  they  introduce  GoEmotion,  a"
        },
        {
          "the case with emotion-enriched word embedding -": "models.  They \nalso  discuss \nthat \neven \nthough",
          "The  papers  reviewed  so  far  reflect  a  common": ""
        },
        {
          "the case with emotion-enriched word embedding -": "",
          "The  papers  reviewed  so  far  reflect  a  common": "manually annotated dataset of 58 thousand Reddit"
        },
        {
          "the case with emotion-enriched word embedding -": "DistilBERT  was  the  least  accurate  model,  it  was",
          "The  papers  reviewed  so  far  reflect  a  common": ""
        },
        {
          "the case with emotion-enriched word embedding -": "",
          "The  papers  reviewed  so  far  reflect  a  common": "Comments  (in  English),  labeled  for  27  emotions"
        },
        {
          "the case with emotion-enriched word embedding -": "the  fastest  model  while  XLNet  was  the  slowest.",
          "The  papers  reviewed  so  far  reflect  a  common": ""
        },
        {
          "the case with emotion-enriched word embedding -": "",
          "The  papers  reviewed  so  far  reflect  a  common": "categories (based on semantic space theory)."
        },
        {
          "the case with emotion-enriched word embedding -": "The decreasing order of computational resources is",
          "The  papers  reviewed  so  far  reflect  a  common": ""
        },
        {
          "the case with emotion-enriched word embedding -": "",
          "The  papers  reviewed  so  far  reflect  a  common": "The  authors  argue  that  in  contrast  to  Ekman's"
        },
        {
          "the case with emotion-enriched word embedding -": "given as XLNet, BERT, RoBERTa, DistilBERT.",
          "The  papers  reviewed  so  far  reflect  a  common": ""
        },
        {
          "the case with emotion-enriched word embedding -": "",
          "The  papers  reviewed  so  far  reflect  a  common": "taxonomy, which includes only a positive emotion"
        },
        {
          "the case with emotion-enriched word embedding -": "Finally,  the  authors  conclude  that  pre-trained",
          "The  papers  reviewed  so  far  reflect  a  common": ""
        },
        {
          "the case with emotion-enriched word embedding -": "",
          "The  papers  reviewed  so  far  reflect  a  common": "(joy), this new taxonomy proposal includes a more"
        },
        {
          "the case with emotion-enriched word embedding -": "models based on transformers prove to be effective",
          "The  papers  reviewed  so  far  reflect  a  common": ""
        },
        {
          "the case with emotion-enriched word embedding -": "",
          "The  papers  reviewed  so  far  reflect  a  common": "significant  number  of  positive,  negative, \nand"
        },
        {
          "the case with emotion-enriched word embedding -": "in  detecting  emotions \nin \ntexts,  with  RoBERTa",
          "The  papers  reviewed  so  far  reflect  a  common": ""
        },
        {
          "the case with emotion-enriched word embedding -": "",
          "The  papers  reviewed  so  far  reflect  a  common": "ambiguous  emotion  categories,  which  may  be"
        },
        {
          "the case with emotion-enriched word embedding -": "presenting \nthe  best  metrics \nfor  accuracy  and",
          "The  papers  reviewed  so  far  reflect  a  common": ""
        },
        {
          "the case with emotion-enriched word embedding -": "",
          "The  papers  reviewed  so  far  reflect  a  common": "suitable  for  comprehension  tasks  that  require  an"
        },
        {
          "the case with emotion-enriched word embedding -": "macro-f1.  This  work \nis \nimportant \nto  show \nthe",
          "The  papers  reviewed  so  far  reflect  a  common": ""
        },
        {
          "the case with emotion-enriched word embedding -": "",
          "The  papers  reviewed  so  far  reflect  a  common": "understanding \nof \nemotional \nexpressions \nin"
        },
        {
          "the case with emotion-enriched word embedding -": "potential  use  of  pre-trained  models \nin  specific",
          "The  papers  reviewed  so  far  reflect  a  common": ""
        },
        {
          "the case with emotion-enriched word embedding -": "",
          "The  papers  reviewed  so  far  reflect  a  common": "conversations, such as analyzing feedback sent by"
        },
        {
          "the case with emotion-enriched word embedding -": "scenarios \n(as \nin \nrecognizing \nemotions), \njust",
          "The  papers  reviewed  so  far  reflect  a  common": ""
        },
        {
          "the case with emotion-enriched word embedding -": "",
          "The  papers  reviewed  so  far  reflect  a  common": "a customer or interacting with chatbots."
        },
        {
          "the case with emotion-enriched word embedding -": "finetuning the model.",
          "The  papers  reviewed  so  far  reflect  a  common": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "To create this fine-grained dataset, the authors": "used  a  data  dump  that  contains  comments  from",
          "3 \nExperimental protocol": ""
        },
        {
          "To create this fine-grained dataset, the authors": "",
          "3 \nExperimental protocol": "One of the many challenges of NLU is to identify"
        },
        {
          "To create this fine-grained dataset, the authors": "2005  to  January  2019.  They  selected  subreddits",
          "3 \nExperimental protocol": ""
        },
        {
          "To create this fine-grained dataset, the authors": "",
          "3 \nExperimental protocol": "the  best  embeddings  to  extract  specific  semantic"
        },
        {
          "To create this fine-grained dataset, the authors": "with more than 10k comments and applied a series",
          "3 \nExperimental protocol": ""
        },
        {
          "To create this fine-grained dataset, the authors": "",
          "3 \nExperimental protocol": "information (for example, emotions) and identify"
        },
        {
          "To create this fine-grained dataset, the authors": "of  data  curation  (which  is  not  specified  in  the",
          "3 \nExperimental protocol": ""
        },
        {
          "To create this fine-grained dataset, the authors": "",
          "3 \nExperimental protocol": "different \ntechniques \nto \ndeal  with \nlong-term"
        },
        {
          "To create this fine-grained dataset, the authors": "original paper) to ensure the data does not reinforce",
          "3 \nExperimental protocol": ""
        },
        {
          "To create this fine-grained dataset, the authors": "",
          "3 \nExperimental protocol": "dependency. In recent years, we have seen different"
        },
        {
          "To create this fine-grained dataset, the authors": "language bias. They removed subreddits based on",
          "3 \nExperimental protocol": ""
        },
        {
          "To create this fine-grained dataset, the authors": "",
          "3 \nExperimental protocol": "architectures \nemerging \nto \ndeal \nwith \nthese"
        },
        {
          "To create this fine-grained dataset, the authors": "\"not  safe  for  work\"  public  list. They  kept  vulgar",
          "3 \nExperimental protocol": ""
        },
        {
          "To create this fine-grained dataset, the authors": "",
          "3 \nExperimental protocol": "challenges, such as RNNs, LSTM, BiLSTM. More"
        },
        {
          "To create this fine-grained dataset, the authors": "comments  because  they  could  include  negative",
          "3 \nExperimental protocol": ""
        },
        {
          "To create this fine-grained dataset, the authors": "",
          "3 \nExperimental protocol": "recently, the Transformers architecture (Vaswani et"
        },
        {
          "To create this fine-grained dataset, the authors": "emotions.  They  also  used  a  pilot  model  (trained",
          "3 \nExperimental protocol": ""
        },
        {
          "To create this fine-grained dataset, the authors": "",
          "3 \nExperimental protocol": "al., \n2017) \nhas \nshown \nto \nbe \nalternatives \nin"
        },
        {
          "To create this fine-grained dataset, the authors": "with \n2.2K \nannotated \nexamples) \nto \nexclude",
          "3 \nExperimental protocol": ""
        },
        {
          "To create this fine-grained dataset, the authors": "",
          "3 \nExperimental protocol": "addressing these limitations."
        },
        {
          "To create this fine-grained dataset, the authors": "subreddits that consist of more than 30% of neutral",
          "3 \nExperimental protocol": ""
        },
        {
          "To create this fine-grained dataset, the authors": "",
          "3 \nExperimental protocol": "More  recently,  transformers'  language  models"
        },
        {
          "To create this fine-grained dataset, the authors": "comments or less than 20% negative, positive, or",
          "3 \nExperimental protocol": ""
        },
        {
          "To create this fine-grained dataset, the authors": "",
          "3 \nExperimental protocol": "have shown better language understanding abilities"
        },
        {
          "To create this fine-grained dataset, the authors": "ambiguous  comments.  They  also  used  a  pilot",
          "3 \nExperimental protocol": ""
        },
        {
          "To create this fine-grained dataset, the authors": "",
          "3 \nExperimental protocol": "to achieve state-of-the-art results in many different"
        },
        {
          "To create this fine-grained dataset, the authors": "model to balance the emotions in the dataset.",
          "3 \nExperimental protocol": ""
        },
        {
          "To create this fine-grained dataset, the authors": "",
          "3 \nExperimental protocol": "NLP and NLU tasks."
        },
        {
          "To create this fine-grained dataset, the authors": "The annotation process consisted of three raters",
          "3 \nExperimental protocol": ""
        },
        {
          "To create this fine-grained dataset, the authors": "",
          "3 \nExperimental protocol": "It \nis \nimportant \nto  highlight \nthat  different"
        },
        {
          "To create this fine-grained dataset, the authors": "for each example. When no raters agree on at least",
          "3 \nExperimental protocol": ""
        },
        {
          "To create this fine-grained dataset, the authors": "",
          "3 \nExperimental protocol": "implementations \nof \nlanguage \nmodels \nhave"
        },
        {
          "To create this fine-grained dataset, the authors": "one  emotion  label,  they  assigned  two  additional",
          "3 \nExperimental protocol": ""
        },
        {
          "To create this fine-grained dataset, the authors": "",
          "3 \nExperimental protocol": "emerged from the original idea of the Transformer"
        },
        {
          "To create this fine-grained dataset, the authors": "raters. All raters were native English speakers from",
          "3 \nExperimental protocol": ""
        },
        {
          "To create this fine-grained dataset, the authors": "",
          "3 \nExperimental protocol": "architecture. We hypothesize that different models"
        },
        {
          "To create this fine-grained dataset, the authors": "India, \nbut \nthe \nauthors \ndid \nnot \ndetail \ntheir",
          "3 \nExperimental protocol": ""
        },
        {
          "To create this fine-grained dataset, the authors": "",
          "3 \nExperimental protocol": "can  present  different  performance  (f1-score)  and"
        },
        {
          "To create this fine-grained dataset, the authors": "background (education, profession, and so on).",
          "3 \nExperimental protocol": ""
        },
        {
          "To create this fine-grained dataset, the authors": "",
          "3 \nExperimental protocol": "time \nto  complete \nin \nthe \ntask  of \nrecognizing"
        },
        {
          "To create this fine-grained dataset, the authors": "The  authors  used \nthe  dataset \nto \ntrain \ntwo",
          "3 \nExperimental protocol": ""
        },
        {
          "To create this fine-grained dataset, the authors": "",
          "3 \nExperimental protocol": "emotions."
        },
        {
          "To create this fine-grained dataset, the authors": "models: a baseline model using a BiLSTM and a",
          "3 \nExperimental protocol": ""
        },
        {
          "To create this fine-grained dataset, the authors": "",
          "3 \nExperimental protocol": "BERT  was  one  of  the  first  models  based  on"
        },
        {
          "To create this fine-grained dataset, the authors": "BERT-base  (Devlin  et  al.,  2018)  for \nthe  final",
          "3 \nExperimental protocol": ""
        },
        {
          "To create this fine-grained dataset, the authors": "",
          "3 \nExperimental protocol": "Transformers to achieve great results in many NLP"
        },
        {
          "To create this fine-grained dataset, the authors": "experiment.  For  the  BERT  model,  they  added  a",
          "3 \nExperimental protocol": ""
        },
        {
          "To create this fine-grained dataset, the authors": "",
          "3 \nExperimental protocol": "tasks.  However,  we  assume \nthat  more \nrecent"
        },
        {
          "To create this fine-grained dataset, the authors": "dense output layer on top of the pre-trained model",
          "3 \nExperimental protocol": ""
        },
        {
          "To create this fine-grained dataset, the authors": "",
          "3 \nExperimental protocol": "Transformers \nlanguage  models  could  be  more"
        },
        {
          "To create this fine-grained dataset, the authors": "for  finetuning,  with  a  sigmoid  cross-entropy  loss",
          "3 \nExperimental protocol": ""
        },
        {
          "To create this fine-grained dataset, the authors": "",
          "3 \nExperimental protocol": "efficient \nthan  BERT.  Thus, \nour \ngoal \nis \nto"
        },
        {
          "To create this fine-grained dataset, the authors": "function to support multi-label classification.",
          "3 \nExperimental protocol": ""
        },
        {
          "To create this fine-grained dataset, the authors": "",
          "3 \nExperimental protocol": "investigate different transformer language models'"
        },
        {
          "To create this fine-grained dataset, the authors": "They \nkept  most \nof \nthe \nhyperparameters",
          "3 \nExperimental protocol": ""
        },
        {
          "To create this fine-grained dataset, the authors": "",
          "3 \nExperimental protocol": "behavior in this type of task."
        },
        {
          "To create this fine-grained dataset, the authors": "presented by the original paper. They only change",
          "3 \nExperimental protocol": ""
        },
        {
          "To create this fine-grained dataset, the authors": "the batch size (to 16) and learning rate (to 5e-5).",
          "3 \nExperimental protocol": ""
        },
        {
          "To create this fine-grained dataset, the authors": "They  also  found  that  training  for  at  least  four",
          "3 \nExperimental protocol": ""
        },
        {
          "To create this fine-grained dataset, the authors": "",
          "3 \nExperimental protocol": "3.1 \nData"
        },
        {
          "To create this fine-grained dataset, the authors": "epochs is indicated for this dataset, but training for",
          "3 \nExperimental protocol": ""
        },
        {
          "To create this fine-grained dataset, the authors": "",
          "3 \nExperimental protocol": "In \nthis \nresearch, \nthe  dataset  we  will  use \nis"
        },
        {
          "To create this fine-grained dataset, the authors": "more epochs results in overfitting.",
          "3 \nExperimental protocol": ""
        },
        {
          "To create this fine-grained dataset, the authors": "",
          "3 \nExperimental protocol": "GoEmotion, released by Demszky et al. (2020). It"
        },
        {
          "To create this fine-grained dataset, the authors": "The performance of the best model, BERT, on",
          "3 \nExperimental protocol": ""
        },
        {
          "To create this fine-grained dataset, the authors": "",
          "3 \nExperimental protocol": "is  the  largest  manually  annotated  dataset  of  58"
        },
        {
          "To create this fine-grained dataset, the authors": "the  test  set,  achieved  an  average  F1-score  of  .46",
          "3 \nExperimental protocol": ""
        },
        {
          "To create this fine-grained dataset, the authors": "",
          "3 \nExperimental protocol": "thousand  Reddit  Comments  (in  English),  labeled"
        },
        {
          "To create this fine-grained dataset, the authors": "(std=.19) for the full taxonomy (27 emotions). The",
          "3 \nExperimental protocol": ""
        },
        {
          "To create this fine-grained dataset, the authors": "",
          "3 \nExperimental protocol": "for 27 emotion categories (based on semantic space"
        },
        {
          "To create this fine-grained dataset, the authors": "BiLSTM  model  performed  significantly  worse",
          "3 \nExperimental protocol": ""
        },
        {
          "To create this fine-grained dataset, the authors": "",
          "3 \nExperimental protocol": "theory) or neutral."
        },
        {
          "To create this fine-grained dataset, the authors": "than BERT, obtaining an average F1-score of .41",
          "3 \nExperimental protocol": ""
        },
        {
          "To create this fine-grained dataset, the authors": "",
          "3 \nExperimental protocol": "Other \nannotated  datasets \nare \navailable \nfor"
        },
        {
          "To create this fine-grained dataset, the authors": "for the full taxonomy.",
          "3 \nExperimental protocol": ""
        },
        {
          "To create this fine-grained dataset, the authors": "",
          "3 \nExperimental protocol": "emotion recognition tasks, but they mainly rely on"
        },
        {
          "To create this fine-grained dataset, the authors": "Authors suggest that it is a promising approach",
          "3 \nExperimental protocol": ""
        },
        {
          "To create this fine-grained dataset, the authors": "",
          "3 \nExperimental protocol": "basic  emotion  theory  from  Paul  Ekman,  which"
        },
        {
          "To create this fine-grained dataset, the authors": "integrating affective science and NLU, but there is",
          "3 \nExperimental protocol": ""
        },
        {
          "To create this fine-grained dataset, the authors": "",
          "3 \nExperimental protocol": "comprises only six basic emotions. For this reason,"
        },
        {
          "To create this fine-grained dataset, the authors": "much  room  for  improvement.  In  this  sense,  we",
          "3 \nExperimental protocol": ""
        },
        {
          "To create this fine-grained dataset, the authors": "",
          "3 \nExperimental protocol": "we decided to use GoEmotion and investigate how"
        },
        {
          "To create this fine-grained dataset, the authors": "investigated \nthe \nperformance \nof \ndifferent",
          "3 \nExperimental protocol": ""
        },
        {
          "To create this fine-grained dataset, the authors": "",
          "3 \nExperimental protocol": "different transformer language models can handle"
        },
        {
          "To create this fine-grained dataset, the authors": "transformers-based  language  with  this  dataset. A",
          "3 \nExperimental protocol": ""
        },
        {
          "To create this fine-grained dataset, the authors": "",
          "3 \nExperimental protocol": "a dataset of fine-grained emotions."
        },
        {
          "To create this fine-grained dataset, the authors": "similar approach to the one driven by (Adoma et",
          "3 \nExperimental protocol": ""
        },
        {
          "To create this fine-grained dataset, the authors": "al.,  2020),  but  using  a  fine-grained  dataset  of",
          "3 \nExperimental protocol": ""
        },
        {
          "To create this fine-grained dataset, the authors": "emotions",
          "3 \nExperimental protocol": ""
        },
        {
          "To create this fine-grained dataset, the authors": "",
          "3 \nExperimental protocol": "3.2 \nMetrics"
        },
        {
          "To create this fine-grained dataset, the authors": "",
          "3 \nExperimental protocol": "We will compute three standard metrics to measure"
        },
        {
          "To create this fine-grained dataset, the authors": "",
          "3 \nExperimental protocol": "the model's performance: precision, recall, and F1-"
        },
        {
          "To create this fine-grained dataset, the authors": "",
          "3 \nExperimental protocol": "score for each class. We will use F1-score (macro)"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "as  used \nin \nthe  original  paper  of  GoEmotion": "(Demszky et al., 2020) to compare the performance",
          "RoBERTa was trained with those two corpora plus": "three  corpora  from  different  domains:  CC-News,"
        },
        {
          "as  used \nin \nthe  original  paper  of  GoEmotion": "among all the models.",
          "RoBERTa was trained with those two corpora plus": "OpenWeb Text, and Stories, which totals 160GB of"
        },
        {
          "as  used \nin \nthe  original  paper  of  GoEmotion": "We  will  also  measure  the  time  taken  for  each",
          "RoBERTa was trained with those two corpora plus": "text."
        },
        {
          "as  used \nin \nthe  original  paper  of  GoEmotion": "model \nto \nrun \nto  completion  on \ntraining  and",
          "RoBERTa was trained with those two corpora plus": "They also proposed improvements to the model"
        },
        {
          "as  used \nin \nthe  original  paper  of  GoEmotion": "evaluation.",
          "RoBERTa was trained with those two corpora plus": "design. For the training procedure, instead of using"
        },
        {
          "as  used \nin \nthe  original  paper  of  GoEmotion": "",
          "RoBERTa was trained with those two corpora plus": "Next Sentence Prediction (NSP) task from BERT's"
        },
        {
          "as  used \nin \nthe  original  paper  of  GoEmotion": "",
          "RoBERTa was trained with those two corpora plus": "pre-training, they introduced dynamic masking so"
        },
        {
          "as  used \nin \nthe  original  paper  of  GoEmotion": "3.3 \nModels",
          "RoBERTa was trained with those two corpora plus": ""
        },
        {
          "as  used \nin \nthe  original  paper  of  GoEmotion": "",
          "RoBERTa was trained with those two corpora plus": "that the masked token changes during the training"
        },
        {
          "as  used \nin \nthe  original  paper  of  GoEmotion": "In the original paper, Demszky et al. (2020) used a",
          "RoBERTa was trained with those two corpora plus": "epochs.  They  also  trained  the  model  on  longer"
        },
        {
          "as  used \nin \nthe  original  paper  of  GoEmotion": "BiLSTM as baseline model. The BiLSTM model",
          "RoBERTa was trained with those two corpora plus": "sequences (Liu et al., 2019)."
        },
        {
          "as  used \nin \nthe  original  paper  of  GoEmotion": "performed achieved an average F1-score of 0.41.",
          "RoBERTa was trained with those two corpora plus": "XLNet \n(Yang \net \nal., \n2019) \nis \na \nlarge"
        },
        {
          "as  used \nin \nthe  original  paper  of  GoEmotion": "They  also  used  a  BERT  implementation  as  the",
          "RoBERTa was trained with those two corpora plus": "bidirectional \ntransformer \nthat \nuses \nimproved"
        },
        {
          "as  used \nin \nthe  original  paper  of  GoEmotion": "main model.",
          "RoBERTa was trained with those two corpora plus": "training  methodology, \nlarger \ndata, \nand  more"
        },
        {
          "as  used \nin \nthe  original  paper  of  GoEmotion": "Google \nreleased \nthe  Bidirectional  Encoder",
          "RoBERTa was trained with those two corpora plus": "computational  power.    To  improve  the  training,"
        },
        {
          "as  used \nin \nthe  original  paper  of  GoEmotion": "Representations  from  Transformers  (BERT)  pre-",
          "RoBERTa was trained with those two corpora plus": "XLNet \ndevelopers \nintroduced \npermutation"
        },
        {
          "as  used \nin \nthe  original  paper  of  GoEmotion": "trained  language  model  (Devlin  et  al.,  2018)  in",
          "RoBERTa was trained with those two corpora plus": "language  modeling.  In  contrast  to  BERT,  which"
        },
        {
          "as  used \nin \nthe  original  paper  of  GoEmotion": "2018. \nIt \nis \nconsidered \nthe  milestone \nof \na",
          "RoBERTa was trained with those two corpora plus": "predicts  only  the  masked  15%  tokens  (Masked"
        },
        {
          "as  used \nin \nthe  original  paper  of  GoEmotion": "rediscovery \nof \nthe  NLP \narea \nfor \nimproving",
          "RoBERTa was trained with those two corpora plus": "tokens), XLNET predicts all tokens but in random"
        },
        {
          "as  used \nin \nthe  original  paper  of  GoEmotion": "performance in different natural language tasks. In",
          "RoBERTa was trained with those two corpora plus": "order.  XLNet  was  trained  with  over  130  GB  of"
        },
        {
          "as  used \nin \nthe  original  paper  of  GoEmotion": "the  GoEmotion  original  paper,  the  BERT  model",
          "RoBERTa was trained with those two corpora plus": "textual  data.  In  addition  to  BERT's  two  datasets,"
        },
        {
          "as  used \nin \nthe  original  paper  of  GoEmotion": "performed significantly better than BiLSTM, with",
          "RoBERTa was trained with those two corpora plus": "they \nincluded \nthree  more \ncorpora:  Giga5,"
        },
        {
          "as  used \nin \nthe  original  paper  of  GoEmotion": "an average F1-score of 0.46 against an average F1-",
          "RoBERTa was trained with those two corpora plus": "ClueWeb, \nand \nCommon \nCrawl. \nXLNet"
        },
        {
          "as  used \nin \nthe  original  paper  of  GoEmotion": "score of 0.41 from the baseline model.",
          "RoBERTa was trained with those two corpora plus": "outperformed BERT on 20 tasks, such as question"
        },
        {
          "as  used \nin \nthe  original  paper  of  GoEmotion": "In this research, we will assume BERT as our",
          "RoBERTa was trained with those two corpora plus": "answering,  natural  language  inference,  sentiment"
        },
        {
          "as  used \nin \nthe  original  paper  of  GoEmotion": "baseline  model.  Despite  the  excellent  results  in",
          "RoBERTa was trained with those two corpora plus": "analysis, and so on."
        },
        {
          "as  used \nin \nthe  original  paper  of  GoEmotion": "different benchmarks, this is a model that has some",
          "RoBERTa was trained with those two corpora plus": "ELECTRA  is  a  model  which  training  process"
        },
        {
          "as  used \nin \nthe  original  paper  of  GoEmotion": "limitations.  Since  the  release  of  BERT,  different",
          "RoBERTa was trained with those two corpora plus": "does not rely on masked language such as BERT or"
        },
        {
          "as  used \nin \nthe  original  paper  of  GoEmotion": "models  were  proposed \nto  address  some  BERT",
          "RoBERTa was trained with those two corpora plus": "RoBERTa. \nInstead  of  masking \nthe \ninput  and"
        },
        {
          "as  used \nin \nthe  original  paper  of  GoEmotion": "limitations. For this reason, we will investigate the",
          "RoBERTa was trained with those two corpora plus": "predicting \nit, \nthe  ELECTRA  approach  replaces"
        },
        {
          "as  used \nin \nthe  original  paper  of  GoEmotion": "performance  of  four  recent  transformer  language",
          "RoBERTa was trained with those two corpora plus": "some tokens with plausible alternatives generated"
        },
        {
          "as  used \nin \nthe  original  paper  of  GoEmotion": "models \n(DistilBert, \nRoBERTa, \nXLNet, \nand",
          "RoBERTa was trained with those two corpora plus": "by  a  small  generator  network.  Then,  the  model"
        },
        {
          "as  used \nin \nthe  original  paper  of  GoEmotion": "ELECTRA) in the Emotion Recognition task. We",
          "RoBERTa was trained with those two corpora plus": "does not predict the original identities of the token"
        },
        {
          "as  used \nin \nthe  original  paper  of  GoEmotion": "will assume BERT as our baseline model.",
          "RoBERTa was trained with those two corpora plus": "replaced,  and  it  trains  a  discriminative  model  to"
        },
        {
          "as  used \nin \nthe  original  paper  of  GoEmotion": "DistilBERT \nis  a  distilled  version  of  BERT,",
          "RoBERTa was trained with those two corpora plus": "predict whether a generator sample replaced tokens"
        },
        {
          "as  used \nin \nthe  original  paper  of  GoEmotion": "which is smaller, faster, cheaper, and lighter. This",
          "RoBERTa was trained with those two corpora plus": "in the input. The authors argued that this approach"
        },
        {
          "as  used \nin \nthe  original  paper  of  GoEmotion": "model  is  inspired  by  the  Knowledge  distillation",
          "RoBERTa was trained with those two corpora plus": "works  well  at  scale.  The  model  had  the  same"
        },
        {
          "as  used \nin \nthe  original  paper  of  GoEmotion": "approach. It is a compression technique to train a",
          "RoBERTa was trained with those two corpora plus": "performance  as  RoBERTa  and  XLNet  but  using"
        },
        {
          "as  used \nin \nthe  original  paper  of  GoEmotion": "small model to reproduce a larger model's behavior",
          "RoBERTa was trained with those two corpora plus": "less than ¼ of the computing resources. The model"
        },
        {
          "as  used \nin \nthe  original  paper  of  GoEmotion": "(that is the reason it is also called teacher-student",
          "RoBERTa was trained with those two corpora plus": "also outperformed them when the same computing"
        },
        {
          "as  used \nin \nthe  original  paper  of  GoEmotion": "learning). Using this technique reduces the size of",
          "RoBERTa was trained with those two corpora plus": "amount was available (Clark et al., 2020)."
        },
        {
          "as  used \nin \nthe  original  paper  of  GoEmotion": "a BERT model by 40%, while 97% of its language",
          "RoBERTa was trained with those two corpora plus": ""
        },
        {
          "as  used \nin \nthe  original  paper  of  GoEmotion": "capabilities are kept. The model is also 60% faster",
          "RoBERTa was trained with those two corpora plus": ""
        },
        {
          "as  used \nin \nthe  original  paper  of  GoEmotion": "",
          "RoBERTa was trained with those two corpora plus": "3.4 \nParameters Settings"
        },
        {
          "as  used \nin \nthe  original  paper  of  GoEmotion": "(Sanh et al., 2019).",
          "RoBERTa was trained with those two corpora plus": ""
        },
        {
          "as  used \nin \nthe  original  paper  of  GoEmotion": "RoBERTa \n(Robustly  optimized  BERT  pre-",
          "RoBERTa was trained with those two corpora plus": "When \nfinetuning \nall  Pre-trained \ntransformers"
        },
        {
          "as  used \nin \nthe  original  paper  of  GoEmotion": "training  Approach)  is  a  model  developed  by  a",
          "RoBERTa was trained with those two corpora plus": "language \nmodel, \nwe \nkeep \nmost \nof \nthe"
        },
        {
          "as  used \nin \nthe  original  paper  of  GoEmotion": "Facebook team to improve BERT implementation.",
          "RoBERTa was trained with those two corpora plus": "hyperparameters set by Devlin et al. (2019) intact"
        },
        {
          "as  used \nin \nthe  original  paper  of  GoEmotion": "The  researchers  proposed  some  changes  to  the",
          "RoBERTa was trained with those two corpora plus": "and only change the batch size and learning rate,"
        },
        {
          "as  used \nin \nthe  original  paper  of  GoEmotion": "original model (Liu et al., 2019).",
          "RoBERTa was trained with those two corpora plus": "based on the settings proposed by Demszky et al."
        },
        {
          "as  used \nin \nthe  original  paper  of  GoEmotion": "First,  they  used  a  larger  dataset.  BERT  was",
          "RoBERTa was trained with those two corpora plus": "(2020).  We  set  the  learning  rate  to  5e-5  and"
        },
        {
          "as  used \nin \nthe  original  paper  of  GoEmotion": "trained  on  a  combination  of  BookCopus  plus",
          "RoBERTa was trained with those two corpora plus": "batch size to 16. We train the model for four"
        },
        {
          "as  used \nin \nthe  original  paper  of  GoEmotion": "English Wikipedia text, which totals 16GB of text.",
          "RoBERTa was trained with those two corpora plus": "epochs. The threshold to set a result as positive"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 4: summarizes our four models' gratitude 0.86 0.90 0.90 0.90 0.90",
      "data": [
        {
          "was 0.30. Parameter settings for all models are": "",
          "However, along with BERT, ELECTRA achieved": "the best performance for the neutral class."
        },
        {
          "was 0.30. Parameter settings for all models are": "detailed:",
          "However, along with BERT, ELECTRA achieved": ""
        },
        {
          "was 0.30. Parameter settings for all models are": "",
          "However, along with BERT, ELECTRA achieved": ""
        },
        {
          "was 0.30. Parameter settings for all models are": "",
          "However, along with BERT, ELECTRA achieved": "BERT"
        },
        {
          "was 0.30. Parameter settings for all models are": "Learning rate: 5e-5 \n•",
          "However, along with BERT, ELECTRA achieved": ""
        },
        {
          "was 0.30. Parameter settings for all models are": "",
          "However, along with BERT, ELECTRA achieved": "0.65"
        },
        {
          "was 0.30. Parameter settings for all models are": "Batch Size: 16 \n•",
          "However, along with BERT, ELECTRA achieved": "0.80"
        },
        {
          "was 0.30. Parameter settings for all models are": "",
          "However, along with BERT, ELECTRA achieved": "0.47"
        },
        {
          "was 0.30. Parameter settings for all models are": "Epochs: 4  \n•",
          "However, along with BERT, ELECTRA achieved": ""
        },
        {
          "was 0.30. Parameter settings for all models are": "",
          "However, along with BERT, ELECTRA achieved": "0.34"
        },
        {
          "was 0.30. Parameter settings for all models are": "Threshold: 0.30 \n•",
          "However, along with BERT, ELECTRA achieved": ""
        },
        {
          "was 0.30. Parameter settings for all models are": "",
          "However, along with BERT, ELECTRA achieved": "0.36"
        },
        {
          "was 0.30. Parameter settings for all models are": "",
          "However, along with BERT, ELECTRA achieved": "0.39"
        },
        {
          "was 0.30. Parameter settings for all models are": "A",
          "However, along with BERT, ELECTRA achieved": "0.37"
        },
        {
          "was 0.30. Parameter settings for all models are": "ll \nmodels \nwere \nimplemented \nusing \nthe",
          "However, along with BERT, ELECTRA achieved": ""
        },
        {
          "was 0.30. Parameter settings for all models are": "",
          "However, along with BERT, ELECTRA achieved": "0.54"
        },
        {
          "was 0.30. Parameter settings for all models are": "huggingface  library. The  training  process  used",
          "However, along with BERT, ELECTRA achieved": ""
        },
        {
          "was 0.30. Parameter settings for all models are": "",
          "However, along with BERT, ELECTRA achieved": "0.49"
        },
        {
          "was 0.30. Parameter settings for all models are": "the same computing environment (Tesla T4 GPU).",
          "However, along with BERT, ELECTRA achieved": "0.28"
        },
        {
          "was 0.30. Parameter settings for all models are": "",
          "However, along with BERT, ELECTRA achieved": "0.39"
        },
        {
          "was 0.30. Parameter settings for all models are": "",
          "However, along with BERT, ELECTRA achieved": "0.45"
        },
        {
          "was 0.30. Parameter settings for all models are": "",
          "However, along with BERT, ELECTRA achieved": "0.43"
        },
        {
          "was 0.30. Parameter settings for all models are": "3.5 \nResults",
          "However, along with BERT, ELECTRA achieved": ""
        },
        {
          "was 0.30. Parameter settings for all models are": "",
          "However, along with BERT, ELECTRA achieved": "0.34"
        },
        {
          "was 0.30. Parameter settings for all models are": "",
          "However, along with BERT, ELECTRA achieved": "0.60"
        },
        {
          "was 0.30. Parameter settings for all models are": "T",
          "However, along with BERT, ELECTRA achieved": "0.86"
        },
        {
          "was 0.30. Parameter settings for all models are": "able \n4 \nsummarizes \nour \nfour \nmodels'",
          "However, along with BERT, ELECTRA achieved": ""
        },
        {
          "was 0.30. Parameter settings for all models are": "",
          "However, along with BERT, ELECTRA achieved": "0.00"
        },
        {
          "was 0.30. Parameter settings for all models are": "performance \n(DistiBERT,  RoBERTa,  XLNet,",
          "However, along with BERT, ELECTRA achieved": ""
        },
        {
          "was 0.30. Parameter settings for all models are": "",
          "However, along with BERT, ELECTRA achieved": "0.51"
        },
        {
          "was 0.30. Parameter settings for all models are": "and Electra) plus our BERT Baseline based on the",
          "However, along with BERT, ELECTRA achieved": "0.78"
        },
        {
          "was 0.30. Parameter settings for all models are": "original  paper  of  GoEMotion  (Demszky  et  al.,",
          "However, along with BERT, ELECTRA achieved": "0.35"
        },
        {
          "was 0.30. Parameter settings for all models are": "",
          "However, along with BERT, ELECTRA achieved": "0.51"
        },
        {
          "was 0.30. Parameter settings for all models are": "2020).",
          "However, along with BERT, ELECTRA achieved": ""
        },
        {
          "was 0.30. Parameter settings for all models are": "",
          "However, along with BERT, ELECTRA achieved": "0.36"
        },
        {
          "was 0.30. Parameter settings for all models are": "The model that achieved the highest f1-score",
          "However, along with BERT, ELECTRA achieved": ""
        },
        {
          "was 0.30. Parameter settings for all models are": "",
          "However, along with BERT, ELECTRA achieved": "0.21"
        },
        {
          "was 0.30. Parameter settings for all models are": "(macro-average) \nwas \nRoBERTa \nwith \n.49",
          "However, along with BERT, ELECTRA achieved": "0.15"
        },
        {
          "was 0.30. Parameter settings for all models are": "(std=.23). The model obtained the best result for",
          "However, along with BERT, ELECTRA achieved": "0.66"
        },
        {
          "was 0.30. Parameter settings for all models are": "",
          "However, along with BERT, ELECTRA achieved": "0.49"
        },
        {
          "was 0.30. Parameter settings for all models are": "14 out of a total of 28 classes. The model did not",
          "However, along with BERT, ELECTRA achieved": ""
        },
        {
          "was 0.30. Parameter settings for all models are": "",
          "However, along with BERT, ELECTRA achieved": "0.50"
        },
        {
          "was 0.30. Parameter settings for all models are": "achieve  the  worst  performance  for  any  class  of",
          "However, along with BERT, ELECTRA achieved": ""
        },
        {
          "was 0.30. Parameter settings for all models are": "",
          "However, along with BERT, ELECTRA achieved": "0.68"
        },
        {
          "was 0.30. Parameter settings for all models are": "emotions.",
          "However, along with BERT, ELECTRA achieved": ""
        },
        {
          "was 0.30. Parameter settings for all models are": "",
          "However, along with BERT, ELECTRA achieved": "0.46"
        },
        {
          "was 0.30. Parameter settings for all models are": "Two  models  achieved  the  second-highest  f1-",
          "However, along with BERT, ELECTRA achieved": "0.19"
        },
        {
          "was 0.30. Parameter settings for all models are": "score (macro-average) but with different standard",
          "However, along with BERT, ELECTRA achieved": ""
        },
        {
          "was 0.30. Parameter settings for all models are": "",
          "However, along with BERT, ELECTRA achieved": "Table 4:  Results of different models for GoEmotion"
        },
        {
          "was 0.30. Parameter settings for all models are": "deviations. DistilBERT obtained the result of .48",
          "However, along with BERT, ELECTRA achieved": ""
        },
        {
          "was 0.30. Parameter settings for all models are": "",
          "However, along with BERT, ELECTRA achieved": ""
        },
        {
          "was 0.30. Parameter settings for all models are": "(std = .21) while XLNet obtained the result of .48",
          "However, along with BERT, ELECTRA achieved": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 4: summarizes our four models' gratitude 0.86 0.90 0.90 0.90 0.90",
      "data": [
        {
          "T4 GPU).": ""
        },
        {
          "T4 GPU).": ""
        },
        {
          "T4 GPU).": "Time to"
        },
        {
          "T4 GPU).": "complete"
        },
        {
          "T4 GPU).": "BERT"
        },
        {
          "T4 GPU).": "DistilBERT"
        },
        {
          "T4 GPU).": ""
        },
        {
          "T4 GPU).": "RoBERTa"
        },
        {
          "T4 GPU).": ""
        },
        {
          "T4 GPU).": "XLNet"
        },
        {
          "T4 GPU).": ""
        },
        {
          "T4 GPU).": "ELECTRA"
        },
        {
          "T4 GPU).": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "interesting  model.  It  was  a  quick  model  to  train": "",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": "Computational \nLinguistics, \npages \n4040–4054,"
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "(compared  to  the  others),  achieved  the  same  F1-",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": ""
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": "Stroudsburg, \nPA, \nUSA. \nAssociation \nfor"
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "score  as  larger  models  (XLNet),  and  surpassed",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": ""
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": "Computational Linguistics."
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "BERT.",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": ""
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": "Jacob  Devlin,  Ming-Wei  Chang,  Kenton  Lee,  and"
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": "Kristina  Toutanova.  2018.  BERT:  Pre-training  of"
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "4 \nConclusion",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": ""
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": "Deep  Bidirectional  Transformers \nfor  Language"
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "Emotion Recognition in texts is an essential task in",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": "Understanding. , October."
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "the  field  of  affective  computing. We  investigated",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": ""
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": "Yinhan  Liu,  Myle  Ott,  Naman  Goyal,  Jingfei  Du,"
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "different \nlanguage  models'  performance  when",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": "Mandar  Joshi,  Danqi  Chen,  Omer  Levy,  Mike"
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "using  GoEmotion,  a \nlarge  manually  annotated",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": "Lewis,  Luke  Zettlemoyer,  and  Veselin  Stoyanov."
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "dataset for fine-grained emotion.",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": "2019.  RoBERTa:  A  Robustly  Optimized  BERT"
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": "Pretraining Approach. , July."
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "We  used  BERT  as  our  baseline  model  and",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": ""
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "compared \nit  with  four  additional \ntransformers-",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": "Marco \nPolignano, \nPierpaolo \nBasile,  Marco \nde"
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "based  models  (DistillBERT,  RoBERTa,  XLNet,",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": "Gemmis, \nand  Giovanni \nSemeraro. \n2019.  A"
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "and ELECTRA). Except for the ELECTRA model,",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": "Comparison \nof  Word-Embeddings \nin  Emotion"
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": "Detection from Text using BiLSTM, CNN and Self-"
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "which  had \nthe  worst  F1-score  (.33), \nthe  other",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": ""
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": "Attention. \nIn  Adjunct  Publication  of \nthe  27th"
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "models \nhad  more \nsimilar \nresults.  RoBERTa",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": ""
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": "Conference  on  User  Modeling,  Adaptation  and"
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "achieved \nthe  best  F1-score \n(.49), \nfollowed  by",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": ""
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": "Personalization - UMAP'19 Adjunct, pages 63–68,"
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "DistillBERT  (.48),  XLNet  (.48),  and  then  BERT",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": ""
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": "New York, New York, USA. ACM Press."
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "(.46).  However,  when  we  look  at  the  metric  of",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": ""
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": "Victor  Sanh,  Lysandre  Debut,  Julien  Chaumond,  and"
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "computational cost and time to complete, we argue",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": ""
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": "Thomas Wolf. 2019. DistilBERT, a distilled version"
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "that the DistillBERT model is the most efficient for",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": ""
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": "of  BERT:  smaller,  faster,  cheaper  and \nlighter. \n,"
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "this type of task.",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": ""
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": "October."
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "The  results  show  us  that  we  still  have  much",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": ""
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": "Klaus  R.  Scherer  and  Harald  G.  Wallbott.  1994."
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "room to improve both models and create datasets",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": ""
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": "Evidence for universality and cultural variation of"
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "for fine-grained emotions.",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": ""
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": "differential emotion response patterning. Journal of"
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": "Personality and Social Psychology, 66(2):310–328."
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "5 \nReferences",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": ""
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": "Ashish  Vaswani,  Noam  Shazeer,  Niki  Parmar,  Jakob"
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "Acheampong \nFrancisca  Adoma,  Nunoo  Mensah",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": "Uszkoreit,  Llion  Jones, Aidan  N.  Gomez,  Lukasz"
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "Henry, \nand  Wenyu  Chen. \n2020.  Comparative",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": "Kaiser, and Illia Polosukhin. 2017. Attention Is All"
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "Analyses of Bert, Roberta, Distilbert, and Xlnet for",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": "You Need. , June."
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "Text-Based \nEmotion  Recognition. \n2020 \n17th",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": ""
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": "Zhilin  Yang,  Zihang  Dai,  Yiming  Yang, \nJaime"
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "International  Computer  Conference  on  Wavelet",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": ""
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": "Carbonell, Ruslan Salakhutdinov, and Quoc V. Le."
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "Active \nMedia \nTechnology \nand \nInformation",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": ""
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": "2019. \nXLNet: \nGeneralized \nAutoregressive"
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "Processing, \nICCWAMTIP  2020(November):117–",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": ""
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": "Pretraining for Language Understanding. , June."
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "121",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": ""
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "Erdenebileg Batbaatar, Meijing Li, and Keun Ho Ryu.",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": ""
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "2019. \nSemantic-Emotion  Neural  Network \nfor",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": ""
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "Emotion  Recognition  From  Text. \nIEEE  Access,",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": ""
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "7:111866–111878.",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": ""
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "Kevin  Clark,  Minh-Thang  Luong,  Quoc  V.  Le,  and",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": ""
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "Christopher  D.  Manning.  2020.  ELECTRA:  Pre-",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": ""
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "training  Text  Encoders  as  Discriminators  Rather",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": ""
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "Than Generators. , March.",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": ""
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "Alan  S.  Cowen  and  Dacher  Keltner.  2021.  Semantic",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": ""
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "Space  Theory:  A  Computational  Approach \nto",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": ""
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "Emotion. Trends in Cognitive Sciences, 25(2):124–",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": ""
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "136, February.",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": ""
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "Dorottya \nDemszky, \nDana \nMovshovitz-Attias,",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": ""
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "Jeongwoo  Ko, Alan  Cowen,  Gaurav  Nemade,  and",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": ""
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "Sujith Ravi. 2020. GoEmotions: A Dataset of Fine-",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": ""
        },
        {
          "interesting  model.  It  was  a  quick  model  to  train": "Grained  Emotions. \nIn  Proceedings  of \nthe  58th",
          "Annual \nMeeting \nof \nthe \nAssociation \nfor": ""
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Comparative Analyses of Bert, Roberta, Distilbert, and Xlnet for Text-Based Emotion Recognition",
      "authors": [
        "References Acheampong",
        "Francisca Adoma",
        "Nunoo Mensah Henry",
        "Wenyu Chen"
      ],
      "year": "2020",
      "venue": "17th International Computer Conference on Wavelet Active Media Technology and Information Processing"
    },
    {
      "citation_id": "2",
      "title": "Semantic-Emotion Neural Network for Emotion Recognition From Text",
      "authors": [
        "Erdenebileg Batbaatar",
        "Meijing Li",
        "Keun Ho"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "3",
      "title": "ELECTRA: Pretraining Text Encoders as Discriminators Rather Than Generators",
      "authors": [
        "Kevin Clark",
        "Minh-Thang Luong",
        "Quoc Le",
        "Christopher Manning"
      ],
      "year": "2020",
      "venue": "ELECTRA: Pretraining Text Encoders as Discriminators Rather Than Generators"
    },
    {
      "citation_id": "4",
      "title": "Semantic Space Theory: A Computational Approach to Emotion",
      "authors": [
        "Alan Cowen",
        "Dacher Keltner"
      ],
      "year": "2021",
      "venue": "Trends in Cognitive Sciences"
    },
    {
      "citation_id": "5",
      "title": "GoEmotions: A Dataset of Fine-Grained Emotions",
      "authors": [
        "Dorottya Demszky",
        "Dana Movshovitz-Attias",
        "Jeongwoo Ko",
        "Alan Cowen",
        "Gaurav Nemade",
        "Sujith Ravi"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "6",
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2018",
      "venue": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
    },
    {
      "citation_id": "7",
      "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "year": "2019",
      "venue": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
    },
    {
      "citation_id": "8",
      "title": "A Comparison of Word-Embeddings in Emotion Detection from Text using BiLSTM, CNN and Self-Attention",
      "authors": [
        "Marco Polignano",
        "Pierpaolo Basile",
        "Marco De Gemmis",
        "Giovanni Semeraro"
      ],
      "year": "2019",
      "venue": "Adjunct Publication of the 27th Conference on User Modeling, Adaptation and Personalization -UMAP'19 Adjunct"
    },
    {
      "citation_id": "9",
      "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
      "authors": [
        "Victor Sanh",
        "Lysandre Debut",
        "Julien Chaumond",
        "Thomas Wolf"
      ],
      "year": "2019",
      "venue": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"
    },
    {
      "citation_id": "10",
      "title": "Evidence for universality and cultural variation of differential emotion response patterning",
      "authors": [
        "Klaus Scherer",
        "Harald Wallbott"
      ],
      "year": "1994",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "11",
      "title": "Attention Is All You Need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Lukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Attention Is All You Need"
    },
    {
      "citation_id": "12",
      "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding",
      "authors": [
        "Zhilin Yang",
        "Zihang Dai",
        "Yiming Yang",
        "Jaime Carbonell",
        "Ruslan Salakhutdinov",
        "V Quoc",
        "Le"
      ],
      "year": "2019",
      "venue": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"
    }
  ]
}