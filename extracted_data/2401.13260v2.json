{
  "paper_id": "2401.13260v2",
  "title": "Mf-Aed-Aec: Speech Emotion Recognition By Leveraging Multimodal Fusion, Asr Error Detection, And Asr Error Correction",
  "published": "2024-01-24T06:55:55Z",
  "authors": [
    "Jiajun He",
    "Xiaohan Shi",
    "Xingfeng Li",
    "Tomoki Toda"
  ],
  "keywords": [
    "speech emotion recognition",
    "multi-modal fusion",
    "ASR error detection",
    "ASR error correction"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The prevalent approach in speech emotion recognition (SER) involves integrating both audio and textual information to comprehensively identify the speaker's emotion, with the text generally obtained through automatic speech recognition (ASR). An essential issue of this approach is that ASR errors from the text modality can worsen the performance of SER. Previous studies have proposed using an auxiliary ASR error detection task to adaptively assign weights of each word in ASR hypotheses. However, this approach has limited improvement potential because it does not address the coherence of semantic information in the text. Additionally, the inherent heterogeneity of different modalities leads to distribution gaps between their representations, making their fusion challenging. Therefore, in this paper, we incorporate two auxiliary tasks, ASR error detection (AED) and ASR error correction (AEC), to enhance the semantic coherence of ASR text, and further introduce a novel multimodal fusion (MF) method to learn shared representations across modalities. We refer to our method as MF-AED-AEC. Experimental results indicate that MF-AED-AEC significantly outperforms the baseline model by a margin of 4.1%.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Multimodal speech emotion recognition (SER) aims to identify and comprehend human emotions by integrating information from various perceptual modalities, such as audio, text, video, and images. SER has gained extensive attention due to its wide-ranging applications in fields like human-computer interaction, healthcare, and intelligent customer service  [1, 2] .\n\nIn this paper, we focus on the two most common modalities (speech and text) for multimodal SER. For the speech modality, early research focused on extracting low-level features such as Melfrequency cepstral coefficients (MFCCs) and filter banks (FBanks), or handcrafted features  [3] . Recently, with the advancement of deep learning, recurrent neural networks (RNNs)  [4] , convolutional neural networks (CNNs)  [5] , and transformer models  [6]  have significantly improved the performance of SER  [7] . Furthermore, with the notable success of self-supervised learning (SSL), speech-based pretrained models like wav2vec  [8] , HuBERT  [9] , and WavLM  [10]  have achieved state-of-the-art (SOTA) performance in SER.\n\nFor the text modality, while text-based pretrained models such as BERT  [11] , DeBERTa  [12] , and RoBERTa  [13]  have demonstrated excellent performance in multimodal SER tasks, the accu-These authors annotated with ⋆ contributed equally to this work. racy of automatic speech recognition (ASR) results is equally crucial for achieving accurate SER result  [14, 15] . To mitigate the impact of ASR errors, Santoso et al. combined self-attention mechanisms with word-level confidence measures to reduce the importance of high-error probability words, thus improving SER performance  [16] . However, this approach is heavily reliant on the performance of the ASR system and lacks generalization ability. Additionally, Lin et al. introduced an auxiliary ASR error detection task to determine the probabilities of erroneous words, indicating how much trust should be placed in each word of ASR hypotheses, thereby enhancing multimodal SER performance  [14] . However, this method solely focuses on error detection in ASR hypotheses without correction, which means it does not improve the coherence of semantic information. Therefore, it has certain limitations in enhancing SER results.\n\nOn the other hand, for multimodal SER tasks, how to perform multimodal fusion between the speech and text modalities is another key focus. Previous approaches have included simple feature concatenation  [17] , CNNs  [18] , and cross-modal attention  [19] , among others. While these methods have shown some effectiveness, they often face challenges arising from the representation gap between different modalities. In recent multimodal tasks like speech recognition  [20]  and sentiment analysis  [21] [22] [23] , researchers have proposed learning two distinct representations to enhance multimodal learning. The first representation is modality-invariant, mapping all modalities of the same utterance into a shared subspace with distributional alignment. This captures the commonalities of multimodal signals and the shared motives and goals of the speaker, which influence the emotional state of the utterance. Additionally, they learn modality-specific representations tailored to each modality. Combining these two representations provides a comprehensive perspective on multimodal data for downstream tasks  [24] .\n\nMotivated by these aforementioned observations, we propose a multi-task learning approach for multimodal SER based on two auxiliary tasks: ASR error detection (AED) and ASR error correction (AEC). Specifically, we introduce an AED module to identify the locations of ASR errors. Subsequently, we employ an AEC module to correct these errors, thereby reducing the impact of ASR errors on SER tasks. In addition, we also design a multimodal fusion module to learn modality-specific and modality-invariant representations within a shared audio-textual modality space. These two representations are then fused and utilized for downstream SER tasks. Empirical results demonstrate the effectiveness of this approach. In summary, our main contributions are as follows:\n\n• We introduce a multi-task learning method by incorporating two auxiliary tasks focused on ASR error detection and correction. This approach improves the coherence of semantic information",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Meaning Of The Icons:",
      "text": "h S , i ∈ {1 ,2, ..., m}\n\nh T , j ∈ {1 ,2, ..., n}\n\nFig.  1 . Overall architecture of the proposed MF-AED-AEC model.\n\nof ASR hypotheses, thereby augmenting SER performance. • We propose a multi-modal fusion approach that leverages shared modality characteristics to bridge the gap between heterogeneous modalities, effectively enhancing SER performance. • Our proposed MF-AED-AEC method outperforms previous baseline models by a large margin on the IEMOCAP dataset.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Proposed Method",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Problem Formulation",
      "text": "The multimodal multi-task SER challenge we address can be expressed as the function f (S, T ) = (L, C), where the speech modality S = (s1, s2, • • • , sm) consists of m frames extracted from an utterance. The text modality T = (t1, t2, • • • , tn) represents the original ASR hypotheses of an utterance, comprising n tokens. All tokens are mapped to a predefined WordPiece vocabulary  [25] . Additionally, operating within our multitask learning framework, the primary task focuses on emotion classification, yielding output L ∈ {l1, l2, • • • , le}, where e represents the emotional categories. Concurrently, the auxiliary tasks encompass AED and AEC. The outcome of these auxiliary tasks is C = (c1, c2, • • • , cp), denoting the human-annotated transcripts of the utterance, comprising p tokens.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Embedding Module",
      "text": "Our embedding module consists of acoustic embedding and token embedding, as illustrated in Fig.  1 . In this section, we provide a detailed explanation. Contextual Speech Representations. To acquire comprehensive contextual representations of acoustic features, we leverage a pretrained SSL model, HuBERT  [9]  as our acoustic encoder. Hu-BERT employs a combination of CNN layers and a transformer encoder to capture both speech features and contextual context. We adopt HS = (h\n\nS ) to symbolize the acoustic hidden representations generated through HuBERT.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Contextual Token Representations. The Hidden Representations Ht",
      "text": "T ) of the model inputs T are obtained by using the pretrained language model BERT  [11]  as our text encoder.\n\nwhere TE and PE denote the token embedding and position embedding, respectively.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Asr Error Detection (Aed) Module",
      "text": "Similarly to  [26] , we align T and C by determining the longest common subsequence (LCS) between them. The aligned tokens are labeled KEEP (K), whereas the remaining tokens are labeled DELETE (D) or CHANGE (C). The label prediction layer is a straightforward fully connected (FC) network with three classes.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "P (Yo|H",
      "text": "where h\n\n(o) T ∈ HT and yo, o ∈ {1, • • • , n} are the output of the BERT encoder and predicted labeling operations, respectively.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Asr Error Correction (Aec) Module",
      "text": "Our decoder operates in parallel to the tokens predicted as C. For the k th change position, the decoding sequence can be represented as\n\n), where d is the length of the decoding sequence, generated by the transformer decoder. We compute the decoder inputs at step t as follows:\n\nwhere TE and PE are the same token embedding and position embedding as in Eq. (  1 ), respectively. z 1,k is initialized by a special start token <BOS>. h\n\nT is the output of the BERT encoder at the k th change position. \"⊕\" denotes a concatenate function. Then, a generic transformer decoder is applied to obtain the decoder layer output, where the query input Q is the decoder input. Both the key input K and the value input V are the hidden representations of the BERT encoder  [27] :\n\nwhere\n\nt+1,k is the decoder layer output. Finally, the generation output is calculated as:\n\n(5)",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Multimodal Fusion (Mf) Module",
      "text": "Inspired by  [20] , our multimodal fusion (MF) module is composed of two cross-modal encoder (CME) blocks and one modalityinvariant representations (MIR) block. The objective is to facilitate the learning of modality-specific representations and modalityinvariant representations. In this section, we provide an in-depth explanation of the operation of each CME block and the MIR block. CME Block is structured akin to a standard transformer layer, featuring an h-head cross-attention module  [28] , residual connections, and FC layers. In order to acquire speech-aware token representations and token-aware speech representations, we utilize two CME blocks. This is achieved by employing HT (or HS) as queries and HS (or HT ) as keys and values within each CME block.\n\nMIR Block utilizes a hybrid-modal attention (HMA) module to extract the shared information from each modality-specific representation that pertains to both modalities:\n\nwhere i denotes either the speech or text modality, and HST signifies the concatenation of HS and HT , comprising both speech and text information. The resulting features are subsequently summed with HST , culminating in the ultimate modality-invariant representation:\n\nwhere the \"Norm\" represents layer normalization  [29] , \"Conv 1d \" denotes 1×1 convolution followed by PReLU activation  [30] .\n\nHMA Block initiates with a cross-attention layer aimed at extracting the shared information from each modality-specific representation, pertinent to both modalities:\n\n(10) To enhance the feature's modality invariance, a parallel convolutional network is employed to learn a mask that filters out modalityspecific information:  (11)  where \"σ\" denotes Sigmoid activation and \"⊗\" indicates elementwise multiplication. Finally, the modality-specific and modalityinvariant representations are concatenated together to get the final multimodal fusion representations H (fus) ST :",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emotion Classification Module",
      "text": "The emotion classification is performed by applying the temporal average pooling layer on the output feature H (fus)\n\nST of the MF module, followed by an FC layer and a SoftMax activation function.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "P (Yemo|H (Fus)",
      "text": "ST ) = SoftMax(FC(AvgPooling(H (fus) ST ))),  (13)  where yemo is the predicted emotion classification.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Joint Training",
      "text": "The learning process is optimized through three cross entropy loss functions that correspond to emotion classification, AED, and AEC. Lossemo = -log(P (yemo|H\n\n)\n\nLosse = -\n\nThese three loss functions are linearly combined as the overall objective during the training stage:\n\nwhere γ is the hyperparameter for adjusting the weight between Loss d and Losse and β is the hyperparameter for adjusting the weight between main task and auxiliary tasks.\n\nDuring the inference stage, the AED and AEC modules are excluded. The remaining network accepts speech data and ASR hypotheses as input and outputs emotion classification results.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments And Results",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiment Settings",
      "text": "Our method was implemented using Python 3.10.0 and Pytorch 1.11.0. The model was trained and evaluated on a computer with Intel(R) Xeon(R) Gold 6248 CPU @ 2.50GHz, 32GB RAM and one NVIDIA Tesla V100 GPU. The acoustic encoder was initialized using hubert-base-ls960, ultimately yielding acoustic representations characterized by a dimensionality of 768. The text encoder employed bert-base-uncased model for initialization. The vocabulary size for word tokenization was set to 30522. We set the hidden size as 768, the number of attention layers as 12, and the number of attention heads as 12. Both the HuBERT model and the BERT model were finetuned during the training stage. The transformer decoder adopted a single-layer transformer with a hidden size of 768. We used Adam  [31]  as the optimizer with a batch size of 16. For training, we kept the learning rate constant at 1e -5 , which worked well for all our configurations. For our multi-task learning setup, we set γ to 3 and set β to 0.1. To evaluate the classification performance, we used unweighted average recall (UAR).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dataset",
      "text": "To evaluate the effectiveness of our proposed model, we carried out experiments on the Interactive Emotional Dyadic Motion Capture (IEMOCAP) dataset  [32]  1 , which comprised roughly 12 hours of speech from ten speakers participating in five scripted sessions. Consistent with prior work in SER, we employed 5531 utterances that were categorized into four emotion categories: \"neutral\" (1708), \"angry\" (1103), \"happy\" (including \"excited\") (595 + 1041), and \"sad\" (1084). Likewise, we conducted experiments with the 5-fold leave-one-session-out cross-validation.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Results And Analysis",
      "text": "Our experimental results are shown in Table  1 , where we conduct experiments on both single-modal and multi-modal setups. For the single-modal experiments, we select the HuBERT model and the BERT model as speech and text baseline models, respectively. The text includes human-annotated transcripts and ASR hypotheses obtained from the \"openai/whisper-medium.en\" in the Whisper ASR model  [33]  with a word error rate (WER) of 20.48% on the IEMO-CAP dataset. All baseline models employ the same emotion classification module, as depicted in Fig.  1 (e) . Our proposed single-modal model operates on the text modality, achieving emotion classification through multi-task joint training involving the emotion classification task, ASR error detection task, and ASR error correction task. It is evident that while our proposed model's UAR is lower than that of the single-speech modality, the disparity when compared to using transcripts is merely 0.1%. Moreover, our proposed model's UAR is 1.2% higher than the UAR from single-task learning. Furthermore, in the multi-modal setup, we choose the baseline model that combines the aforementioned acoustic and text representations. We applied average pooling operations separately to the acoustic and text representations and concatenate them, followed by an FC layer and SoftMax activation function to compute emotion classification results. Evidently, the multi-modal baseline model demonstrates superior performance in contrast to the single-modal models, thereby highlighting the benefits derived from utilizing multi-modal inputs. Our proposed MF-AED-AEC achieves the best performance, even surpassing the UAR value achieved using human-annotated transcripts (75.5%) by a large margin of 3.8%, demonstrating the marked effectiveness of our model.\n\nThe differences between our proposed MF-AEC-AED model and the baseline model are mainly reflected in three modules: the AED module, the AEC module, and the MF module. To determine the impact of these modules on emotion recognition task performance, we conduct a series of ablation experiments, and the results of these ablation studies are presented in Table  2 .\n\nFirstly, we discuss the impact of the AED module. Specifically, we remove the AED module and only introduce the AEC module as an auxiliary task. This requires the model to correct errors in each utterance from scratch, rather than solely correcting errors detected. It is noticeable that in both single-modal and multi-modal models, the absence of the AED module leads to a notable decrease in UAR results. The UAR of the single-modal model decreases by 1.3%, while the UAR of the multi-modal model experiences a decline of 1.5%. This demonstrates the necessity of the AED module. Additionally, we note that the UAR result (66.3%) with only the AEC module in the single-modal case is even worse than the baseline result without this module (66.4%). This phenomenon occurs because directly using a neural machine translation (NMT) model for AEC can even increase the WER  [34] . Unlike NMT, which often requires modification of almost all input tokens, AEC involves fewer modifications but is more challenging. Hence, we need to consider the characteristics of ASR output and thoughtfully design models for AEC, which is why we introduced the two auxiliary tasks of AED and AEC. Secondly, we discuss the impact of the AEC module. Similarly, for both single-modal and multi-modal models, without the AEC module, there is a significant decrease in UAR results. The singlemodal model's UAR drops by 1.0%, and the multi-modal model's UAR drops by 1.1%, demonstrating the necessity of the AEC module. This module improves emotion recognition performance by correcting erroneous text positions. Finally, we discuss the impact of the MF module. Specifically, in the absence of the MF module, we apply average pooling operations separately to acoustic and text representations, followed by a concatenation operation. It can be seen that without the MF module, UAR performance decreases by 1.9%, demonstrating the effectiveness of our multi-modal fusion module. Interestingly, even without MF module, the UAR result (77.4%) is higher than that obtained with transcripts (75.5%). The higher performance may be attributed to the AED and AEC modules enhancing generalization, particularly valuable in the multi-modal scenario. These modules enable the model to better handle variations and nuances present in real transcriptions compared to the single modality setup.\n\nIn conclusion, through ablation experiments, we confirm the contributions of the AED, AEC, and MF modules in enhancing the performance of emotion recognition tasks. The introduction and integration of these modules enable our MF-AEC-AED model to achieve superior performance in multi-modal SER tasks, showcasing its flexibility and robustness, especially in the presence of ASR errors.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we propose MF-AEC-AED, a novel multimodal multi-task SER method. MF-AEC-AED leverages a novel multimodal fusion network to learn modality-specific representations and modality-invariant representations. In addition, we design two auxiliary tasks, namely AED and AEC, aimed at enhancing the coherence of semantic information within the text modality. Results on the IEMOCAP dataset validate the effectiveness of the proposed method to ASR errors. In the future, we plan to introduce auxiliary tasks of visual modality and contrastive learning to further improve the performance of emotion recognition.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overall architecture of the proposed MF-AED-AEC model.",
      "page": 2
    },
    {
      "caption": "Figure 1: In this section, we provide a",
      "page": 2
    },
    {
      "caption": "Figure 1: (e). Our proposed single-modal",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "‡ Information Technology Center, Nagoya University": "ABSTRACT"
        },
        {
          "‡ Information Technology Center, Nagoya University": ""
        },
        {
          "‡ Information Technology Center, Nagoya University": "The prevalent approach in speech emotion recognition (SER)\nin-"
        },
        {
          "‡ Information Technology Center, Nagoya University": ""
        },
        {
          "‡ Information Technology Center, Nagoya University": "volves\nintegrating both audio and textual\ninformation to compre-"
        },
        {
          "‡ Information Technology Center, Nagoya University": ""
        },
        {
          "‡ Information Technology Center, Nagoya University": "hensively identify the speaker’s emotion, with the text generally ob-"
        },
        {
          "‡ Information Technology Center, Nagoya University": ""
        },
        {
          "‡ Information Technology Center, Nagoya University": "tained through automatic speech recognition (ASR). An essential is-"
        },
        {
          "‡ Information Technology Center, Nagoya University": ""
        },
        {
          "‡ Information Technology Center, Nagoya University": "sue of this approach is that ASR errors from the text modality can"
        },
        {
          "‡ Information Technology Center, Nagoya University": ""
        },
        {
          "‡ Information Technology Center, Nagoya University": "worsen the performance of SER. Previous\nstudies have proposed"
        },
        {
          "‡ Information Technology Center, Nagoya University": ""
        },
        {
          "‡ Information Technology Center, Nagoya University": "using an auxiliary ASR error detection task to adaptively assign"
        },
        {
          "‡ Information Technology Center, Nagoya University": ""
        },
        {
          "‡ Information Technology Center, Nagoya University": "weights of each word in ASR hypotheses. However,\nthis approach"
        },
        {
          "‡ Information Technology Center, Nagoya University": ""
        },
        {
          "‡ Information Technology Center, Nagoya University": "has limited improvement potential because it does not address the"
        },
        {
          "‡ Information Technology Center, Nagoya University": ""
        },
        {
          "‡ Information Technology Center, Nagoya University": "coherence of\nsemantic information in the text.\nAdditionally,\nthe"
        },
        {
          "‡ Information Technology Center, Nagoya University": ""
        },
        {
          "‡ Information Technology Center, Nagoya University": "inherent heterogeneity of different modalities leads to distribution"
        },
        {
          "‡ Information Technology Center, Nagoya University": ""
        },
        {
          "‡ Information Technology Center, Nagoya University": "gaps between their representations, making their fusion challenging."
        },
        {
          "‡ Information Technology Center, Nagoya University": ""
        },
        {
          "‡ Information Technology Center, Nagoya University": "Therefore, in this paper, we incorporate two auxiliary tasks, ASR er-"
        },
        {
          "‡ Information Technology Center, Nagoya University": ""
        },
        {
          "‡ Information Technology Center, Nagoya University": "ror detection (AED) and ASR error correction (AEC), to enhance the"
        },
        {
          "‡ Information Technology Center, Nagoya University": ""
        },
        {
          "‡ Information Technology Center, Nagoya University": "semantic coherence of ASR text, and further introduce a novel multi-"
        },
        {
          "‡ Information Technology Center, Nagoya University": ""
        },
        {
          "‡ Information Technology Center, Nagoya University": "modal\nfusion (MF) method to learn shared representations across"
        },
        {
          "‡ Information Technology Center, Nagoya University": ""
        },
        {
          "‡ Information Technology Center, Nagoya University": "modalities. We refer to our method as MF-AED-AEC. Experimen-"
        },
        {
          "‡ Information Technology Center, Nagoya University": ""
        },
        {
          "‡ Information Technology Center, Nagoya University": "tal results indicate that MF-AED-AEC significantly outperforms the"
        },
        {
          "‡ Information Technology Center, Nagoya University": ""
        },
        {
          "‡ Information Technology Center, Nagoya University": "baseline model by a margin of 4.1%."
        },
        {
          "‡ Information Technology Center, Nagoya University": ""
        },
        {
          "‡ Information Technology Center, Nagoya University": "Index Terms— speech emotion recognition, multi-modal\nfu-"
        },
        {
          "‡ Information Technology Center, Nagoya University": ""
        },
        {
          "‡ Information Technology Center, Nagoya University": "sion, ASR error detection, ASR error correction"
        },
        {
          "‡ Information Technology Center, Nagoya University": ""
        },
        {
          "‡ Information Technology Center, Nagoya University": ""
        },
        {
          "‡ Information Technology Center, Nagoya University": "1.\nINTRODUCTION"
        },
        {
          "‡ Information Technology Center, Nagoya University": ""
        },
        {
          "‡ Information Technology Center, Nagoya University": "Multimodal speech emotion recognition (SER) aims to identify and"
        },
        {
          "‡ Information Technology Center, Nagoya University": "comprehend human emotions by integrating information from var-"
        },
        {
          "‡ Information Technology Center, Nagoya University": "ious perceptual modalities, such as audio,\ntext, video, and images."
        },
        {
          "‡ Information Technology Center, Nagoya University": "SER has gained extensive attention due to its wide-ranging appli-"
        },
        {
          "‡ Information Technology Center, Nagoya University": "cations in fields like human-computer\ninteraction, healthcare, and"
        },
        {
          "‡ Information Technology Center, Nagoya University": "intelligent customer service [1, 2]."
        },
        {
          "‡ Information Technology Center, Nagoya University": "In this paper, we focus on the two most common modalities"
        },
        {
          "‡ Information Technology Center, Nagoya University": "(speech and text)\nfor multimodal SER. For\nthe speech modality,"
        },
        {
          "‡ Information Technology Center, Nagoya University": "early research focused on extracting low-level features such as Mel-"
        },
        {
          "‡ Information Technology Center, Nagoya University": "frequency cepstral coefficients (MFCCs) and filter banks (FBanks),"
        },
        {
          "‡ Information Technology Center, Nagoya University": "or handcrafted features [3]. Recently, with the advancement of deep"
        },
        {
          "‡ Information Technology Center, Nagoya University": "learning, recurrent neural networks (RNNs) [4], convolutional neu-"
        },
        {
          "‡ Information Technology Center, Nagoya University": "ral networks (CNNs)\n[5], and transformer models [6] have signif-"
        },
        {
          "‡ Information Technology Center, Nagoya University": "icantly improved the performance of SER [7].\nFurthermore, with"
        },
        {
          "‡ Information Technology Center, Nagoya University": "the notable success of self-supervised learning (SSL), speech-based"
        },
        {
          "‡ Information Technology Center, Nagoya University": "pretrained models like wav2vec [8], HuBERT [9], and WavLM [10]"
        },
        {
          "‡ Information Technology Center, Nagoya University": "have achieved state-of-the-art (SOTA) performance in SER."
        },
        {
          "‡ Information Technology Center, Nagoya University": "For the text modality, while text-based pretrained models such"
        },
        {
          "‡ Information Technology Center, Nagoya University": "as BERT [11], DeBERTa [12],\nand RoBERTa [13] have demon-"
        },
        {
          "‡ Information Technology Center, Nagoya University": ""
        },
        {
          "‡ Information Technology Center, Nagoya University": "strated excellent performance in multimodal SER tasks,\nthe accu-"
        },
        {
          "‡ Information Technology Center, Nagoya University": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 1. Overall architecture of the proposed MF-AED-AEC model.": "of ASR hypotheses, thereby augmenting SER performance."
        },
        {
          "Fig. 1. Overall architecture of the proposed MF-AED-AEC model.": "• We propose a multi-modal fusion approach that\nleverages shared"
        },
        {
          "Fig. 1. Overall architecture of the proposed MF-AED-AEC model.": ""
        },
        {
          "Fig. 1. Overall architecture of the proposed MF-AED-AEC model.": "modality characteristics to bridge the gap between heterogeneous"
        },
        {
          "Fig. 1. Overall architecture of the proposed MF-AED-AEC model.": "modalities, effectively enhancing SER performance."
        },
        {
          "Fig. 1. Overall architecture of the proposed MF-AED-AEC model.": "• Our proposed MF-AED-AEC method outperforms previous base-"
        },
        {
          "Fig. 1. Overall architecture of the proposed MF-AED-AEC model.": "line models by a large margin on the IEMOCAP dataset."
        },
        {
          "Fig. 1. Overall architecture of the proposed MF-AED-AEC model.": ""
        },
        {
          "Fig. 1. Overall architecture of the proposed MF-AED-AEC model.": "2. PROPOSED METHOD"
        },
        {
          "Fig. 1. Overall architecture of the proposed MF-AED-AEC model.": ""
        },
        {
          "Fig. 1. Overall architecture of the proposed MF-AED-AEC model.": "2.1. Problem Formulation"
        },
        {
          "Fig. 1. Overall architecture of the proposed MF-AED-AEC model.": ""
        },
        {
          "Fig. 1. Overall architecture of the proposed MF-AED-AEC model.": "The multimodal multi-task SER challenge we address can be ex-"
        },
        {
          "Fig. 1. Overall architecture of the proposed MF-AED-AEC model.": ""
        },
        {
          "Fig. 1. Overall architecture of the proposed MF-AED-AEC model.": "pressed as the function f (S, T ) = (L, C), where the speech modal-"
        },
        {
          "Fig. 1. Overall architecture of the proposed MF-AED-AEC model.": ""
        },
        {
          "Fig. 1. Overall architecture of the proposed MF-AED-AEC model.": "ity S = (s1, s2, · · ·\n, sm) consists of m frames extracted from an"
        },
        {
          "Fig. 1. Overall architecture of the proposed MF-AED-AEC model.": ""
        },
        {
          "Fig. 1. Overall architecture of the proposed MF-AED-AEC model.": "utterance.\nThe text modality T = (t1, t2, · · ·\n, tn) represents the"
        },
        {
          "Fig. 1. Overall architecture of the proposed MF-AED-AEC model.": ""
        },
        {
          "Fig. 1. Overall architecture of the proposed MF-AED-AEC model.": "original ASR hypotheses of an utterance, comprising n tokens. All"
        },
        {
          "Fig. 1. Overall architecture of the proposed MF-AED-AEC model.": "tokens are mapped to a predefined WordPiece vocabulary [25]. Ad-"
        },
        {
          "Fig. 1. Overall architecture of the proposed MF-AED-AEC model.": ""
        },
        {
          "Fig. 1. Overall architecture of the proposed MF-AED-AEC model.": "ditionally, operating within our multitask learning framework,\nthe"
        },
        {
          "Fig. 1. Overall architecture of the proposed MF-AED-AEC model.": "primary task focuses on emotion classification, yielding output L ∈"
        },
        {
          "Fig. 1. Overall architecture of the proposed MF-AED-AEC model.": ""
        },
        {
          "Fig. 1. Overall architecture of the proposed MF-AED-AEC model.": "{l1, l2, · · ·\n, le}, where e represents the emotional categories. Con-"
        },
        {
          "Fig. 1. Overall architecture of the proposed MF-AED-AEC model.": ""
        },
        {
          "Fig. 1. Overall architecture of the proposed MF-AED-AEC model.": "currently,\nthe auxiliary tasks encompass AED and AEC. The out-"
        },
        {
          "Fig. 1. Overall architecture of the proposed MF-AED-AEC model.": "come of these auxiliary tasks is C = (c1, c2, · · ·\n, cp), denoting the"
        },
        {
          "Fig. 1. Overall architecture of the proposed MF-AED-AEC model.": "human-annotated transcripts of the utterance, comprising p tokens."
        },
        {
          "Fig. 1. Overall architecture of the proposed MF-AED-AEC model.": ""
        },
        {
          "Fig. 1. Overall architecture of the proposed MF-AED-AEC model.": "2.2. Embedding Module"
        },
        {
          "Fig. 1. Overall architecture of the proposed MF-AED-AEC model.": ""
        },
        {
          "Fig. 1. Overall architecture of the proposed MF-AED-AEC model.": "Our embedding module consists of acoustic embedding and token"
        },
        {
          "Fig. 1. Overall architecture of the proposed MF-AED-AEC model.": "embedding, as illustrated in Fig.\n1.\nIn this section, we provide a"
        },
        {
          "Fig. 1. Overall architecture of the proposed MF-AED-AEC model.": "detailed explanation."
        },
        {
          "Fig. 1. Overall architecture of the proposed MF-AED-AEC model.": "Contextual Speech Representations. To acquire comprehen-"
        },
        {
          "Fig. 1. Overall architecture of the proposed MF-AED-AEC model.": ""
        },
        {
          "Fig. 1. Overall architecture of the proposed MF-AED-AEC model.": "sive contextual\nrepresentations of acoustic features, we leverage a"
        },
        {
          "Fig. 1. Overall architecture of the proposed MF-AED-AEC model.": "pretrained SSL model, HuBERT [9] as our acoustic encoder. Hu-"
        },
        {
          "Fig. 1. Overall architecture of the proposed MF-AED-AEC model.": "BERT employs a combination of CNN layers and a transformer en-"
        },
        {
          "Fig. 1. Overall architecture of the proposed MF-AED-AEC model.": "coder\nto capture both speech features and contextual context. We"
        },
        {
          "Fig. 1. Overall architecture of the proposed MF-AED-AEC model.": ", h(2)\n, · · ·\n, h(m)\n) to symbolize the acoustic hid-\nadopt HS = (h(1)"
        },
        {
          "Fig. 1. Overall architecture of the proposed MF-AED-AEC model.": "S\nS\nS"
        },
        {
          "Fig. 1. Overall architecture of the proposed MF-AED-AEC model.": "den representations generated through HuBERT."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "output, where the query input Q is the decoder input. Both the key",
          "2.6. Emotion Classification Module": ""
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "",
          "2.6. Emotion Classification Module": "The emotion classification is performed by applying the temporal"
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "input K and the value input V are the hidden representations of the",
          "2.6. Emotion Classification Module": ""
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "",
          "2.6. Emotion Classification Module": "average pooling layer on the output feature H (fus)\nof the MF module,"
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "BERT encoder [27]:",
          "2.6. Emotion Classification Module": "ST"
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "",
          "2.6. Emotion Classification Module": "followed by an FC layer and a SoftMax activation function."
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "O(gen)\n(4)\nt+1,k = TransformerDecoder(H (z)\nt,k , HT , HT ),",
          "2.6. Emotion Classification Module": ""
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "",
          "2.6. Emotion Classification Module": ") = SoftMax(FC(AvgPooling(H (fus)\n))),\n(13)\nP (yemo|H (fus)\nST\nST"
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "where O(gen)\nlayer output.\nFinally,\nthe generation\nt+1,k is the decoder",
          "2.6. Emotion Classification Module": ""
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "output is calculated as:",
          "2.6. Emotion Classification Module": "where yemo is the predicted emotion classification."
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "P (gen)\n(5)\nt+1,k = SoftMax(FC(O(gen)\nt+1,k)).",
          "2.6. Emotion Classification Module": ""
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "",
          "2.6. Emotion Classification Module": "2.7.\nJoint Training"
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "",
          "2.6. Emotion Classification Module": "The learning process is optimized through three cross entropy loss"
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "2.5. Multimodal Fusion (MF) Module",
          "2.6. Emotion Classification Module": ""
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "",
          "2.6. Emotion Classification Module": "functions that correspond to emotion classification, AED, and AEC."
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "Inspired\nby\n[20],\nour multimodal\nfusion\n(MF) module\nis\ncom-",
          "2.6. Emotion Classification Module": ""
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "",
          "2.6. Emotion Classification Module": "(cid:88)"
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "posed of two cross-modal encoder (CME) blocks and one modality-",
          "2.6. Emotion Classification Module": "))\n(14)\nLossemo = −\nlog(P (yemo|H (fus)\nST"
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "invariant representations (MIR) block. The objective is to facilitate",
          "2.6. Emotion Classification Module": ""
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "the\nlearning\nof modality-specific\nrepresentations\nand modality-",
          "2.6. Emotion Classification Module": "(cid:88) o\n))\n(15)\nlog(P (yo|h(o)\nLossd = −\nT"
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "invariant\nrepresentations.\nIn this\nsection, we provide an in-depth",
          "2.6. Emotion Classification Module": ""
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "explanation of the operation of each CME block and the MIR block.",
          "2.6. Emotion Classification Module": ""
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "",
          "2.6. Emotion Classification Module": "(cid:88) k\n(cid:88) t\nlog(P (gen)\n).\nLosse = −"
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "",
          "2.6. Emotion Classification Module": "t,k\n(16)"
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "CME Block is structured akin to a standard transformer layer,",
          "2.6. Emotion Classification Module": ""
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "featuring an h-head cross-attention module [28],\nresidual connec-",
          "2.6. Emotion Classification Module": ""
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "",
          "2.6. Emotion Classification Module": "These three loss functions are linearly combined as the overall"
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "tions, and FC layers.\nIn order\nto acquire speech-aware token rep-",
          "2.6. Emotion Classification Module": ""
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "",
          "2.6. Emotion Classification Module": "objective during the training stage:"
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "resentations and token-aware speech representations, we utilize two",
          "2.6. Emotion Classification Module": ""
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "CME blocks. This is achieved by employing HT (or HS) as queries",
          "2.6. Emotion Classification Module": ""
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "",
          "2.6. Emotion Classification Module": "(17)\nLoss = Lossemo + β · (γ · Lossd + Losse),"
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "and HS (or HT ) as keys and values within each CME block.",
          "2.6. Emotion Classification Module": ""
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "",
          "2.6. Emotion Classification Module": "where γ is\nthe hyperparameter\nfor adjusting the weight between"
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "(6)\nQ = HS (or HT ), K = HT (or HS), V = HT (or HS)",
          "2.6. Emotion Classification Module": ""
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "",
          "2.6. Emotion Classification Module": "and β is\nthe hyperparameter\nfor\nadjusting the\nand Losse\nLossd"
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "",
          "2.6. Emotion Classification Module": "weight between main task and auxiliary tasks."
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "(or H (spe)\nH (spe)\n) = FC(Cross-Attention(Q, K, V )).\n(7)\nT\nS",
          "2.6. Emotion Classification Module": ""
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "",
          "2.6. Emotion Classification Module": "During the inference stage,\nthe AED and AEC modules are ex-"
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "MIR Block utilizes a hybrid-modal attention (HMA) module to",
          "2.6. Emotion Classification Module": "cluded. The remaining network accepts speech data and ASR hy-"
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "extract the shared information from each modality-specific represen-",
          "2.6. Emotion Classification Module": "potheses as input and outputs emotion classification results."
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "tation that pertains to both modalities:",
          "2.6. Emotion Classification Module": ""
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "",
          "2.6. Emotion Classification Module": "3. EXPERIMENTS AND RESULTS"
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "H (b)\n= HMA(H (spe)\n(8)\n, HST ), i ∈ {S, T },\ni\ni",
          "2.6. Emotion Classification Module": ""
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "where i denotes either the speech or text modality, and HST signifies",
          "2.6. Emotion Classification Module": "3.1. Experiment Settings"
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "the concatenation of HS and HT , comprising both speech and text",
          "2.6. Emotion Classification Module": ""
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "",
          "2.6. Emotion Classification Module": "Our method was\nimplemented using Python 3.10.0 and Pytorch"
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "information. The resulting features are subsequently summed with",
          "2.6. Emotion Classification Module": ""
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "",
          "2.6. Emotion Classification Module": "1.11.0.\nThe model was trained and evaluated on a computer with"
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "HST , culminating in the ultimate modality-invariant representation:",
          "2.6. Emotion Classification Module": ""
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "",
          "2.6. Emotion Classification Module": "Intel(R) Xeon(R) Gold 6248 CPU @ 2.50GHz, 32GB RAM and"
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "(cid:88)",
          "2.6. Emotion Classification Module": "one NVIDIA Tesla V100 GPU. The acoustic encoder was initialized"
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": ")),\nH (inv)\n(9)\n= Norm(HST +\nConv1d(H (b)\ni\nST",
          "2.6. Emotion Classification Module": ""
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "",
          "2.6. Emotion Classification Module": "using hubert-base-ls960,\nultimately yielding acoustic\nrepresenta-"
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "i∈{S,T }",
          "2.6. Emotion Classification Module": ""
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "",
          "2.6. Emotion Classification Module": "tions characterized by a dimensionality of 768.\nThe text encoder"
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "where the “Norm” represents layer normalization [29], “Conv1d”",
          "2.6. Emotion Classification Module": "employed bert-base-uncased model\nfor\ninitialization. The vocabu-"
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "denotes 1×1 convolution followed by PReLU activation [30].",
          "2.6. Emotion Classification Module": "lary size for word tokenization was set to 30522. We set the hidden"
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "HMA Block initiates with a cross-attention layer aimed at ex-",
          "2.6. Emotion Classification Module": "size as 768,\nthe number of attention layers as 12, and the number"
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "tracting the shared information from each modality-specific repre-",
          "2.6. Emotion Classification Module": "of attention heads as 12. Both the HuBERT model and the BERT"
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "sentation, pertinent to both modalities:",
          "2.6. Emotion Classification Module": "model were finetuned during the training stage.\nThe transformer"
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "",
          "2.6. Emotion Classification Module": "decoder adopted a single-layer\ntransformer with a hidden size of"
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "H (share)\n, H (spe)\n), i ∈ {S, T }.\n= Cross-Attention(HST , H (spe)\ni\ni\ni",
          "2.6. Emotion Classification Module": "768. We used Adam [31] as\nthe optimizer with a batch size of"
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "(10)",
          "2.6. Emotion Classification Module": "16. For training, we kept\nthe learning rate constant at 1e−5, which"
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "To enhance the feature’s modality invariance, a parallel convolu-",
          "2.6. Emotion Classification Module": "worked well for all our configurations. For our multi-task learning"
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "tional network is employed to learn a mask that filters out modality-",
          "2.6. Emotion Classification Module": "setup, we set γ to 3 and set β to 0.1. To evaluate the classification"
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "specific information:",
          "2.6. Emotion Classification Module": "performance, we used unweighted average recall (UAR)."
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "H (b)\n= H (share)",
          "2.6. Emotion Classification Module": "3.2. Dataset"
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "(11)\n⊕ HST )), i ∈ {S, T },\n⊗σ(Conv1d(H (spe)\ni\ni\ni",
          "2.6. Emotion Classification Module": ""
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "",
          "2.6. Emotion Classification Module": "To evaluate the effectiveness of our proposed model, we carried out"
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "where “σ” denotes Sigmoid activation and “⊗” indicates element-",
          "2.6. Emotion Classification Module": ""
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "",
          "2.6. Emotion Classification Module": "experiments on the Interactive Emotional Dyadic Motion Capture"
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "wise multiplication.\nFinally,\nthe modality-specific and modality-",
          "2.6. Emotion Classification Module": ""
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "",
          "2.6. Emotion Classification Module": "(IEMOCAP) dataset\n[32]\n1, which comprised roughly 12 hours of"
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "invariant\nrepresentations are concatenated together\nto get\nthe final",
          "2.6. Emotion Classification Module": ""
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "",
          "2.6. Emotion Classification Module": "speech from ten speakers participating in five scripted sessions. Con-"
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "multimodal fusion representations H (fus)\n:",
          "2.6. Emotion Classification Module": ""
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "ST",
          "2.6. Emotion Classification Module": "sistent with prior work in SER, we employed 5531 utterances that"
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "H (fus)\n= H (spe)\n⊕ H (spe)\n⊕ H (inv)\n.\n(12)",
          "2.6. Emotion Classification Module": "1https://sail.usc.edu/iemocap/"
        },
        {
          "generic transformer decoder\nis applied to obtain the decoder\nlayer": "ST\nS\nT\nST",
          "2.6. Emotion Classification Module": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: , where we conduct",
      "data": [
        {
          "were categorized into four emotion categories:\n“neutral” (1708),": "“angry” (1103), “happy” (including “excited”)\n(595 + 1041), and",
          "the single-modal case is even worse than the baseline result without": "this module (66.4%). This phenomenon occurs because directly us-"
        },
        {
          "were categorized into four emotion categories:\n“neutral” (1708),": "“sad” (1084). Likewise, we conducted experiments with the 5-fold",
          "the single-modal case is even worse than the baseline result without": "ing a neural machine translation (NMT) model\nfor AEC can even"
        },
        {
          "were categorized into four emotion categories:\n“neutral” (1708),": "leave-one-session-out cross-validation.",
          "the single-modal case is even worse than the baseline result without": "increase the WER [34]. Unlike NMT, which often requires modifi-"
        },
        {
          "were categorized into four emotion categories:\n“neutral” (1708),": "",
          "the single-modal case is even worse than the baseline result without": "cation of almost all input tokens, AEC involves fewer modifications"
        },
        {
          "were categorized into four emotion categories:\n“neutral” (1708),": "3.3. Results and Analysis",
          "the single-modal case is even worse than the baseline result without": "but is more challenging. Hence, we need to consider the characteris-"
        },
        {
          "were categorized into four emotion categories:\n“neutral” (1708),": "",
          "the single-modal case is even worse than the baseline result without": "tics of ASR output and thoughtfully design models for AEC, which"
        },
        {
          "were categorized into four emotion categories:\n“neutral” (1708),": "Our experimental\nresults are shown in Table 1, where we conduct",
          "the single-modal case is even worse than the baseline result without": ""
        },
        {
          "were categorized into four emotion categories:\n“neutral” (1708),": "",
          "the single-modal case is even worse than the baseline result without": "is why we introduced the two auxiliary tasks of AED and AEC."
        },
        {
          "were categorized into four emotion categories:\n“neutral” (1708),": "experiments on both single-modal and multi-modal setups. For the",
          "the single-modal case is even worse than the baseline result without": ""
        },
        {
          "were categorized into four emotion categories:\n“neutral” (1708),": "single-modal experiments, we select\nthe HuBERT model and the",
          "the single-modal case is even worse than the baseline result without": ""
        },
        {
          "were categorized into four emotion categories:\n“neutral” (1708),": "",
          "the single-modal case is even worse than the baseline result without": "Table 2. Impacts of different modules on the proposed model."
        },
        {
          "were categorized into four emotion categories:\n“neutral” (1708),": "BERT model as speech and text baseline models, respectively. The",
          "the single-modal case is even worse than the baseline result without": ""
        },
        {
          "were categorized into four emotion categories:\n“neutral” (1708),": "text\nincludes human-annotated transcripts and ASR hypotheses ob-",
          "the single-modal case is even worse than the baseline result without": "Method\nModel\nModality\nUAR (%)"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 1: , where we conduct",
      "data": [
        {
          "tics of ASR output and thoughtfully design models for AEC, which": ""
        },
        {
          "tics of ASR output and thoughtfully design models for AEC, which": "is why we introduced the two auxiliary tasks of AED and AEC."
        },
        {
          "tics of ASR output and thoughtfully design models for AEC, which": ""
        },
        {
          "tics of ASR output and thoughtfully design models for AEC, which": ""
        },
        {
          "tics of ASR output and thoughtfully design models for AEC, which": "Table 2. Impacts of different modules on the proposed model."
        },
        {
          "tics of ASR output and thoughtfully design models for AEC, which": ""
        },
        {
          "tics of ASR output and thoughtfully design models for AEC, which": "Model"
        },
        {
          "tics of ASR output and thoughtfully design models for AEC, which": ""
        },
        {
          "tics of ASR output and thoughtfully design models for AEC, which": "Proposed"
        },
        {
          "tics of ASR output and thoughtfully design models for AEC, which": ""
        },
        {
          "tics of ASR output and thoughtfully design models for AEC, which": "w/o AED"
        },
        {
          "tics of ASR output and thoughtfully design models for AEC, which": ""
        },
        {
          "tics of ASR output and thoughtfully design models for AEC, which": "w/o AEC"
        },
        {
          "tics of ASR output and thoughtfully design models for AEC, which": ""
        },
        {
          "tics of ASR output and thoughtfully design models for AEC, which": "Proposed"
        },
        {
          "tics of ASR output and thoughtfully design models for AEC, which": "w/o AED"
        },
        {
          "tics of ASR output and thoughtfully design models for AEC, which": ""
        },
        {
          "tics of ASR output and thoughtfully design models for AEC, which": "w/o AEC"
        },
        {
          "tics of ASR output and thoughtfully design models for AEC, which": "w/o MF"
        },
        {
          "tics of ASR output and thoughtfully design models for AEC, which": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 1: , where we conduct",
      "data": [
        {
          "transcripts is merely 0.1%. Moreover, our proposed model’s UAR is": ""
        },
        {
          "transcripts is merely 0.1%. Moreover, our proposed model’s UAR is": "1.2% higher than the UAR from single-task learning."
        },
        {
          "transcripts is merely 0.1%. Moreover, our proposed model’s UAR is": ""
        },
        {
          "transcripts is merely 0.1%. Moreover, our proposed model’s UAR is": ""
        },
        {
          "transcripts is merely 0.1%. Moreover, our proposed model’s UAR is": "Table 1. Comparison results to baseline networks on IEMOCAP."
        },
        {
          "transcripts is merely 0.1%. Moreover, our proposed model’s UAR is": "Method\nModel\nModality\nUAR (%)"
        },
        {
          "transcripts is merely 0.1%. Moreover, our proposed model’s UAR is": "HuBERT\nSpeech\n69.8"
        },
        {
          "transcripts is merely 0.1%. Moreover, our proposed model’s UAR is": ""
        },
        {
          "transcripts is merely 0.1%. Moreover, our proposed model’s UAR is": "BERT\nText (Transcripts)\n67.7"
        },
        {
          "transcripts is merely 0.1%. Moreover, our proposed model’s UAR is": "Single-modal"
        },
        {
          "transcripts is merely 0.1%. Moreover, our proposed model’s UAR is": "BERT\nText (ASR)\n66.4"
        },
        {
          "transcripts is merely 0.1%. Moreover, our proposed model’s UAR is": "BERT + AED"
        },
        {
          "transcripts is merely 0.1%. Moreover, our proposed model’s UAR is": "Text (ASR)\n67.6"
        },
        {
          "transcripts is merely 0.1%. Moreover, our proposed model’s UAR is": "+ AEC (Proposed)"
        },
        {
          "transcripts is merely 0.1%. Moreover, our proposed model’s UAR is": ""
        },
        {
          "transcripts is merely 0.1%. Moreover, our proposed model’s UAR is": "HuBERT + BERT\nSpeech + Text (Transcripts)\n75.5"
        },
        {
          "transcripts is merely 0.1%. Moreover, our proposed model’s UAR is": ""
        },
        {
          "transcripts is merely 0.1%. Moreover, our proposed model’s UAR is": "Multi-modal\nHuBERT + BERT\nSpeech + Text (ASR)\n75.2"
        },
        {
          "transcripts is merely 0.1%. Moreover, our proposed model’s UAR is": "MF-AED-AEC (Proposed)\nSpeech + Text (ASR)\n79.3"
        },
        {
          "transcripts is merely 0.1%. Moreover, our proposed model’s UAR is": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "References": "",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": "attention based feature level fusion for speech emotion recog-"
        },
        {
          "References": "[1] M. J. Al-Dujaili and A. Ebrahimi-Moghadam,\n“Speech emo-",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": "nition,” Proc. ICASSP, pp. 6314–6318, 2021."
        },
        {
          "References": "tion recognition: a comprehensive survey,” Wireless Personal",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": ""
        },
        {
          "References": "",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": "[18]\nJ. Liu, Z. Liu, et al.,\n“Temporal attention convolutional net-"
        },
        {
          "References": "Communications, vol. 129, pp. 2525–2561, 2023.",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": ""
        },
        {
          "References": "",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": "work for\nspeech emotion recognition with latent\nrepresenta-"
        },
        {
          "References": "[2]\nJ. Tian, D. Hu, et al.,\n“Semi-supervised multimodal emotion",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": "tion,” Proc. Interspeech, pp. 2337–2341, 2020."
        },
        {
          "References": "recognition with consensus decision-making and label correc-",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": ""
        },
        {
          "References": "",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": "[19] D. Krishna and A. Patil,\n“Multimodal emotion recognition"
        },
        {
          "References": "tion,” Proc. MRAC, pp. 67–73, 2023.",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": ""
        },
        {
          "References": "",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": "using cross-modal attention and 1d convolutional neural net-"
        },
        {
          "References": "[3]\nS. Ghosh, U. Tyagi, et al.,\n“Mmer: Multimodal multi-task",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": "works,” Proc. Interspeech, pp. 4243–4247, 2020."
        },
        {
          "References": "learning for speech emotion recognition,”\nProc. Interspeech,",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": ""
        },
        {
          "References": "",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": "[20] Y. Hu, C. Chen,\net\nal.,\n“Mir-gan:\nRefining\nframe-level"
        },
        {
          "References": "pp. 1209–1213, 2023.",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": ""
        },
        {
          "References": "",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": "modality-invariant representations with adversarial network for"
        },
        {
          "References": "[4]\nP. Liu, K. Li, et al.,\n“Group gated fusion on attention-based",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": "audio-visual\nspeech recognition,”\nProc. ACL, pp. 11610–"
        },
        {
          "References": "bidirectional alignment\nfor multimodal emotion recognition,”",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": "11625, 2023."
        },
        {
          "References": "Proc. Interspeech, pp. 379–383, 2020.",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": ""
        },
        {
          "References": "",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": "[21] D. Hazarika, R. Zimmermann,\net\nal.,\n“Misa: Modality-"
        },
        {
          "References": "",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": "invariant\nand-specific\nrepresentations\nfor multimodal\nsenti-"
        },
        {
          "References": "[5] W. Fan, X. Xu, et al.,\n“Isnet:\nIndividual standardization net-",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": ""
        },
        {
          "References": "",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": "ment analysis,” Proc. ACMMM, pp. 1122–1131, 2020."
        },
        {
          "References": "IEEE/ACM Transac-\nwork for speech emotion recognition,”",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": ""
        },
        {
          "References": "tions on Audio, Speech, and Language Processing, vol. 30, pp.",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": ""
        },
        {
          "References": "",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": "[22] W. Yu, H. Xu, et al.,\n“Learning modality-specific representa-"
        },
        {
          "References": "1803–1814, 2022.",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": ""
        },
        {
          "References": "",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": "tions with self-supervised multi-task learning for multimodal"
        },
        {
          "References": "",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": "sentiment analysis,” Proc. AAAI, pp. 10790–10797, 2021."
        },
        {
          "References": "[6] W. Chen, X. Xing, et al.,\n“Speechformer: A hierarchical ef-",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": ""
        },
        {
          "References": "ficient framework incorporating the characteristics of speech,”",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": ""
        },
        {
          "References": "",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": "[23] Y. Yao and R. Mihalcea,\n“Modality-specific learning rates"
        },
        {
          "References": "Proc. Interspeech, pp. 346–350, 2022.",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": ""
        },
        {
          "References": "",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": "for effective multimodal additive late-fusion,” Proc. ACL, pp."
        },
        {
          "References": "",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": "1824–1834, 2022."
        },
        {
          "References": "[7] W. Fan, X. Xing,\net\nal.,\n“Mgat: Multi-granularity atten-",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": ""
        },
        {
          "References": "tion based transformers for multi-modal emotion recognition,”",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": ""
        },
        {
          "References": "",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": "[24] D. Yang, H. Kuang, et al.,\n“Learning modality-specific and-"
        },
        {
          "References": "Proc. ICASSP, pp. 1–5, 2023.",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": ""
        },
        {
          "References": "",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": "agnostic\nrepresentations\nfor\nasynchronous multimodal\nlan-"
        },
        {
          "References": "",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": "guage sequences,” Proc. ACMMM, pp. 1708–1717, 2022."
        },
        {
          "References": "[8]\nS. Schneider, A. Baevski, et al., “wav2vec: Unsupervised pre-",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": ""
        },
        {
          "References": "training for speech recognition,” Proc. Interspeech, pp. 3465–",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": "[25] Y. Wu, M. Schuster, et al.,\n“Google’s neural machine trans-"
        },
        {
          "References": "3469, 2019.",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": "lation system: Bridging the gap between human and machine"
        },
        {
          "References": "",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": "translation,” arXiv:1609.08144, 2016."
        },
        {
          "References": "[9] W.-N. Hsu, B. Bolte, et al.,\n“Hubert: Self-supervised speech",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": ""
        },
        {
          "References": "representation learning by masked prediction of hidden units,”",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": "[26]\nJ. He, Z. Yang, et al.,\n“ED-CEC: Improving rare word recog-"
        },
        {
          "References": "IEEE/ACM Transactions on Audio,\nSpeech,\nand Language",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": "nition using asr postprocessing based on error detection and"
        },
        {
          "References": "Processing, vol. 29, pp. 3451–3460, 2021.",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": "context-aware error correction,” Proc. ASRU, pp. 1–6, 2023."
        },
        {
          "References": "[10]\nS. Chen, C. Wang, et al., “Wavlm: Large-scale self-supervised",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": "[27] A. Vaswani, N. Shazeer, et al.,\n“Attention is all you need,”"
        },
        {
          "References": "pre-training for full stack speech processing,” IEEE Journal of",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": "Proc. NeurIPS, vol. 30, pp. 6000–6010, 2017."
        },
        {
          "References": "Selected Topics in Signal Processing, vol. 16, pp. 1505–1518,",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": ""
        },
        {
          "References": "",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": "[28] Y.-H. H. Tsai, S. Bai,\net al.,\n“Multimodal\ntransformer\nfor"
        },
        {
          "References": "2022.",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": ""
        },
        {
          "References": "",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": "unaligned multimodal\nlanguage sequences,”\nProc. ACL, pp."
        },
        {
          "References": "[11]\nJ. Devlin, M.-W. Chang, et al.,\n“Bert:\nPre-training of deep",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": "6558–6569, 2019."
        },
        {
          "References": "bidirectional\ntransformers for language understanding,” Proc.",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": ""
        },
        {
          "References": "",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": "[29]\nJ.\nL. Ba,\nJ. R. Kiros,\net\nal.,\n“Layer\nnormalization,”"
        },
        {
          "References": "NAACL-HLT, p. 4171–4186, 2019.",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": ""
        },
        {
          "References": "",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": "arXiv:1607.06450, 2016."
        },
        {
          "References": "[12]\nP. He, X. Liu, et al.,\n“Deberta: Decoding-enhanced bert with",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": ""
        },
        {
          "References": "",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": "[30] K. He, X. Zhang, et al.,\n“Delving deep into rectifiers: Sur-"
        },
        {
          "References": "disentangled attention,” Proc. ICLR, 2021.",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": ""
        },
        {
          "References": "",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": "passing human-level performance on imagenet classification,”"
        },
        {
          "References": "",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": "Proc. ICCV, pp. 1026–1034, 2015."
        },
        {
          "References": "[13] Y. Liu, M. Ott, et al.,\n“Roberta:\na robustly optimized bert",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": ""
        },
        {
          "References": "pretraining approach,” Proc. ICLR, 2020.",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": ""
        },
        {
          "References": "",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": "[31] D. P Kingma and J. Ba, “Adam: A method for stochastic opti-"
        },
        {
          "References": "",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": "mization,” Proc. ICLR, pp. 1–15, 2015."
        },
        {
          "References": "[14] B. Lin and L. Wang,\n“Robust multi-modal\nspeech emotion",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": ""
        },
        {
          "References": "recognition with asr error adaptation,” Proc. ICASSP, pp. 1–5,",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": ""
        },
        {
          "References": "",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": "[32] C. Busso, M. Bulut, et al., “IEMOCAP: Interactive emotional"
        },
        {
          "References": "2023.",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": "Language resources and"
        },
        {
          "References": "",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": "dyadic motion capture database,”"
        },
        {
          "References": "",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": "evaluation, pp. 335–359, 2008."
        },
        {
          "References": "[15] X. Shi,\nJ. He,\net\nal.,\n“On the\neffectiveness of\nasr\nrepre-",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": ""
        },
        {
          "References": "sentations\nin real-world noisy speech emotion recognition,”",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": "[33] A. Radford, J. W. Kim, et al.,\n“Robust speech recognition via"
        },
        {
          "References": "arXiv:2311.07093, 2023.",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": "large-scale weak supervision,” Proc. ICML, 2023."
        },
        {
          "References": "[16]\nJ. Santoso, T. Yamada, et al.,\n“Speech emotion recognition",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": "[34] Y. Leng, X. Tan, et al.,\n“Fastcorrect:\nFast error correction"
        },
        {
          "References": "based on self-attention weight correction for acoustic and text",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": "with edit alignment for automatic speech recognition,” Proc."
        },
        {
          "References": "features,” IEEE Access, vol. 10, pp. 115732–115743, 2022.",
          "[17] Y Gao, J. Liu, et al.,\n“Domain-adversarial autoencoder with": "NeurIPS, pp. 21708–21719, 2021."
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Speech emotion recognition: a comprehensive survey",
      "authors": [
        "M Al-Dujaili",
        "A Ebrahimi-Moghadam"
      ],
      "year": "2023",
      "venue": "Wireless Personal Communications"
    },
    {
      "citation_id": "2",
      "title": "Semi-supervised multimodal emotion recognition with consensus decision-making and label correction",
      "authors": [
        "J Tian",
        "D Hu"
      ],
      "year": "2023",
      "venue": "Proc. MRAC"
    },
    {
      "citation_id": "3",
      "title": "Mmer: Multimodal multi-task learning for speech emotion recognition",
      "authors": [
        "S Ghosh",
        "U Tyagi"
      ],
      "year": "2023",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "4",
      "title": "Group gated fusion on attention-based bidirectional alignment for multimodal emotion recognition",
      "authors": [
        "P Liu",
        "K Li"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "5",
      "title": "Isnet: Individual standardization network for speech emotion recognition",
      "authors": [
        "W Fan",
        "X Xu"
      ],
      "year": "2022",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "6",
      "title": "Speechformer: A hierarchical efficient framework incorporating the characteristics of speech",
      "authors": [
        "W Chen",
        "X Xing"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "7",
      "title": "Mgat: Multi-granularity attention based transformers for multi-modal emotion recognition",
      "authors": [
        "W Fan",
        "X Xing"
      ],
      "year": "2023",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "8",
      "title": "wav2vec: Unsupervised pretraining for speech recognition",
      "authors": [
        "S Schneider",
        "A Baevski"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "9",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "10",
      "title": "Wavlm: Large-scale self-supervised pre-training for full stack speech processing",
      "authors": [
        "S Chen",
        "C Wang"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "11",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang"
      ],
      "year": "2019",
      "venue": "Proc. NAACL-HLT"
    },
    {
      "citation_id": "12",
      "title": "Deberta: Decoding-enhanced bert with disentangled attention",
      "authors": [
        "P He",
        "X Liu"
      ],
      "year": "2021",
      "venue": "Proc. ICLR"
    },
    {
      "citation_id": "13",
      "title": "Roberta: a robustly optimized bert pretraining approach",
      "authors": [
        "Y Liu",
        "M Ott"
      ],
      "year": "2020",
      "venue": "Proc. ICLR"
    },
    {
      "citation_id": "14",
      "title": "Robust multi-modal speech emotion recognition with asr error adaptation",
      "authors": [
        "B Lin",
        "L Wang"
      ],
      "year": "2023",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "15",
      "title": "On the effectiveness of asr representations in real-world noisy speech emotion recognition",
      "authors": [
        "X Shi",
        "J He"
      ],
      "year": "2023",
      "venue": "On the effectiveness of asr representations in real-world noisy speech emotion recognition",
      "arxiv": "arXiv:2311.07093"
    },
    {
      "citation_id": "16",
      "title": "Speech emotion recognition based on self-attention weight correction for acoustic and text features",
      "authors": [
        "J Santoso",
        "T Yamada"
      ],
      "year": "2022",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "17",
      "title": "Domain-adversarial autoencoder with attention based feature level fusion for speech emotion recognition",
      "authors": [
        "J Gao",
        "Liu"
      ],
      "year": "2021",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "18",
      "title": "Temporal attention convolutional network for speech emotion recognition with latent representation",
      "authors": [
        "J Liu",
        "Z Liu"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "19",
      "title": "Multimodal emotion recognition using cross-modal attention and 1d convolutional neural networks",
      "authors": [
        "D Krishna",
        "A Patil"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "20",
      "title": "Mir-gan: Refining frame-level modality-invariant representations with adversarial network for audio-visual speech recognition",
      "authors": [
        "Y Hu",
        "C Chen"
      ],
      "year": "2023",
      "venue": "Proc. ACL"
    },
    {
      "citation_id": "21",
      "title": "Misa: Modalityinvariant and-specific representations for multimodal sentiment analysis",
      "authors": [
        "D Hazarika",
        "R Zimmermann"
      ],
      "year": "2020",
      "venue": "Proc. ACMMM"
    },
    {
      "citation_id": "22",
      "title": "Learning modality-specific representations with self-supervised multi-task learning for multimodal sentiment analysis",
      "authors": [
        "W Yu",
        "H Xu"
      ],
      "year": "2021",
      "venue": "Proc. AAAI"
    },
    {
      "citation_id": "23",
      "title": "Modality-specific learning rates for effective multimodal additive late-fusion",
      "authors": [
        "Y Yao",
        "R Mihalcea"
      ],
      "year": "2022",
      "venue": "Proc. ACL"
    },
    {
      "citation_id": "24",
      "title": "Learning modality-specific andagnostic representations for asynchronous multimodal language sequences",
      "authors": [
        "D Yang",
        "H Kuang"
      ],
      "year": "2022",
      "venue": "Proc. ACMMM"
    },
    {
      "citation_id": "25",
      "title": "Google's neural machine translation system: Bridging the gap between human and machine translation",
      "authors": [
        "Y Wu",
        "M Schuster"
      ],
      "year": "2016",
      "venue": "Google's neural machine translation system: Bridging the gap between human and machine translation",
      "arxiv": "arXiv:1609.08144"
    },
    {
      "citation_id": "26",
      "title": "ED-CEC: Improving rare word recognition using asr postprocessing based on error detection and context-aware error correction",
      "authors": [
        "J He",
        "Z Yang"
      ],
      "year": "2023",
      "venue": "Proc. ASRU"
    },
    {
      "citation_id": "27",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer"
      ],
      "year": "2017",
      "venue": "Proc. NeurIPS"
    },
    {
      "citation_id": "28",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Y.-H Tsai",
        "S Bai"
      ],
      "year": "2019",
      "venue": "Proc. ACL"
    },
    {
      "citation_id": "29",
      "title": "Layer normalization",
      "authors": [
        "J Ba",
        "J Kiros"
      ],
      "year": "2016",
      "venue": "Layer normalization",
      "arxiv": "arXiv:1607.06450"
    },
    {
      "citation_id": "30",
      "title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
      "authors": [
        "K He",
        "X Zhang"
      ],
      "year": "2015",
      "venue": "Proc. ICCV"
    },
    {
      "citation_id": "31",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2015",
      "venue": "Proc. ICLR"
    },
    {
      "citation_id": "32",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "33",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "A Radford",
        "J Kim"
      ],
      "year": "2023",
      "venue": "Proc. ICML"
    },
    {
      "citation_id": "34",
      "title": "Fastcorrect: Fast error correction with edit alignment for automatic speech recognition",
      "authors": [
        "Y Leng",
        "X Tan"
      ],
      "year": "2021",
      "venue": "Proc. NeurIPS"
    }
  ]
}