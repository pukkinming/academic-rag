{
  "paper_id": "2308.14568v1",
  "title": "Time-Frequency Transformer: A Novel Time Frequency Joint Learning Method For Speech Emotion Recognition",
  "published": "2023-08-28T13:34:02Z",
  "authors": [
    "Yong Wang",
    "Cheng Lu",
    "Yuan Zong",
    "Hailun Lian",
    "Yan Zhao",
    "Sunan Li"
  ],
  "keywords": [
    "Speech emotion recognition",
    "Time-frequency domain",
    "Transformer Time Encoder Time Transformer Frequency Encoder Frequency Transformer Time-Frequency Encoder Time-Frequency Transformer Liner Projection Liner Projection Linear Projection Classifier Time Partition Frequency Partition Mel-Spectrogram"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this paper, we propose a novel time-frequency joint learning method for speech emotion recognition, called Time-Frequency Transformer. Its advantage is that the Time-Frequency Transformer can excavate global emotion patterns in the time-frequency domain of speech signal while modeling the local emotional correlations in the time domain and frequency domain respectively. For the purpose, we first design a Time Transformer and Frequency Transformer to capture the local emotion patterns between frames and inside frequency bands respectively, so as to ensure the integrity of the emotion information modeling in both time and frequency domains. Then, a Time-Frequency Transformer is proposed to mine the time-frequency emotional correlations through the local timedomain and frequency-domain emotion features for learning more discriminative global speech emotion representation. The whole process is a time-frequency joint learning process implemented by a series of Transformer models. Experiments on IEMOCAP and CASIA databases indicate that our proposed method outdoes the state-of-the-art methods.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech Emotion Recognition (SER) aims to use computers to automatically analyze and recognize the emotional state in human speech  [1] , which has become a hotspot in many fields, e. g., affective computing and Human-Computer Interaction (HCI)  [2] . For SER task, how to obtain a speech emotion representation with high discrimination and strong generalization is a key step to realize the superior performance of speech emotion classification  [3] ,  [4] .\n\nIn order to recognition speech emotions well, early SER works mainly combined some low-level descriptor (LLD) features or their combinations  [5] , e. g., Mel-Frequency Cepstral Coefficients (MFCC), Zero-Crossing Rate, and Pitch, with classifiers  [6] , e. g., k-Nearest Neighbor (KNN) and Support Vector Machine (SVM), for emotion prediction. Furthermore, with the rapid development of deep learning, high-dimensional speech emotion features generated by deep neural networks (DNN), e. g., Convolutional Neural Network (CNN)  [7]  and Recurrent Neural Network (RNN)  [8] , have emerged on SER and achieved superior performance. Currently, the input features of DNNs are mainly based on spectrogram, e. g., magnitude spectrogram  [9]  and Mel-spectrogram  [10] , which are the time-frequency representations of speech signals.\n\nThe time and frequency domains of the spectrogram contain rich emotional information. To excavate them, a practical approach is joint time-frequency modeling strategy with the input features of spectrograms  [10] ,  [3] . Among these methods, combining CNN and RNN structure, i. e., CNN+LSTM, is a classic method, which utilizes CNN and RNN to encode the information in frequency and time domains, respectively. For instance, Satt et al.  [11]  combined CNN with a special RNN (i. e., LSTM) to model the time-frequency domain of emotional speech. Wu et al.  [12]  proposed a recurrent capsules network to extract time-frequency emotional information from the spectrogram.\n\nAlthough current time-frequency joint learning methods have achieved certain success on SER, they still suffer from two issues. The first one is that they usually shared the modeling both in time and frequency domains, ignoring the specificality of the respective domains. For instance, the time-frequency domain shares a uniform size convolution kernel in CNN  [9]  and a uniform-scale feature map is performed on the time-frequency domain in RNN  [11] . Therefore, separate modeling of time-domain and frequency-domain information should be considered to ensure the specificity and integrity of the encoding of time-frequency domain information. The other issue is that only some low-level feature fusion operations (e. g., splicing and weighting) are adopted in time frequency joint learning process, leading to poor discriminativeness of fusion features  [10] . This indicates that the effective fusion of emotional information in the time-frequency domain is also the key for time-frequency joint learning.\n\nTo cope with the above issues, we propose a novel Transformer-based time frequency domain joint learning method for SER, called Time-Frequency Transformer, which consisting of three modules, i.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Proposed Method",
      "text": "In this section, we will introduce our proposed Time-Frequency Transformer shown in Figure . 1, including three modules, i.e., Time Transformer module, Frequency Transformer module and Time-Frequency Transformer module.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Time Transformer Module",
      "text": "The role of Time Transformer is to capture the local emotion correlations across time frames for time domain encoding of emotional speech. This module utilizes the time encoder to reduce the dimensionality of the input feature x of the model and extract part of the emotional information in time-domain of x. Then, MSA mechanism of the Transformer encoder is used to calculate the emotional correlation between frames, making the model focus on emotion related frames and reducing the impact of emotion unrelated frames on speech emotion recognition.\n\nBasically, the time encoder consists of a 2D convolutional layer, a 2D batch normalization layer, an activation function layer, a 2D convolutional layer, a 2D batch normalization layer and an activation function layer in sequence. We convolute the input spectrogram feature x ∈ R b×c×f ×d frame by frame and extract temporal emotional information. Each convolution layer is followed by a batch normalization layer and an activation function layer to speed up model training and improve the model's nonlinear representation capabilities. The output of the second activation function layer is x′ ∈ R b×c1×(f /4)×d , where b and f represent the number of samples selected for each training and the number of Mel-filter banks, c and c1 represent the number of input channels of the spectrogram feature and the number of output channels of the last convolutional layer and d is the number of frames of the spectrogram feature.The process can be represented as\n\nwhere C t (•) is the convolutional operation in each frame by a 2D convolutional layer, BN (•) and Act(•) is batch normalization and activation function operations respectively.\n\nWe utilizes a Transformer encoder to perform temporal attention focusing on x′ . The Transformer encoder consists of a MSA sub-layer and a feed-forward neural network using residual connections.The Transformer encoder applies multiple attention heads to achieve the model parallel training. By dividing the input into multiple feature subspaces and applying a self-attention mechanism in the subspaces, the model can be trained in parallel while capturing emotional information.\n\nWe take the mean value of the convolution output channel dimension of x′ to get s ′ ∈ R b×(f /4)×d and then transpose s ′ to get ŝ′ ∈ R b×d×(f /4) . The feature ŝ′ ∈ R b×d×(f /4) is used as the input of the time-domain Transformer encoder. We can represent the process as\n\nwhere M SA(•), LN (•) and M LP (•) is MSA, layer normalization and feed-forward neural network respectively in a Transformer encoder. Besides, m is the output of ŝ′ after the MSA and layer normalization of a Transformer encoder and q ∈ R b×d×(f /4) is the output of a Transformer encoder. The output feature q will be used as one of the inputs of time-frequency Transformer module after linear mapping.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Frequency Transformer Module",
      "text": "This module has a similar structure to the time Transformer module, both containing a Transformer encoder. The difference between the two modules is that this module performs 2D convolutional operation on the input features x frequency band by frequency band, which is used for reducing dimension of feature and extracting frequency-domain emotional information. We use 2D convolutional operation C f (•) on each frequency band of feature x. Then, we normalize the feature and activate the feature using the activation function. Finally get the output x′′ ∈ R b×c1×f ×(d/4) of the last activation function layer. The operations can be represented as\n\nWe take the mean value of the convolution output channel dimension of x′′ to get ŝ′′ ∈ R b×f ×(d/4) . Then, we use a Transformer encoder to calculate the emotional correlation between frequency bands for ŝ′′ , so that the model focuses on the emotional related parts in frequency domain, reducing the impact of emotional unrelated frequency bands on speech emotion recognition. The operations can be represented as\n\nwhere n is the output of ŝ′′ after the MSA and layer normalization of the Transformer encoder and k ∈ R b×f ×(d/4) is the output of the Transformer encoder. After linear mapping, k will be used as one of the inputs to the time-frequency Transformer module.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Time-Frequency Transformer Module",
      "text": "Time-Frequency Transformer Module aims to aggregate the local emotion encoding in time and frequency domains generated by Time Transformer and Frequency Transformer into the global emotion representation in time-frequency domain. Therefore, the main function of this module is to use the time-frequency domain features of the speech that have been weighted by the MSA mechanism to use the Multi-head Attention(MA) mechanism  [13]  to further weight, so that the model pays more attention to the emotion-related segments in time-frequency domain, thereby improving accuracy of speech emotion recognition. First, We utilize the 2D convolution operation C(•) to encode x in the time-frequency domain. After that, we normalize and activate the feature to obtain x ∈ R b×c1×(f /4)×(d/4) . The process can be represented as\n\nThen, we take the mean value of x in the convolution output channel dimension to get v ∈ R b×(f /4)×(d/4) . We linearly map q and k to get q ∈ R b×(f /4)×(d/4) and k ∈ R b×(f /4)×(d/4) respectively. The q, k and v are used as the input of the CO-Transformer encoder. Compared with an original Transformer encoder, we replace the MSA sub-layer in the encoder with a MA sub-layer to obtain a CO-Transformer encoder. A CO-Transformer encoder is composed of a MA sub-layer and a feed-forward neural network using residual connection.The main difference between MSA and MA is that when doing attention calculations, the inputs Q (Query Vector), K (Keyword Vector), and V (Value Vector) of MSA are the same, but the inputs Q ,K, and V of MA are different. The process can be represented as\n\nwhere M A(•) is MA and p is the output of the input features after the MA and layer normalization of the CO-Transformer encoder. Besides, y ∈ R b×(f /4)×(d/4) is the output of the CO-Transformer encoder. The obtained y is the input of the classifier and finally obtain the predicted emotional category.\n\nThe classifier consists of a pooling layer and a fully connected layer. The main function of the pooling layer is to reduce the feature dimension. The pooling layer takes the mean and standard deviation in the frequency-domain dimension of y and concatenates them to get ŷ ∈ R b×(d/2) , which can be represented as\n\nwhere P ool(•) is the pooling operation. We calculate the prediction probability of all emotions through a fully connected layer and ultimately obtain ŷ′ ∈ R b×c , where c is the number of emotion categories of the corpus. We take the emotion with the largest prediction probability as the predicted emotion of the model and optimize the model by reducing the cross-entropy loss Loss between the predicted emotion label ŷ′′ ∈ R b×c and the true emotion label z ∈ R b×c . The operations can be represented as\n\nwhere F C(•), Sof tmax(•) and CrossEntropyLoss(•) is the fully connected layer operation, Softmax function  [14]  and Cross-Entropy loss function  [15]  respectively.\n\n3 Experiments",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Experimental Databases",
      "text": "Extensive experiments are conducted on two well-known speech emotion databases, i.e., IEMOCAP  [16]  and CASIA  [17] .\n\n-IEMOCAP is an audio-visual database released by the Sail Laboratory of the University of Southern California. This database consists of five dyadic sessions, and each session is performed by a male actor and a female actor in improvised and scripted scenarios to obtain various emotions (angry, happy, sad, neutral, frustrated, excited, fearful, surprised, disgusted, and others). We select the audio samples in improvised scenario, including",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experimental Protocol",
      "text": "In the experiment, we follow the same protocol of the previous research  [10]  and adopt the Leave-One-Speaker-Out (LOSO) cross-validation for evaluation.\n\nSpecifically, for CASIA, when one speaker's samples are served as the testing data, the remaining three speakers' samples are used for training. Similarly, for IEMOCAP, we use one speaker's samples as the testing data and other speakers' samples as the training data. Moreover, since IEMOCAP contains 5 sessions, the leave-one-session-out cross-validation protocol (one session's samples as the testing data and four sessions' samples as the training data) is also a common way for evaluation  [18] . Therefore, in Table  4 , we also choose some methods using this protocol to compare with our proposed method.\n\nIn this paper, we choose the weighted average recall (WAR)  [6]  and the unweighted average recall (UAR)  [5] , which are widely-used SER evaluation indicators, to effectively measure the performance of the proposed method.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experimental Setting",
      "text": "Before the feature extraction, we preprocess the audio samples by dividing them into small segments of 80 frames (20ms per frame). With this operation, samples are not only augmented, but also maintain the integrity of speech emotions. After that, we preemphasize the speech segments and the pre-emphasis coefficient is 0.97. Then, we use a 20ms Hamming window with a frame shift of 10ms to extract log-Mel-spectrogram, where the number of points of Fast Fourier Transform (FFT) and bands of Mel-filter are 512 and 80 respectively. Finally, the model input features of b=64, c=1, f =80, d=80 are obtained.\n\nBesides, the parameters of C t (•), C f (•), C(•) are shown in Table  2 . The BN (•) and Act(•) denote the BatchNorm function  [19]  and the ReLU function  [14] , respectively. The parameters of the Transformer encoder used in our model are shown in Table  3 . The proposed method is implemented by Pytorch  [20]  with NVIDIA A10 Tensor Core GPUs, which is trained from scratch with 1000 epochs and optimized by Adam optimizer  [21]  with the initialized learning rate of 0.001.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experimental Results And Analysis",
      "text": "Results on IEMOCAP We selected some state-of-the-art methods for performance comparison with the proposed method, i.e., a model based on MSA that fuses acoustic and linguistic features (MSA-AL)  [18]  , a model that combines CNN with Long Short Term Memory (LSTM) and uses spectrogram as the input features (CNN-LSTM)  [11] , spectro-temporal and CNN with attention model(STC-Attention)  [22] , a Deep Neural Network (DNN) method combining Gaussian Mixture Models (GMM) and Hidden Markov Models (HMM) using subspace alignment strategy (DNN-SAli)  [23] , a method of using Gated Recurrent Unit (GRU) in CNN layers and combining with sequential Capsules (GRU-CNN-SeqCap)  [12] , and a DNN method with Bottleneck features (DNN-BN)  [24] . GA-BEL  [25]  39.50 39.50 ELM-DNN  [26]  41.17 41.17 LoHu  [27]  43.50 43.50 DCNN-DTPM  [28]   The experimental results are shown in Table  4 . From the results, we can find something interesting. Firstly, our proposed Time-Frequency Transformer achieves the best performance on both WAR (74.43%) and UAR (62.90%) compared to other mentioned methods. Moreover, compared to all methods using Leave-One-Speaker-Out protocol, our proposed method achieves a promising increase over 10% in term of WAR and 1.5% in UAR.\n\nThe confusion matrix of IEMOCAP is shown in Figure . 2(a). What we can observe first is that the proposed method exhibits a excellent performance in classifying specific emotions, e.g., angry, neutral and sad. However, it is difficult for the proposed model to correctly recognize the emotion happy. As shown in the figure, 72% happy samples are misclassified as neutral and only 17% are correctly classified. Obviously, it cannot be caused by the reason that happy is more close to neural than other emotions (negative emotions: anger and sad) since the possibility of neutral samples being misclassified as happy is only 0.73%. This situation lead us to consider the other reason which is the unbalanced sample size. Since the number of happy samples in IEMOCAP is only 284 which is the smallest among all emotions, the model cannot learn the unique emotional characteristics of happy well. It may lead to this situation that the happy samples are more likely to be mistaken for neutral.\n\nResults on CASIA Some state-of-the-art methods that also use the LOSO protocol are used for comparison with the proposed method, including Genetic Algorithm (GA) combined with Brain Emotional Learning (BEL) model (GA-BEL)  [25] , Extreme Learning Machine (ELM) combined with DNN (ELM-DNN)  [26] , weighted spectral features based on Local Hu moments (LoHu)  [27] , Deep CNN (DCNN) combined with a Discriminant Temporal Pyramid Matching (DTPM) strategy (DCNN-DTPM)  [28]  and an Attentive Time-Frequency Neural Network (ATFNN)  [10] . The results on the CASIA database is shown in Table  4 . It is obvious that our method achieves state-of-the-art performance among all algorithms. Specifically, our method obtains the best result on WAR (53.17%) and UAR (53.17%) than all comparison methods. Since the sample numbers of the 6 emotions of CASIA used in the experiment are balanced, WAR and UAR are equal. Besides, our results are not only the best, but also far superior to other methods. Even compared to ATFNN which is the second best method, the proposed method still obtain a over 4% performance increase.\n\nFrom the confusion matrix of CASIA in Figure . 2(b), it is obvious that the proposed method has a high recognition rate in four types of emotions(angry, fear, neutral, sad), but the recognition effect on happy and surprise is poor. Since happy is easily misclassified as sad, it may be caused by the pendulum effect  [29]  in psychology. Human emotions are characterized by multiplicity and bipolarity under the influence of external stimuli. Beside that, surprise is always confused with fear. Due to the similar arousal  [30]  of the two emotions, it may lead to them inducing each other.",
      "page_start": 8,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview of Time-Frequency Transformer for speech emotion recognition. It",
      "page": 3
    },
    {
      "caption": "Figure 2: Confusion matrices on IEMOCAP and CASIA.",
      "page": 10
    },
    {
      "caption": "Figure 3: Visualization of log-Mel-spectrogram, T-Trans Attention, F-Trans Attention,",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "IEMOCAP": "CASIA",
          "English": "Mandarin",
          "angry (289) happy (284)\nneutral (1099) sad (608)": "angry (200) fear (200)\nhappy (200) neutral (200)\nsad (200) surprise (200)",
          "2280": "1200"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "IEMOCAP": "",
          "Leave One Session/Speaker Out (LOSO)\n(5 Sessions or 10 Speakers)": "",
          "MSA-AL [18]": "CNN-LSTM [11]",
          "72.34": "68.80",
          "58.31": "59.40"
        },
        {
          "IEMOCAP": "",
          "Leave One Session/Speaker Out (LOSO)\n(5 Sessions or 10 Speakers)": "",
          "MSA-AL [18]": "STC-Attention [22]",
          "72.34": "61.32",
          "58.31": "60.43"
        },
        {
          "IEMOCAP": "",
          "Leave One Session/Speaker Out (LOSO)\n(5 Sessions or 10 Speakers)": "",
          "MSA-AL [18]": "DNN-SALi [23]",
          "72.34": "62.28",
          "58.31": "58.02"
        },
        {
          "IEMOCAP": "",
          "Leave One Session/Speaker Out (LOSO)\n(5 Sessions or 10 Speakers)": "",
          "MSA-AL [18]": "GRU-CNN-SeqCap [12] 72.73",
          "72.34": "",
          "58.31": "59.71"
        },
        {
          "IEMOCAP": "",
          "Leave One Session/Speaker Out (LOSO)\n(5 Sessions or 10 Speakers)": "",
          "MSA-AL [18]": "DNN-BN [24]",
          "72.34": "59.7",
          "58.31": "61.4"
        },
        {
          "IEMOCAP": "",
          "Leave One Session/Speaker Out (LOSO)\n(5 Sessions or 10 Speakers)": "",
          "MSA-AL [18]": "Ours",
          "72.34": "74.43",
          "58.31": "62.90"
        },
        {
          "IEMOCAP": "CASIA",
          "Leave One Session/Speaker Out (LOSO)\n(5 Sessions or 10 Speakers)": "Leave One Speaker Out (LOSO)\n(4 Speakers)",
          "MSA-AL [18]": "GA-BEL [25]",
          "72.34": "39.50",
          "58.31": "39.50"
        },
        {
          "IEMOCAP": "",
          "Leave One Session/Speaker Out (LOSO)\n(5 Sessions or 10 Speakers)": "",
          "MSA-AL [18]": "ELM-DNN [26]",
          "72.34": "41.17",
          "58.31": "41.17"
        },
        {
          "IEMOCAP": "",
          "Leave One Session/Speaker Out (LOSO)\n(5 Sessions or 10 Speakers)": "",
          "MSA-AL [18]": "LoHu [27]",
          "72.34": "43.50",
          "58.31": "43.50"
        },
        {
          "IEMOCAP": "",
          "Leave One Session/Speaker Out (LOSO)\n(5 Sessions or 10 Speakers)": "",
          "MSA-AL [18]": "DCNN-DTPM [28]",
          "72.34": "45.42",
          "58.31": "45.42"
        },
        {
          "IEMOCAP": "",
          "Leave One Session/Speaker Out (LOSO)\n(5 Sessions or 10 Speakers)": "",
          "MSA-AL [18]": "ATFNN [10]",
          "72.34": "48.75",
          "58.31": "48.75"
        },
        {
          "IEMOCAP": "",
          "Leave One Session/Speaker Out (LOSO)\n(5 Sessions or 10 Speakers)": "",
          "MSA-AL [18]": "Ours",
          "72.34": "53.17",
          "58.31": "53.17"
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Computational paralinguistics: emotion, affect and personality in speech and language processing",
      "authors": [
        "B Schuller",
        "A Batliner"
      ],
      "year": "2013",
      "venue": "Computational paralinguistics: emotion, affect and personality in speech and language processing"
    },
    {
      "citation_id": "2",
      "title": "Speech emotion recognition: Two decades in a nutshell, benchmarks, and ongoing trends",
      "authors": [
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "3",
      "title": "Speech emotion recognition: Emotional models, databases, features, preprocessing methods, supporting modalities, and classifiers",
      "authors": [
        "M Akçay",
        "K Oguz"
      ],
      "year": "2020",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "4",
      "title": "Domain invariant feature learning for speaker-independent speech emotion recognition",
      "authors": [
        "C Lu",
        "Y Zong",
        "W Zheng",
        "Y Li",
        "C Tang",
        "B Schuller"
      ],
      "year": "2022",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "5",
      "title": "Deep neural networks for acoustic emotion recognition: Raising the benchmarks. In: 2011 IEEE international conference on acoustics, speech and signal processing (ICASSP)",
      "authors": [
        "A Stuhlsatz",
        "C Meyer",
        "F Eyben",
        "T Zielke",
        "G Meier",
        "B Schuller"
      ],
      "year": "2011",
      "venue": "Deep neural networks for acoustic emotion recognition: Raising the benchmarks. In: 2011 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "6",
      "title": "Acoustic emotion recognition: A benchmark comparison of performances",
      "authors": [
        "B Schuller",
        "B Vlasenko",
        "F Eyben",
        "G Rigoll",
        "A Wendemuth"
      ],
      "year": "2009",
      "venue": "2009 IEEE Workshop on Automatic Speech Recognition & Understanding"
    },
    {
      "citation_id": "7",
      "title": "Deep learning techniques for speech emotion recognition, from databases to models",
      "authors": [
        "B Abbaschian",
        "D Sierra-Sosa",
        "A Elmaghraby"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "8",
      "title": "Speech emotion recognition with dual-sequence lstm architecture",
      "authors": [
        "J Wang",
        "M Xue",
        "R Culhane",
        "E Diao",
        "J Ding",
        "V Tarokh"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "9",
      "title": "Learning salient features for speech emotion recognition using convolutional neural networks",
      "authors": [
        "Q Mao",
        "M Dong",
        "Z Huang",
        "Y Zhan"
      ],
      "year": "2014",
      "venue": "IEEE transactions on multimedia"
    },
    {
      "citation_id": "10",
      "title": "Speech emotion recognition via an attentive time-frequency neural network",
      "authors": [
        "C Lu",
        "W Zheng",
        "H Lian",
        "Y Zong",
        "C Tang",
        "S Li",
        "Y Zhao"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Computational Social Systems"
    },
    {
      "citation_id": "11",
      "title": "Efficient emotion recognition from speech using deep learning on spectrograms",
      "authors": [
        "A Satt",
        "S Rozenberg",
        "R Hoory"
      ],
      "year": "2017",
      "venue": "Efficient emotion recognition from speech using deep learning on spectrograms"
    },
    {
      "citation_id": "12",
      "title": "Speech emotion recognition using capsule networks",
      "authors": [
        "X Wu",
        "S Liu",
        "Y Cao",
        "X Li",
        "J Yu",
        "D Dai",
        "X Ma",
        "S Hu",
        "Z Wu",
        "X Liu"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "13",
      "title": "An attentive survey of attention models",
      "authors": [
        "S Chaudhari",
        "V Mithal",
        "G Polatkan",
        "R Ramanath"
      ],
      "year": "2021",
      "venue": "ACM Transactions on Intelligent Systems and Technology (TIST)"
    },
    {
      "citation_id": "14",
      "title": "Activation functions in deep learning: A comprehensive survey and benchmark",
      "authors": [
        "S Dubey",
        "S Singh",
        "B Chaudhuri"
      ],
      "year": "2022",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "15",
      "title": "Generalized cross entropy loss for training deep neural networks with noisy labels",
      "authors": [
        "Z Zhang",
        "M Sabuncu"
      ],
      "year": "2018",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "16",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "17",
      "title": "Design of speech corpus for mandarin text to speech",
      "authors": [
        "J Zhang",
        "H Jia"
      ],
      "year": "2008",
      "venue": "The blizzard challenge 2008 workshop"
    },
    {
      "citation_id": "18",
      "title": "Deep encoded linguistic and acoustic cues for attention based end to end speech emotion recognition",
      "authors": [
        "S Bhosale",
        "R Chakraborty",
        "S Kopparapu"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "19",
      "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "authors": [
        "S Ioffe",
        "C Szegedy"
      ],
      "year": "2015",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "20",
      "title": "Pytorch: An imperative style, high-performance deep learning library",
      "authors": [
        "A Paszke",
        "S Gross",
        "F Massa",
        "A Lerer",
        "J Bradbury",
        "G Chanan",
        "T Killeen",
        "Z Lin",
        "N Gimelshein",
        "L Antiga"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "21",
      "title": "A method for stochastic optimization",
      "authors": [
        "K Adam"
      ],
      "year": "2014",
      "venue": "A method for stochastic optimization",
      "arxiv": "arXiv:1412.69801412"
    },
    {
      "citation_id": "22",
      "title": "Representation learning with spectrotemporal-channel attention for speech emotion recognition",
      "authors": [
        "L Guo",
        "L Wang",
        "C Xu",
        "J Dang",
        "E Chng",
        "H Li"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "23",
      "title": "Revisiting hidden markov models for speech emotion recognition",
      "authors": [
        "S Mao",
        "D Tao",
        "G Zhang",
        "P Ching",
        "T Lee"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "24",
      "title": "Dnn-based emotion recognition based on bottleneck acoustic features and lexical features",
      "authors": [
        "E Kim",
        "J Shin"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "25",
      "title": "Speech emotion recognition based on an improved brain emotion learning model",
      "authors": [
        "Z Liu",
        "Q Xie",
        "M Wu",
        "W Cao",
        "Y Mei",
        "J Mao"
      ],
      "year": "2018",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "26",
      "title": "Speech emotion recognition using deep neural network and extreme learning machine",
      "authors": [
        "K Han",
        "D Yu",
        "I Tashev"
      ],
      "year": "2014",
      "venue": "Interspeech"
    },
    {
      "citation_id": "27",
      "title": "Weighted spectral features based on local hu moments for speech emotion recognition",
      "authors": [
        "Y Sun",
        "G Wen",
        "J Wang"
      ],
      "year": "2015",
      "venue": "Biomedical signal processing and control"
    },
    {
      "citation_id": "28",
      "title": "Speech emotion recognition using deep convolutional neural network and discriminant temporal pyramid matching",
      "authors": [
        "S Zhang",
        "S Zhang",
        "T Huang",
        "W Gao"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "29",
      "title": "The putt and the pendulum: Ironic effects of the mental control of action",
      "authors": [
        "D Wegner",
        "M Ansfield",
        "D Pilloff"
      ],
      "year": "1998",
      "venue": "Psychological Science"
    },
    {
      "citation_id": "30",
      "title": "Affective video content representation and modeling",
      "authors": [
        "A Hanjalic",
        "L Xu"
      ],
      "year": "2005",
      "venue": "IEEE transactions on multimedia"
    }
  ]
}