{
  "paper_id": "2111.02717v1",
  "title": "Facial Emotion Recognition Using Deep Residual Networks In Real-World Environments",
  "published": "2021-11-04T10:08:22Z",
  "authors": [
    "Panagiotis Tzirakis",
    "Dénes Boros",
    "Elnar Hajiyev",
    "Björn W. Schuller"
  ],
  "keywords": [
    "Affective computing",
    "deep learning",
    "facial feature extraction"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Automatic affect recognition using visual cues is an important task towards a complete interaction between humans and machines. Applications can be found in tutoring systems and human computer interaction. A critical step towards that direction is facial feature extraction. In this paper, we propose a facial feature extractor model trained on an in-the-wild and massively collected video dataset provided by the RealEyes company. The dataset consists of a million labelled frames and 2, 616 thousand subjects. As temporal information is important to the emotion recognition domain, we utilise LSTM cells to capture the temporal dynamics in the data. To show the favourable properties of our pre-trained model on modelling facial affect, we use the RECOLA database, and compare with the current state-of-the-art approach. Our model provides the best results in terms of concordance correlation coefficient.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Automatic analysis of facial affect is a vital process in humanto-human interactions. Currently, intelligent systems, such as robots and virtual humans, try to use emotion recognition models to make the interaction with humans more natural. To this end, such systems should automatically sense and adapt their responses according to the human behaviour. One application can be found in an automatic tutoring system, where the system adjusts the level of the tutorial depending on the user's affective state, such as excitement or boredom. In another application, a fatigue state can be predicted by utilising affect states  [1] .\n\nFor the past twenty years, a number of studies have been proposed in the literature to recognise emotions  [2, 3] . Most of them were focused on predicting the emotions in posed settings, namely, controlled environments. However, a shift has been observed in the affective community towards the prediction of the affective state of individuals in real-world environments  [4] . The reason for this shift is the observation that posed facial expressions can vary considerably in naturalistic behaviour  [5] [6] [7] .\n\nDeep Neural Networks (DNNs) have been widely used in the affective computing domain and have shown to provide state-of-the-art-results in several emotion recognition tasks  [3, 4] . In particular, Convolution Neural Networks (CNNs) are widely used to extract features from images. As emotions are expressed in temporal boundaries, it is important for the models to capture the contextual dynamics in the signal. To this end, Recurrent Neural Networks (RNNs) are utilised that can effectively model temporal patterns in the data.\n\nIn this paper, we present a pre-trained deep residual network of 50 layers (ResNet-50)  [8]  that is trained in an end-toend manner utilising a real-world industrial dataset collected by the RealEyes company  1  . The dataset is comprised of more than a million images and more than 2, 616 thousand subjects for categorical emotion recognition, and in particular predicting eight emotions. To the best of our knowledge, we are the first to train a model on thousands of subjects for real-world automatic emotion recognition. The main drawback of this dataset is the highly unbalanced number of labels, which we tackle using boosting methods. We show the favourable properties of our model to recognise the affective state by utilising the REmote COLlaborative and Affective (RECOLA) database  [9] . More particularly, we train the ResNet-50 in four different ways: (i) from scratch, i. e., random initialisation of the weights, (ii) utilising the pre-trained one from Im-ageNet  [10] , (iii) utilising the pre-trained one from AffWild database  [3] , and (iv) using our pre-trained model. Our model surpasses the performance of the other three models in terms of Concordance Correlation Coefficient (ρ c ) 2 .",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Several studies have been proposed to model affect using visual cues. For instance, Huang et al.  [11]  proposed a bistage learning framework for image-based emotion recognition. In particular, the authors combine DNNs and hypergraphs by first training the DNN for an emotion classification task, and then consider each node in the last fully connected layer as an attribute and use it to form a hyperedge in a hypergraph. In a more recent study, Kollias et al.  [3]  proposed an in-the-wild dataset, named Aff-Wild, that consists of 298 videos of 200 subjects, with a total length of more than 30 hours. They trained end-to-end deep architectures consisting of CNNs, that spatially extract features from the image, and RNNs, so as the contextual dynamics in sequential data can be captured. The feature extraction models were based on the ResNet-50 and the VGG-Face/VGG-16  [12]  architectures, with two fully connected layers on top, before feeding the features to a 2-layer LSTM network. VGG produced the best results in their experiments. However, their models were trained on a dataset that consists of a small number of individuals (only 200), whereas our model was trained on more than thousand subjects. The interested reader is referenced to  [13]  for more studies in the literature for visual-based emotion recognition.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Methods: Model Design",
      "text": "Our network comprises of two parts: (a) a feature extractor, that extracts spatial features from the images, and (b) a contextual extractor that captures the temporal dynamics in the data. Our feature extractor is a ResNet-50. The extracted features are being fed to a 2-layer LSTM that captures the temporal information in the data. Finally, a 2-unit fully connected layer is inserted that represents the output (arousal, and valence) of the network.\n\nFeature Extractor: In traditional face recognition an important step is feature extraction. This is performed by using hand-crafted features such as Scale Invariant Feature Transform (SIFT) and Histogram of Oriented Gradients (HOG). In the last ten years convolution networks have been proposed to extract features from faces  [14] .\n\nFor our purposes, we utilise a deep residual network (ResNet) of 50 layers  [8] . As input to the network we use the raw pixel intensities from the cropped faces of a subject's video. The architecture of the network starts with a 7 × 7 convolutional layer with 64 feature maps, followed by a max pooling layer of size 3 × 3. Then, four bottleneck architectures are utilised to extract higher-level abstractions where after these architectures a shortcut connection is added. Each of these architectures is comprised of three convolutional layers of sizes 1 × 1, 3 × 3, and 1 × 1, for each residual function. Table  1  shows the replication and the sizes of the feature maps for each bottleneck architecture. The last layer of the network is an average pooling. Contextual Extractor: The spatial-modelling of the signal removes background noise and enhances specific parts of the signal in our task, but it does not model the temporal structure of sequential data such as the videos in our dataset. To model such structure, we feed the extracted facial features to a 2layer LSTM network.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Model Pretraining",
      "text": "In this section, we describe the dataset provided by RealEyes (Sec. 4.1). In addition, we present the training methodology used to tackle the unbalanced data (Sec. 4.2) along with the loss function to train the ResNet-50 model (Sec. 4.4).",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Realeyes Dataset",
      "text": "The model was trained on a dataset provided by the RealEyes company that specialises on recognising human emotions. The dataset is comprised of videos collected by individuals from a personal computer camera recording while they were watching an advertisement. Each video contains one participant. The aim was to collect spontaneous facial behaviour in arbitrary recording conditions. We should point out that no audio information is available in the videos due to privacy issues. The dataset is comprised of 4, 973 videos, with a total length of more than 31 hours. The total number of subjects is 2, 616, with 1, 099 being males and 1, 517 females. Table  2  shows a summary of the statistics of the dataset.\n\nEach frame of the dataset is annotated by seven expert annotators  3  and for eight emotions, namely, neutral, happy, surprise, disgust, contempt, confusion, empathy, and other. In particular, all annotators were instructed both orally and through a document on the process to follow for the annotation. This document included a list of some well identified emotional cues for all emotions, providing a common basis for the annotation task. On top of that, the experts used their own appraisal of the subject's emotional state for creating the annotations.\n\nThe annotation process provided the freedom to the annotators to watch the videos, pause, re-wind, and then mark the start of an emotional state. The final label of the frame is given by a winner-takes-all approach, meaning the maximum number of votes for each emotion. Table  3",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Training Process",
      "text": "The input to our network are the raw pixel intensities from the face extracted from the frames of the videos using the Multi-Domain Convolutional Neural Network Tracker (MD-Net)  [15]  tracking algorithm. The extracted frames of the face are then resized to resolution 150 × 150.\n\nA major problem that occurs when trying to train our network with this dataset, is the highly unbalanced labels. Most of the labels are neutral (≈ 78 %), and this poses the threat of the network to classify all of the frames as neutral. To deal with this, we use a boosting method by re-sampling the frames that contain labels with the lowest portion in the dataset, namely, happy, surprise, disgust, confusion, empathy, and contempt. In particular, we re-sampled window video frames which contain the least frequent emotions and at the same time do not contain frequent emotions such as neutral.\n\nThe window length was chosen to be 150 frames same as in our training framework.\n\nAn important issue that occurs is that emotions are temporally expressed. That is, in a sequence of frames. Hence, our re-sampling strategy is based on this fact and we sampled sequences of frames instead of just individual frames. After this process, we randomly split the dataset based on the participants to training and validation set (80% training, 20% validation). Table  4  shows the number of frames per emotion after the re-sampling.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emotion",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Setup",
      "text": "We utilised the Adam optimisation algorithm to train the model with a fixed learning rate of 4 * 10 -5 . We used a mini-batch of size 2 with a sequence length of 150 frames.\n\nFor our purposes we used data augmentation methods. In particular, the data were augmented by resizing the image to size 170 × 170 and randomly cropping it to equal its original size. With this method, the model becomes scale invariant. In addition, colour augmentation is used by introducing random brightness and saturation to the image. Finally, to speed up the convergence of the optimisation algorithm we initialise the weights of our feature extractor using the pre-trained model on ImageNet  [10] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Loss Function",
      "text": "We should note that each frame in our dataset contains multiple labels, and as such our loss function is formulated as follows:\n\nwhere z are the labels and x the predictions. We should point out that we use a sigmoid function (σ) because we want to predict multiple emotions (labels) for each frame. Finally, we weight each sample based on the reciprocal of the number frames of each emotion. Hence, the emotion with the lowest number of samples (Contempt) has the highest weight, while the emotion with the highest number of frames (Neutral) the lowest weight.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dataset",
      "text": "To test our pre-trained model, we utilise the REmote COLlaborative and Affective (RECOLA) database introduced by Ringeval et al.  [16] . A part of the database was used in the Audio/Visual Emotion Challenge and Workshop (AVEC) 2016-2018 challenges  [17] . For our purposes, we utilise the full database, which contains 46 different recordings divided into three different parts (train, devel, and test) while balancing the gender, age, and mother tongue. In total, four modalities are contained in the database, i. e., audio, video, electrocardiogram (ECG), and electro-dermal activity (EDA). The original labels of the RECOLA are re-sampled at a constant frame rate of 40 ms. The data is then averaged over all raters by considering the inter-evaluator agreement, to provide a 'gold standard'  [18] . Each record is 5 min (300 s) audio data with a sampling rate of 16 kHz.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experimental Setup",
      "text": "The optimisation method we used to train our model, throughout all experiments, is the Adam optimisation method with a fixed learning rate of 4 * 10 -5 . We used a mini-batch of size 2 with a sequence length of 150 frames (12 s) -total 300 frames per batch -in both training and testing. Our loss function is based on the concordance correlation coefficient (ρ c )  [19] , and the best model is selected based on the highest ρ c on the validation set. The number of epochs was set to 300. We stopped the training process if no improvement on the validation score was observed after 20 epochs. Finally, a chain of post processing methods is applied: (a) median filtering (the size of the window was between 0.04 s and 20 s)  [17] , (b) centring (by finding the ground truth's and the prediction's bias)  [20] , (c) scaling (with the scaling factor the ratio between the standard deviation of the ground truth and the prediction)  [20] , and (d) time-shifting (forward in time with values between 0.04 s and 10 s). Any of these methods is kept when we observe a better ρ c on the validation set, and then applied to the test partition with the same configuration.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results",
      "text": "We fine-tune the model trained on the RealEyes dataset on the RECOLA database. For our training purposes we do not use any kind of data augmentation, such as colour distortion or random cropping. We compare the model with models initialised using two approaches: (i) from scratch, i. e. randomly using Xavier initialisatin, (ii) using the weights of the model pre-trained on the ImageNet dataset, and (iii) the current stateof-the-art AffWildNet  [3] . The same training framework was used to all models, i. e., a feature-and a contextual-extractor. Table  5  shows the results. As can be observed, our model provides the best results compared to the other methods in both the arousal and valence dimensions, which indicates the superiority of the pretrained model. We should also point out that our pre-trained model has faster convergence (i. e., the time to reach its best performance) of the optimisation than the other two models. In particular, our model convergence's is more than 2 times faster than the pre-trained one from ImageNet, and almost 3 times faster than the random initialisation. Finally, we should note that all the models perform better in the valence dimension rather than the arousal one, which has also been shown in other emotion recognition studies that utilise the face for dimensional emotion recognition  [21] .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusions And Future Work",
      "text": "In this paper, we proposed a pre-trained facial feature extractor, namely, a ResNet-50 model, which was trained on a large in-the-wild dataset provided by the RealEyes company with eight emotions. Before training the model, we used a boosting technique by resampling the dataset due to the highly unbalanced labels. Although the model was trained for categorical emotion recognition, we showed that fine-tuning it on the RECOLA database provides the best results on both arousal and valence dimensions when comparing it with models whose weights are initialised (i) randomly, (ii) using the pre-trained model on ImageNet, and (iii) using the current state-of-the-art AffWildNet  [3] . Our model surpasses the performance of the other models in terms of the concordance correlation coefficient, thus, making it an excellent tool for recognising facial affect.\n\nFor future work, we intend to perform more experiments on other dimensional emotion recognition datasets such as on the cross-cultural Sentiment Analysis in the Wild (SEWA) database. In addition, we will include categorical emotion recognition datasets",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4 Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany": ""
        },
        {
          "4 Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany": "ABSTRACT"
        },
        {
          "4 Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany": ""
        },
        {
          "4 Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany": "Automatic affect\nrecognition using visual cues is an impor-"
        },
        {
          "4 Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany": ""
        },
        {
          "4 Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany": "tant task towards a complete interaction between humans and"
        },
        {
          "4 Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany": ""
        },
        {
          "4 Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany": "machines. Applications can be found in tutoring systems and"
        },
        {
          "4 Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany": ""
        },
        {
          "4 Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany": "human computer interaction. A critical step towards that di-"
        },
        {
          "4 Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany": ""
        },
        {
          "4 Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany": "rection is facial feature extraction.\nIn this paper, we propose"
        },
        {
          "4 Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany": ""
        },
        {
          "4 Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany": "a facial feature extractor model trained on an in-the-wild and"
        },
        {
          "4 Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany": ""
        },
        {
          "4 Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany": "massively collected video dataset provided by the RealEyes"
        },
        {
          "4 Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany": ""
        },
        {
          "4 Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany": "company. The dataset consists of a million labelled frames"
        },
        {
          "4 Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany": ""
        },
        {
          "4 Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany": "and 2, 616 thousand subjects. As temporal information is im-"
        },
        {
          "4 Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany": ""
        },
        {
          "4 Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany": "portant to the emotion recognition domain, we utilise LSTM"
        },
        {
          "4 Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany": ""
        },
        {
          "4 Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany": "cells to capture the temporal dynamics in the data. To show"
        },
        {
          "4 Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany": ""
        },
        {
          "4 Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany": "the favourable properties of our pre-trained model on mod-"
        },
        {
          "4 Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany": ""
        },
        {
          "4 Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany": "elling facial affect, we use the RECOLA database, and com-"
        },
        {
          "4 Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany": ""
        },
        {
          "4 Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany": "pare with the current state-of-the-art approach. Our model"
        },
        {
          "4 Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany": "provides the best results in terms of concordance correlation"
        },
        {
          "4 Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany": "coefﬁcient."
        },
        {
          "4 Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany": ""
        },
        {
          "4 Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany": "Index Terms— Affective computing, deep learning,\nfa-"
        },
        {
          "4 Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany": ""
        },
        {
          "4 Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany": "cial feature extraction"
        },
        {
          "4 Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany": ""
        },
        {
          "4 Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany": ""
        },
        {
          "4 Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany": ""
        },
        {
          "4 Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany": "1.\nINTRODUCTION"
        },
        {
          "4 Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany": ""
        },
        {
          "4 Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany": "Automatic analysis of facial affect is a vital process in human-"
        },
        {
          "4 Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany": "to-human interactions.\nCurrently,\nintelligent systems,\nsuch"
        },
        {
          "4 Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany": "as robots and virtual humans,\ntry to use emotion recognition"
        },
        {
          "4 Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany": "models to make the interaction with humans more natural. To"
        },
        {
          "4 Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany": "this end, such systems should automatically sense and adapt"
        },
        {
          "4 Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany": "their responses according to the human behaviour. One appli-"
        },
        {
          "4 Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany": "cation can be found in an automatic tutoring system, where"
        },
        {
          "4 Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany": "the system adjusts the level of the tutorial depending on the"
        },
        {
          "4 Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany": "user’s affective state, such as excitement or boredom.\nIn an-"
        },
        {
          "4 Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany": "other application, a fatigue state can be predicted by utilising"
        },
        {
          "4 Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany": "affect states [1]."
        },
        {
          "4 Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany": ""
        },
        {
          "4 Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany": "For the past\ntwenty years, a number of studies have been"
        },
        {
          "4 Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany": "proposed in the literature to recognise emotions [2, 3]. Most"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table 1: The replication of each bottleneck architecture of",
      "data": [
        {
          "2. RELATED WORK": "Several studies have been proposed to model affect using vi-",
          "of the network is an average pooling.": "Table 1: The replication of each bottleneck architecture of"
        },
        {
          "2. RELATED WORK": "sual cues.\nFor\ninstance, Huang et al.\n[11] proposed a bi-",
          "of the network is an average pooling.": "the ResNet-50 along with the size of the features maps of the"
        },
        {
          "2. RELATED WORK": "stage learning framework for image-based emotion recogni-",
          "of the network is an average pooling.": "convolutions."
        },
        {
          "2. RELATED WORK": "tion.\nIn particular,\nthe authors combine DNNs and hyper-",
          "of the network is an average pooling.": ""
        },
        {
          "2. RELATED WORK": "graphs by ﬁrst training the DNN for an emotion classiﬁcation",
          "of the network is an average pooling.": ""
        },
        {
          "2. RELATED WORK": "",
          "of the network is an average pooling.": "Bottleneck layer\nReplication\n# Feature maps"
        },
        {
          "2. RELATED WORK": "task, and then consider each node in the last fully connected",
          "of the network is an average pooling.": ""
        },
        {
          "2. RELATED WORK": "layer as an attribute and use it\nto form a hyperedge in a hy-",
          "of the network is an average pooling.": ""
        },
        {
          "2. RELATED WORK": "",
          "of the network is an average pooling.": "1st\n3\n64, 64, 256"
        },
        {
          "2. RELATED WORK": "pergraph.\nIn a more recent study, Kollias et al. [3] proposed",
          "of the network is an average pooling.": ""
        },
        {
          "2. RELATED WORK": "",
          "of the network is an average pooling.": "4\n128, 128, 512\n2nd"
        },
        {
          "2. RELATED WORK": "an in-the-wild dataset, named Aff-Wild,\nthat consists of 298",
          "of the network is an average pooling.": ""
        },
        {
          "2. RELATED WORK": "",
          "of the network is an average pooling.": "3rd\n6\n256, 256, 1024"
        },
        {
          "2. RELATED WORK": "videos of 200 subjects, with a total\nlength of more than 30",
          "of the network is an average pooling.": ""
        },
        {
          "2. RELATED WORK": "",
          "of the network is an average pooling.": "4th\n3\n512, 512, 2048"
        },
        {
          "2. RELATED WORK": "hours.\nThey trained end-to-end deep architectures consist-",
          "of the network is an average pooling.": ""
        },
        {
          "2. RELATED WORK": "ing of CNNs,\nthat spatially extract features from the image,",
          "of the network is an average pooling.": ""
        },
        {
          "2. RELATED WORK": "",
          "of the network is an average pooling.": "Contextual Extractor: The spatial-modelling of the signal"
        },
        {
          "2. RELATED WORK": "and RNNs, so as the contextual dynamics in sequential data",
          "of the network is an average pooling.": ""
        },
        {
          "2. RELATED WORK": "",
          "of the network is an average pooling.": "removes background noise and enhances speciﬁc parts of the"
        },
        {
          "2. RELATED WORK": "can be captured. The feature extraction models were based",
          "of the network is an average pooling.": ""
        },
        {
          "2. RELATED WORK": "",
          "of the network is an average pooling.": "signal in our task, but it does not model the temporal structure"
        },
        {
          "2. RELATED WORK": "on the ResNet-50 and the VGG-Face/VGG-16 [12] architec-",
          "of the network is an average pooling.": ""
        },
        {
          "2. RELATED WORK": "",
          "of the network is an average pooling.": "of sequential data such as the videos in our dataset. To model"
        },
        {
          "2. RELATED WORK": "tures, with two fully connected layers on top, before feeding",
          "of the network is an average pooling.": ""
        },
        {
          "2. RELATED WORK": "",
          "of the network is an average pooling.": "such structure, we feed the extracted facial\nfeatures to a 2-"
        },
        {
          "2. RELATED WORK": "the features to a 2-layer LSTM network. VGG produced the",
          "of the network is an average pooling.": ""
        },
        {
          "2. RELATED WORK": "",
          "of the network is an average pooling.": "layer LSTM network."
        },
        {
          "2. RELATED WORK": "best results in their experiments. However, their models were",
          "of the network is an average pooling.": ""
        },
        {
          "2. RELATED WORK": "trained on a dataset\nthat consists of a small number of indi-",
          "of the network is an average pooling.": ""
        },
        {
          "2. RELATED WORK": "viduals (only 200), whereas our model was trained on more",
          "of the network is an average pooling.": ""
        },
        {
          "2. RELATED WORK": "",
          "of the network is an average pooling.": "4. MODEL PRETRAINING"
        },
        {
          "2. RELATED WORK": "than thousand subjects.\nThe interested reader\nis referenced",
          "of the network is an average pooling.": ""
        },
        {
          "2. RELATED WORK": "to [13] for more studies in the literature for visual-based emo-",
          "of the network is an average pooling.": "In this section, we describe the dataset provided by RealEyes"
        },
        {
          "2. RELATED WORK": "tion recognition.",
          "of the network is an average pooling.": "(Sec. 4.1).\nIn addition, we present the training methodology"
        },
        {
          "2. RELATED WORK": "",
          "of the network is an average pooling.": "used to tackle the unbalanced data (Sec. 4.2) along with the"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 4: The numberof framesof each emotion after over-",
      "data": [
        {
          "Table 2: The dataset statistics provided by RealEyes.": ""
        },
        {
          "Table 2: The dataset statistics provided by RealEyes.": ""
        },
        {
          "Table 2: The dataset statistics provided by RealEyes.": ""
        },
        {
          "Table 2: The dataset statistics provided by RealEyes.": "Emotion"
        },
        {
          "Table 2: The dataset statistics provided by RealEyes.": ""
        },
        {
          "Table 2: The dataset statistics provided by RealEyes.": "Happy"
        },
        {
          "Table 2: The dataset statistics provided by RealEyes.": ""
        },
        {
          "Table 2: The dataset statistics provided by RealEyes.": "Surprise"
        },
        {
          "Table 2: The dataset statistics provided by RealEyes.": "Confusion"
        },
        {
          "Table 2: The dataset statistics provided by RealEyes.": "Disgust"
        },
        {
          "Table 2: The dataset statistics provided by RealEyes.": "Contempt"
        },
        {
          "Table 2: The dataset statistics provided by RealEyes.": "Empathy"
        },
        {
          "Table 2: The dataset statistics provided by RealEyes.": ""
        },
        {
          "Table 2: The dataset statistics provided by RealEyes.": "Neutral"
        },
        {
          "Table 2: The dataset statistics provided by RealEyes.": ""
        },
        {
          "Table 2: The dataset statistics provided by RealEyes.": "Other"
        },
        {
          "Table 2: The dataset statistics provided by RealEyes.": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 4: The numberof framesof each emotion after over-",
      "data": [
        {
          "The annotation process provided the freedom to the an-": "notators to watch the videos, pause, re-wind, and then mark",
          "The window length was chosen to be 150 frames same as in": "our training framework."
        },
        {
          "The annotation process provided the freedom to the an-": "the start of an emotional state. The ﬁnal label of the frame is",
          "The window length was chosen to be 150 frames same as in": "An important\nissue that occurs is that emotions are tem-"
        },
        {
          "The annotation process provided the freedom to the an-": "given by a winner-takes-all approach, meaning the maximum",
          "The window length was chosen to be 150 frames same as in": "porally expressed. That\nis,\nin a sequence of frames. Hence,"
        },
        {
          "The annotation process provided the freedom to the an-": "number of votes for each emotion. Table 3 shows the number",
          "The window length was chosen to be 150 frames same as in": "our re-sampling strategy is based on this fact and we sampled"
        },
        {
          "The annotation process provided the freedom to the an-": "of frames per emotion.",
          "The window length was chosen to be 150 frames same as in": "sequences of\nframes instead of\njust\nindividual\nframes. Af-"
        },
        {
          "The annotation process provided the freedom to the an-": "",
          "The window length was chosen to be 150 frames same as in": "ter\nthis process, we randomly split\nthe dataset based on the"
        },
        {
          "The annotation process provided the freedom to the an-": "",
          "The window length was chosen to be 150 frames same as in": "participants to training and validation set (80% training, 20%"
        },
        {
          "The annotation process provided the freedom to the an-": "Attribute\nValue",
          "The window length was chosen to be 150 frames same as in": ""
        },
        {
          "The annotation process provided the freedom to the an-": "",
          "The window length was chosen to be 150 frames same as in": "validation). Table 4 shows the number of frames per emotion"
        },
        {
          "The annotation process provided the freedom to the an-": "# Videos\n4,973",
          "The window length was chosen to be 150 frames same as in": ""
        },
        {
          "The annotation process provided the freedom to the an-": "",
          "The window length was chosen to be 150 frames same as in": "after the re-sampling."
        },
        {
          "The annotation process provided the freedom to the an-": "# Frames\n1,059,505",
          "The window length was chosen to be 150 frames same as in": ""
        },
        {
          "The annotation process provided the freedom to the an-": "Frames per video µ(σ)\n380(308.39)",
          "The window length was chosen to be 150 frames same as in": ""
        },
        {
          "The annotation process provided the freedom to the an-": "",
          "The window length was chosen to be 150 frames same as in": "Emotion\n# Training Frames\n# Validation Frames"
        },
        {
          "The annotation process provided the freedom to the an-": "Image Resolution\n640 × 480",
          "The window length was chosen to be 150 frames same as in": ""
        },
        {
          "The annotation process provided the freedom to the an-": "# Subjects\n2,616",
          "The window length was chosen to be 150 frames same as in": "Happy\n298,222\n9,494"
        },
        {
          "The annotation process provided the freedom to the an-": "Females/Males\n2,983/1,990",
          "The window length was chosen to be 150 frames same as in": "237,041\n1,518\nSurprise"
        },
        {
          "The annotation process provided the freedom to the an-": "[18-69]\nAge variation",
          "The window length was chosen to be 150 frames same as in": "Confusion\n265,495\n9,377"
        },
        {
          "The annotation process provided the freedom to the an-": "# Annotators\n7",
          "The window length was chosen to be 150 frames same as in": "Disgust\n32,044\n346"
        },
        {
          "The annotation process provided the freedom to the an-": "8\n# Emotions",
          "The window length was chosen to be 150 frames same as in": "18,537\n569\nContempt"
        },
        {
          "The annotation process provided the freedom to the an-": "",
          "The window length was chosen to be 150 frames same as in": "Empathy\n21,444\n745"
        },
        {
          "The annotation process provided the freedom to the an-": "Table 2: The dataset statistics provided by RealEyes.",
          "The window length was chosen to be 150 frames same as in": ""
        },
        {
          "The annotation process provided the freedom to the an-": "",
          "The window length was chosen to be 150 frames same as in": "52,375\n1,703\nOther"
        },
        {
          "The annotation process provided the freedom to the an-": "",
          "The window length was chosen to be 150 frames same as in": "Neutral\n877,410\n18,895"
        },
        {
          "The annotation process provided the freedom to the an-": "",
          "The window length was chosen to be 150 frames same as in": "Total Count\n1,505,443\n43,492"
        },
        {
          "The annotation process provided the freedom to the an-": "Emotion\n# Frames",
          "The window length was chosen to be 150 frames same as in": ""
        },
        {
          "The annotation process provided the freedom to the an-": "",
          "The window length was chosen to be 150 frames same as in": "Table 4: The number of frames of each emotion after over-"
        },
        {
          "The annotation process provided the freedom to the an-": "Happy\n60,179",
          "The window length was chosen to be 150 frames same as in": ""
        },
        {
          "The annotation process provided the freedom to the an-": "",
          "The window length was chosen to be 150 frames same as in": "sampling, provided by RealEyes."
        },
        {
          "The annotation process provided the freedom to the an-": "Surprise\n15,786",
          "The window length was chosen to be 150 frames same as in": ""
        },
        {
          "The annotation process provided the freedom to the an-": "56,683\nConfusion",
          "The window length was chosen to be 150 frames same as in": ""
        },
        {
          "The annotation process provided the freedom to the an-": "Disgust\n10,065",
          "The window length was chosen to be 150 frames same as in": ""
        },
        {
          "The annotation process provided the freedom to the an-": "8,581\nContempt",
          "The window length was chosen to be 150 frames same as in": "4.3. Experimental Setup"
        },
        {
          "The annotation process provided the freedom to the an-": "Empathy\n10,173",
          "The window length was chosen to be 150 frames same as in": ""
        },
        {
          "The annotation process provided the freedom to the an-": "",
          "The window length was chosen to be 150 frames same as in": "We utilised the Adam optimisation algorithm to train the"
        },
        {
          "The annotation process provided the freedom to the an-": "Neutral\n840,611",
          "The window length was chosen to be 150 frames same as in": ""
        },
        {
          "The annotation process provided the freedom to the an-": "",
          "The window length was chosen to be 150 frames same as in": "model with a ﬁxed learning rate of 4 ∗ 10−5. We used a"
        },
        {
          "The annotation process provided the freedom to the an-": "57,425\nOther",
          "The window length was chosen to be 150 frames same as in": ""
        },
        {
          "The annotation process provided the freedom to the an-": "",
          "The window length was chosen to be 150 frames same as in": "mini-batch of size 2 with a sequence length of 150 frames."
        },
        {
          "The annotation process provided the freedom to the an-": "Total Count\n1,059,505",
          "The window length was chosen to be 150 frames same as in": "For our purposes we used data augmentation methods.\nIn"
        },
        {
          "The annotation process provided the freedom to the an-": "",
          "The window length was chosen to be 150 frames same as in": "particular, the data were augmented by resizing the image to"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "we weight each sample based on the reciprocal of the number": ""
        },
        {
          "we weight each sample based on the reciprocal of the number": "frames of each emotion. Hence,\nthe emotion with the lowest"
        },
        {
          "we weight each sample based on the reciprocal of the number": ""
        },
        {
          "we weight each sample based on the reciprocal of the number": "number of samples (Contempt) has the highest weight, while"
        },
        {
          "we weight each sample based on the reciprocal of the number": ""
        },
        {
          "we weight each sample based on the reciprocal of the number": "the emotion with the highest number of frames (Neutral) the"
        },
        {
          "we weight each sample based on the reciprocal of the number": ""
        },
        {
          "we weight each sample based on the reciprocal of the number": "lowest weight."
        },
        {
          "we weight each sample based on the reciprocal of the number": ""
        },
        {
          "we weight each sample based on the reciprocal of the number": "5. EXPERIMENTS"
        },
        {
          "we weight each sample based on the reciprocal of the number": ""
        },
        {
          "we weight each sample based on the reciprocal of the number": "5.1. Dataset"
        },
        {
          "we weight each sample based on the reciprocal of the number": ""
        },
        {
          "we weight each sample based on the reciprocal of the number": "To test our pre-trained model, we utilise the REmote COL-"
        },
        {
          "we weight each sample based on the reciprocal of the number": ""
        },
        {
          "we weight each sample based on the reciprocal of the number": "laborative and Affective (RECOLA) database introduced by"
        },
        {
          "we weight each sample based on the reciprocal of the number": "Ringeval et al. [16]. A part of the database was used in the Au-"
        },
        {
          "we weight each sample based on the reciprocal of the number": "dio/Visual Emotion Challenge and Workshop (AVEC) 2016-"
        },
        {
          "we weight each sample based on the reciprocal of the number": "2018 challenges [17].\nFor our purposes, we utilise the full"
        },
        {
          "we weight each sample based on the reciprocal of the number": "database, which contains 46 different recordings divided into"
        },
        {
          "we weight each sample based on the reciprocal of the number": "three different parts (train, devel, and test) while balancing the"
        },
        {
          "we weight each sample based on the reciprocal of the number": "gender, age, and mother tongue.\nIn total, four modalities are"
        },
        {
          "we weight each sample based on the reciprocal of the number": "contained in the database,\ni. e., audio, video, electrocardio-"
        },
        {
          "we weight each sample based on the reciprocal of the number": "gram (ECG), and electro-dermal activity (EDA). The original"
        },
        {
          "we weight each sample based on the reciprocal of the number": "labels of\nthe RECOLA are re-sampled at a constant\nframe"
        },
        {
          "we weight each sample based on the reciprocal of the number": "rate of 40 ms.\nThe data is then averaged over all\nraters by"
        },
        {
          "we weight each sample based on the reciprocal of the number": "considering the inter-evaluator agreement, to provide a ‘gold"
        },
        {
          "we weight each sample based on the reciprocal of the number": "standard’ [18]. Each record is 5 min (300 s) audio data with a"
        },
        {
          "we weight each sample based on the reciprocal of the number": "sampling rate of 16 kHz."
        },
        {
          "we weight each sample based on the reciprocal of the number": ""
        },
        {
          "we weight each sample based on the reciprocal of the number": ""
        },
        {
          "we weight each sample based on the reciprocal of the number": "5.2. Experimental Setup"
        },
        {
          "we weight each sample based on the reciprocal of the number": ""
        },
        {
          "we weight each sample based on the reciprocal of the number": "The optimisation method we used to train our model, through-"
        },
        {
          "we weight each sample based on the reciprocal of the number": ""
        },
        {
          "we weight each sample based on the reciprocal of the number": "out all experiments, is the Adam optimisation method with a"
        },
        {
          "we weight each sample based on the reciprocal of the number": ""
        },
        {
          "we weight each sample based on the reciprocal of the number": "ﬁxed learning rate of 4 ∗ 10−5. We used a mini-batch of size 2"
        },
        {
          "we weight each sample based on the reciprocal of the number": ""
        },
        {
          "we weight each sample based on the reciprocal of the number": "with a sequence length of 150 frames (12 s) - total 300 frames"
        },
        {
          "we weight each sample based on the reciprocal of the number": "per batch -\nin both training and testing.\nOur\nloss function"
        },
        {
          "we weight each sample based on the reciprocal of the number": ""
        },
        {
          "we weight each sample based on the reciprocal of the number": "is based on the concordance correlation coefﬁcient (ρc) [19],"
        },
        {
          "we weight each sample based on the reciprocal of the number": "and the best model is selected based on the highest ρc on the"
        },
        {
          "we weight each sample based on the reciprocal of the number": ""
        },
        {
          "we weight each sample based on the reciprocal of the number": "validation set.\nThe number of epochs was set\nto 300. We"
        },
        {
          "we weight each sample based on the reciprocal of the number": ""
        },
        {
          "we weight each sample based on the reciprocal of the number": "stopped the training process if no improvement on the vali-"
        },
        {
          "we weight each sample based on the reciprocal of the number": ""
        },
        {
          "we weight each sample based on the reciprocal of the number": "dation score was observed after 20 epochs. Finally, a chain"
        },
        {
          "we weight each sample based on the reciprocal of the number": ""
        },
        {
          "we weight each sample based on the reciprocal of the number": "of post processing methods is applied:\n(a) median ﬁltering"
        },
        {
          "we weight each sample based on the reciprocal of the number": ""
        },
        {
          "we weight each sample based on the reciprocal of the number": "(the size of the window was between 0.04 s and 20 s) [17], (b)"
        },
        {
          "we weight each sample based on the reciprocal of the number": ""
        },
        {
          "we weight each sample based on the reciprocal of the number": "centring (by ﬁnding the ground truth’s and the prediction’s"
        },
        {
          "we weight each sample based on the reciprocal of the number": ""
        },
        {
          "we weight each sample based on the reciprocal of the number": "bias)\n[20],\n(c) scaling (with the scaling factor\nthe ratio be-"
        },
        {
          "we weight each sample based on the reciprocal of the number": ""
        },
        {
          "we weight each sample based on the reciprocal of the number": "tween the standard deviation of the ground truth and the pre-"
        },
        {
          "we weight each sample based on the reciprocal of the number": ""
        },
        {
          "we weight each sample based on the reciprocal of the number": "diction) [20], and (d) time-shifting (forward in time with val-"
        },
        {
          "we weight each sample based on the reciprocal of the number": ""
        },
        {
          "we weight each sample based on the reciprocal of the number": "ues between 0.04 s and 10 s). Any of these methods is kept"
        },
        {
          "we weight each sample based on the reciprocal of the number": ""
        },
        {
          "we weight each sample based on the reciprocal of the number": "when we observe a better ρc on the validation set, and then"
        },
        {
          "we weight each sample based on the reciprocal of the number": ""
        },
        {
          "we weight each sample based on the reciprocal of the number": "applied to the test partition with the same conﬁguration."
        },
        {
          "we weight each sample based on the reciprocal of the number": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "7. REFERENCES": "",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": "November 2016, pp. 243–247."
        },
        {
          "7. REFERENCES": "[1] Q. Ji, Z. Zhu, and P. Lan, “Real-time nonintrusive moni-",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": ""
        },
        {
          "7. REFERENCES": "",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": "[12] O. M. Parkhi, A. Vedaldi, and A. Zisserman, “Deep face"
        },
        {
          "7. REFERENCES": "toring and prediction of driver fatigue,” Transactions on",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": ""
        },
        {
          "7. REFERENCES": "",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": "recognition,” in Proc. BMVC, 2015, p. 6."
        },
        {
          "7. REFERENCES": "Vehicular Technology, vol. 53, pp. 1052–1068, 2004.",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": ""
        },
        {
          "7. REFERENCES": "",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": "[13] Byoung Chul Ko,\n“A brief\nreview of\nfacial emotion"
        },
        {
          "7. REFERENCES": "[2] P.\nTzirakis,\nG.\nTrigeorgis, M.A. Nicolaou,\nB.W.",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": ""
        },
        {
          "7. REFERENCES": "",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": "recognition based on visual\ninformation,”\nsensors, vol."
        },
        {
          "7. REFERENCES": "Schuller,\nand S. Zafeiriou,\n“End-to-end multimodal",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": ""
        },
        {
          "7. REFERENCES": "",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": "18, no. 2, pp. 401, 2018."
        },
        {
          "7. REFERENCES": "emotion recognition using deep neural networks,” IEEE",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": ""
        },
        {
          "7. REFERENCES": "Journal of Selected Topics in Signal Processing, vol. 11,",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": ""
        },
        {
          "7. REFERENCES": "",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": "[14] S. Zhang, S. Zhang, T. Huang,\nand W. Gao,\n“Mul-"
        },
        {
          "7. REFERENCES": "pp. 1301–1309, 2017.",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": ""
        },
        {
          "7. REFERENCES": "",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": "timodal deep convolutional neural network for audio-"
        },
        {
          "7. REFERENCES": "",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": "visual emotion recognition,”\nin Proc. ICMR, 2016, pp."
        },
        {
          "7. REFERENCES": "[3] D. Kollias, P. Tzirakis, M. A Nicolaou, A. Papaioan-",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": ""
        },
        {
          "7. REFERENCES": "",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": "281–284."
        },
        {
          "7. REFERENCES": "nou, G. Zhao, B. Schuller,\nI. Kotsia, and S. Zafeiriou,",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": ""
        },
        {
          "7. REFERENCES": "“Deep affect prediction in-the-wild: Aff-wild database",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": ""
        },
        {
          "7. REFERENCES": "",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": "[15] H. Nam and B. Han, “Learning multi-domain convolu-"
        },
        {
          "7. REFERENCES": "arXiv\nand challenge, deep architectures, and beyond,”",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": ""
        },
        {
          "7. REFERENCES": "",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": "tional neural networks for visual tracking,” in Proceed-"
        },
        {
          "7. REFERENCES": "preprint arXiv:1804.10938, 2018.",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": ""
        },
        {
          "7. REFERENCES": "",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": "ings of\nthe IEEE Conference on Computer Vision and"
        },
        {
          "7. REFERENCES": "",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": "Pattern Recognition, 2016, pp. 4293–4302."
        },
        {
          "7. REFERENCES": "[4] P. Tzirakis, J. Chen, S. Zafeiriou, and B. Schuller, “End-",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": ""
        },
        {
          "7. REFERENCES": "to-end multimodal affect\nrecognition in real-world en-",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": ""
        },
        {
          "7. REFERENCES": "",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": "[16] F. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne,"
        },
        {
          "7. REFERENCES": "vironments,”\nInformation Fusion, vol. 68, pp. 46–53,",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": ""
        },
        {
          "7. REFERENCES": "",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": "“Introducing the recola multimodal corpus of\nremote"
        },
        {
          "7. REFERENCES": "2021.",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": ""
        },
        {
          "7. REFERENCES": "",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": "collaborative and affective interactions,”\nin Proc. FG."
        },
        {
          "7. REFERENCES": "",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": "IEEE, 2013, pp. 1–8."
        },
        {
          "7. REFERENCES": "[5] C. A. Corneanu, M. O. Sim´on,\nJ. F. Cohn, and S. E.",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": ""
        },
        {
          "7. REFERENCES": "Guerrero, “Survey on rgb, 3d, thermal, and multimodal",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": ""
        },
        {
          "7. REFERENCES": "",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": "[17] M.\nValstar,\nJ.\nGratch,\nB.\nSchuller,\nF.\nRingeval,"
        },
        {
          "7. REFERENCES": "approaches for\nfacial expression recognition: History,",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": ""
        },
        {
          "7. REFERENCES": "",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": "D. Lalanne, M. Torres, S. Scherer, G. Stratou, R. Cowie,"
        },
        {
          "7. REFERENCES": "trends, and affect-related applications,” Transactions on",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": ""
        },
        {
          "7. REFERENCES": "",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": "and M. Pantic,\n“Avec 2016: Depression, mood, and"
        },
        {
          "7. REFERENCES": "Pattern Analysis and Machine Intelligence, vol. 38, no.",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": ""
        },
        {
          "7. REFERENCES": "",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": "emotion recognition workshop and challenge,” in Proc."
        },
        {
          "7. REFERENCES": "8, pp. 1548–1568, 2016.",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": ""
        },
        {
          "7. REFERENCES": "",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": "AVEC Workshop. ACM, 2016, pp. 3–10."
        },
        {
          "7. REFERENCES": "[6] Evangelos Sariyanidi, Hatice Gunes, and Andrea Cav-",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": ""
        },
        {
          "7. REFERENCES": "",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": "[18]\nJ. Han,\nZ.\nZhang,\nF. Ringeval,\nand\nB.\nSchuller,"
        },
        {
          "7. REFERENCES": "allaro,\n“Automatic analysis of\nfacial affect: A survey",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": ""
        },
        {
          "7. REFERENCES": "",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": "“Reconstruction-error-based\nlearning\nfor\ncontinuous"
        },
        {
          "7. REFERENCES": "IEEE\nof registration, representation, and recognition,”",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": ""
        },
        {
          "7. REFERENCES": "",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": "emotion recognition in speech,” in Proc. ICASSP. IEEE,"
        },
        {
          "7. REFERENCES": "transactions on pattern analysis and machine intelli-",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": ""
        },
        {
          "7. REFERENCES": "",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": "2017, pp. 2367–2371."
        },
        {
          "7. REFERENCES": "gence, vol. 37, no. 6, pp. 1113–1133, 2015.",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": ""
        },
        {
          "7. REFERENCES": "",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": "[19] F. Ringeval, B. Schuller, M. Valstar, J. Gratch, R. Cowie,"
        },
        {
          "7. REFERENCES": "[7] P. Tzirakis, S. Zafeiriou, and B. Schuller, “Chapter 18 -",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": ""
        },
        {
          "7. REFERENCES": "",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": "S. Scherer, S. Mozgai, N. Cummins, M. Schmitt, and"
        },
        {
          "7. REFERENCES": "real-world automatic continuous affect recognition from",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": ""
        },
        {
          "7. REFERENCES": "",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": "M. Pantic, “Avec 2017: Real-life depression, and affect"
        },
        {
          "7. REFERENCES": "audiovisual signals,”\nin Multimodal Behavior Analysis",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": ""
        },
        {
          "7. REFERENCES": "",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": "recognition workshop and challenge,”\nin Proceedings"
        },
        {
          "7. REFERENCES": "in the Wild, vol. 1, pp. 387 – 406. Academic Press, 2019.",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": ""
        },
        {
          "7. REFERENCES": "",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": "of\nthe 7th Annual Workshop on Audio/Visual Emotion"
        },
        {
          "7. REFERENCES": "[8] K. He, X. Zhang, S. Ren, and J. Sun,\n“Deep residual",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": "Challenge. ACM, 2017, pp. 3–9."
        },
        {
          "7. REFERENCES": "learning for\nimage recognition,”\nin CVPR, 2016, pp.",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": ""
        },
        {
          "7. REFERENCES": "",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": "[20] M. K¨achele, P. Thiam, G. Palm, F. Schwenker,\nand"
        },
        {
          "7. REFERENCES": "770–778.",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": ""
        },
        {
          "7. REFERENCES": "",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": "M. Schels,\n“Ensemble methods\nfor\ncontinuous\naf-"
        },
        {
          "7. REFERENCES": "[9] F. Ringeval, A Sonderegger, J. Sauer, and D. Lalanne,",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": "fect recognition: Multi-modality, temporality, and chal-"
        },
        {
          "7. REFERENCES": "“Introducing the RECOLA Multimodal Corpus of Re-",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": "lenges,”\nin Proc. AVEC Workshop. ACM, 2015, pp. 9–"
        },
        {
          "7. REFERENCES": "mote Collaborative and Affective Interactions,” in Proc.",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": "16."
        },
        {
          "7. REFERENCES": "FG-Workshop. IEEE, 2013, pp. 1–8.",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": ""
        },
        {
          "7. REFERENCES": "",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": "[21] P. Tzirakis, S. Zafeiriou, and B. W. Schuller, “End2you–"
        },
        {
          "7. REFERENCES": "[10]\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": "the imperial toolkit for multimodal proﬁling by end-to-"
        },
        {
          "7. REFERENCES": "and Li Fei-Fei,\n“Imagenet: A large-scale hierarchical",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": "end learning,” arXiv preprint arXiv:1802.01115, 2018."
        },
        {
          "7. REFERENCES": "image database,” in 2009 IEEE conference on computer",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": ""
        },
        {
          "7. REFERENCES": "vision and pattern recognition. Ieee, 2009, pp. 248–255.",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": ""
        },
        {
          "7. REFERENCES": "[11] Yuchi Huang\nand Hanqing\nLu,\n“Deep\nlearning",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": ""
        },
        {
          "7. REFERENCES": "driven hypergraph representation for image-based emo-",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": ""
        },
        {
          "7. REFERENCES": "the International\ntion recognition,”\nin Proceedings of",
          "Conference on Multimodal\nInteraction, Tokyo,\nJapan,": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Real-time nonintrusive monitoring and prediction of driver fatigue",
      "authors": [
        "Q Ji",
        "Z Zhu",
        "P Lan"
      ],
      "year": "2004",
      "venue": "Transactions on Vehicular Technology"
    },
    {
      "citation_id": "3",
      "title": "End-to-end multimodal emotion recognition using deep neural networks",
      "authors": [
        "P Tzirakis",
        "G Trigeorgis",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2017",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "4",
      "title": "Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond",
      "authors": [
        "D Kollias",
        "P Tzirakis",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "B Schuller",
        "I Kotsia",
        "S Zafeiriou"
      ],
      "year": "2018",
      "venue": "Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond",
      "arxiv": "arXiv:1804.10938"
    },
    {
      "citation_id": "5",
      "title": "Endto-end multimodal affect recognition in real-world environments",
      "authors": [
        "P Tzirakis",
        "J Chen",
        "S Zafeiriou",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "6",
      "title": "Survey on rgb, 3d, thermal, and multimodal approaches for facial expression recognition: History, trends, and affect-related applications",
      "authors": [
        "C Corneanu",
        "M Simón",
        "J Cohn",
        "S Guerrero"
      ],
      "year": "2016",
      "venue": "Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "7",
      "title": "Automatic analysis of facial affect: A survey of registration, representation, and recognition",
      "authors": [
        "Evangelos Sariyanidi",
        "Hatice Gunes",
        "Andrea Cavallaro"
      ],
      "year": "2015",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "8",
      "title": "Chapter 18real-world automatic continuous affect recognition from audiovisual signals",
      "authors": [
        "P Tzirakis",
        "S Zafeiriou",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "Multimodal Behavior Analysis in the Wild"
    },
    {
      "citation_id": "9",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "CVPR"
    },
    {
      "citation_id": "10",
      "title": "Introducing the RECOLA Multimodal Corpus of Remote Collaborative and Affective Interactions",
      "authors": [
        "F Ringeval",
        "J Sonderegger",
        "D Sauer",
        "Lalanne"
      ],
      "year": "2013",
      "venue": "Proc. FG-Workshop"
    },
    {
      "citation_id": "11",
      "title": "Imagenet: A large-scale hierarchical image database",
      "authors": [
        "Jia Deng",
        "Wei Dong",
        "Richard Socher",
        "Li-Jia Li",
        "Kai Li",
        "Li Fei-Fei"
      ],
      "year": "2009",
      "venue": "Imagenet: A large-scale hierarchical image database"
    },
    {
      "citation_id": "12",
      "title": "Deep learning driven hypergraph representation for image-based emotion recognition",
      "authors": [
        "Yuchi Huang",
        "Hanqing Lu"
      ],
      "year": "2016",
      "venue": "Proceedings of the International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "13",
      "title": "Deep face recognition",
      "authors": [
        "O Parkhi",
        "A Vedaldi",
        "A Zisserman"
      ],
      "year": "2015",
      "venue": "Proc. BMVC"
    },
    {
      "citation_id": "14",
      "title": "A brief review of facial emotion recognition based on visual information",
      "authors": [
        "Chul Byoung",
        "Ko"
      ],
      "year": "2018",
      "venue": "sensors"
    },
    {
      "citation_id": "15",
      "title": "Multimodal deep convolutional neural network for audiovisual emotion recognition",
      "authors": [
        "S Zhang",
        "S Zhang",
        "T Huang",
        "W Gao"
      ],
      "year": "2016",
      "venue": "Proc. ICMR"
    },
    {
      "citation_id": "16",
      "title": "Learning multi-domain convolutional neural networks for visual tracking",
      "authors": [
        "H Nam",
        "B Han"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "17",
      "title": "Introducing the recola multimodal corpus of remote collaborative and affective interactions",
      "authors": [
        "F Ringeval",
        "A Sonderegger",
        "J Sauer",
        "D Lalanne"
      ],
      "year": "2013",
      "venue": "Proc. FG. IEEE"
    },
    {
      "citation_id": "18",
      "title": "Avec 2016: Depression, mood, and emotion recognition workshop and challenge",
      "authors": [
        "M Valstar",
        "J Gratch",
        "B Schuller",
        "F Ringeval",
        "D Lalanne",
        "M Torres",
        "S Scherer",
        "G Stratou",
        "R Cowie",
        "M Pantic"
      ],
      "year": "2016",
      "venue": "Proc. AVEC Workshop"
    },
    {
      "citation_id": "19",
      "title": "Reconstruction-error-based learning for continuous emotion recognition in speech",
      "authors": [
        "J Han",
        "Z Zhang",
        "F Ringeval",
        "B Schuller"
      ],
      "year": "2017",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "20",
      "title": "Avec 2017: Real-life depression, and affect recognition workshop and challenge",
      "authors": [
        "F Ringeval",
        "B Schuller",
        "M Valstar",
        "J Gratch",
        "R Cowie",
        "S Scherer",
        "S Mozgai",
        "N Cummins",
        "M Schmitt",
        "M Pantic"
      ],
      "year": "2017",
      "venue": "Proceedings of the 7th Annual Workshop on Audio/Visual Emotion Challenge"
    },
    {
      "citation_id": "21",
      "title": "Ensemble methods for continuous affect recognition: Multi-modality, temporality, and challenges",
      "authors": [
        "M Kächele",
        "P Thiam",
        "G Palm",
        "F Schwenker",
        "M Schels"
      ],
      "year": "2015",
      "venue": "Proc. AVEC Workshop"
    },
    {
      "citation_id": "22",
      "title": "End2youthe imperial toolkit for multimodal profiling by end-toend learning",
      "authors": [
        "P Tzirakis",
        "S Zafeiriou",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "End2youthe imperial toolkit for multimodal profiling by end-toend learning",
      "arxiv": "arXiv:1802.01115"
    }
  ]
}