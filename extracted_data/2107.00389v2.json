{
  "paper_id": "2107.00389v2",
  "title": "Investigating The Reliability Of Self-Report Data In The Wild: The Quest For Ground Truth",
  "published": "2021-07-01T12:01:34Z",
  "authors": [
    "Nan Gao",
    "Mohammad Saiedur Rahaman",
    "Wei Shao",
    "Flora D. Salim"
  ],
  "keywords": [
    "Self-report Measures",
    "Ecological Momentary Assessment",
    "Physiological Signals",
    "Reliability",
    "Emotion Prediction",
    "Ground Truth",
    "Field Study"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Inferring human mental state (e.g., emotion, depression, engagement) with sensing technology is one of the most valuable challenges in the affective computing area, which has a profound impact in all industries interacting with humans. Self-report is the most common way to quantify how people think, but prone to subjectivity and various responses bias. It is usually used as the ground truth for human mental state prediction. In recent years, many data-driven machine learning models are built based on self-report annotations as the target value. In this research, we investigate the reliability of self-report data in the wild by studying the confidence level of responses and survey completion time. We conduct a case study (i.e., student engagement inference) by recruiting 23 students in a high school setting over a period of 4 weeks. Overall, our participants volunteered 488 self-reported responses and sensing data from smart wristbands. We find that the physiologically measured student engagement and perceived student engagement are not always consistent. The findings from this research have great potential to benefit future studies in predicting engagement, depression, stress, and other emotion-related states in the field of affective computing and sensing technologies.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "In recent decades, with the advances of wearables and IoT devices, sensing technologies have been increasingly investigated to infer human emotion and mental characteristics, which becomes a hot topic in the Ubicomp community, especially surrounding the prediction of mood  [23, 34] , depression  [30, 33] , stress  [18] , engagement  [4, 10, 15] , concentration  [25] , personality traits  [11, 31]  etc. Understanding human emotion and mental state with sensing technologies in real-time can help design intervention strategies to prevent mental health issues among people.\n\nOne of the most commonly used methods for measuring emotion and mental state is to ask participants to respond to self-report surveys (e.g.,  [4, 11, 13] ). An alternative to self-report survey is the Ecological Momentary Assessment (EMA), which is designed to repeatedly collect human responses in real-time in natural settings. In the emotion sensing area, the responses from self-report surveys or EMAs are usually regarded as the measure of ground truth  [4, 10, 18, 29, 34]  when building the machine learning (ML) prediction model. They are usually served as the target variables while the features extracted from sensing data are used as the predictor in ML contexts. Then, the predictor is mapped to the target variables through the empirical relationship determined by the data. Moller et al.  [22]  pointed out researchers should not trust the selfreports blindly, but take into consideration that the responses can be unreliable.\n\nIn this research, we investigate the reliability of self-report data by investigating the patterns of the reported confidence level and survey completion time. Then we focus on the emotion sensing area and use the learning engagement as an example to compare the physiologically measured engagement and perceived engagement. We conduct a field study in a private high school, and 488 selfreport responses and wearable data are collected from 23 student participants over 144 classes and 10 courses for 4 weeks. In sum, our contributions are as follows:\n\n‚Ä¢ For the first time, we investigate the reliability of self-report data by studying the confidence level of self-reported responses. Then we compare the confidence level of responses with the survey completion time to better understand the reliability of self-report data. ‚Ä¢ Taking the student learning engagement as an example, we find the perceived student engagement and physiologically measured engagement are not always consistent. ‚Ä¢ We point out the risk of using subjective annotations as the ground truth, and discuss the possibility to use physiological signals as objective measures of student engagement.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Works 2.1 Inferring Emotion And Mental State With Sensing Technology",
      "text": "In Ubicomp community, many studies have assessed human emotion and mental characteristics with sensing technologies (e.g., engagement  [10, 15] , stress  [18] , mood  [23, 29] , depression  [1, 30] ), which provide an attractive alternative to traditional self-report surveys or EMAs. King et al.  [18]  proposed a passive sensing framework for detecting pregnant mothers in the wild, with the micro-EMA questions as a measurable ground truth for stress. Similarly, Gao et al.  [10]  predicted student learning engagement with physiological sensing data, with the adapted In-class Student Engagement Questionnaire (ISEQ)  [7]  as the ground truth of learning engagement. Wang et al.  [30]  tracked depression dynamics in college students using mobile and wearable sensing approaches, with PHQ-4  [19]  and PHQ-8  [20]  scores as the ground truth of depression. Zhang et al.  [34]  detected the human compound emotion from smartphone sensing data with the self-report responses as the ground truth of emotions. It has become a common practice to regard the subjective responses (e.g., EMA, self-report survey) as the ground truth, and features extracted from sensing data are fed into the data-driven model for emotion and mental state prediction.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Reliability Of Self-Report Data",
      "text": "Many researchers worked on designing or adapting psychology questionnaires to achieve higher validity and reliability and mitigate response biases  [2, 3, 14, 17, 28] . Clark et al.  [3]  reviewed recent literature for psychological scale validation and Huston et al.  [14]  compared the reliability of different forms of self-reported life satisfaction. Moller et al.  [22]  explored the reliability of selfreporting responses under different conditions. They conducted a six-week self-reporting study on smartphone usage. They found that self-reports cannot provide the full image of user behaviours and participants could significantly overestimate the duration of app usage. Though they showed the inaccuracy of self-reports, they gave suggestions for the design of a self-report study (e.g., set reminders, not overcharge participants ) instead of solutions to evaluating the reliability of self-reports. Moreover, they used the survey questions related to real-world behaviour (e.g., smartphone usage) which is easier to be quantified compared with subjective attitudes. Wash et al.  [32]  investigated the agreement between self-report and behaviors. They found that security research based on selfreports is unreliable for certain behaviours. Especially, when the behavior involves awareness rather than actions, people are less able to answer the questions accurately. Similar to  [22] , they revealed the unreliability of self-reports through comparing with the actual behaviours.\n\nDifferent from previous studies, this research has several advantages: (1) we investigate the reliability of self-report data through the subjective confidence level provided by users; (2) we reveal the risks of using self-report responses as the ground truth, especially for emotion sensing in Ubicomp community, by comparing the physiological measured engagement and perceived engagement.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Data Collection 3.1 Case Study",
      "text": "We collected a dataset  [8, 9]  (available on Figshare 1  ) from a field study in a high school over 4 weeks . The study has been approved by the Human Research Ethics Committee at RMIT University, which was furthermore approved by the principal of the high school. We have recruited 23 students (15-17 years old, 13 female and 10 male) and 6 teachers (33-62 years old, 4 female and 2 male) in Year 10. After returning the signed consent forms by teachers and students (and their guardians), the participants were asked to complete an online survey recording their demographic information (e.g., age, gender, class information, etc.).\n\nBefore the data collection, all Empatica E4 wristbands were synchronized with the E4 Manager App from the same laptop to make sure the internal clocks are correct. During the data collection, student participants were asked to wear the wristband on the nondominating hands at school-time. They were reminded by the class representative to complete online questionnaires (EMAs) three times a day at 11:00, 13:25, 15:35 (right after the 2 ùëõùëë , 4 ùë°‚Ñé , 5 ùë°‚Ñé class). For teacher participants, they only need to wear the wristband during their classes and complete the EMA right after their class.\n\nAs a token of appreciation, participants were distributed four movie vouchers for 4-week data collection. Participation in this research project was completely voluntary, and participants were free to withdraw from the project at any stage.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Measures",
      "text": "3.2.1 Student multi-dimensional engagement. We used the selfreport to collect subjective assessments of student engagement. It is the most commonly used method to measure student engagement, because it can clearly reflect subjective perceptions of students. According to previous studies  [5, 6] , other methods such as interviews, teacher ratings and observations are vulnerable to external factors. The student engagement questionnaire includes 5 items 2  related to the emotional, behavioural, and cognitive engagement of the validated In-class Student Engagement Questionnaires (ISEQ)  [7] , which was proved to be effective for multidimensional engagement measurement compared with the traditional long survey. Similar to previous studies  [13, 15] , we slightly adapted the questions to &RQILGHQFH/HYHO 1XPEHUVRI5HVSRQVHV Figure  1 : Distribution of confidence level for all self-report responses suit high school classes and make it easier for underage students to understand. In the questionnaire, each item is rated with a 5-point Likert scale from 'strongly disagree' to 'strongly agree'.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Confidence Level.",
      "text": "At the end of the self-report EMA, we asked the participants to choose their confidence level for their previous responses: \"Please rate your confidence level for your answers in this survey (optional)\". Then the participants need to choose their option from the 5-point Likert scales, where 1 = not confident, 2 = slightly confident, 3 = moderately confident, 4 = very confident, 5 = extremely confident. The default option is 3: moderately confident. We make this question optional rather than mandatory to minimize the possibility of users answering questions randomly.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Physiological Signals.",
      "text": "We assessed participants' physiological signals (EDA, PPG, ACC, ST) using the Empatica E4 wristbands. PPG sensor measures the blood volume pulse (BVP) at 64 Hz, from which the inter-beat interval (IBI) and heart rate variability (HRV) can be derived. ACC sensor records 3-axis acceleration at 32 Hz to capture motion-based activities. The optical thermometer captures peripheral skin temperature (ST) at 4 Hz. EDA sensor records the constantly fluctuating changes in the electrical properties of the skin at 4 Hz. When the level of sweat increases, the conductivity of the skin increases. For most people, when they experience increased cognitive workload, emotional arousal or physical exertion, the brain will send innervating signals to the skin to increase sweating. Even though they may not feel any sweat on the skin surface, the conductivity increases noticeably.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Reliability Of Self-Report Data 4.1 Confidence Level Of Responses",
      "text": "During the data collection process, we collected the confidence level of self-report from different participants. Figure  1  shows the distribution of confidence level of different participants. We can see that most participants have a moderate degree of confidence in their responses, but a small number of participants (whose confidence level is 1 or 2) are not very confident in their responses.\n\nThen, we investigate whether the same participant tends to have a similar confidence level. Figure  2  shows the boxplot of confidence level across different participants. We find that different participants tend to have very different confidence levels. For example, some participants (e.g., P1, P20) are usually strongly confident (>4) in their self-report responses, while some participants (e.g., P10, P12, P15) are generally not very confident in their responses. In addition, some participants (e.g., P1, P20, P15) tend to have similar confidence levels in longitudinal studies but some participants (e.g., P16, P3) have very different confidence levels at different times of data collection. The above phenomenon is in line with our daily experience.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Completion Time And Reliability",
      "text": "Malhotra et al.  [21]  found that the survey completion time is one of the indicators of response quality, although it is affected by multiple factors and varies from person to person. In this research, for each self-reported questionnaire, we collected the completion time automatically recorded by the Qualtrics timing question, which is a hidden question added to the questionnaire to track the time spent by the respondent on that page. Figure  3  shows the survey completion time for all participants. We can see that different participants have very different survey completion time. Most participants complete the survey between 30 to 50 seconds, however, some participants (e.g., P17) spend a lot more time to complete the survey and some participants (e.g., P10, P12) complete the survey in a very short time.\n\nThen we study whether the survey completion time is correlated with the confidence levels. Figure  4  shows that the survey completion time is positively related to the confidence level. Participants with higher survey completion time tend to have a higher confidence level of the survey. We also investigate how the confidence level correlated with other factors such as the time of the day and weekday, but we do not find the strong correlation between them.  In future research, it will be interesting to use survey completion time as an indicator of survey reliability and assign appropriate weights to self-report responses for more accurate human mentalstate prediction.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Perceived Vs. Physiologically Measured Engagement",
      "text": "For the calculation of the perceived engagement scores, we reversed the responses in item 2 and item 4 and then calculated an average score based on the 5-point Likert scale for each dimension of engagement. Then the overall engagement scores were calculated based on all the five items, where 1 indicates the lowest engagement and 5 is the highest engagement. Figure  5  shows the distribution of overall perceived engagement across student participants. We can see that different participants tend to have very different perceived engagement. Some participants (e.g., P1, P9, P14) are usually highly engaged in the class while some participants (e.g., P8) have low engagement levels. Gao et al.  [10]  built the engagement prediction model with the perceived engagement being regarded as the ground truth.\n\nPhysiological signals (e.g., EDA, HR, ST signals) have been explored in previous studies to infer student engagement level  [4, 10] . For example, the EDA level is usually considered a good indicator of physiological and psychological arousal (e.g., student engagement  [10] , emotional state  [4] ). Increased heart rate indicates the increased efforts and is used as an indirect measure of engagement  [26] . It has been shown that changes in heart rate are related to greater mental efforts and higher information processing demands. Additionally, changes in skin temperature were shown to be correlated with social and mood context  [16] .\n\nWe show an example of EDA changes for different participants in the same class in Figure  6 . It can be seen that the EDA signals of the first two participants are very similar and there is a strong physiological synchrony  [24]  between them. Physiological synchrony refers to the association or interdependence of physiological activity between two or more individuals, which has been found in many scenarios. Physiological synchrony between individuals can be indicative of group engagement  [24] , and has been used to measure the classroom emotional climate  [12]  and quantify participants' agreement on self-report engagement  [13] .\n\nIn Figure  6 , strong physiological synchrony between P15 and P17 indicates they have similar engagement patterns. Additionally, they both are likely to be highly engaged because (1) their EDA signals have multiple peaks at a similar time, which is a good indicator of emotion arousal; (2) if they are not engaged in class, their EDA changes should be more random instead of being similar. What's more, participant P20 is likely to have lower engagement than participants P17 and P20 since the EDA signal of P20 is more random and the number of peaks is not as many as that of the other two participants. However, based on the self-report responses, the engagement score of three participants P15, P17 and P20 are 4.2, 3.2 and 4.4. From this example, we find that (1) participants with very similar physiological patterns may have very different perceived engagement annotations (see P15 and P17); (2) participants with very similar annotations may have very different physiological patterns (see P15 and P20).",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Discussion And Conclusion",
      "text": "Self-report is one of the most common ways to study the human psychological state and attitude in human-based studies. In the affective computing area, self-report annotations are usually served as the ground truth for predicting human mental state with sensing technologies. Especially in recent years, various data-driven models are built with self-report data as the target variable. However, selfreport data is prone to subjectivity and various responses bias, making it risky and inaccurate to be used as the ground truth in predicting the psychological state (e.g., emotion, depression, engagement, etc.) from sensing data.\n\nIn this research, we investigate the reliability of self-report data in the wild from two aspects: (1) For the first time, we study the confidence level of self-report responses from participants, and compare the confidence level with the survey completion time to better understand the reliability of self-report data; (2) To the best of our knowledge, we are the first to compare the perceived and physiologically measures of student engagement. We find that the perceived self-report engagement are not always consistent with the physiologically measured engagement. Participants with similar physiological patterns may report very different perceived engagement and participants with similar self-report annotations may also have very different physiological patterns. By contrasting the self-report and physiological measures, we reveal the potential risks of only using subjective annotations as the ground truth.\n\nThis research is a very promising step towards the study of reliability of self-report data in the wild. It serves as a wake-up call for the emotion and mental sensing research in the Ubicomp community which usually regards the self-report annotations as the ground truth for predicting human mental state. Why do students feel more engaged in class if their bodies say otherwise? Should we trust their subjective self-report responses more, or their objective physiological responses? Is there a better way to understand and model human mental state instead of only using self-report annotations as the ground truth? We hope that more research will be done to explore this issue in the future.",
      "page_start": 5,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Distribution of confidence level for all self-report",
      "page": 3
    },
    {
      "caption": "Figure 1: shows the",
      "page": 3
    },
    {
      "caption": "Figure 2: shows the boxplot of confidence",
      "page": 3
    },
    {
      "caption": "Figure 2: Confidence level across different participants",
      "page": 3
    },
    {
      "caption": "Figure 3: Survey completion time of participants",
      "page": 3
    },
    {
      "caption": "Figure 3: shows the survey completion time for all participants.",
      "page": 3
    },
    {
      "caption": "Figure 4: shows that the survey comple-",
      "page": 3
    },
    {
      "caption": "Figure 4: Linear regression of survey completion time with",
      "page": 4
    },
    {
      "caption": "Figure 5: The distribution of overall engagement across stu-",
      "page": 4
    },
    {
      "caption": "Figure 5: shows the distribution of",
      "page": 4
    },
    {
      "caption": "Figure 6: An example of the EDA changes for three different",
      "page": 4
    },
    {
      "caption": "Figure 6: It can be seen that the EDA signals of",
      "page": 4
    },
    {
      "caption": "Figure 6: , strong physiological synchrony between P15 and",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "\u0000\u0014 \u0000\u0015",
          "Column_2": "\u0000\u0016 \u0000\u0017",
          "Column_3": "\u0000\u0018",
          "Column_4": "\u0000\u0019",
          "Column_5": "\u0000\u001a",
          "Column_6": "\u0000\u0014\u0000\u0013 \u0000\u0014",
          "Column_7": "\u0000\u0014 \u0000\u0014\u0000\u0015\n\u00003\u0000D\u0000U\u0000W\u0000L",
          "Column_8": "\u0000\u0014\u0000\u0016\n\u0000F\u0000L\u0000S\u0000D",
          "Column_9": "\u0000\u0014\u0000\u0017 \u0000\u0014\u0000\u0018\n\u0000Q\u0000W\u0000\u0003\u0000,\u0000'",
          "Column_10": "\u0000\u0014\u0000\u0019 \u0000\u0014\u0000\u001a",
          "Column_11": "\u0000\u0014\u0000\u001b",
          "Column_12": "\u0000\u0014\u0000",
          "Column_13": "\u0000\u0015\u0000\u0013 \u0000\u0015",
          "Column_14": "\u0000\u0014 \u0000\u0015\u0000\u0015",
          "Column_15": "\u0000\u0015\u0000\u0016"
        },
        {
          "Column_1": "ure2",
          "Column_2": ":Con",
          "Column_3": "fid",
          "Column_4": "en",
          "Column_5": "ce",
          "Column_6": "leve",
          "Column_7": "lac",
          "Column_8": "ros",
          "Column_9": "sdiff",
          "Column_10": "eren",
          "Column_11": "tpa",
          "Column_12": "rti",
          "Column_13": "cipa",
          "Column_14": "nts",
          "Column_15": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "\u0000\u0014",
          "Column_2": "\u0000\u0015",
          "Column_3": "\u0000\u0016",
          "Column_4": "\u0000\u0017",
          "Column_5": "\u0000\u0018",
          "Column_6": "\u0000\u0019",
          "Column_7": "\u0000\u001a",
          "Column_8": "\u0000\u001b",
          "Column_9": "\u0000",
          "Column_10": "\u0000\u0014\u0000\u0013\n\u00003\u0000D",
          "Column_11": "\u0000\u0014\u0000\u0014\n\u0000U\u0000W\u0000L\u0000F",
          "Column_12": "\u0000\u0014\u0000\u0015\n\u0000L\u0000S\u0000D",
          "Column_13": "\u0000\u0014\u0000\u0016\n\u0000Q\u0000W\u0000\u0003",
          "Column_14": "\u0000\u0014\u0000\u0017\n\u0000,\u0000'",
          "Column_15": "\u0000\u0014\u0000\u0018",
          "Column_16": "\u0000\u0014\u0000\u0019",
          "Column_17": "\u0000\u0014\u0000\u001a",
          "Column_18": "\u0000\u0014\u0000\u001b",
          "Column_19": "\u0000\u0014\u0000",
          "Column_20": "\u0000\u0015\u0000\u0013",
          "Column_21": "\u0000\u0015\u0000\u0014",
          "Column_22": "\u0000\u0015\u0000\u0015",
          "Column_23": "\u0000\u0015\u0000\u0016"
        },
        {
          "Column_1": "re",
          "Column_2": "5:",
          "Column_3": "T",
          "Column_4": "he",
          "Column_5": "di",
          "Column_6": "str",
          "Column_7": "ib",
          "Column_8": "ut",
          "Column_9": "io",
          "Column_10": "no",
          "Column_11": "f",
          "Column_12": "ov",
          "Column_13": "er",
          "Column_14": "all",
          "Column_15": "en",
          "Column_16": "ga",
          "Column_17": "ge",
          "Column_18": "m",
          "Column_19": "en",
          "Column_20": "t",
          "Column_21": "acr",
          "Column_22": "os",
          "Column_23": "s"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "What's Your Current Stress Level? Detection of Stress Patterns from GSR Sensor Data",
      "authors": [
        "Jorn Bakker",
        "Mykola Pechenizkiy",
        "Natalia Sidorova"
      ],
      "year": "2011",
      "venue": "IEEE"
    },
    {
      "citation_id": "2",
      "title": "Not Another Questionnaire! Maximizing the Response Rate, Predicting Nonresponse and Assessing Non-response Bias in Postal Questionnaire Studies of GPs",
      "authors": [
        "Stephen Barclay",
        "Chris Todd",
        "Ilora Finlay",
        "Gunn Grande",
        "Penny Wyatt"
      ],
      "year": "2002",
      "venue": "Not Another Questionnaire! Maximizing the Response Rate, Predicting Nonresponse and Assessing Non-response Bias in Postal Questionnaire Studies of GPs"
    },
    {
      "citation_id": "3",
      "title": "Constructing validity: New developments in creating objective measuring instruments",
      "authors": [
        "Lee Anna",
        "David Watson"
      ],
      "year": "2019",
      "venue": "Constructing validity: New developments in creating objective measuring instruments"
    },
    {
      "citation_id": "4",
      "title": "Unobtrusive Assessment of Students' Emotional Engagement during Lectures using Electrodermal Activity Sensors",
      "authors": [
        "Elena Di Lascio",
        "Shkurta Gashi",
        "Silvia Santini"
      ],
      "year": "2018",
      "venue": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies"
    },
    {
      "citation_id": "5",
      "title": "School Engagement: Potential of the Concept, State of the Evidence",
      "authors": [
        "Jennifer Fredricks",
        "Phyllis Blumenfeld",
        "Alison Paris"
      ],
      "year": "2004",
      "venue": "Review of Educational Research"
    },
    {
      "citation_id": "6",
      "title": "The Measurement of Student Engagement: A Comparative Analysis of Various Methods and Student Selfreport Instruments",
      "authors": [
        "A Jennifer",
        "Wendy Fredricks",
        "Mccolskey"
      ],
      "year": "2012",
      "venue": "Handbook of Research on Student Engagement"
    },
    {
      "citation_id": "7",
      "title": "Development of a Self-report Instrument for Measuring in-class Student Engagement Reveals that Pretending to Engage is a Significant Unrecognized Problem",
      "authors": [
        "Kathryn Fuller",
        "S Nilushi",
        "Som Karunaratne",
        "Betty Naidu",
        "Jennifer Exintaris",
        "Michael Short",
        "Scott Wolcott",
        "Paul Singleton",
        "White"
      ],
      "year": "2018",
      "venue": "PLoS ONE"
    },
    {
      "citation_id": "8",
      "title": "Gauge and En-Gage Datasets. Figshare",
      "authors": [
        "Nan Gao",
        "Max Marschall",
        "Jane Burry",
        "Simon Watkins",
        "Flora Salim"
      ],
      "year": "2021",
      "venue": "Gauge and En-Gage Datasets. Figshare",
      "doi": "10.25439/rmt.14578908"
    },
    {
      "citation_id": "9",
      "title": "Understanding Occupants' Behaviour, Engagement, Emotion, and Comfort Indoors with Heterogeneous Sensors and Wearables",
      "authors": [
        "Nan Gao",
        "Max Marschall",
        "Jane Burry",
        "Simon Watkins",
        "Flora Salim"
      ],
      "year": "2021",
      "venue": "Understanding Occupants' Behaviour, Engagement, Emotion, and Comfort Indoors with Heterogeneous Sensors and Wearables",
      "arxiv": "arXiv:2105.06637[cs.HC]"
    },
    {
      "citation_id": "10",
      "title": "Gage: Predicting in-class Emotional, Behavioural and Cognitive Engagement in the Wild",
      "authors": [
        "Nan Gao",
        "Wei Shao",
        "Mohammad Saiedur Rahaman",
        "Flora Salim"
      ],
      "year": "2020",
      "venue": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies"
    },
    {
      "citation_id": "11",
      "title": "Predicting Personality Traits from Physical Activity Intensity",
      "authors": [
        "Nan Gao",
        "Wei Shao",
        "Flora Salim"
      ],
      "year": "2019",
      "venue": "Computer"
    },
    {
      "citation_id": "12",
      "title": "Using Students' Physiological Synchrony to Quantify the Classroom Emotional Climate",
      "authors": [
        "Shkurta Gashi",
        "Elena Di Lascio",
        "Silvia Santini"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 ACM International Joint Conference and 2018 International Symposium on Pervasive and Ubiquitous Computing and Wearable Computers"
    },
    {
      "citation_id": "13",
      "title": "Using Unobtrusive Wearable Sensors to Measure the Physiological Synchrony between Presenters and Audience Members",
      "authors": [
        "Shkurta Gashi",
        "Elena Di Lascio",
        "Silvia Santini"
      ],
      "year": "2019",
      "venue": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies"
    },
    {
      "citation_id": "14",
      "title": "Comparing the Reliability and Validity of Global Self-Report Measures of Subjective Well-Being With Experiential Day Reconstruction Measures",
      "authors": [
        "Nathan Hudson",
        "Ivana Anusic",
        "Richard Lucas",
        "M Brent Donnellan"
      ],
      "year": "2020",
      "venue": "Assessment",
      "doi": "10.1177/1073191117744660"
    },
    {
      "citation_id": "15",
      "title": "EngageMon: Multi-Modal Engagement Sensing for Mobile Games",
      "authors": [
        "Sinh Huynh",
        "Seungmin Kim",
        "Jeonggil Ko",
        "Rajesh Krishna Balan",
        "Youngki Lee"
      ],
      "year": "2018",
      "venue": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies"
    },
    {
      "citation_id": "16",
      "title": "Thermal Infrared Imaging in Psychophysiology: Potentialities and Limits",
      "authors": [
        "Stephanos Ioannou",
        "Vittorio Gallese",
        "Arcangelo Merla"
      ],
      "year": "2014",
      "venue": "Thermal Infrared Imaging in Psychophysiology: Potentialities and Limits"
    },
    {
      "citation_id": "17",
      "title": "Stepovers and Signal Detection: Response Sensitivity and Bias in the Differentiation of Genuine and Deceptive Football Actions",
      "authors": [
        "Robin Jackson",
        "Hayley Barton",
        "Kelly Ashford",
        "Bruce Abernethy"
      ],
      "year": "2018",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "18",
      "title": "Micro-stress EMA: A Passive Sensing Framework for Detecting in-the-wild Stress in Pregnant Mothers",
      "authors": [
        "Zachary D King",
        "Judith Moskowitz",
        "Begum Egilmez",
        "Shibo Zhang",
        "Lida Zhang",
        "Michael Bass",
        "John Rogers",
        "Roozbeh Ghaffari",
        "Laurie Wakschlag",
        "Nabil Alshurafa"
      ],
      "year": "2019",
      "venue": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies"
    },
    {
      "citation_id": "19",
      "title": "An Ultra-brief Screening Scale for Anxiety and Depression: The PHQ-4. Psychosomatics",
      "authors": [
        "Kurt Kroenke",
        "Robert Spitzer",
        "Janet Williams",
        "Bernd L√∂we"
      ],
      "year": "2009",
      "venue": "An Ultra-brief Screening Scale for Anxiety and Depression: The PHQ-4. Psychosomatics"
    },
    {
      "citation_id": "20",
      "title": "The PHQ-8 as a Measure of Current Depression in the General Population",
      "authors": [
        "Kurt Kroenke",
        "Tara Strine",
        "Robert Spitzer",
        "Janet Williams",
        "Joyce Berry",
        "Ali Mokdad"
      ],
      "year": "2009",
      "venue": "Journal of Affective Disorders"
    },
    {
      "citation_id": "21",
      "title": "Completion Time and Response Order Effects in Web Surveys",
      "authors": [
        "Neil Malhotra"
      ],
      "year": "2008",
      "venue": "Public Opinion Quarterly"
    },
    {
      "citation_id": "22",
      "title": "Investigating Self-reporting Behavior in Long-term Studies",
      "authors": [
        "Andreas M√∂ller",
        "Matthias Kranz",
        "Barbara Schmid",
        "Luis Roalter",
        "Stefan Diewald"
      ],
      "year": "2013",
      "venue": "Proceedings of the SIGCHI Conference on Human Factors in Computing Systems"
    },
    {
      "citation_id": "23",
      "title": "Prediction of Mood Instability with Passive Sensing",
      "authors": [
        "Koustuv Mehrab Bin Morshed",
        "Richard Saha",
        "Sidney Li",
        "Munmun D'mello",
        "Gregory Choudhury",
        "Thomas Abowd",
        "Pl√∂tz"
      ],
      "year": "2019",
      "venue": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies"
    },
    {
      "citation_id": "24",
      "title": "Interpersonal Autonomic Physiology: A Systematic Review of the Literature. Personality and Social Psychology Review",
      "authors": [
        "Marisa Richard V Palumbo",
        "Lisa Marraccini",
        "Oliver Weyandt",
        "Heather Wilder-Smith",
        "Siwei Mcgee",
        "Matthew Liu",
        "Goodwin"
      ],
      "year": "2017",
      "venue": "Interpersonal Autonomic Physiology: A Systematic Review of the Literature. Personality and Social Psychology Review"
    },
    {
      "citation_id": "25",
      "title": "An Ambient-physical System to Infer Concentration in Open-plan Workplace",
      "authors": [
        "Mohammad Saiedur Rahaman",
        "Jonathan Liono",
        "Yongli Ren",
        "Jeffrey Chan",
        "Shaw Kudo",
        "Tim Rawling",
        "Flora Salim"
      ],
      "year": "2020",
      "venue": "IEEE Internet of Things Journal",
      "doi": "10.1109/JIOT.2020.2996219"
    },
    {
      "citation_id": "26",
      "title": "Engagement in Video and Audio Narratives: Contrasting Self-report and Physiological Measures",
      "authors": [
        "Nicole Daniel C Richardson",
        "Lara Griffin",
        "Auburn Zaki",
        "Jiachen Stephenson",
        "Thomas Yan",
        "Richard Curry",
        "John Noble",
        "Jeremy Hogan",
        "Joseph Skipper",
        "Devlin"
      ],
      "year": "2020",
      "venue": "Engagement in Video and Audio Narratives: Contrasting Self-report and Physiological Measures"
    },
    {
      "citation_id": "27",
      "title": "A Motivational Perspective on Engagement and Disaffection: Conceptualization and Assessment of Children's Behavioral and Emotional Participation in Academic Activities in the Classroom",
      "authors": [
        "Ellen Skinner",
        "Thomas Kindermann",
        "Carrie Furrer"
      ],
      "year": "2009",
      "venue": "Educational and Psychological Measurement"
    },
    {
      "citation_id": "28",
      "title": "Ineffectiveness of Reverse Wording of Questionnaire Items: Let's Learn from Cows in the Rain",
      "authors": [
        "Eric Van Sonderen",
        "Robbert Sanderman",
        "James Coyne"
      ],
      "year": "2013",
      "venue": "PLoS ONE"
    },
    {
      "citation_id": "29",
      "title": "StudentLife: Assessing Mental Health, Academic Performance and Behavioral Trends of College Students using Smartphones",
      "authors": [
        "Rui Wang",
        "Fanglin Chen",
        "Zhenyu Chen",
        "Tianxing Li",
        "Gabriella Harari",
        "Stefanie Tignor",
        "Xia Zhou",
        "Dror Ben-Zeev",
        "Andrew Campbell"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing"
    },
    {
      "citation_id": "30",
      "title": "Tracking Depression Dynamics in College Students using Mobile Phone and Wearable Sensing",
      "authors": [
        "Rui Wang",
        "Weichen Wang",
        "Alex Dasilva",
        "Jeremy Huckins",
        "William Kelley",
        "Todd Heatherton",
        "Andrew Campbell"
      ],
      "year": "2018",
      "venue": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies"
    },
    {
      "citation_id": "31",
      "title": "Sensing Behavioral Change over Time: Using Within-person Variability Features from Mobile Sensing to Predict Personality Traits",
      "authors": [
        "Weichen Wang",
        "Gabriella Harari",
        "Rui Wang",
        "Shayan Sandrine R M√ºller",
        "Kizito Mirjafari",
        "Andrew Masaba",
        "Campbell"
      ],
      "year": "2018",
      "venue": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies"
    },
    {
      "citation_id": "32",
      "title": "Can People Self-Report Security Accurately? Agreement Between Self-Report and Behavioral Measures",
      "authors": [
        "Rick Wash",
        "Emilee Rader",
        "Chris Fennell"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems"
    },
    {
      "citation_id": "33",
      "title": "Leveraging Routine Behavior and Contextuallyfiltered Features for Depression Detection among College Students",
      "authors": [
        "Xuhai Xu",
        "Prerna Chikersal",
        "Afsaneh Doryab",
        "Daniella Villalba",
        "Janine Dutcher",
        "Michael Tumminia",
        "Tim Althoff",
        "Sheldon Cohen",
        "Kasey Creswell",
        "J David Creswell"
      ],
      "year": "2019",
      "venue": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies"
    },
    {
      "citation_id": "34",
      "title": "Moodexplorer: Towards Compound Emotion Detection via Smartphone Sensing",
      "authors": [
        "Xiao Zhang",
        "Wenzhong Li",
        "Xu Chen",
        "Sanglu Lu"
      ],
      "year": "2018",
      "venue": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies"
    }
  ]
}