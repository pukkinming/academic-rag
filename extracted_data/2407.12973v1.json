{
  "paper_id": "2407.12973v1",
  "title": "Temporal Label Hierachical Network For Compound Emotion Recognition",
  "published": "2024-07-17T19:38:44Z",
  "authors": [
    "Sunan Li",
    "Hailun Lian",
    "Cheng Lu",
    "Yan Zhao",
    "Tianhua Qi",
    "Hao Yang",
    "Yuan Zong",
    "Wenming Zheng"
  ],
  "keywords": [
    "CCS CONCEPTS",
    "Human-centered computing → Human computer interaction (HCI);",
    "Computing methodologies → Artificial intelligence Multimodal emotion recognition, compound emotion, feature fusion"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The emotion recognition has attracted more attention in recent decades. Although significant progress has been made in the recognition technology of the seven basic emotions, existing methods are still hard to tackle compound emotion recognition that occurred commonly in practical application. This article introduces our achievements in the 7th Field Emotion Behavior Analysis (ABAW) competition. In the competition, we selected pre trained ResNet18 and Transformer, which have been widely validated, as the basic network framework. Considering the continuity of emotions over time, we propose a time pyramid structure network for frame level emotion prediction. Furthermore. At the same time, in order to address the lack of data in composite emotion recognition, we utilized fine-grained labels from the DFEW database to construct training data for emotion categories in competitions. Taking into account the characteristics of valence arousal of various complex emotions,",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition is a technology aimed at endowing machines with the ability to identify, process, and understand human emotions. For example, previous work often using elaborated designed hand-crafted features such as LBP and IS09 and machine learning based methods support vector machine (SVM)  [1] , Gaussian mixture model (GMM)  [3] , supervised dictionary learning  [2]  and sparse representation  [19]    [20]  to classify emotion class. In recent years, with the rapid advancement of deep learning techniques, various emotion recognition methods have been proposed. For example, in  [17]  Li et.al using label revision method to cope with emotion recognition in nosiy environments. In  [18]  Lu et.al impose sparse constrain on the reconstruction matrix to select more effective features However, these methods often focus on the recognition of seven basic emotions. In practical applications, complex emotions composed of combinations of these basic emotions are more commonly encountered. There is relatively little research in this domain, and the lack of high-quality databases for complex emotions hinders further development in this field. To facilitate the development of compound emotions recognition, the 7th Affective Behavior Analysis in-the-wild (ABAW) hold the Compound Expression (CE) Recogntion based on the C-EXPR-DB database  [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14] [15] [16] 21] .\n\nThe remainder of this paper is organized as follows. The framework of our modal including training dataset preparation and the backbone of our methods are described in section 2. In section 3, we show our experimental results on the challenge dataset to evaluate the effectiveness of our proposed method. Finally, in section 4, we conclude this paper.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "The Proposed Method 2.1 Feature Extraction And Fusion",
      "text": "We first employed the OpenFace toolkit for face detection in every frame of the video. For images where faces were difficult to detect, we supplemented with the closest temporally adjacent face to obtain a face image corresponding to every original video frame. Considering the temporal continuity of emotional states, despite the task requiring emotion classification prediction for each frame, we constructed a temporal pyramid structure of image sequences to acquire more robust emotional features. Three sets of image sequences at different temporal scales were composed as follows: a sequence of 15 frames starting from the current frame, a sequence of 15 frames sampled from a quarter-length segment of the video where the current frame resides, and a sequence of 15 frames sampled from the entire video. These hierarchical sequences of three image sets were parallelly fed into a spatiotemporal feature extraction network consisting of ResNet18 and Transformer. For each frame image, we averaged the sum of all classification results obtained from different image sequences to derive the final classification result.\n\nAdditionally, due to inherent data imbalance in the training set, as depicted in Fig.  1  of emotional cycles, we utilized the DFEW database to train the network for positive and negative classification in valence and arousal to assist the final classification results. Specifically, for each frame, if both valence and arousal were positive, it was directly categorized as a compound emotion of happinesssurprise; if both were negative, a judgment was made among three compound emotions carrying sadness components. Since other sets of compound emotions do not exhibit mutual exclusivity in valence and arousal, valence and arousal were not used as assisting information for their recognition. The overall network framework is illustrated in Fig.  2 .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Experiment 3.1 Dataset And Preprocessing",
      "text": "To select appropriate training and validation data, we utilized the DFEW database, which features detailed emotion labels. Established by Jiang et al. in 2020, this database initially collected over 1500 high-resolution movie clips depicting near-real scenarios, yielding 16,372 facial expression videos. Each video segment was independently labeled by 10 annotators with one of the basic emotions (happiness, sadness, neutral, anger, surprise, disgust, fear). The final true label for each video segment was determined based on emotions chosen by more than 6 annotators. Ultimately, 12,059 video segments were selected. For the competition task, we curated a training set comprising 1864 samples, ensuring each component of composite emotions was represented by ratings from at least 3 annotators. Considering significant sample imbalances and the mutual exclusivity of happiness and disgust across the seven composite emotions, additional single-emotion data from the DFEW database were included to balance the dataset. Furthermore, recognizing the unique positions of happiness and sadness on the emotional wheel, we performed valence and arousal-based positive classification using the DFEW database to assist in determining the final emotional categories.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Training Settings",
      "text": "All image resolution used in this paper is consistently set to 224 × 224. During training, the number of epochs is set to 50. Crossentropy is utilized as the classification loss function, and the Adam is selected as the optimizer, and the learning rate is set at 3e-4 according to experiment performance, and the batch size is 90.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Result And Discussion",
      "text": "According to the performance assessment rules of the competition, the evaluation the performance of compound expressions recognition by the average F1 Score across all 7 compound expressions. Therefore, the evaluation criterion is:",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we propose a hierarchical composite emotion recognition network in both temporal and label spaces. The emotional category for each frame is determined by aggregating classification information from image sequences across different time spans to provide final discriminative information. Simultaneously, a hierarchical classification strategy is designed based on the differences in emotional dynamics across emotional composites. The final results demonstrate promising performance on ABAW7.",
      "page_start": 2,
      "page_end": 2
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: of emotional cycles, we utilized the DFEW data-",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "xhzongyuan@seu.edu.cn": "",
          "Southeast University": "Nanjing, China"
        },
        {
          "xhzongyuan@seu.edu.cn": "",
          "Southeast University": "wenming_zheng@seu.edu.cn"
        },
        {
          "xhzongyuan@seu.edu.cn": "",
          "Southeast University": "we constructed a classification framework from coarse to fine in"
        },
        {
          "xhzongyuan@seu.edu.cn": "ABSTRACT",
          "Southeast University": ""
        },
        {
          "xhzongyuan@seu.edu.cn": "",
          "Southeast University": "the label space."
        },
        {
          "xhzongyuan@seu.edu.cn": "The emotion recognition has attracted more attention in recent",
          "Southeast University": ""
        },
        {
          "xhzongyuan@seu.edu.cn": "decades. Although significant progress has been made in the recog-",
          "Southeast University": ""
        },
        {
          "xhzongyuan@seu.edu.cn": "nition technology of the seven basic emotions, existing methods are",
          "Southeast University": "CCS CONCEPTS"
        },
        {
          "xhzongyuan@seu.edu.cn": "still hard to tackle compound emotion recognition that occurred",
          "Southeast University": "• Human-centered computing → Human computer interac-"
        },
        {
          "xhzongyuan@seu.edu.cn": "commonly in practical application. This article introduces our",
          "Southeast University": "tion (HCI); • Computing methodologies → Artificial intelli-"
        },
        {
          "xhzongyuan@seu.edu.cn": "achievements in the 7th Field Emotion Behavior Analysis (ABAW)",
          "Southeast University": "gence."
        },
        {
          "xhzongyuan@seu.edu.cn": "competition. In the competition, we selected pre trained ResNet18",
          "Southeast University": ""
        },
        {
          "xhzongyuan@seu.edu.cn": "and Transformer, which have been widely validated, as the basic",
          "Southeast University": "KEYWORDS"
        },
        {
          "xhzongyuan@seu.edu.cn": "network framework. Considering the continuity of emotions over",
          "Southeast University": "Multimodal emotion recognition, compound emotion, feature fu-"
        },
        {
          "xhzongyuan@seu.edu.cn": "time, we propose a time pyramid structure network for frame level",
          "Southeast University": "sion"
        },
        {
          "xhzongyuan@seu.edu.cn": "emotion prediction. Furthermore. At the same time, in order to ad-",
          "Southeast University": ""
        },
        {
          "xhzongyuan@seu.edu.cn": "",
          "Southeast University": "ACM Reference Format:"
        },
        {
          "xhzongyuan@seu.edu.cn": "dress the lack of data in composite emotion recognition, we utilized",
          "Southeast University": ""
        },
        {
          "xhzongyuan@seu.edu.cn": "",
          "Southeast University": "Sunan Li†, Hailun Lian†, Cheng Lu, Yan Zhao, Tianhua Qi, Hao Yang, Yuan"
        },
        {
          "xhzongyuan@seu.edu.cn": "fine-grained labels from the DFEW database to construct training",
          "Southeast University": ""
        },
        {
          "xhzongyuan@seu.edu.cn": "",
          "Southeast University": "Zong∗, and Wenming Zheng∗. 2024. Temporal Label Hierachical Network"
        },
        {
          "xhzongyuan@seu.edu.cn": "data for emotion categories in competitions. Taking into account",
          "Southeast University": ""
        },
        {
          "xhzongyuan@seu.edu.cn": "",
          "Southeast University": "In Proceedings of ACM Conference"
        },
        {
          "xhzongyuan@seu.edu.cn": "the characteristics of valence arousal of various complex emotions,",
          "Southeast University": ""
        },
        {
          "xhzongyuan@seu.edu.cn": "",
          "Southeast University": "(Conference’17). ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/"
        },
        {
          "xhzongyuan@seu.edu.cn": "",
          "Southeast University": "nnnnnnn.nnnnnnn"
        },
        {
          "xhzongyuan@seu.edu.cn": "∗Corresponding author.",
          "Southeast University": ""
        },
        {
          "xhzongyuan@seu.edu.cn": "†Both authors contributed equally to this research.",
          "Southeast University": ""
        },
        {
          "xhzongyuan@seu.edu.cn": "",
          "Southeast University": "1\nINTRODUCTION"
        },
        {
          "xhzongyuan@seu.edu.cn": "",
          "Southeast University": "Emotion recognition is a technology aimed at endowing machines"
        },
        {
          "xhzongyuan@seu.edu.cn": "Permission to make digital or hard copies of all or part of this work for personal or",
          "Southeast University": ""
        },
        {
          "xhzongyuan@seu.edu.cn": "",
          "Southeast University": "with the ability to identify, process, and understand human emo-"
        },
        {
          "xhzongyuan@seu.edu.cn": "classroom use is granted without fee provided that copies are not made or distributed",
          "Southeast University": ""
        },
        {
          "xhzongyuan@seu.edu.cn": "for profit or commercial advantage and that copies bear this notice and the full citation",
          "Southeast University": "tions. For example, previous work often using elaborated designed"
        },
        {
          "xhzongyuan@seu.edu.cn": "on the first page. Copyrights for components of this work owned by others than ACM",
          "Southeast University": ""
        },
        {
          "xhzongyuan@seu.edu.cn": "",
          "Southeast University": "hand-crafted features such as LBP and IS09 and machine learning"
        },
        {
          "xhzongyuan@seu.edu.cn": "must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,",
          "Southeast University": ""
        },
        {
          "xhzongyuan@seu.edu.cn": "",
          "Southeast University": "based methods support vector machine (SVM) [1], Gaussian mixture"
        },
        {
          "xhzongyuan@seu.edu.cn": "to post on servers or to redistribute to lists, requires prior specific permission and/or a",
          "Southeast University": ""
        },
        {
          "xhzongyuan@seu.edu.cn": "fee. Request permissions from permissions@acm.org.",
          "Southeast University": "model (GMM) [3], supervised dictionary learning [2] and sparse"
        },
        {
          "xhzongyuan@seu.edu.cn": "Conference’17, July 2017, Washington, DC, USA",
          "Southeast University": "representation [19] [20] to classify emotion class. In recent years,"
        },
        {
          "xhzongyuan@seu.edu.cn": "© 2024 Association for Computing Machinery.",
          "Southeast University": ""
        },
        {
          "xhzongyuan@seu.edu.cn": "",
          "Southeast University": "with the rapid advancement of deep learning techniques, various"
        },
        {
          "xhzongyuan@seu.edu.cn": "ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00",
          "Southeast University": ""
        },
        {
          "xhzongyuan@seu.edu.cn": "https://doi.org/10.1145/nnnnnnn.nnnnnnn",
          "Southeast University": "emotion recognition methods have been proposed. For example,"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Conference’17, July 2017, Washington, DC, USA": "in [17] Li et.al using label revision method to cope with emotion"
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": ""
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": "recognition in nosiy environments. In [18] Lu et.al impose sparse"
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": ""
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": "constrain on the reconstruction matrix to select more effective"
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": ""
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": "features"
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": ""
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": "However, these methods often focus on the recognition of seven"
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": ""
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": "basic emotions. In practical applications, complex emotions com-"
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": ""
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": "posed of combinations of these basic emotions are more commonly"
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": ""
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": "encountered. There is relatively little research in this domain, and"
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": ""
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": "the lack of high-quality databases for complex emotions hinders"
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": ""
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": "further development\nin this field. To facilitate the development"
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": ""
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": "of compound emotions recognition,\nthe 7th Affective Behavior"
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": ""
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": "Analysis in-the-wild (ABAW) hold the Compound Expression (CE)"
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": ""
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": "Recogntion based on the C-EXPR-DB database[4–16, 21]."
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": ""
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": "The remainder of this paper is organized as follows. The frame-"
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": ""
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": "work of our modal including training dataset preparation and the"
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": ""
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": "backbone of our methods are described in section 2. In section 3, we"
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": ""
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": "show our experimental results on the challenge dataset to evaluate"
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": ""
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": "the effectiveness of our proposed method. Finally, in section 4, we"
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": ""
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": "conclude this paper."
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": ""
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": ""
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": ""
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": ""
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": ""
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": "2\nTHE PROPOSED METHOD"
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": ""
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": "2.1\nFeature Extraction and Fusion"
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": ""
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": "We first employed the OpenFace toolkit for face detection in ev-"
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": ""
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": "ery frame of the video. For images where faces were difficult to"
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": ""
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": "detect, we supplemented with the closest temporally adjacent face"
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": ""
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": "to obtain a face image corresponding to every original video frame."
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": "Considering the temporal continuity of emotional states, despite the"
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": ""
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": "task requiring emotion classification prediction for each frame, we"
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": ""
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": "constructed a temporal pyramid structure of image sequences to ac-"
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": ""
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": "quire more robust emotional features. Three sets of image sequences"
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": ""
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": "at different temporal scales were composed as follows: a sequence"
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": ""
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": "of 15 frames starting from the current\nframe, a sequence of 15"
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": "frames sampled from a quarter-length segment of the video where"
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": "the current frame resides, and a sequence of 15 frames sampled"
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": ""
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": "from the entire video. These hierarchical sequences of three image"
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": ""
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": "sets were parallelly fed into a spatiotemporal feature extraction"
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": ""
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": "network consisting of ResNet18 and Transformer. For each frame"
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": ""
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": "image, we averaged the sum of all classification results obtained"
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": ""
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": "from different image sequences to derive the final classification"
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": ""
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": "result."
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": ""
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": "Additionally, due to inherent data imbalance in the training set,"
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": ""
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": "as depicted in Fig. 1 of emotional cycles, we utilized the DFEW data-"
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": ""
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": "base to train the network for positive and negative classification in"
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": "valence and arousal to assist the final classification results. Specif-"
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": ""
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": "ically, for each frame,\nif both valence and arousal were positive,"
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": ""
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": "it was directly categorized as a compound emotion of happiness-"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "as depicted in Fig. 1 of emotional cycles, we utilized the DFEW data-": ""
        },
        {
          "as depicted in Fig. 1 of emotional cycles, we utilized the DFEW data-": "base to train the network for positive and negative classification in"
        },
        {
          "as depicted in Fig. 1 of emotional cycles, we utilized the DFEW data-": "valence and arousal to assist the final classification results. Specif-"
        },
        {
          "as depicted in Fig. 1 of emotional cycles, we utilized the DFEW data-": ""
        },
        {
          "as depicted in Fig. 1 of emotional cycles, we utilized the DFEW data-": "ically, for each frame,\nif both valence and arousal were positive,"
        },
        {
          "as depicted in Fig. 1 of emotional cycles, we utilized the DFEW data-": ""
        },
        {
          "as depicted in Fig. 1 of emotional cycles, we utilized the DFEW data-": "it was directly categorized as a compound emotion of happiness-"
        },
        {
          "as depicted in Fig. 1 of emotional cycles, we utilized the DFEW data-": ""
        },
        {
          "as depicted in Fig. 1 of emotional cycles, we utilized the DFEW data-": "surprise; if both were negative, a judgment was made among three"
        },
        {
          "as depicted in Fig. 1 of emotional cycles, we utilized the DFEW data-": ""
        },
        {
          "as depicted in Fig. 1 of emotional cycles, we utilized the DFEW data-": "compound emotions carrying sadness components. Since other sets"
        },
        {
          "as depicted in Fig. 1 of emotional cycles, we utilized the DFEW data-": ""
        },
        {
          "as depicted in Fig. 1 of emotional cycles, we utilized the DFEW data-": "of compound emotions do not exhibit mutual exclusivity in valence"
        },
        {
          "as depicted in Fig. 1 of emotional cycles, we utilized the DFEW data-": ""
        },
        {
          "as depicted in Fig. 1 of emotional cycles, we utilized the DFEW data-": "and arousal, valence and arousal were not used as assisting infor-"
        },
        {
          "as depicted in Fig. 1 of emotional cycles, we utilized the DFEW data-": ""
        },
        {
          "as depicted in Fig. 1 of emotional cycles, we utilized the DFEW data-": "mation for their recognition. The overall network framework is"
        },
        {
          "as depicted in Fig. 1 of emotional cycles, we utilized the DFEW data-": ""
        },
        {
          "as depicted in Fig. 1 of emotional cycles, we utilized the DFEW data-": "illustrated in Fig. 2."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(a) The overall structure of the network.": "[4] Dimitrios Kollias. 2022. Abaw: Valence-arousal estimation, expression recogni-"
        },
        {
          "(a) The overall structure of the network.": "tion, action unit detection & multi-task learning challenges. In Proceedings of the"
        },
        {
          "(a) The overall structure of the network.": "IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2328–2336."
        },
        {
          "(a) The overall structure of the network.": "[5] Dimitrios Kollias. 2023. ABAW: learning from synthetic data & multi-task learn-"
        },
        {
          "(a) The overall structure of the network.": "ing challenges. In European Conference on Computer Vision. Springer, 157–172."
        },
        {
          "(a) The overall structure of the network.": "[6] Dimitrios Kollias. 2023. Multi-Label Compound Expression Recognition: C-EXPR"
        },
        {
          "(a) The overall structure of the network.": "Database & Network. In Proceedings of the IEEE/CVF Conference on Computer"
        },
        {
          "(a) The overall structure of the network.": "Vision and Pattern Recognition. 5589–5598."
        },
        {
          "(a) The overall structure of the network.": "[7] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. [n. d.]. Analysing Affective"
        },
        {
          "(a) The overall structure of the network.": "Behavior in the First ABAW 2020 Competition. In 2020 15th IEEE International"
        },
        {
          "(a) The overall structure of the network.": "Conference on Automatic Face and Gesture Recognition (FG 2020)(FG). 794–800."
        },
        {
          "(a) The overall structure of the network.": "Face\n[8] Dimitrios Kollias, Viktoriia Sharmanska, and Stefanos Zafeiriou. 2019."
        },
        {
          "(a) The overall structure of the network.": "Behavior a la carte: Expressions, Affect and Action Units in a Single Network."
        },
        {
          "(a) The overall structure of the network.": "arXiv preprint arXiv:1910.11111 (2019)."
        },
        {
          "(a) The overall structure of the network.": "[9] Dimitrios Kollias, Viktoriia Sharmanska, and Stefanos Zafeiriou. 2021. Distribu-"
        },
        {
          "(a) The overall structure of the network.": "tion Matching for Heterogeneous Multi-Task Learning: a Large-scale Face Study."
        },
        {
          "(a) The overall structure of the network.": "arXiv preprint arXiv:2105.03790 (2021)."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Conference’17, July 2017, Washington, DC, USA": "preprint arXiv:2103.15792 (2021)."
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": "[15] Dimitrios Kollias and Stefanos Zafeiriou. 2021. Analysing affective behavior"
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": "in the second abaw2 competition. In Proceedings of the IEEE/CVF International"
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": "Conference on Computer Vision. 3652–3660."
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": "[16] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav Dhall, Shreya Ghosh,"
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": "Chunchang Shao, and Guanyu Hu. 2024. 7th ABAW Competition: Multi-Task"
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": "Learning and Compound Expression Recognition. arXiv preprint arXiv:2407.03835"
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": "(2024)."
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": "Sunan Li, Hailun Lian, Cheng Lu, Yan Zhao, Chuangao Tang, Yuan Zong, and\n[17]"
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": "Wenming Zheng. 2023. Multimodal emotion recognition in noisy environment"
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": "based on progressive label revision. In Proceedings of the 31st ACM International"
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": "Conference on Multimedia. 9571–9575."
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": "[18] Cheng Lu, Yuan Zong, Chuangao Tang, Hailun Lian, Hongli Chang, Jie Zhu,"
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": "Sunan Li, and Yan Zhao. 2022.\nImplicitly Aligning Joint Distributions for Cross-"
        },
        {
          "Conference’17, July 2017, Washington, DC, USA": "Corpus Speech Emotion Recognition. Electronics 11, 17 (2022), 2745."
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Bagged support vector machines for emotion recognition from speech",
      "authors": [
        "Anjali Bhavan",
        "Pankaj Chauhan",
        "Rajiv Ratn Hitkul",
        "Shah"
      ],
      "year": "2019",
      "venue": "Knowledge-Based Systems",
      "doi": "10.1016/j.knosys.2019.104886"
    },
    {
      "citation_id": "2",
      "title": "Multiview Supervised Dictionary Learning in Speech Emotion Recognition",
      "authors": [
        "J Mehrdad",
        "Pouria Gangeh",
        "Ali Fewzee",
        "Mohamed Ghodsi",
        "Fakhri Kamel",
        "Karray"
      ],
      "year": "2014",
      "venue": "IEEE/ACM Transactions on Audio Speech and Language Processing"
    },
    {
      "citation_id": "3",
      "title": "GMM Supervector Based SVM with Spectral Features for Speech Emotion Recognition",
      "authors": [
        "Hao Hu",
        "Ming Xu",
        "Wei Wu"
      ],
      "year": "2007",
      "venue": "Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "4",
      "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & multi-task learning challenges",
      "authors": [
        "Dimitrios Kollias"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "5",
      "title": "ABAW: learning from synthetic data & multi-task learning challenges",
      "authors": [
        "Dimitrios Kollias"
      ],
      "year": "2023",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "6",
      "title": "Multi-Label Compound Expression Recognition: C-EXPR Database & Network",
      "authors": [
        "Dimitrios Kollias"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "7",
      "title": "Analysing Affective Behavior in the First ABAW 2020 Competition",
      "authors": [
        "Kollias",
        "E Schulc",
        "Hajiyev",
        "Zafeiriou"
      ],
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)(FG)"
    },
    {
      "citation_id": "8",
      "title": "Face Behavior a la carte: Expressions, Affect and Action Units in a Single Network",
      "authors": [
        "Dimitrios Kollias",
        "Viktoriia Sharmanska",
        "Stefanos Zafeiriou"
      ],
      "year": "2019",
      "venue": "Face Behavior a la carte: Expressions, Affect and Action Units in a Single Network",
      "arxiv": "arXiv:1910.11111"
    },
    {
      "citation_id": "9",
      "title": "Distribution Matching for Heterogeneous Multi-Task Learning: a Large-scale Face Study",
      "authors": [
        "Dimitrios Kollias",
        "Viktoriia Sharmanska",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Distribution Matching for Heterogeneous Multi-Task Learning: a Large-scale Face Study",
      "arxiv": "arXiv:2105.03790"
    },
    {
      "citation_id": "10",
      "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & emotional reaction intensity estimation challenges",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "Alice Baird",
        "Alan Cowen",
        "Stefanos Zafeiriou"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "11",
      "title": "The 6th Affective Behavior Analysis in-thewild (ABAW) Competition",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "Alan Cowen",
        "Stefanos Zafeiriou",
        "Chunchang Shao",
        "Guanyu Hu"
      ],
      "year": "2024",
      "venue": "The 6th Affective Behavior Analysis in-thewild (ABAW) Competition",
      "arxiv": "arXiv:2402.19344"
    },
    {
      "citation_id": "12",
      "title": "Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "A Mihalis",
        "Athanasios Nicolaou",
        "Guoying Papaioannou",
        "Björn Zhao",
        "Irene Schuller",
        "Stefanos Kotsia",
        "Zafeiriou"
      ],
      "year": "2019",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "13",
      "title": "Expression, Affect, Action Unit Recognition: Aff-Wild2, Multi-Task Learning and ArcFace",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2019",
      "venue": "Expression, Affect, Action Unit Recognition: Aff-Wild2, Multi-Task Learning and ArcFace",
      "arxiv": "arXiv:1910.04855"
    },
    {
      "citation_id": "14",
      "title": "Affect Analysis in-the-wild: Valence-Arousal, Expressions, Action Units and a Unified Framework. arXiv Conference'17",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2017",
      "venue": "Affect Analysis in-the-wild: Valence-Arousal, Expressions, Action Units and a Unified Framework. arXiv Conference'17",
      "arxiv": "arXiv:2103.15792"
    },
    {
      "citation_id": "15",
      "title": "Analysing affective behavior in the second abaw2 competition",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "16",
      "title": "th ABAW Competition: Multi-Task Learning and Compound Expression Recognition",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou",
        "Irene Kotsia",
        "Abhinav Dhall",
        "Shreya Ghosh",
        "Chunchang Shao",
        "Guanyu Hu"
      ],
      "year": "2024",
      "venue": "th ABAW Competition: Multi-Task Learning and Compound Expression Recognition",
      "arxiv": "arXiv:2407.03835"
    },
    {
      "citation_id": "17",
      "title": "Multimodal emotion recognition in noisy environment based on progressive label revision",
      "authors": [
        "Sunan Li",
        "Hailun Lian",
        "Cheng Lu",
        "Yan Zhao",
        "Chuangao Tang",
        "Yuan Zong",
        "Wenming Zheng"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "18",
      "title": "Implicitly Aligning Joint Distributions for Cross-Corpus Speech Emotion Recognition",
      "authors": [
        "Cheng Lu",
        "Yuan Zong",
        "Chuangao Tang",
        "Hailun Lian",
        "Hongli Chang",
        "Jie Zhu",
        "Sunan Li",
        "Yan Zhao"
      ],
      "year": "2022",
      "venue": "Electronics"
    },
    {
      "citation_id": "19",
      "title": "Speech emotion recognition using amplitude modulation parameters and a combined feature selection procedure",
      "authors": [
        "Arianna Mencattini",
        "Eugenio Martinelli",
        "Giovanni Costantini",
        "Massimiliano Todisco",
        "Barbara Basile",
        "Marco Bozzali",
        "Corrado Di"
      ],
      "year": "2014",
      "venue": "Knowledge-Based Systems",
      "doi": "10.1016/j.knosys.2014.03.019"
    },
    {
      "citation_id": "20",
      "title": "Speech emotion recognition using kernel sparse representation based classifier",
      "authors": [
        "P Sharma",
        "V Abrol",
        "A Sachdev",
        "A Dileep"
      ],
      "year": "2016",
      "venue": "2016 24th European Signal Processing Conference"
    },
    {
      "citation_id": "21",
      "title": "Aff-wild: Valence and arousal 'in-thewild'challenge",
      "authors": [
        "Stefanos Zafeiriou",
        "Dimitrios Kollias",
        "A Mihalis",
        "Athanasios Nicolaou",
        "Guoying Papaioannou",
        "Irene Zhao",
        "Kotsia"
      ],
      "year": "2017",
      "venue": "Computer Vision and Pattern Recognition Workshops"
    }
  ]
}