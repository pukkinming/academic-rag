{
  "paper_id": "2504.18799v1",
  "title": "A Survey On Multimodal Music Emotion Recognition",
  "published": "2025-04-26T05:11:04Z",
  "authors": [
    "Rashini Liyanarachchi",
    "Aditya Joshi",
    "Erik Meijering"
  ],
  "keywords": [
    "Multimodal Music Emotion Recognition",
    "Music Information Retrieval",
    "Deep Learning",
    "Multimodal Feature Extraction",
    "Feature Fusion",
    "Recommendation Systems"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal music emotion recognition (MMER) is an emerging discipline in music information retrieval that has experienced a surge in interest in recent years. This survey provides a comprehensive overview of the current state-of-the-art in MMER. Discussing the different approaches and techniques used in this field, the paper introduces a four-stage MMER framework, including multimodal data selection, feature extraction, feature processing, and final emotion prediction. The survey further reveals significant advancements in deep learning methods and the increasing importance of feature fusion techniques. Despite these advancements, challenges such as the need for large annotated datasets, datasets with more modalities, and real-time processing capabilities remain. This paper also contributes to the field by identifying critical gaps in current research and suggesting potential directions for future research. The gaps underscore the importance of developing robust, scalable, and interpretable models for MMER, with implications for applications in music recommendation systems, therapeutic tools, and entertainment. CCS Concepts: • Computing methodologies → Artificial intelligence; Natural language processing; Computer vision; Machine learning; • Applied computing → Sound and music computing.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The emotional impact of music is profound and pervasive, influencing the listener's mood, behavior, and cognitive functions  [94] . The ability to automatically detect emotions in music has significant implications across various domains, including music recommendation systems, therapeutic applications, multimedia content creation and analysis, humancomputer interaction (HCI), along with other commercial applications. Understanding emotional indicators can enhance user experiences in music streaming services, where personalized recommendations based on emotional preferences can lead to more satisfying and engaging interactions  [51] . In therapeutic settings, music is often used as a tool for emotional expression and regulation, making accurate emotion detection crucial for developing effective music-based interventions  [54] . Additionally, in multimedia content creation and analysis, identifying the emotional tone of music can aid in selecting the right soundtrack to evoke desired emotions in films, advertisements, and other media projects. This precision ensures that the audio complements the visual narrative, amplifying the overall, desired emotional impact  [101] . In HCI, understanding the emotions conveyed by music can enhance the emotional responsiveness and empathy of systems, fostering more immersive and personalized user experiences. By interpreting the emotional intent of music, Table  1 . Summary of musical features correlated with discrete emotions. Features for all emotions except \"Surprise\" are adopted from Patrik et al.  [50] .\n\n• Pitch: The position of an individual sound within the entire spectrum of sound.\n\n• Interval: The discrete change from one pitch to another (major 3rd, perfect 5th, etc.  [93] ).\n\n• Articulation: The way notes are played in sequence (legato, staccato, etc.  [11] ).\n\n• Rhythm: The pattern of beats and how they are grouped together.\n\n• Melody: The sequence of notes that are perceived as a single entity.\n\n• Dynamics: The volume of the music, including changes in loudness and softness.\n\nHowever, the same feature can be used in a similar manner to express different emotions. As an example, a fast tempo can be used to express happiness, anger, or fear. Thus, each feature in and of itself is neither necessary nor conclusive. Despite this ambiguity, the greater the number of indicators employed, the more dependable the communication becomes  [49] .",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Emotion Models",
      "text": "Understanding and utilizing emotion models is fundamental in MER. Emotion models provide frameworks for categorizing and interpreting the complex and often subjective nature of emotions conveyed through music, facilitating more consistent and comparable results across research and applications. Among the widely recognized models (Table  2 ), Russell's Circumplex Model  [79]  and Thayer's Model  [92]  are noteworthy here. Russell's model maps emotions onto a circular structure defined by valence (expressing feelings from negative to positive) and arousal (expressing the intensity of the emotion from low to high), offering a continuous and dynamic perspective on emotional experiences (Fig.  1 ).\n\nThayer's Model, in contrast, introduces the dimensions of energy-stress and calm-tired, focusing on the physiological aspects of emotions.\n\nOther models include Plutchik's Wheel of Emotions  [75]  and Ekman's Basic Emotions  [25] . Plutchik's model arranges eight primary emotions in a wheel, highlighting their combinations and interactions, while Ekman's model identifies six universal emotions: happiness, sadness, fear, disgust, anger, and surprise. These discrete emotion models provide a straightforward approach to categorizing emotional content. More complex models such as Scherer's Component Process Model (CPM)  [82]  and Lindquist's Conceptual Act Model  [62]  emphasize the dynamic and constructed nature of emotions. CPM focuses on the dynamic changes in emotion components, whereas Lindquist's model integrates core affect with conceptual knowledge, underscoring the role of cognitive processes in emotional experiences.\n\nAdditionally, models such as Positive Activation Negative Activation (PANA)  [102]  and Geneva Emotion Wheel (GEW)  [83]  offer nuanced insights into how emotions are constructed and experienced. Barrett's theory  [6]  suggests that emotions are constructed in the moment by core affect and conceptual knowledge, while the GEW maps complex emotions in a circular manner, reflecting their multidimensional nature. The PANA model distinguishes between positive and negative activation dimensions. Furthermore, Lazarus' Cognitive-Mediational Theory  [61]  highlights the critical role of cognitive appraisal in emotional responses. According to this theory, emotions arise from an individual's evaluation of an event's significance",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Model",
      "text": "Classes / Dimensions Categorical / Dimensional Domain\n\nRussell's Circumplex Model  [79]  2 Dimensional General Thayer's Model  [92]  2 Dimensional General Plutchik's Wheel of Emotions  [75]  8 Categorical General Ekman's Basic Emotions  [25]  6 Categorical General Scherer's Component Process Model  [82]  Varies Dimensional General Lindquist's Conceptual Act Model  [62]  Varies Both General Positive Activation Negative Activation Model  [102]  2 Dimensional General Geneva Emotion Wheel  [83]  20 Categorical General Barrett's Model  [6]  Varies Dimensional General Lazarus's Cognitive-Mediational Theory  [61]  Varies Both General Watson and Tellegen's Circumplex Model of Affect  [102]  2 Dimensional General Geneva Emotional Music Scale Model  [110]  9 Categorical Music Hevner's Emotional Model  [76]  8 Categorical Music  and their ability to cope with it. This appraisal process determines the emotional reaction and subsequent coping strategies, providing a framework for understanding how personal interpretation influences emotional experiences.\n\nMoreover, Watson and Tellegen's Circumplex Model of Affect  [102] , similar to Russell's model, emphasizes positive and negative affects. Each of these models contributes unique perspectives, enhancing our understanding of the intricate relationship between music and emotions.\n\nTwo emotion models have been used in the context of music, namely, the Geneva Emotional Music Scale (GEMS)  [110]  and Hevner's Emotional Model  [76] . The GEMS model identifies distinct emotional responses such as wonder, transcendence, tenderness, nostalgia, peacefulness, power, joyful activation, tension, and sadness, providing a tailored framework for exploring the rich and nuanced emotional landscape evoked by musical experiences. In contrast, Hevner's model classifies music emotions based on a set of adjectives divided into eight groups. Although this discrete model is less smooth than Thayer's, it is useful in distinguishing between specific emotional states described in musical terms.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Modalities Used In Mmer",
      "text": "MMER is a multidisciplinary field that uses various modalities to understand and classify the emotional content of music. By combining different sources of information, researchers can develop more accurate and nuanced models. The primary modalities used in MMER are (Fig.  2 ):\n\n(1) Audio: This refers to the sonic aspect of music, including pitch, loudness, and tempo. It is used to understand and identify the emotions conveyed through the musical piece. (2) Lyrics: Textual content of songs play a role in conveying underlying emotions. Features such as word choice, themes, and sentiment can be used to identify and interpret the emotions conveyed by the lyrics.\n\n(3) Visuals: Music is often accompanied by videos, live performances, or animations containing visual indicators like color, lighting, facial expressions, and body language, which contribute to the overall emotional experience of the music.\n\n(4) Symbolic Data: MIDI (Musical Instrument Digital Interface) is a mode of music representation that uses discrete events and parameters such as notes, dynamics, and tempo. It enables studying musical structure and patterns to determine emotional content. This symbolic data can be mapped to acoustic features, such as sound intensity, pitch variation, and rhythmic patterns, providing a bridge between the abstract representation of music and its actual auditory characteristics, thereby enhancing the analysis of emotional expression.\n\n(",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Music Emotion Recognition Datasets",
      "text": "Many datasets are available for MER research (Table  3 ). However, most of them are unimodal, typically containing only audio. Broadly speaking, existing datasets can be divided into two categories: song-level emotion annotation (SLEA) and continuous emotion annotation (CEA). The annotation labels used in each case can be categorical (distinct emotions) or dimensional (numerical values over a number of emotion dimensions such as arousal and valence  [79] ). The latter is particularly useful for capturing the nuanced and dynamic nature of musical emotions.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Song-Level Emotion Annotation (Slea).",
      "text": "Most datasets use song-level annotation, where a single emotion is assigned to a complete song. These datasets typically contain annotations that label each song with the predominant emotion. The assumption that a song contains a single emotion may not always be true but may work well as a simplification when using MER in a downstream task such as music information retrieval or recommendation.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Continuous Emotion Annotation (Cea).",
      "text": "In contrast to song-level annotation, continuous annotation provides time-varying labels that capture the dynamic nature of emotions throughout a song. The time intervals for these annotations are typically determined based on the granularity required for accurate emotion tracking, often ranging from 1 to 5 seconds. Shorter intervals provide a finer resolution of emotional changes but require more detailed analysis, Manuscript while longer intervals may capture broader emotional trends but with less temporal precision. The choice of interval depends on the specific objectives of the study and the nature of the music content being analyzed. These datasets are crucial for studies on music emotion variation detection (MEVD).",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "Evaluation metrics are essential for assessing the performance and robustness of MMER methods. MMER may be modeled as a classification or a regression task. For the classification formulation of MMER, Accuracy (A) is a commonly used metric representing the probability of correct classification within a dataset, and provides a basic measure of overall correctness. It is calculated as the ratio of correctly predicted outcomes to the total number of predictions made.\n\nPrecision (P) and recall (R) are useful for evaluating the reliability and completeness of positive predictions, respectively.\n\nPrecision measures the ratio of correctly predicted positive observations to the total predicted positives, while recall measures the ratio of correctly predicted positives to all actual positives. The F1 score, being the harmonic mean of precision and recall, is particularly useful for handling imbalanced datasets, offering a balanced view of the model's performance  [34, 84] .\n\nFor regression tasks in MMER, metrics such as the mean absolute error (MAE), the coefficient of determination (R 2 ), and the root mean squared error (RMSE) are frequently used. MAE estimates the average magnitude of prediction errors, allowing a direct interpretation of the average error. R 2 evaluates the degree to which a given regression model accurately represents the sample data. RMSE, on the other hand, prioritizes greater errors, making it more vulnerable to outliers. The concordance correlation coefficient (CCC) is also used to assess the agreement between predicted and actual values by integrating precision and accuracy into a single metric. Additionally, area under the receiver operating characteristic curve (AUROC) is employed to evaluate the classifier's ability to distinguish between classes at various threshold settings, providing a comprehensive measure of separability. In ranking tasks, metrics such as hits score on top k (hits@k) and mean average precision on top k (map@k) are utilized to evaluate the accuracy and order of a model's top predictions, respectively. These metrics collectively provide a robust framework for evaluating the diverse aspects of MMER performance  [30, 88] .\n\nFurthermore, public competitions and challenges provide researchers in the field with benchmarks to evaluate and compare methods. These challenges utilize standardized datasets, consistent evaluation protocols and metrics, and allow competitors to rank their methods against the state of the art. Currently, there are no established benchmarks developed specifically for MMER. The only available benchmarks are for audio-based MER. Some of these include the \"Emotion and Themes in Music\" task in MediaEval 1  (most recent: 2021) and the \"Audio K-POP Mood Classification\" task in MIREX 2 (most recent: 2020). However, both are based on audio classification only.\n\nSelecting the appropriate evaluation metrics is very important for effective performance assessment and improvement.\n\nThe chosen metrics should align with the task-specific characteristics and the nature of the dataset. To obtain a comprehensive understanding of the strengths and weaknesses of their models, researchers can utilize a combination of the discussed metrics.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Methods And Techniques",
      "text": "MMER research can be summarized using a four-stage framework (Fig.  3 ), proceeding from modality and data selection",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Feature Extraction",
      "text": "Feature extraction is a crucial step in MMER, as it transforms data from different modalities into representations of relevant information about a given piece of music that can be used as a basis for further processing and ultimate prediction. In this section we present a detailed overview of feature extraction in MMER, including the data formats, methods and techniques used to obtain audio features, lyric features, visual features, symbolic features, physiological features, textual (user-generated content) features, and metadata/contextual features.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Audio",
      "text": "Features. These features are the most widely studied in MER as well as MMER. The key audio elements conveying or expressing emotion in music are tempo, rhythm, mode, melody, and dynamics. Fu et al.  [26]  divide audio features into three categories: low-level, mid-level, and top-level (track-level) features/labels (Fig.  4 ). Semantic labels, at the top-level, offer insights into the ways in which people perceive and interpret music like style, genre, emotion, etc. As obtaining these levels is difficult from lower-level features, it makes the top-level features abstract  [115] . Therefore, in this survey, we primarily focus on low-and mid-level features, as these serve as the foundation for extracting top-level features like emotion. Top-level features and fusion are included in the discussion of emotion prediction (Section 3.3), aligning with the ultimate objective of understanding emotional responses to music.\n\nLow-Level Features. These can be broadly categorized as spectral features (SFs) and temporal features (TFs). The spectral characteristics of music include the timbre and tonal properties and are captured by SFs present in a relatively short time interval  [46] . SFs are obtained using signal processing techniques such as Fourier transformation, spectral/cepstral analysis, etc. Furthermore, the SFs commonly used in MER consist of two types: spectrograms and low-level descriptors (LLDs). The short-time Fourier transform (STFT) is used to generate spectrograms, and the input for models using these is in the form of a two-dimensional (2D) map consisting of a sequence of short-time spectrograms. LLDs calculated using single-frame audio are further processed to obtain high-level statistical functions (HSFs)  [13] . Examples of LLDs are the mel-frequency cepstrum coefficient (MFCC)  [65] , zero crossing rate (ZCR), spectral centroid, spread, roll-off, and flux, and chroma features  [47] . Examples of HSFs are the maximum, mean, and variance. In recent literature, the most commonly used timbre features are MFCCs, ZCR, and chroma features, while the centroid is the most widely used spectral feature (Table  4 ). However, some authors have used unique features, such as joined filter banks (an intermediate product between mel-spectrograms and MFCCs) and 60 handcrafted features to generate input for the classification model  [113] .\n\nTFs, on the other hand, capture the time-related aspects of audio signals, such as rhythm, tempo, and energy dynamics that are essential for expressing emotions in music. These features capture aspects such as beat consistency, tempo variations, and loudness fluctuations. Examples of TFs include short-time energy (STE), tempo, and beat strength, which characterize the rhythmic and dynamic elements of a musical piece. Unlike SFs, TFs focus on how these audio characteristics evolve over time. For example, the progression of energy or the consistency of rhythmic patterns can significantly affect how arousal or emotional intensity is perceived  [107] .\n\nThe integration of TFs with SFs enhances the representation of musical content by combining instantaneous timbral information with temporal progression, leading to more effective emotion recognition systems. Whether considering SFs or TFs, advanced methods such as convolutional neural networks (CNNs) and recurrent neural network (RNNs) automatically learn hierarchical feature representations, capturing complex patterns and temporal dependencies at multiple scales that are indicative of emotions  [16] .\n\nMid-Level Features. In MMER, the use of mid-level features is not as common as in other areas such as cover-song detection. The three most used mid-level features are rhythm, pitch, and harmony. Rhythm is described using two important indicators: beat and tempo (beats-per-minute). The tempo can correspond with other features such as pitch, which is also a mid-level feature, in order to recognize an emotion (see Table  1 ).",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Lyric",
      "text": "Features. The lyrics of a song play a crucial part in expressing its emotional charge. Most research uses natural language processing (NLP) models as classifiers or regressors. Therefore, the words of the lyrics need to be converted into specific features to be fed as input to these models. Pyrovolakis et al.  [77]   multi-emotion classification problems, Edmonds and Sedoc  [23]  utilized the NRC Hashtag Emotion Lexion  [68]  to convert lyrical data into feature vectors with a length of 9. Wang et al.  [99]  incorporated rhyme as a feature using a rhyme system they created for their study on emotion detection in Chinese songs. images  [21, 100, 114] . With regard to dynamic features, Pandeya et al.  [73]  proposed a method for analyzing video segments that contain only faces. They extracted these using the cascade classifier feature of OpenCV 3  . Facial expression is a visual feature used in the majority of MER literature. Chen  [14]  used the 68 face feature points obtained using the Dlib  4  Python library to extract these facial expressions. A common strategy to capture the dynamics of movement within a video is to employ optical flow and motion analysis  [69] . To infer emotions more accurately, researchers have used context-aware networks that consider the scene's broader context, including the environment and character interactions  [73] . Additionally, slow-fast networks, which process video data at multiple temporal resolutions, capture both detailed spatial information and rapid temporal changes  [64] .\n\n3.1.4 Symbolic Features. These features are extracted from symbolic music scores such as MIDI. Most symbolic music scores used in MER are represented in MIDI, which is an informative resource that provides note information (pitch, duration, etc.), timing information (tempo and time signature), and other relevant details. However, in the case of multimodal or unimodal MER, MIDI files are subjected to additional processing in order to extract symbolic domain features such as melody, rhythm, and other dynamic information. Zhao and Yoshii  [118]  utilized a RNN, specifically a bidirectional gated recurrent unit (BiGRU), to extract symbolic domain features from MIDI in order to pass them to the emotion classifier. Others, such as Thammasan  [90] , employed the MIRToolbox developed by Laurier et al.  [59]  to extract musical features from MIDI files.\n\n3.1.5 Physiological Features. These can be derived from several sources, including EEG, ECG, heart rate variability (HRV), galvanic skin response (GSR), electromyography (EMG), and respiratory rate (RSP)  [12]  in both multimodal and unimodal MER.\n\nEEG, in particular, is widely used in the field of MER  [18]  as it captures the brain's electrical activity and provides insights into the emotional responses elicited by music. To extract emotion-related features, it is important to preprocess high-dimensional EEG signals, which may include a significant number of irrelevant features. In literature, the most popular EEG features for emotion detection are fractal dimension (FD), power spectral density (PSD), Higuchi fractal dimension (HFD), multiscale fractal dimension (MFD), spectrograms, and the wavelet transform (WT)  [18] . Some of the methods and tools used to preprocess EEG and obtain these features are  [18]  the Higuchi algorithm  [33]  for FD, the STFT, WT, and the Hilbert-Huang Transform (HHT) for PSD, and PyEEG  [5]  for HFD. Wavelet domain features can be extracted using the STFT, high-order crossing analysis (HOC), and hybrid adaptive filtering (HAF). To remove eye movements, facial muscle movements, heartbeats, and other distortions, independent component analysis (ICA) and blind source separation (BSS) can be used  [18] .\n\nSimilar to EEG, ECG signals have also been used for MER. Hsu et al.  [35]  developed the sequential forward floating selection-kernel-based class separability (SFFS-KBCS) feature selection algorithm and also utilized generalized discriminant analysis (GDA) in order to select significant ECG features for emotion detection. Furthermore, Naji et al.  [70]  calculated several features using the RR (time intervals between consecutive R peaks) time frames extracted from the  ECG signal. Some of these features are statistical features (e.g. mean and standard deviation), nonlinear features (e.g. sample entropy), and triangular phase space mapping.\n\nHu et al.  [37]  defined the relationship between music-induced emotions and physiological signals. They collected electrodermal activity (EDA), blood volume pulse (BVP), inter-beat interval (IBI), heart rate (HR), and skin temperature (TEMP) data and obtained a set of features that can be used in MER (see Table  5  for a summary of these and other physiological features).",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Textual Features.",
      "text": "Beyond lyrics, other forms of textual data, such as user reviews, social media comments, and textual descriptions of music, have been employed to gauge emotional responses to music. These texts often capture subjective experiences and sentiments of listeners, which can be analyzed using NLP techniques to infer the emotional impact of music. These features are also extracted similarly to other textual data discussed above  [106] .",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Metadata And Contextual Features.",
      "text": "Methods employing metadata and contextual information such as song title and artist name as a modality for their final prediction, typically use pretrained models such as BERT to extract the features  [117] .",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Feature Processing",
      "text": "After features have been extracted (Stage 2) from the raw input data (Stage 1), they typically need to be further processed (Stage 3) before they can be used effectively for final emotion prediction (Stage 4). There are three main feature processing approaches that researchers have used up to now (Fig.  3 ), which we summarize in this section.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Approach 1: Feature Concatenation.",
      "text": "The most straightforward approach is to concatenate the modality-specific features into a unified feature set. Previous research has explored various techniques to accomplish this. Some studies employ a simple vector representation to merge all features within the same space  [60] , while others adopt more advanced methods, such as utilizing a bimodal deep Boltzmann machine (DBM) for feature fusion  [39] . This approach is related to what is generally called early fusion in the deep learning literature  [43, 78] .",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Approach 2:",
      "text": "Modality-Specific Feature Processing. An alternative approach is to train modality-specific models and then combine their outputs. There are two variants of this approach, which we refer to as Approach 2-A and Approach 2-B (Fig.  3 ). In Approach 2-A, each modality-specific model produces an embedding, which is used as input to the final multimodal emotion prediction model. This is known as intermediate fusion  [43, 78] . Most works using this approach train unimodal models, taking the extracted features as input, and then the last few layers of the models are removed to obtain the embeddings  [20] . Alternatively, the unimodal models can be used directly as in Approach 2-B, providing unimodal predictions that can be aggregated to produce a final multimodal prediction. This situation corresponds to late fusion  [43, 78] .\n\n3.2.3 Approach 3: Cross-Modal Feature Processing. The most sophisticated approach is to perform cross-modal feature processing. This concept is increasingly adopted by the field of MER and is especially gaining popularity in MMER.\n\nGenerally speaking, cross-modal processing combines inputs from different modalities to create a single output. This output is then used as the input for the final classifier, regressor, or fusion model. Unlike multimodal processing, which focuses on integrating multiple types of data to improve overall model performance, cross-modal processing emphasizes how interactions between different modalities can enhance or influence the processing of each data type, creating a more cohesive and contextually aware representation. This takes the concept of intermediate fusion to a higher level.\n\nRecent works  [97, 105, 117]  have used cross-modal processing to assess the interaction between audio and lyrics.\n\nThe input consists of pairs of lyric and audio fragments, where each input is typically a single sentence of lyrics and the corresponding audio, and the output is an emotion feature vector. A key element of cross-modal processing in these works is the use of an emotion long-short-term memory (E-LSTM) cell, an enhancement of the traditional LSTM, which processes the next lyric-audio pair along with the emotion vector of the past pair to maintain a consistent emotional state with respect to the previous interaction and to avoid the emotion independence between the two channels. The historical emotion vector is designed to preserve intense emotional information, while the current emotion vector updates relatively weaker emotions. This combined emotion vector is iteratively updated with the historical emotion vector, which helps decide which emotional levels in the song should be maintained and which past emotions need to be revised (see  [97]  for detailed mathematical formulations of the operations in this process).",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "Emotion Prediction",
      "text": "Given the processed features (Stage 3), the final task (Stage 4) is to predict the emotion they convey. The methods used to accomplish this task depend on the form in which the processed features are presented. As alluded to in the previous section, this corresponds to different fusion strategies (Fig.  5 ), which we elaborate on here as we discuss their usage in ultimate emotion prediction (see Table  6  for an overview of state-of-the-art MMER methods and performance).",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Emotion",
      "text": "Prediction by Feature-Level Fusion. Feature-level fusion, also known as early fusion, involves combining all extracted features from different modalities into a single high-dimensional feature vector (Approach 1). This unified feature vector is then used to train a single classifier such as support vector machines (SVMs)  [60, 104] , auto encoders  [119] , or CNNs  [81, 118]  for emotion prediction. A common approach to feature-level fusion is feature concatenation, where features from modalities such as audio and lyrics are merged into one vector and fed into a classification model. As this creates a multimodal feature space, challenges arise in integrating heterogeneous features from different modalities into a cohesive representation  [108] . Researchers have explored various techniques to address these challenges, including dimensionality reduction and normalization, which aim to better align the feature vectors of varying modalities. Dimensionality reduction methods such as principal component analysis (PCA) aim to reduce the number of features while retaining the most important variance in the data, and normalization methods such as z-score normalization and min-max scaling are often applied to ensure that features from different modalities are on a similar scale, which improves the performance of classification algorithms  [55] . Despite these efforts, direct concatenation can Manuscript sometimes fail to preserve critical emotional information due to the inherent differences between the representations of modal features  [13] .",
      "page_start": 14,
      "page_end": 15
    },
    {
      "section_name": "Emotion Prediction By Decision-Level",
      "text": "Fusion. Decision-level fusion, also known as late fusion, combines the outputs of unimodal classifiers or regressors (Approach 2-B). These outputs can be aggregated using algebraic combination rules such as \"Min, \" \"Max, \" or \"Sum\"  [13, 40, 63, 73, 77, 112] . Methods in this category can be further divided into linear probability fusion (LPF) and stacking ensemble learning (SEL)  [13] . While LPF is widely used in general machine learning applications  [56] , SEL is more prevalent in the MMER literature  [13, 77] . Advanced techniques for decision-level fusion include late fusion subtask merging (LFSM), which integrates audio and lyrics for multimodal emotion categorization  [40] , and stacking, which assigns distinct weights to each category or modality for better integration  [63, 73] . Recent advancements have focused on improving decision-level fusion by employing neural networks for weighting outputs.\n\nHowever, this approach has shown limited efficacy  [43] . Instead, stacking methods that combine the outputs of multiple models without fully fusing them have been shown to enhance performance  [112] .\n\nTo obtain unimodal emotion predictions, whether as final output or as input to multimodal models, various methods have been used. For audio, earlier studies have used SVMs, random forests, and logistic regression  [39, 60, 104, 108] ,\n\nwhile nowadays most audio-based MER is done using deep learning methods such as CNNs and RNNs. Others have experimented with combining a CNN and an RNN in a single model  [45] . Continuous audio-based MER is often done by a combination of CNNs and LSTMs  [14, 81, 112] . Furthermore, bidirectional LSTMs (BiLSTMs) with attention layers have been used  [112] . For audio classification, CNNs have been found to outperform BiGRU models  [80] . Some researchers have leveraged transfer learning by using pretrained models on large-scale audio datasets, which are then fine-tuned for specific emotion recognition tasks  [13, 31] . Building on this, fusion models have combined CNNs for SF extraction and\n\nLSTMs for TF extraction to enhance feature representation  [13, 112] , while convolutional autoencoders with frequency and time masking techniques have been used to emphasize important features and reduce noise  [32] . Researchers have also adopted self-attention mechanisms in RNNs which dynamically weigh the importance of different audio segments  [77] , and end-to-end learning approaches like SampleCNN to process raw audio signals directly using very small filters  [32] . Additionally, they have incorporated multiview neural networks to integrate multiple perspectives of the audio signal  [112] .\n\nAlthough audio has thus far been the most prominent modality in MER, other modalities are gaining popularity.\n\nEarlier studies on lyric-based MER largely utilized traditional classifiers such as SVMs  [60, 104, 108] . However, these methods often fall short in capturing the deeper contextual and semantic nuances of lyrics, which are crucial for accurate emotion recognition  [14] . More recent research in lyric-based MER has shifted to deep learning models (Table  6 ). CNNs, RNNs, and LSTMs are utilized for their ability to handle sequential data and maintain context over long text spans. These models identify and retain emotional indicators spread across multiple lines of lyrics. Neural networks such as the restricted Boltzmann machine (RBM) are used for unsupervised feature learning for lyric classification  [119] .\n\nAdditionally, the BERT model, with its transformer-based architecture, is used to understand the bidirectional context of words, enhancing the accuracy of emotion detection in lyrics  [14, 117] . To our knowledge, there exist no works using Approach 2-B with video. MIDI has also been used for training MER models using symbolic features derived from it.\n\nNotably, some studies have employed an SVM as the MER model  [90] , while others have utilized an RNN, specifically a BiGRU  [118] . Physiological signals such as EEG have been used with an SVM model  [90]  and electrodermal activity with a CNN for MER  [109] . We are not aware of studies that have used textual data and metadata/contextual data for unimodal emotion recognition.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Emotion Prediction By",
      "text": "Model-Level Fusion. Model-level fusion, also referred to as middle-level or intermediate fusion, involves utilizing embeddings (Approach 2-A) or predictions derived from modality-specific models (Approach 2-B) to train a final emotion prediction model. This bridges feature-level and decision-level fusion, offering a balance between combining raw features and final predictions. Techniques for model-level fusion include combining embeddings generated by modality-specific models, such as those for audio and lyrics, into a shared representation for emotion recognition. Additionally, methods like the Hough voting mechanism within the Hough forest model have been explored, effectively integrating cues from multiple modalities to predict music emotions  [104] . This allows modalities to contribute jointly while maintaining their distinct representations, thereby enhancing emotion prediction performance  [20] .",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Emotion Prediction By",
      "text": "Cross-Modal Fusion. Cross-modal feature fusion has emerged as a promising approach in MMER, gaining traction in more recent studies  [97, 105, 117] . This approach mainly aims at modeling and exploiting interaction between modalities at the feature level, effectively allowing the model to incorporate complementary information from different modalities. Unlike traditional fusion strategies, cross-modal fusion insists on direct processing of shared and unique aspects of each modality, fostering richer and more holistic representation.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Manuscript",
      "text": "Cross-modal processing typically involves aligning and integrating features from different modalities in a shared latent space, where relationships between modalities can be explored. Techniques such as attention mechanisms  [117]  and graph-based methods are often used to model dependencies and interactions between modalities. For example, attention mechanisms dynamically weigh the importance of specific features in one modality based on their relevance to another, enhancing the model's ability to prioritize meaningful interactions. Similarly, graph-based approaches construct cross-modal graphs to represent and learn complex relationships between modalities, facilitating a structured fusion process. Although graph-based approaches are not commonly applied in MMER, they have been widely used in other areas of multimodal research such as video understanding  [116] , and audio-visual event localization  [103] .",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Current Status And Future Directions",
      "text": "The field of MMER has seen significant advancements in recent years. Nevertheless, there is still much room for further performance improvement (Table  6 ). To assist ongoing developments, we summarize the current trends (Section 4.1)\n\nand challenges (Section 4.2), as well as potential future research directions (Section 4.3).",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Current Trends",
      "text": "Analyzing the recent literature, we observed some noticeable development trends in MMER, especially the growing availability of multimodal datasets and the shift from traditional machine learning methods to increasingly advanced multimodal deep learning methods.   3 ), including video, physiological signals, and textual data such as comments and metadata. Moreover, due to the dynamic nature of emotion in music, the field is now moving away from static processing, as it may not yield optimal and sufficiently detailed predictions for a given application. Therefore, datasets such as MERP  [58] , DEAM  [3] , PMEmo  [111] , and MuVi  [17]  include dynamic annotation rather than static annotation. Also, the advancement of techniques such as RNN models has made dynamic processing more efficient, further accelerating the emergence of dynamically annotated datasets.\n\nOne of the key driving factors in recent efforts to expand and create multimodal datasets is that they allow for the development of better emotion prediction methods. For example, in a study by Liu and Tan  [63] , the highest accuracies achieved by unimodal methods were 70.6% for audio and 62.9% for lyrics, while multimodal methods reached 79.2%.\n\nAnother study, by Chen and Li  [13] , reported 68.1% and 74.2% accuracy for audio and lyric classification, respectively, compared to 78.2% accuracy for the multimodal model.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Multimodal",
      "text": "Methods. Over the years, the majority of MMER papers have considered audio and lyrics as the primary modalities (Table  6 ), and the developed methods have progressed from traditional machine learning techniques such as SVMs to more and more sophisticated deep learning models based on CNNs, RNNs (in particular LSTMs), and recently also Transformers  [13, 14, 80, 81, 112] . In addition, various fusion strategies have been developed to effectively combine the two modalities. The highest accuracy of 94.58% for classification to date has been achieved by employing a CNN for audio analysis, BERT for lyrics analysis, and using late fusion of the two  [77] . Other modalities such as Manuscript metadata, song structure, MIDI, and video have also been incorporated into MMER systems  [72, 105, 117] . A common approach is to combine audio with video, although the highest accuracy reported thus far is only 77.9%  [14] . Modalities such as MIDI and physiological signals have as yet seen limited usage, with SVMs and CNNs being the most popular methods for prediction. The use of metadata has also received little attention to date. Processing of such data is typically done using models such as BERT and ALBERT  [117] .\n\nBeyond traditional machine learning and deep learning models, some researchers have explored alternative technologies like the OpenSMILE 5  toolkit, which can process a variety of modalities including audio, visual data, and physiological signals, offering a versatile approach to MMER. Another very important development highlighted in this survey is the use of cross-modal processing, which allows for more comprehensive and accurate emotion detection by enabling methods to process and correlate information from different sensory inputs simultaneously. Leveraging diverse modalities, cross-modal processing enhances the ability of MMER methods to understand complex emotional cues, leading to improved performance in various applications, from virtual assistants to mental health monitoring tools  [55] .",
      "page_start": 18,
      "page_end": 19
    },
    {
      "section_name": "Current Challenges",
      "text": "Despite recent advancements and the growing popularity of MMER, it is still quite far from achieving human-like performance (Table  6 ). One of the main challenges is the subjectivity of music experience. Different people can feel different emotions in a single song. Even the same person can feel different emotions for the same song depending on their mood  [52] . According to research, emotion can be influenced by a variety of factors, including culture, genre preference, age, level of music expertise, gender, and even the weather. Therefore, some studies have considered the profile data of annotators and listeners when making predictions  [58] , and more recent works  [29]  even considered the time and weather during listening to the song when making predictions. Accurately classifying music using categorical emotion models is very difficult. In dimensional emotion models, particularly the widely used valence-arousal model, each quadrant has an extensive range of emotions (Fig.  1 ), making precise annotation ambiguous and challenging. It might be beneficial to introduce a new emotion model specifically for the purpose of MER.\n\nAnother primary issue is the limited availability of datasets. The majority of datasets, such as DEAP, emoMV, 4Q  [57, 71, 91]  do not include complete song audios due to copyright restrictions. Furthermore, the number of songs or song excerpts is very limited, ranging from 100s to 1,000s. Some more recent datasets including Audioset  [27] , MTG-Jamendo  [9] , MuSe  [1] , and Music4all  [74]  have 17k, 55k, 90k, and 109k songs, respectively (Table  3 ), but they contain only a single modality (mostly audio only), making them unsuitable for MMER. Another limitation of current datasets is that they tend to focus on a single genre of music, resulting in a lack of diversity in the songs included. For instance, DEAM mainly includes rock and techno music, and MSD (Million Song Dataset)  [7]  consists primarily of pop music. Thus, there is an urgent need for larger and more diverse multimodal datasets for MMER. This would also open the door for the creation of state-of-the-art benchmarks for MMER, which are currently lacking.\n\nA continuing problem in MER in general is the insufficient use of music theory and concepts, such as considering the mode the music is written in and loudness variations, in the process of recognizing emotions. It is known that concepts like tempo, pitch, and rhythm can convey emotion (Section 2.1), but thus far only a small number of studies have made use of these.",
      "page_start": 19,
      "page_end": 20
    },
    {
      "section_name": "Future Directions",
      "text": "To deal with dataset limitations, one option is to employ unsupervised learning methods such as autoencoders  [119]  as an alternative to the supervised approaches predominantly employed in prior studies. Unsupervised learning enables the utilization of custom unlabeled datasets, which researchers can generate using various APIs and tools. For example, the SoundCloud API 6  can facilitate audio data collection, Lyrics.ovh  7  and MusixMatch 8  can provide lyrics data, and video data can be sourced using Python libraries such as PyTube  9  . This approach allows researchers to overcome reliance on preexisting datasets, offering greater flexibility and customization in data collection to meet specific research objectives. Additionally, incorporating annotators from different cultures when creating datasets can provide a diverse perspective on music emotions. Another approach to overcome the dataset limitation problem is to use transfer learning.\n\nAlthough transfer learning is widely utilized and has produced excellent results in other domains such as computer vision  [44] , it has seen very little use in the field of MER, and should be explored in the future.\n\nAnother potential future direction for MER is to incorporate different modalities other than only audio and lyrics  [107] . Researchers can acquire a better understanding of the listener's emotional response to music by combining physiological data like EEG signals, heart rate variability, and skin conductance. These physiological indicators offer a more objective assessment of emotional states, complementing personal judgements based on audio and lyrical content.\n\nMIDI is also an invaluable modality to incorporate. To comprehend the structural and expressive elements of a piece, one can analyze the precise symbolic information that MIDI provides about the music, such as note pitches, intervals, and dynamics. Furthermore, as noted in the previous section, involving music theory and concepts is a promising future direction. Integrating these many senses can result in a more complete and comprehensive understanding of emotions in music  [55] .\n\nReal-time MER is another promising future direction with many possible applications. Currently, many MER models are neither user-friendly nor optimized for quick execution, demanding GPUs and considerable processing time.\n\nHowever, the advancement of real-time technology has the potential to transform multiple sectors. Real-time emotion detection in therapeutic settings can improve emotional regulation techniques and mental health treatments by giving instant feedback to therapists and patients. Real-time analysis could help mood guide playlist systems improve user experience by dynamically changing song choices to better fit the listener's current emotional state. Furthermore, real-time emotion identification could be used in commercial applications to deliver more effective and personalized marketing content, such as targeted advertising  [89] . Real-time emotion recognition in music has great potential for both private and public applications as technology develops and these models become more affordable and effective.\n\nCreating real-time MMER applications is quite challenging, primarily due to high computational demands. Combining and synchronizing data streams like audio, video, and text requires advanced algorithms and significant processing power  [4] . This complexity often hampers real-time processing. Additionally, the substantial resource demands such as increased memory, processing power, and energy consumption pose further difficulties. This is particularly problematic for portable or embedded systems, where resources are limited. As a result, implementing real-time MMER on less powerful devices can be impractical, hindering broader applications  [87] . However, these can be mitigated by adopting efficient data fusion methods. Developing lightweight algorithms that effectively combine multiple modalities with minimal computational overhead is a key strategy. Additionally, leveraging parallel processing techniques or utilizing specialized hardware, such as GPUs, can significantly enhance the system's ability to handle multiple data streams simultaneously without compromising speed. These approaches help maintain the real-time functionality of the system while addressing the inherent computational and resource demands.",
      "page_start": 20,
      "page_end": 21
    },
    {
      "section_name": "Conclusion",
      "text": "MMER is an upcoming and developing field in the area of music information retrieval. Although unimodal MER has achieved exceptional performances, MMER has not reached human-like accuracy yet. From the studies surveyed in this paper, we conclude that the majority of MMER methods are limited to using audio and lyrics as the primary modalities.\n\nIncorporating other modalities such as video, MIDI, physiological signals, metadata and other (con)textural data may provide additional information and improve emotion recognition performance. Moreover, as most works employ machine learning and deep learning methods to train emotion prediction models, primary factors limiting progress in MMER are the absence of a precise emotional model, limited datasets, and a lack of state-of-the-art benchmarks. Thus, future research should focus on the integration of a wider variety of modalities, the exploration of new deep learning architectures, and the development of comprehensive evaluation frameworks. Notwithstanding current challenges and limitations, MMER holds significant promise. As the field continues to grow, MMER will not only bridge the gap between human-like emotional understanding and machine perception, but also introduce new opportunities for enhancing user experiences and emotional intelligence in technology, revolutionizing human-computer interaction, multimedia content production, medical treatment, recommendation systems, and commercial applications.",
      "page_start": 21,
      "page_end": 21
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Russell’s Circumplex Model. Emotions are characterized by valence (ranging from negative to positive) along the horizontal",
      "page": 5
    },
    {
      "caption": "Figure 2: Modalities used in MMER.",
      "page": 6
    },
    {
      "caption": "Figure 3: ), proceeding from modality and data selection",
      "page": 8
    },
    {
      "caption": "Figure 3: Framework summarizing past and current MMER methods.",
      "page": 9
    },
    {
      "caption": "Figure 4: ). Semantic labels, at",
      "page": 9
    },
    {
      "caption": "Figure 4: Categorization of audio features.",
      "page": 11
    },
    {
      "caption": "Figure 3: ), which we summarize in this section.",
      "page": 13
    },
    {
      "caption": "Figure 3: ). In Approach 2-A, each modality-specific model produces an embedding, which is used as input",
      "page": 13
    },
    {
      "caption": "Figure 5: ), which we elaborate on here as we discuss their usage in",
      "page": 14
    },
    {
      "caption": "Figure 5: Comparison of fusion methods in music emotion prediction.",
      "page": 15
    },
    {
      "caption": "Figure 1: ), making precise annotation ambiguous and challenging. It",
      "page": 19
    }
  ],
  "tables": [
    {
      "caption": "Table 1: ) can be conveyed by specific musical features, such as [10]:",
      "page": 2
    },
    {
      "caption": "Table 1: Summary of musical features correlated with discrete emotions. Features for all emotions except “Surprise” are adopted from",
      "page": 3
    },
    {
      "caption": "Table 2: Emotion models used in MER.",
      "page": 4
    },
    {
      "caption": "Table 3: Publicly available MER datasets. Size indicates the number of songs or fragments in the dataset. Annotation includes SLEA",
      "page": 7
    },
    {
      "caption": "Table 3: ). However, most of them are unimodal, typically containing only",
      "page": 7
    },
    {
      "caption": "Table 4: ). However, some authors have used unique features, such as joined filter banks (an intermediate",
      "page": 10
    },
    {
      "caption": "Table 4: Summary of audio features used in recent literature (2020-2024).",
      "page": 11
    },
    {
      "caption": "Table 5: Categorization of physiological features.",
      "page": 13
    },
    {
      "caption": "Table 5: for a summary of these and other",
      "page": 13
    },
    {
      "caption": "Table 6: for an overview of state-of-the-art MMER methods and performance).",
      "page": 14
    },
    {
      "caption": "Table 6: ). CNNs, RNNs, and LSTMs are utilized for their ability to handle sequential data and maintain context over long text",
      "page": 16
    },
    {
      "caption": "Table 6: Summary of literature on MMER with performance.",
      "page": 18
    },
    {
      "caption": "Table 6: ). To assist ongoing developments, we summarize the current trends (Section 4.1)",
      "page": 18
    },
    {
      "caption": "Table 3: ), including video,",
      "page": 18
    },
    {
      "caption": "Table 6: ), and the developed methods have progressed from traditional machine learning techniques",
      "page": 18
    },
    {
      "caption": "Table 6: ). One of the main challenges is the subjectivity of music experience. Different people can feel",
      "page": 19
    },
    {
      "caption": "Table 3: ), but they contain only a",
      "page": 19
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "MuSe: The musical sentiment dataset",
      "authors": [
        "Christopher Akiki",
        "Manuel Burghardt"
      ],
      "year": "2021",
      "venue": "Journal of Open Humanities Data",
      "doi": "10.5334/johd.33"
    },
    {
      "citation_id": "2",
      "title": "Studying emotion induced by music through a crowdsourcing game",
      "authors": [
        "Anna Aljanaki",
        "Frans Wiering",
        "Remco Veltkamp"
      ],
      "year": "2016",
      "venue": "Information Processing & Management",
      "doi": "10.1016/j.ipm.2015.03.004"
    },
    {
      "citation_id": "3",
      "title": "Developing a benchmark for emotional analysis of music",
      "authors": [
        "Anna Aljanaki",
        "Yi-Hsuan Yang",
        "Mohammad Soleymani"
      ],
      "year": "2017",
      "venue": "PLOS ONE",
      "doi": "10.1371/journal.pone.0173392"
    },
    {
      "citation_id": "4",
      "title": "Multimodal Machine Learning: A Survey and Taxonomy",
      "authors": [
        "Tadas Baltrušaitis",
        "Chaitanya Ahuja",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Multimodal Machine Learning: A Survey and Taxonomy",
      "arxiv": "arXiv:1705.09406[cs.LG"
    },
    {
      "citation_id": "5",
      "title": "PyEEG: An open source Python module for EEG/MEG feature extraction",
      "authors": [
        "Forrest Sheng",
        "Xin Liu",
        "Christina Zhang"
      ],
      "year": "2011",
      "venue": "Computational Intelligence and Neuroscience",
      "doi": "10.1155/2011/406391"
    },
    {
      "citation_id": "6",
      "title": "Solving the emotion paradox: Categorization and the experience of emotion",
      "authors": [
        "Lisa Feldman"
      ],
      "year": "2006",
      "venue": "Personality and Social Psychology Review",
      "doi": "10.1207/s15327957pspr1001_2"
    },
    {
      "citation_id": "7",
      "title": "The million song dataset",
      "authors": [
        "Thierry Bertin-Mahieux",
        "P Daniel",
        "Brian Ellis",
        "Paul Whitman",
        "Lamere"
      ],
      "year": "2011",
      "venue": "International Society for Music Information Retrieval Conference (ISMIR). International Society for Music Information Retrieval",
      "doi": "10.7916/D8NZ8J07"
    },
    {
      "citation_id": "8",
      "title": "MusAV: A dataset of relative arousal-valence annotations for validation of audio models",
      "authors": [
        "Dmitry Bogdanov",
        "Xabier Lizarraga-Seijas",
        "Pablo Alonso-Jiménez",
        "Xavier Serra"
      ],
      "year": "2022",
      "venue": "International Society for Music Information Retrieval Conference (ISMIR). International Society for Music Information Retrieval",
      "doi": "10.5281/zenodo.7316746"
    },
    {
      "citation_id": "9",
      "title": "The MTG-Jamendo dataset for automatic music tagging",
      "authors": [
        "Dmitry Bogdanov",
        "Minz Won",
        "Philip Tovstogan",
        "Alastair Porter",
        "Xavier Serra"
      ],
      "year": "2019",
      "venue": "International Conference on Machine Learning. Semantic Scholar"
    },
    {
      "citation_id": "10",
      "title": "A comparative study of perspectives in musical structural features and emotional stimuli",
      "authors": [
        "Cibele Maia"
      ],
      "year": "2017",
      "venue": "Honors Theses"
    },
    {
      "citation_id": "11",
      "title": "The perceptual and emotional consequences of articulation in music",
      "authors": [
        "Nathan Carr",
        "Kirk Olsen",
        "William Thompson"
      ],
      "year": "2023",
      "venue": "Music Perception",
      "doi": "10.1525/mp.2023.40.3.202"
    },
    {
      "citation_id": "12",
      "title": "Music mood and human emotion recognition based on physiological signals: A systematic review",
      "authors": [
        "Vybhav Chaturvedi",
        "Arman Beer Kaur",
        "Vedansh Varshney",
        "Anupam Garg",
        "Gurpal Singh Chhabra",
        "Munish Kumar"
      ],
      "year": "2021",
      "venue": "Multimedia Systems"
    },
    {
      "citation_id": "13",
      "title": "A multimodal music emotion classification method based on multifeature combined network classifier",
      "authors": [
        "Changfeng Chen",
        "Qiang Li"
      ],
      "year": "2020",
      "venue": "Mathematical Problems in Engineering",
      "doi": "10.1155/2020/4606027"
    },
    {
      "citation_id": "14",
      "title": "A novel long short-term memory network model for multimodal music emotion analysis in affective computing",
      "authors": [
        "Wenwen Chen"
      ],
      "year": "2022",
      "venue": "Journal of Applied Science and Engineering",
      "doi": "10.6180/jase.202303_26(3).0008"
    },
    {
      "citation_id": "15",
      "title": "The AMG1608 dataset for music emotion recognition",
      "authors": [
        "Yu-An Chen",
        "Yi-Hsuan Yang",
        "Ju-Chiang Wang",
        "Homer Chen"
      ],
      "year": "2015",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing",
      "doi": "10.1109/ICASSP.2015.7178058"
    },
    {
      "citation_id": "16",
      "title": "Convolutional recurrent neural networks for music classification",
      "authors": [
        "Keunwoo Choi",
        "György Fazekas",
        "Mark Sandler",
        "Kyunghyun Cho"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing",
      "doi": "10.1109/ICASSP.2017.7952585"
    },
    {
      "citation_id": "17",
      "title": "Predicting emotion from music videos: Exploring the relative contribution of visual and auditory information to affective responses",
      "authors": [
        "Phoebe Chua",
        "Dimos Makris",
        "Dorien Herremans",
        "Gemma Roig",
        "Kat Agres"
      ],
      "year": "2022",
      "venue": "Predicting emotion from music videos: Exploring the relative contribution of visual and auditory information to affective responses",
      "doi": "10.48550/arXiv.2202.10453",
      "arxiv": "arXiv:2202.10453"
    },
    {
      "citation_id": "18",
      "title": "A review: Music-emotion recognition and analysis based on EEG signals",
      "authors": [
        "Xu Cui",
        "Yongrong Wu",
        "Jipeng Wu",
        "Zhiyu You"
      ],
      "year": "2022",
      "venue": "Frontiers in Neuroinformatics",
      "doi": "10.3389/fninf.2022.997282"
    },
    {
      "citation_id": "19",
      "title": "LDA based emotion recognition from lyrics",
      "authors": [
        "K Dakshina",
        "Rajeswari Sridhar"
      ],
      "year": "2014",
      "venue": "Advanced Computing, Networking and Informatics",
      "doi": "10.1007/978-3-319-07353-8_22"
    },
    {
      "citation_id": "20",
      "title": "Music mood detection based on audio and lyrics with deep neural net",
      "authors": [
        "Rémi Delbouys",
        "Romain Hennequin",
        "Francesco Piccoli",
        "Jimena Royo-Letelier",
        "Manuel Moussallam"
      ],
      "year": "2018",
      "venue": "Music mood detection based on audio and lyrics with deep neural net",
      "doi": "10.48550/arXiv.1809.07276",
      "arxiv": "arXiv:1809.07276"
    },
    {
      "citation_id": "21",
      "title": "Audio and face video emotion recognition in the wild using deep neural networks and small datasets",
      "authors": [
        "Wan Ding",
        "Mingyu Xu",
        "Dongyan Huang",
        "Weisi Lin",
        "Minghui Dong",
        "Xinguo Yu",
        "Haizhou Li"
      ],
      "year": "2016",
      "venue": "ACM International Conference on Multimodal Interaction (ICMI)",
      "doi": "10.1145/2993148.2997637"
    },
    {
      "citation_id": "22",
      "title": "Understanding how the presence of music in advertisements influences consumer behaviour",
      "authors": [
        "Isabela Dogaru",
        "Adrian Furnham",
        "Alastair Mcclelland"
      ],
      "year": "2024",
      "venue": "Acta Psychologica",
      "doi": "10.1016/j.actpsy.2024.104333"
    },
    {
      "citation_id": "23",
      "title": "Multi-emotion classification for song lyrics",
      "authors": [
        "Darren Edmonds",
        "João Sedoc"
      ],
      "year": "2021",
      "venue": "Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis"
    },
    {
      "citation_id": "24",
      "title": "A comparison of the discrete and dimensional models of emotion in music",
      "authors": [
        "Tuomas Eerola",
        "Jonna Vuoskoski"
      ],
      "year": "2011",
      "venue": "Psychology of Music",
      "doi": "10.1177/0305735610362821"
    },
    {
      "citation_id": "25",
      "title": "Universals and cultural differences in facial expressions of emotion",
      "authors": [
        "Paul Ekman"
      ],
      "year": "1971",
      "venue": "Nebraska Symposium on Motivation"
    },
    {
      "citation_id": "26",
      "title": "A survey of audio-based music classification and annotation",
      "authors": [
        "Zhouyu Fu",
        "Guojun Lu",
        "Kai Ting",
        "Dengsheng Zhang"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Multimedia",
      "doi": "10.1109/TMM.2010.2098858"
    },
    {
      "citation_id": "27",
      "title": "Audio Set: An ontology and human-labeled dataset for audio events",
      "authors": [
        "F Jort",
        "Gemmeke",
        "P Daniel",
        "Dylan Ellis",
        "Aren Freedman",
        "Wade Jansen",
        "R Lawrence",
        "Manoj Moore",
        "Marvin Plakal",
        "Ritter"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing",
      "doi": "10.1109/ICASSP.2017.7952261"
    },
    {
      "citation_id": "28",
      "title": "TROMPA-MER: An open dataset for personalized music emotion recognition",
      "authors": [
        "Juan Sebastián Gómez-Cañón",
        "Nicolás Gutiérrez-Páez",
        "Lorenzo Porcaro",
        "Alastair Porter",
        "Estefanía Cano",
        "Perfecto Herrera-Boyer",
        "Aggelos Gkiokas",
        "Patricia Santos",
        "Davinia Hernández-Leo",
        "Casper Karreman",
        "Emilia Gómez"
      ],
      "year": "2022",
      "venue": "Journal of Intelligent Information Systems",
      "doi": "10.1007/s10844-022-00746-0"
    },
    {
      "citation_id": "29",
      "title": "SiTunes: A situational music recommendation dataset with physiological and psychological signals",
      "authors": [
        "Vadim Grigorev",
        "Jiayu Li",
        "Weizhi Ma",
        "Zhiyu He",
        "Min Zhang",
        "Yiqun Liu",
        "Ming Yan",
        "Ji Zhang"
      ],
      "year": "2024",
      "venue": "Conference on Human Information Interaction and Retrieval (CHIIR)",
      "doi": "10.1145/3627508.3638343"
    },
    {
      "citation_id": "30",
      "title": "A survey of music emotion recognition",
      "authors": [
        "Donghong Han",
        "Yanru Kong",
        "Han Jiayi",
        "Guoren Wang"
      ],
      "year": "2022",
      "venue": "Frontiers of Computer Science",
      "doi": "10.1007/s11704-021-0569-4"
    },
    {
      "citation_id": "31",
      "title": "Music emotion recognition based on a neural network with an inception-gru residual structure",
      "authors": [
        "Xiao Han",
        "Fuyang Chen",
        "Junrong Ban"
      ],
      "year": "2023",
      "venue": "Electronics",
      "doi": "10.3390/electronics12040978"
    },
    {
      "citation_id": "32",
      "title": "Music emotion recognition based on segment-level two-stage learning",
      "authors": [
        "Na He",
        "Sam Ferguson"
      ],
      "year": "2022",
      "venue": "International Journal of Multimedia Information Retrieval",
      "doi": "10.1007/s13735-022-00230-z"
    },
    {
      "citation_id": "33",
      "title": "Approach to an irregular time series on the basis of the fractal theory",
      "authors": [
        "T Higuchi"
      ],
      "year": "1988",
      "venue": "Physica D: Nonlinear Phenomena",
      "doi": "10.1016/0167-2789(88)90081-4"
    },
    {
      "citation_id": "34",
      "title": "A review on evaluation metrics for data classification evaluations",
      "authors": [
        "Mohammad Hossin",
        "M Sulaiman"
      ],
      "year": "2015",
      "venue": "International Journal of Data Mining & Knowledge Management Process",
      "doi": "10.5121/ijdkp.2015.5201"
    },
    {
      "citation_id": "35",
      "title": "Automatic ECG-based emotion recognition in music listening",
      "authors": [
        "Yu-Liang Hsu",
        "Jeen-Shing Wang",
        "Wei-Chun Chiang",
        "Chien-Han Hung"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2017.2781732"
    },
    {
      "citation_id": "36",
      "title": "Detecting music-induced emotion based on acoustic analysis and physiological sensing: A multimodal approach",
      "authors": [
        "Xiao Hu",
        "Fanjie Li",
        "Ruilun Liu"
      ],
      "year": "2022",
      "venue": "Applied Sciences",
      "doi": "10.3390/app12189354"
    },
    {
      "citation_id": "37",
      "title": "On the relationships between music-induced emotion and physiological signals",
      "authors": [
        "Xiao Hu",
        "Fanjie Li",
        "Jeremy Ng"
      ],
      "year": "2018",
      "venue": "International Society for Music Information Retrieval Conference (ISMIR)"
    },
    {
      "citation_id": "38",
      "title": "Emotion-driven Piano Music Generation via Two-stage Disentanglement and Functional Representation",
      "authors": [
        "Jingyue Huang",
        "Ke Chen",
        "Yi-Hsuan Yang"
      ],
      "year": "2024",
      "venue": "Emotion-driven Piano Music Generation via Two-stage Disentanglement and Functional Representation",
      "arxiv": "arXiv:2407.20955[cs.SD"
    },
    {
      "citation_id": "39",
      "title": "Bi-modal deep Boltzmann machine based musical emotion classification",
      "authors": [
        "Moyuan Huang",
        "Wenge Rong",
        "Tom Arjannikov",
        "Nan Jiang",
        "Zhang Xiong"
      ],
      "year": "2016",
      "venue": "Artificial Neural Networks and Machine Learning (ICANN)",
      "doi": "10.1007/978-3-319-44781-0_24"
    },
    {
      "citation_id": "40",
      "title": "AIDA-UPM at SemEval-2022 Task 5: Exploring multimodal late information fusion for multimedia automatic misogyny identification",
      "authors": [
        "Álvaro Huertas-García",
        "Helena Liz",
        "Guillermo Villar-Rodríguez",
        "Alejandro Martín",
        "Javier Huertas-Tato",
        "David Camacho"
      ],
      "year": "2022",
      "venue": "International Workshop on Semantic Evaluation (SemEval)",
      "doi": "10.18653/v1/2022.semeval-1.107"
    },
    {
      "citation_id": "41",
      "title": "EMOPIA: A multi-modal pop piano dataset for emotion recognition and emotion-based music generation",
      "authors": [
        "Hsiao-Tzu Hung",
        "Joann Ching",
        "Seungheon Doh",
        "Nabin Kim",
        "Juhan Nam",
        "Yi-Hsuan Yang"
      ],
      "year": "2021",
      "venue": "EMOPIA: A multi-modal pop piano dataset for emotion recognition and emotion-based music generation",
      "arxiv": "arXiv:2108.01374"
    },
    {
      "citation_id": "42",
      "title": "Automated music emotion recognition: A systematic evaluation",
      "authors": [
        "Arefin Huq",
        "Pablo Bello",
        "Robert Rowe"
      ],
      "year": "2010",
      "venue": "Journal of New Music Research",
      "doi": "10.1080/09298215.2010.513733"
    },
    {
      "citation_id": "43",
      "title": "A comprehensive review on deep learning-based data fusion",
      "authors": [
        "Mazhar Hussain",
        "Mattias Nils",
        "Jan Lundgren",
        "Seyed Jalaleddin"
      ],
      "year": "2024",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2024.3508271"
    },
    {
      "citation_id": "44",
      "title": "A review of deep transfer learning and recent advancements",
      "authors": [
        "Mohammadreza Iman",
        "Hamid Reza Arabnia",
        "Khaled Rasheed"
      ],
      "year": "2023",
      "venue": "Technologies",
      "doi": "10.3390/technologies11020040"
    },
    {
      "citation_id": "45",
      "title": "Music emotion recognition via end-to-end multimodal neural networks",
      "authors": [
        "Byungsoo Jeon",
        "Chanju Kim",
        "Adrian Kim",
        "Dongwon Kim",
        "Jangyeon Park",
        "Jung-Woo Ha"
      ],
      "year": "2017",
      "venue": "ACM Conference on Recommender Systems (RecSys)"
    },
    {
      "citation_id": "46",
      "title": "Learning temporal features using a deep neural network and its application to music genre classification",
      "authors": [
        "Il-Young Jeong",
        "Kyogu Lee"
      ],
      "year": "2016",
      "venue": "International Society for Music Information Retrieval Conference (ISMIR). International Society for Music Information Retrieval"
    },
    {
      "citation_id": "47",
      "title": "A review: Music feature extraction from an audio signal",
      "authors": [
        "Mukkamala Jitendra",
        "Radhika Yalavarthi"
      ],
      "year": "2020",
      "venue": "International Journal of Advanced Trends in Computer Science and Engineering",
      "doi": "10.30534/ijatcse/2020/11922020"
    },
    {
      "citation_id": "48",
      "title": "Machine learning approaches for emotion classification of music: A systematic literature review",
      "authors": [
        "Charles Joseph",
        "Sugeeswari Lekamge"
      ],
      "year": "2019",
      "venue": "International Conference on Advancements in Computing (ICAC)",
      "doi": "10.1109/ICAC49085.2019.9103378"
    },
    {
      "citation_id": "49",
      "title": "Communicating emotion in music performance: A review and theoretical framework",
      "authors": [
        "Patrik Juslin"
      ],
      "year": "2001",
      "venue": "Music and Emotion: Theory and Research",
      "doi": "10.1093/oso/9780192631886.003.0014"
    },
    {
      "citation_id": "50",
      "title": "Expression, perception, and induction of musical emotions: A review and a questionnaire study of everyday listening",
      "authors": [
        "Patrik Juslin",
        "Petri Laukka"
      ],
      "year": "2004",
      "venue": "Journal of New Music Research",
      "doi": "10.1080/0929821042000317813"
    },
    {
      "citation_id": "51",
      "title": "Emotions, mechanisms, and individual differences in music listening: A stratified random sampling approach",
      "authors": [
        "Patrik Juslin",
        "Laura Sakka",
        "T Gonçalo",
        "Olivier Barradas",
        "Lartillot"
      ],
      "year": "2022",
      "venue": "Music Perception",
      "doi": "10.1525/mp.2022.40.1.55"
    },
    {
      "citation_id": "52",
      "title": "Emotional responses to music: The need to consider underlying mechanisms",
      "authors": [
        "Patrik Juslin",
        "Daniel Västfjäll"
      ],
      "year": "2008",
      "venue": "Behavioral and Brain Sciences",
      "doi": "10.1017/S0140525X08005293"
    },
    {
      "citation_id": "53",
      "title": "EmoGen: Eliminating Subjective Bias in Emotional Music Generation",
      "authors": [
        "Chenfei Kang",
        "Peiling Lu",
        "Botao Yu",
        "Xu Tan",
        "Wei Ye",
        "Shikun Zhang",
        "Jiang Bian"
      ],
      "year": "2023",
      "venue": "EmoGen: Eliminating Subjective Bias in Emotional Music Generation",
      "arxiv": "arXiv:2307.01229"
    },
    {
      "citation_id": "54",
      "title": "Music as therapy",
      "authors": [
        "J Kathi",
        "Suzanne Kemper",
        "Danhauer"
      ],
      "year": "2005",
      "venue": "Southern Medical Journal",
      "doi": "10.1097/01.smj.0000154773.11986.39"
    },
    {
      "citation_id": "55",
      "title": "Music emotion recognition: A state of the art review",
      "authors": [
        "Youngmoo Kim",
        "Erik Schmidt",
        "Raymond Migneco",
        "Brandon Morton",
        "Patrick Richardson",
        "Jeffrey Scott",
        "Jacquelin Speck",
        "Douglas Turnbull"
      ],
      "year": "2010",
      "venue": "International Society for Music Information Retrieval Conference (ISMIR). International Society for Music Information Retrieval"
    },
    {
      "citation_id": "56",
      "title": "On combining classifiers",
      "authors": [
        "Josef Kittler",
        "Mr",
        "Robert Hatef",
        "Jiri Duin",
        "Matas"
      ],
      "year": "1998",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "doi": "10.1109/34.667881"
    },
    {
      "citation_id": "57",
      "title": "DEAP: A database for emotion analysis using physiological signals",
      "authors": [
        "Sander Koelstra",
        "Christian Muhl",
        "Mohammad Soleymani",
        "Jong-Seok Lee",
        "Ashkan Yazdani",
        "Touradj Ebrahimi",
        "Thierry Pun"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/T-AFFC.2011.15"
    },
    {
      "citation_id": "58",
      "title": "MERP: A music dataset with emotion ratings and raters' profile information",
      "authors": [
        "En Yan Koh",
        "Kin Cheuk",
        "Kwan Yee Heung",
        "Kat Agres",
        "Dorien Herremans"
      ],
      "year": "2023",
      "venue": "Sensors",
      "doi": "10.3390/s23010382"
    },
    {
      "citation_id": "59",
      "title": "A Matlab toolbox for music information retrieval",
      "authors": [
        "Olivier Lartillot",
        "Petri Toiviainen",
        "Tuomas Eerola"
      ],
      "year": "2007",
      "venue": "Annual Conference of the Gesellschaft für Klassifikation"
    },
    {
      "citation_id": "60",
      "title": "Multimodal music mood classification using audio and lyrics",
      "authors": [
        "Cyril Laurier",
        "Jens Grivolla",
        "Perfecto Herrera"
      ],
      "year": "2008",
      "venue": "International Conference on Machine Learning and Applications (ICMLA). IEEE",
      "doi": "10.1109/ICMLA.2008.96"
    },
    {
      "citation_id": "61",
      "title": "Vexing research problems inherent in cognitive-mediational theories of emotion-and some solutions",
      "authors": [
        "Richard Lazarus"
      ],
      "year": "1995",
      "venue": "Psychological Inquiry",
      "doi": "10.1207/s15327965pli0603_1"
    },
    {
      "citation_id": "62",
      "title": "Constructing emotion: The experience of fear as a conceptual act",
      "authors": [
        "Kristen Lindquist",
        "Lisa Feldman"
      ],
      "year": "2008",
      "venue": "Psychological Science",
      "doi": "10.1111/j.1467-9280.2008.02174.x"
    },
    {
      "citation_id": "63",
      "title": "Research on multi-modal music emotion classification based on audio and lyrics",
      "authors": [
        "Gaojun Liu",
        "Zhiyuan Tan"
      ],
      "year": "2020",
      "venue": "IEEE Information Technology, Networking, Electronic and Automation Control Conference (ITNEC)",
      "doi": "10.1109/ITNEC48623.2020.9084846"
    },
    {
      "citation_id": "64",
      "title": "An emotion-based personalized music recommendation framework for emotion improvement",
      "authors": [
        "Zhiyuan Liu",
        "Wei Xu",
        "Wenping Zhang",
        "Qiqi Jiang"
      ],
      "year": "2023",
      "venue": "Information Processing & Management",
      "doi": "10.1016/j.ipm.2022.103256"
    },
    {
      "citation_id": "65",
      "title": "Mel frequency cepstral coefficients for music modeling",
      "authors": [
        "Beth Logan"
      ],
      "year": "2000",
      "venue": "International Symposium on Music Information Retrieval"
    },
    {
      "citation_id": "66",
      "title": "MERGE -A bimodal dataset for static music emotion recognition",
      "authors": [
        "Pedro Lima Louro",
        "Hugo Redinho",
        "Ricardo Santos",
        "Ricardo Malheiro",
        "Renato Panda",
        "Rui Pedro"
      ],
      "year": "2024",
      "venue": "MERGE -A bimodal dataset for static music emotion recognition",
      "arxiv": "arXiv:2407.06060"
    },
    {
      "citation_id": "67",
      "title": "Music emotion recognition from lyrics: A comparative study",
      "authors": [
        "Ricardo Malheiro",
        "Renato Panda",
        "Paulo Gomes",
        "Rui Pedro"
      ],
      "year": "2013",
      "venue": "International Workshop on Music and Machine Learning (MML)"
    },
    {
      "citation_id": "68",
      "title": "Crowdsourcing a word-emotion association lexicon",
      "authors": [
        "M Saif",
        "Peter Mohammad",
        "Turney"
      ],
      "year": "2013",
      "venue": "Computational Intelligence",
      "doi": "10.1111/j.1467-8640.2012.00460.x"
    },
    {
      "citation_id": "69",
      "title": "Advancements in motion detection within video streams through the integration of optical flow estimation and 3D-convolutional neural network architectures",
      "authors": [
        "Pampati Nagaraju",
        "Manchala Sadanandam"
      ],
      "year": "2024",
      "venue": "Journal of Electrical Systems",
      "doi": "10.52783/jes.3238"
    },
    {
      "citation_id": "70",
      "title": "Classification of music-induced emotions based on information fusion of forehead biosignals and electrocardiogram",
      "authors": [
        "Mohsen Naji",
        "Mohammad Firoozabadi",
        "Parviz Azadfallah"
      ],
      "year": "2013",
      "venue": "Cognitive Computation",
      "doi": "10.1007/s12559-013-9239-7"
    },
    {
      "citation_id": "71",
      "title": "Musical texture and expressivity features for music emotion recognition",
      "authors": [
        "Renato Panda",
        "Ricardo Malheiro",
        "Rui Pedro"
      ],
      "year": "2018",
      "venue": "International Society for Music Information Retrieval Conference (ISMIR)"
    },
    {
      "citation_id": "72",
      "title": "Multi-modal music emotion recognition: A new dataset, methodology and comparative analysis",
      "authors": [
        "Renato Panda",
        "Ricardo Malheiro",
        "Bruno Rocha",
        "António Oliveira",
        "Rui Pedro"
      ],
      "year": "2013",
      "venue": "International Symposium on Computer Music Multidisciplinary Research (CMMR)"
    },
    {
      "citation_id": "73",
      "title": "Deep-learning-based multimodal emotion classification for music videos",
      "authors": [
        "Raj Yagya",
        "Bhuwan Pandeya",
        "Joonwhoan Bhattarai",
        "Lee"
      ],
      "year": "2021",
      "venue": "Sensors",
      "doi": "10.3390/s21144927"
    },
    {
      "citation_id": "74",
      "title": "Music4All: A new music database and its applications",
      "authors": [
        "Igor André",
        "Pegoraro Santana",
        "Fabio Pinhelli",
        "Juliano Donini",
        "Leonardo Catharin",
        "Rafael Biazus Mangolin",
        "Yandre Maldonado E Gomes Da"
      ],
      "year": "2020",
      "venue": "2020 International Conference on Systems, Signals and Image Processing",
      "doi": "10.1109/IWSSIP48289.2020.9145170"
    },
    {
      "citation_id": "75",
      "title": "EMOTION: Theory, Research, and Experience",
      "authors": [
        "Robert Plutchik",
        "Henry Kellerman"
      ],
      "year": "2013",
      "venue": "EMOTION: Theory, Research, and Experience"
    },
    {
      "citation_id": "76",
      "title": "The circumplex model of affect: An integrative approach to affective neuroscience, cognitive development, and psychopathology",
      "authors": [
        "Jonathan Posner",
        "James Russell",
        "Bradley Peterson"
      ],
      "year": "2005",
      "venue": "Development and Psychopathology",
      "doi": "10.1017/S0954579405050340"
    },
    {
      "citation_id": "77",
      "title": "Multi-modal song mood detection with deep learning",
      "authors": [
        "Konstantinos Pyrovolakis",
        "Paraskevi Tzouveli",
        "Giorgos Stamou"
      ],
      "year": "2022",
      "venue": "Sensors",
      "doi": "10.3390/s22031065"
    },
    {
      "citation_id": "78",
      "title": "Deep multimodal learning: A survey on recent advances and trends",
      "authors": [
        "Dhanesh Ramachandram",
        "Graham Taylor"
      ],
      "year": "2017",
      "venue": "IEEE Signal Processing Magazine",
      "doi": "10.1109/MSP.2017.2738401"
    },
    {
      "citation_id": "79",
      "title": "A circumplex model of affect",
      "authors": [
        "James Russell"
      ],
      "year": "1980",
      "venue": "Journal of Personality and Social Psychology",
      "doi": "10.1037/h0077714"
    },
    {
      "citation_id": "80",
      "title": "Transformer-based automatic music mood classification using multi-modal framework",
      "authors": [
        "A Sujeesha",
        "Rajan Rajeev"
      ],
      "year": "2023",
      "venue": "Journal of Computer Science and Technology",
      "doi": "10.24215/16666038.23.e02"
    },
    {
      "citation_id": "81",
      "title": "Multimodal music emotion recognition in Indonesian songs based on CNN-LSTM, XLNet transformers",
      "authors": [
        "Andrew Sams",
        "Amalia Zahra"
      ],
      "year": "2023",
      "venue": "Bulletin of Electrical Engineering and Informatics",
      "doi": "10.11591/eei.v12i1.4231"
    },
    {
      "citation_id": "82",
      "title": "Appraisal considered as a process of multilevel sequential checking",
      "authors": [
        "Klaus Scherer"
      ],
      "year": "2001",
      "venue": "Appraisal Processes in Emotion: Theory, Methods, Research",
      "doi": "10.1093/oso/9780195130072.003.0005"
    },
    {
      "citation_id": "83",
      "title": "What are emotions? And how can they be measured?",
      "authors": [
        "Klaus Scherer"
      ],
      "year": "2005",
      "venue": "Social Science Information",
      "doi": "10.1177/0539018405058216"
    },
    {
      "citation_id": "84",
      "title": "A systematic analysis of performance measures for classification tasks",
      "authors": [
        "Marina Sokolova",
        "Guy Lapalme"
      ],
      "year": "2009",
      "venue": "Information Processing & Management",
      "doi": "10.1016/j.ipm.2009.03.002"
    },
    {
      "citation_id": "85",
      "title": "1000 songs for emotional analysis of music",
      "authors": [
        "Mohammad Soleymani",
        "Micheal Caro",
        "Erik Schmidt",
        "Cheng-Ya Sha",
        "Yi-Hsuan Yang"
      ],
      "year": "2013",
      "venue": "ACM International Workshop on Crowdsourcing for Multimedia (CrowdMM)",
      "doi": "10.1145/2506364.2506365"
    },
    {
      "citation_id": "86",
      "title": "The Emotion-to-Music Mapping Atlas (EMMA): A systematically organized online database of emotionally evocative music excerpts",
      "authors": [
        "Hannah Strauß",
        "Julia Vigl",
        "Peer-Ole Jacobsen",
        "Martin Bayer",
        "Francesca Talamini",
        "Wolfgang Vigl",
        "Eva Zangerle",
        "Marcel Zentner"
      ],
      "year": "2024",
      "venue": "Behavior Research Methods",
      "doi": "10.3758/s13428-024-02336-0"
    },
    {
      "citation_id": "87",
      "title": "Efficient Processing of Deep Neural Networks: A Tutorial and Survey",
      "authors": [
        "Vivienne Sze",
        "Yu-Hsin Chen",
        "Tien-Ju Yang",
        "Joel Emer"
      ],
      "year": "2017",
      "venue": "Efficient Processing of Deep Neural Networks: A Tutorial and Survey",
      "arxiv": "arXiv:1703.09039[cs.CV"
    },
    {
      "citation_id": "88",
      "title": "Comparative assessment of regression models based on model evaluation metrics",
      "authors": [
        "V Abhishek",
        "Tatachar"
      ],
      "year": "2021",
      "venue": "International Journal of Innovative Technology and Exploring Engineering"
    },
    {
      "citation_id": "89",
      "title": "Infusing affective computing models into advertising research on emotions",
      "authors": [
        "Wen Jing",
        "Chuan Taylor",
        "Anghelcev Ching-Hua",
        "Sar George",
        "T Sela",
        "Xu Joseph",
        "Yanzhen"
      ],
      "year": "2024",
      "venue": "Journal of Advertising",
      "doi": "10.1080/00913367.2024.2409254"
    },
    {
      "citation_id": "90",
      "title": "Multimodal fusion of EEG and musical features in music-emotion recognition",
      "authors": [
        "Nattapong Thammasan",
        "Ken-Ichi Fukui",
        "Masayuki Numao"
      ],
      "year": "2017",
      "venue": "AAAI Conference on Artificial Intelligence",
      "doi": "10.1609/aaai.v31i1.11112"
    },
    {
      "citation_id": "91",
      "title": "EmoMV: Affective music-video correspondence learning datasets for classification and retrieval",
      "authors": [
        "Ha Thi",
        "Phuong Thao",
        "Gemma Roig",
        "Dorien Herremans"
      ],
      "year": "2023",
      "venue": "Information Fusion",
      "doi": "10.1016/j.inffus.2022.10.002"
    },
    {
      "citation_id": "92",
      "title": "The Biopsychology of Mood and Arousal",
      "authors": [
        "Robert Thayer"
      ],
      "year": "1989",
      "venue": "The Biopsychology of Mood and Arousal",
      "doi": "10.1093/oso/9780195068276.001.0001"
    },
    {
      "citation_id": "93",
      "title": "Intervals and scales",
      "authors": [
        "F William",
        "Thompson"
      ],
      "year": "2013",
      "venue": "The Psychology of Music",
      "doi": "10.1016/B978-0-12-381460-9.00004-3"
    },
    {
      "citation_id": "94",
      "title": "Arousal, mood, and the Mozart effect",
      "authors": [
        "F William",
        "E Thompson",
        "Gabriela Schellenberg",
        "Husain"
      ],
      "year": "2001",
      "venue": "Psychological Science",
      "doi": "10.1111/1467-9280.00345"
    },
    {
      "citation_id": "95",
      "title": "Multimodal music emotion recognition method based on the combination of knowledge distillation and transfer learning",
      "authors": [
        "Guiying Tong"
      ],
      "year": "2022",
      "venue": "Scientific Programming",
      "doi": "10.1155/2022/2802573"
    },
    {
      "citation_id": "96",
      "title": "Semantic annotation and retrieval of music and sound effects",
      "authors": [
        "Douglas Turnbull",
        "Luke Barrington",
        "David Torres",
        "Gert Lanckriet"
      ],
      "year": "2008",
      "venue": "IEEE Transactions on Audio, Speech, and Language Processing",
      "doi": "10.1109/TASL.2007.913750"
    },
    {
      "citation_id": "97",
      "title": "MMD-MII Model: A multilayered analysis and multimodal integration interaction approach revolutionizing music emotion classification",
      "authors": [
        "Jingyi Wang",
        "Alireza Sharifi",
        "Thippa Gadekallu",
        "Achyut Shankar"
      ],
      "year": "2024",
      "venue": "International Journal of Computational Intelligence Systems",
      "doi": "10.1007/s44196-024-00489-6"
    },
    {
      "citation_id": "98",
      "title": "Towards time-varying music auto-tagging based on CAL500 expansion",
      "authors": [
        "Shuo-Yang Wang",
        "Ju-Chiang Wang",
        "Yi-Hsuan Yang",
        "Hsin-Min Wang"
      ],
      "year": "2014",
      "venue": "IEEE International Conference on Multimedia and Expo (ICME)",
      "doi": "10.1109/ICME.2014.6890290"
    },
    {
      "citation_id": "99",
      "title": "Music emotion classification of Chinese songs based on lyrics using TF*IDF and rhyme",
      "authors": [
        "Xing Wang",
        "Chen Xiaoou",
        "Deshun Yang",
        "Yuqian Wu"
      ],
      "year": "2011",
      "venue": "International Society for Music Information Retrieval Conference (ISMIR). International Society for Music Information Retrieval"
    },
    {
      "citation_id": "100",
      "title": "Recognizing human emotional state from audiovisual signals",
      "authors": [
        "Yongjin Wang",
        "Ling Guan"
      ],
      "year": "2008",
      "venue": "IEEE Transactions on Multimedia",
      "doi": "10.1109/TMM.2008.927665"
    },
    {
      "citation_id": "101",
      "title": "Multimedia content analysis-using both audio and visual clues",
      "authors": [
        "Yao Wang",
        "Zhu Liu",
        "Jin-Cheng Huang"
      ],
      "year": "2000",
      "venue": "IEEE Signal Processing Magazine",
      "doi": "10.1109/79.888862"
    },
    {
      "citation_id": "102",
      "title": "The two general activation systems of affect: Structural findings, evolutionary considerations, and psychobiological evidence",
      "authors": [
        "David Watson",
        "David Wiese",
        "Jatin Vaidya",
        "Auke Tellegen"
      ],
      "year": "1999",
      "venue": "Journal of Personality and Social Psychology",
      "doi": "10.1037/0022-3514.76.5.820"
    },
    {
      "citation_id": "103",
      "title": "Cross-modal attention network for temporal inconsistent audio-visual event localization",
      "authors": [
        "Zhenyu Hanyu Xuan",
        "Shuo Zhang",
        "Jian Chen",
        "Yan Yang",
        "Yan"
      ],
      "year": "2020",
      "venue": "AAAI Conference on Artificial Intelligence",
      "doi": "10.1609/aaai.v34i01.5361"
    },
    {
      "citation_id": "104",
      "title": "Multimodal music mood classification by fusion of audio and lyrics",
      "authors": [
        "Like Hao Xue",
        "Feng Xue",
        "Su"
      ],
      "year": "2015",
      "venue": "International Conference on Multimedia Modeling (MMM)",
      "doi": "10.1007/978-3-319-14442-9_3"
    },
    {
      "citation_id": "105",
      "title": "COSMIC: Music emotion recognition combining structure analysis and modal interaction",
      "authors": [
        "Liang Yang",
        "Zhexu Shen",
        "Jingjie Zeng",
        "Xi Luo",
        "Hongfei Lin"
      ],
      "year": "2024",
      "venue": "Multimedia Tools and Applications",
      "doi": "10.1007/s11042-023-15376-z"
    },
    {
      "citation_id": "106",
      "title": "Improve the application of reinforcement learning and multi-modal information in music sentiment analysis",
      "authors": [
        "Qi Yang",
        "Songhu Liu",
        "Tianzhuo Gong"
      ],
      "year": "2025",
      "venue": "Expert Systems",
      "doi": "10.1111/exsy.13416"
    },
    {
      "citation_id": "107",
      "title": "Machine recognition of music emotion: A review",
      "authors": [
        "Yi-Hsuan Yang",
        "Homer Chen"
      ],
      "year": "2012",
      "venue": "ACM Transactions on Intelligent Systems and Technology",
      "doi": "10.1145/2168752.2168754"
    },
    {
      "citation_id": "108",
      "title": "Toward multi-modal music emotion classification",
      "authors": [
        "Yi-Hsuan Yang",
        "Yu-Ching Lin",
        "Heng-Tze Cheng",
        "I-Bin Liao",
        "Yeh-Chin Ho",
        "Homer Chen"
      ],
      "year": "2008",
      "venue": "Pacific-Rim Conference on Multimedia (PCM)",
      "doi": "10.1007/978-3-540-89796-5_8"
    },
    {
      "citation_id": "109",
      "title": "A multimodal framework for large-scale emotion recognition by fusing music and electrodermal activity signals",
      "authors": [
        "Guanghao Yin",
        "Shouqian Sun",
        "Dian Yu",
        "Dejian Li",
        "Kejun Zhang"
      ],
      "year": "2022",
      "venue": "ACM Transactions on Multimedia Computing, Communications, and Applications",
      "doi": "10.1145/3490686"
    },
    {
      "citation_id": "110",
      "title": "Emotions evoked by the sound of music: Characterization, classification, and measurement",
      "authors": [
        "Marcel Zentner",
        "Didier Grandjean",
        "Klaus Scherer"
      ],
      "year": "2008",
      "venue": "Emotion",
      "doi": "10.1037/1528-3542.8.4.494"
    },
    {
      "citation_id": "111",
      "title": "The PMEmo dataset for music emotion recognition",
      "authors": [
        "Kejun Zhang",
        "Hui Zhang",
        "Simeng Li",
        "Changyuan Yang",
        "Lingyun Sun"
      ],
      "year": "2018",
      "venue": "International Conference on Multimedia Retrieval (ICMR)",
      "doi": "10.1145/3206025.3206037"
    },
    {
      "citation_id": "112",
      "title": "Research on music emotional expression based on reinforcement learning and multimodal information",
      "authors": [
        "Lige Zhang",
        "Zhen Tian"
      ],
      "year": "2022",
      "venue": "Mobile Information Systems",
      "doi": "10.1155/2022/2616220"
    },
    {
      "citation_id": "113",
      "title": "Modularized composite attention network for continuous music emotion recognition",
      "authors": [
        "Meixian Zhang",
        "Yonghua Zhu",
        "Wenjun Zhang",
        "Yunwen Zhu",
        "Tianyu Feng"
      ],
      "year": "2022",
      "venue": "Multimedia Tools and Applications",
      "doi": "10.1007/s11042-022-13577-6"
    },
    {
      "citation_id": "114",
      "title": "Learning affective features with a hybrid deep model for audio-visual emotion recognition",
      "authors": [
        "Shiqing Zhang",
        "Shiliang Zhang",
        "Tiejun Huang",
        "Wen Gao",
        "Qi Tian"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology",
      "doi": "10.1109/TCSVT.2017.2719043"
    },
    {
      "citation_id": "115",
      "title": "Multimodal emotion recognition using a hierarchical fusion convolutional neural network",
      "authors": [
        "Yong Zhang",
        "Cheng Cheng",
        "Yidie Zhang"
      ],
      "year": "2021",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2021.3049516"
    },
    {
      "citation_id": "116",
      "title": "Multi-modal interaction graph convolutional network for temporal language localization in videos",
      "authors": [
        "Zongmeng Zhang",
        "Xianjing Han",
        "Xuemeng Song",
        "Yan Yan",
        "Liqiang Nie"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Image Processing",
      "doi": "10.1109/TIP.2021.3113791"
    },
    {
      "citation_id": "117",
      "title": "Multimodal music emotion recognition with hierarchical cross-modal attention network",
      "authors": [
        "Jiahao Zhao",
        "Ganghui Ru",
        "Yi Yu",
        "Yulun Wu",
        "Dichucheng Li",
        "Wei Li"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Multimedia and Expo (ICME)",
      "doi": "10.1109/ICME52920.2022.9859812"
    },
    {
      "citation_id": "118",
      "title": "Multimodal multifaceted music emotion recognition based on self-attentive fusion of psychology-inspired symbolic and acoustic features",
      "authors": [
        "Jiahao Zhao",
        "Kazuyoshi Yoshii"
      ],
      "year": "2023",
      "venue": "Asia Pacific Signal and Information Processing Association Annual Summit and Conference",
      "doi": "10.1109/APSIPAASC58517.2023.10317539"
    },
    {
      "citation_id": "119",
      "title": "Multimodel music emotion recognition using unsupervised deep neural networks",
      "authors": [
        "Jianchao Zhou",
        "Xiaoou Chen",
        "Deshun Yang"
      ],
      "year": "2019",
      "venue": "Conference on Sound and Music Technology (CSMT)",
      "doi": "10.1007/978-981-13-8707-4_3"
    }
  ]
}