{
  "paper_id": "2008.11241v1",
  "title": "Angus: Real-Time Manipulation Of Vocal Roughness For Emotional Speech Transformations",
  "published": "2020-08-25T19:06:03Z",
  "authors": [
    "Marco Liuni",
    "Luc Ardaillon",
    "Louise Bonal",
    "Lou Seropian",
    "Jean-Julien Aucouturier"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Vocal arousal, the non-linear acoustic features taken on by human and animal vocalizations when highly aroused, has an important communicative function because it signals aversive states such as fear, pain or distress. In this work, we present a computationally-efficient, real-time voice transformation algorithm, ANGUS, which uses amplitude modulation and time-domain filtering to simulate roughness, an important component of vocal arousal, in arbitrary voice recordings. In a series of 4 studies, we show that ANGUS allows parametric control over the spectral features of roughness like the presence of sub-harmonics and noise; that ANGUS increases the emotional negativity perceived by listeners, to a comparable level as a non-real-time analysis/resynthesis algorithm from the state-of-the-art; that listeners cannot distinguish transformed and non-transformed sounds above chance level; and that ANGUS has a similar emotional effect on animal vocalizations and musical instrument sounds than on human vocalizations. A real-time implementation of ANGUS is made available as open-source software, for use in experimental emotion reseach and affective computing.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "When alarmed, threatened or highly aroused, the vocalizations of humans and many non-human animals take on a number of non-linear acoustic features, such as subharmonics and broadband noise, which give them a rough and noisy sound quality  [1] . Vocal arousal in screams, cries, grunts or moans has an important communicative function in the human expressive repertoire, because it signals aversive states such as fear, pain or distress  [2]  and more generally modulates the perceived valence, arousal and dominance of emotional vocalizations  [3] . The perception of vocal arousal triggers fast and stereotypical responses in listeners  [4]  and evokes activity in areas linked to the brain's threat response system  [5] . Beyond language, vocal arousal and non-linearities are also prominently featured in popular music, e.g. in the growling vocal style of jazz or popular music singers such as Louis Armstrong, Tom Waits or Kurt Cobain  [6]  and, perhaps most remarkably, in extreme music such as metal  [7] .\n\nFrom a computational point of view, the analysis and synthesis of vocal arousal is important for all experimental systems concerned with negative emotional states, such as anger (e.g., detecting aggression in speech -  [8] ), fear (e.g., using screams to induce anxiety experimentally -  [9] ) or pain (e.g., automated medical assessment -  [10] ). For voice analysis, the measurement of the specific non-linear features of vocal arousal is often formulated in terms of jitter and shimmer (cycle-to-cycle variations in the period and amplitude of glottal pulses, respectively) and harmonic-to-noise ratio (HNR)  [11] . However, the control of nonlinear vocal behaviour in synthesized or existing recordings is less well-established. Classical glottal models, which represent individual glottal pulses as a parametric function  [12, 13] , or additive sinusoidal models, which generate a separate sine wave for each harmonic  [14, 15] , can be modified to generate variability in the glottal shape or periodicity. For instance, to create additional subharmonics, one can use a second glottal model at a pseudo-harmonic frequency to modulate the main series of pulses  [16]  or add additional components at an integer ratio of the fundamental frequency  [14] ; to add jitter or shimmer, one can add stochastic variations to the phase or amplitude of sinusoidal components  [17] . However, these synthesis techniques do not make it easy to modify pre-recorded vocal signals (e.g. dynamically modulate the arousal of video game character's scream, add growl on a singer's voice) or voices synthesized with methods such as concatenative text-to-speech  [18]  or sample-based deep network methods  [19] . Adding vocal non-linearities on such signals can be done with analysis-resynthesis techniques that, first, estimate the recording's series of glottal pulses  [20]  and, then, resynthesize the vocal signal from a manipulated series of pulses with artificially-varied amplitude and period  [21] [22] [23] [24] . However, these techniques rely on an explicit model of pulse variability, which is typically learned from one or several target examples of naturally rough voices  [25] , and it is unclear how such predetermined patterns should be selected for unknown voices. Moreover, because of the computational complexity of the initial stage of glottal source estimation, these techniques cannot operate in real-time, i.e. dynamically manipulate vocal arousal in an incoming vocal stream, thus ruling out most applications in continuous, dyadic interactions  [26] .\n\nHere, we present a new acoustic transformation algorithm, called ANGUS 1 , which aims to simulate the rough timbral quality of vocal arousal without attempting to directly control jitter and shimmer at the glottal source level. The algorithm, which uses amplitude modulation and time-domain filtering to add sub-harmonics to the original signal, is computationally extremely efficient and operates in real-time. In the following, we compare the algorithm to a state-of-the-art non-real-time alternative method based on pulse resynthesis [24, Section 6.3.2] and evaluate (1) its ability to parametrically (albeit indirectly) modulate a vocal signal's jitter and shimmer, (2) its impact on listener's judgements of the emotional negativity, (3) on judgements of transformation naturalness and (4) its ability to transform both vocal and non-vocal (musical, environmental) sounds.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Algorithms Angus",
      "text": "Our proposed approach to model vocal arousal uses amplitude modulation and time-domain filtering to efficiently add sub-harmonics in the original signal. Amplitude modulation consists in multiplying in the time-domain a carrier signal with a modulating signal that has a lower frequency and an amplitude varying in the range [0, 1], centered around 1. Let x c (t) = A c cos(ω c t) be the carrier signal, with angular frequency ω c and amplitude A c , and x m (t) = 1 + hcos(ω m t) be the modulating signal with angular frequency ω m and modulation depth h ∈ [0, 1]. We then have: The resulting signal contains the original signal x c (t), and two new sinusoids: y + (t) and y -(t) at frequency ω c + ω m and ω c -ω m . In case x c (t) is a voice signal at fundamental frequency f 0 , approximated by a sum of N harmonics: x c (t) = N i=1 A i cos(iω 0 t), where ω 0 = 2πf 0 , the modulation of this signal by x m (t) is the sum of each harmonic modulated individually:\n\nwhere\n\nAugust 27, 2020 3/16\n\nBy choosing appropriate values for ω m , it is thus possible to generate sub-harmonics y + i and y - i between each of the original signal's partials, at frequencies ω 0 ± ω m . In particular, setting ω m = ω 0 /k generates pairs of sub-harmonics around each harmonic at iω 0 ± ω 0 /k. This is the approach we use here. In this work, we will evaluate the impact of adding sub-harmonics at ω 0 /k with k ∈ {2, 3, 4, 5} .\n\nIn addition, because amplitude modulation applies the same amplitude factor h to all created sub-harmonics (Eq.2), a provision is added to avoid unrealistically high amplitude for the lowest of these sub-harmonics, by high-pass filtering the sub-harmonic residual. As the original signal x c (t) is fully preserved in the modulated signal (Eq.2), the sub-harmonics can be isolated by subtracting this original signal from the modulated one: y sub (t) = y(t) -x c (t). Once the subharmonics are isolated, they are filtered with a second order bi-quad high-pass filter, with cutoff frequency f cut = 4f 0 , before being added back to the original signal by simple summation. We thus obtain the final rough voice signal as: y rough (t) = x c (t) + αHP (y sub (t)), where HP (y sub (t)) denotes the high-pass filtered sub-harmonics and α > 0 is a mixing factor. In this work, we will evaluate the impact of α for the values α ∈ {.25, .5, .75, 1} (modulation depth fixed to h = 1).\n\nA real-time implementation of this algorithm, as an open-source patch (MIT License) based on the closed-source software Max (Cycling '74), as well as a series of sound examples, are available at http://forumnet.ircam.fr/product/angus  [27] . The only computations required for the transformation are one multiplication for amplitude modulation, one subtraction to isolate sub-harmonics, a few multiplications and additions for filtering (depending on the order of the filter), and one addition and one multiplication for the final mixing step. This thus makes this approach especially suitable for real-time.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Control Algorithm",
      "text": "In this work, we compare the ANGUS algorithm to a more sophisticated, but non-real-time state-of-art method based on pulse analysis and resynthesis  [24, Section 6.3.2] . Contrary to ANGUS, this control algorithm requires the availability of original rough recordings from which a pattern of jitter and shimmer can be analysed. The algorithm then resynthesizes the original recording with varying levels of jitter and shimmer.\n\nThe control algorithm (thereafter, CONTROL) uses the PaN parametric speech synthesis engine [24, Section 3.5.2] to precisely control the positions and amplitudes of synthesized pulses. In more details, CONTROL first uses peak-picking to identify each individual pitch cycles, and computes the time series of their inter-peak-intervals (the variability of which is related to jitter) and inter-peak-amplitude-differences (related to shimmer). Then, it creates a new series of synthetic glottal pulses using the PaN synthesis engine, by interpolating at a ratio α c between the original jitter and shimmer time-series (α c =1.0) and a strictly periodic and iso-amplitude series set at the local average (slowly varying) period and amplitude of the original series (α c =0). Finally, the vocal signal is obtained by filtering the pulses with the corresponding vocal tract filter estimated on the original recording using the true-envelope algorithm  [28] , and by adding the high-frequency noise component extracted from the original recording (above the highest prominent harmonic). In this work, we use mixing factors α c ∈ {0, 0.25, 0.5, 0.75, 1.0}, where α c =1.0 corresponding to levels of jitter and shimmer that are identical to that of the originally rough signals.\n\nNote that, in the following, stimuli compared between the ANGUS and CONTROL procedures were not strictly equivalent: while we use ANGUS to add subharmonics to clear voice, thus increasing roughness, we use the CONTROL algorithm to decrease the roughness of originally rough voice (see Study 1-4-Procedure). In other words, both algorithms were not applied to identical stimuli, but to pairs of clear/rough recordings by the same speaker. The purpose of this procedure is therefore not to evaluate ANGUS as a computational alternative to the CONTROL algorithm, but to provide a set of reference stimuli in which jitter and shimmer are parametrically controlled and to evaluate how well ANGUS is able to simulate the higher-level perceptual-cognitive aspects of these stimuli, in terms of scalability (Study 1), perceived valence (Study 2), and perceived naturalness (Study 3), in addition to its transferability to non-vocal stimuli (Study 4), which is not possible with CONTROL.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Study 1: Effect On Jitter And Shimmer",
      "text": "Although ANGUS does not directly model pulse frequency and amplitude, the signal manipulations used here are indirectly related to shimmer and jitter. First, amplitude modulations with modulation frequency ω m proportional to ω 0 are expected to create periodic patterns of shimmer. Second, while sub-harmonics are not expected to create variations of pulse periodicity (although, conversely, increased jitter would create sub-harmonics  [29] ), the modifications imposed to the waveform by ANGUS may also affect the estimation of pulse positions used to measure jitter in typical implementations (such as Praat  [30] ). Voice processed with ANGUS are therefore expected to have increased levels for both measures. To quantify this effect, we evaluate here the effect of the k and α parameters of ANGUS on vocal jitter and shimmer, and compare with that of the α c parameter of the CONTROL algorithm, which is designed to directly and linearly modulate these two measures.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Stimuli",
      "text": "Stimuli for the experiment consisted of 24 short, 1-second recordings of human vocalizations (12 neutral, 12 rough). Vocalizations were recorded by one female (F1) and two male (M1,M2) actors instructed to shout/sing phonemes [a] (for F1,M1,M2) and [i] (for F1) at three different pitches (in the range [450,480], [570,600] and [520,570] Hz for F1;  [200, 215] , [250,270] and [315,340] Hz for M1 and M2), with a clear, loud voice (neutral stimuli), and to scream the same material with an angry, excited voice (rough stimuli). The resulting 24 stimuli (F1a,F1i,M1a,M2a × 3 pitches × neutral/rough) were then manually segmented to their middle 600ms sustained section, cutting onset and decay, and normalized in loudness. Original neutral files (12 stimuli) were processed with ANGUS at 4 levels of α ∈ {.25, .5, .75, 1}, and a single modulating signal at different subharmonic ratio ω0 k , k ∈ {2, 3, 4, 5}, resulting in 192 transformed files. Original rough stimuli (9 stimuli; for technical issues, M2a files were not processed) were transformed with CONTROL, at 4 levels of mixing factor α c ∈ {.25, .5, .75, 1.0}, resulting in 36 stimuli.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Procedure",
      "text": "Local jitter (average absolute difference between consecutive periods, divided by the average period) and shimmer (average absolute difference between the amplitudes of consecutive periods, divided by the average amplitude) were measured from recordings transformed with ANGUS and CONTROL using the Praat software  [30] . Statistical effects of the algorithm parameters on both measures were inferred from the sample of original recordings (N=12) using a rmANOVA, with k,α (ANGUS) and α c (CONTROL) as within-item factors; statistics reported below with F and p values.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Results",
      "text": "As expected, CONTROL had a significant, linear impact on the jitter (F(1,8)=8.69, p=.018) and shimmer (F(1,8)=23.51, p=.001) of the transformed sounds, which culminated at α c = 1.0, with a mean jitter value of 1.4% (above the 1.040% threshold for pathological speech, as expected for the scream register  [30] ) and mean shimmer value of 15% (above the 3.8% threshold for pathological speech).\n\nAlthough it did not directly model these measures, ANGUS also had a significant effect on measured jitter, which increased both with α (F(1,11)=6.63, p=.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Stimuli",
      "text": "Stimuli were the same as in Study 1, and consisted of 24 original neutral recordings (12 neutral, 12 rough), 192 transformed stimuli obtained from the original neutral recordings by manipulation with ANGUS (at various levels of α and k), and 36 transformed stimuli obtained from the originally rough recordings by manipulation with CONTROL (at various levels of α c ).",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Procedure",
      "text": "Study 2,3 and 4 were conducted in a single experimental session, in the order 2-4-3. In Study 2, participants were presented with all 252 stimuli in randomized order. In the current study (as well as Study 4), instructions given to participants implied that all stimuli were genuine (non-transformed) recordings of human vocalizations. In each trial, participants had to evaluate the emotion communicated by the speaker, using a unipolar continuous scale anchored by 1, corresponding to a sung note with no emotion and 10, corresponding to a highly negative and highly aroused expression. For each participant, ratings of negativity were averaged over all extracts that were transformed with the same parameter settings, and statistical effects of the algorithm's parameters were inferred from the sample of participants (N=22) using a rmANOVA, with pitch, k,α (ANGUS) and α c (CONTROL) as within-item factors.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Results",
      "text": "Both the k and α parameters of ANGUS had strong effects on the perceived negativity of the transformed extracts (Figure  3 -Middle & Right). Subharmonic ratio k (F(3,60)=12.38, p=2e-06) had a stronger effect for k ∈ {2, 3} (M=5.0) than k ∈ {4, 5} (M=4.8). At k=3, the increase of negativity over that of neutral recordings was ∆=+0.38 point of scale (95%-CI: [0.31,0.45]). Mixing factor α (F(3,60)=33.89, p=6e-13) had a linearly increasing effect on negativity (Figure  3 -Middle), with α=1.0 resulting in a ∆=+0.49 [0.41,0.57] point increase over the negativity of corresponding neutral extracts.\n\nFor comparison, we examined the effect of the α c mixing factor of the CONTROL algorithm. There was a significant effect on perceived negativity (F(3,60)=8.08, p=.0001), which was comparable in amplitude to that of ANGUS α (Figure  3 -Middle). At α c =1.0, the mean increase of negativity over that of neutral extracts was ∆=0.42 [0.28,0.56].\n\nIndependently from algorithm parameters, there was a strong effect of vocalization pitch on perceived negativity (F(2,40)=40.42, p=2e-10). High-pitch vocalizations (M=5.3) were judged more negative and aroused than low-pitch vocalizations (M=4.4; ∆ = +0.99 [+0.83,1.14]; Figure  3-Left) .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Stimuli",
      "text": "Stimuli consisted of 36 non-manipulated extracts and 36 manipulated extracts, incl. 12 ANGUS recordings and 24 CONTROL recordings. Stimuli were obtained from the same Left: High-pitch vocalizations were judged more negative than low-pitch vocalizations. Middle: ANGUS α (solid lines) had a linearly increasing effect on negativity, comparable in strength to that of the CONTROL algorithm (dashed line). Right: Subharmonic ratios k=2 and 3 had the strongest effect of negativity. Error bars: 95% CI on the mean. original recordings as Study 1, in the following manner: the 36 non-manipulated stimuli were obtained from the 12 original rough recordings (F1a,F1i,M1a,M2a × 3 pitches), each segmented into 3 different 600ms sections at various positions into the file, i.e. they were physically different extracts of the same utterances. The 12 ANGUS stimuli were obtained from the 12 original neutral recordings by transformation at the fixed setting k=3 and α=.75. The 24 CONTROL stimuli were obtained from the 12 original rough recordings (F1a,F1i,M1a,M2a × 3 pitches) by transformation at α c =.75. Twelve of these stimuli were obtained using the jitter and shimmer profile of the original rough recording of the same speaker (e.g. F1a ← F1a, as described in Section ). In addition, twelve other CONTROL stimuli were obtained by cross-synthesis, using the jitter and shimmer profile of another rough original recording in the same dataset (F1a←F1i, × 3 pitches; F1a←M2a, × 3 pitches; F1i←F1a, × 3 pitches; M2a←M1a, × 3 pitches).",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Procedure",
      "text": "Upon completion of Study 2 and 4, and before the current study, it was revealed to participants that some of the sounds had in fact been artificially manipulated to sound negative and aroused. Participants were then presented with all 72 stimuli in randomized order and explained that about half of these stimuli were manipulated to sound negative and aroused, and half were original recordings of speakers embodying these emotions. At each trial, participants had to indicate whether they thought the sound was original or transformed. We evaluated the algorithms' naturalness by computing each participant's hit rate and sensitivity index d' (computed with the log-linear correction  [31] ) corresponding to the successful detection of the 36 manipulated trials (chance level= 0.5).",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Results",
      "text": "The detection hit rate for ANGUS was statistically above chance level (H=0.58, [0.52,0.64]) but, because of a large number of false alarms (FA=0.57, i.e. many of the original samples were believed to be manipulated), d' was not statistically different from zero (d'=-0.005, [-0.17,0.17]). This indicates that participants were not better than chance at discriminating sounds transformed with ANGUS from genuine rough sounds.\n\nThe detection hit rate for the CONTROL algorithm was not statistically different from chance (H=0.48, [0.41,0.54]), and d' was even significantly negative (d'=-0.31, [-0.48,-0.13]), indicating that participants were consistent in finding original sounds less natural than sounds transformed with CONTROL. Correspondingly, there was a significant effect of algorithm ANGUS or CONTROL on participant d' (F(1,20)=6.87, p=.016), indicating that CONTROL stimuli sounded more natural to participants than ANGUS stimuli, but only because participants paradoxically found that CONTROL sounds were more natural than original sounds.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Study 4: Effect On Non-Vocal Stimuli",
      "text": "One major difference between the transformation approach of ANGUS and the analysis/resynthesis approach of the CONTROl algorithm used here, as well as other approaches for jitter modeling in the literature  [20] [21] [22] [23] , is that ANGUS makes no other assumption on the input signal as its having identifiable f 0 . Thus ANGUS can be applied to non-human vocal sounds, such as animal vocalizations, as well as non-vocal sounds such as musical instruments or alarms. There is a vast biological literature suggesting that roughness is a signal of threat and negative salience throughout the human and animal communicative repertoire  [32] [33] [34] . In addition, these signals, shaped by biological evolution, have also been selected for in cultural artefacts, such as musical sounds and sound alarms, which aim at the same effect  [7, 35] . In  [36] , musical sequences transformed with distortion were evaluated as more negative and aroused than non-transformed sequences; in  [5] , so were musical instrument samples and alarm sounds when manipulated with 40-60Hz temporal modulations. Here, we test the capacity of a single generative model, ANGUS, to increase the perceived negativity of a variety of non-vocal sounds.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Participants",
      "text": "Participants were the same as in Study 2 (N=21).",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Stimuli",
      "text": "Stimuli consisted in 4 groups of 36 recordings, each composed of 12 matched manipulated and non-manipulated sounds and an additional 12 non-manipulated sounds with no manipulated equivalents. \"Scream\" stimuli included the 24 stimuli of Study 3 (F1a,F1i,M1a,M2a × 3 pitches × neutral/rough), as well as 12 non-related screams recorded in similar conditions by 2 male and 2 female additional speakers, on varied phonemes ([o]:5, [u]:5, [a]:1, [i]:1) at non-controlled pitches. \"Music\" stimuli were extracted from the McGill University Master Samples sound library  [37] , and included single note recordings of three wind (bugle, clarinet, trombone) and one string (violin) instrument, each performed at three different pitches. Twelve of these recordings were matched with ANGUS transformations (24 sounds: 4 instruments × 3 pitches × neutral/rough), and an additional twelve were left non-transformed. \"Animal\" stimuli were extracted from the Freesound public sound-effect database (http://www.freesound.org) and included 24 matched transformed and non-transformed recordings of animal vocalizations (cat:2, cow:3, dog:2, goat:2, sheep:3), as well as 12 unmatched additional recordings (songbird:2, cat:2, cow:1, dog:1, goat:1, rooster:2, seagull:2, swan:1). \"Object\" stimuli were extracted from the Freesound database and included 24 matched transformed and non-transformed recordings of alarms and object sounds (phone ring:1, squeaky door:2, siren:4, clock alarm:2, machine:1, spring:2), as well as 12 additional unmatched sounds (siren:4, car horn:2, spring:1, phone ring:1, clock alarm:2, machine:2). For all sounds, as in Study 3, ANGUS August 27, 2020 9/16 stimuli were obtained from the original neutral recordings by transformation at the fixed setting k=3 and α=.75.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Procedure",
      "text": "The procedure was the same as Study 2. Participants listened to the 144 sounds, blocked by sound category (order: screams, animals, objects, music), in randomized order within each block. Unmatched sounds were included in each block (see above) to avoid attracting participants' attention to a too systematic matching of transformed and non-transformed stimuli. In each trial, participants had to evaluate the emotion communicated by the extract, using a unipolar continuous scale anchored by 1, corresponding to no emotion, and 10, corresponding to a highly negative and highly aroused emotion (we used a single construct for both emotional valence and arousal, as previous research has shown that these were identically affected by vocal roughness  [5, 38] ) . Instructions differed for each type of sound as follows: for screams, participants were asked to evaluate the emotion of the speaker (as in Study 2); for animals, participants were asked to evaluate the physiological state of the animal, from calm/resting to excited/fearful/aggressive; for objects, participants were asked to evaluate to what type of situation or object usage a sound designer could use the sounds, from neutral informative situations with no urgency or danger, to highly urgent or dangerous situations that require immediate attention; for musical instruments, participants were asked to evaluate to what type of musical ambiance a music composer could use these sounds, from a calm, neutral passage expressing little emotion to a high-energy musical ambiance composed to evoke tension, excitation or fear. For each participant, ratings of negativity were averaged over the 12 matched transformed and non-transformed sounds in each category, and statistical effects were inferred from the sample of participants (N=21) using a rmANOVA with sound category and manipulation (original/ANGUS) as within-item factors.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Results",
      "text": "Irrespective of ANGUS, there was a main effect of sound category on perceived negativity (F(3,60)= 10.97, p=7.67e-06), in which human vocalizations were generally evaluated as less negative than other sound categories. More importantly, there was a main effect of the ANGUS manipulation on perceived negativity across sound categories (F(1,20)=65.   4 ).",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Discussion",
      "text": "In this work, we present a real-time acoustic transformation algorithm, ANGUS, which uses amplitude modulation and time-domain filtering to simulate vocal arousal without attempting to directly control jitter and shimmer at the glottal source level. In a series of 4 studies, we show that (1) ANGUS allows parametric, albeit indirect, control over a vocal recording's shimmer as well as its measured jitter, that (2) ANGUS increases perceived emotional negativity of the transformed sounds to a comparable level as a non-real-time analysis/resynthesis algorithm from the state-of-the-art, that (3) listeners cannot distinguish transformed and non-transformed sounds above chance level and (4) that ANGUS has a similar emotional effect on animal vocalizations and musical instrument sounds than on human vocalizations. The fact that ANGUS modulates classical measures of jitter and shimmer (Study 1) confirms that subharmonics as created by ANGUS are related to irregularities of the period and amplitude of glottal pulses. It is important to note that the variations possible with ANGUS, and those with direct pulse modeling do not entirely overlap: for instance, frequency modulation of the pulses in fact generates an infinite series of sub-harmonics, rather than a small number as done here, and with more complex amplitude relations than what we model here. In addition, there is more to vocal arousal than roughness, e.g. broadband noise  [36] , pitch jumps  [14]  and modification of the spectral slope due to vocal effort  [39] , which ANGUS does not attempt to simulate. Yet, results from Study 2 suggest that the timbral subspace of vocal arousal explored by ANGUS is sufficient to evoke negative appraisals, and to a degree that is not strikingly less important than complete jitter and shimmer modeling, at least in our dataset.\n\nResults from Study 2 established that ANGUS and, incidentally, direct jitter and shimmer modeling with the CONTROL algorithm, resulted in a consistent, but relatively small, ∆=+0.5 point increase of perceived negativity on a 10-point scale, compared to non-manipulated sounds. This may be due to the fact that ANGUS only controls vocal timbre (and, even more precisely, only one component of all vocal timbral phenomena associated with vocal arousal, namely roughness), and that there are others, possibly more prominent, cues of emotional negativity that would drive participant judgement in a more ecological situation. For instance, mean pitch alone had here a ∆=+1.0 effect on negativity. Similarly, loudness, which is normalized in the present study, would also contribute to perceived negativity and arousal  [40] . For maximum effect, all of these cues should be combined.\n\nWhile results from Study 3 suggest that voices manipulated with ANGUS cannot be reliably discriminated from non-transformed voices in terms of naturalness, it was surprising that voices manipulated with the CONTROL algorithm were judged consistently more natural than non-transformed recordings. It may be that including both ANGUS and CONTROL recordings in the same test biased participants to identify only one type of cue for artificiality (e.g. temporal modulation), and to attribute to the 'untransformed' category anything else that did not include these cues (e.g. sounds transformed with the control algorithm). It is also possible that, by using short 600ms extracts, we created a situation in which greatly unstable vocalizations (such as those included in the original stimuli) were difficult to resolve as coherent signals  [41] , with the result of favoring transformed stimuli that displayed a less complex set of cues within the same time. For a more complete evaluation of how ANGUS affects naturalness, it may be necessary to replicate the study with longer extracts and only one type of manipulated sounds.\n\nFinally, the results of Study 4 confirm a growing line of research showing that emotional auditory cues such as vocal arousal  [5, 36]  but also pitch or spectral content  [42] [43] [44] , are largely transferable across stimulus domains. While the fact that subharmonics result in increased negativity in both human and animal vocalizations is biologically founded, because of cross-species similarities in the acoustic properties of the vocal apparatus and its neural control  [32] , it is more remarkable that cues of vocal arousal should modulate non-vocal musical sounds to the same effect. This is consistent with recent theoretical views according to which the expression of emotions by music, a culturally-evolved phenomenon, exploits biologically-evolved perceptual mechanisms designed to process communicative information in voices and gestures  [35, 45, 46] . For instance, joyful music is often associated with fast pace and animated pitch contours (as is happy speech), melancholic music with slower and flatter melodic lines and dark timbres (as is sad speech)  [42, 43]  and, as seen here, exciting music is associated with high levels of roughness, as are angry shouts  [7, 47] .\n\nIn previous research, alarm sounds were made more frightening by adding temporal modulations  [5] . That the same effect is not seen here with alarms and machine sounds requires further investigation: it is possible, e.g., that some of these stimuli did not have clear enough f 0 to allow manipulation with ANGUS, or that the transformation interacted with the semantics of the varied sound sources included in this category, making only some of the sounds more negative. It is in fact likely that such context effects affect appraisals of roughness for all types of sounds  [48] , allowing e.g. a rough shout to be perceived as a positive bout of laughter  [14] , or rough music as pleasingly empowering  [7] . Tools like ANGUS, which allow to control cues of vocal arousal at the stimulus level, contribute to make possible more research on how these cues are cognitively appraised in ecological situations  [38] .\n\nOn the whole, the present set of results establish that ANGUS is an effective and computationally-efficient method to control vocal arousal in a variety of sounds, for all situations where the focus is less on the precise acoustic control of e.g. pulse characteristics than on the real-time, emotional effect on listeners.",
      "page_start": 10,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The ANGUS algorithm. ANGUS simulates vocal arousal using amplitude",
      "page": 3
    },
    {
      "caption": "Figure 2: Effect of ANGUS and CONTROL on the measured jitter and",
      "page": 6
    },
    {
      "caption": "Figure 3: -Middle & Right). Subharmonic ratio k",
      "page": 7
    },
    {
      "caption": "Figure 3: -Middle), with α=1.0 resulting in",
      "page": 7
    },
    {
      "caption": "Figure 3: Effect of vocalization pitch and ANGUS parameters on perceived",
      "page": 8
    },
    {
      "caption": "Figure 4: Effect of ANGUS on the perceived negativity of vocal and non-vocal",
      "page": 11
    }
  ],
  "tables": [],
  "citations": [
    {
      "citation_id": "1",
      "title": "Calls out of chaos: the adaptive significance of nonlinear phenomena in mammalian vocal production",
      "authors": [
        "W Fitch",
        "J Neubauer",
        "H Herzel"
      ],
      "year": "2002",
      "venue": "Animal Behaviour"
    },
    {
      "citation_id": "2",
      "title": "Vocal communication of simulated pain",
      "authors": [
        "J Raine",
        "K Pisanski",
        "J Simner",
        "D Reby"
      ],
      "year": "2018",
      "venue": "Bioacoustics"
    },
    {
      "citation_id": "3",
      "title": "The perceptual effects of manipulating nonlinear phenomena in synthetic nonverbal vocalizations",
      "authors": [
        "A Anikin"
      ],
      "year": "2019",
      "venue": "Bioacoustics"
    },
    {
      "citation_id": "4",
      "title": "Exposure to arousal-inducing sounds facilitates visual search",
      "authors": [
        "E Asutay",
        "D Västfjäll"
      ],
      "year": "2017",
      "venue": "Scientific reports"
    },
    {
      "citation_id": "5",
      "title": "Human screams occupy a privileged niche in the communication soundscape",
      "authors": [
        "L Arnal",
        "A Flinker",
        "A Kleinschmidt",
        "A Giraud",
        "D Poeppel"
      ],
      "year": "2015",
      "venue": "Current Biology"
    },
    {
      "citation_id": "6",
      "title": "Growl voice in ethnic and pop styles",
      "authors": [
        "K Sakakibara",
        "L Fuks",
        "H Imagawa",
        "N Tayama"
      ],
      "year": "2004",
      "venue": "Leonardo"
    },
    {
      "citation_id": "7",
      "title": "Enjoy The Violence: Is appreciation for extreme music the result of cognitive control over the threat response system? Music Perception",
      "authors": [
        "R Ollivier",
        "L Goupil",
        "M Liuni",
        "J Aucouturier"
      ],
      "year": "2019",
      "venue": "Enjoy The Violence: Is appreciation for extreme music the result of cognitive control over the threat response system? Music Perception"
    },
    {
      "citation_id": "8",
      "title": "Detecting Aggression in Voice Using Inverse Filtered Speech Features",
      "authors": [
        "S Sahoo",
        "A Routray"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "9",
      "title": "Threat of Scream'paradigm: A tool for studying sustained physiological and subjective anxiety",
      "authors": [
        "M Beaurenaut",
        "E Tokarski",
        "G Dezecache",
        "J Grèzes",
        "The"
      ],
      "year": "2019",
      "venue": "bioRxiv"
    },
    {
      "citation_id": "10",
      "title": "Automatic detection of pain intensity",
      "authors": [
        "Z Hammal",
        "J Cohn"
      ],
      "year": "2012",
      "venue": "Proceedings of the 14th ACM international conference on Multimodal interaction"
    },
    {
      "citation_id": "11",
      "title": "Vocal expression of emotion: Acoustic properties of speech are associated with emotional intensity and context. Psychological science",
      "authors": [
        "J Bachorowski",
        "M Owren"
      ],
      "year": "1995",
      "venue": "Vocal expression of emotion: Acoustic properties of speech are associated with emotional intensity and context. Psychological science"
    },
    {
      "citation_id": "12",
      "title": "A four-parameter model of glottal flow",
      "authors": [
        "G Fant",
        "J Liljencrants",
        "Lin Qg"
      ],
      "year": "1985",
      "venue": "STL-QPSR"
    },
    {
      "citation_id": "13",
      "title": "A computationally efficient alternative for the liljencrants-fant model and its perceptual evaluation",
      "authors": [
        "R Veldhuis"
      ],
      "year": "1998",
      "venue": "The Journal of the Acoustical Society of America"
    },
    {
      "citation_id": "14",
      "title": "An open-source tool for synthesizing nonverbal vocalizations",
      "authors": [
        "A Anikin",
        "Soundgen"
      ],
      "year": "2019",
      "venue": "Behavior research methods"
    },
    {
      "citation_id": "15",
      "title": "Seewave, a free modular tool for sound analysis and synthesis",
      "authors": [
        "J Sueur",
        "T Aubin",
        "C Simonis"
      ],
      "year": "2008",
      "venue": "Bioacoustics"
    },
    {
      "citation_id": "16",
      "title": "A study of glottal excitation synthesizers for different voice qualities",
      "authors": [
        "J Alonso",
        "M Ferrer",
        "P Henríquez",
        "López-De Ipina",
        "K Cabrera",
        "J Travieso"
      ],
      "year": "2015",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "17",
      "title": "Development and perceptual assessment of a synthesizer of disordered voices",
      "authors": [
        "S Fraj",
        "J Schoentgen",
        "F Grenez"
      ],
      "year": "2012",
      "venue": "The Journal of the Acoustical Society of America"
    },
    {
      "citation_id": "18",
      "title": "Unit selection in a concatenative speech synthesis system using a large speech database",
      "authors": [
        "A Hunt",
        "A Black"
      ],
      "year": "1996",
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing Conference Proceedings"
    },
    {
      "citation_id": "19",
      "title": "A generative model for raw audio",
      "authors": [
        "Oord Avd",
        "S Dieleman",
        "H Zen",
        "K Simonyan",
        "O Vinyals",
        "A Graves"
      ],
      "year": "2016",
      "venue": "A generative model for raw audio"
    },
    {
      "citation_id": "20",
      "title": "Mixed source model and its adapted vocal tract filter estimate for voice transformation and synthesis",
      "authors": [
        "G Degottex",
        "P Lanchantin",
        "A Roebel",
        "X Rodet"
      ],
      "year": "2013",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "21",
      "title": "Introducing roughness in individuality transformation through jitter modeling and modification",
      "authors": [
        "A Verma",
        "A Kumar"
      ],
      "year": "2005",
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "22",
      "title": "Stochastic models of pitch jitter and amplitude shimmer for voice modification",
      "authors": [
        "D Ruinskiy",
        "Y Lavner"
      ],
      "year": "2008",
      "venue": "Electrical and Electronics Engineers in Israel"
    },
    {
      "citation_id": "23",
      "title": "Transforming modal voice into irregular voice by amplitude scaling of individual glottal cycles",
      "authors": [
        "T Bõhm",
        "N Audibert",
        "S Shattuck-Hufnagel",
        "G Németh",
        "V Aubergé"
      ],
      "year": "2008",
      "venue": "Journal of the Acoustical Society of America"
    },
    {
      "citation_id": "24",
      "title": "Synthesis and expressive transformation of singing voice",
      "authors": [
        "L Ardaillon"
      ],
      "year": "2017",
      "venue": "Synthesis and expressive transformation of singing voice"
    },
    {
      "citation_id": "25",
      "title": "Generation of growl-type voice qualities by spectral morphing",
      "authors": [
        "J Bonada",
        "M Blaauw"
      ],
      "year": "2013",
      "venue": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "26",
      "title": "Beyond correlation: acoustic transformation methods for the experimental study of emotional voice and speech",
      "authors": [
        "P Arias",
        "L Rachman",
        "M Liuni",
        "J Aucouturier"
      ],
      "year": "2020",
      "venue": "Emotion Review"
    },
    {
      "citation_id": "27",
      "title": "Vocal distortion and real-time processing of roughness",
      "authors": [
        "M Gentilucci",
        "L Ardaillon",
        "M Liuni"
      ],
      "year": "2018",
      "venue": "Proceedings of the International Computer Music Conference (ICMC)"
    },
    {
      "citation_id": "28",
      "title": "Efficient spectral envelope estimation and its application to pitch shifting and envelope preservation",
      "authors": [
        "A Röbel",
        "X Rodet"
      ],
      "year": "2005",
      "venue": "Proc. of the 8th Int. Conference on Digital Audio Effects (DAFx'05)"
    },
    {
      "citation_id": "29",
      "title": "Nonlinear behavior of vocal fold vibration: the role of coupling between the vocal folds",
      "authors": [
        "A Giovanni",
        "M Ouaknine",
        "B Guelfucci",
        "P Yu",
        "M Zanaret",
        "J Triglia"
      ],
      "year": "1999",
      "venue": "Journal of Voice"
    },
    {
      "citation_id": "30",
      "title": "Praat: doing phonetics by computer",
      "authors": [
        "P Boersma",
        "D Weenink"
      ],
      "venue": "Praat: doing phonetics by computer"
    },
    {
      "citation_id": "31",
      "title": "Corrections for extreme proportions and their biasing effects on estimated values ofd",
      "authors": [
        "M Hautus"
      ],
      "year": "1995",
      "venue": "Behavior Research Methods, Instruments, & Computers"
    },
    {
      "citation_id": "32",
      "title": "Calls out of chaos: the adaptive significance of nonlinear phenomena in mammalian vocal production",
      "authors": [
        "W Fitch",
        "J Neubauer",
        "H Herzel"
      ],
      "year": "2002",
      "venue": "Animal behaviour"
    },
    {
      "citation_id": "33",
      "title": "The sound of arousal: The addition of novel non-linearities increases responsiveness in marmot alarm calls",
      "authors": [
        "D Blumstein",
        "C Recapet"
      ],
      "year": "2009",
      "venue": "Ethology"
    },
    {
      "citation_id": "34",
      "title": "Human Non-linguistic Vocal Repertoire: Call Types and Their Meaning",
      "authors": [
        "A Anikin",
        "R Bååth",
        "T Persson"
      ],
      "year": "2018",
      "venue": "Journal of nonverbal behavior"
    },
    {
      "citation_id": "35",
      "title": "Animal signals and emotion in music: coordinating affect across groups",
      "authors": [
        "G Bryant"
      ],
      "year": "2013",
      "venue": "Frontiers in psychology"
    },
    {
      "citation_id": "36",
      "title": "The sound of arousal in music is context-dependent",
      "authors": [
        "D Blumstein",
        "G Bryant",
        "P Kaye"
      ],
      "year": "2012",
      "venue": "Biology letters"
    },
    {
      "citation_id": "37",
      "title": "",
      "authors": [
        "F Opolko",
        "J Wapnick",
        "Mcgill"
      ],
      "year": "1989",
      "venue": ""
    },
    {
      "citation_id": "38",
      "title": "Sound context modulates perceived vocal emotion",
      "authors": [
        "M Liuni",
        "E Ponsot",
        "G Bryant",
        "J Aucouturier"
      ],
      "year": "2020",
      "venue": "Behavioural Processes"
    },
    {
      "citation_id": "39",
      "title": "Vocal Effort Modification for Singing Synthesis",
      "authors": [
        "O Perrotin",
        "C Alessandro"
      ],
      "year": "2016",
      "venue": "Vocal Effort Modification for Singing Synthesis"
    },
    {
      "citation_id": "40",
      "title": "Acoustic profiles in vocal emotion expression",
      "authors": [
        "R Banse",
        "K Scherer"
      ],
      "year": "1996",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "41",
      "title": "The source dilemma hypothesis: Perceptual uncertainty contributes to musical emotion",
      "authors": [
        "T Bonin",
        "L Trainor",
        "M Belyk",
        "P Andrews"
      ],
      "year": "2016",
      "venue": "Cognition"
    },
    {
      "citation_id": "42",
      "title": "Communication of emotions in vocal expression and music performance: Different channels, same code?",
      "authors": [
        "P Juslin",
        "P Laukka"
      ],
      "year": "2003",
      "venue": "Psychological bulletin"
    },
    {
      "citation_id": "43",
      "title": "A Comparison of Acoustic Cues in Music and Speech for Three Dimensions of Affect",
      "authors": [
        "G Ilie",
        "W Thompson"
      ],
      "year": "2006",
      "venue": "Music Perception"
    },
    {
      "citation_id": "44",
      "title": "Human emotions track changes in the acoustic environment",
      "authors": [
        "W Ma",
        "W Thompson"
      ],
      "year": "2015",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "45",
      "title": "Emotional responses to music: The need to consider underlying mechanisms",
      "authors": [
        "P Juslin",
        "D Västfjäll"
      ],
      "year": "2008",
      "venue": "Behavioral and brain sciences"
    },
    {
      "citation_id": "46",
      "title": "A multi-sensory code for emotional arousal",
      "authors": [
        "B Sievers",
        "C Lee",
        "W Haslett",
        "T Wheatley"
      ],
      "year": "1906",
      "venue": "Proceedings of the Royal Society B"
    },
    {
      "citation_id": "47",
      "title": "Terrifying film music mimics alarming acoustic feature of human screams",
      "authors": [
        "C Trevor",
        "L Arnal",
        "S Frühholz"
      ],
      "year": "2020",
      "venue": "The Journal of the Acoustical Society of America"
    },
    {
      "citation_id": "48",
      "title": "Neurobiology: Sounding the alarm",
      "authors": [
        "P Belin",
        "R Zatorre"
      ],
      "year": "2015",
      "venue": "Current Biology"
    }
  ]
}