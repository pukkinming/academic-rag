{
  "paper_id": "2210.04935v1",
  "title": "Deep Insights Of Learning Based Micro Expression Recognition: A Perspective On Promises, Challenges And Research Needs",
  "published": "2022-10-10T18:08:24Z",
  "authors": [
    "Monu Verma",
    "Santosh Kumar Vipparthi",
    "Girdhari Singh"
  ],
  "keywords": [
    "Micro expression recognition",
    "Facial expression recognition",
    "CNN models",
    "deep learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Micro expression recognition (MER) is a very challenging area of research due to its intrinsic nature and finegrained changes. In the literature, the problem of MER has been solved through handcrafted/descriptor-based techniques. However, in recent times, deep learning (DL) based techniques have been adopted to gain higher performance for MER. Also, rich survey articles on MER are available by summarizing the datasets, experimental settings, conventional and deep learning methods. In contrast, these studies lack the ability to convey the impact of network design paradigms and experimental setting strategies for DL based MER. Therefore, this paper aims to provide a deep insight into the DL-based MER frameworks with a perspective on promises in network model designing, experimental strategies, challenges, and research needs. Also, the detailed categorization of available MER frameworks is presented in various aspects of model design and technical characteristics. Moreover, an empirical analysis of the experimental and validation protocols adopted by MER methods is presented. The challenges mentioned earlier and network design strategies may assist the affective computing research community in forge ahead in MER research. Finally, we point out the future directions, research needs and draw our conclusions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "M ICRO-expressions (MEs) exhibit the insight into true feelings of a person even if he/she is trying to hide the genuine emotions within the manifested emotion (macro expression). Ekman  [1]  introduced various deceptive expressions (known as MEs) after investigating a depressed patient's interview video who attempted to commit suicide. Ekman observed that, the patient surpasses his intensive sadness in happiness within 1/12 seconds. However, these expressions were spotted in a few frames of the video, recorded through a standard 25 fps device but serves enough clues to sense the true sentiments of the patient. Thus, MEs can be decisive in the fields of spotting genuine psychological activities  [2] , e.g., lie detection, psychoanalysis, criminal interrogation, medical diagnosis, pain detection, autism disorder, and business negotiation.  Monu  Verma is with the Electrical and Computer Engineering, University of Miami, FL, USA, 33155. Santosh Kumar Vipparthi is with the CVPR lab of the Indian Institute of Technology Ropar Department of Electrical Engineering, Roopnagar, India, 140001. Girdhari Singh is with the Department of Computer Science and Engineering, Malaviya National Institute of Technology, Jaipur, INDIA, 302017. E-mail: monuverma.cv@gmail.com, skvipparthi@iitrpr.ac.in and gsingh.cse@mnit.ac.in MER can broadly be divided into three steps: preprocessing, feature extraction and emotion classification. Preprocessing Techniques: The first step of MER is to spot the MEs and then detect the RoIs from them ME spotting is a vital step for automatic ME analysis as it locates the segments of micro-movements in a MEs video. Thus, precise ME spotting can decrease the redundant information and improve the performance of MER further. Recently, few studies  [3] -  [5]  focused on ME spotting using deep learning methods. Li et al.  [6]  introduced an ME spotting method for spontaneous ME datasets. Furthermore, Zhang et al.  [3]  designed a deep learning-based ME spotting method by extracting features from video clips. Tran et al.  [5]  proposed a deep sequence model for ME spotting. Moreover, Liong et al.  [4]  proposed an automatic apex frame spotting model. (A more detailed categorization and analysis of the ME spotting can be found in existing MER surveys  [6] -  [9] ). Further, face alignment and noise filtration are employed to systematize the input data samples for better feature extraction and learning  [8] . Some of the MER frameworks also utilized the motion magnification (MM)  [10] ,  [11]  and temporal normalization  [12] ,  [13]  techniques to enhance the visibility of the minute temporal variations and normalize the frames. Recently, deep learning (DL) based approaches require huge dataset for training. However, all available MEs datasets are far from the enough data samples. Therefore, data augmentation techniques such as: random crop and rotation in terms of the spatial domain, shifting, magnification  [13]  and synthetic data generation using generative adversarial networks (GANs)  [14] ,  [15]  are also gaining the attention to enhance the data samples. The more details can be seen in  [8] ,  [16] .\n\nBased on feature extraction and classification methods, MER approaches can be categorized into traditional and deep learning based approaches. Traditional handcrafted MER methods: The traditional MER methods rely on the predesigned feature descriptors to encode the spatial and temporal changes from the MEs video sequences. In literature many robust spatio-temporal feature descriptors: local binary pattern (LBP) and its variants: three orthogonal planes (LBP-TOP)  [17] , LBP with six intersection points (LBP-SIP)  [18] , spatiotemporal LBP with integral projection (STLBP-IP)  [19] , revisited integral projection (DiSTLBP-RIP)  [20]  etc. were proposed for MER. Furthermore, some optical flow based descriptors: main directional mean optical flow (MDMO)  [21] , sparseMDMO  [22] , FHOFO  [23] , facial dynamic map (FDM) and color based descriptors: TICS  [24]  were introduced to encode the features of MEs. After that, the encoded features are forwarded to the classifiers such as support vector machine (SVM), neural networks (NN), etc., which learn the distinctive properties of the emotion classes. Sensitivity and specificity of the traditional descriptors have gained good performance as compared to professionally trained specialists. However, it is still difficult to manually design a robust descriptor for capturing quick subtle changes in MEs. The detailed summary of the traditional MER approaches are listed in the supplementary draft (supplementary: Table  III ), A more detailed categorization of traditional methods and classifiers can be found in  [6] ,  [9] .\n\nDeep Learning based MER methods: The supervised techniques of deep learning adaptively learn the features from the raw data and classify the emotion classes accordingly. This paper aims to describe the details of network design strategies followed in the literature in-terms of downsampling, multi-stream, multi-scale, deep or shallow networks, kernels depth, sizes, etc., for MER. Moreover, there is no standard evaluation protocol, class settings and metrics for the fair comparison of the models were not present in the literature. Therefore, it is difficult to come up with a common conclusion of the performances for the existing state-of-theart approaches. Thus, these factors motivated us to present a detailed survey by addressing the effects of selecting the input formats, evaluation strategies, implementation settings, evaluation metrics on MER performance. More details of input formats such as apex frame, onset-apex-offset frames, compressed single instance image, and image sequences are studied and discussed their effect on the model's overall performance. Similarly, the evaluation strategies like person dependent, a person independent, composite, cross-domain, and class settings like 3-, 4-, 5-and 7-emotion classes, and implementation setting like learning rates, data augmentation, evaluation metrics: recognition accuracy, F1-score, unweighted F1-score, unweighted average recall, mean diagonal value of the confusion matrix and its impact on overall models performances is discussed in detailed.  1",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Comparison With Previous Reviews",
      "text": "In past years, many notable surveys related to MER approaches have been published and the details are summarized in Table  I . Many articles focus to summarise the details of datasets, preprocessing techniques, traditional MEs spotting along with the feature extraction algorithms, classifiers and experimental settings. Also, the survey of the MER framework is available in  [8] ,  [9] ,  [16] ,  [25] ,  [26] . Firstly, Merghani et al.  [25]  briefly described CNN frameworks and the technical differences with the traditional approaches in MER. Further, a detailed study of MEs databases and the comparative analysis of the data with challenges in data accumulation and labeling, evaluation metrics, and strategies are discussed. Similarly, Goh et al.  [16]  presented a survey by including datasets, preprocessing techniques, MEs spotting, and feature extraction algorithms. The main focus of the study is to highlight the ME features by dividing them into three categories: low, mid, and high-level features. Also, Zhou et al  [26]  presented a brief survey of the available traditional and deep learning techniques with preprocessing techniques and datasets. Apart from the similar data collection, evaluation matrix, and categorization of the conventional and CNN methods, Xie et al  [8]  present the details of the macro-to-micro feature adoption and synthetic data generation to balance MEs data samples. Guerdelli et al.  [28]  prsented a detailed survey of facial expression datasets by including both macro and micro expressions. In recent times, Ben et ac.  [9]  presented a \"micro-and-macro expression warehouse (MMEW)\" dataset by incorporating both micro and macro expressions. Also, a detailed study of the challenges while creating the datasets is presented. The apparent technical differences between the conventional approaches in the literature are presented for ME. In addition, a brief study of the CNN techniques is given in this article. From the above details, it is clear that the available survey articles focus on presenting a categorization of the available models and datasets. Similarly, Li et al.  [27]  presented a detailed study of deep learning methods for MER, challenging datasets, and comparative analysis between most influential MER methods. Moreover, the study also detailed the remaining challenges and future scope of the MER. However, they fail to comprehensively analyze the essential aspects like model designing, evaluation strategies, challenges, and research needs for deep learning models. Therefore, this article focuses on presenting a detailed survey on:  The survey also conducted the competitive analysis of conventional MER approaches.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Arxiv-2018 [25]",
      "text": "A Review on Facial Micro-Expressions Analysis: Datasets, Features and Metrics A survey of the various feature extraction algorithms, classifiers and evaluation metrics for MER. The survey also presents the datasets , challenges in data accumulation, comparative analysis between available datasets, evaluation metrics, validation strategies and analysis of learning approaches as compared to traditional approaches.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "4.",
      "text": "Vis.Comp-2020  [16]  (Springer)\n\nMicro-expression recognition: an updated review of current trends, challenges and solutions A survey of traditional approaches for MEs detection and recognition by categorizing solutions into low-level, mid-level, and high-level solutions. The comparative review of the existing deep learning frameworks with research needs and the deep study of the technical characteristics like, end-to-end vs two-stage architecture, downsampling, multi-stream/scale structure, deeper vs shallow network, etc. has done. Moreover, the MER approaches used different metric calculation or platforms or number of emotion classes or input strategies; therefore it is difficult to compare the performance of MER frameworks. We reviews these factors in detail and highlighted the effects on the performance of MER.\n\nTo the best of our knowledge this is the first attempt to comparatively analyze the role of various designing modules, evaluation strategies, and experimental settings for learning based MER frameworks. The main aim of this study is to help researchers or affective computing community to concentrate on the effective designing modules and experimental settings to design a robust MER framework. The detailed comparison between recently published survey  [9]  and proposed survey is presented in the supplementary file. In addition, the supplementary file also included the visual presentation of the different DL-based MER approaches. The detailed information about the handcrafted MER approaches is tabulated in Table  III . of the supplementary file.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Ii. Deep Learning Based Micro Expression Recognition",
      "text": "The MER frameworks demand spatio-temporal feature learning with momentary changes to capture the subtle variations of MEs. These factors make the design and development of deep learning models for MER an incredibly challenging task. In this section, we present an empirical review of deep learning methods highlighted in Fig.  1 . Based on available framework architectures in the literature, we categorize these frameworks into three broad categories: multi-stage (Section II-A), an end-to-end (Section II-B), and transfer learning (Section II-C) based MER frameworks as shown in Table  II . We further divide the categories into subcategories based on different network characteristics: 2D-CNN, multi-scale/stream, cap-",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Multi-Stage Frameworks",
      "text": "In MER, most of the multi-stage frameworks are twostage, where, in the first stage, the handcrafted descriptors are employed to extract the primary features. In the second stage, the CNN network is used to learn the expressive features of the MER. Based on characteristics of these deep learning MER models, two-stage frameworks are further classified into subcategories: 2D-CNN, multi-stream/scale, capsule, RCN and 3D-CNN. More details of the sub categorization is discussed as follows.\n\n1) 2D-CNN Models: Almost all deep learning based MER approaches adopt the 2D-CNN networks  [11] ,  [29] -  [32] . Learning the spatio-temporal features from 2D-CNN is an insignificant problem. Therefore, to maintain the compatibility with 2D-CNN, researchers have designed two-stage frameworks  [11] ,  [29] -  [34] . Where, in the first stage optical flow or single instance image is computed by applying handcrafted approaches. While in the second stage the 2D-CNN model is designed to learn the MEs specific features. Specifically, Verma et al.et al.  [29]  introduced a 2D-CNN MER framework. The framework first computed the single image instances by applying the dynamic imaging. Furthermore, the 2D-CNN model LEARNet is designed to learn the MEs specific features. Gupta  [35]  introduced the 2D-CNN MERASTC approach for MER by encoding the subtle deformations through action units (AUs), landmarks, gaze, and appearance features of MER. Discussion: For MER, 2D-CNN models require an auxiliary first stage to process the spatio-temporal information into 2dimensional formats (Supplementary Fig.  2 ). However, the 2D-CNN are easy to design as well as require very less computational cost as compared to spatio-temporal networks like CNN-LSTM, RCN, 3D-CNN etc. In addition 2D-CNN approaches  [33] -  [35]  have shown impressive performance as shown in Table IV-V. Therefore, 2D-CNN networks gain much attention as compared to other techniques in MER as reported in Table  II .\n\n2) Multi-Stream Networks: Multi-stream networks capture a diverse range of features from different streams to learn salient edge variations of the MEs. In the literature, many two-stage MER frameworks  [33] ,  [36] -  [38]  exploit the hybrid feature learning capability of the multi-stream networks. Li et at.  [11]  designed two streams (local and global path) based networks to use the coupled features of local (sub-regions) and global (whole face), respectively. Song et al.  [33]  proposed a three streams-based CNN network consisting of a staticspatial stream, local-spatial stream, and dynamic-temporal stream to capture three different clues in three different frames. Furthermore, spatial features are concatenated and fed to the single LSTM to learn the temporal features. Khor et al.  [36]  introduced a CNN model with two streams, channel wise stacking for spatial enrichment and feature wise stacking for temporal enrichment. Liong et al.  [39]  designed a shallow three stream network to learn the optical flows guided features. Similarly, to use the optical flow guided features, Ganet et al.  [37]  introduced two stream networks for MER. Yang et al.  [38]  exploit the feature discriminative capability of the VGGNet-16 to capture the spatial feature of MEs in three streams: ME sequences, optical flow, and optical strain. Liu et al.  [37]  introduced a five-stream network with capsuleNet to improve the performance of MER. Also, a detailed comparison of the performances of multi-stream networks is tabulated in Table IV-V. Discussion: In literature Multi-Stream networks achieve high performance in MER over sequential 2D-CNN networks. The multi-stream networks can capture the diverse range of features from different streams and boost the efficiency of the network. Moreover, the multi-stream networks benefited MER frameworks to learn enough features with shallow networks and small data samples. From Table IV-V it is evident that multi-stream networks outperform the sequential networks in terms of performance.\n\n3) Multi-Scale Networks: Multi-scale feature representations have been successfully used in two-stage MER  [12] ,  [30] . Zhou et al.  [12]  a dual-inception network that operates in three scales (1 × 1, 3 × 3 and 5 × 5) for feature encoding of MEs. Verma et al.  [29]  introduced a multi-scale based lateral assertive hybrid network to capture the micro-level features of an expression in the facial regions. Zhai et al.  [62]  extended the the LearNet approch and introduced the displacement generating module based MER (DGMER) framework. Song et al.  [33]  encodes spatio-temporal features by employing 5 × 5 and 3 × 3 sized filters in a consecutive manner. Verma et al.  [31]  improve the robustness of MER with hybrid (fusion of 3 × 3 and 5 × 5) local receptive feature blocks. Furthermore, Verma et al.  [30]  designed the AffectiveNet by incorporating MICRoFeat block to conserve the scale-invariant features with3 × 3, 5 × 5, 7 × 7 and 11 × 11 sized convolution (conv) filters. Discussion: Similar to multi-stream MER frameworks, multiscale MER frameworks also achieve impressive performance. The multi-scale convolution layers guide the network towards both minute and abstract level features,which are significant to describe the distinctive features of different micro expressions. The more detailed analysis based on literature results have been included in section II-D and Table IV-V.\n\n4) Capsule Networks: CNNs have shown impressive performance in literature. However, CNN models are computationally expensive and need a lot of data to train a model for specific-domain tasks. Moreover, the CNN model can pay attention to the translation in variance but failed to learn the rotation in variance. To resolve these issues, Sabour et al.  [63]  introduced the concept of Capsule. Capsule is a group of neurons to maintain the part-whole relationship and handle the viewpoint in variance. Some two-stage MER approaches  [37] ,  [48]  have exploited the capability of Capsule networks. Very first, Quang et al.  [48]  used the Capsule networks along with ResNet 18 and secured 4 th position in MEGC-2019 challenge). Liu et al.  [37]  utilized the Capsule module with a multi-stream CNN for MER. Discussion: The capsule based networks can handle both translation and rotation variations and design a more robust MER network as compared to CNNs. Moreover, capsule networks facilitate the more concrete features represented, which can be interpreted to understand the behaviour of the network (how the network is learning the MEs' features). However, the capsule based MER approaches are not widely notable due to complex nature and computational cost, though they have shown great promise. The capsule-based networks are still evolving and there is lots of scope for the researchers to create better and faster architectures so that it will be the baseline for solving any expression (MaEs or MEs) classification problem.\n\n5) Recurrent Convolutional Networks: Recurrent convolutional networks (RCNs) enable every unit to incorporate context information in an arbitrarily large region in the current conv layer and allows learning microlevel edge variations. In literature, Xia et al.  [10] ,  [13] ,  [49]  exploit the RCN by following two-stage architecture to learn the representation of subtle facial movements from image sequences. In these studies the recurrent connection within the feed-forwarded conv layers are employed to learn the temporal variations of image sequences extracted by multiple-scale receptive fields. Discussion: CNN's were inspired by early findings in the study of biological vision and share properties of the visual system of the brain. One notable distinction is that CNN is often a feed-forward design, whereas the visual system of the brain is abundant with recurrent connections. Thus, RCNbased architectures are benefited with more microbiologically realistic than their feed-forward counterparts. In addition, the activities of RCN units evolve over time as the activity of each unit is modulated by the activities of its neighbouring units, which allows the network to learn distinctive edge variations with temporal information in MEs under challenging conditions over CNN as reported in Table  III -VI.\n\n6) 3D-CNN Models: Most of the existing MER approaches  [11] ,  [45] ,  [48]  rely only on the apex frame/single instance for the analysis of MEs through 2D-CNNs. However, some studies emphasize the importance of dynamic aspects for detecting the subtle changes  [64]  and its effect on the performance of MER. In MEs video, each frame has its own significance towards the identification of the emotion class. Whereas some other CNN models  [36] ,  [38] ,  [50] , exploit the capability of 2D CNN and LSTM/RNN to elicit the spatial and temporal features, respectively. However, these models are not capable of extracting joint features of spatial and temporal variations, simultaneously  [47] . Therefore to overcome the above issues, recently, some of MER approaches  [39] ,  [46] ,  [59] ,  [60]  have taken advantage of the 3D-CNN network to capture, both  spatial and temporal features simultaneously by adopting two stage architecture. Discussion: The 3D convolutional layers are reasonable to learn the spatio-temporal information simultaneously. However, 3D-CNNs do not gain much attention in MER as it holds huge parameters and requires more computation power as compared to others such as 2D-CNN, CNN-LSTM, RCN etc. Also, deciding the number of hyper parameters such as layers, 3D down-sampling, and number of filters, in the network is a challenging task.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. End-To-End Frameworks",
      "text": "An end-to-end model means that the CNN model takes the raw data as input and gives the final response without any aid of external modules or blocks (see supplementary Fig.  5a ). Based on characteristics of the deep learning models, end-toend frameworks are also categorized as: multi-scale, CNN-LSTM, 3D-CNN, Graph based, and NAS based MER.\n\n1) Multi-Scale Networks: As discussed in Section II-A3, multi-scale networks have shown great performance in twostage networks. Similarly, to exploit the capability of multiscale conv layers, Wang et al.  [50]  proposed an end-to-end micro-attention for MER by utilizing two scales 1 × 1 and 3 × 3 to encode the micro expressive features. Discussion: Similar to multi-stage networks, the multi-scale convolution layers also achieve impressive performance in case of end-to-end MER models. MEs are holding very sensitive and subtle information, thereby extracting the minute muscle changes within coarse facial features. It is important to learn minute as well as abstract features of the facial appearance and multi-scale convolution layers learn the distinctive features of true emotions. Thus, multi-scale convolutional layers can be embedded in any type of deep learning MER models to extract the disparities between different micro expressions.\n\n2) CNN-LSTM Models: In CNN-LSTM networks, CNN is used to extract the spatial features and LSTM is included to learn the time-scale dependent information that resides along with the frame sequences. First, Kim et al.  [40]  used 2D-CNN followed by LSTM to encode the spatial and temporal features in MEs videos. Similarly, other work  [36] ,  [38] ,  [65]  also utilized the combination of CNN and LSTM based conv networks to design end-to-end MER frameworks. CNN is employed to encode the MEs frames into spatial feature vectors and then MEs classes are predicted by passing the resultant features through the LSTM module. Choi et al.  [56]  introduced an integrated framework of CNN and LSTM for landmark feature map-based MER.\n\nDiscussion: The CNN-LSTM based MER framework allows to capture and classify the spatio-temporal features of MEs in an end-to-end manner. However, CNN-LSTM networks first learn the spatial features and then temporal features. Therefore, sometimes these networks fail to correlate the spatial and temporal features simultaneously and fail to achieve good performance. Thus, 3D-CNN based models are introduced to resolve the issue of CNN-LSTM and learn the spatio-temporal features simultaneously. The detailed insights of models are discussed in Section II-D.\n\n3) 3D-CNN Models: As discussed in Section II-A6, cascaded MER architectures such as CNN with LSTM or RNN are not capable of extracting joint features of spatial and temporal variations, simultaneously  [47] . The capability of 3D-CNN to describe the spatio-temporal features for MEs in an end-to-end manner was first presented in  [47] . Xie et al.  [53]  adopts the 3D ConvNet based Pseudo-3D to design a light weighted end-to-end architecture. Furthermore, AU node features are extracted and processed through the AU graph relation learning module to describe the emotion classes of MEs.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "4) Graph Based Cnn Models:",
      "text": "Recently, the graph based end-to-end CNN approaches have achieved attention and attracted the researchers  [53] ,  [55] ,  [66] ,  [67]  in the field of MER. Lei et al.  [55]  exploited the capability of landmarks and proposed a graph temporal CNN (Graph-TCN) to capture the local muscle movements of the MEs. The Graph-TCN method consists of two streams: node and edge feature extraction streams. Finally, both node and edge features are merged to classify the emotion label of MEs. Xie et al.  [53]  used the AUs relation graph to learn the subtle facial muscle movements for MER.\n\nEach expression originated due to facial muscle movements and divided the face into small regions named action units (AUs) to represent the affective expression regions, defined in Facial Action Coding System (FACS)  [68]  and Micro-Expression Training Tool (METT)  [69] . Thus, AUs are frequently used to describe how emotions are physically expressed. Most of the graph-based deep learning models incorporated the AUs relationship. Lo et al.  [70]  introduced MER-GCN, an AU-oriented MER architecture based on Graph Convolutional Network (GCN)  [71] , where GCN layers are able to explore the dependency laying between AU nodes for MER. Similarly, Xie et al.  [53]  and Lei et al.  [67]  exploit utilized the GCN to discover the AUs relationship. Lei et al.  [55]  utilized the graph structure for node and edge feature extraction. Discussion: The graph based models are growing gradually in MER. The new affective computing researchers have a lot of scope in the graph based FER/MER approaches. The graph based models are complex to understand but need very less computation cost with great efficiency, which is the future demand to work with memory limited or hand-held devices.\n\n5) GANs based MER: With the advancement in learning based methods, the performance of the MER is improved but still limited due to the lack of large-scale training data, computation and design expertise. Some recent works  [15] ,  [53] ,  [54] , have been focused on these issues and provide solutions by exploiting the power of GANs. Yu et al.  [15]  introduced a capsule enhanced GAN to generate the synthetic MEs with identity aware faces to increase the data-samples for better training. Xie et al.  [53]  came up with the AU intensity controller GAN to produce synthetic data for resolving the problem of limited and biased data-samples.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "6) Nas Based Mer:",
      "text": "The robust deep learning models are designed manually based on trial-and-error engineering and need expert knowledge, further this is very time consuming and requires high level domain expertise in designing CNN networks. Very first, Verma et al.  [58]  focused on these factors and came up with a conclusion that instead of spending time and effort designing the best possible CNN, in the hope of improved performance, it is prudent to design algorithms to search for the best CNN model for MER. Discussion: NAS algorithms are able to search and automatically design the best optimal CNN model with minimum human intervention. Initially, NAS based algorithms  [72] ,  [73]  required huge computation and took many days to train a task specific CNN model. However, recent NAS based approaches  [74]  focus on the faster searching and training architectures. Specifically, in MEs, NAS based algorithms need extra efforts to design MEs feature adaptive inner as well as outer architecture search. The MER field is still far from automatic model designing and has a lot of scope to develop better and faster NAS based MER algorithms. Also, selecting robust operation and number of cells for MER application is one of the prominent steps for the performance improvements in MER.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "C. Transfer Learning Based Networks",
      "text": "All available datasets for MER are relatively smaller as compared to other computer vision tasks. However, it is a well-known fact that the direct training of deep networks from scratch over smaller datasets is prone to overfitting. To mitigate the effect of overfitting, many studies have taken advantage of pre-trained weights (fine-tuning) of well-known models: AlexNet  [75] , VGG  [76] , ResNet  [77] , etc., which are trained over large-scale datasets such as ImageNet, FaceNet, etc. However, Patel et al.  [41]  explore that all pre-trained weights are not fitting well to discriminate against the MEs due to the low intensity of facial movements. Although, features of MaEs and MEs share some feature similarities in facial texture and muscle movements. Therefore, to exploit the capability of pre-trained features by considering domain adaption, Patel et al.  [41]  retrained ImageNet weights for macro datasets: CK+ and SPOS to train the model for expressive features. Next, resultant features are fine-tuned over micro expression datasets. On the same hypothesis, in MEGC-18  [78]  and MEGC-19 challenge  [79] , researchers  [42] ,  [45] ,  [50]  utilized the pre-trained weights of ResNet and its variants  [77] , trained over ImageNet and further to learn the expression specific features model retrained over MaE datasets: CK+, OULU, JAFFE, and MUG. Further, by adopting the domain features of the expressions from MaEs the CNN models are fine-tuned for ME tasks. The pertained weights of ResNet-18 have been widely used in the literature  [42] ,  [45] ,  [54]  due to their feature learning capability. Further, Wang et al.  [65]  designed a CNN network: transferring long-term convolutional neural networks (TLCNN) by combining 2DCNN and LSTM. First, TLCNN is trained over MaE large-scale datasets: KDEF, MMI, and TFID to ac-quire the knowledge of emotion-specific features. Afterward, TLCNN weights are used to retrain the model for MER. Khor et al.  [36]  fine-tuned the VGG-16 with VGGface weights to capture the enriched spatial feature of MEs. Moreover, Yang et al.  [38]  introduced a MERTA network by combining VGG-16 and LSTM to exploit the spatio-temporal features of ME sequences and their respective optical flow and -strain. Li et al.  [11]  have benefited from pre-trained weights VGG Face to guide the deep CNN network for smaller MEs datasets. Besides fine-tuning and domain adaption, knowledge distillation is another effective transfer learning approach. To provide a compact solution for training data and computation, Sun et al.  [33]  proposed knowledge distillation to transfer knowledge of AUs to MER through teacher-student CNN learning. The main aim of the framework is to guide the shallow student network by transferring the knowledge of features from a pre-trained deep teacher network. Discussion: The pre-trained weights or transfer learning is a sure-fire concept to solve the problem of overfitting and speed up the learning process with smaller sized datasets. The welltrained models: VGG-16, ResNet, GoogleNet etc. benefited the MER approaches to reduce the problem of overfitting upto some extent. However, most of these models are trained over ImageNet dataset, which has contrast data samples related to MEs with low muscle intensity, subtle and rapid changes. Therefore, pre-trained weights of ImageNet are not suitable for MER. Whereas pre-trained weights over face images or macro datasets are more advisable as these datasets hold analogous features in terms of facial structure and shape. Moreover, pretrained weights can be utilized in both multi-stage and endto-end MER networks. More detailed technical characteristics are discussed in section II-D.\n\n1) End-to-end vs multi-stage frameworks: In a two-stage network, a handcrafted feature descriptor such as dynamic imaging  [29] , affective motion imaging  [30] , optical flows  [39] ,  [46] ,  [60]  etc., is used in the first stage to capture the primary features. While, in the second stage, the CNN network is used to learn stage-1 features. For example, Verma et al.  [29] ,  [30]  utilized the dynamic and affective motion imaging to capture the spatiotemporal features into a single instance and then CNN network is designed to learn the MEs features (see",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "D. Discussion On Technical Characteristics Of Dl Based Mer",
      "text": "This section represents the deep insights of technical characteristics in various decisive aspects such as down sampling, multi-scale/-stream CNNs, shallow v/s deeper CNNs, and effects of kernel sizes in convolutional layers of the deep learning-based MER approaches.\n\n1) Impact of downsampling with convolution and pooling: Down sampling plays a significant role to reduce the number of parameters and ensures higher computational speed in CNN framework. In the case of MER, majorly, max pooling and convolution with stride operations were used for dimensionality reduction. The max-pooling layer captures the highlevel edge information by applying max operation. However, max pooling operations tend to focus on high level stack information by ignoring minors, which play a key role in the recognition of micro level features. However, the convolution adds to the inter-feature dependencies and reduces the dimension by parameter learning among channels rather than fixing it. The impact of convolution with strides over Fig.  3 : The performance analysis with pooling and conv layer with stride 2 in (a) quantitative  [58]  and (b) qualitative  [29] , manner.\n\nmax pooling operations on MER datasets are shown in Fig.  3 . The Fig.  3a  represents that the convolution with stride gains significant improvement in accuracy as compared to pooling over CASME-I, CASME-II, SMIC, CAS(ME) 2 and SAMM datasets. Moreover, Fig.  3b  depicts the visual effects of the max pooling over convolution with stride. From Fig.  3b , it is clearly visible that the response map of pooling loses more information as compared to convolution. The more details about the technical differences with qualitative and quantitative measures are discussed in  [29] ,  [30] ,  [80] . From Fig.  3  and  [29] ,  [30] ,  [80]  it is clear that convolution with stride outperforms the max pooling operation in MER.\n\n2) Impact of Multi-Scale/-Stream and Sequential CNN Frameworks on MER: Multi-scale/stream feature representations have been successfully used in MER and achieve good performance in the literature  [12] ,  [33] ,  [38] . From the literature  [12] ,  [33] ,  [38] , it is evident that linearly coupled conv layers with identical filter size (VGG-16, VGG-19, ResNet) have failed to capture homogeneous scaled receptive fields and avoid fine-tuned edge variations. However, multiscale/stream CNN models could capture detailed features from small to extensive regions, by applying multi-conv layers with different scale filters. The qualitative comparison between single-scale/stream and multi-scale/stream conv layers are depicted in Fig.  4 . From Fig.  4a , it is quite clear that CNN based models built on single branch linearly connected conv layers, lack in gathering adequate features of facial appearance due to repetitive cross-correlation operation. Whereas, Fig.  4b  represents the capability of multi scale/stream conv layers in learning of significant discriminable features from the expressive regions of the MEs. Moreover, the quantitative results for models  [11] ,  [29] ,  [31] ,  [33]  on different datasets: CASME-I, CASME-II, CAS(ME) 2 , SAMM and SMIC, are represented in Fig.  5 . The quantitative results have also proven a higher generalization capabilities of the mutli-scale/stream over single-scale/linear CNN frameworks. Based on both qualitative and quantitative results analysis, we can conclude that multi-scale/stream CNNs acquired more MEs features and outperformed the single stream/linear CNN frameworks. The existing multiscale/ stream models are detailed in Table  II  and corresponding results for different datasets over single-domain leave one subject out, composite-domain leave one subject out and cross-domain/dataset validation protocols are tabulated in Table  III -VI.\n\n3) Does deeper network affect the performance of the MER?: Yes, in general, the CNN model requires a large amount of data samples for efficient training. However, publicly available datasets for MER consist of limited data samples and tend to cause over-fitting. Moreover, deep/dense networks like AlexNet, VGG-11, VGG-16, SqueezeNet, GoogleNet and ResNet-18, etc also failed to capture minute features but were liable in emotion classification. Deep/dense networks  [39] ,  [84]  may vanish the micro-level features of the expressive regions due to progressive convolution and pooling operation. Therefore, most state-of-the-art MER approaches  [34] ,  [35] ,  [39] ,  [48]  adopt shallow and light weighted CNN models for MER. The quantitative results of literature study are analyzed in Fig.  6 . In Fig.  6 , we can observe that, deep/dense networks: AlexNet, VGG-11, VGG-16, SqueezeNet, GoogleNet and ResNet-18 have failed to achieve superior performance as compared to shallower networks: OffApex, STSTNet, CLF, MTC. Based on results, we can conclude that shallow networks are preferable and achieve high performance in terms of accuracy as well as F1-score as compared to deeper networks. Moreover, deeper networks are computationally expensive as compared to shallow networks.\n\n4) Does kernel sizes have any impact on CNN layers?: Yes, the kernel sizes also come under important paradigms of the CNN model designing. Kernel sizes directly affect the performance of the model along with computation cost. The comparative analysis between different sized kernels on datasets: CASME-I and CASME-II for MER frameworks  [30] ,  [31]  are demonstrated in Fig.  7 . Moreover, the qualitative effect of various kernel sizes is depicted in Fig.  4 . Based on literature study  [31] , it is clear that kernel sizes 3×3 and 5×5 are more capable to define the MEs features and achieve higher performance in MER. From the observations the smaller kernel sizes are preferable for MER applications. Kernels with large scales (7×7 and 11×11) have a larger receptive field per layer and allow the extraction of generic features spread across the image. Therefore, these filters focus on abstract transitional information and skip the minute information, which is quite important in MER.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Iii. Training And Evaluation Strategies",
      "text": "The performance of any framework is affected by two factors: 1) the model uncertainty in architecture design 2) the evaluation strategy, i. e. the strictness in the data division to validate the generalization strength of the framework. The technical aspects of designing paradigms are already discussed in the previous section (learning based MER models and studying technical characteristics). In this section, first we focus on the datasets and evaluation metrics (section III-A). Further, we discussed the validation strategies available in the literature to validate the robustness of the MER frameworks (section III-C). Moreover, we have studied the different The recognition rate analysis of multi-scale/stream (LearNet  [29] , OrigiNet  [31] ,\n\nLGConv  [11]  and TSCNN  [33] ) and single-scale/ linear (AlexNet  [75] , VGG-16  [76] , VGG-19  [76] , ResNet  [77]  and Mob-Net  [81] ) CNN frameworks on five datasets (a) CASME-I, (b) CASME-II, CAS(ME) 2 , SAMM and SMIC. Here, Proposed implies for the proposed approach in that particular publication (X coordinate).\n\nparadigms of experimental strategies in detail and observations are provided in section III-D.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "A. Datasets And Evaluation Metrics",
      "text": "The current research needs across all the computer vision applications are motivated by the success of deep learning algorithms. The success of the deep learning algorithms highly depends on the availability of sufficient training data with variations of the populations and environments as much as possible. The higher the diversity in the present training data, the more robustly one can estimate the model parameters. In this section, we primarily discuss the publicly available MEs datasets (highlighted by purple color in the Fig.  1 ) that have been used for evaluating the MER methods. In the literature, ME datasets can be broadly classified into a dataset in-lab environment and a dataset in-wild.\n\n1) Traditional Datasets (Lab environment): The six traditional datasets: CASME-I, CASME-II, CAS(ME) 2 , SMIC, SAMM and MMEW have been widely used in the literature Fig.  6 : The performance analysis of various deep  [75] -  [77] ,  [82] ,  [83]  and shallow  [34] ,  [35] ,  [39] ,  [48]    video samples annotated with 7 emotion classes: Happiness (24), Surprise  (13) , Anger  (20) , Disgust (8), Sadness (3), Fear  (7) , and Others  (84) . Recently, a micro-and-macro expression warehouse (MMEW)  [9]  dataset with the largest pool of MEs was introduced. The MMEW dataset contains 300 macro and micro image sequences of 36 participants. The MMEW dataset is annotated with FACS and 7 emotion classes: Happiness (36), Anger (8), Surprise (89), Disgust (72), Fear  (16) , Sadness (13), and Others (102).\n\n2) Wild Dataset: All traditional datasets samples are elicited in a lab-controlled environment and lack the detail and divinity of real-life challenges. The MEVIEW dataset  [85]  is the first MEs dataset that incorporated wild environment challenges: occlusion, illumination variations, candid faces etc. The video samples are collected from the website. Specifically, all the samples are downloaded from YouTube videos of poker games. The database contains 31 samples from 16 subjects, having both macro and micro expressions. More detailed analysis of datasets can be found in  [85] .",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "B. Evaluation Metrics And Significance",
      "text": "The widely used evaluation metrics for MER are recognition accuracy (Acc.), weighted f1-score (F1), weighted average recall (WAR), Un-weighted f1-score (UF1), Unweighted average recall (UAR), and mean diagonal value of the confusion matrix. The accuracy is calculated by computing average hit rate across the all emotion class samples. Let T P , T N , F P and F N be the true positive, true negative, false positive Fig.  9 : The evolution in validation strategies in past years. and false negative, respectively. The recognition accuracy is computed by using Eq.  [1] .",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Recognition Accuracy =",
      "text": "T P + T N T P + T N + F P + F N\n\n(1)\n\nThe accuracy can be largely contributed by a large number of true negatives and not focus on false negative and false positive. Thus, Acc. is liable to bias data and reflect partial effectiveness of the MER frameworks. Whereas, F1 score is a better measure to balance between T P , T N , F P and F N . The F1-score is calculated by using Eq.  [2] .\n\nWhere, T and C represents the total number of samples and emotion classes, respectively. F1 Score is considered as a better measure because of its balancing nature with uneven class distribution (large number of actual negatives).\n\nThough MER datasets have heavy imbalanced annotations, both Acc and F1 score failed to justify the efficacy of the MER Models. Recently, UAR and UF1 have drawn much attention due to their unbiased nature of evaluation. Both UAR and UF1 computed the performance of a model w.r.t number of classes without consideration of samples per class. The UAR and UF1 are calculated by using Eq.  [3]  and Eq.  [4] , respectively.\n\nMoreover, some of the methods  [42]  utilized the WAR for evaluation. The WAR is computed by using Eq.  [5] .\n\nSome of the MER frameworks  [11] ,  [43] ,  [48] ,  [56]  also adopt the mean diagonal value of the confusion matrix to show the detailed generalization of the model.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "C. Validation Strategies/Protocols",
      "text": "In literature, many state-of-that-art techniques adopted diverse data-division strategies to prove the robustness of the algorithm. Therefore, the supervised techniques we broadly   II -Table  VI . More details of the data-division strategies are discussed as follows.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "1) Person Dependent Evaluation (Pde):",
      "text": "Based on literature study, PDE setup can be further divided into two categories 1) k-fold cross validation and 2) leave one video out (LOVO). In k-fold cross validation data is randomly divided into a ratio of 100-P:P. Where, in each iteration P% of data is used for inference to validate the performance of the model and remaining data samples are reserved for the training. Some studies  [29] ,  [47]  follow the PDE setup with 80:20 ratio. Recently, Thuseethan et al.  [57]  used a 90:10 ratio with 10-fold cross-validation to validate the effectiveness of the framework. In LOVO, one expression video of a person is used for inference and remaining all data samples are used for training. Thus, there are immense chances to use the same person's expression in both training and testing data. The articles  [10] ,  [21] ,  [23]  in literature adopted LOVO setup to prove the robustness of their models.\n\n2) Person Independent Evaluation (PIE): The PIE setup follows a strict division for training and testing sets. It ensures evaluation over videos with unseen subject's identities. Like PDE, the PIE setup can also be further categorized in singledomain leave-one-subject-out (SD-LOSO) and compositedomain leave-one-subject-out (CD-LOSO). In SD-LOSO, a single dataset is used, and samples are partitioned in such a way that all expressions of a particular subject at a particular iteration act as a testing set, and the remaining data is considered as the training set. The SD-LOSO is the most widely used validation strategy in the literature  [10] ,  [13] ,  [48] ,  [52] ,  [53] . The list of models with SD-LOSO is tabulated in Table  II . The CD-LOSO is one of the recently emerging validation setups to validate the robustness of the model with domain shifts. First MEGC 2018  [78]  introduced the CD-LOSO setup for the MEs challenge. The CD-LOSO data division was introduced by combining CASME-II, SAMM, and SMIC with three emotion classes: positive, negative, and surprise. Further, the composite dataset is used by adopting an LOSO set up to evaluate the robustness and generalization of the model for MER with domain shifts. Based on literature study  [10] ,  [12] ,  [36] ,  [42] ,  [45] ,  [48] ,  [54]  CD-LOSO is a highly recommended validation strategy as it ensures evaluation on unseen faces with ethnicity variations. It is observed from the literature, recent works on MER adopted the CD-LOSO along with SD-LOSO to validate the performance of the MER frameworks as shown in Fig.  9 .\n\n3) Cross Domain Evaluation (CDE): CDE is another setup that ensures PIE by training a model over a particular dataset and testing on a different dataset. The MEGC-2018  [78]  used the CDE setup to evaluate the efficacy of the submitted MER frameworks. Peng et al.  [42]  and Khor et al.  [36]  have successfully proven the performance of the models on the CDE setup of the MEGC-18 challenge. Wang et al.  [50]  performed two cross-domain experiments: CASME-II dataset is used for training and testing results are evaluated on SAMM dataset and vice versa. Choi et al.  [56]  have utilized the pair of CASME-II and SMIC dataset for CDE. First, CASME-II is considered for training, while SMIC is used for inference. Further, SMIC is used for training, and CASME-II is reserved for testing. Moreover, Verma et al.  [30]  have conducted nine experiments over four datasets: CASME-I, CASME-II, CAS(ME) 2 , and SAMM. Three experiments were conducted by using CASME-I as a training dataset and CASME-II, CAS(ME) 2 , SAMM as testing datasets, individually. Another three experiments were evaluated by using CASME-II as a training dataset and CASME-I, CAS(ME) 2 , SAMM as testing datasets, individually. Similarly, the other three experiments were performed by using SAMM as a training dataset and the remaining three as testing datasets. Based on the literature, CDE setups are very less popular but have the most recommendable data validation strategies to gain much generalization capabilities of the FER models.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "D. Discussion On Experimental Setups And Validation Strategies",
      "text": "In literature, we noticed that the standard evaluation protocols for MER techniques are not available as many authors follow contrast experimental setups to prove the robustness of the MER techniques. The term contrast is defined in terms of number of samples used for training and testing, input selection strategies, number of expression classes adopted in training or dropped some of emotion classes due to a smaller number of images  [10] ,  [30] , type of validation strategy adopted to prove the robustness, etc. Based on the above contrast settings, it is harder to compare the performance of these techniques directly. Therefore, this section aims to provide a detailed study on experimental setups adopted in the literature. Also, analyzed the effect of adopting different evaluation setups like, PDE, PIE and CDE, data augmentation, input selection, number of emotion classes etc, over the performance of the MER approaches. 1) Impact of PDE, PIE and CDE on MER performance: In person dependent evaluation, the training and testing set contains frames from the same category. Thus, there is a possibility of similar samples of the same subject present on both training and testing data sets, which leads to inflated performance during testing. However, it may fail in real world scenarios. Thus, there is a need to evaluate the model performance over unseen or person-independent scenarios. This also makes the process of model design much more challenging to ensure robust performance even in real world scenarios. Therefore, the PIE ensures stable performance over PDE on unseen data.\n\nIn literature many existing works  [10] ,  [12] ,  [36] ,  [42] ,  [45] ,  [48] ,  [54] ,  [58]  have opted the PIE setup's evaluation (More detailed categorization is indexed in Table  II ). Further, to study the effect of PDE as compared to PIE setup, we accumulated some results for PDE (including both 10-folds and LOVO) and PIE (SD-LOSO) setups as shown in Fig.  10 . More specifically, Fig.  10a  illustrated the results analysis for the models  [57] , evaluated over 10-fold and SD-LOSO validation strategies. Whereas, Fig.  10b  shows the comparative results for model  [44]  evaluated over LOVO and SD-LOSO validation strategies. Based on Fig.  10 , it is evident that the models that adopted PDE setups outperform in accuracy over the PDE setup. Thereby, the PDE results are unreliable to validate the actual robustness of the deep learning models. Similarly, the model's performance depends on a person's identity, the cross culture and ethnicity variations.\n\nTherefore, recent works  [15] ,  [35] ,  [39] ,  [60]  also focused on the CD-LOSO validation strategy as it ensures person independence with sparse diversity in domains. However, it is hard to achieve impressive performance with the strictness of CD-LOSO. As we can see in Fig.  9 , the best accuracy results for CD-LOSO are 76.9%, which has enough margin as compared to the results of PDE and SD-LOSO. Furthermore, some work  [30] ,  [36] ,  [42] ,  [50]  also adopted the CDE validation strategy to evaluate the models with more challenging scenarios such as cross-ethnicities, out-group, illumination, and resolution variations. Therefore, the CDE validation protocol evaluates the model's robustness more towards real-world scenarios. From Fig.  9 , the best accuracy over CDE setup is 62.8%, which is very less as compared to PDE as well as PIE validation setups. Therefore, benchmarking the performances of different CNN, GAN models in a standard evaluation setup (PDE/PIE/CDE) is an important scope in micro expression research. 2) Impact of Data Augmentation on MER performance: Since MEs datasets consist of a limited number of samples and imbalanced classes, which leads to model overfit. Many existing MER approaches  [11] ,  [13] ,  [30] ,  [31] ,  [40] ,  [42] ,  [48] ,  [55] ,  [57] ,  [65] ,  [86]  adopt the data augmentation techniques to create a sufficient pool of data samples for training. 2D-CNN based approaches  [30] ,  [31] ,  [40] ,  [42] ,  [48] ,  [55] ,  [57] ,  [65] ,  [86]  performed the basic operations like flipping, rotating, color shift, smoothing to increase the data samples. Xia et al.  [13]  introduced the two new augmentation techniques with temporal connectivity. While the recent work  [15] ,  [53]  utilized the GAN based model to generate the synthetic MEs data for data augmentation. In literature  [15] ,  [53] , the patchGAN network is used to generate synthetic images/samples using the apex frame. The quality of the synthetic (fake/augmented) images are validated by using i.e., adversarial loss, consistency loss, attention loss, AU intensity loss, SSIM loss, ME loss and sequence authenticity loss. Similarly,  [53]  utilizes a 3DCon-vNet architecture to distinguish the synthetic sequences from the real ones. Li et al.  [11]  collect the five nearest frames to the apex frame to enhance the size of datasets. From literature  [11] ,  [13] ,  [30] ,  [31] ,  [40] ,  [42] ,  [48] ,  [53] ,  [55] ,  [57] ,  [65] ,  [86] , we observed that data-augmentation resolves the issue of overfitting up to some extent and improves the performance of the model as shown in Fig.  11  From the analysis of the results, it is clear that the models trained over augmented datasets achieved higher accuracy as compared to models without augmented datasets. However, some of the MER approaches  [32] ,  [34] ,  [36] ,  [39] ,  [46]  significantly improves the models performance without augmentation by introducing optical flow and shallow network designing. Therefore, it is observed that there is scope to design robust models without increasing the sample size.\n\n3) Impact of Input Selection Strategies on MER performance: Most of the existing MER approaches follow three types of input formats: apex frame  [39] ,  [48] ,  [55] , onsetapex-offset frames  [15] ,  [52]  and whole videos  [46] ,  [47]  as shown in Fig.  2 . Majority of the existing MER approaches  [11] ,  [39] ,  [45]  rely only on the apex frame for the analysis. However, some studies emphasize the importance of dynamic aspects for detecting the subtle changes  [64]  and its effect on the performance of MER. In a MEs video, each frame has its own significance towards the identification of the emotion class. Therefore, apex frame-based approaches are lacking to analyze the motion information, which has its own potential to describe the MEs classes. Therefore, whole video input is more effective and reliable in MER. Nevertheless, the whole video input is incredibly challenging to handle and achieves less results as compared to apex frames input as shown in Fig.  12 . In Fig.  12 , we included top three accuracy results achieved by different models over CASME-II and SAMM Fig.  12:  The recognition rate with two input selection strategies on two datasets: CASME-II and SAMM. Here, best three (top)  [33] -  [35] ,  [54] ,  [56] -  [58] ,  [61]  accuracy results are selected for the comparative analysis.\n\ndatasets with apex and whole video inputs, respectively. From Fig.  12 , it is quite clear that models with whole video input formats acquire less accuracy results as compared to apex frames but have more reliability to the capability of delivering appearance information along with time variants. Therefore, utilizing complete frames in a video is more effective and reliable in MER than using a single apex frame. To analyze the impact of emotion classes over the performance of a model, we analysed the existing MER approaches and results are highlighted in Fig.  13 . From Fig.  13 , it is clear that the performance of the models is improved by reducing the number of emotion classes. More specifically, models for 3-emotion classes gained the highest recognition accuracy and models for 4-emotion classes attained second highest and so on. Based on the observations and results of existing MER frameworks, we can conclude that a greater number of classes (7/8 emotion classes) create more confusion for models to generalize the emotion classes in true positives. However, in real life scenarios humans can exhibit a wide range of facial expressions. Thus, using a reduced set of emotions to carry on the experiments is a bit misleading. The fact that existing datasets lack enough labels for some emotions is a challenge that the research on the topic owes to face. Thereby, there is enough scope to provide a persuasive solution to handle such emotion classes instead of simply merging or dropping.",
      "page_start": 13,
      "page_end": 15
    },
    {
      "section_name": "Iv. Research Needs And Future Directions",
      "text": "This section describes the research needs and future directions of the MER approaches. More details of the critical issues in the literature which are unresolved or get least attention in the MER approaches are discussed. Therefore, there is scope for the upcoming researchers to design robust solutions for MER applications.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "A. Unbiased Learning",
      "text": "The publicly available ME's datasets have imbalanced sample sizes in the emotion classes. The imbalancing nature of the dataset leads the CNN models bias towards a dominating class. Thus, there is an immense need to develop an unbiased learning algorithm to enhance the generalization ability of the model. To handle the imbalancing issue in the emotion classes, some of the existing MER approaches  [46] ,  [65]  dropped the emotion classes, which contain very few samples. While some other approaches  [44] ,  [53]  have created new emotion classes by merging the existing emotions as positive, negative, surprise, and other. However, combined emotion classes are still imbalanced. Therefore, Xia et al.  [13]  took a step forward to resolve the imbalanced data samples problem and proposed temporal augmentation to create a balanced dataset. Furthermore, Xie et al.  [53]  proposed a GAN based model to generate the synthetic data samples to alleviate the problem of unbalancing and trained a model with unbiased learning. From the above discussion it is evident that there is a need to develop a balanced dataset and robust deep learning technique to handle imbancing emotion class problems for MER.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "B. Cross-Cultured Me Dataset",
      "text": "Most of the available ME datasets are developed in-lab environments with single cultured subjects. According to Xie et al.  [87] , the study on single cultured samples lead to ingroup downside problems. Thus, the model trained over single targeted ethnic participants may be biased and lead to underperform on cross-cultured MEs. Therefore, there is a need to collect cross-cultured data samples of MEs. The cross-cultured ME datasets will ensure the fair analysis of MER frameworks and it is a great addition to the affective computing research community.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "C. Group Emotion Recognition",
      "text": "Identifying the common emotion, shared among a group of people is known as group-level emotion recognition (GER). The GER plays a significant role in a wide variety of applications such as security, surveillance, early event prediction, image retrieval, and social era. Much study on GER using macro expression recognition is available in the literature  [89] ,  [90] . However, due to the challenges of MEs there is no group emotion dataset available for MER. Thus, there is scope to develop balanced and cross-cultured group emotion micro expression dataset.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "D. Motion Magnification In Mer",
      "text": "MEs are involuntary which cannot be captured in normal sight as they usually occur only for the minute interval. Therefore, it is hard to train a system to detect these variations and identify the relevant emotion class in MEs video sequences. Fig.  13:  The accuracy performance on four different emotion classes of CASME-II dataset for various existing MER frameworks: STRCNN  [44] , ELRCN  [36] , CapsuleNet  [48] , MER-GCN  [70] , AU-GACNN  [53] , IM-CNN  [88] , CNN-LSTM  [40] , STRCN-A  [44] , STRCN-G  [44] , and FR  [61] . Here, annotated results for STRCNN, ELRCN, CapsuleNet, MER-GCN, AU-GACNN are directly taken from the AU-GACNN  [53]  published results. While, results of IM-CNN, CNN-LSTM, STRCN-A, STRCN-G, FR are grab from the FR  [61] .\n\nIn literature, Eulerian Video Magnification (EVM)  [91]  and learning-based MM  [92]  are adopted for magnifying the MEs. However, both approaches are not specifically designed for the MEs and sometimes destroy the emotional features. Therefore, there is a need to magnify the micro variations of emotion, developing application dependent motion magnification algorithms could improve the performance of the model.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "E. Multi-Modal In Mer",
      "text": "Ongoing MER research is not achieving enough performance to use in real-time applications due to the low intensity and subtle nature. Moreover, the available dataset samples are not enough to train the MER frameworks. Therefore, to enhance the ability to recognize micro variations, in recent times multi-modal algorithms  [93] ,  [94]  gain the attention of researchers by supplying sufficient information to enhance the model performance. Thus, there is a need to develop an application specific (MER) dataset with multi-modalities like body gestures, eye gaze, Electrocardiogram (ECG), electroencephalogram (EEG), etc. Also, there is an immense need to design a robust algorithm to handle these multi modalities.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "F. Mer In-Wild",
      "text": "In real world applications, to analyze the emotional state of a person, models designing in the lab environment may fail as the movement of the subject is dynamic, viewing angle of the camera is not static, illumination and lighting conditions are dynamic in nature. Due to the challenging nature of micro expressions, only one data set in-wild for MER  [85]  is available in the literature. Also, extremely limited articles on MER in the wild are available in the literature. Therefore, there is a lot of scope to develop a robust algorithm to handle the challenges of the in-wild and scope to develop cross-cultured dataset in the wild for MER.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "G. Mer In Psychological Disorders",
      "text": "Psychological disorders such as autism spectrum, bipolar, anxiety and stress related disorders affect a person's thoughts, behavior, feelings and sense of well-being. In these disorders people have a state of low mood and aversion to activity. In such a low mood and lack of interest, the facial expression appears different from the ones in normal states. Some research  [95] ,  [96]  have been made for psychological disorders through facial expression recognition in past years. However, incredible challenges in dataset accumulation due to privacy of neurotic subjects, limited labeled datasets and lack of deep learning based solutions make it an appealing research area to be explored. The available datasets are not focusing on the privacy of neurotic subjects which is a very crucial aspect in social lives. The future work requires more attention towards both data collection and algorithm development for psychological disorders analysis through FER/MER.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "H. Mer In Entertainment",
      "text": "Online games have gained a lot of popularity due to the features of collaboration, communication, and interaction. However, the virtual world of gaming is still primitive and far from real-world communication. For example, players still communicate through text chats, avatars have no activities related to natural body gestures, facial expressions, and so forth. Therefore, there is a need for a robust automatic expression system that can be integrated into the gaming system to control the facial expressions of avatars and enhance the interest of players by providing a virtual interface near the real world. Moreover, facial expression analysis can be integrated into video-controlled devices for entertainment (music, movies, games, YouTube, etc.) and the dynamic balancing system will automatically adjust the entertainment level based on the user's facial expressions.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "I. Mer In Education System",
      "text": "In recent COVID pandemic years, the standard offline education systems transferred to the online education system. Effective online education is the primary need to maintain the education gap of each age group of students. The online education system allows students from anywhere to access the classes as well as experimental work from a distance via the Internet. However, this is way far away from physical education. In such critical situations students as well as tutors face many challenges like lack of attention, high chances of distraction etc. Resultant performance of the students degrades and it increases the chances of many mental problems such as stress, anxiety and depression. Thus, there is a lot of scope to develop a robust online education system by integration of facial expression analysis to scan the expressive features of the students. This will allow tutors to survey the attentiveness of the students in class and help them accordingly.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "V. Conclusion",
      "text": "This paper presents deep insights of learning-based MER frameworks with a perspective on promises in model designing, experiment strategies, challenges, and research needs. Particularly, the existing learning-based MER frameworks are analyzed in terms of model design and evaluation frameworks. The variety of existing deep learning architectures are examined and their effect on MER performances are discussed. The important paradigms in model designing and evaluation for MER are presented. Also, the impact of data division strategies like PDE, PIE and CDE on the model performance and the limitations of these strategies are discussed. The challenges in designing robust MER models and the current research needs are discussed. Further, the available datasets, challenges and the evaluation metrics utilized to test the efficacy of MER frameworks are discussed. In addition, presented useful insights of future needs and research guidance to carry forward the research in MER.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Deep Insights Of Learning Based Mer Frameworks:",
      "text": "A Perspective on Promises, Challenges and Research Needs (Supplementary Document) Monu Verma, Santosh Kumar Vipparthi, and Girdhari Singh,",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "I. Introduction",
      "text": "This document first summarizes the computational complexity requirement of existing DL-based MER approaches in terms of total number of parameters and execution time in Table  I . Further, Table  II  and Fig.  1  represents the data samples of the all MER datasets and statistical details of them, respectively. Further, the traditional handcrafted MER approaches are summarized and indexed in Table  III . The documents detailed the differences between macro and micro expression. The document represents the different types of deep learning based MER approaches. The 2D-CNN based architecture are depicted in Fig.  2a  and 2c . Moreover, these architectures also represents three different input formats: dynamic imaging, landmark points, affective motion imaging, applied to process the spatio-temporal data through twodimensional convolutions networks. Similarly Fig.  2b  and Fig.  2c , demonstrated the multi-stream and -scale based CNN architectures, respectively. The Fig.  3a  and 3b  represents the examples of capsule module and recurrent convolutional network (RCN) based multi-stage MER frameworks. Moreover, 3D-CNN and graph based MER frameworks are shown in the Fig.  4a  and 4b . The recent evolving techniques: GAN, Student-Teacher and NAS for MER are demonstrated in Fig.  5 , 6a and 6b, respectively. Moreover, we discuss the differences between existing surveys and the proposed surveys in detail. More detailed comparison with proposed survey and recent survey [M:47] is tabulated in Table  IV .    However, the proposed survey highlights the comparative study of existing deep learning MER frameworks and their characteristics:\n\nProposed 1) Section 1: Starting with the introduction, steps involved in any MER approaches like pre-processing, feature extraction, and classification have been discussed in brief. Further, the differences in macro and micro-expressions in terms of muscles nature, durations, challenges in data collections, challenges in deep learning models' designing are highlighted. Also, how the proposed survey is different from the existing surveys for MER is discussed in the introduction.     3 4 5 6 7 ) and studied all technical aspects (two-stage vs end-to-end frameworks, the impact of downsampling with convolution and pooling, the impact of multi-scale/-stream and single-scale/linear CNN frameworks, does deeper network affect the performance of the MER?, does kernel sizes have any impact on CNN layers?) of the deep learning approaches and their effects on the performance of the MER, which will help the new researchers to design and develop robust MER frameworks.\n\n3) Section 3: represents the training and evaluation strategy. Initially, based on environmental challenges: lab and in-wild environments, MEs datasets are discussed. Further, the performance evaluation metrics and their significance in MER have been reported. Moreover, the various validation strategies: LOSO, LOVO, Composite, Cross-Domain, random split are discussed by broadly categorizing into three categories: PDE, PIE, and CDE. Finally, the effects of experimental settings: data augmentation, validation strategies, input selection, and the number of expression classes, over the performance of MER, are studied. 4) In Section 4: some critical issues in MER also point out which require the attention of researchers to improve the efficacy of the MER approaches. Moreover, future directions such as Group Emotion Recognition, Motion Magnification in MER, Multi-modal in MER, MER inwild, MER in Psychological Disorders, are presented. 5) In Section 5-6: future directions: are discussed\n\nThe key difference between TPAMI and our proposed survey is detailed out in Supplementary document-Table  IV .",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Table Iv:",
      "text": "The key difference between TPAMI [M:42] and our proposed survey.\n\nProposed TPAMI 1. The detailed survey of deep learning MER approaches has been done by categorizing them into four major categories: multi-stage, end-to-end, transfer learning and GANs based. These categories are further divided into sub-categories such as 2D-CNN, multi-stream/scale, capsule, 3D-CNN, CNN-LSTM, Graph-based, etc., with benefits and limits of each model.\n\n1. Few deep learning MER approaches and transfer learning approaches are explained in this.\n\n2. The survey collected the performance majors of existing DL based MER approaches (Table  3 4 5 6 7 ) and studied all technical aspects (two-stage vs end-to-end frameworks, the impact of downsampling with convolution and pooling , impact of multi-scale/-stream and single-scale/linear CNN frameworks, does deeper network affect the performance of the MER?, does kernel sizes have any impact on CNN layers?) of the deep learning approaches and their effects on the performance of the MER, which will help the new researchers to design and develop robust MER frameworks.\n\n2. There is no discussion about the technical characteristics of the DL based MER approaches.\n\n3. The survey provides a detailed explanation of the evaluation metrics (accuracy, F1-score, Recall, UF1, UAR, confusion matrix) and validation strategies (PDE: LOVO, 80/20 split, PIE: LOSO, composite LOSO, CDE), which plays a significant role in MER approaches.\n\n3. There is no discussion about evaluation metrics and validation strategies.\n\n4. In literature, there are no standard protocols to evaluate the MER approaches. The MER approaches follow contrast experimental setup in terms of total validation protocols, number of samples, input selection, participants, expression classes, etc., or dropped some of the emotion classes due to a smaller number of images [M:13], [M:56]. Therefore, it is hard to compare these approaches directly due to both positive and negative impacts on the MER performance. Therefore, in this survey, we have done a detailed study based on experimental results of existing literature to analyses the effects of different evaluation setups: PDE, PIE, and CDE, and other experimental settings: data augmentation, input selection, number of emotion classes, etc., over the performance of the MER approaches. Thus, new researchers will get awareness about the selection of experimental settings in DLbased MER.\n\n4. There is no discussion about the effects of experimental settings and protocols in this survey.\n\n5. The survey is briefly discussed the MEs datasets by categorizing them into two parts: lab and in-wild environment.\n\n5. The survey is more towards MEs datasets, issues in existing datasets, and proposed a new data warehouse by incorporating macro and micro-expressions. 6. The survey detailed out the difference between macro and micro in terms of muscles nature, duration, challenges in data collections, challenges in deep learning models' designing.\n\n6. The survey detailed out the difference between macro and micro in terms of the nature of muscles, which provoked emotions, Neuropsychology procedure, and duration.",
      "page_start": 25,
      "page_end": 25
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Based on available",
      "page": 3
    },
    {
      "caption": "Figure 1: Milestones of MER, including datasets , traditional , recent deep learning methods, challenges and survey paper. Most",
      "page": 4
    },
    {
      "caption": "Figure 2: A ﬂow diagram of deep learning frameworks for",
      "page": 4
    },
    {
      "caption": "Figure 2: Further, a deep analysis of DL based MER frameworks has",
      "page": 4
    },
    {
      "caption": "Figure 2: ). However, the",
      "page": 4
    },
    {
      "caption": "Figure 2: a and 3b). Therefore, the performance",
      "page": 9
    },
    {
      "caption": "Figure 3: The performance analysis with pooling and conv layer",
      "page": 9
    },
    {
      "caption": "Figure 3: The Fig. 3a represents that the convolution with stride",
      "page": 9
    },
    {
      "caption": "Figure 3: b depicts the visual effects",
      "page": 9
    },
    {
      "caption": "Figure 3: b, it is clearly visible that the response map of pooling",
      "page": 9
    },
    {
      "caption": "Figure 3: and [29], [30], [80] it is clear that convolution with",
      "page": 9
    },
    {
      "caption": "Figure 4: The feature maps responses generated by employing",
      "page": 10
    },
    {
      "caption": "Figure 4: From Fig. 4a, it is quite clear that CNN",
      "page": 10
    },
    {
      "caption": "Figure 5: The quantitative results have also proven",
      "page": 10
    },
    {
      "caption": "Figure 6: In Fig. 6, we can",
      "page": 10
    },
    {
      "caption": "Figure 7: Moreover, the qualitative",
      "page": 10
    },
    {
      "caption": "Figure 4: . Based on",
      "page": 10
    },
    {
      "caption": "Figure 5: The recognition rate analysis of multi-scale/stream (LearNet [29], OrigiNet [31], LGConv [11] and TSCNN [33]) and",
      "page": 11
    },
    {
      "caption": "Figure 1: ) that have",
      "page": 11
    },
    {
      "caption": "Figure 6: The performance analysis of various deep [75]–[77],",
      "page": 11
    },
    {
      "caption": "Figure 7: The accuracy performance analysis for different kernel:",
      "page": 11
    },
    {
      "caption": "Figure 8: Differences between person dependent, person inde-",
      "page": 12
    },
    {
      "caption": "Figure 9: The evolution in validation strategies in past years.",
      "page": 12
    },
    {
      "caption": "Figure 10: The performance analysis of different MER frame-",
      "page": 13
    },
    {
      "caption": "Figure 8: Also, the impact of data-",
      "page": 13
    },
    {
      "caption": "Figure 9: 3) Cross Domain Evaluation (CDE): CDE is another setup",
      "page": 13
    },
    {
      "caption": "Figure 10: a illustrated the results analysis for the models [57],",
      "page": 14
    },
    {
      "caption": "Figure 10: b shows the comparative results for model",
      "page": 14
    },
    {
      "caption": "Figure 10: , it is evident that the models that adopted",
      "page": 14
    },
    {
      "caption": "Figure 9: , the best accuracy results for",
      "page": 14
    },
    {
      "caption": "Figure 9: , the best accuracy over CDE setup is 62.8%,",
      "page": 14
    },
    {
      "caption": "Figure 11: The performance of the (a) STRCN-A and STRCNG",
      "page": 14
    },
    {
      "caption": "Figure 11: (a) and Fig. 11(b). Fig. 11(a)",
      "page": 15
    },
    {
      "caption": "Figure 2: Majority of the existing MER approaches",
      "page": 15
    },
    {
      "caption": "Figure 12: In Fig. 12, we included top three accuracy results",
      "page": 15
    },
    {
      "caption": "Figure 12: The recognition rate with two input selection strategies",
      "page": 15
    },
    {
      "caption": "Figure 12: , it is quite clear that models with whole video input",
      "page": 15
    },
    {
      "caption": "Figure 13: From Fig. 13, it is clear",
      "page": 15
    },
    {
      "caption": "Figure 13: The accuracy performance on four different emotion",
      "page": 16
    },
    {
      "caption": "Figure 1: represents the data",
      "page": 20
    },
    {
      "caption": "Figure 2: a and 2c. Moreover, these",
      "page": 20
    },
    {
      "caption": "Figure 2: c, demonstrated the multi-stream and -scale based CNN",
      "page": 20
    },
    {
      "caption": "Figure 3: a and 3b represents the",
      "page": 20
    },
    {
      "caption": "Figure 4: a and 4b. The recent evolving techniques: GAN,",
      "page": 20
    },
    {
      "caption": "Figure 1: Data samples of all available MER datasets: a) SMIC, b) CASME-I, c) CASME-II, d) CAS(ME)2, e) SAMM, f)",
      "page": 21
    },
    {
      "caption": "Figure 2: The two-stage MER approaches based on (a) 2D-CNN",
      "page": 23
    },
    {
      "caption": "Figure 3: Multi-stage methods with (a) capsule module [M:83]",
      "page": 23
    },
    {
      "caption": "Figure 4: MER approaches (end-to-end) based on (a) 3D-CNN",
      "page": 23
    },
    {
      "caption": "Figure 5: Representing GAN based synthetic image generation",
      "page": 23
    },
    {
      "caption": "Figure 6: End-to-end methods (a) teacher-student based macro-",
      "page": 23
    },
    {
      "caption": "Figure 1: of the manuscript.",
      "page": 25
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Datasets SDL-MER MEGC-19\nHandcrafted Aouayeb et al. See et al.\nChallenges SpRCNN MEGC-18 MERTA Sp3DCNN DCB-DB MTMTNet LGConD\nLearning based Xia et al. Merghani et al. Yang et al. Reddy et al. Xia et al. Xia et al. Li et al.\nSurveys SDF ApexFace CNN-LSTM NMER DSSN TSCNN STRCNN AuGACN GraphTCN LARNet FR\nPatel et al. Li et al. Wang et al. Liu et al. Khor et al. Song et al. Xia et al. Xie et al. Lei et al. Hashmi et al. Zhou et al.\nSurvey-I ESC-SFR Ma-MiER ELRCN-TE STSTNet 3D-Flow CapsuleNet DMER-KD CLFM CEP 3DCNN STSTNet+GA\nYan et al. Kim et al. Peng et al. Khor et al. Liong et al. Wang et al. Quang et al. Sun et al. Choi et al. Thuseethan et al. Zhao et al. Liu et al.\nCASME-II\nYan et al.\nDualInc OFF-Apex MOSTFRL MicroAtt RCN ICE-GAN\nCASME TIC ColorSpaces MEVIEW CAS(ME)2 SAMM Zhou et al. Gan et al. Kim et al. Wang et al. Xia et al. Yu et al. MMEW\nYan et al. Wang et al. Wang et al. HUSAK et al. Qu et al. Davison et al. Ben et al. Survey-XI\nSurvey-IX Li et al.\nBen et al.\n013 2014 2015 2016 2017 FHOHO ME-Ap2e 0 x 1 8 Sparse MDMO 2019 20 R 20 eliefF 2021 2022\nL S i M et I a C l . LB G P- u T o O e P t & al N . N H S u T a L n B g P e - t I a P l . Happy et al. Liong et al. Liu et al. Zhang et al. G S u u e r r v a d e l e . y ll - i X e t\nLBP-SIP MDMO FDM H. STLBP-RIP DRLS TTRM RSTR\nWang et al. Liu et al. Xu et al. Zong et al. Zong et al. Zong et al. Zhang et al.\nSurvey-III\nSurvey-II Oh et al. Survey-V Survey-VI, Goh et al.\nTakalkar et Merghani et Survey-VII, Zhou et al.\nal. Survey-IV al. Survey-VIII, Xie et al.\nLi et al.": "",
          "Datasets\nHandcrafted\nChallenges\nLearning based\nSurveys": "",
          "SDL-MER MEGC-19\nAouayeb et al. See et al.\nSpRCNN MEGC-18 MERTA Sp3DCNN DCB-DB MTMTNet LGConD\nXia et al. Merghani et al. Yang et al. Reddy et al. Xia et al. Xia et al. Li et al.\nSDF ApexFace CNN-LSTM NMER DSSN TSCNN STRCNN AuGACN GraphTCN LARNet FR\nPatel et al. Li et al. Wang et al. Liu et al. Khor et al. Song et al. Xia et al. Xie et al. Lei et al. Hashmi et al. Zhou et al.\nESC-SFR Ma-MiER ELRCN-TE STSTNet 3D-Flow CapsuleNet DMER-KD CLFM CEP 3DCNN STSTNet+GA\nKim et al. Peng et al. Khor et al. Liong et al. Wang et al. Quang et al. Sun et al. Choi et al. Thuseethan et al. Zhao et al. Liu et al.\nDualInc OFF-Apex MOSTFRL MicroAtt RCN ICE-GAN": "",
          "Column_4": "DualInc OFF-Apex MOSTFRL MicroAtt RCN ICE-GAN",
          "SDL-MER\nAouayeb et al.": "",
          "MEGC-19\nSee et al.": "",
          "Column_7": ""
        },
        {
          "Datasets SDL-MER MEGC-19\nHandcrafted Aouayeb et al. See et al.\nChallenges SpRCNN MEGC-18 MERTA Sp3DCNN DCB-DB MTMTNet LGConD\nLearning based Xia et al. Merghani et al. Yang et al. Reddy et al. Xia et al. Xia et al. Li et al.\nSurveys SDF ApexFace CNN-LSTM NMER DSSN TSCNN STRCNN AuGACN GraphTCN LARNet FR\nPatel et al. Li et al. Wang et al. Liu et al. Khor et al. Song et al. Xia et al. Xie et al. Lei et al. Hashmi et al. Zhou et al.\nSurvey-I ESC-SFR Ma-MiER ELRCN-TE STSTNet 3D-Flow CapsuleNet DMER-KD CLFM CEP 3DCNN STSTNet+GA\nYan et al. Kim et al. Peng et al. Khor et al. Liong et al. Wang et al. Quang et al. Sun et al. Choi et al. Thuseethan et al. Zhao et al. Liu et al.\nCASME-II\nYan et al.\nDualInc OFF-Apex MOSTFRL MicroAtt RCN ICE-GAN\nCASME TIC ColorSpaces MEVIEW CAS(ME)2 SAMM Zhou et al. Gan et al. Kim et al. Wang et al. Xia et al. Yu et al. MMEW\nYan et al. Wang et al. Wang et al. HUSAK et al. Qu et al. Davison et al. Ben et al. Survey-XI\nSurvey-IX Li et al.\nBen et al.\n013 2014 2015 2016 2017 FHOHO ME-Ap2e 0 x 1 8 Sparse MDMO 2019 20 R 20 eliefF 2021 2022\nL S i M et I a C l . LB G P- u T o O e P t & al N . N H S u T a L n B g P e - t I a P l . Happy et al. Liong et al. Liu et al. Zhang et al. G S u u e r r v a d e l e . y ll - i X e t\nLBP-SIP MDMO FDM H. STLBP-RIP DRLS TTRM RSTR\nWang et al. Liu et al. Xu et al. Zong et al. Zong et al. Zong et al. Zhang et al.\nSurvey-III\nSurvey-II Oh et al. Survey-V Survey-VI, Goh et al.\nTakalkar et Merghani et Survey-VII, Zhou et al.\nal. Survey-IV al. Survey-VIII, Xie et al.\nLi et al.": "",
          "Datasets\nHandcrafted\nChallenges\nLearning based\nSurveys": "",
          "SDL-MER MEGC-19\nAouayeb et al. See et al.\nSpRCNN MEGC-18 MERTA Sp3DCNN DCB-DB MTMTNet LGConD\nXia et al. Merghani et al. Yang et al. Reddy et al. Xia et al. Xia et al. Li et al.\nSDF ApexFace CNN-LSTM NMER DSSN TSCNN STRCNN AuGACN GraphTCN LARNet FR\nPatel et al. Li et al. Wang et al. Liu et al. Khor et al. Song et al. Xia et al. Xie et al. Lei et al. Hashmi et al. Zhou et al.\nESC-SFR Ma-MiER ELRCN-TE STSTNet 3D-Flow CapsuleNet DMER-KD CLFM CEP 3DCNN STSTNet+GA\nKim et al. Peng et al. Khor et al. Liong et al. Wang et al. Quang et al. Sun et al. Choi et al. Thuseethan et al. Zhao et al. Liu et al.\nDualInc OFF-Apex MOSTFRL MicroAtt RCN ICE-GAN": "",
          "Column_4": "MEVIEW CAS(ME)2 SAMM Zhou et al. Gan et al. Kim et al. Wang et al. Xia et al. Yu et al. MMEW\nHUSAK et al. Qu et al. Davison et al. Ben et al. Survey-XI\nSurvey-IX Li et al.\nBen et al.",
          "SDL-MER\nAouayeb et al.": "",
          "MEGC-19\nSee et al.": "",
          "Column_7": ""
        },
        {
          "Datasets SDL-MER MEGC-19\nHandcrafted Aouayeb et al. See et al.\nChallenges SpRCNN MEGC-18 MERTA Sp3DCNN DCB-DB MTMTNet LGConD\nLearning based Xia et al. Merghani et al. Yang et al. Reddy et al. Xia et al. Xia et al. Li et al.\nSurveys SDF ApexFace CNN-LSTM NMER DSSN TSCNN STRCNN AuGACN GraphTCN LARNet FR\nPatel et al. Li et al. Wang et al. Liu et al. Khor et al. Song et al. Xia et al. Xie et al. Lei et al. Hashmi et al. Zhou et al.\nSurvey-I ESC-SFR Ma-MiER ELRCN-TE STSTNet 3D-Flow CapsuleNet DMER-KD CLFM CEP 3DCNN STSTNet+GA\nYan et al. Kim et al. Peng et al. Khor et al. Liong et al. Wang et al. Quang et al. Sun et al. Choi et al. Thuseethan et al. Zhao et al. Liu et al.\nCASME-II\nYan et al.\nDualInc OFF-Apex MOSTFRL MicroAtt RCN ICE-GAN\nCASME TIC ColorSpaces MEVIEW CAS(ME)2 SAMM Zhou et al. Gan et al. Kim et al. Wang et al. Xia et al. Yu et al. MMEW\nYan et al. Wang et al. Wang et al. HUSAK et al. Qu et al. Davison et al. Ben et al. Survey-XI\nSurvey-IX Li et al.\nBen et al.\n013 2014 2015 2016 2017 FHOHO ME-Ap2e 0 x 1 8 Sparse MDMO 2019 20 R 20 eliefF 2021 2022\nL S i M et I a C l . LB G P- u T o O e P t & al N . N H S u T a L n B g P e - t I a P l . Happy et al. Liong et al. Liu et al. Zhang et al. G S u u e r r v a d e l e . y ll - i X e t\nLBP-SIP MDMO FDM H. STLBP-RIP DRLS TTRM RSTR\nWang et al. Liu et al. Xu et al. Zong et al. Zong et al. Zong et al. Zhang et al.\nSurvey-III\nSurvey-II Oh et al. Survey-V Survey-VI, Goh et al.\nTakalkar et Merghani et Survey-VII, Zhou et al.\nal. Survey-IV al. Survey-VIII, Xie et al.\nLi et al.": "",
          "Datasets\nHandcrafted\nChallenges\nLearning based\nSurveys": "",
          "SDL-MER MEGC-19\nAouayeb et al. See et al.\nSpRCNN MEGC-18 MERTA Sp3DCNN DCB-DB MTMTNet LGConD\nXia et al. Merghani et al. Yang et al. Reddy et al. Xia et al. Xia et al. Li et al.\nSDF ApexFace CNN-LSTM NMER DSSN TSCNN STRCNN AuGACN GraphTCN LARNet FR\nPatel et al. Li et al. Wang et al. Liu et al. Khor et al. Song et al. Xia et al. Xie et al. Lei et al. Hashmi et al. Zhou et al.\nESC-SFR Ma-MiER ELRCN-TE STSTNet 3D-Flow CapsuleNet DMER-KD CLFM CEP 3DCNN STSTNet+GA\nKim et al. Peng et al. Khor et al. Liong et al. Wang et al. Quang et al. Sun et al. Choi et al. Thuseethan et al. Zhao et al. Liu et al.\nDualInc OFF-Apex MOSTFRL MicroAtt RCN ICE-GAN": "2016",
          "Column_4": "2017 2018 2019 2020 2021 2022",
          "SDL-MER\nAouayeb et al.": "",
          "MEGC-19\nSee et al.": "",
          "Column_7": ""
        },
        {
          "Datasets SDL-MER MEGC-19\nHandcrafted Aouayeb et al. See et al.\nChallenges SpRCNN MEGC-18 MERTA Sp3DCNN DCB-DB MTMTNet LGConD\nLearning based Xia et al. Merghani et al. Yang et al. Reddy et al. Xia et al. Xia et al. Li et al.\nSurveys SDF ApexFace CNN-LSTM NMER DSSN TSCNN STRCNN AuGACN GraphTCN LARNet FR\nPatel et al. Li et al. Wang et al. Liu et al. Khor et al. Song et al. Xia et al. Xie et al. Lei et al. Hashmi et al. Zhou et al.\nSurvey-I ESC-SFR Ma-MiER ELRCN-TE STSTNet 3D-Flow CapsuleNet DMER-KD CLFM CEP 3DCNN STSTNet+GA\nYan et al. Kim et al. Peng et al. Khor et al. Liong et al. Wang et al. Quang et al. Sun et al. Choi et al. Thuseethan et al. Zhao et al. Liu et al.\nCASME-II\nYan et al.\nDualInc OFF-Apex MOSTFRL MicroAtt RCN ICE-GAN\nCASME TIC ColorSpaces MEVIEW CAS(ME)2 SAMM Zhou et al. Gan et al. Kim et al. Wang et al. Xia et al. Yu et al. MMEW\nYan et al. Wang et al. Wang et al. HUSAK et al. Qu et al. Davison et al. Ben et al. Survey-XI\nSurvey-IX Li et al.\nBen et al.\n013 2014 2015 2016 2017 FHOHO ME-Ap2e 0 x 1 8 Sparse MDMO 2019 20 R 20 eliefF 2021 2022\nL S i M et I a C l . LB G P- u T o O e P t & al N . N H S u T a L n B g P e - t I a P l . Happy et al. Liong et al. Liu et al. Zhang et al. G S u u e r r v a d e l e . y ll - i X e t\nLBP-SIP MDMO FDM H. STLBP-RIP DRLS TTRM RSTR\nWang et al. Liu et al. Xu et al. Zong et al. Zong et al. Zong et al. Zhang et al.\nSurvey-III\nSurvey-II Oh et al. Survey-V Survey-VI, Goh et al.\nTakalkar et Merghani et Survey-VII, Zhou et al.\nal. Survey-IV al. Survey-VIII, Xie et al.\nLi et al.": "",
          "Datasets\nHandcrafted\nChallenges\nLearning based\nSurveys": "LBP-TOP&NN STLBP-IP\nGuo et al. Huang et al.\nLBP-SIP\nWang et al.",
          "SDL-MER MEGC-19\nAouayeb et al. See et al.\nSpRCNN MEGC-18 MERTA Sp3DCNN DCB-DB MTMTNet LGConD\nXia et al. Merghani et al. Yang et al. Reddy et al. Xia et al. Xia et al. Li et al.\nSDF ApexFace CNN-LSTM NMER DSSN TSCNN STRCNN AuGACN GraphTCN LARNet FR\nPatel et al. Li et al. Wang et al. Liu et al. Khor et al. Song et al. Xia et al. Xie et al. Lei et al. Hashmi et al. Zhou et al.\nESC-SFR Ma-MiER ELRCN-TE STSTNet 3D-Flow CapsuleNet DMER-KD CLFM CEP 3DCNN STSTNet+GA\nKim et al. Peng et al. Khor et al. Liong et al. Wang et al. Quang et al. Sun et al. Choi et al. Thuseethan et al. Zhao et al. Liu et al.\nDualInc OFF-Apex MOSTFRL MicroAtt RCN ICE-GAN": "FHOHO ME-Apex Sparse MDMO ReliefF\nHappy et al. Liong et al. Liu et al. Zhang et al. Survey-X\nGuerdelli et\nal.\nMDMO FDM H. STLBP-RIP DRLS TTRM RSTR\nLiu et al. Xu et al. Zong et al. Zong et al. Zong et al. Zhang et al.",
          "Column_4": "FHOHO ME-Apex Sparse MDMO ReliefF\nHappy et al. Liong et al. Liu et al. Zhang et al. Survey-X\nGuerdelli et\nal.",
          "SDL-MER\nAouayeb et al.": "",
          "MEGC-19\nSee et al.": "",
          "Column_7": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "DualInc OFF-Apex MOSTFRL MicroAtt RCN ICE-GAN": "Zhou et al. Gan et al. Kim et al. Wang et al. Xia et al. Yu et al."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "I/P\nPretrained Multi-Scale Macro\nWeights Adaptive\n(ImageNet) Weights\nCNN-LSTM RCN\nhole Video Optical flows\nAttention GAN\nFeatures SoftmaxPT\nubset of image\nsequences Active image\nWeight update Error\nBack Propagation\npex frame m M ag o n ti i o fi n ed CapsuleNet 3D-CNN\n2D-CNN Graph TCN\nPatchwise NAS based\nnset-Apex- Dynamic Analaysis Multi-Stream CNN\nffset frames image": "",
          "Pretrained Multi-Scale Macro\nWeights Adaptive\n(ImageNet) Weights\nCNN-LSTM RCN\nAttention GAN\nFeatures SoftmaxPT": "",
          "Column_3": "",
          "O/P\nA\nC\nF\nH\nR\nSa\nSu\nT\nN\nP\nO\nSu": "A"
        },
        {
          "I/P\nPretrained Multi-Scale Macro\nWeights Adaptive\n(ImageNet) Weights\nCNN-LSTM RCN\nhole Video Optical flows\nAttention GAN\nFeatures SoftmaxPT\nubset of image\nsequences Active image\nWeight update Error\nBack Propagation\npex frame m M ag o n ti i o fi n ed CapsuleNet 3D-CNN\n2D-CNN Graph TCN\nPatchwise NAS based\nnset-Apex- Dynamic Analaysis Multi-Stream CNN\nffset frames image": "",
          "Pretrained Multi-Scale Macro\nWeights Adaptive\n(ImageNet) Weights\nCNN-LSTM RCN\nAttention GAN\nFeatures SoftmaxPT": "",
          "Column_3": "",
          "O/P\nA\nC\nF\nH\nR\nSa\nSu\nT\nN\nP\nO\nSu": "C"
        },
        {
          "I/P\nPretrained Multi-Scale Macro\nWeights Adaptive\n(ImageNet) Weights\nCNN-LSTM RCN\nhole Video Optical flows\nAttention GAN\nFeatures SoftmaxPT\nubset of image\nsequences Active image\nWeight update Error\nBack Propagation\npex frame m M ag o n ti i o fi n ed CapsuleNet 3D-CNN\n2D-CNN Graph TCN\nPatchwise NAS based\nnset-Apex- Dynamic Analaysis Multi-Stream CNN\nffset frames image": "",
          "Pretrained Multi-Scale Macro\nWeights Adaptive\n(ImageNet) Weights\nCNN-LSTM RCN\nAttention GAN\nFeatures SoftmaxPT": "",
          "Column_3": "RCN",
          "O/P\nA\nC\nF\nH\nR\nSa\nSu\nT\nN\nP\nO\nSu": "F\nH"
        },
        {
          "I/P\nPretrained Multi-Scale Macro\nWeights Adaptive\n(ImageNet) Weights\nCNN-LSTM RCN\nhole Video Optical flows\nAttention GAN\nFeatures SoftmaxPT\nubset of image\nsequences Active image\nWeight update Error\nBack Propagation\npex frame m M ag o n ti i o fi n ed CapsuleNet 3D-CNN\n2D-CNN Graph TCN\nPatchwise NAS based\nnset-Apex- Dynamic Analaysis Multi-Stream CNN\nffset frames image": "",
          "Pretrained Multi-Scale Macro\nWeights Adaptive\n(ImageNet) Weights\nCNN-LSTM RCN\nAttention GAN\nFeatures SoftmaxPT": "",
          "Column_3": "",
          "O/P\nA\nC\nF\nH\nR\nSa\nSu\nT\nN\nP\nO\nSu": "R"
        },
        {
          "I/P\nPretrained Multi-Scale Macro\nWeights Adaptive\n(ImageNet) Weights\nCNN-LSTM RCN\nhole Video Optical flows\nAttention GAN\nFeatures SoftmaxPT\nubset of image\nsequences Active image\nWeight update Error\nBack Propagation\npex frame m M ag o n ti i o fi n ed CapsuleNet 3D-CNN\n2D-CNN Graph TCN\nPatchwise NAS based\nnset-Apex- Dynamic Analaysis Multi-Stream CNN\nffset frames image": "",
          "Pretrained Multi-Scale Macro\nWeights Adaptive\n(ImageNet) Weights\nCNN-LSTM RCN\nAttention GAN\nFeatures SoftmaxPT": "",
          "Column_3": "GAN",
          "O/P\nA\nC\nF\nH\nR\nSa\nSu\nT\nN\nP\nO\nSu": "Sa"
        },
        {
          "I/P\nPretrained Multi-Scale Macro\nWeights Adaptive\n(ImageNet) Weights\nCNN-LSTM RCN\nhole Video Optical flows\nAttention GAN\nFeatures SoftmaxPT\nubset of image\nsequences Active image\nWeight update Error\nBack Propagation\npex frame m M ag o n ti i o fi n ed CapsuleNet 3D-CNN\n2D-CNN Graph TCN\nPatchwise NAS based\nnset-Apex- Dynamic Analaysis Multi-Stream CNN\nffset frames image": "",
          "Pretrained Multi-Scale Macro\nWeights Adaptive\n(ImageNet) Weights\nCNN-LSTM RCN\nAttention GAN\nFeatures SoftmaxPT": "",
          "Column_3": "",
          "O/P\nA\nC\nF\nH\nR\nSa\nSu\nT\nN\nP\nO\nSu": "Su\nT"
        },
        {
          "I/P\nPretrained Multi-Scale Macro\nWeights Adaptive\n(ImageNet) Weights\nCNN-LSTM RCN\nhole Video Optical flows\nAttention GAN\nFeatures SoftmaxPT\nubset of image\nsequences Active image\nWeight update Error\nBack Propagation\npex frame m M ag o n ti i o fi n ed CapsuleNet 3D-CNN\n2D-CNN Graph TCN\nPatchwise NAS based\nnset-Apex- Dynamic Analaysis Multi-Stream CNN\nffset frames image": "",
          "Pretrained Multi-Scale Macro\nWeights Adaptive\n(ImageNet) Weights\nCNN-LSTM RCN\nAttention GAN\nFeatures SoftmaxPT": "Weight update Error\nBack Propagation",
          "Column_3": "",
          "O/P\nA\nC\nF\nH\nR\nSa\nSu\nT\nN\nP\nO\nSu": ""
        },
        {
          "I/P\nPretrained Multi-Scale Macro\nWeights Adaptive\n(ImageNet) Weights\nCNN-LSTM RCN\nhole Video Optical flows\nAttention GAN\nFeatures SoftmaxPT\nubset of image\nsequences Active image\nWeight update Error\nBack Propagation\npex frame m M ag o n ti i o fi n ed CapsuleNet 3D-CNN\n2D-CNN Graph TCN\nPatchwise NAS based\nnset-Apex- Dynamic Analaysis Multi-Stream CNN\nffset frames image": "",
          "Pretrained Multi-Scale Macro\nWeights Adaptive\n(ImageNet) Weights\nCNN-LSTM RCN\nAttention GAN\nFeatures SoftmaxPT": "",
          "Column_3": "",
          "O/P\nA\nC\nF\nH\nR\nSa\nSu\nT\nN\nP\nO\nSu": "N\nP"
        },
        {
          "I/P\nPretrained Multi-Scale Macro\nWeights Adaptive\n(ImageNet) Weights\nCNN-LSTM RCN\nhole Video Optical flows\nAttention GAN\nFeatures SoftmaxPT\nubset of image\nsequences Active image\nWeight update Error\nBack Propagation\npex frame m M ag o n ti i o fi n ed CapsuleNet 3D-CNN\n2D-CNN Graph TCN\nPatchwise NAS based\nnset-Apex- Dynamic Analaysis Multi-Stream CNN\nffset frames image": "",
          "Pretrained Multi-Scale Macro\nWeights Adaptive\n(ImageNet) Weights\nCNN-LSTM RCN\nAttention GAN\nFeatures SoftmaxPT": "",
          "Column_3": "",
          "O/P\nA\nC\nF\nH\nR\nSa\nSu\nT\nN\nP\nO\nSu": "O\nSu"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Nonverbal leakage and clues to deception",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1969",
      "venue": "Psychiatry"
    },
    {
      "citation_id": "2",
      "title": "A survey: facial microexpression recognition",
      "authors": [
        "M Takalkar",
        "M Xu",
        "Q Wu",
        "Z Chaczko"
      ],
      "year": "2018",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "3",
      "title": "Smeconvnet: A convolutional neural network for spotting spontaneous facial microexpression from long videos",
      "authors": [
        "Z Zhang",
        "T Chen",
        "H Meng",
        "G Liu",
        "X Fu"
      ],
      "year": "2018",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "4",
      "title": "Automatic apex frame spotting in micro-expression database",
      "authors": [
        "S.-T Liong",
        "J See",
        "K Wong",
        "A Le Ngo",
        "Y.-H Oh",
        "R Phan"
      ],
      "year": "2015",
      "venue": "2015 3rd IAPR Asian conference on pattern recognition (ACPR)"
    },
    {
      "citation_id": "5",
      "title": "Micro-expression spotting: A new benchmark",
      "authors": [
        "T.-K Tran",
        "Q.-N Vo",
        "X Hong",
        "X Li",
        "G Zhao"
      ],
      "year": "2021",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "6",
      "title": "Towards reading hidden emotions: A comparative study of spontaneous micro-expression spotting and recognition methods",
      "authors": [
        "X Li",
        "X Hong",
        "A Moilanen",
        "X Huang",
        "T Pfister",
        "G Zhao",
        "M Pietikäinen"
      ],
      "year": "2017",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "7",
      "title": "A survey of automatic facial micro-expression analysis: databases, methods, and challenges",
      "authors": [
        "Y.-H Oh",
        "J See",
        "A Le Ngo",
        "R -W. Phan",
        "V Baskaran"
      ],
      "year": "2018",
      "venue": "Frontiers in psychology"
    },
    {
      "citation_id": "8",
      "title": "An overview of facial micro-expression analysis: Data, methodology and challenge",
      "authors": [
        "H.-X Xie",
        "L Lo",
        "H.-H Shuai",
        "W.-H Cheng"
      ],
      "year": "2020",
      "venue": "An overview of facial micro-expression analysis: Data, methodology and challenge",
      "arxiv": "arXiv:2012.11307"
    },
    {
      "citation_id": "9",
      "title": "Video-based facial micro-expression analysis: A survey of datasets, features and algorithms",
      "authors": [
        "X Ben",
        "Y Ren",
        "J Zhang",
        "S.-J Wang",
        "K Kpalma",
        "W Meng",
        "Y.-J Liu"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "10",
      "title": "Revealing the invisible with model and data shrinking for composite-database micro-expression recognition",
      "authors": [
        "Z Xia",
        "W Peng",
        "H.-Q Khor",
        "X Feng",
        "G Zhao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "11",
      "title": "Joint local and global information learning with single apex frame detection for micro-expression recognition",
      "authors": [
        "Y Li",
        "X Huang",
        "G Zhao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "12",
      "title": "Dual-inception network for crossdatabase micro-expression recognition",
      "authors": [
        "L Zhou",
        "Q Mao",
        "L Xue"
      ],
      "year": "2019",
      "venue": "Dual-inception network for crossdatabase micro-expression recognition"
    },
    {
      "citation_id": "13",
      "title": "Spatiotemporal recurrent convolutional networks for recognizing spontaneous microexpressions",
      "authors": [
        "Z Xia",
        "X Hong",
        "X Gao",
        "X Feng",
        "G Zhao"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "14",
      "title": "Joint pose and expression modeling for facial expression recognition",
      "authors": [
        "F Zhang",
        "T Zhang",
        "Q Mao",
        "C Xu"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "15",
      "title": "Ice-gan: Identity-aware and capsule-enhanced gan for micro-expression recognition and synthesis",
      "authors": [
        "J Yu",
        "C Zhang",
        "Y Song",
        "W Cai"
      ],
      "year": "2020",
      "venue": "Ice-gan: Identity-aware and capsule-enhanced gan for micro-expression recognition and synthesis",
      "arxiv": "arXiv:2005.04370"
    },
    {
      "citation_id": "16",
      "title": "Micro-expression recognition: an updated review of current trends, challenges and solutions",
      "authors": [
        "K Goh",
        "C Ng",
        "L Lim",
        "U Sheikh"
      ],
      "year": "2020",
      "venue": "The Visual Computer"
    },
    {
      "citation_id": "17",
      "title": "Dynamic texture recognition using local binary patterns with an application to facial expressions",
      "authors": [
        "G Zhao",
        "M Pietikainen"
      ],
      "year": "2007",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "18",
      "title": "Lbp with six intersection points: Reducing redundant information in lbp-top for micro-expression recognition",
      "authors": [
        "Y Wang",
        "J See",
        "R -W. Phan",
        "Y.-H Oh"
      ],
      "year": "2014",
      "venue": "Asian conference on computer vision"
    },
    {
      "citation_id": "19",
      "title": "Facial microexpression recognition using spatiotemporal local binary pattern with integral projection",
      "authors": [
        "X Huang",
        "S.-J Wang",
        "G Zhao",
        "M Piteikainen"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE international conference on computer vision workshops"
    },
    {
      "citation_id": "20",
      "title": "Discriminative spatiotemporal local binary pattern with revisited integral projection for spontaneous facial micro-expression recognition",
      "authors": [
        "X Huang",
        "S.-J Wang",
        "X Liu",
        "G Zhao",
        "X Feng",
        "M Pietikäinen"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "21",
      "title": "A main directional mean optical flow feature for spontaneous microexpression recognition",
      "authors": [
        "Y.-J Liu",
        "J.-K Zhang",
        "W.-J Yan",
        "S.-J Wang",
        "G Zhao",
        "X Fu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "22",
      "title": "Sparse mdmo: Learning a discriminative feature for spontaneous micro-expression recognition",
      "authors": [
        "Y.-J Liu",
        "B.-J Li",
        "Y.-K Lai"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "23",
      "title": "Fuzzy histogram of optical flow orientations for micro-expression recognition",
      "authors": [
        "S Happy",
        "A Routray"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "24",
      "title": "Micro-expression recognition using color spaces",
      "authors": [
        "S.-J Wang",
        "W.-J Yan",
        "X Li",
        "G Zhao",
        "C.-G Zhou",
        "X Fu",
        "M Yang",
        "J Tao"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "25",
      "title": "A review on facial microexpressions analysis: datasets, features and metrics",
      "authors": [
        "W Merghani",
        "A Davison",
        "M Yap"
      ],
      "year": "2018",
      "venue": "A review on facial microexpressions analysis: datasets, features and metrics",
      "arxiv": "arXiv:1805.02397"
    },
    {
      "citation_id": "26",
      "title": "A survey of micro-expression recognition",
      "authors": [
        "L Zhou",
        "X Shao",
        "Q Mao"
      ],
      "year": "2021",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "27",
      "title": "Deep learning for micro-expression recognition: A survey",
      "authors": [
        "Y Li",
        "J Wei",
        "Y Liu",
        "J Kauttonen",
        "G Zhao"
      ],
      "year": "2021",
      "venue": "Deep learning for micro-expression recognition: A survey",
      "arxiv": "arXiv:2107.02823"
    },
    {
      "citation_id": "28",
      "title": "Macro-and micro-expressions facial datasets: A survey",
      "authors": [
        "H Guerdelli",
        "C Ferrari",
        "W Barhoumi",
        "H Ghazouani",
        "S Berretti"
      ],
      "year": "2022",
      "venue": "Sensors"
    },
    {
      "citation_id": "29",
      "title": "Learnet: Dynamic imaging network for micro expression recognition",
      "authors": [
        "M Verma",
        "S Vipparthi",
        "G Singh",
        "S Murala"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "30",
      "title": "Affectivenet: Affectivemotion feature learning for micro expression recognition",
      "authors": [
        "M Verma",
        "S Vipparthi",
        "G Singh"
      ],
      "year": "2020",
      "venue": "IEEE Multi-Media"
    },
    {
      "citation_id": "31",
      "title": "Non-linearities improve originet based on active imaging for micro expression recognition",
      "authors": [
        "M Verma",
        "S Vipparthi",
        "G Singh"
      ],
      "year": "2020",
      "venue": "2020 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "32",
      "title": "Dual-stream shallow networks for facial micro-expression recognition",
      "authors": [
        "H.-Q Khor",
        "J See",
        "S.-T Liong",
        "R Phan",
        "W Lin"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Image Processing"
    },
    {
      "citation_id": "33",
      "title": "Recognizing spontaneous micro-expression using a three-stream convolutional neural network",
      "authors": [
        "B Song",
        "K Li",
        "Y Zong",
        "J Zhu",
        "W Zheng",
        "J Shi",
        "L Zhao"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "34",
      "title": "Offapexnet on micro-expression recognition system",
      "authors": [
        "Y Gan",
        "S.-T Liong",
        "W.-C Yau",
        "Y.-C Huang",
        "L.-K Tan"
      ],
      "year": "2019",
      "venue": "Signal Processing: Image Communication"
    },
    {
      "citation_id": "35",
      "title": "Merastc: Micro-expression recognition using effective feature encodings and 2d convolutional neural network",
      "authors": [
        "P Gupta"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "36",
      "title": "Enriched long-term recurrent convolutional network for facial micro-expression recognition",
      "authors": [
        "H.-Q Khor",
        "J See",
        "R Phan",
        "W Lin"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "37",
      "title": "Offset or onset frame: A multi-stream convolutional neural network with capsulenet module for micro-expression recognition",
      "authors": [
        "N Liu",
        "X Liu",
        "Z Zhang",
        "X Xu",
        "T Chen"
      ],
      "year": "2020",
      "venue": "2020 5th International Conference on Intelligent Informatics and Biomedical Sciences (ICIIBMS)"
    },
    {
      "citation_id": "38",
      "title": "Merta: microexpression recognition with ternary attentions",
      "authors": [
        "B Yang",
        "J Cheng",
        "Y Yang",
        "B Zhang",
        "J Li"
      ],
      "year": "2019",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "39",
      "title": "Shallow triple stream three-dimensional cnn (ststnet) for micro-expression recognition",
      "authors": [
        "S.-T Liong",
        "Y Gan",
        "H.-Q John See",
        "Y.-C Huang"
      ],
      "year": "2019",
      "venue": "Shallow triple stream three-dimensional cnn (ststnet) for micro-expression recognition"
    },
    {
      "citation_id": "40",
      "title": "Micro-expression recognition with expression-state constrained spatio-temporal feature representations",
      "authors": [
        "D Kim",
        "W Baddar",
        "Y Ro"
      ],
      "year": "2016",
      "venue": "Proceedings of the 24th ACM international conference on Multimedia"
    },
    {
      "citation_id": "41",
      "title": "Selective deep features for microexpression recognition",
      "authors": [
        "D Patel",
        "X Hong",
        "G Zhao"
      ],
      "year": "2016",
      "venue": "2016 23rd international conference on pattern recognition (ICPR"
    },
    {
      "citation_id": "42",
      "title": "From macro to micro expression recognition: Deep learning on small datasets using transfer learning",
      "authors": [
        "M Peng",
        "Z Wu",
        "Z Zhang",
        "T Chen"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "43",
      "title": "Can micro-expression be recognized based on single apex frame?",
      "authors": [
        "Y Li",
        "X Huang",
        "G Zhao"
      ],
      "year": "2018",
      "venue": "IEEE International Conference on Image Processing"
    },
    {
      "citation_id": "44",
      "title": "Spontaneous facial microexpression recognition via deep convolutional network",
      "authors": [
        "Z Xia",
        "X Feng",
        "X Hong",
        "G Zhao"
      ],
      "year": "2018",
      "venue": "2018 Eighth International Conference on Image Processing Theory, Tools and Applications (IPTA)"
    },
    {
      "citation_id": "45",
      "title": "A neural micro-expression recognizer",
      "authors": [
        "Y Liu",
        "H Du",
        "L Zheng",
        "T Gedeon"
      ],
      "year": "2019",
      "venue": "2019 14th IEEE international conference on automatic face and gesture recognition"
    },
    {
      "citation_id": "46",
      "title": "Micro-expression recognition based on 3d flow convolutional neural network",
      "authors": [
        "J Li",
        "Y Wang",
        "J See",
        "W Liu"
      ],
      "year": "2019",
      "venue": "Pattern Analysis and Applications"
    },
    {
      "citation_id": "47",
      "title": "Spontaneous facial micro-expression recognition using 3d spatiotemporal convolutional neural networks",
      "authors": [
        "S Reddy",
        "S Karri",
        "S Dubey",
        "S Mukherjee"
      ],
      "year": "2019",
      "venue": "2019 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "48",
      "title": "Capsulenet for microexpression recognition",
      "authors": [
        "N Van Quang",
        "J Chun",
        "T Tokuyama"
      ],
      "year": "2019",
      "venue": "Capsulenet for microexpression recognition"
    },
    {
      "citation_id": "49",
      "title": "Cross-database microexpression recognition with deep convolutional networks",
      "authors": [
        "Z Xia",
        "H Liang",
        "X Hong",
        "X Feng"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 3rd International Conference on Biometric Engineering and Applications"
    },
    {
      "citation_id": "50",
      "title": "Micro-attention for microexpression recognition",
      "authors": [
        "C Wang",
        "M Peng",
        "T Bi",
        "T Chen"
      ],
      "year": "2020",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "51",
      "title": "A spatiotemporal deep learning solution for automatic micro-expressions recognition from local facial regions",
      "authors": [
        "M Aouayeb",
        "W Hamidouche",
        "K Kpalma",
        "A Benazza-Benyahia"
      ],
      "year": "2019",
      "venue": "2019 IEEE 29th International Workshop on Machine Learning for Signal Processing"
    },
    {
      "citation_id": "52",
      "title": "Dynamic micro-expression recognition using knowledge distillation",
      "authors": [
        "B Sun",
        "S Cao",
        "D Li",
        "J He",
        "L Yu"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "53",
      "title": "Au-assisted graph attention convolutional network for micro-expression recognition",
      "authors": [
        "H.-X Xie",
        "L Lo",
        "H.-H Shuai",
        "W.-H Cheng"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "54",
      "title": "Learning from macroexpression: a micro-expression recognition framework",
      "authors": [
        "B Xia",
        "W Wang",
        "S Wang",
        "E Chen"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "55",
      "title": "A novel graph-tcn with a graph structured representation for micro-expression recognition",
      "authors": [
        "L Lei",
        "J Li",
        "T Chen",
        "S Li"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "56",
      "title": "Facial micro-expression recognition using two-dimensional landmark feature maps",
      "authors": [
        "D Choi",
        "B Song"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "57",
      "title": "Complex emotion profiling: An incremental active learning based approach with sparse annotations",
      "authors": [
        "S Thuseethan",
        "S Rajasegarar",
        "J Yearwood"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "58",
      "title": "Automer: Spatiotemporal neural architecture search for microexpression recognition",
      "authors": [
        "M Verma",
        "M Reddy",
        "Y Meedimale",
        "M Mandal",
        "S Vipparthi"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "59",
      "title": "A two-stage 3d cnn based learning method for spontaneous microexpression recognition",
      "authors": [
        "S Zhao",
        "H Tao",
        "Y Zhang",
        "T Xu",
        "K Zhang",
        "Z Hao",
        "E Chen"
      ],
      "year": "2021",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "60",
      "title": "Microexpression recognition using advanced genetic algorithm",
      "authors": [
        "K.-H Liu",
        "Q.-S Jin",
        "H.-C Xu",
        "Y.-S Gan",
        "S.-T Liong"
      ],
      "year": "2021",
      "venue": "Signal Processing: Image Communication"
    },
    {
      "citation_id": "61",
      "title": "Feature refinement: An expression-specific feature learning and fusion method for micro-expression recognition",
      "authors": [
        "L Zhou",
        "Q Mao",
        "X Huang",
        "F Zhang",
        "Z Zhang"
      ],
      "year": "2022",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "62",
      "title": "Displacement generating module based end-to-end micro-expression recognition network",
      "authors": [
        "Z Zhai",
        "H Sun",
        "J Zhao",
        "Z Dong",
        "S He",
        "H Zhao"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Conference on Systems, Man, and Cybernetics (SMC)"
    },
    {
      "citation_id": "63",
      "title": "Dynamic routing between capsules",
      "authors": [
        "S Sabour",
        "N Frosst",
        "G Hinton"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "64",
      "title": "Deciphering the enigmatic face: The importance of facial dynamics in interpreting subtle facial expressions",
      "authors": [
        "Z Ambadar",
        "J Schooler",
        "J Cohn"
      ],
      "year": "2005",
      "venue": "Psychological science"
    },
    {
      "citation_id": "65",
      "title": "Micro-expression recognition with small sample size by transferring long-term convolutional neural network",
      "authors": [
        "S.-J Wang",
        "B.-J Li",
        "Y.-J Liu",
        "W.-J Yan",
        "X Ou",
        "X Huang",
        "F Xu",
        "X Fu"
      ],
      "year": "2018",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "66",
      "title": "Micro-expression classification based on landmark relations with graph attention convolutional network",
      "authors": [
        "A Kumar",
        "B Bhanu"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "67",
      "title": "Micro-expression recognition based on facial graph representation learning and facial action unit fusion",
      "authors": [
        "L Lei",
        "T Chen",
        "S Li",
        "J Li"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "68",
      "title": "Facial action coding system",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1978",
      "venue": "Environmental Psychology & Nonverbal Behavior"
    },
    {
      "citation_id": "69",
      "title": "Mett. micro expression training tool",
      "authors": [
        "P Ekman"
      ],
      "year": "2003",
      "venue": "Mett. micro expression training tool"
    },
    {
      "citation_id": "70",
      "title": "Mer-gcn: Microexpression recognition based on relation modeling with graph convolutional networks",
      "authors": [
        "L Lo",
        "H.-X Xie",
        "H.-H Shuai",
        "W.-H Cheng"
      ],
      "year": "2020",
      "venue": "2020 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR)"
    },
    {
      "citation_id": "71",
      "title": "Semi-supervised classification with graph convolutional networks",
      "authors": [
        "M Welling",
        "T Kipf"
      ],
      "year": "2016",
      "venue": "J. International Conference on Learning Representations"
    },
    {
      "citation_id": "72",
      "title": "Darts: Differentiable architecture search",
      "authors": [
        "H Liu",
        "K Simonyan",
        "Y Yang"
      ],
      "year": "2018",
      "venue": "Darts: Differentiable architecture search",
      "arxiv": "arXiv:1806.09055"
    },
    {
      "citation_id": "73",
      "title": "Auto-deeplab: Hierarchical neural architecture search for semantic image segmentation",
      "authors": [
        "C Liu",
        "L.-C Chen",
        "F Schroff",
        "H Adam",
        "W Hua",
        "A Yuille",
        "L Fei-Fei"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "74",
      "title": "Migo-nas: Towards fast and generalizable neural architecture search",
      "authors": [
        "X Zheng",
        "R Ji",
        "Y Chen",
        "Q Wang",
        "B Zhang",
        "J Chen",
        "Q Ye",
        "F Huang",
        "Y Tian"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "75",
      "title": "Imagenet classification with deep convolutional neural networks",
      "authors": [
        "A Krizhevsky",
        "I Sutskever",
        "G Hinton"
      ],
      "year": "2012",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "76",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2014",
      "venue": "CoRR"
    },
    {
      "citation_id": "77",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2015",
      "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "78",
      "title": "Facial micro-expressions grand challenge 2018 summary",
      "authors": [
        "M Yap",
        "J See",
        "X Hong",
        "S.-J Wang"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "79",
      "title": "Megc 2019-the second facial micro-expressions grand challenge",
      "authors": [
        "J See",
        "M Yap",
        "J Li",
        "X Hong",
        "S.-J Wang"
      ],
      "year": "2019",
      "venue": "Megc 2019-the second facial micro-expressions grand challenge"
    },
    {
      "citation_id": "80",
      "title": "Expertnet: Exigent features preservative network for facial expression recognition",
      "authors": [
        "M Verma",
        "J Bhui",
        "S Vipparthi",
        "G Singh"
      ],
      "year": "2018",
      "venue": "Proceedings of the 11th Indian Conference on Computer Vision, Graphics and Image Processing"
    },
    {
      "citation_id": "81",
      "title": "Mobilenets: Efficient convolutional neural networks for mobile vision applications",
      "authors": [
        "A Howard",
        "M Zhu",
        "B Chen",
        "D Kalenichenko",
        "W Wang",
        "T Weyand",
        "M Andreetto",
        "H Adam"
      ],
      "year": "2017",
      "venue": "Mobilenets: Efficient convolutional neural networks for mobile vision applications",
      "arxiv": "arXiv:1704.04861"
    },
    {
      "citation_id": "82",
      "title": "Squeezenet: Alexnet-level accuracy with 50x fewer parameters and¡ 0.5 mb model size",
      "authors": [
        "F Iandola",
        "S Han",
        "M Moskewicz",
        "K Ashraf",
        "W Dally",
        "K Keutzer"
      ],
      "year": "2016",
      "venue": "Squeezenet: Alexnet-level accuracy with 50x fewer parameters and¡ 0.5 mb model size",
      "arxiv": "arXiv:1602.07360"
    },
    {
      "citation_id": "83",
      "title": "Deep learning approaches for facial emotion recognition: A case study on fer-2013",
      "authors": [
        "P Giannopoulos",
        "I Perikos",
        "I Hatzilygeroudis"
      ],
      "year": "2018",
      "venue": "Deep learning approaches for facial emotion recognition: A case study on fer-2013"
    },
    {
      "citation_id": "84",
      "title": "Hinet: Hybrid inherited feature learning network for facial expression recognition",
      "authors": [
        "M Verma",
        "S Vipparthi",
        "G Singh"
      ],
      "year": "2019",
      "venue": "IEEE Letters of the Computer Society"
    },
    {
      "citation_id": "85",
      "title": "Spotting facial micro-expressions \"in the wild",
      "authors": [
        "P Husák",
        "J Cech",
        "J Matas"
      ],
      "year": "2017",
      "venue": "22nd Computer Vision Winter Workshop"
    },
    {
      "citation_id": "86",
      "title": "Larnet: Real-time detection of facial micro expression using lossless attention residual network",
      "authors": [
        "M Hashmi",
        "B Ashish",
        "V Sharma",
        "A Keskar",
        "N Bokde",
        "J Yoon",
        "Z Geem"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "87",
      "title": "Downside risk measure: Theory and applications in portfolio choice and risk management",
      "authors": [
        "A.-X Xie"
      ],
      "year": "2002",
      "venue": "Downside risk measure: Theory and applications in portfolio choice and risk management"
    },
    {
      "citation_id": "88",
      "title": "Image based facial micro-expression recognition using deep learning on small datasets",
      "authors": [
        "M Takalkar",
        "M Xu"
      ],
      "year": "2017",
      "venue": "2017 international conference on digital image computing: techniques and applications (DICTA)"
    },
    {
      "citation_id": "89",
      "title": "From individual to group-level emotion recognition: Emotiw 5.0",
      "authors": [
        "A Dhall",
        "R Goecke",
        "S Ghosh",
        "J Joshi",
        "J Hoey",
        "T Gedeon"
      ],
      "year": "2017",
      "venue": "Proceedings of the 19th ACM international conference on multimodal interaction"
    },
    {
      "citation_id": "90",
      "title": "Automatic emotion recognition for groups: a review",
      "authors": [
        "E Veltmeijer",
        "C Gerritsen",
        "K Hindriks"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "91",
      "title": "Eulerian video magnification for revealing subtle changes in the world",
      "authors": [
        "H.-Y Wu",
        "M Rubinstein",
        "E Shih",
        "J Guttag",
        "F Durand",
        "W Freeman"
      ],
      "year": "2012",
      "venue": "ACM transactions on graphics (TOG)"
    },
    {
      "citation_id": "92",
      "title": "Learning-based video motion magnification",
      "authors": [
        "T.-H Oh",
        "R Jaroensri",
        "C Kim",
        "M Elgharib",
        "F Durand",
        "W Freeman",
        "W Matusik"
      ],
      "year": "2018",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "93",
      "title": "M3er: Multiplicative multimodal emotion recognition using facial, textual, and speech cues",
      "authors": [
        "T Mittal",
        "U Bhattacharya",
        "R Chandra",
        "A Bera",
        "D Manocha"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "94",
      "title": "Emoticon: Context-aware multimodal emotion recognition using frege's principle",
      "authors": [
        "T Mittal",
        "P Guhan",
        "U Bhattacharya",
        "R Chandra",
        "A Bera",
        "D Manocha"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "95",
      "title": "The relationship between child maltreatment and emotion recognition",
      "authors": [
        "M Koizumi",
        "H Takagishi"
      ],
      "year": "2014",
      "venue": "PloS one"
    },
    {
      "citation_id": "96",
      "title": "Avec 2016: Depression, mood, and emotion recognition workshop and challenge",
      "authors": [
        "M Valstar",
        "J Gratch",
        "B Schuller",
        "F Ringeval",
        "D Lalanne",
        "M Torres",
        "S Scherer",
        "G Stratou",
        "R Cowie",
        "M Pantic"
      ],
      "year": "2016",
      "venue": "Avec 2016: Depression, mood, and emotion recognition workshop and challenge"
    }
  ]
}