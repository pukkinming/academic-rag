{
  "paper_id": "2308.14320v1",
  "title": "Video Multimodal Emotion Recognition System For Real World Applications",
  "published": "2023-08-28T06:04:33Z",
  "authors": [
    "Sun-Kyung Lee",
    "Jong-Hwan Kim"
  ],
  "keywords": [
    "multimodal emotion recognition",
    "voice activity detection",
    "speech-to-text",
    "human-computer interaction"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This paper proposes a system capable of recognizing a speaker's utterance-level emotion through multimodal cues in a video. The system seamlessly integrates multiple AI models to first extract and pre-process multimodal information from the raw video input. Next, an end-to-end MER model sequentially predicts the speaker's emotions at the utterance level. Additionally, users can interactively demonstrate the system through the implemented interface.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotions have a significant impact on people in their daily lives. Individuals perceive and understand the emotions of others through three major modalities; facial expressions (visual), paralinguistics (acoustic) and spoken words (textual), This information is crucial for building social relationships. As videos are gaining more and more popularity as a means of communication, precisely capturing the emotions of targets using these multimodal cues is crucial for active human-computer interaction applications. With recent development of deep neural networks, various deep learning based multimodal emotion recognition (MER) models have been presented. Most of the existing works focused on research-level fusion methods of the different multimodal inputs. However, there have been only a few attempts to develop actually operational systems for real world applications  [1] .\n\nTo tackle this issue, we propose an MER system capable of capturing a speaker's emotion from a raw video of any length. Since processing the entire video at once is computationally infeasible, the system initially performs utterance-level pre-processing. Specifically, the video is segmented into multiple clips containing each utterance, and multimodal information for MER is extracted respectively. Subsequently, an end-to-end MER model sequentially takes the pre-processed unaligned visual, acoustic and textual modalities, and predicts the speaker's emotion at the utterance level. This approach enables users to trace changes in the speaker's emotions at each utterance. Additionally, by averaging all the utterance-level predictions, the overall emotions of the entire video can be observed. We also provide an interface that enables users to demonstrate the system by uploading videos. *Corresponding Author",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "End-To-End Mer Model",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Model Design",
      "text": "Previously, the majority of approaches to MER were two-stage, involving the extraction of fixed features from each modality using hand-crafted algorithms in the first stage, followed by their fusion in the second stage. One limitation of this method was that the extracted features were not fine-tuned for the task of MER. Consequently, recent works have made a progress by employing an end-to-end learning approach for joint optimization of feature extraction and fusion.\n\nOne important thing to note is that gathering a sufficient amount of MER data to train a large model can be costly. To tackle this issue, we leveraged modal-specific large pre-trained models for each modality and jointly trained them with a feature fusion component. With recent success of large pre-trained models and transfer learning, the integrated model can fully leverage modal-specific knowledge, even with a relatively small amount of MER data.\n\nTo be more specific, InceptionResNet 1 pre-trained on VG-GFace2, DistilHuBERT  [2]  and ALBERT  [3]  are chosen as backbones for visual, acoustic and textual modalities, respectively. The face images, raw audio array, and tokenized words pass through their respective backbones and 1D convolution layers (for capturing temporal information). Each embedding is then averaged to obtain representative modal features. To fuse these modalities, the averaged features are concatenated, and two linear layers are used to make the final prediction.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Training And Results",
      "text": "We trained the model on the CMU-MOSEI dataset  [4] , one of the largest MER datasets available, which was restructured by  [5] . The model is designed to take 5 face images, 10 seconds of audio and 100 tokens of words as inputs where they are either truncated or padded with 0 depending on their length. Since the dataset is multi-labeled, we optimized the model using binary cross-entropy loss and set thresholds for each emotion  [5] . These thresholds were selected to maximize the F1 score in the validation set. Finally, the learning rates of the backbones were set to be 1/10 of the entire model. This way, the backbones can gradually adapt to the task of MER without totally forgetting the modal-specific knowledge. After training, the model achieved an F1 score of 48.3 and accuracy of 72.7 on the test set, outperforming the previous state-of-the-art results  [1] .  Figure  2  shows the overall flow of the proposed MER system. For each input video, the system utilizes the Python library librosa 2  to extract the audio information, and then applies an open source voice activity detection (VAD) model to obtain the timestamps of each utterance from the audio data. Silero VAD 3  was specifically chosen for this purpose in our system. According to the timestamps information, the utterance-level acoustic modality can be easily retrieved. For the utterance-level visual modality, 5 frames are sampled from all the frames and MTCNN 1 is applied to crop face regions from them. As there is no direct way to get textual modality from the video, a speechto-text model needs to be applied. We adopted Silero Models  4 among various open source STT models. Now, with the information of the three extracted modalities, the pre-trained MER model can predict the speaker's emotion at the utterance level. By averaging all the utterance-level emotion probabilities, the overall video-level emotions can also be recognized.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Mer System",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "System Interface",
      "text": "The implemented system interface is shown in Figure  3 . In the left window, a user can upload a video. When clicking the Submit button, the video is processed following the system description provided above. Firstly, the system processes the video",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we presented a novel system for real world applications of MER in videos.The model at the core of the system is an end-to-end MER model based on large pre-trained models, which leverages the power of modal-specific knowledge to achieve state-of-the-art performance. This model is seamlessly integrated with other AI models such as VAD and STT to enable utterance-level emotion prediction for speakers in videos. Additionally, users can intuitively interact with the system through the provided system interface. Overall, we believe that our proposed system has the potential for widespread usability in diverse fields, such as healthcare, education, and entertainment.",
      "page_start": 2,
      "page_end": 2
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overall design of the proposed MER model.",
      "page": 1
    },
    {
      "caption": "Figure 2: Overall flow of the proposed MER system.",
      "page": 2
    },
    {
      "caption": "Figure 2: shows the overall flow of the proposed MER sys-",
      "page": 2
    },
    {
      "caption": "Figure 3: Overall structure of the MER system interface.",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Abstract": "",
          "2. End-to-End MER Model": "2.1. Model Design"
        },
        {
          "Abstract": "This paper proposes a system capable of recognizing a speaker's",
          "2. End-to-End MER Model": "Previously, the majority of approaches to MER were two-stage,"
        },
        {
          "Abstract": "utterance-level emotion through multimodal cues\nin a video.",
          "2. End-to-End MER Model": "involving the extraction of fixed features from each modality us-"
        },
        {
          "Abstract": "The system seamlessly integrates multiple AI models\nto first",
          "2. End-to-End MER Model": "ing hand-crafted algorithms in the first stage, followed by their"
        },
        {
          "Abstract": "extract and pre-process multimodal\ninformation from the raw",
          "2. End-to-End MER Model": "fusion in the second stage. One limitation of this method was"
        },
        {
          "Abstract": "video input. Next, an end-to-end MER model sequentially pre-",
          "2. End-to-End MER Model": "that\nthe extracted features were not fine-tuned for\nthe task of"
        },
        {
          "Abstract": "dicts the speaker's emotions at the utterance level. Additionally,",
          "2. End-to-End MER Model": "MER. Consequently, recent works have made a progress by em-"
        },
        {
          "Abstract": "users can interactively demonstrate the system through the im-",
          "2. End-to-End MER Model": "ploying an end-to-end learning approach for joint optimization"
        },
        {
          "Abstract": "plemented interface.",
          "2. End-to-End MER Model": "of feature extraction and fusion."
        },
        {
          "Abstract": "Index Terms: multimodal emotion recognition, voice activity",
          "2. End-to-End MER Model": "One important\nthing to note is that gathering a sufficient"
        },
        {
          "Abstract": "detection, speech-to-text, human-computer interaction",
          "2. End-to-End MER Model": "amount of MER data to train a large model can be costly. To"
        },
        {
          "Abstract": "",
          "2. End-to-End MER Model": "tackle this issue, we leveraged modal-specific large pre-trained"
        },
        {
          "Abstract": "",
          "2. End-to-End MER Model": "models for each modality and jointly trained them with a fea-"
        },
        {
          "Abstract": "1.\nIntroduction",
          "2. End-to-End MER Model": "ture fusion component. With recent success of large pre-trained"
        },
        {
          "Abstract": "",
          "2. End-to-End MER Model": "models and transfer\nlearning,\nthe integrated model can fully"
        },
        {
          "Abstract": "Emotions have\na\nsignificant\nimpact on people\nin their daily",
          "2. End-to-End MER Model": ""
        },
        {
          "Abstract": "",
          "2. End-to-End MER Model": "leverage modal-specific knowledge, even with a relatively small"
        },
        {
          "Abstract": "lives.\nIndividuals perceive and understand the emotions of oth-",
          "2. End-to-End MER Model": ""
        },
        {
          "Abstract": "",
          "2. End-to-End MER Model": "amount of MER data."
        },
        {
          "Abstract": "ers through three major modalities; facial expressions (visual),",
          "2. End-to-End MER Model": ""
        },
        {
          "Abstract": "",
          "2. End-to-End MER Model": "To be more specific, InceptionResNet1 pre-trained on VG-"
        },
        {
          "Abstract": "paralinguistics (acoustic) and spoken words (textual), This in-",
          "2. End-to-End MER Model": ""
        },
        {
          "Abstract": "",
          "2. End-to-End MER Model": "GFace2, DistilHuBERT [2] and ALBERT [3] are chosen as"
        },
        {
          "Abstract": "formation is crucial for building social relationships. As videos",
          "2. End-to-End MER Model": ""
        },
        {
          "Abstract": "",
          "2. End-to-End MER Model": "backbones for visual, acoustic and textual modalities,\nrespec-"
        },
        {
          "Abstract": "are gaining more and more popularity as a means of communi-",
          "2. End-to-End MER Model": ""
        },
        {
          "Abstract": "",
          "2. End-to-End MER Model": "tively. The face images, raw audio array, and tokenized words"
        },
        {
          "Abstract": "cation, precisely capturing the emotions of targets using these",
          "2. End-to-End MER Model": ""
        },
        {
          "Abstract": "",
          "2. End-to-End MER Model": "pass\nthrough their\nrespective backbones and 1D convolution"
        },
        {
          "Abstract": "multimodal cues is crucial for active human-computer interac-",
          "2. End-to-End MER Model": ""
        },
        {
          "Abstract": "",
          "2. End-to-End MER Model": "layers (for capturing temporal\ninformation). Each embedding"
        },
        {
          "Abstract": "tion applications. With recent development of deep neural net-",
          "2. End-to-End MER Model": ""
        },
        {
          "Abstract": "",
          "2. End-to-End MER Model": "is\nthen averaged to obtain representative modal\nfeatures.\nTo"
        },
        {
          "Abstract": "works, various deep learning based multimodal emotion recog-",
          "2. End-to-End MER Model": ""
        },
        {
          "Abstract": "",
          "2. End-to-End MER Model": "fuse these modalities,\nthe averaged features are concatenated,"
        },
        {
          "Abstract": "nition (MER) models have been presented. Most of the existing",
          "2. End-to-End MER Model": ""
        },
        {
          "Abstract": "",
          "2. End-to-End MER Model": "and two linear layers are used to make the final prediction."
        },
        {
          "Abstract": "works focused on research-level\nfusion methods of\nthe differ-",
          "2. End-to-End MER Model": ""
        },
        {
          "Abstract": "ent multimodal\ninputs. However,\nthere have been only a few",
          "2. End-to-End MER Model": ""
        },
        {
          "Abstract": "attempts to develop actually operational systems for real world",
          "2. End-to-End MER Model": ""
        },
        {
          "Abstract": "applications [1].",
          "2. End-to-End MER Model": ""
        },
        {
          "Abstract": "To tackle\nthis\nissue, we propose\nan MER system capa-",
          "2. End-to-End MER Model": ""
        },
        {
          "Abstract": "ble of capturing a speaker's emotion from a raw video of any",
          "2. End-to-End MER Model": ""
        },
        {
          "Abstract": "length.\nSince processing the entire video at once is computa-",
          "2. End-to-End MER Model": ""
        },
        {
          "Abstract": "tionally infeasible, the system initially performs utterance-level",
          "2. End-to-End MER Model": ""
        },
        {
          "Abstract": "pre-processing. Specifically, the video is segmented into multi-",
          "2. End-to-End MER Model": ""
        },
        {
          "Abstract": "ple clips containing each utterance, and multimodal information",
          "2. End-to-End MER Model": ""
        },
        {
          "Abstract": "for MER is extracted respectively. Subsequently, an end-to-end",
          "2. End-to-End MER Model": ""
        },
        {
          "Abstract": "MER model sequentially takes the pre-processed unaligned vi-",
          "2. End-to-End MER Model": ""
        },
        {
          "Abstract": "sual, acoustic and textual modalities, and predicts the speaker's",
          "2. End-to-End MER Model": ""
        },
        {
          "Abstract": "emotion at\nthe utterance level. This approach enables users to",
          "2. End-to-End MER Model": ""
        },
        {
          "Abstract": "trace changes in the speaker's emotions at each utterance. Ad-",
          "2. End-to-End MER Model": ""
        },
        {
          "Abstract": "ditionally, by averaging all\nthe utterance-level predictions,\nthe",
          "2. End-to-End MER Model": ""
        },
        {
          "Abstract": "overall emotions of the entire video can be observed. We also",
          "2. End-to-End MER Model": ""
        },
        {
          "Abstract": "provide an interface that enables users to demonstrate the sys-",
          "2. End-to-End MER Model": ""
        },
        {
          "Abstract": "",
          "2. End-to-End MER Model": "Figure 1: Overall design of the proposed MER model."
        },
        {
          "Abstract": "tem by uploading videos.",
          "2. End-to-End MER Model": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2.2. Training and Results": "We trained the model on the CMU-MOSEI dataset [4], one of"
        },
        {
          "2.2. Training and Results": "the largest MER datasets available, which was restructured by"
        },
        {
          "2.2. Training and Results": "[5]. The model is designed to take 5 face images, 10 seconds of"
        },
        {
          "2.2. Training and Results": "audio and 100 tokens of words as inputs where they are either"
        },
        {
          "2.2. Training and Results": "truncated or padded with 0 depending on their\nlength.\nSince"
        },
        {
          "2.2. Training and Results": "the dataset\nis multi-labeled, we optimized the model using bi-"
        },
        {
          "2.2. Training and Results": "nary cross-entropy loss and set thresholds for each emotion [5]."
        },
        {
          "2.2. Training and Results": "These thresholds were selected to maximize the F1 score in the"
        },
        {
          "2.2. Training and Results": "validation set. Finally, the learning rates of the backbones were"
        },
        {
          "2.2. Training and Results": "set\nto be 1/10 of\nthe entire model.\nThis way,\nthe backbones"
        },
        {
          "2.2. Training and Results": "can gradually adapt\nto the task of MER without\ntotally forget-"
        },
        {
          "2.2. Training and Results": "ting the modal-specific knowledge. After\ntraining,\nthe model"
        },
        {
          "2.2. Training and Results": "achieved an F1 score of 48.3 and accuracy of 72.7 on the test"
        },
        {
          "2.2. Training and Results": "set, outperforming the previous state-of-the-art results [1]."
        },
        {
          "2.2. Training and Results": "3. MER System"
        },
        {
          "2.2. Training and Results": ""
        },
        {
          "2.2. Training and Results": "3.1.\nSystem Design"
        },
        {
          "2.2. Training and Results": ""
        },
        {
          "2.2. Training and Results": ""
        },
        {
          "2.2. Training and Results": ""
        },
        {
          "2.2. Training and Results": ""
        },
        {
          "2.2. Training and Results": ""
        },
        {
          "2.2. Training and Results": ""
        },
        {
          "2.2. Training and Results": ""
        },
        {
          "2.2. Training and Results": ""
        },
        {
          "2.2. Training and Results": ""
        },
        {
          "2.2. Training and Results": ""
        },
        {
          "2.2. Training and Results": ""
        },
        {
          "2.2. Training and Results": ""
        },
        {
          "2.2. Training and Results": ""
        },
        {
          "2.2. Training and Results": ""
        },
        {
          "2.2. Training and Results": "Figure 2: Overall flow of the proposed MER system."
        },
        {
          "2.2. Training and Results": ""
        },
        {
          "2.2. Training and Results": ""
        },
        {
          "2.2. Training and Results": "Figure 2 shows the overall flow of the proposed MER sys-"
        },
        {
          "2.2. Training and Results": ""
        },
        {
          "2.2. Training and Results": "tem. For each input video, the system utilizes the Python library"
        },
        {
          "2.2. Training and Results": ""
        },
        {
          "2.2. Training and Results": "librosa2\nto extract\nthe audio information, and then applies an"
        },
        {
          "2.2. Training and Results": ""
        },
        {
          "2.2. Training and Results": "open source voice activity detection (VAD) model to obtain the"
        },
        {
          "2.2. Training and Results": ""
        },
        {
          "2.2. Training and Results": "timestamps of each utterance from the audio data. Silero VAD3"
        },
        {
          "2.2. Training and Results": "was specifically chosen for this purpose in our system. Accord-"
        },
        {
          "2.2. Training and Results": "ing to the timestamps information,\nthe utterance-level acoustic"
        },
        {
          "2.2. Training and Results": "modality can be easily retrieved.\nFor\nthe utterance-level vi-"
        },
        {
          "2.2. Training and Results": ""
        },
        {
          "2.2. Training and Results": "sual modality, 5 frames are sampled from all\nthe frames and"
        },
        {
          "2.2. Training and Results": ""
        },
        {
          "2.2. Training and Results": "MTCNN1 is applied to crop face regions from them. As there is"
        },
        {
          "2.2. Training and Results": "no direct way to get textual modality from the video, a speech-"
        },
        {
          "2.2. Training and Results": ""
        },
        {
          "2.2. Training and Results": "to-text model needs to be applied. We adopted Silero Models4"
        },
        {
          "2.2. Training and Results": "among various open source STT models. Now, with the infor-"
        },
        {
          "2.2. Training and Results": "mation of the three extracted modalities,\nthe pre-trained MER"
        },
        {
          "2.2. Training and Results": ""
        },
        {
          "2.2. Training and Results": "model can predict\nthe speaker's emotion at\nthe utterance level."
        },
        {
          "2.2. Training and Results": "By averaging all\nthe utterance-level emotion probabilities,\nthe"
        },
        {
          "2.2. Training and Results": ""
        },
        {
          "2.2. Training and Results": "overall video-level emotions can also be recognized."
        },
        {
          "2.2. Training and Results": ""
        },
        {
          "2.2. Training and Results": ""
        },
        {
          "2.2. Training and Results": "3.2.\nSystem Interface"
        },
        {
          "2.2. Training and Results": ""
        },
        {
          "2.2. Training and Results": "The implemented system interface is shown in Figure 3.\nIn the"
        },
        {
          "2.2. Training and Results": ""
        },
        {
          "2.2. Training and Results": "left window, a user can upload a video. When clicking the Sub-"
        },
        {
          "2.2. Training and Results": ""
        },
        {
          "2.2. Training and Results": "mit button, the video is processed following the system descrip-"
        },
        {
          "2.2. Training and Results": ""
        },
        {
          "2.2. Training and Results": "tion provided above.\nFirstly,\nthe system processes\nthe video"
        },
        {
          "2.2. Training and Results": ""
        },
        {
          "2.2. Training and Results": ""
        },
        {
          "2.2. Training and Results": "2https://librosa.org/doc/latest/index.html"
        },
        {
          "2.2. Training and Results": ""
        },
        {
          "2.2. Training and Results": "3https://github.com/snakers4/silero-vad"
        },
        {
          "2.2. Training and Results": ""
        },
        {
          "2.2. Training and Results": "4https://github.com/snakers4/silero-models"
        }
      ],
      "page": 2
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Fv2es: A fully end2end multimodal system for fast yet effective video emotion recognition inference",
      "authors": [
        "Q Wei",
        "X Huang",
        "Y Zhang"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Broadcasting"
    },
    {
      "citation_id": "3",
      "title": "Distilhubert: Speech representation learning by layer-wise distillation of hidden-unit bert",
      "authors": [
        "H.-J Chang",
        "S.-W Yang",
        "H.-Y Lee"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "4",
      "title": "Albert: A lite bert for self-supervised learning of language representations",
      "authors": [
        "Z Lan",
        "M Chen",
        "S Goodman",
        "K Gimpel",
        "P Sharma",
        "R Soricut"
      ],
      "year": "2019",
      "venue": "Albert: A lite bert for self-supervised learning of language representations",
      "arxiv": "arXiv:1909.11942"
    },
    {
      "citation_id": "5",
      "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "A Zadeh",
        "P Liang",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "6",
      "title": "Multimodal end-toend sparse model for emotion recognition",
      "authors": [
        "W Dai",
        "S Cahyawijaya",
        "Z Liu",
        "P Fung"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    }
  ]
}