{
  "paper_id": "2304.06496v2",
  "title": "Eegmatch: Learning With Incomplete Labels For Semi-Supervised Eeg-Based Cross-Subject Emotion Recognition",
  "published": "2023-03-27T12:02:33Z",
  "authors": [
    "Rushuang Zhou",
    "Weishan Ye",
    "Zhiguo Zhang",
    "Yanyang Luo",
    "Li Zhang",
    "Linling Li",
    "Gan Huang",
    "Yining Dong",
    "Yuan-Ting Zhang",
    "Zhen Liang"
  ],
  "keywords": [
    "Electroencephalography (EEG)",
    "Emotion Recognition",
    "Semi-Supervised Learning",
    "Multi-Domain Adaptation",
    "Transfer Learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Electroencephalography (EEG) is an objective tool for emotion recognition and shows promising performance. However, the label scarcity problem is a main challenge in this field, which limits the wide application of EEG-based emotion recognition. In this paper, we propose a novel semisupervised transfer learning framework (EEGMatch) to leverage both labeled and unlabeled EEG data. First, an EEG-Mixup based data augmentation method is developed to generate more valid samples for model learning. Second, a semi-supervised twostep pairwise learning method is proposed to bridge prototypewise and instance-wise pairwise learning, where the prototypewise pairwise learning measures the global relationship between EEG data and the prototypical representation of each emotion class and the instance-wise pairwise learning captures the local intrinsic relationship among EEG data. Third, a semi-supervised multi-domain adaptation is introduced to align the data representation among multiple domains (labeled source domain, unlabeled source domain, and target domain), where the distribution mismatch is alleviated. Extensive experiments are conducted on two benchmark databases (SEED and SEED-IV) under a crosssubject leave-one-subject-out cross-validation evaluation proto-",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "E MOTION recognition using electroencephalography (EEG) signals is a rapid and exciting growing field that recently has attracted considerable interest in affective computing and brain-computer interface  [1] . The majority of the existing EEG-based emotion recognition studies are based on traditional supervised learning, in which all the data in the training set are well labeled. Unfortunately, labeled data are typically difficult, expensive, and time-consuming to obtain, requiring a tremendous human effort. On the other hand, unlabeled data are easier to collect in contrast to labeled data. Compared with supervised learning, semi-supervised learning, which uses both labeled and unlabeled data, could involve less human effort, solve the modeling issue with small labeled training data, and create models that are more generalizable. However, how to design algorithms that efficiently and effectively leverage the presence of both labeled and unlabeled data for improving model learning is now a crucial challenge in semi-supervised EEG-based emotion recognition.\n\nIn the existing supervised EEG-based emotion recognition studies, transfer learning methods have been widely adopted to minimize the individual differences in EEG signals  [2] -  [6] , where the labeled data from the training subjects are considered as the source domain and the unknown data from the testing subjects are treated as the target domain. By aligning the joint distribution of the source and target domain for approximately satisfying the independent and identically distributed (IID) assumption, the individual differences in EEG signals could be alleviated and the model performance on the target domain could be enhanced  [2] ,  [7] . More introductions about the existing EEG-based emotion recognition studies with transfer learning methods are presented in Section II. 1 These authors contributed equally to this work.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Arxiv:2304.06496V2 [Eess.Sp] 30 Aug 2024",
      "text": "In order to achieve reliable model training, the current EEG-based emotion recognition studies using transfer learning methods often need a substantial amount of labeled data in the source domain. If a majority number of data in the source domain are unlabeled, the model training performance would be severely constrained. Semi-supervised learning framework provides great potential to address this problem by taking advantage of a combination of a small number of labeled data and a large amount of unlabeled data and enhancing learning tasks. For example, Zhang et al.  [8]  utilized a holistic semi-supervised learning method  [9]  for EEG representation and showed the superiority of the semi-supervised learning framework when the labeled data are scarce. Further, Zhang et al.  [10]  combined the holistic semi-supervised learning approaches and domain adaptation methods to improve the model performance on cross-subject emotion recognition. Still, there are three limitations in the literature, that should be thoroughly investigated and resolved in future studies on semi-supervised EEG-based cross-subject emotion recognition. (1) Inappropriate data augmentation for EEG signals. Previous studies utilized the Mixup method  [11]  for EEG data augmentation, which ignored the non-stationarity property  [12]  of EEG signals. The EEG samples collected from different subjects at different trials are not IID, which contradicts the main tenet of the Mixup method. (2) Inefficient learning framework for incomplete label task. Previous works were based on a pointwise learning framework that relied on abundant and precise labels for model training and were impractical under label scarcity conditions  [13] .  (3)  Ignoring the distribution mismatch between labeled and unlabeled source data. Previous models only considered the distribution mismatch between the labeled source data and the unknown target data  [10] ,  [14] . The distribution alignment for the unlabeled source data was disregarded.\n\nTo address the aforementioned issues, we propose a novel semi-supervised learning framework (EEGMatch) for EEG-based cross-subject emotion recognition with incomplete labels. The proposed EEGMatch is composed of three main modules: EEG-Mixup based data augmentation, semisupervised two-step pairwise learning (prototype-wise and instance-wise), and semi-supervised multi-domain adaptation. The main contributions and novelties of the present study are listed below.\n\n• An appropriate EEG data augmentation strategy is developed. On the basis of the defined EEG samples that meet the IID assumption, an EEG-Mixup based data augmentation method is developed to generate proper augmented samples in both the source and target domains, establishing an effective augmentation pipeline for EEG signals.\n\n• An efficient learning framework with incomplete labels is proposed. A semi-supervised two-step pairwise learning framework, including prototype-wise and instance-wise, is proposed for proper feature learning with incomplete labels. Here, the semi-supervised prototype-wise pairwise learning explores global prototypical representation for each emotion category in a semi-supervised manner, and the semi-supervised instance-wise pairwise learning captures the local intrinsic structures of EEG samples, improving the model performance in label scarcity conditions. • A distribution alignment between labeled and unlabeled source data is incorporated. A semi-supervised multi-domain adaptation method is designed to jointly minimize the distribution shift among the labeled source data, unlabeled source data, and unknown target data.\n\nIt could significantly enhance model generalization, as demonstrated by the derived error bound.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work A. Eeg-Based Emotion Recognition",
      "text": "In the past ten years, the feasibility and stability of using EEG signals to identify individuals' emotional states have been widely explored  [15] -  [23] . In 2013, Duan et al. extracted EEG features to represent the characteristics associated with emotional states and used the support vector machine (SVM) for emotion classification  [15] . With the rapid advancement of deep learning technologies, more and more EEG-based emotion recognition studies have been developed based on deep neural network structures. For example, Zhang et al. proposed cascade and parallel convolutional recurrent neural networks to learn effective emotion-related EEG patterns  [19] . Song et al. introduced dynamical graph convolutional neural networks to model the multichannel EEG features and dynamically capture the intrinsic relationship among different channels  [24] . To extract dynamic multilevel spatial information among EEG channels, Ye et al. proposed a hierarchical dynamic graph convolutional network (HD-GCN)  [25] . Li et al. developed a multiple emotion-related spatial model to extract discriminative graph topologies in EEG brain networks and perform efficient emotion recognition  [26] . Niu et al. developed a novel deep residual neural network with a combination of brain network analysis and channel-spatial attention mechanism, which enabled an effective classification of multiple emotion states  [27] . Aiming at incorporating the spatial and temporal information from EEG signals, Cheng et al. developed a hybrid network (HN-DGTS) by combining temporal self-attention and dynamic graph convolution  [28] . The above models are robust in subject-dependent (within-subject) emotion recognition, where the training and testing data are sampled from the same subject. However, due to the individual differences in EEG signals, the model performance would deteriorate on the subject-independent (cross-subject) emotion-recognition tasks, where the training and testing data are sampled from different subjects  [29] -  [31] .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Eeg-Based Cross-Subject Emotion Recognition With Transfer Learning Methods",
      "text": "To tackle the individual differences in the EEG signals, the existing studies have introduced transfer learning methods to eliminate the feature distribution discrepancy extracted from different subjects and improve the model stability on EEG-based cross-subject emotion recognition tasks  [4] -  [6] ,  [29] ,  [32] -  [34] . Based on non-deep transfer learning methods such as transfer component analysis (TCA) and transductive parameter transfer (TPT), Zheng et al.  [29]  developed personalized emotion recognition models to improve the crosssubject model performance, where the proposed transfer learning framework achieved a mean accuracy of 76.31% (the obtained mean accuracy was only 56.73% when a traditional non-transfer learning framework was adopted). Based on deep transfer learning methods, Jin et al.  [32]  proposed an EEGbased emotion recognition model using the domain adversarial neural network (DANN)  [35] . It was found that the models using the deep transfer learning framework outperformed those using the non-deep transfer learning methods, with the mean accuracy rising from 76.31% to 79.19%. Further, a series of enhanced deep transfer learning frameworks based on DANN structure has been developed for cross-subject emotion recognition. For example, Li et al.  [33]  proposed a domain adaptation method to minimize the distribution shift and generalize the emotion recognition models across subjects. He et al.  [6]  combined the adversarial discriminator and the temporal convolutional networks (TCNs) to further enhance the distribution matching in DANN. Huang et al.  [5]  developed a generator-based domain adaptation model with knowledge free mechanism to maintain the neurological information during the feature alignment. Xu et al.  [4]  utilized the graph attention adversarial training and biological topology information to construct the domain adversarial graph attention model (DAGAM). Peng et al.  [36]  proposed a joint feature adaptation and graph adaptive label propagation method (JAGP) for performance enhancement. In comparison with the original DANN structure, these models achieved superior cross-subject emotion recognition results. In order to build reliable models, each of the aforementioned models needs a significant amount of labeled EEG data in the source domain. However, gathering enough good-quality labeled EEG data is always challenging and time-consuming. It is important to develop new transfer learning strategies for efficient crosssubject emotion recognition from a few labeled data and sufficient unlabeled data. This new setting can be named as semisupervised EEG-based cross-subject emotion recognition.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "C. Semi-Supervised Eeg-Based Cross-Subject Emotion Recognition",
      "text": "In contrast to prior studies of supervised EEG-based crosssubject emotion recognition, Dan et al.  [37]  proposed a possibilistic clustering-promoting semi-supervised model for EEG-based emotion recognition, with less requirement on the labeled training data. Due to the less consideration of the distribution shift between the source and target domains, the cross-subject emotion recognition performance was constrained. To reduce the distribution mismatch between the source and target domains, Zhang et al.  [10]  proposed a semisupervised EEG-based emotion recognition model (PARSE) based on DANN and pairwise representation alignment. The experimental results showed that the proposed method can greatly improve the semi-supervised EEG-based cross-subject emotion recognition performance.\n\nHowever, there are two main limitations of the aforementioned semi-supervised EEG-based cross-subject emotion recognition studies. (1) inappropriate selection of labeled and unlabeled data. The previous studies randomly selected the labeled and unlabeled data in terms of the 1-second segments instead of trials. It is noted that, in the benchmark databases (SEED  [16]  and SEED-IV  [38] ), one trial was composed of a number of 1-second segments, and the same emotional label was assigned to all of the 1-second segments from the same trial. Therefore, a random selection of labeled and unlabeled data in terms of 1-second segments would lead to information leaking and improper model evaluation. Considering the data continuity, we suggest the selection of labeled and unlabeled data in terms of trials to remove the effect of random seeds. Specifically, for each subject in the training set, we select the first N trials as the labeled source domain (S) (both EEG data and labels are available) and the remaining trials as the unlabeled source domain (U) (only EEG data is available). For the subject(s) in the testing set (the target domain (T)), only EEG data is available. (2) inappropriate usage of unlabeled source data. Both Dan et al.  [37]  and Zhang et al.  [10]  did not use the unlabeled source data for model training, which led to a significant loss of data information. Using both labeled and unlabeled source data for modeling would encourage the model to perform more reliably and generalize better to unknown data. In the present study, we propose to jointly utilize both labeled and unlabeled source data for model training and develop a semi-supervised multidomain adaption method to address the distribution alignment issue among S, U, and T.   1 , the proposed model includes three main modules: EEG-Mixup based data augmentation, semisupervised two-step pairwise learning (prototype-wise and instance-wise), and semi-supervised multi-domain adaptation, with three loss functions (semi-supervised pairwise learning loss in the source domain, unsupervised pairwise learning loss in the target domain, and multi-domain adversarial loss). In the EEG-Mixup based data augmentation, the augmented data, A (S), A (U), and A (T), are generated in the three domains (S, U, and T) separately. In the semi-supervised two-step pairwise learning, prototype-wise pairwise learning and instance-wise pairwise learning are involved. For the prototype-wise pairwise learning, the prototypical representation of each emotion category is learned based on the S and U, and the pairwise relationships between all EEG samples and the prototypical representation of different emotion categories are measured. For the instance-wise pairwise learning, considering Y u and Y t are unknown during model training, a semi-supervised pairwise learning method is developed to reconstruct the pairwise relationship within S and U, and an unsupervised pairwise learning method is adopted to reconstruct the pairwise relationships in T. In the semi-supervised (3) Semi-supervised multi-domain adaptation: align the distribution shift among the labeled source domain (S), the unlabeled source domain (U), and the target domain (T). Here, L s pair , L t pair , L disc are the semi-supervised pairwise learning loss in the source domain, the unsupervised pairwise learning loss in the target domain, and the domain discriminator loss, defined in Eq. 14, Eq. 17, and Eq. 25, respectively. multi-domain adaptation, the characterized EEG features in the three domains (f (X s ), f (X u ), and f (X t )) are trained to be as indistinguishable as possible, and the distribution shift among the three domains are aligned. More details about each module in the proposed EEGMatch are presented below.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iii. Methodology",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Eeg-Mixup Based Data Augmentation",
      "text": "The traditional Mixup method  [9]  has achieved great success in the computer vision field, which requires that the pair of samples used for data augmentation should follow the joint distribution  [11] . However, in the EEG-based emotion recognition tasks, the EEG signals collected from different subjects at different trials do not satisfy the IID assumption. In other words, a direct application of the Mixup method on the emotional EEG signals might not be appropriate.\n\nTo tackle the data augmentation problem on the EEG signals, we develop a novel EEG-Mixup method to fit the data in accordance with the IID assumption. The proposed EEG-Mixup method is mainly composed of two parts.(1) Valid data sampling: sampling EEG data that follows the joint distribution. Based on the prior findings  [24] ,  [29] ,  [39]  that the EEG data from the same trial of the same subject follow the IID assumption, the valid EEG data for data augmentation is defined as the emotional EEG signals from the same subject at the same trial, without overlapping. Take the labeled source domain (S) as an example. The valid data sampling could be represented as D s ps,qs = {(x s n , y s n )} N s ps ,qs n=1 , where N s ps,qs is the total sample size of p s subject at q s trial. (2) EEG data augmentation: generating augmented data for the three domains. We conduct data augmentation on the emotional EEG signals which are randomly selected from the valid data. For example, (x s i , y s i ) and x s j , y s j are the two data pairs randomly selected from D s ps,qs . The corresponding augmented data (x s z , y s z ) is computed as\n\nwhere ω ∼ Beta(α, α), and α is a shape parameter of the Beta distribution. In most of the previous studies  [10] ,  [16] ,  [38] -  [41] , the EEG samples from the same trial share the same emotional label (y s z = y s i = y s j ). Then, the augmented data of D s ps,qs could be termed as\n\nwhere Z s ps,qs is the number of augmented samples in A D s ps,qs . Similarly, the EEG-Mixup based data augmentation is also conducted in the other two domains (U and T), and the corresponding augmented data are denoted as A D u pu,qu and A D t pt,qt . It is noted that the emotional labels (Y u and Y t ) in the U and T domains are unknown during the training process, the data augmentation for U and T domains is only applied on the EEG data (X u and X t ), as\n\nZ u pu,qu and Z t pt,qt are the number of augmented samples in A D u pu,qu and A D t pt,qt , respectively. For simplicity, A D s ps,qs , A D u pu,qu , and A D t pt,qt are also represented as A (S), A (U), and A (T) below. The final obtained data for the following data processing is a concatenation of the original data and the augmented data in each domain, given as {S, A (S)}, {U, A (U)}, and {T, A (T)}.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Semi-Supervised Two-Step Pairwise Learning",
      "text": "A semi-supervised two-step pairwise learning framework is proposed to enhance the feature cohesiveness of EEG samples within the same emotion class and enlarge the feature separability of EEG samples between different emotion classes.\n\n1) Semi-supervised prototype-wise pairwise learning: The first step is conducted in a prototype-wise manner. The superiority of prototypical representation in feature learning has been evidenced in various research fields  [42] -  [45] . Specifically, a prototypical representation is an embedding that encodes the most representative information for a given class or relation, serving as the centroid of the features associated with the class  [34] ,  [46] . The existing prototypical learning method is conducted in a supervised manner, which incorporates the label information for prototypical representation estimation. For example, for total c emotion classes, the supervised prototypical representation P is computed as\n\nwhere X = [x 1 ; x 2 , ...;\n\nx n ] and Y = [y 1 ; y 2 , ...; y n ] are the EEG data and the corresponding emotional labels (one-hot representation) in the current mini-batch, respectively. f (•) is a feature extractor with the parameter θ f , and f (X) is a N B × M feature matrix (N B is the batch size for stochastic gradient descent and M is the feature dimensionality). P = [µ 1 ; ...; µ i ; ...; µ c ] and µ i is the prototypical representation of the ith emotion class. Proof see Supplementary Materials: Appendix A.\n\nDuring the supervised prototypical learning, only the labeled samples in the source domain would be considered in the calculation of P . However, the sample size of the labeled source data could be very limited in most cases. To make full use of both labeled and unlabeled data in the source domain for prototypical learning, we propose a novel semi-supervised prototypical learning method by generating the corresponding pseudo labels for the unlabeled data in the source domain as\n\nwhere Cls (f (X u )) is the output prediction of the emotion classifier Cls(•). Considering the superiority of pairwise learning against traditional pointwise learning [10],  [34] ,  [47] ,  [48] , the emotion classifier Cls(•) is defined to estimate the pairwise similarity as\n\nwhere w(•) is a bilinear transformation, x u is an EEG sample from X u . B ∈ R d×d is a trainable bilinear transformation matrix and is not restricted by symmetry or positive definiteness. l i is the calculated similarity between the data feature f (x u ) and ith prototypical representation (µ i ). Then, the predicted pseudo label for x u could be estimated as\n\nFurther, to minimize the entropy in the predicted pseudo label, we adopt a Sharpen function  [9]  to adjust the \"temperature\" of the estimated class distribution Cls (f (x u )) as\n\nwhere η is a hyperparameter for \"temperature\" controlling. Then, based on the labeled source data {X s , Y s } and the unlabeled source data {X u , Ŷu }, Eq. 6 could be rewritten as\n\n(11) In the implementation, for each mini-batch, {X s , Y s } and {X u , -} are randomly sampled from {S, A (S)} and {U, A (U)} defined in Section III-A. Ŷu is the estimated pseudo labels calculated in Eq. 7. Before model training, the prototype matrix P s * is initialized using the Eq. 6 with only labeled source data. Note that the proposed method (Eq.11) can be considered as a special case of supervised prototypical learning (Eq.6). The main difference is that the proposed method preserves the unlabeled data for prototype computation, while the supervised method excludes them. The samples in the target domain ({T, A (T)}) are not used in the prototypical learning to avoid overfitting.\n\n2) Semi-supervised instance-wise pairwise learning: The second step is conducted in an instance-wise manner. To capture the inherent relationships among the EEG data, we estimate the pairwise similarity among samples.\n\nThe instance-to-instance similarity matrix R s p in the source domain is calculated as\n\nwhere x s * i and x s * j are the EEG data sampled from\n\nis defined in Eq. 9. For the labeled source data X s , the groundtruth is given as Y s . For the unlabeled data X u in the source domain, the estimated pseudo labels are Ŷu given in Eq. 7. Then, the groundtruth similarity matrix R s in the source domain is calculated as\n\nwhere Y s * = Y s ; Ŷu , N (•) is a normalization function which normalizes the column vectors of a given matrix to unit vectors. Then, the instance-wise pairwise learning loss in the source domain is defined as\n\nwhere\n\nN indicates the total sample size of the labeled and unlabeled data in the source domain.\n\nFor the target domain, the instance-to-instance similarity matrix R t p is estimated as\n\nwhere x t i and x t j are sampled from the target domain data X t . Cls (f (•)) is defined in Eq. 9. The corresponding groudtruth similarity matrix R t is based on a dynamic thresholding method, given as\n\nwhere τ h and τ l are the defined dynamic upper and lower thresholds, respectively. Then, the instance-wise pairwise learning loss in the target domain is defined as\n\nwhere L t is a N t × N t pairwise loss matrix, given as\n\nM is a N t × N t mask matrix to define the valid pairs involved in the loss calculation, given as\n\nN t is the total sample size of the unlabeled target domain samples X t in the current mini-batch. In other words, for the instance-to-instance similarity relationship that falls into the range of τ l ≤ R t p (i, j) < τ h , the pair relationship x t i , x t j could be considered as invalid and would be excluded in the loss calculation at this round. In order to gradually include more samples for model training, the dynamic thresholds (τ h and τ l ) are set in a progressive learning strategy, as\n\nwhere τ t h and τ t l are the upper and lower thresholds at the current training epoch t, respectively. M axepoch is the maximum training epoch. τ 0 h and τ 0 l are the initial values for the upper and lower thresholds.\n\nOverall, the objective function of the instance-wise pairwise learning is a combination of the instance-wise pairwise learning loss in the source domain (Eq. 14) and the instance-wise pairwise learning loss in the target domain (Eq. 17), given as\n\nwhere P s * T P s * -I F is a penalty term that is applied to avoid redundant feature extraction in the prototypical learning  [45] , and ∥•∥ F is the Frobenius norm.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Semi-Supervised Multi-Domain Adaptation",
      "text": "The traditional supervised domain adaptation methods on EEG data were conducted based on DANN  [35] , where the feature distribution in the source and target domains were aligned  [39] ,  [41] ,  [49] -  [52] . However, in the semi-supervised learning, treating all the labeled and unlabeled source data as a single domain would increase the difficulty of adaptation  [40] ,  [53] . To further eliminate the distribution mismatch among different domains, we introduce a novel semi-supervised multidomain adaptation method in this paper to align the feature representation among the labeled source domain (S), the unlabeled source domain (U), and the target domain (T). Through the semi-supervised multi-domain adaptation, the feature distribution discrepancy among the above three domains could be mitigated, and the generalization ability of f (•) and Cls(•) could be enhanced. The existing multi-source multi-domain adaptation methods  [40] ,  [53] ,  [54]  and domain generalization strategies  [55]  assumed that the label information is complete in the source domain, which is not practical for real-world label scarcity applications. On the contrary, the proposed semi-supervised multi-domain adaptation method assumes that the label information is incomplete in the source domain to address the limitation of the existing methods.\n\nThe theoretical motivation of the proposed semi-supervised multi-domain adaptation method is first presented. In the existing domain adaptation methods for only two domains (labeled source domain S and target domain T), the target error ϵ T (h) could be bounded as  [56]\n\nwhere δ represents the difference in labeling functions across the two domains and is typically assumed to be small under the covariate shift assumption  [57] . ϵ S (h) is the source error based on a given classification hypothesis h characterized by the source classifier. d H (D S , D T ) corresponds to the Hdivergence introduced in  [58] , which measures the divergence between the source domain distribution D S and the target domain distribution D T and could be estimated directly from the error of a binary classifier trained to distinguish the domains  [59] . Alternatively, an accurate estimation of the source error and the H-divergence is important for the optimization of the target error.\n\nHowever, in the semi-supervised learning, an accurate estimation of the source error and the H-divergence would be extremely challenging with only a few labeled source data. To tackle this issue, we suggest using both labeled and unlabeled samples in the source domain for the estimation of the source error and the H-divergence. In the proposed semi-supervised multi-domain adaptation method, the source domain is given as S * = {S, U}, which is composed of both the labeled source data (S) and the unlabeled source data (U). The convex hull Λ S * π of S * π is a set of mixture distributions, given as\n\nwhere D S * π is a distribution computed by the weighted summation of the labeled source domain distribution D S and the unlabeled source domain distribution D U . π S and π U are the corresponding weights, belonging to ∆ 1 (a 1-th dimensional simplex). For the target domain T, D T is the closest element to D T within Λ S * π , which can be calculated as\n\nThen, the generalization upper-bound for the target error ϵ T (h) could be derived using the previous domain adaptation methods  [59] -  [61] .\n\nTheorem 1: For a given classification hypothesis h (∀h ∈ H), the target error ϵ T (h) is bounded as\n\nwhere ϵ S (h) is the error of the labeled source domain and d H (•) represents the H-divergence between the given domains. Proof see Supplementary Materials Appendix B. Under an extreme case where all the samples in the source domain are labeled (U is empty), the bound in Theorem 1 is equal to the bound given in Eq. 21. Next, we will show how to optimize the target error empirically in the proposed semi-supervised multi-domain adaptation method. For an ideal joint hypothesis across S, U, and T domains, the second term (min\n\nin Theorem 1 are assumed to be small under the covariate shift assumption  [56] ,  [57] ,  [60] ,  [62] .\n\nEmpirical minimization of the H-divergence. Based on Theorem 1, it is found that a minimization of the H-divergence among the domains S, U and T is critical for the optimization of the target error ϵ T (h). Previous studies have proved that the empirical H-divergence can be approximated by the classification error of the domain discriminator  [35] ,  [59] . In other words, the minimization of the H-divergence can be achieved by maximizing the domain discriminator loss via adversarial training. Considering the distribution shift among the given three domains, we re-define the domain discriminator loss as\n\nwhere p (x) is a one-hot domain label, referring to the belonging domain of the given EEG sample x. d(•) is a 3-class domain discriminator with parameter θ d , which outputs the domain prediction d(f (x)).\n\nEmpirical minimization of the source error ϵ S (h). Instead of using the pointwise learning loss (e.g. cross-entropy loss) in the existing studies, we introduce to minimize the instance-wise pairwise learning loss as defined in Eq. 20.\n\nEmpirical minimization of the target error ϵ T (h). The final objective loss of the proposed EEGMatch is defined as\n\nwhere L pair (θ f , B) is the instance-wise pairwise learning loss given in Eq. 20, and L disc (θ f , θ d ) is the domain discriminator loss defined in Eq. 25. λ is a balanced hyperparameter to ensure the stability of adversarial training  [35] , defined as\n\nwhere ξ is a factor related to the training epoch, which is equal to the ratio of the current epoch to the maximum epoch.\n\nThe overview of the training details of the EEGMatch is shown in the Supplementary Materials Algorithm S1. During the training process of the feature extractor f (•) and the domain discriminator d (•), the model is optimized with a gradient reversal layer (GRL)  [35] . The GRL layer can realize the adversarial training by reversing the gradient passed backward from d (•) to f (•). In other words, based on the empirical minimization of the H-divergence (maximizing the domain discriminator loss L disc (θ f , θ d )) and the source error ϵ S (h) (minimizing the pairwise loss L pair (θ f , B)), the target error ϵ T (h) could be gradually minimized by decreasing the upper-bound in the training process as stated in Theorem 1.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Iv. Experimental Results",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "A. Benchmark Databases",
      "text": "To evaluate the effectiveness of the proposed EEGMatch, we conduct extensive experiments on three well-known public emotional EEG databases: SEED  [16] , SEED-IV  [38]  and SEED-V  [63] . In the SEED database, a total of three emotions (negative, neutral, and positive) were elicited using 15 movie clips, and the simultaneous EEG signals under the three emotional states were recorded from 15 subjects (7 males and 8 females) using the 62-channel ESI Neuroscan system. In the SEED-IV database, a total of four emotions (happiness, sadness, fear, and neutral) were elicited using 24 movie clips, and the simultaneous EEG signals under the four emotional states were also recorded from 15 subjects (7 males and 8 females) using the 62-channel ESI Neuroscan system. Each subject attended three separate sessions, and each session included 24 trials corresponding to 24 different movie clips. In the SEED-V database, a total of five emotions (happiness, sadness, fear, disgust, and neutral) were elicited using 15 movie clips, and the simultaneous EEG signals under the five emotions were recorded from 16 subjects (6 males and 10 females) using the 62-channel ESI Neuroscan system. Each subject attended three separate sessions, and each session included 15 trials corresponding to 15 different movie clips.\n\nTo make a fair comparison with the existing studies on the two benchmark databases, we also use the pre-computed differential entropy (DE) features as the model input. Specifically, for each trial, the EEG data was divided into a number of 1-second segments, and the DE features were extracted from each 1-second segment at the given five frequency bands (Delta, Theta, Alpha, Beta, and Gamma) from the 62 channels. Then, for each 1-second segment, it was represented by a 310dimensional feature vector (5 frequency bands × 62 channels), which was further filtered by a linear dynamic system method for smooth purpose as the literature  [16] ,  [64] .",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "B. Implementation Details And Model Setting",
      "text": "In the model implementation, the feature extractor f (•) and the domain discriminator d(•) are composed of fullyconnected layers with the Relu activation function. Specifically, the feature extractor f (•) is designed as 310 neurons (input layer)-64 neurons (hidden layer 1)-Relu activation-64 neurons (hidden layer 2)-Relu activation-64 neurons (output feature layer). The domain discriminator d(•) is designed as 64 neurons (input layer)-64 neurons (hidden layer 1)-Relu activation-dropout layer-64 neurons (hidden layer 2)-3 neurons (output layer)-Softmax activation. The size of matrix B given in Eq. 8 is 64 × 64. For the gradient descent and parameter optimization, we adopt the RMSprop optimizer. The learning rate is set to 1e-3 and the sample size of mini-batch is 48. All the trainable parameters are randomly initialized from a uniform distribution and no pretrained models are used. For the hyperparameters, we use L2 regularizes (1e-5) in the proposed model to avoid overfitting problems. In the EEG-Mixup based data augmentation, the coefficient α in the beta distribution is set to 0.5, and the ratio of the augmented sample size to the original sample size is set to 1:1. In the semi-supervised two-step pairwise learning, the temperature parameter η used in Eq. 10 is set to 0.9 for SEED database and 0.8 for SEED-IV database. The dynamic upper and lower thresholds (τ 0 h and τ 0 l ) used in Eq. 19 are initialized as 0.9 and 0.5, respectively. The balance parameter γ used in Eq. 20 is controlled by a constant factor δ of 2. The regularization coefficient β used in Eq. 20 is set to 0.01. For all the compared deep learning methods reproduced in our experiments, we also use the same set of common hyperparameters and EEG differential entropy features as the proposed EEGMatch to ensure fair comparisons, such as the learning rate and the batch size. For the deep domain adaptation methods DANN  [35] , DCORAL  [65] , DDC  [66] , DAN  [51] , we use the same feature extractor f (•) and domain discriminator d(•) as the proposed EEGMatch. The architecture of the classifier used in the methods above is designed as 64 neurons (hidden layer 1)-C neurons (output layer), where C is the number of classes. For the model architecture of the compared deep semi-supervised methods MixMatch  [9] , AdaMatch  [14] , FlexMatch  [67] , SoftMatch  [68] , PARSE  [10] , we use the optimal settings introduced in  [10] . The model-specific hyperparameters used in different semi-supervised methods follow the optimal settings provided by the referenced literature. For the other compared methods, such as TCA  [69] , we follow the optimal setting and training pipelines introduced in the referenced literature.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "C. Experimental Protocol With Incomplete Labels",
      "text": "We also adopt the cross-subject leave-one-subject-out experimental protocol as the existing studies  [24] ,  [39] ,  [41] ,  [50]  for cross-comparison. Specifically, we treat 14 subjects as the source domain and the remaining 1 subject as the target domain. For the source domain, only the first N trials of each subject in one session have labels (the labeled source domain S), and the rest N max -N trials of each subject are without labels (the unlabeled source domain U). The remaining 1 subject's N T trials are considered as the target domain (T) for transfer learning and model evaluation. In the domains S, U and T, the number of original trials used for EEG-Mixup augmentation is N , N max -N and N T , respectively. Given that the augmented sample ratio is set to 1:1, the total number of trials used for model training is 2×(N +N max -N +N T ) = 2 × (N max + N T ). Assuming that there are Z samples within one original trial, the total number of samples for transfer learning is 2 × Z × (N max + N T ). Note that only the original samples in the target domain T are used for model evaluation. We repeat T times until each subject is treated as the target domain for once, where T is the number of subjects in the database. Then we compute the averaged accuracy and the standard deviation as the final model performance. It is noted that the label information of U and T are unknown during model training. Furthermore, to measure the model stability under different incomplete label conditions, we adjust N at different values. Here, considering the SEED and SEED-V databases have 15 trials and the SEED-IV database has 24 trials for each subject in one session, the N value varies from 3 to 12, 5 to 10, and from 4 to 20 for the SEED, SEED-V, and SEED-IV databases, respectively. To ensure consistency with a state-of-the-art model in semi-supervised emotion recognition  [10] , we adopt a similar experimental setup, where the first session of the SEED database and three sessions from the SEED-IV and SEED-V databases are used for experiments.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "D. Cross-Subject Leave-One-Subject-Out Cross-Validation Results With Incomplete Labels",
      "text": "Table I reports the experimental results on the SEED database, in a comparison with the existing popular machine learning or deep learning methods. The results show that the proposed EEGMatch achieves superior performance compared to the other models under different incomplete label conditions, where the average model performance across N = 3, 6, 9, 12 is 91.35±07.03. Compared to the performance achieved by the best competitor in the literature (DDC  [66] : 85.46±08.34), the average performance enhancement under different N values is 5.89%. It demonstrates that, even with only a few labeled data (when N is small), EEGMatch still can TABLE I: The mean accuracies (%) and standard deviations (%) of emotion recognition results using cross-subject leave-onesubject-out cross-validation on the SEED database with incomplete labels. For each subject, N trials have labels and 15 -N trials do not have labels. The model results reproduced by us are indicated by '*'. The experimental results on the SEED-IV and SEED-V databases under different incomplete label conditions are reported in Table  II  and Table  III , which also show the superiority of the proposed EEGMatch on the semi-supervised cross-subject emotion recognition task. Specifically, the average model performance on the SEED-IV database across N = 4, 8, 12, 16, 20 is 65.53±08.31. In addition, the average model performance on the SEED-V database across N = 5, 15 is 62.75±09.02. Compared to the best results in the literature, the average performance enhancement under different N values is 0.93% for the SEED-IV database (DDC  [66] : 64.60±08.82) and 0.28% for the SEED-V database (DDC  [66] : 62.47±08.25). On the other hand, the improvement of the experimental performance on the two databases is less obvious than that on the SEED database. One possible reason is that more emotion classes in the databases would make the predicted pseudo labels ( Ŷu ) more unstable.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "V. Discussion And Conclusion",
      "text": "To fully analyze the performance and the robustness of the proposed model, we conduct various experiments to evaluate the contribution of each module in the EEGMatch and discuss the effect of different hyperparameter settings. All the present results in this session are based on the SEED database.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "A. Ablation Study",
      "text": "In this section, we conduct the ablation study to investigate the contributions of different modules in the proposed model. Table  IV  reports the cross-subject leave-one-subjectout emotion recognition results on the SEED database under different incomplete label conditions. (1) When the EEG-Mixup based data augmentation module is removed from the framework, the recognition performance presents a significant decrease for all incomplete label conditions. The average model performance drops from 91.35±07.03 to 89.23±07.  76 . It shows that the EEG-Mixup based data augmentation can benefit the model performance by generating more valid data for model training. We also replace the proposed EEG-Mixup based data augmentation with the standard Mixup based data augmentation  [11]  and find that the standard Mixup based data augmentation has a negative impact on the performance due to the ignorance of the IID assumption. The average model performance under different N values drops by 3.57%. Note that the number of original trials used for Mixup augmentation is the same as the proposed EEG-Mixup. The augmented sample ratio is also set to 1:1 for both EEG-Mixup and Mixup augmentation methods. (2) Comparing the model performance with and without the semi-supervised prototype-wise pairwise learning, an obvious drop could be observed (from 91.35±07.03 to 88.85±08.89) when the semisupervised prototype-wise pairwise learning is removed from the proposed model framework. The benefit of prototypical learning has also been observed in the other few-shot learning or transfer learning studies  [34] ,  [44] ,  [45] ,  [79] . (3) The significant positive impact of the introduced semi-supervised instance-wise pairwise learning on the model performance is observed, with an average improvement in emotion recognition performance (7.20%) being noted under various N values. The results suggest that the semi-supervised instance-wise pairwise learning can help the model capture inherent structures among EEG samples and extract representative and informative features for the downstream classification tasks. (4) To evaluate the validity of the proposed semi-supervised multi-domain adaptation method, we replace it using the traditional DANNbased domain adaptation (U is not considered in the domain TABLE II: The mean accuracies (%) and standard deviations (%) of emotion recognition results using cross-subject leaveone-subject-out cross-validation on the SEED-IV database with incomplete labels. For each subject, N trials have labels and 24 -N trials do not have labels. The model results reproduced by us are indicated by '*'.\n\nadaptation process) and calculate the model performance under different incomplete label conditions. The results show that, compared to the traditional DANN-based domain adaptation, the proposed semi-supervised multi-domain adaptation method (EEGMatch) could estimate a more accurate upper-bound for the target error by considering the distribution shift over all multiple domains (labeled source domain S, unlabeled source domain U, and target domain T). On the average, the proposed semi-supervised multi-domain adaptation method could enhance the model performance by 1.27%.  (5)  Comparing the model performance with and without the progressive learning strategy, a remarkable performance improvement is observed when the strategy is applied. Specifically, the average model performance under different N values increases by 1.45%. Note that we fix the initial upper and lower thresholds throughout the model training process when the updating strategy is removed (τ 0 h = 0.9, τ 0 l = 0.5). Compared with the fixed thresholds, dynamic and progressive thresholds can allow more samples to participate in the training process, which benefits the model generalization performance  [34] ,  [67] ,  [78] .",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "B. The Training Performance On Different Domains",
      "text": "The training process of the proposed EEGMatch on different domains is reported in Fig.  2  (N = 12). Here, we present the average training performance across different subjects to show the source and target error given in Theorem 1. A higher classification accuracy refers to a lower modeling error. To estimate the H-divergence among different domains\n\n, we adopt the maximum mean discrepancy (MMD)  [80]  calculation here. The findings reveal that the target error ϵ T (h) shown in Fig.  2 (c ) reduces along with a decline in the source error ϵ S (h) shown in Fig.  2  (a) and the H-divergence among different domains shown in Fig.  2 (b) . We also observe that the average accuracy curve on the target domain exhibits some irregularities during model training. This could be influenced by the non-stationarity of the domain adversarial training  [81] . A straightforward way to alleviate this problem is to adjust the learning rate. We carefully examine the impact of varying learning rates on mitigating the observed irregularities and report the investigation results in Supplementary Materials: Appendix G. The learned feature representations by the feature extractor f (•) at different training stages are also visually compared, based on the t-distributed stochastic neighbor embedding (t-SNE) algorithm  [82] . As shown in Fig.  3  (a), it is found that the distribution mismatch of the extracted features from different domains is quite obvious before model training. Along with the minimization of the source error and the Hdivergence defined in Theorem 1, the target error (ϵ T (h)) could be gradually minimized, as shown in Fig.  3 (b)  and (c ). In addition, the distribution mismatch between different domains is reduced, demonstrating that EEGMatch can address the covariate shift problem caused by the non-stationarity property and the individual differences in EEG signals. Our findings support the effectiveness of the proposed semi-supervised multi-domain adaptation in EEG modeling under a semisupervised learning framework.\n\nOverall, these results empirically verify the statement in Theorem 1 that the target error ϵ T (h) could be gradually minimized by decreasing the upper-bound in the training process.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "C. Model Performance On Unseen Target Data",
      "text": "In the existing EEG-based emotion recognition models using transfer learning strategy, all the target data are assumed to be complete and available for domain adaptation  [6] ,  [10] ,  [29] ,  [32] ,  [39] ,  [41] ,  [51] . However, in real-world applications, modeling with all of the target data is impractical. To further evaluate the stability and reliability of the proposed EEGMatch on unseen data, we divide the target domain (T) into a validation set (T v : the first K trials in the target domain) and a test set (T t : the remaining 15 -K in the target domain). Here, the validation set (T v ) is used for domain adaptation, together with the labeled source data (S) and the unlabeled source data (U). Then, the trained model will be validated on the unseen test set (T t ) to evaluate the model generalization capability. It should be noted that U contains the last three trials of the source subjects in this experiment (N = 12). We compare the performance of the EEGMatch with its top-4 competitors in the SEED database and report the results with different K values in Table V. It can be observed that the proposed EEGMatch achieves the best average performance under different K values, which demonstrates its effectiveness on the unseen data.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "D. The Benefit Of The Introduced Unlabeled Source Domain U",
      "text": "To further investigate the contribution of the introduced unlabeled source domain (U) in solving the semi-supervised EEG-based emotion recognition task, we compare the model performance with and without U under different incomplete label conditions. Note that for the model without U, the data augmentation, two-step pairwise learning, and multi-domain adaptation of U domain are not included. The experimental comparison is presented in Fig.  4 . It shows that the introduced unlabeled source domain (U) is beneficial to the emotion recognition task under different label scarcity conditions and helps the model training process to avoid the overfitting problem, especially when the labeled source domain (S) is extremely small.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "E. The Effect Of Hyperparameter Settings",
      "text": "The effect of the hyperparameter settings used in the proposed EEGMatch is also examined. (1) We change the temperature parameter in the Sharpen function (Eq. 10) from 0.1 to 1 with a step of 0.1. The corresponding results are reported in Fig.  5 (a) . It shows that a change in the temperature parameter would slightly affect the model performance, where the optimal temperature parameter value is 0.2 for the SEED database.  (2)  We adjust the augmentation size (the ratio of the number of augmented samples to the number of original samples) from 0 to 5 in the experiment, where 0 indicates no data augmentation and 5 refers to fourfold data augmentation. As reported in Fig.  5 (b ), an increase in the augmentation size could improve the model performance; when the augmentation size reaches 2, the model performs at its optimal. When the augmentation size exceeds 4, it is evident that the EEG-Mixup augmentation begins to negatively impact the model's performance. It indicates that a substantial number of the augmented samples might have an adverse effect on the validity and reliability of the training data. This phenomenon has also been reported by other studies  [83] ,  [84] . In Supplementary Materials D, E, and F, we provide more sensitivity analysis on the other hyper-parameters using the same experiment setting (N = 3), such as the number of trainable parameters, the selection of EEG frequency bands, and the initialization of dynamic thresholds.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "F. Topographic Analysis Of Important Eeg Patterns",
      "text": "In the topographic analysis, we identify the important brain patterns for emotion recognition by computing the mutual information between the brain patterns and prediction labels. Specifically, at the ith validation round, we have the data input of the target domain, given as X t i , which is a M × 310 matrix. Here, M is the sample size in the target domain, and   i indicate the prediction probabilities of different emotions (negative, neutral, and positive). Then, we estimate the mutual information between the input features X t i and the prediction results Ŷ t i using the non-parametric method as stated in  [85] -  [87] . The obtained mutual information matrix is termed as I(X t i , Ŷ t i ) ∈ R 3×310 , indicating a quantification of the inherent dependence between the EEG patterns and the model prediction results. I(X t i , Ŷ t i ) is further normalized to [0, 1], and a larger value refers to a greater informativeness of the EEG patterns to the model prediction at the ith validation round. Fig.  6  shows an average of all the obtained I(X t i , Ŷ t i ) across different validation rounds. It is found that the EEG patterns with greater informativeness for emotion recognition are mainly located in the Beta and Gamma frequency bands  [16] ,  [24]  at the parieto-occipital regions  [39] ,  [41] .",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "G. Conclusion",
      "text": "In this paper, we present a novel semi-supervised learning framework (EEGMatch) for EEG-based cross-subject emotion recognition with incomplete labels. The proposed EEGMatch introduces an efficient EEG-Mixup based data augmentation  Overall, our work presents an efficient and effective semisupervised learning framework for EEG-based cross-subject emotion recognition. By taking into account the distribution shift, the semi-supervised EEG-based cross-subject emotion recognition performance could be greatly enhanced. We believe that the proposed EEGMatch would open up new possibilities for resolving the label missing problem in EEG tasks, in which high-quality labels are challenging to acquire. In the future, further consideration of the class imbalance issue in modeling is necessary, which commonly happens in real-world applications. Moreover, the stabilization of multidomain adversarial training poses an ongoing challenge that warrants increased attention in future research efforts.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Appendix A Proof Of The Computation P",
      "text": "For the total c emotion classes, the supervised prototypical representation P is computed as\n\nwhere X = [x 1 ; x 2 , ...;\n\nx n ] and Y = [y 1 ; y 2 , ...; y n ] are the labeled source data and the corresponding emotional labels (one-hot representation) in the current mini-batch, respectively. f (•) is a feature extractor with the parameter θ f . f (X) is a N B × M feature matrix, where N B is the batch size for stochastic gradient descent and M is the feature dimensionality. P = [µ 1 ; µ 2 ; ...; µ c ], where µ c is the prototypical representation for the cth emotional category.\n\nProof.\n\nwhere N j (j ∈ [1, ..., c]) is the sample size in the jth emotion class. x ij are the EEG samples belonging to the jth emotion class.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Appendix B Proof Of Theorem 1",
      "text": "Theorem 1: Based on the labeled source domain (S), the unlabeled source domain (U), and the target domain (T), the target error ϵ T (h) with a hypothesis h (h ∈ H) is bounded as\n\nwhere ϵ S (h) is the error of the labeled source domain, and d H represents the H-divergence between the given domains. F S (x), F U (x), and F T (x) are the labeling functions of the labeled source domain, the unlabeled source domain, and the target domain, respectively. F T (x) = π S F S (x) + π U F U (x) is a labeling function for any x ∈ Supp(D T ), which is a weighted summation of F S (x) and F U (x) with the weights of π S and π U .\n\nProof. In the standard domain adaptation, the target error ϵ T (h) for a given classification hypothesis h ∈ H is bounded by  [56]  ϵ\n\nwhere D S and D T refer to the labeled source domain and the target domain, respectively.\n\nHowever, under the setting of this study, the source domain is composed of both labeled and unlabeled data. If simply applying the standard domain adaptation in this study by only using the labeled source data, the estimation of the target error would not be accurate enough.\n\nMotivated by the domain projection method introduced in the domain generalization studies  [60] ,  [61] , we can project the target domain D T onto the convex hull of the source and compute its \"projection\" as Eq. 30. Based on the given labeled source domain (D S ), the unlabeled source domain (D U ), and the target domain (D T ), the computation is given as\n\nwhere π S and π U are belong to ∆ 1 and π S + π U is equal to 1. The labeling function of D T could be represented by a weighted summation of the labeling functions of D S and D U , as\n\nwhere F S (x) and F U (x) are the corresponding labeling functions of D S and D U . Then, the target error is bounded as\n\nwhere δ T ,T = min\n\nThe overall source error ϵ T (h) is defined as\n\nwhich is a weighted summation of the labeled source error of ϵ S (h) and the unlabeled source error of ϵ U (h). Considering that the label information of the unlabeled source domain D U is unknown in the training process, we can bound the error ϵ U (h) as\n\nwhere\n\nAccording to the triangle inequality and the sub-additivity of the sup, the upper-bound of the H-divergence can be written as  [60] ,  [61]   for q s = 1 to N do //N s ps,qs is the number of samples from p s subject at q s trial.",
      "page_start": 14,
      "page_end": 15
    },
    {
      "section_name": "5:",
      "text": "for 1 to ratio × N s ps,qs do 6:\n\nrandomly select two samples x s i , y s i and x s j , y s j ;\n\n7:\n\nsample ω from distribution Beta(α, α) for q u = 1 to M -N do //N u pu,qu is the number of samples from p u subject at q u trial. 17:\n\nfor 1 to ratio × N u pu,qu do 18:\n\nrandomly select two samples x u i and x u j ;\n\n19:\n\nsample ω from distribution Beta(α, α)\n\n//N t pt,qt is the number of samples from p t subject at q t trial.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Appendix E Effect Of The Number Of Trainable Parameters",
      "text": "The number of trainable parameters is an important factor that determines the model computation complexity and performance. The computation complexity of the EEGMatch can be estimated by computing the number of trainable parameters in its feature extractor f (•), domain discriminator d(•), and bilinear transformation matrix B. For the feature extractor f (•), it is designed as 310 neurons-H neurons-Relu activation-H neurons-Relu activation-H neurons. For the domain discriminator d(•), it is designed as H neurons-H neurons-Relu activation-dropout layer-H neurons-3 neurons-Softmax activation. The size of matrix B is H × H. H refers to the number of neurons in the hidden layers. Consequently, the number of trainable parameters N um in the EEGMatch can be computed as, N um = (310H + H 2 ) + (H 2 + 3H) + H 2 = 313H + 3H 2 , (37) The higher the number of trainable parameters, the higher the computation complexity. To investigate its effect on the model performance, we adjust the value of H and report the performance of the EEGMatch in Table  VII . The experiments are conducted on the SEED database with the three labeled trials (N = 3). The results indicate that the optimal H value is 64, which achieves the best classification performance. Additionally, it demonstrates the second-lowest computation complexity, indicating a favorable balance between performance and complexity.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Appendix F Sensitivity Analysis Of The Dynamic Thresholds",
      "text": "To ensure correct optimization during model training, we employ two dynamic thresholds ( upper threshold τ h and lower threshold τ l ) to exclude invalid paired samples. In this section, we use a grid-search method to investigate the impact of initial values of the thresholds on model performance. Specifically, we fix the initial upper threshold τ 0 h and adjust the initial lower threshold τ 0 l from 0.1 to 0.5 with a step of 0.1. For simplicity, we conduct our experiments on the SEED database and set the number of labeled trials as three for each source subject (N = 3). As illustrated in Fig.  8 , we can observe that the performance of the proposed EEGMatch is relatively insensitive to the initialization of the    U domains based on a specified ratio. For instance, if the S : U ratio is 1 : 1, 50% of subjects in the training sets are labeled (S) while the remaining 50% are unlabeled (U). We adjust the ratio from 1 : 2 to 2 : 1 and present the results in Table  VIII . Here, the sample balance of the two groups in three domains is considered. The results show that EEGMatch attains an average accuracy of 65.35±10.07 in the EC experiment, with a 1.33% improvement compared to the existing literature. In the EO experiment, EEGMatch achieves an average accuracy of 67.08±09.24, indicating a 2.39% enhancement compared to other models. In summary, the results demonstrate the efficacy of EEGMatch in alleviating the label-missing problem in a real-world database.",
      "page_start": 17,
      "page_end": 17
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , the proposed model includes three",
      "page": 3
    },
    {
      "caption": "Figure 1: The semi-supervised learning framework of the proposed EEGMatch. It consists of three modules. (1) EEG-Mixup",
      "page": 4
    },
    {
      "caption": "Figure 2: (N = 12). Here, we present",
      "page": 10
    },
    {
      "caption": "Figure 2: (c) reduces along with a decline in the source",
      "page": 10
    },
    {
      "caption": "Figure 2: (a) and the H-divergence among",
      "page": 10
    },
    {
      "caption": "Figure 2: (b). We also observe that",
      "page": 10
    },
    {
      "caption": "Figure 3: (a), it is found",
      "page": 11
    },
    {
      "caption": "Figure 3: (b) and (c). In",
      "page": 11
    },
    {
      "caption": "Figure 4: It shows that the introduced",
      "page": 11
    },
    {
      "caption": "Figure 5: (a). It shows that a change in the temperature",
      "page": 11
    },
    {
      "caption": "Figure 5: (b), an increase in the augmentation size",
      "page": 11
    },
    {
      "caption": "Figure 2: The training performance on different domains. (a) The training process on the source domain (S+U). (b) The estimated",
      "page": 12
    },
    {
      "caption": "Figure 3: A visualization of the learned feature representations (a) before training, (b) at the training epoch of 50, and (c) in the",
      "page": 12
    },
    {
      "caption": "Figure 6: shows an average of all the obtained I(Xt",
      "page": 12
    },
    {
      "caption": "Figure 4: The emotion recognition accuracies (%) without and",
      "page": 13
    },
    {
      "caption": "Figure 5: The emotion recognition accuracies (%) of the pro-",
      "page": 13
    },
    {
      "caption": "Figure 6: Topographic analysis of the mutual information be-",
      "page": 13
    },
    {
      "caption": "Figure 7: (a), we randomly select two",
      "page": 15
    },
    {
      "caption": "Figure 7: (b). To illustrate",
      "page": 15
    },
    {
      "caption": "Figure 7: Diagram of the proposed EEG-Mixup augmentation",
      "page": 17
    },
    {
      "caption": "Figure 8: , we can observe that the performance of the proposed",
      "page": 17
    },
    {
      "caption": "Figure 9: The result demonstrates that small",
      "page": 18
    },
    {
      "caption": "Figure 10: The results show",
      "page": 18
    },
    {
      "caption": "Figure 11: and Fig. 12.",
      "page": 18
    },
    {
      "caption": "Figure 8: The mean accuracies (%) of the proposed EEGMatch on target domains under different initial thresholds.",
      "page": 19
    },
    {
      "caption": "Figure 9: The mean accuracies (%) of the proposed EEGMatch under different learning rates.",
      "page": 19
    },
    {
      "caption": "Figure 10: The mean accuracies (%) of the proposed EEGMatch",
      "page": 19
    },
    {
      "caption": "Figure 11: A visualization of the learned feature representations (a) before training, (b) at the training epoch of 50, and (c) in",
      "page": 20
    },
    {
      "caption": "Figure 12: A visualization of the learned feature representations (a) before training, (b) at the training epoch of 50, and (c) in",
      "page": 20
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1": "0"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          ":: llaabbeelleedd ssoouurrccee ddoommaaiinn\n:: uunnllaabbeelleedd ssoouurrccee ddoommaaiinn\n:: ttaarrggeett ddoommaaiinn\n: data augmentation": "CC CCoonnccaatteennaattiioonn ww BBiilliinneeaarr TTrraannssffoorrmmaattiioonn PP SShhaarrppeenn TT DDyynnaammiicc TThhrreesshhoollddiinngg VVaalliidd IInnvvaalliidd",
          "Column_2": "",
          "Column_3": "IInnvvaalliidd"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "e\nlp\nm\na": "S\nm\no\nd\nn\na\nR"
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "e\nlp\nm\na": "S\nm\no\nd\nn\na\nR"
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "l\na\ni\nr\nT\ne\nn\nO",
          "Column_2": "",
          "Column_3": ""
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "l\na\ni\nr\nT\ne\nn\nO",
          "Column_2": "",
          "Column_3": ""
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0=0.6 0=0.7 0=0.8 0=0.9\nh h h h\n0.85 0.85 0.85 0.85\n0.80 0.80 0.80 0.80\n0.75 0.75 0.75 0.75\n0.1 0.2 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5\n0 0 0 0\nl l l l": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "0.2",
          "Column_7": "",
          "Column_8": "0.3\n0\nl",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "0.4",
          "Column_12": "0.5",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "",
          "Column_17": "0.1",
          "Column_18": "",
          "Column_19": "0.2",
          "Column_20": "",
          "Column_21": "",
          "Column_22": "0.3\n0\nl",
          "Column_23": "",
          "Column_24": "0.",
          "Column_25": "4",
          "Column_26": "0.5",
          "Column_27": "",
          "Column_28": "0.1",
          "Column_29": "",
          "Column_30": "0.2",
          "Column_31": "",
          "Column_32": "0.3\n0\nl",
          "Column_33": "",
          "Column_34": "",
          "Column_35": "0.",
          "Column_36": "4",
          "Column_37": "",
          "Column_38": "0.5",
          "Column_39": "",
          "Column_40": "0.1",
          "Column_41": "",
          "Column_42": "0.",
          "Column_43": "2",
          "Column_44": "0.3\n0\nl",
          "Column_45": "",
          "Column_46": "0.4",
          "Column_47": "",
          "Column_48": "0.5"
        },
        {
          "0=0.6 0=0.7 0=0.8 0=0.9\nh h h h\n0.85 0.85 0.85 0.85\n0.80 0.80 0.80 0.80\n0.75 0.75 0.75 0.75\n0.1 0.2 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5\n0 0 0 0\nl l l l": "",
          "Column_2": "F",
          "Column_3": "",
          "Column_4": "ig.",
          "Column_5": "",
          "Column_6": "8:",
          "Column_7": "Th",
          "Column_8": "e m",
          "Column_9": "",
          "Column_10": "e",
          "Column_11": "an a",
          "Column_12": "ccur",
          "Column_13": "",
          "Column_14": "acies (%",
          "Column_15": "",
          "Column_16": "",
          "Column_17": ")",
          "Column_18": "of",
          "Column_19": "the",
          "Column_20": "",
          "Column_21": "p",
          "Column_22": "rop",
          "Column_23": "os",
          "Column_24": "ed",
          "Column_25": "EE",
          "Column_26": "G",
          "Column_27": "Match o",
          "Column_28": "n t",
          "Column_29": "arg",
          "Column_30": "et",
          "Column_31": "do",
          "Column_32": "ma",
          "Column_33": "in",
          "Column_34": "",
          "Column_35": "s",
          "Column_36": "und",
          "Column_37": "",
          "Column_38": "er",
          "Column_39": "different",
          "Column_40": "in",
          "Column_41": "itia",
          "Column_42": "l",
          "Column_43": "thre",
          "Column_44": "sh",
          "Column_45": "old",
          "Column_46": "s.",
          "Column_47": "",
          "Column_48": ""
        },
        {
          "0=0.6 0=0.7 0=0.8 0=0.9\nh h h h\n0.85 0.85 0.85 0.85\n0.80 0.80 0.80 0.80\n0.75 0.75 0.75 0.75\n0.1 0.2 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5\n0 0 0 0\nl l l l": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "(a",
          "Column_7": ")le",
          "Column_8": "arni\nFi",
          "Column_9": "",
          "Column_10": "ng\ng.",
          "Column_11": "rate\n9: T",
          "Column_12": "=0.0\nhe",
          "Column_13": "",
          "Column_14": "1\nmean ac",
          "Column_15": "",
          "Column_16": "",
          "Column_17": "cu",
          "Column_18": "ra",
          "Column_19": "cies",
          "Column_20": "",
          "Column_21": "(",
          "Column_22": "%)",
          "Column_23": "(\nof",
          "Column_24": "b)\nth",
          "Column_25": "lear\ne p",
          "Column_26": "nin\nrop",
          "Column_27": "",
          "Column_28": ".00\nG\nO\n.08\nm\nEG\nwor\n. S\nrai",
          "Column_29": "",
          "Column_30": "tc\nper\n09.\nls.\natc\nda\nH.\nom",
          "Column_31": "",
          "Column_32": "nd\nen\nin\nsu\nn\nas\nJ.\nrin",
          "Column_33": "",
          "Column_34": "",
          "Column_35": "di\nEE\nca\nm\nev\nan\nfac",
          "Column_36": "",
          "Column_37": "",
          "Column_38": "ent\nat\na\nth\nng\nFE\n. M\nsed",
          "Column_39": "",
          "Column_40": "ngr\ng r\nves\nnha\nde\nl-\nss-s\nnd",
          "Column_41": "",
          "Column_42": "=\ns.\nn\nem\nns\nsi\nect\nnet",
          "Column_43": "",
          "Column_44": "01\nrag\ntc\ntet\npr\notio\nybo",
          "Column_45": "",
          "Column_46": "acc\npa\nef\nem\neco\nand",
          "Column_47": "",
          "Column_48": "acy\nto\nacy\nn a\ntion\nonic"
        },
        {
          "0=0.6 0=0.7 0=0.8 0=0.9\nh h h h\n0.85 0.85 0.85 0.85\n0.80 0.80 0.80 0.80\n0.75 0.75 0.75 0.75\n0.1 0.2 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5\n0 0 0 0\nl l l l": "90\n)%(\n85\nycaruccA\n80\n75\n1 3\nThe",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "",
          "Column_17": "",
          "Column_18": "",
          "Column_19": "",
          "Column_20": "",
          "Column_21": "",
          "Column_22": "",
          "Column_23": "",
          "Column_24": "",
          "Column_25": "",
          "Column_26": "",
          "Column_27": "",
          "Column_28": "",
          "Column_29": "",
          "Column_30": "",
          "Column_31": "",
          "Column_32": "",
          "Column_33": "",
          "Column_34": "",
          "Column_35": "",
          "Column_36": "",
          "Column_37": "",
          "Column_38": "",
          "Column_39": "",
          "Column_40": "",
          "Column_41": "",
          "Column_42": "",
          "Column_43": "",
          "Column_44": "",
          "Column_45": "",
          "Column_46": "",
          "Column_47": "",
          "Column_48": ""
        },
        {
          "0=0.6 0=0.7 0=0.8 0=0.9\nh h h h\n0.85 0.85 0.85 0.85\n0.80 0.80 0.80 0.80\n0.75 0.75 0.75 0.75\n0.1 0.2 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5\n0 0 0 0\nl l l l": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "1",
          "Column_6": "",
          "Column_7": "T",
          "Column_8": "",
          "Column_9": "3\nhe",
          "Column_10": "",
          "Column_11": "num",
          "Column_12": "5\nb",
          "Column_13": "er o",
          "Column_14": "",
          "Column_15": "7\nf so",
          "Column_16": "urc",
          "Column_17": "",
          "Column_18": "9\ne s",
          "Column_19": "",
          "Column_20": "ubj",
          "Column_21": "",
          "Column_22": "11\nect",
          "Column_23": "s",
          "Column_24": "",
          "Column_25": "13",
          "Column_26": "",
          "Column_27": "",
          "Column_28": "",
          "Column_29": "",
          "Column_30": "",
          "Column_31": "",
          "Column_32": "",
          "Column_33": "",
          "Column_34": "",
          "Column_35": "",
          "Column_36": "",
          "Column_37": "",
          "Column_38": "",
          "Column_39": "",
          "Column_40": "",
          "Column_41": "",
          "Column_42": "",
          "Column_43": "",
          "Column_44": "",
          "Column_45": "",
          "Column_46": "",
          "Column_47": "",
          "Column_48": ""
        },
        {
          "0=0.6 0=0.7 0=0.8 0=0.9\nh h h h\n0.85 0.85 0.85 0.85\n0.80 0.80 0.80 0.80\n0.75 0.75 0.75 0.75\n0.1 0.2 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5\n0 0 0 0\nl l l l": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "he\nferen\nsba\n:1,\nthe\n1:\nsam\nered\nccur\nimpr",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "n ac\numb\nona\n% of\naini\no 2\nbal\nhe r\nof\nmen",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "ie\ns\ncifi\nec\n0%\nnd\nof\ns\n±\nmp",
          "Column_13": "",
          "Column_14": "",
          "Column_15": ") of\ne su\natio\nth\nunl\nsent\ntwo\nth\n7 in\nto",
          "Column_16": "",
          "Column_17": "",
          "Column_18": "pro\nts.\nrins\ninin\nled(\nres\noups\nEEG\nEC\nexi",
          "Column_19": "",
          "Column_20": "",
          "Column_21": "",
          "Column_22": "ed\ne,i\nts\nWe\nin\nthre\nch\neri\nlit",
          "Column_23": "",
          "Column_24": "",
          "Column_25": "Mat\nS:\nabel\nustt\nVI\nma\nins\nt, w\nure.",
          "Column_26": "",
          "Column_27": "",
          "Column_28": "",
          "Column_29": "",
          "Column_30": "",
          "Column_31": "",
          "Column_32": "",
          "Column_33": "",
          "Column_34": "",
          "Column_35": "",
          "Column_36": "",
          "Column_37": "",
          "Column_38": "",
          "Column_39": "",
          "Column_40": "",
          "Column_41": "",
          "Column_42": "",
          "Column_43": "",
          "Column_44": "",
          "Column_45": "",
          "Column_46": "",
          "Column_47": "",
          "Column_48": ""
        }
      ],
      "page": 19
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "Tca"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "",
      "authors": [
        "Kpca"
      ],
      "venue": ""
    },
    {
      "citation_id": "3",
      "title": "",
      "authors": [
        "Knn"
      ],
      "venue": ""
    },
    {
      "citation_id": "4",
      "title": "",
      "authors": [
        "Dan"
      ],
      "venue": ""
    },
    {
      "citation_id": "5",
      "title": "",
      "authors": [
        "Dann"
      ],
      "venue": ""
    },
    {
      "citation_id": "6",
      "title": "",
      "authors": [
        "Mixmatch"
      ],
      "venue": ""
    },
    {
      "citation_id": "7",
      "title": "",
      "authors": [
        "Adamatch"
      ],
      "venue": ""
    },
    {
      "citation_id": "8",
      "title": "",
      "authors": [
        "Knn"
      ],
      "venue": ""
    },
    {
      "citation_id": "9",
      "title": "",
      "authors": [
        "Dan"
      ],
      "venue": ""
    },
    {
      "citation_id": "10",
      "title": "",
      "authors": [
        "Dann"
      ],
      "venue": ""
    },
    {
      "citation_id": "11",
      "title": "",
      "authors": [
        "Mixmatch"
      ],
      "venue": ""
    },
    {
      "citation_id": "12",
      "title": "",
      "authors": [
        "Adamatch"
      ],
      "venue": ""
    },
    {
      "citation_id": "13",
      "title": "",
      "authors": [
        "Flexmatch"
      ],
      "venue": ""
    },
    {
      "citation_id": "14",
      "title": "The mean accuracies (%) and standard deviations (%) of emotion recognition results using cross-subject leaveone-subject-out cross-validation on the SEED-V database with incomplete labels. For each subject, N trials have labels and 15 -N trials do not have labels. The model results reproduced by us are indicated by",
      "authors": [
        "Table Iii"
      ],
      "venue": "The mean accuracies (%) and standard deviations (%) of emotion recognition results using cross-subject leaveone-subject-out cross-validation on the SEED-V database with incomplete labels. For each subject, N trials have labels and 15 -N trials do not have labels. The model results reproduced by us are indicated by"
    },
    {
      "citation_id": "15",
      "title": "Methods N = 5 N = 10 Average Methods N = 5 N = 10 Average SVM",
      "venue": "Methods N = 5 N = 10 Average Methods N = 5 N = 10 Average SVM"
    },
    {
      "citation_id": "16",
      "title": "",
      "authors": [
        "Gfk"
      ],
      "venue": ""
    },
    {
      "citation_id": "17",
      "title": "",
      "authors": [
        "Dan"
      ],
      "venue": ""
    },
    {
      "citation_id": "18",
      "title": "",
      "authors": [
        "Dann"
      ],
      "venue": ""
    },
    {
      "citation_id": "19",
      "title": "",
      "authors": [
        "Mixmatch"
      ],
      "venue": ""
    },
    {
      "citation_id": "20",
      "title": "",
      "authors": [
        "Adamatch"
      ],
      "venue": ""
    },
    {
      "citation_id": "21",
      "title": "",
      "authors": [
        "Flexmatch"
      ],
      "venue": ""
    },
    {
      "citation_id": "22",
      "title": "",
      "authors": [
        "Softmatch"
      ],
      "venue": ""
    },
    {
      "citation_id": "23",
      "title": "Cross-subject emotion recognition brain-computer interface based on fnirs and dbjnet",
      "authors": [
        "X Si",
        "H He",
        "J Yu",
        "D Ming"
      ],
      "year": "2023",
      "venue": "Cyborg and Bionic Systems"
    },
    {
      "citation_id": "24",
      "title": "Transfer learning in brain-computer interfaces",
      "authors": [
        "V Jayaram",
        "M Alamgir",
        "Y Altun",
        "B Scholkopf",
        "M Grosse-Wentrup"
      ],
      "year": "2016",
      "venue": "IEEE Computational Intelligence Magazine"
    },
    {
      "citation_id": "25",
      "title": "From regional to global brain: A novel hierarchical spatial-temporal neural network model for EEG emotion recognition",
      "authors": [
        "Y Li",
        "W Zheng",
        "L Wang",
        "Y Zong",
        "Z Cui"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "26",
      "title": "DAGAM: a domain adversarial graph attention model for subject independent EEG-based emotion recognition",
      "authors": [
        "T Xu",
        "W Dang",
        "J Wang",
        "Y Zhou"
      ],
      "year": "2022",
      "venue": "DAGAM: a domain adversarial graph attention model for subject independent EEG-based emotion recognition",
      "arxiv": "arXiv:2202.12948"
    },
    {
      "citation_id": "27",
      "title": "Generator-based domain adaptation method with knowledge free for cross-subject EEG emotion recognition",
      "authors": [
        "D Huang",
        "S Zhou",
        "D Jiang"
      ],
      "year": "2022",
      "venue": "Cognitive Computation"
    },
    {
      "citation_id": "28",
      "title": "An adversarial discriminative temporal convolutional network for EEG-based cross-domain emotion recognition",
      "authors": [
        "Z He",
        "Y Zhong",
        "J Pan"
      ],
      "year": "2022",
      "venue": "Computers in biology and medicine"
    },
    {
      "citation_id": "29",
      "title": "Application of covariate shift adaptation techniques in brain-computer interfaces",
      "authors": [
        "Y Li",
        "H Kambara",
        "Y Koike",
        "M Sugiyama"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "30",
      "title": "Holistic semi-supervised approaches for EEG representation learning",
      "authors": [
        "G Zhang",
        "A Etemad"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "31",
      "title": "MixMatch: a holistic approach to semi-supervised learning",
      "authors": [
        "D Berthelot",
        "N Carlini",
        "I Goodfellow",
        "N Papernot",
        "A Oliver",
        "C Raffel"
      ],
      "year": "2019",
      "venue": "MixMatch: a holistic approach to semi-supervised learning",
      "arxiv": "arXiv:1905.02249"
    },
    {
      "citation_id": "32",
      "title": "PARSE: pairwise alignment of representations in semi-supervised EEG learning for emotion recognition",
      "authors": [
        "G Zhang",
        "A Etemad"
      ],
      "year": "2022",
      "venue": "PARSE: pairwise alignment of representations in semi-supervised EEG learning for emotion recognition",
      "arxiv": "arXiv:2202.05400"
    },
    {
      "citation_id": "33",
      "title": "Mixup: Beyond empirical risk minimization",
      "authors": [
        "H Zhang",
        "C Moustapha",
        "D Yann",
        "L.-P David"
      ],
      "year": "2018",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "34",
      "title": "A comparative stationarity analysis of EEG signals",
      "authors": [
        "V Rasoulzadeh",
        "E Erkus",
        "T Yogurt",
        "I Ulusoy",
        "S Zergeroglu"
      ],
      "year": "2017",
      "venue": "Annals of Operations Research"
    },
    {
      "citation_id": "35",
      "title": "Classification from pairwise similarity and unlabeled data",
      "authors": [
        "H Bao",
        "G Niu",
        "M Sugiyama"
      ],
      "year": "2018",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "36",
      "title": "AdaMatch: a unified approach to semi-supervised learning and domain adaptation",
      "authors": [
        "D Berthelot",
        "R Roelofs",
        "K Sohn",
        "N Carlini",
        "A Kurakin"
      ],
      "year": "2021",
      "venue": "AdaMatch: a unified approach to semi-supervised learning and domain adaptation",
      "arxiv": "arXiv:2106.04732"
    },
    {
      "citation_id": "37",
      "title": "Differential entropy feature for EEG-based emotion classification",
      "authors": [
        "R.-N Duan",
        "J.-Y Zhu",
        "B.-L Lu"
      ],
      "year": "2013",
      "venue": "2013 6th International IEEE/EMBS Conference on Neural Engineering (NER)"
    },
    {
      "citation_id": "38",
      "title": "Investigating critical frequency bands and channels for EEG-based emotion recognition with deep neural networks",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Autonomous Mental Development"
    },
    {
      "citation_id": "39",
      "title": "EEG-based mild depressive detection using feature selection methods and classifiers",
      "authors": [
        "X Li",
        "B Hu",
        "S Sun",
        "H Cai"
      ],
      "year": "2016",
      "venue": "Computer methods and programs in biomedicine"
    },
    {
      "citation_id": "40",
      "title": "Real-time movie-induced discrete emotion recognition from EEG signals",
      "authors": [
        "Y.-J Liu",
        "M Yu",
        "G Zhao",
        "J Song",
        "Y Ge",
        "Y Shi"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "41",
      "title": "Cascade and parallel convolutional recurrent neural networks on EEG-based intention recognition for brain computer interface",
      "authors": [
        "D Zhang",
        "L Yao",
        "X Zhang",
        "S Wang",
        "W Chen",
        "R Boots",
        "B Benatallah"
      ],
      "year": "2018",
      "venue": "Proceedings of the aaai conference on artificial intelligence"
    },
    {
      "citation_id": "42",
      "title": "Multi-method fusion of cross-subject emotion recognition based on high-dimensional EEG features",
      "authors": [
        "F Yang",
        "X Zhao",
        "W Jiang",
        "P Gao",
        "G Liu"
      ],
      "year": "2019",
      "venue": "Frontiers in computational neuroscience"
    },
    {
      "citation_id": "43",
      "title": "Adaptive tunable q wavelet transform-based emotion identification",
      "authors": [
        "S Khare",
        "V Bajaj",
        "G Sinha"
      ],
      "year": "2020",
      "venue": "IEEE transactions on instrumentation and measurement"
    },
    {
      "citation_id": "44",
      "title": "EEGFuseNet: Hybrid unsupervised deep feature characterization and fusion for high-dimensional EEG with an application to emotion recognition",
      "authors": [
        "Z Liang",
        "R Zhou",
        "L Zhang",
        "L Li",
        "G Huang",
        "Z Zhang",
        "S Ishii"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "45",
      "title": "Spatial-rhythmic network as a biomarker of familial risk for psychotic bipolar disorder",
      "authors": [
        "L Jiang",
        "Y Liang",
        "S Genon",
        "R He",
        "Q Yang",
        "C Yi",
        "L Yu",
        "D Yao",
        "S Eickhoff",
        "D Dong"
      ],
      "year": "2023",
      "venue": "Nature Mental Health"
    },
    {
      "citation_id": "46",
      "title": "EEG emotion recognition using Average Performance 64",
      "authors": [
        "T Song",
        "W Zheng",
        "P Song",
        "Z Cui"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "47",
      "title": "Hierarchical dynamic graph convolutional network with interpretability for EEG-based emotion recognition",
      "authors": [
        "M Ye",
        "C Chen",
        "T Zhang"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "48",
      "title": "Effective emotion recognition by learning discriminative graph topologies in EEG brain networks",
      "authors": [
        "C Li",
        "P Li",
        "Y Zhang",
        "N Li",
        "Y Si",
        "F Li",
        "Z Cao",
        "H Chen",
        "B Chen",
        "D Yao"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "49",
      "title": "A brain network analysisbased double way deep neural network for emotion recognition",
      "authors": [
        "W Niu",
        "C Ma",
        "X Sun",
        "M Li",
        "Z Gao"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "50",
      "title": "Hybrid network using dynamic graph convolution and temporal self-attention for EEG-based emotion recognition",
      "authors": [
        "C Cheng",
        "Z Yu",
        "Y Zhang",
        "L Feng"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "51",
      "title": "Personalizing EEG-based affective models with transfer learning",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2016",
      "venue": "Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, ser. IJCAI'16"
    },
    {
      "citation_id": "52",
      "title": "Transferring subspaces between subjects in brain-computer interfacing",
      "authors": [
        "W Samek",
        "F Meinecke",
        "K.-R Müller"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "53",
      "title": "Learning a common dictionary for subject-transfer decoding with resting calibration",
      "authors": [
        "H Morioka",
        "A Kanemura",
        "J -I. Hirayama",
        "M Shikauchi",
        "T Ogawa",
        "S Ikeda",
        "M Kawanabe",
        "S Ishii"
      ],
      "year": "2015",
      "venue": "NeuroImage"
    },
    {
      "citation_id": "54",
      "title": "EEG-based emotion recognition using domain adaptation network",
      "authors": [
        "Y.-M Jin",
        "Y.-D Luo",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2017",
      "venue": "2017 International Conference on Orange Technologies (ICOT)"
    },
    {
      "citation_id": "55",
      "title": "Domain adaptation for EEG emotion recognition based on latent representation similarity",
      "authors": [
        "J Li",
        "S Qiu",
        "C Du",
        "Y Wang",
        "H He"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "56",
      "title": "PR-PL: A novel prototypical representation based pairwise learning framework for emotion recognition using EEG signals",
      "authors": [
        "R Zhou",
        "Z Zhang",
        "H Fu",
        "L Zhang",
        "L Li",
        "G Huang",
        "F Li",
        "X Yang",
        "Y Dong",
        "Y.-T Zhang"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "57",
      "title": "Domain-adversarial training of neural networks",
      "authors": [
        "Y Ganin",
        "E Ustinova",
        "H Ajakan",
        "P Germain",
        "H Larochelle",
        "F Laviolette",
        "M Marchand",
        "V Lempitsky"
      ],
      "year": "2016",
      "venue": "The journal of machine learning research"
    },
    {
      "citation_id": "58",
      "title": "Joint feature adaptation and graph adaptive label propagation for cross-subject emotion recognition from EEG signals",
      "authors": [
        "Y Peng",
        "W Wang",
        "W Kong",
        "F Nie",
        "B.-L Lu",
        "A Cichocki"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "59",
      "title": "Possibilistic clustering-promoting semi-supervised learning for EEG-based emotion recognition",
      "authors": [
        "Y Dan",
        "J Tao",
        "J Fu",
        "D Zhou"
      ],
      "year": "2021",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "60",
      "title": "Emotionmeter: A multimodal framework for recognizing human emotions",
      "authors": [
        "W.-L Zheng",
        "W Liu",
        "Y Lu",
        "B.-L Lu",
        "A Cichocki"
      ],
      "year": "2018",
      "venue": "IEEE transactions on cybernetics"
    },
    {
      "citation_id": "61",
      "title": "An efficient LSTM network for emotion recognition from multichannel EEG signals",
      "authors": [
        "X Du",
        "C Ma",
        "G Zhang",
        "J Li",
        "Y.-K Lai",
        "G Zhao",
        "X Deng",
        "Y.-J Liu",
        "H Wang"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "62",
      "title": "Multisource transfer learning for cross-subject EEG emotion recognition",
      "authors": [
        "J Li",
        "S Qiu",
        "Y.-Y Shen",
        "C.-L Liu",
        "H He"
      ],
      "year": "2019",
      "venue": "IEEE transactions on cybernetics"
    },
    {
      "citation_id": "63",
      "title": "EEG-based emotion recognition using regularized graph neural networks",
      "authors": [
        "P Zhong",
        "D Wang",
        "C Miao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "64",
      "title": "PANET: few-shot image semantic segmentation with prototype alignment",
      "authors": [
        "K Wang",
        "J Liew",
        "Y Zou",
        "D Zhou",
        "J Feng"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "65",
      "title": "A self-training hierarchical prototype-based approach for semisupervised classification",
      "authors": [
        "X Gu"
      ],
      "year": "2020",
      "venue": "Information Sciences"
    },
    {
      "citation_id": "66",
      "title": "Prototypical networks for fewshot learning",
      "authors": [
        "J Snell",
        "K Swersky",
        "R Zemel"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "67",
      "title": "Unsupervised domain adaptation with similarity learning",
      "authors": [
        "P Pinheiro"
      ],
      "year": "2018",
      "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "68",
      "title": "Prototypical representation learning for relation extraction",
      "authors": [
        "N Ding",
        "X Wang",
        "Y Fu",
        "G Xu",
        "R Wang",
        "P Xie",
        "Y Shen",
        "F Huang",
        "H.-T Zheng",
        "R Zhang"
      ],
      "year": "2021",
      "venue": "Prototypical representation learning for relation extraction",
      "arxiv": "arXiv:2103.11647"
    },
    {
      "citation_id": "69",
      "title": "Pairwise supervision can provably elicit a decision boundary",
      "authors": [
        "H Bao",
        "T Shimada",
        "L Xu",
        "I Sato",
        "M Sugiyama"
      ],
      "year": "2022",
      "venue": "Proceedings of The 25th International Conference on Artificial Intelligence and Statistics, ser. Proceedings of Machine Learning"
    },
    {
      "citation_id": "70",
      "title": "Supervised contrastive learning",
      "authors": [
        "P Khosla",
        "P Teterwak",
        "C Wang",
        "A Sarna",
        "Y Tian",
        "P Isola",
        "A Maschinot",
        "C Liu",
        "D Krishnan"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "71",
      "title": "WGAN domain adaptation for EEG-based emotion recognition",
      "authors": [
        "Y Luo",
        "S Zhang",
        "W Zheng",
        "B Lu"
      ],
      "year": "2018",
      "venue": "International Conference on Neural Information Processing"
    },
    {
      "citation_id": "72",
      "title": "A novel bi-hemispheric discrepancy model for EEG emotion recognition",
      "authors": [
        "Y Li",
        "L Wang",
        "W Zheng",
        "Y Zong",
        "L Qi",
        "Z Cui",
        "T Zhang",
        "T Song"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "73",
      "title": "Cross-subject emotion recognition using deep adaptation networks",
      "authors": [
        "H Li",
        "Y.-M Jin",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2018",
      "venue": "Neural Information Processing"
    },
    {
      "citation_id": "74",
      "title": "Plug-and-play domain adaptation for crosssubject EEG-based emotion recognition",
      "authors": [
        "L Zhao",
        "X Yan",
        "B Lu"
      ],
      "year": "2021",
      "venue": "Plug-and-play domain adaptation for crosssubject EEG-based emotion recognition"
    },
    {
      "citation_id": "75",
      "title": "MS-MDA: Multisource marginal distribution adaptation for cross-subject and cross-session EEG emotion recognition",
      "authors": [
        "H Chen",
        "M Jin",
        "Z Li",
        "C Fan",
        "J Li",
        "H He"
      ],
      "year": "2021",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "76",
      "title": "MEERNet: multi-source EEGbased emotion recognition network for generalization across subjects and sessions",
      "authors": [
        "H Chen",
        "Z Li",
        "M Jin",
        "J Li"
      ],
      "year": "2021",
      "venue": "2021 43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)"
    },
    {
      "citation_id": "77",
      "title": "Reducing the subject variability of EEG signals with adversarial domain generalization",
      "authors": [
        "B.-Q Ma",
        "H Li",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2019",
      "venue": "Neural Information Processing"
    },
    {
      "citation_id": "78",
      "title": "A theory of learning from different domains",
      "authors": [
        "S Ben-David",
        "J Blitzer",
        "K Crammer",
        "A Kulesza",
        "F Pereira",
        "J Vaughan"
      ],
      "year": "2010",
      "venue": "Machine learning"
    },
    {
      "citation_id": "79",
      "title": "Impossibility theorems for domain adaptation",
      "authors": [
        "S David",
        "T Lu",
        "T Luu",
        "D Pál"
      ],
      "year": "2010",
      "venue": "Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics. JMLR Workshop and Conference Proceedings"
    },
    {
      "citation_id": "80",
      "title": "Detecting change in data streams",
      "authors": [
        "D Kifer",
        "S Ben-David",
        "J Gehrke"
      ],
      "year": "2004",
      "venue": "VLDB"
    },
    {
      "citation_id": "81",
      "title": "Analysis of representations for domain adaptation",
      "authors": [
        "S Ben-David",
        "J Blitzer",
        "K Crammer",
        "F Pereira"
      ],
      "year": "2006",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "82",
      "title": "Generalizing to unseen domains via distribution matching",
      "authors": [
        "I Albuquerque",
        "J Monteiro",
        "M Darvishi",
        "T Falk",
        "I Mitliagkas"
      ],
      "year": "2019",
      "venue": "Generalizing to unseen domains via distribution matching",
      "arxiv": "arXiv:1911.00804"
    },
    {
      "citation_id": "83",
      "title": "Adversarial multiple source domain adaptation",
      "authors": [
        "H Zhao",
        "S Zhang",
        "G Wu",
        "J Moura",
        "J Costeira",
        "G Gordon"
      ],
      "year": "2018",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "84",
      "title": "Domain adversarial neural networks for domain generalization: When it works and how to improve",
      "authors": [
        "A Sicilia",
        "X Zhao",
        "S Hwang"
      ],
      "year": "2021",
      "venue": "Domain adversarial neural networks for domain generalization: When it works and how to improve",
      "arxiv": "arXiv:2102.03924"
    },
    {
      "citation_id": "85",
      "title": "Comparing recognition performance and robustness of multimodal deep learning models for multimodal emotion recognition",
      "authors": [
        "W Liu",
        "J.-L Qiu",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "86",
      "title": "Off-line and on-line vigilance estimation based on linear dynamical system and manifold learning",
      "authors": [
        "L.-C Shi",
        "B.-L Lu"
      ],
      "year": "2010",
      "venue": "2010 Annual International Conference of the IEEE Engineering in Medicine and Biology"
    },
    {
      "citation_id": "87",
      "title": "Deep CORAL: Correlation alignment for deep domain adaptation",
      "authors": [
        "B Sun",
        "K Saenko"
      ],
      "year": "2016",
      "venue": "Computer Vision -ECCV 2016 Workshops"
    },
    {
      "citation_id": "88",
      "title": "Deep domain confusion: Maximizing for domain invariance",
      "authors": [
        "E Tzeng",
        "J Hoffman",
        "N Zhang",
        "K Saenko",
        "T Darrell"
      ],
      "year": "2014",
      "venue": "CoRR"
    },
    {
      "citation_id": "89",
      "title": "FlexMatch: Boosting semi-supervised learning with curriculum pseudo labeling",
      "authors": [
        "B Zhang",
        "Y Wang",
        "W Hou",
        "H Wu",
        "J Wang",
        "M Okumura",
        "T Shinozaki"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "90",
      "title": "FixMatch: simplifying semisupervised learning with consistency and confidence",
      "authors": [
        "K Sohn",
        "D Berthelot",
        "N Carlini",
        "Z Zhang",
        "H Zhang",
        "C Raffel",
        "E Cubuk",
        "A Kurakin",
        "C.-L Li"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "91",
      "title": "Domain adaptation via transfer component analysis",
      "authors": [
        "S Pan",
        "I Tsang",
        "J Kwok",
        "Q Yang"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Neural Networks"
    },
    {
      "citation_id": "92",
      "title": "Least squares support vector machine classifiers",
      "authors": [
        "J Suykens",
        "J Vandewalle"
      ],
      "year": "1999",
      "venue": "Neural Processing Letters"
    },
    {
      "citation_id": "93",
      "title": "Unsupervised visual domain adaptation using subspace alignment",
      "authors": [
        "B Fernando",
        "A Habrard",
        "M Sebban",
        "T Tuytelaars"
      ],
      "year": "2013",
      "venue": "Proceedings of the IEEE international conference on computer vision"
    },
    {
      "citation_id": "94",
      "title": "Kernel PCA and de-noising in feature spaces",
      "authors": [
        "S Mika",
        "B Schölkopf",
        "A Smola",
        "K.-R Müller",
        "M Scholz",
        "G Rätsch"
      ],
      "year": "1999",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "95",
      "title": "Random forests",
      "authors": [
        "Breiman"
      ],
      "year": "2001",
      "venue": "Machine Learning"
    },
    {
      "citation_id": "96",
      "title": "Multi-class AdaBoost",
      "authors": [
        "J Zhu",
        "A Arbor",
        "T Hastie"
      ],
      "year": "2006",
      "venue": "Statistics & Its Interface"
    },
    {
      "citation_id": "97",
      "title": "Return of frustratingly easy domain adaptation",
      "authors": [
        "B Sun",
        "J Feng",
        "K Saenko"
      ],
      "year": "2016",
      "venue": "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, ser. AAAI'16"
    },
    {
      "citation_id": "98",
      "title": "Geodesic flow kernel for unsupervised domain adaptation",
      "authors": [
        "B Gong",
        "Y Shi",
        "F Sha",
        "K Grauman"
      ],
      "year": "2012",
      "venue": "2012 IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "99",
      "title": "Alternative k-nearest neighbour rules in supervised pattern recognition: Part 1. k-nearest neighbour classification by using alternative voting rules",
      "authors": [
        "D Coomans",
        "D Massart"
      ],
      "year": "1982",
      "venue": "Analytica Chimica Acta"
    },
    {
      "citation_id": "100",
      "title": "SoftMatch: Addressing the quantity-quality trade-off in semi-supervised learning",
      "authors": [
        "H Chen",
        "R Tao",
        "Y Fan",
        "Y Wang",
        "J Wang",
        "B Schiele",
        "X Xie",
        "B Raj",
        "M Savvides"
      ],
      "year": "2023",
      "venue": "SoftMatch: Addressing the quantity-quality trade-off in semi-supervised learning",
      "arxiv": "arXiv:2301.10921"
    },
    {
      "citation_id": "101",
      "title": "Infinite mixture prototypes for few-shot learning",
      "authors": [
        "K Allen",
        "E Shelhamer",
        "H Shin",
        "J Tenenbaum"
      ],
      "year": "2019",
      "venue": "Proceedings of the 36th International Conference on Machine Learning, ser. Proceedings of Machine Learning"
    },
    {
      "citation_id": "102",
      "title": "Equivalence of distance-based and rkhs-based statistics in hypothesis testing",
      "authors": [
        "D Sejdinovic",
        "B Sriperumbudur",
        "A Gretton",
        "K Fukumizu"
      ],
      "year": "2013",
      "venue": "The annals of statistics"
    },
    {
      "citation_id": "103",
      "title": "Wasserstein generative adversarial networks",
      "authors": [
        "M Arjovsky",
        "S Chintala",
        "L Bottou"
      ],
      "year": "2017",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "104",
      "title": "Visualizing data using t-sne",
      "authors": [
        "V Laurens",
        "G Hinton"
      ],
      "year": "2008",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "105",
      "title": "EEG data augmentation for emotion recognition using a conditional Wasserstein GAN",
      "authors": [
        "Y Luo",
        "B.-L Lu"
      ],
      "year": "2018",
      "venue": "2018 40th annual international conference of the IEEE engineering in medicine and biology society (EMBC)"
    },
    {
      "citation_id": "106",
      "title": "Data augmentation for enhancing EEG-based emotion recognition with deep generative models",
      "authors": [
        "Y Luo",
        "L.-Z Zhu",
        "Z.-Y Wan",
        "B.-L Lu"
      ],
      "year": "2020",
      "venue": "Journal of Neural Engineering"
    },
    {
      "citation_id": "107",
      "title": "Sample estimate of the entropy of a random vector",
      "authors": [
        "L Kozachenko",
        "N Leonenko"
      ],
      "venue": "Problemy Peredachi Informatsii"
    },
    {
      "citation_id": "108",
      "title": "Estimating mutual information",
      "authors": [
        "A Kraskov",
        "H Stögbauer",
        "P Grassberger"
      ],
      "year": "2004",
      "venue": "Physical review E"
    },
    {
      "citation_id": "109",
      "title": "Mutual information between discrete and continuous data sets",
      "authors": [
        "B Ross"
      ],
      "year": "2014",
      "venue": "PloS one"
    },
    {
      "citation_id": "110",
      "title": "Pseudo-labeling and confirmation bias in deep semi-supervised learning",
      "authors": [
        "E Arazo",
        "D Ortego",
        "P Albert",
        "N O'connor",
        "K Mcguinness"
      ],
      "year": "2020",
      "venue": "2020 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "111",
      "title": "MDD patients and healthy controls EEG data (new). figshare,\" Dataset. MDD Patients and Healthy Controls EEG Data",
      "authors": [
        "W Mumtaz"
      ],
      "year": "2016",
      "venue": "MDD patients and healthy controls EEG data (new). figshare,\" Dataset. MDD Patients and Healthy Controls EEG Data",
      "doi": "10.6084/m9.figshare"
    }
  ]
}