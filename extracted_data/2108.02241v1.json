{
  "paper_id": "2108.02241v1",
  "title": "Attentive Cross-Modal Connections For Deep Multimodal Wearable-Based Emotion Recognition",
  "published": "2021-08-04T18:40:32Z",
  "authors": [
    "Anubhav Bhatti",
    "Behnam Behinaein",
    "Dirk Rodenburg",
    "Paul Hungler",
    "Ali Etemad"
  ],
  "keywords": [
    "Affective Computing",
    "Multimodal Representation Learning",
    "Attention Mechanism"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Classification of human emotions can play an essential role in the design and improvement of human-machine systems. While individual biological signals such as Electrocardiogram (ECG) and Electrodermal Activity (EDA) have been widely used for emotion recognition with machine learning methods, multimodal approaches generally fuse extracted features or final classification/regression results to boost performance. To enhance multimodal learning, we present a novel attentive cross-modal connection to share information between convolutional neural networks responsible for learning individual modalities. Specifically, these connections improve emotion classification by sharing intermediate representations among EDA and ECG and apply attention weights to the shared information, thus learning more effective multimodal embeddings. We perform experiments on the WESAD dataset to identify the best configuration of the proposed method for emotion classification. Our experiments show that the proposed approach is capable of learning strong multimodal representations and outperforms a number of baselines methods.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Affective computing is an evolving field that focuses on the development of systems and devices to recognize, interpret, process, and simulate human emotions  [1] ,  [2] . Previous studies have shown that signals such as Electrocardiogram (ECG) and Electrodermal Activity (EDA), also called Galvanic Skin Response, have the potential to detect affective states such as stress levels in humans  [3] -  [5] . Generally, different deep learning models can be used to extract features from each modality to be used in classifiers for predicting the emotion class  [6] -  [9] . On the other hand, different modalities often contain rich complementary or overlapping information that needs to be considered to boost performance through fusionbased representation learning strategies. Integrating multiple modalities, their learned features, and/or the intermediate decisions of machine learning models responsible for processing them for a discriminative task is referred to as multimodal fusion.\n\nFeature-level fusion, also referred to as early fusion, and decision-level fusion, also called late fusion, are two popular approaches for combining different modalities  [10] -  [13] . In the former strategy, representations extracted by the feature extractors are combined and then fed to a classifier for the classification task, while in the latter, decisions obtained based on the representations of individual modalities are combined to reach a final decision.\n\nRecent works  [14] ,  [15]  have shown that cross-connections between different hidden layers of neural networks responsible for learning different modalities may result in an effective exchange of learned representations for exploiting complementary or avoiding redundant information. This, in turn, may result in better performance. However, those studies have focused on directly connecting different modality streams without performing any additional processing on the exchanged information. Moreover, exhaustive experiments have not been presented to show the impact of different possible configurations of such cross-modal connections, for example, their location within a deep multimodal pipeline, or the source and the target modality of the shared information.\n\nIn this paper, we expand on this notion and propose a novel attentive cross-modal connection (henceforth, referred to as AttX connections) to learn strong shared representations for multimodal (ECG-EDA) affective computing. The crossconnections between the networks responsible for learning each modality use an attention mechanism to regulate the fusion of intermediate representations by learning the importance of each modality while training. We introduce three different variations of these connections for sharing information from ECG to EDA (Type I), EDA to ECG (Type II), and simultaneously sharing information between both (Type III).\n\nOur contributions in this paper can be summarized as follows.  (1)  We extend the notion of cross-modal connections by introducing an attention module to regulate the exchange of information between modalities. We also introduce three variations of the proposed connections to control the flow of information better. (2) We apply our proposed method to multimodal ECG-EDA emotion recognition and perform extensive experiments on the publicly available WESAD dataset to show the flexibility and impact of our method in obtaining better or competitive performance to the state-of-the-art. (3) Our experiments show that bi-directional sharing of information between ECG and EDA results in stronger and more effective representations instead of sharing ECG information with EDA alone or vice-versa. Moreover, our study shows that adding the proposed mechanism in multiple different locations within a multimodal pipeline is advantageous vs. restricting information sharing/fusion to a single point.   [14] ,  [15]  can be used for fusing H ecg and H eda , such methods fail to regulate the flow of information between the two modalities, thus fail to adaptively control fused information where necessary.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "B. Attentive Cross-Modal Connections (Attx)",
      "text": "In this section, details of attentive cross-modal connections are discussed. As shown in Figure  1 , we denote the inputs to the convolutional block j of ECG and EDA networks by X j ecg and X j eda , respectively. Accordingly, the input ECG and EDA to the network (first block) are X 1 ecg and X 1 eda , respectively. The learned representation obtained from feeding X 1  ecg to H ecg and X 1 eda to H eda , after convolutional block j of the network can be denoted by Z j ecg and Z j eda , respectively. To obtain rich multimodal representations through intermediate fusions of Z j ecg and Z j eda while allowing for flexible and adaptive regulation of the shared information, we propose attentive cross-connections (AttX) between H ecg and H eda . Attention is a mechanism that focuses on the most salient parts of the input or features by emphasizing (weights) on a subset of the feature set. Such mechanisms have been widely employed in natural language processing  [16] , computer vision  [17] , and bio-signal analysis  [18] . Here, we intend to utilize this mechanism for regulating the flow of information through intermediate cross-modal connections.\n\nTo do so, we first concatenate Z j ecg and Z j eda for a given j to obtain\n\n, where S j ∈ R n×m×d , d is the number of modalities, and n and m are the dimensions of Z j ecg (or Z j eda ). The tensor S j is subsequently fed to an attention layer where the weights for each modality are computed as  [19] :\n\nwhere W j ∈ R d×d is the learned weight matrix, ReLU is a rectified linear unit activation function, U j is the projection of the representation set S j , w j u ∈ R m is a learned weight vector, sof tmax() computes the softmax along the second axis, and θ j ∈ R n×d×m is the final calculated attention weight tensor. The transpose operation is done along the second and third axes. We set d = 2 as we have two modalities (ECG and EDA). We split θ j into two tensors, θ ecg = θ * ,1, * ∈ R n×m and θ eda = θ * ,2, * ∈ R n×m which are the attention weights for ECG and EDA, respectively. Accordingly, we define:\n\nwhere denotes the element-wise multiplication. Equation  3 indicates that the ECG and EDA representations are weighted by their computed attention matrices, respectively. Then the ECG and EDA inputs to the next convolutional layers are constructed as:\n\nrespectively. In the above equations, ⊗ denotes concatenation operation, X j+1 ecg represents the concatenated representations of Z j ecg and Ẑj eda , and X j+1 eda represents the concatenated representations of Z j eda and Ẑj ecg . The X j+1 ecg and X j+1 eda are batch normalized before feeding into the next layer.\n\nIn summary, our proposed attentive cross-modal connection exploits an intermediate set of learned representations from one modality and uses it in the other modality to boost the overall performance by taking advantage of the complementary information in both streams. Furthermore, our approach applies learned attention weights to both modalities to focus on the more salient modality where needed.\n\nTo study the effects of combining modalities at different locations within deep neural networks on the performance, we define three different types of attentive cross-modal connections (Type I, II, and III shown in Figure  1 ). These connections share intermediate feature representations between the modality streams. For connection Type I, the AttX block only computes the attention weights for ECG, denotes as θ ecg , that are then multiplied with Z are then used to transform Z j ecg and Z j eda to obtain Ẑj ecg and Ẑj eda , respectively. Further, these weighted intermediate representations are concatenated with Z j eda and Z j ecg using Equations 4 and 5. To specify the exact locations in which AttX connections are added in the network, we divide our network into four different stages. Figure  2  shows the AttX connections Type III at the end of stages 1, 2, and 3. The locations and combinations of the proposed AttX connections will be tuned empirically through several experiments, which will be discussed in Section III.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Network Architecture Details",
      "text": "As mentioned earlier, we require two different deep neural networks H ecg and H eda . The proposed network is depicted in Figure  2 . For simplicity, we first leave out the AttX and batch normalization operations from our description and tend to it once H ecg and H eda have been discussed. Here, H ecg is designed as a 1D CNN with a residual squeeze and excitation (SE-ResNet)  [20]   A global average pooling layer follows this to obtain feature representation Y ecg . The representation Y ecg is projected onto a latent space to obtain a smaller dimension, Y ecg , using two FC layers with output sizes of 64 followed by ReLU. For H eda , we design a CNN with 4 main blocks (see Figure  2 ). Similar to H ecg , the first stage is a single Conv. Block. Next, two consecutive Conv. Blocks are used for each of the following two stages. This is followed by two Conv. Blocks, an SE Block (Figure  2 (c )), and a global average pooling layer. The representation Y eda is projected onto a latent space to obtain a smaller dimension, Y eda , using 2 FC layers with output sizes of 64 followed by ReLU. Finally, the learned representations (Y ecg and Y eda ) are merged and fed to a classifier with 2 FC layers followed by a ReLU activation. The size of both FC layers is 128, and a softmax layer is used for classification. Given the H ecg and H eda models described above, the AttX mechanism (Type I, II, or III) proposed earlier can be integrated into different single stages or combinations of stages, followed by a batch normalization operation. Figure  2  (a) depicts a Type III AttX integrated into all three stages simultaneously.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iii. Experiments",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Dataset And Pre-Processing",
      "text": "We use the multimodal dataset on WEarable Stress and Affect Detection (WESAD)  [21] . This dataset has collected physiological signals from 15 subjects using two different devices worn by the subjects, one on the chest and the other on the wrist. For our study, we only consider the ECG and the EDA data sampled at 700 Hz. We perform a number of basic pre-processing steps to clean the ECG and EDA modalities. For ECG, a Butterworth bandpass filter is applied with a passband frequency of 5-15 Hz  [22] ,  [23] . The raw EDA signal is filtered using a lowpass filter with a cut-off frequency of 3 Hz. Both ECG and EDA are normalized using user-specific z-score normalization  [8] . ECG and EDA are then re-sampled from 700 Hz to 256 Hz and are segmented into 10-second windows with 50% overlap to form individual samples  [24] . To create binary stress vs. non-stress classes for evaluation purposes, the amusement and neutral classes were aggregated to create the non-stress class.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Implementation Details And Evaluation",
      "text": "Our model is implemented using Keras with TensorFlow backend on an NVIDIA GeForce RTX 2080 Ti GPU. To rigorously test our model, we use the Leave-One-Subject-Out (LOSO) evaluation scheme for binary classification. We use a standard Adam optimizer with a learning rate of 1 × 10 -3 and cross-entropy loss. A batch size of 16 is used, and the network is trained for 100 epochs. For evaluating our method, we use accuracy and F1-score with macro-averaging.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Performance And Comparison",
      "text": "We evaluate the performance of the proposed AttX connections when added to the network. We investigate both the location (stage) at which these connections can be added individually as well as their combinations. First, we add AttX Type I to stages 1 through 3. Next, we experiment by adding two AttX connections simultaneously, to stages 1 & 2, 1 & 3, and 2 & 3. This is followed by adding three AttX connections to stages 1, 2, & 3 at the same time. Next, we repeat the same experiments for AttX Type II, followed by AttX Type III.\n\nTable  I  shows the experimental results of the proposed AttX connections (Type I, II, and III) integrated into different stages of the model. From Table  I , we can conclude that in most cases sharing information in both directions among modalities helps the network to perform better than sharing in just one direction. In line with the above, we can observe that the best performance for binary classification is obtained by connecting AttX Type III connections to stages 1 & 2.\n\nNext, we compare our best performing configuration with state-of-the-art methods in Table  II . From Table  II , we observe that our model achieves an accuracy and F1 score of 92.08%  and 91.11, respectively, which are comparable to  [21]  and outperform  [13] ,  [25] , despite having used considerably fewer modalities. This points to the fact that our framework has been more effective in capturing the information in the ECG and EDA modalities. Lastly, we perform detailed ablations to evaluate the impact of each component in our multimodal solution and present the results in Table  III . Further, we observe that multi-stage information sharing (ours w/o attention) performs better than the feature-fusion and the uni-modal schemes. Finally, we also present the results of our full AttX with the best configuration, which shows considerable improvement in the results.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iv. Conclusion And Future Work",
      "text": "This paper proposed an attentive cross-modal connection, AttX, to learn strong shared representations from ECG and EDA for the multimodal classification of emotions. We proposed a model consisting of individual pipelines for ECG and EDA, and employed AttX connections to regulate the flow of intermediate information between the two pipelines. Three different variations of AttX were evaluated (Type I, II, and III). Our analysis suggests that the best performance is obtained when multiple AttX connections are added at different locations in the network. Moreover, AttX Type III proved more effective than Type I and II, indicating that bi-directional information sharing is more beneficial to the model as a whole. Lastly, in comparison to other multimodal solutions with even more modalities, our method showed comparable or better results.\n\nIt should be noted that while we observed interesting trends in the type, location, and number of AttX connections required to obtain optimum performance, additional studies can be performed in future work to further investigate its generalizability and impact on other datasets and modalities.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The three conﬁgurations for the proposed AttX is presented: Type",
      "page": 2
    },
    {
      "caption": "Figure 1: , we denote the inputs to",
      "page": 2
    },
    {
      "caption": "Figure 2: Our proposed multimodal pipeline along with AttX connections stress classiﬁcation (a). The symbol L represents addition, N represents concatenation,",
      "page": 3
    },
    {
      "caption": "Figure 2: shows the AttX",
      "page": 3
    },
    {
      "caption": "Figure 2: For simplicity, we ﬁrst leave out the AttX",
      "page": 3
    },
    {
      "caption": "Figure 2: (b)) is used followed by two SE ID Blocks (Figure",
      "page": 3
    },
    {
      "caption": "Figure 2: ). Similar to Hecg, the ﬁrst stage is a single Conv. Block.",
      "page": 3
    },
    {
      "caption": "Figure 2: (c)), and a global average pooling",
      "page": 3
    },
    {
      "caption": "Figure 2: (a) depicts a Type III AttX integrated into all three stages",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "Unimodal\nUnimodal\nFeat. Fus.\nOurs w/o att.\nOurs (AttX)",
          "Modality": "EDA\nECG\nEDA, ECG\nEDA, ECG\nEDA, ECG",
          "Accuracy Mac. F1": "78.65\n78.51\n81.49\n87.09\n92.08"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fusion Conﬁguration": "Connection Type",
          "Accuracy": "",
          "Macro F1": "",
          "Weighted F1": ""
        },
        {
          "Fusion Conﬁguration": "Feature-Level Fusion",
          "Accuracy": "81.49",
          "Macro F1": "78.60",
          "Weighted F1": "82.41"
        },
        {
          "Fusion Conﬁguration": "Type I\n(EDA→ECG)",
          "Accuracy": "84.37\n79.67\n87.63\n84.42\n84.13\n88.33\n88.72",
          "Macro F1": "82.64\n77.95\n86.39\n81.35\n81.13\n86.14\n87.85",
          "Weighted F1": "84.52\n79.72\n87.90\n85.37\n85.47\n89.05\n88.53"
        },
        {
          "Fusion Conﬁguration": "Type II\n(ECG→EDA)",
          "Accuracy": "87.85\n81.35\n81.74\n88.67\n85.90\n90.45\n87.93",
          "Macro F1": "85.90\n77.02\n79.85\n87.12\n82.59\n88.58\n85.80",
          "Weighted F1": "88.05\n83.11\n81.89\n88.96\n87.03\n90.58\n89.06"
        },
        {
          "Fusion Conﬁguration": "Type III\n(ECG↔EDA)",
          "Accuracy": "88.58\n85.42\n88.12\n92.08\n88.53\n89.06\n90.00",
          "Macro F1": "86.49\n82.14\n86.78\n91.11\n85.94\n87.24\n88.66",
          "Weighted F1": "88.78\n86.09\n88.12\n92.08\n88.78\n89.12\n89.91"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Affective computing",
      "authors": [
        "R Picard"
      ],
      "year": "2000",
      "venue": "Affective computing"
    },
    {
      "citation_id": "2",
      "title": "Affective computing: A review",
      "authors": [
        "J Tao",
        "T Tan"
      ],
      "year": "2005",
      "venue": "International Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "3",
      "title": "Stress detection using physiological sensors",
      "authors": [
        "R Sioni",
        "L Chittaro"
      ],
      "year": "2015",
      "venue": "Computer"
    },
    {
      "citation_id": "4",
      "title": "Feature selection framework for xgboost based on electrodermal activity in stress detection",
      "authors": [
        "C.-P Hsieh",
        "Y.-T Chen",
        "W.-K Beh",
        "A.-Y Wu"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Workshop on Signal Processing Systems (SiPS)"
    },
    {
      "citation_id": "5",
      "title": "Discriminating stress from cognitive load using a wearable eda device",
      "authors": [
        "C Setz",
        "B Arnrich",
        "J Schumm",
        "R La Marca",
        "G Tröster",
        "U Ehlert"
      ],
      "year": "2009",
      "venue": "IEEE Transactions on Information Technology in Biomedicine"
    },
    {
      "citation_id": "6",
      "title": "Deep ecgnet: An optimal deep learning framework for monitoring mental stress using ultra short-term ecg signals",
      "authors": [
        "B Hwang",
        "J You",
        "T Vaessen",
        "I Myin-Germeys",
        "C Park",
        "B.-T Zhang"
      ],
      "year": "2018",
      "venue": "TELEMEDICINE and e-HEALTH"
    },
    {
      "citation_id": "7",
      "title": "Classification of cognitive load and expertise for adaptive simulation using deep multitask learning",
      "authors": [
        "P Sarkar",
        "K Ross",
        "A Ruberto",
        "D Rodenbura",
        "P Hungler",
        "A Etemad"
      ],
      "year": "2019",
      "venue": "2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "8",
      "title": "Self-supervised ecg representation learning for emotion recognition",
      "authors": [
        "P Sarkar",
        "A Etemad"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "9",
      "title": "Self-supervised learning for ecg-based emotion recognition",
      "authors": [
        "P Sarkar",
        "A Etemad"
      ],
      "year": "2020",
      "venue": "2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "10",
      "title": "An explainable deep fusion network for affect recognition using physiological signals",
      "authors": [
        "J Lin",
        "S Pan",
        "C Lee",
        "S Oviatt"
      ],
      "year": "2019",
      "venue": "Proceedings of the 28th ACM International Conference on Information and Knowledge Management"
    },
    {
      "citation_id": "11",
      "title": "Continuous inference of psychological stress from sensory measurements collected in the natural environment",
      "authors": [
        "K Plarre",
        "A Raij",
        "S Hossain",
        "A Ali",
        "M Nakajima",
        "M Al'absi",
        "E Ertin",
        "T Kamarck",
        "S Kumar",
        "M Scott"
      ],
      "year": "2011",
      "venue": "Proceedings of the 10th ACM/IEEE International Conference on Information Processing in Sensor Networks"
    },
    {
      "citation_id": "12",
      "title": "Utilizing deep learning towards multi-modal bio-sensing and vision-based affective computing",
      "authors": [
        "S Siddharth",
        "T.-P Jung",
        "T Sejnowski"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "13",
      "title": "Emotion assessment using feature fusion and decision fusion classification based on physiological data: Are we there yet?",
      "authors": [
        "P Bota",
        "C Wang",
        "A Fred",
        "H Silva"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "14",
      "title": "X-cnn: Crossmodal convolutional neural networks for sparse datasets",
      "authors": [
        "P Veličković",
        "D Wang",
        "N Lane",
        "P Liò"
      ],
      "year": "2016",
      "venue": "2016 IEEE Symposium Series on Computational Intelligence (SSCI)"
    },
    {
      "citation_id": "15",
      "title": "Xflow: Cross-modal deep neural networks for audiovisual classification",
      "authors": [
        "C Cangea",
        "P Veličković",
        "P Liò"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "16",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "17",
      "title": "An introductory survey on attention mechanisms in computer vision problems",
      "authors": [
        "J Sun",
        "J Jiang",
        "Y Liu"
      ],
      "year": "2020",
      "venue": "2020 6th International Conference on Big Data and Information Analytics (BigDIA)"
    },
    {
      "citation_id": "18",
      "title": "Classification of hand movements from eeg using a deep attention-based lstm network",
      "authors": [
        "G Zhang",
        "V Davoodnia",
        "A Sepas-Moghaddam",
        "Y Zhang",
        "A Etemad"
      ],
      "year": "2020",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "19",
      "title": "Multi-level multiple attentions for contextual multimodal sentiment analysis",
      "authors": [
        "S Poria",
        "E Cambria",
        "D Hazarika",
        "N Mazumder",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Data Mining (ICDM)"
    },
    {
      "citation_id": "20",
      "title": "Squeeze-and-excitation networks",
      "authors": [
        "J Hu",
        "L Shen",
        "G Sun"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "21",
      "title": "Introducing wesad, a multimodal dataset for wearable stress and affect detection",
      "authors": [
        "P Schmidt",
        "A Reiss",
        "R Duerichen",
        "C Marberger",
        "K Van Laerhoven"
      ],
      "year": "2018",
      "venue": "Proceedings of the 20th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "22",
      "title": "A real-time QRS detection algorithm",
      "authors": [
        "J Pan",
        "W Tompkins"
      ],
      "year": "1985",
      "venue": "IEEE Transaction on Biomedical Engineering"
    },
    {
      "citation_id": "23",
      "title": "Unsupervised multi-modal representation learning for affective computing with multi-corpus wearable data",
      "authors": [
        "K Ross",
        "P Hungler",
        "A Etemad"
      ],
      "year": "2020",
      "venue": "Unsupervised multi-modal representation learning for affective computing with multi-corpus wearable data"
    },
    {
      "citation_id": "24",
      "title": "Ecg biometric recognition without fiducial detection",
      "authors": [
        "K Plataniotis",
        "D Hatzinakos",
        "J Lee"
      ],
      "year": "2006",
      "venue": "2006 Biometrics symposium: Special session on research at the biometric consortium conference"
    },
    {
      "citation_id": "25",
      "title": "Evaluating KNN performance on WESAD dataset",
      "authors": [
        "D Bajpai",
        "L He"
      ],
      "year": "2020",
      "venue": "2020 12th International Conference on Computational Intelligence and Communication Networks (CICN)"
    }
  ]
}