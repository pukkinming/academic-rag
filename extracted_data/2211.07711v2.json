{
  "paper_id": "2211.07711v2",
  "title": "Multilevel Transformer For Multimodal Emotion Recognition",
  "published": "2022-10-26T10:31:24Z",
  "authors": [
    "Junyi He",
    "Meimei Wu",
    "Meng Li",
    "Xiaobo Zhu",
    "Feng Ye"
  ],
  "keywords": [
    "multi-granularity emotion recognition",
    "multilevel transformer",
    "highway network",
    "fine-grained interaction",
    "Bert"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal emotion recognition has attracted much attention recently. Fusing multiple modalities effectively with limited labeled data is a challenging task. Considering the success of pre-trained model and fine-grained nature of emotion expression, we think it is reasonable to take these two aspects into consideration. Unlike previous methods that mainly focus on one aspect, we introduce a novel multi-granularity framework, which combines fine-grained representation with pre-trained utterance-level representation. Inspired by Transformer TTS, we propose a multilevel transformer model to perform fine-grained multimodal emotion recognition. Specifically, we explore different methods to incorporate phoneme-level embedding with word-level embedding. To perform multi-granularity learning, we simply combine multilevel transformer model with Bert. Extensive experimental results show that multilevel transformer model outperforms previous state-of-the-art approaches on IEMOCAP dataset. Multi-granularity model achieves additional performance improvement.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech emotion recognition (SER) is a promising area of research mainly for human-computer interaction system, which aims to recognize the emotion state (such as happy, angry, and sad) of a speaker from his/her speech  [1] . There are mainly two challenges for the SER task. One is the lack of large-scale labeled data since labeling emotion is subjective and multi-person annotation is needed, which requires a lot of time and human effort  [2] . The other challenge is that emotion expression is multimodal and fine-grained  [3] . How to combine different modalities effectively is a long way to explore.\n\nOne common solution to limited labeled data is to leverage transfer learning-based approaches. Recently a class of techniques known as self-supervised learning (SSL) architectures have achieved state-of-the-art (SOTA) performance in natural language processing (NLP)  [4, 5, 6]  and speech recognition  [7, 8, 9] . For emotion recognition task, F. A. Acheampong et al.  [10]  and L. Pepino et al.  [11]  have done great jobs with text or speech pre-trained models respectively. However the above methods only focused on one modality.\n\nA large number of approaches have been developed to learn the interaction between different modalities. For the approaches with the pre-trained models, S. Siriwardhana et al.  [12]  and Z. Zhao et al.  [2]  explored early fusion and late fusion of text and speech representations respectively for emotion recognition leveraging both Bert  [4]  related and Wav2vec  [7]  related models. The results showed that late fusion models generally got better results. However, the above interaction of late fusion between different modalities was only based on aggregated pre-trained text and speech embedding. For the approaches without pre-trained models, researchers utilized different models  [13, 14, 3]  to interact different modalities. S. Yoon et al.  [13]  built a deep neural network with recurrent neural networks (RNN) to learn vocal representations and text representations and then concatenated them for emotion classification. However the approach was based on utterance-level fusion. H. Xu et al.  [14]  proposed a fine-grained method to learn the alignment between speech and text, together with long short-term memory (LSTM) network to model the sequence for emotion recognition. Nevertheless, LSTM only consumed the input sequentially and the interaction within a single modality was not fully explored. H. Li et al.  [3]  proposed a fine-grained emotion recognition model with a temporal alignment mean-max pooling operation and cross-modality mechanism. Nevertheless, it required additional work for labeling and alignment prediction to use aligned information in production system. SSL enables us to use a large unlabelled dataset to train models that can be later used to extract representations and fine-tune for specific problems with limited amount of labeled data  [12] . However, the above-mentioned aggregated pre-trained embedding is a good representation for the entire sentence, not for the specific words or voice fragments. To further improve the performance of SER, we need to explore an effective way to add fine-grained interaction between different modalities with limited additional human effort. Transformer TTS  [15]  is a fine-grained model in the text to speech (TTS) area. For the training process, with the phoneme and mel sequence as input, Transformer TTS network generates mel spectrogram. Inspired by this work, we can use the similar structure to utilize audio and text information at fine-grained level without additional alignment. Phoneme sequence plays an important role to generate mel spectrogram in the TTS task. For the SER task, sometimes the stress of a sentence is on some specific phonemes, however compared with the word input, only phoneme information is not enough.\n\nTo overcome above challenges, we propose a novel multigranularity framework to merge pre-trained utterance-level representation with fine-grained representation. For the fine-grained part, we propose a multilevel transformer model to introduce crossmodal interaction among voice fragments, words, and phonemes. We compare different methods to incorporate the phoneme embedding with word embedding. Vanilla transformer  [16]  is added to further aggregate the sequential multimodal representations. To perform multi-granularity learning, we simply combine multilevel transformer model with the pre-trained model. In this article, the pre-trained model that we choose is Bert  [4] . Our experimental results on the Interactive Emotional Dyadic Motion Capture (IEMOCAP)  [17]  dataset show that multilevel transformer model achieves state-of-the-art results. The multi-granularity model yields additional performance boost.\n\nIn summary, our main contributions are as follows:\n\n• We conduct fine-grained learning with multilevel transformer model (Section 3.1) to obtain fine-grained cross modality information from voice fragments, words, and phonemes.\n\n• We propose a simple but effective multi-granularity fusion arXiv:2211.07711v2 [cs.CL] 16 Nov 2022 framework to combine fine-grained representation with pre-trained utterance-level representation (Section 3.2).\n\n• We design and evaluate our approaches quantitatively on IEMOCAP dataset. Experimental results show that multilevel transformer model outperforms existing state-of-the-art methods. Multi-granularity model gives additional performance improvement (Section 4).",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "After the classical machine learning models such as the Hidden Markov Model  [18]  and the Gaussian Mixture Model  [19] , were employed based on handcrafted low-level features or statistical high-level features, models with deep neural networks have been actively studied in SER. D. Bertero et al.  [20]  proposed the model consisting of the convolution neural network (CNN) that extracted high-level features from raw spectrogram features. A. Satt et al.  [21]  proposed an end-to-end model with CNN and LSTM network to capture the contextual information.\n\nRecently, multimodal models that make use of both audio and text information for SER have attracted much attention. S. Yoon et al.  [13]  employed RNN to encode audio and text and then used the last hidden state of a recurrent modality encoder as a query and used the other encoded modality as a key-value pair in the attention mechanism. However the interaction between different modalities was not fully explored. H. Xu et al.  [14]  designed the model with LSTM to learn the alignment between the audio and text from the attention mechanism. However, the interaction within a single modality was not fully explored. H. Li et al.  [3]  proposed a fine-grained emotion recognition model from aligned audio and text by using temporal mean-max alignment pooling method and cross modality module, which needed aligned audio and text as input.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Proposed Methods",
      "text": "In this section, we first introduce our multilevel transformer model. Then, we present our multi-granularity model, consisting of multilevel transformer model and Bert.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Multilevel Transformer Model",
      "text": "Here we first introduce the overall architecture of our multilevel transformer model. Then we focus on the detailed parts.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Architecture",
      "text": "Transformer TTS  [15]  is a neural TTS model based on Tacotron2  [22]  and transformer  [16] . Inspired by Transformer TTS, we propose our multilevel transformer model. As shown in Fig  1,   text input is firstly transformed into phoneme and word. After that, the output is processed by the highway network  [23] , followed by encoder prenet (3-layer CNN and 1-layer projection), and then is fed into the text encoder. The mel spectrogram is processed with a 2-layer fully connected network. The output of fully connected network and previous text encoder is sent into cross-modality interaction module, followed by deep fusion module. The output of the deep fusion module is used to predict emotion category probability.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Overall Process",
      "text": "For the SER task, we utilize the actual mel spectrogram and text information not only in the training stage, but also in the inference stage. What's more, similar to BERT's [class] token, we prepend one dummy mel vector to the sequence of actual mel spectrogram m = (m dummy , m1, m2, ..., m T ). In TTS task, the dummy mel vector is used to predict the first mel spectrogram. In our scenario, it is used to the calculate the final aggregated representation.\n\nFor the TTS task, during the inference stage, TTS converts an input text sequence x = (x1, x2, ..., xT ) into an output mel spectrogram sequence o = (o1, o2, ..., o T ) and each predicted ot is conditioned on predicted outputs o1, o2, ..., ot-1. This conversion can be formulated as the following conditional probability:\n\nNevertheless, our task is to predict the emotion category with whole available information, not to predict mel spectrogram in a sequence to sequence manner. In the inference stage, we combine the text input with whole golden mel spectrogram sequence instead of the previous predicted one. Thus the emotion category probability p can be computed by:\n\nwhere g(x, m) is the function that calculates the probability of each emotion category with text input and mel spectrogram.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Text To Phoneme And Word Embedding",
      "text": "Similar to Transformer TTS, text input is firstly transformed into the phoneme sequence, which carries fine-grained information. In our scenario, phoneme information is also useful since in some circumstance, the focus of the emotion is on some specific phonemes. Following  [24, 25] , we obtain the phoneme level embedding of each word using CNN. The outputs of the CNN are max-pooled over the entire width to obtain a fixed-size vector for each word.\n\nIn addition, the word level information is also important for emotion recognition. So we add the word embedding via Glove  [26]  since it carries additional fine-grained information.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Combination Of Phoneme And Word Embedding",
      "text": "Here we explore two different combination methods.\n\nConcatenation is a straightforward way to combine phoneme embedding with word embedding. For the first method, the above embedding is simply concatenated, followed by encoder pre-net.\n\nFor the other method, we try to use highway network  [23]  since it usually utilizes the gating mechanism to pass information efficiently through several layers. With the reference of  [24] , The concatenation of the phoneme and word embedding vectors u is passed to a two-layer highway network to fuse multi-level information effectively:\n\nwhere H(u) is a parametric transformation (an affine projection followed by ReLU  [27] ) of the input u and T (u) is a gating unit, which controls how much transformation is applied and how much copy of the original input is activated. Then, the multi-level textual information is processed by the encoder pre-net to model long-term context.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Transformer Modules",
      "text": "We use vanilla transformer  [16]  structure for text encoder, crossmodality interaction module and deep fusion module. Text encoder contains self-attention layer to fuse information from words and phonemes. Cross-modality interaction module includes selfattention layer and encoder-decoder attention layer to integrate the output from text encoder with phoneme information. We add vanilla transformer blocks including self-attention layer to deeply fuse multimodal sequential representations after the cross-modality interaction module. Finally, corresponding to the dummy mel input, we take the first output vector of deep fusion module as the global representation and apply a linear projection based on it with logits output.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Loss",
      "text": "For the TTS task, Transformer TTS model generates mel spectrogram and stop token. The predicted ones are compared with the ground truth to calculate the TTS loss.\n\nInspired by  [28] , we try to adopt multi-task learning to optimize the joint loss of TTS and SER, however the performance does not improve in our scenario. So only the logits of the last projection layer are used to classify the input example, with cross entropy as the loss function:  where p i,k is the probabilistic value that indicates the final probabilities of class k of utterance i and y i,k = 1 if the i-th sample belongs to k-th class.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Multi-Granularity Model",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Basic Components Introduction",
      "text": "Bert has achieved SOTA performance in natural language processing. Bert model consists of 12 layers and the embedding dimension size is 768.\n\nFor the multilevel transformer part, the structure is the same with our previous introduction.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Model Pipeline",
      "text": "Our multi-granularity model is depicted in Fig.  2 . The success of Bert model in sentence classification tasks highlights the effective use of the CLS token, which can be used as a representation for the entire sequence  [12] . Hence, we utilize the CLS embedding of Bert to obtain pre-trained utterance representation.\n\nThe CLS embedding generated from deep fusion module of multilevel transformer model is used to provide fine-grained multimodal representation.\n\nA late fusion mechanism followed by a classification head works remarkably well with fine-tuned \"Bert-Like\" pre-trained SSL models even in a multimodal setting  [12] . With the reference of  [29] , after the above CLS embedding of different models is processed by the projection respectively, we simply concatenate the outputs. Finally we send the concatenated embedding through the classification head, which includes a fully connected layer that outputs logits.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Dataset",
      "text": "We use the IEMOCAP  [17]  dataset, which is the most widely used dataset in emotion recognition research. It contains approximately To be comparable with previous related researches  [13] , 4 categories of emotions are used: angry (1103 utterances), sad (1084 utterances), neutral (1708 utterances) and happy (1636 utterances, merged with excited), resulting in a total of 5531 utterances. We perform a 5-fold cross-validation with 3, 1, 1 in train, dev, and test sets respectively. Every experiment is run for 3 times to reduce randomness, and the averaged result is used as the final performance score.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Implementation Detail",
      "text": "We implement the proposed models by using the PyTorch deep learning framework. For the acoustic data, we extract the 128dimensional filterbank features from speech signals. The window size and hop size are set to 25ms and 12ms respectively. For the text data, we use 300-dimensional Glove  [26]  embedding for the word. The hidden size of all transformer layers is set to 128. Both models are trained on a Tesla V100 GPU. Adam optimizer  [30]  is chosen. Learning rate is set to 1e-5 and batch size is set to 4. Weighted accuracy (WA) of the validation data set is used as the early stop criteria. Weighted accuracy (WA) and unweighted accuracy (UA) are calculated for test data set.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Multilevel Transformer Performance Evaluation",
      "text": "We evaluate our multilevel transformer model on the IEMOCAP dataset. For a fair comparison, all the approaches are implemented based on the same dataset with 5-fold cross validation configuration. The results are presented in the first block of Table  1 . From the table, we can see that multilevel transformer model outperforms all the baseline models.\n\nFor the ablation study, we conduct several experiments to evaluate key factors in our proposed model. In the second block of Table  1 , we find that word with Glove  [26]  embedding improves the performance a lot compared with the phoneme sequence. We should also mention that  [3]  needs the aligned audio and text as input which requires additional work for production use. However, by virtue of the cross-modality interaction module, multilevel transformer model with word input achieves the competitive results without alignment information. The results also show that the input with both word and phoneme information yields additional performance improvement. Compared with the concatenation, highway network lifts the per- We compare the number of transformer layers in different modules to further explore the performance impact. In Table  2 , we find that for the IEMOCAP dataset, transformer structure with one or two layers usually gets the better results for multilevel transformer model. Multilevel transformer model achieves the best results with 1-layer text encoder, 1-layer cross-modality interaction module, and 2-layer deep fusion module.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Multi-Granularity Model Performance Evaluation",
      "text": "We compare the performance between multi-granularity model and its components. As shown in Table  3 , our multi-granularity model shows better results than Bert and multilevel transformer model. It is easy and straightforward to combine fine-grained representation with pre-trained utterance-level representation to further improve the performance for emotion recognition task.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper, we first propose multilevel transformer model to perform fine-grained interaction between different modalities from voice fragments, words, and phonemes for speech emotion recognition. As per our knowledge, this is the first time that Transformer TTS structure is used in SER task. Then we introduce a multi-granularity framework to integrate fine-grained representation with pre-trained utterance-level representation in a simple but effective way. Extensive experiment results show that multilevel transformer model outperforms existing state-of-the-art methods. Multi-granularity model achieves additional performance improvement. We think this method can be taken as a reference for other pretrained models. We will make the code publicly available. In future, we will further explore the way to perform multi-granularity emotion recognition with acoustic pre-trained models, such as Wav2vec 2.0.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , text input",
      "page": 2
    },
    {
      "caption": "Figure 1: Overview of multilevel transformer.",
      "page": 2
    },
    {
      "caption": "Figure 2: Overview of the multi-granularity model.",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "360 DigiTech, China": "different models [13, 14, 3] to interact different modalities. S. Yoon"
        },
        {
          "360 DigiTech, China": "et al.\n[13] built a deep neural network with recurrent neural net-"
        },
        {
          "360 DigiTech, China": ""
        },
        {
          "360 DigiTech, China": "works (RNN) to learn vocal representations and text representations"
        },
        {
          "360 DigiTech, China": ""
        },
        {
          "360 DigiTech, China": "and then concatenated them for emotion classiﬁcation. However the"
        },
        {
          "360 DigiTech, China": ""
        },
        {
          "360 DigiTech, China": "approach was based on utterance-level fusion. H. Xu et al. [14] pro-"
        },
        {
          "360 DigiTech, China": ""
        },
        {
          "360 DigiTech, China": "posed a ﬁne-grained method to learn the alignment between speech"
        },
        {
          "360 DigiTech, China": ""
        },
        {
          "360 DigiTech, China": "and text, together with long short-term memory (LSTM) network to"
        },
        {
          "360 DigiTech, China": ""
        },
        {
          "360 DigiTech, China": "model\nthe sequence for emotion recognition. Nevertheless, LSTM"
        },
        {
          "360 DigiTech, China": ""
        },
        {
          "360 DigiTech, China": "only consumed the input sequentially and the interaction within a"
        },
        {
          "360 DigiTech, China": ""
        },
        {
          "360 DigiTech, China": "single modality was not fully explored. H. Li et al.\n[3] proposed a"
        },
        {
          "360 DigiTech, China": ""
        },
        {
          "360 DigiTech, China": "ﬁne-grained emotion recognition model with a temporal alignment"
        },
        {
          "360 DigiTech, China": ""
        },
        {
          "360 DigiTech, China": "mean-max pooling operation and cross-modality mechanism. Nev-"
        },
        {
          "360 DigiTech, China": ""
        },
        {
          "360 DigiTech, China": "ertheless, it required additional work for labeling and alignment pre-"
        },
        {
          "360 DigiTech, China": ""
        },
        {
          "360 DigiTech, China": "diction to use aligned information in production system."
        },
        {
          "360 DigiTech, China": ""
        },
        {
          "360 DigiTech, China": "SSL enables us to use a large unlabelled dataset\nto train mod-"
        },
        {
          "360 DigiTech, China": ""
        },
        {
          "360 DigiTech, China": "els that can be later used to extract representations and ﬁne-tune for"
        },
        {
          "360 DigiTech, China": ""
        },
        {
          "360 DigiTech, China": "speciﬁc problems with limited amount of\nlabeled data [12]. How-"
        },
        {
          "360 DigiTech, China": ""
        },
        {
          "360 DigiTech, China": "ever,\nthe above-mentioned aggregated pre-trained embedding is a"
        },
        {
          "360 DigiTech, China": ""
        },
        {
          "360 DigiTech, China": "good representation for the entire sentence, not for the speciﬁc words"
        },
        {
          "360 DigiTech, China": ""
        },
        {
          "360 DigiTech, China": "or voice fragments.\nTo further\nimprove the performance of SER,"
        },
        {
          "360 DigiTech, China": ""
        },
        {
          "360 DigiTech, China": "we need to explore an effective way to add ﬁne-grained interaction"
        },
        {
          "360 DigiTech, China": "between different modalities with limited additional human effort."
        },
        {
          "360 DigiTech, China": ""
        },
        {
          "360 DigiTech, China": "Transformer TTS [15] is a ﬁne-grained model\nin the text\nto speech"
        },
        {
          "360 DigiTech, China": "(TTS) area.\nFor\nthe training process, with the phoneme and mel"
        },
        {
          "360 DigiTech, China": ""
        },
        {
          "360 DigiTech, China": "sequence as input, Transformer TTS network generates mel spec-"
        },
        {
          "360 DigiTech, China": ""
        },
        {
          "360 DigiTech, China": "trogram.\nInspired by this work, we can use the similar structure to"
        },
        {
          "360 DigiTech, China": ""
        },
        {
          "360 DigiTech, China": "utilize audio and text information at ﬁne-grained level without addi-"
        },
        {
          "360 DigiTech, China": ""
        },
        {
          "360 DigiTech, China": "tional alignment. Phoneme sequence plays an important role to gen-"
        },
        {
          "360 DigiTech, China": ""
        },
        {
          "360 DigiTech, China": "erate mel spectrogram in the TTS task. For the SER task, sometimes"
        },
        {
          "360 DigiTech, China": ""
        },
        {
          "360 DigiTech, China": "the stress of a sentence is on some speciﬁc phonemes, however com-"
        },
        {
          "360 DigiTech, China": ""
        },
        {
          "360 DigiTech, China": "pared with the word input, only phoneme information is not enough."
        },
        {
          "360 DigiTech, China": ""
        },
        {
          "360 DigiTech, China": "To\novercome\nabove\nchallenges, we\npropose\na\nnovel multi-"
        },
        {
          "360 DigiTech, China": ""
        },
        {
          "360 DigiTech, China": "granularity\nframework\nto merge\npre-trained\nutterance-level\nrep-"
        },
        {
          "360 DigiTech, China": ""
        },
        {
          "360 DigiTech, China": "resentation with ﬁne-grained representation.\nFor\nthe ﬁne-grained"
        },
        {
          "360 DigiTech, China": ""
        },
        {
          "360 DigiTech, China": "part, we propose a multilevel transformer model to introduce cross-"
        },
        {
          "360 DigiTech, China": ""
        },
        {
          "360 DigiTech, China": "modal\ninteraction among voice fragments, words, and phonemes."
        },
        {
          "360 DigiTech, China": ""
        },
        {
          "360 DigiTech, China": "We\ncompare different methods\nto incorporate\nthe phoneme\nem-"
        },
        {
          "360 DigiTech, China": ""
        },
        {
          "360 DigiTech, China": "bedding with word embedding. Vanilla transformer\n[16]\nis added"
        },
        {
          "360 DigiTech, China": ""
        },
        {
          "360 DigiTech, China": "to further aggregate the sequential multimodal\nrepresentations. To"
        },
        {
          "360 DigiTech, China": ""
        },
        {
          "360 DigiTech, China": "perform multi-granularity learning, we simply combine multilevel"
        },
        {
          "360 DigiTech, China": ""
        },
        {
          "360 DigiTech, China": "transformer model with the pre-trained model.\nIn this article,\nthe"
        },
        {
          "360 DigiTech, China": ""
        },
        {
          "360 DigiTech, China": "pre-trained model\nthat we\nchoose\nis Bert\n[4].\nOur\nexperimen-"
        },
        {
          "360 DigiTech, China": ""
        },
        {
          "360 DigiTech, China": "tal\nresults on the\nInteractive Emotional Dyadic Motion Capture"
        },
        {
          "360 DigiTech, China": ""
        },
        {
          "360 DigiTech, China": "(IEMOCAP)\n[17] dataset\nshow that multilevel\ntransformer model"
        },
        {
          "360 DigiTech, China": ""
        },
        {
          "360 DigiTech, China": "achieves state-of-the-art results. The multi-granularity model yields"
        },
        {
          "360 DigiTech, China": ""
        },
        {
          "360 DigiTech, China": "additional performance boost."
        },
        {
          "360 DigiTech, China": ""
        },
        {
          "360 DigiTech, China": "In summary, our main contributions are as follows:"
        },
        {
          "360 DigiTech, China": ""
        },
        {
          "360 DigiTech, China": "• We conduct ﬁne-grained learning with multilevel\ntransformer"
        },
        {
          "360 DigiTech, China": "model (Section 3.1) to obtain ﬁne-grained cross modality informa-"
        },
        {
          "360 DigiTech, China": "tion from voice fragments, words, and phonemes."
        },
        {
          "360 DigiTech, China": "• We propose\na\nsimple but\neffective multi-granularity fusion"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "framework to combine ﬁne-grained representation with pre-trained": ""
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": "utterance-level representation (Section 3.2)."
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": "• We\ndesign\nand\nevaluate\nour\napproaches\nquantitatively\non"
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": ""
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": "IEMOCAP\ndataset.\nExperimental\nresults\nshow that multilevel"
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": "transformer model outperforms\nexisting state-of-the-art methods."
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": ""
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": "Multi-granularity model gives additional performance improvement"
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": ""
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": "(Section 4)."
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": ""
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": ""
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": "2. RELATED WORK"
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": ""
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": "After\nthe\nclassical machine\nlearning models\nsuch as\nthe Hidden"
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": ""
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": "Markov Model\n[18] and the Gaussian Mixture Model\n[19], were"
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": ""
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": "employed\nbased\non\nhandcrafted\nlow-level\nfeatures\nor\nstatistical"
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": ""
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": "high-level\nfeatures, models with deep neural networks have been"
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": "actively studied in SER. D. Bertero et al.\n[20] proposed the model"
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": "consisting of\nthe convolution neural network (CNN)\nthat extracted"
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": ""
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": "high-level\nfeatures\nfrom raw spectrogram features.\nA. Satt et al."
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": ""
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": "[21] proposed an end-to-end model with CNN and LSTM network"
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": ""
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": "to capture the contextual information."
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": ""
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": "Recently, multimodal models that make use of both audio and"
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": ""
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": "text\ninformation for SER have attracted much attention. S. Yoon et"
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": ""
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": "al.\n[13] employed RNN to encode audio and text and then used the"
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": ""
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": "last hidden state of a recurrent modality encoder as a query and used"
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": ""
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": "the other encoded modality as a key-value pair in the attention mech-"
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": "anism. However the interaction between different modalities was not"
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": "fully explored. H. Xu et al.\n[14] designed the model with LSTM to"
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": ""
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": "learn the alignment between the audio and text\nfrom the attention"
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": ""
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": "mechanism. However,\nthe interaction within a single modality was"
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": "not fully explored. H. Li et al.\n[3] proposed a ﬁne-grained emotion"
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": ""
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": ""
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": "recognition model\nfrom aligned audio and text by using temporal"
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": "mean-max alignment pooling method and cross modality module,"
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": "which needed aligned audio and text as input."
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": ""
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": ""
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": "3. PROPOSED METHODS"
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": "In this section, we ﬁrst\nintroduce our multilevel\ntransformer model."
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": ""
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": "Then, we present our multi-granularity model, consisting of multi-"
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": "level transformer model and Bert."
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": ""
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": "3.1. Multilevel transformer model"
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": ""
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": "Here we ﬁrst\nintroduce the overall architecture of our multilevel"
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": ""
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": "transformer model. Then we focus on the detailed parts."
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": ""
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": ""
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": "3.1.1. Architecture"
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": ""
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": "Transformer TTS [15]\nis a neural TTS model based on Tacotron2"
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": ""
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": "[22] and transformer\n[16].\nInspired by Transformer TTS, we pro-"
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": ""
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": "pose our multilevel transformer model. As shown in Fig 1, text input"
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": "is ﬁrstly transformed into phoneme and word. After that, the output"
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": "is processed by the highway network [23], followed by encoder pre-"
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": "net (3-layer CNN and 1-layer projection), and then is fed into the text"
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": ""
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": "encoder. The mel spectrogram is processed with a 2-layer fully con-"
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": ""
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": "nected network. The output of fully connected network and previous"
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": ""
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": "text encoder is sent into cross-modality interaction module, followed"
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": ""
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": "by deep fusion module. The output of the deep fusion module is used"
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": ""
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": "to predict emotion category probability."
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": ""
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": "3.1.2. Overall process"
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": "For\nthe SER task, we utilize the actual mel spectrogram and text"
        },
        {
          "framework to combine ﬁne-grained representation with pre-trained": "information not only in the training stage, but also in the inference"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3.1.3.\nText to phoneme and word embedding": "Similar to Transformer TTS, text input is ﬁrstly transformed into the"
        },
        {
          "3.1.3.\nText to phoneme and word embedding": "phoneme sequence, which carries ﬁne-grained information.\nIn our"
        },
        {
          "3.1.3.\nText to phoneme and word embedding": ""
        },
        {
          "3.1.3.\nText to phoneme and word embedding": "scenario, phoneme information is also useful since in some circum-"
        },
        {
          "3.1.3.\nText to phoneme and word embedding": "stance, the focus of the emotion is on some speciﬁc phonemes."
        },
        {
          "3.1.3.\nText to phoneme and word embedding": "Following [24, 25], we obtain the phoneme level embedding of"
        },
        {
          "3.1.3.\nText to phoneme and word embedding": "each word using CNN. The outputs of the CNN are max-pooled over"
        },
        {
          "3.1.3.\nText to phoneme and word embedding": "the entire width to obtain a ﬁxed-size vector for each word."
        },
        {
          "3.1.3.\nText to phoneme and word embedding": "In addition,\nthe word level\ninformation is also important\nfor"
        },
        {
          "3.1.3.\nText to phoneme and word embedding": "emotion recognition. So we add the word embedding via Glove [26]"
        },
        {
          "3.1.3.\nText to phoneme and word embedding": "since it carries additional ﬁne-grained information."
        },
        {
          "3.1.3.\nText to phoneme and word embedding": "3.1.4. Combination of phoneme and word embedding"
        },
        {
          "3.1.3.\nText to phoneme and word embedding": "Here we explore two different combination methods."
        },
        {
          "3.1.3.\nText to phoneme and word embedding": ""
        },
        {
          "3.1.3.\nText to phoneme and word embedding": ""
        },
        {
          "3.1.3.\nText to phoneme and word embedding": "Concatenation is a straightforward way to combine phoneme"
        },
        {
          "3.1.3.\nText to phoneme and word embedding": "embedding with word embedding.\nFor\nthe ﬁrst method,\nthe above"
        },
        {
          "3.1.3.\nText to phoneme and word embedding": "embedding is simply concatenated, followed by encoder pre-net."
        },
        {
          "3.1.3.\nText to phoneme and word embedding": ""
        },
        {
          "3.1.3.\nText to phoneme and word embedding": "For the other method, we try to use highway network [23] since"
        },
        {
          "3.1.3.\nText to phoneme and word embedding": ""
        },
        {
          "3.1.3.\nText to phoneme and word embedding": "it usually utilizes\nthe gating mechanism to pass\ninformation efﬁ-"
        },
        {
          "3.1.3.\nText to phoneme and word embedding": "ciently through several\nlayers. With the reference of [24], The con-"
        },
        {
          "3.1.3.\nText to phoneme and word embedding": ""
        },
        {
          "3.1.3.\nText to phoneme and word embedding": "catenation of the phoneme and word embedding vectors u is passed"
        },
        {
          "3.1.3.\nText to phoneme and word embedding": "to a two-layer highway network to fuse multi-level\ninformation ef-"
        },
        {
          "3.1.3.\nText to phoneme and word embedding": "fectively:"
        },
        {
          "3.1.3.\nText to phoneme and word embedding": ""
        },
        {
          "3.1.3.\nText to phoneme and word embedding": ""
        },
        {
          "3.1.3.\nText to phoneme and word embedding": "Z(u) = H(u) · T (u) + u · (1 − T (u))\n(3)"
        },
        {
          "3.1.3.\nText to phoneme and word embedding": ""
        },
        {
          "3.1.3.\nText to phoneme and word embedding": "where H(u) is a parametric transformation (an afﬁne projection fol-"
        },
        {
          "3.1.3.\nText to phoneme and word embedding": "lowed by ReLU [27]) of the input u and T (u) is a gating unit, which"
        },
        {
          "3.1.3.\nText to phoneme and word embedding": ""
        },
        {
          "3.1.3.\nText to phoneme and word embedding": "controls how much transformation is applied and how much copy of"
        },
        {
          "3.1.3.\nText to phoneme and word embedding": "the original input is activated. Then, the multi-level textual informa-"
        },
        {
          "3.1.3.\nText to phoneme and word embedding": "tion is processed by the encoder pre-net to model long-term context."
        },
        {
          "3.1.3.\nText to phoneme and word embedding": ""
        },
        {
          "3.1.3.\nText to phoneme and word embedding": ""
        },
        {
          "3.1.3.\nText to phoneme and word embedding": "3.1.5.\nTransformer modules"
        },
        {
          "3.1.3.\nText to phoneme and word embedding": ""
        },
        {
          "3.1.3.\nText to phoneme and word embedding": ""
        },
        {
          "3.1.3.\nText to phoneme and word embedding": "We use vanilla transformer\n[16]\nstructure for\ntext encoder, cross-"
        },
        {
          "3.1.3.\nText to phoneme and word embedding": ""
        },
        {
          "3.1.3.\nText to phoneme and word embedding": "modality interaction module\nand deep fusion module.\nText\nen-"
        },
        {
          "3.1.3.\nText to phoneme and word embedding": "coder contains self-attention layer\nto fuse information from words"
        },
        {
          "3.1.3.\nText to phoneme and word embedding": "and phonemes.\nCross-modality interaction module includes\nself-"
        },
        {
          "3.1.3.\nText to phoneme and word embedding": "attention\nlayer\nand\nencoder-decoder\nattention\nlayer\nto\nintegrate"
        },
        {
          "3.1.3.\nText to phoneme and word embedding": ""
        },
        {
          "3.1.3.\nText to phoneme and word embedding": "the output\nfrom text encoder with phoneme information. We add"
        },
        {
          "3.1.3.\nText to phoneme and word embedding": ""
        },
        {
          "3.1.3.\nText to phoneme and word embedding": "vanilla transformer blocks\nincluding self-attention layer\nto deeply"
        },
        {
          "3.1.3.\nText to phoneme and word embedding": ""
        },
        {
          "3.1.3.\nText to phoneme and word embedding": "fuse multimodal sequential representations after the cross-modality"
        },
        {
          "3.1.3.\nText to phoneme and word embedding": ""
        },
        {
          "3.1.3.\nText to phoneme and word embedding": "interaction module. Finally, corresponding to the dummy mel input,"
        },
        {
          "3.1.3.\nText to phoneme and word embedding": ""
        },
        {
          "3.1.3.\nText to phoneme and word embedding": "we take the ﬁrst output vector of deep fusion module as the global"
        },
        {
          "3.1.3.\nText to phoneme and word embedding": ""
        },
        {
          "3.1.3.\nText to phoneme and word embedding": "representation and apply a linear projection based on it with logits"
        },
        {
          "3.1.3.\nText to phoneme and word embedding": ""
        },
        {
          "3.1.3.\nText to phoneme and word embedding": "output."
        },
        {
          "3.1.3.\nText to phoneme and word embedding": ""
        },
        {
          "3.1.3.\nText to phoneme and word embedding": ""
        },
        {
          "3.1.3.\nText to phoneme and word embedding": "3.1.6.\nLoss"
        },
        {
          "3.1.3.\nText to phoneme and word embedding": ""
        },
        {
          "3.1.3.\nText to phoneme and word embedding": "For\nthe TTS task, Transformer TTS model generates mel spectro-"
        },
        {
          "3.1.3.\nText to phoneme and word embedding": ""
        },
        {
          "3.1.3.\nText to phoneme and word embedding": "gram and stop token.\nThe predicted ones are compared with the"
        },
        {
          "3.1.3.\nText to phoneme and word embedding": ""
        },
        {
          "3.1.3.\nText to phoneme and word embedding": "ground truth to calculate the TTS loss."
        },
        {
          "3.1.3.\nText to phoneme and word embedding": ""
        },
        {
          "3.1.3.\nText to phoneme and word embedding": "Inspired by [28], we try to adopt multi-task learning to optimize"
        },
        {
          "3.1.3.\nText to phoneme and word embedding": ""
        },
        {
          "3.1.3.\nText to phoneme and word embedding": "the joint\nloss of TTS and SER, however\nthe performance does not"
        },
        {
          "3.1.3.\nText to phoneme and word embedding": "improve in our scenario.\nSo only the logits of\nthe last projection"
        },
        {
          "3.1.3.\nText to phoneme and word embedding": "layer are used to classify the input example, with cross entropy as"
        },
        {
          "3.1.3.\nText to phoneme and word embedding": "the loss function:"
        },
        {
          "3.1.3.\nText to phoneme and word embedding": ""
        },
        {
          "3.1.3.\nText to phoneme and word embedding": ""
        },
        {
          "3.1.3.\nText to phoneme and word embedding": "N(cid:88) i\nK(cid:88) k\nL = −\n(4)\nyi,klogpi,k"
        },
        {
          "3.1.3.\nText to phoneme and word embedding": ""
        },
        {
          "3.1.3.\nText to phoneme and word embedding": "=1\n=1"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: Comparison between multilevel transformer models and Table2:Comparisonofthenumberofdifferenttransformermodules",
      "data": [
        {
          "transformer models and\nTable 1: Comparison between multilevel": "previous state-of-the-art models.",
          "Table 2: Comparison of the number of different transformer modules": "for multilevel transformer model."
        },
        {
          "transformer models and\nTable 1: Comparison between multilevel": "Proposed Methods\nWA\nUA",
          "Table 2: Comparison of the number of different transformer modules": "Text Encoder\nCross-mod\nDeep Fusion\nWA\nUA"
        },
        {
          "transformer models and\nTable 1: Comparison between multilevel": "S. Yoon et al. [13]\n0.690 ± 0.011\n0.696 ± 0.013",
          "Table 2: Comparison of the number of different transformer modules": "3\n3\n1\n0.725\n0.736"
        },
        {
          "transformer models and\nTable 1: Comparison between multilevel": "H. Xu et al. [14]\n0.692 ± 0.006\n0.699 ± 0.007",
          "Table 2: Comparison of the number of different transformer modules": "2\n2\n1\n0.729\n0.737"
        },
        {
          "transformer models and\nTable 1: Comparison between multilevel": "H. Li et al. [3]\n0.719 ± 0.003\n0.728 ± 0.004",
          "Table 2: Comparison of the number of different transformer modules": "1\n1\n1\n0.729\n0.740"
        },
        {
          "transformer models and\nTable 1: Comparison between multilevel": "0.730 ± 0.003\n0.741 ± 0.001\nOur proposal",
          "Table 2: Comparison of the number of different transformer modules": "0.741\n1\n1\n2\n0.730"
        },
        {
          "transformer models and\nTable 1: Comparison between multilevel": "",
          "Table 2: Comparison of the number of different transformer modules": "1\n1\n3\n0.724\n0.733"
        },
        {
          "transformer models and\nTable 1: Comparison between multilevel": "Ablation Study\nWA\nUA",
          "Table 2: Comparison of the number of different transformer modules": ""
        },
        {
          "transformer models and\nTable 1: Comparison between multilevel": "",
          "Table 2: Comparison of the number of different transformer modules": "0.731\n2\n2\n2\n0.739"
        },
        {
          "transformer models and\nTable 1: Comparison between multilevel": "phoneme only\n0.689 ± 0.002\n0.701 ± 0.005",
          "Table 2: Comparison of the number of different transformer modules": "2\n2\n3\n0.722\n0.732"
        },
        {
          "transformer models and\nTable 1: Comparison between multilevel": "word only\n0.719 ± 0.002\n0.727 ± 0.002",
          "Table 2: Comparison of the number of different transformer modules": ""
        },
        {
          "transformer models and\nTable 1: Comparison between multilevel": "concatenation\n0.729 ± 0.003\n0.738 ± 0.003",
          "Table 2: Comparison of the number of different transformer modules": ""
        },
        {
          "transformer models and\nTable 1: Comparison between multilevel": "",
          "Table 2: Comparison of the number of different transformer modules": "Table 3: Comparison between multilevel transformer model and its"
        },
        {
          "transformer models and\nTable 1: Comparison between multilevel": "0.730 ± 0.003\n0.741 ± 0.001\nhighway network",
          "Table 2: Comparison of the number of different transformer modules": ""
        },
        {
          "transformer models and\nTable 1: Comparison between multilevel": "",
          "Table 2: Comparison of the number of different transformer modules": "components"
        },
        {
          "transformer models and\nTable 1: Comparison between multilevel": "w/o deep fusion module\n0.724 ± 0.009\n0.734 ± 0.006",
          "Table 2: Comparison of the number of different transformer modules": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6. REFERENCES": "",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": "recognition using hidden markov models,” Speech Commun.,"
        },
        {
          "6. REFERENCES": "[1] N. Sato\nand Y. Obuchi,\n“Emotion\nrecognition\nusing mel-",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": "vol. 41, no. 4, pp. 603–623, 2003."
        },
        {
          "6. REFERENCES": "frequency cepstral coefﬁcients,” IMT, vol. 2, no. 3, pp. 835–",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": ""
        },
        {
          "6. REFERENCES": "",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": "[19] M. E. Ayadi, M. S. Kamel, and F. Karray, “Speech emotion"
        },
        {
          "6. REFERENCES": "848, 2007.",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": ""
        },
        {
          "6. REFERENCES": "",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": "recognition using gaussian mixture vector autoregressive mod-"
        },
        {
          "6. REFERENCES": "[2] Z. Zhao, Y. Wang,\nand Y. Wang,\n“Multi-level\nfusion\nof",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": "els,” in ICASSP, 2007, pp. 957–960."
        },
        {
          "6. REFERENCES": "wav2vec 2.0 and BERT for multimodal emotion recognition,”",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": ""
        },
        {
          "6. REFERENCES": "",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": "[20] D. Bertero and P. Fung, “A ﬁrst look into a convolutional neural"
        },
        {
          "6. REFERENCES": "in Interspeech, 2022, pp. 4725–4729.",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": ""
        },
        {
          "6. REFERENCES": "",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": "network for speech emotion detection,” in ICASSP, 2017, pp."
        },
        {
          "6. REFERENCES": "[3] H. Li, W. Ding, Z. Wu, and Z. Liu, “Learning ﬁne-grained",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": "5115–5119."
        },
        {
          "6. REFERENCES": "cross modality excitement for speech emotion recognition,” in",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": ""
        },
        {
          "6. REFERENCES": "",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": "[21] A. Satt, S. Rozenberg, and R. Hoory, “Efﬁcient emotion recog-"
        },
        {
          "6. REFERENCES": "Interspeech, 2021, pp. 3375–3379.",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": ""
        },
        {
          "6. REFERENCES": "",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": "nition from speech using deep learning on spectrograms,” in"
        },
        {
          "6. REFERENCES": "[4]\nJ. Devlin, M. Chang, K. Lee, and K. Toutanova, “BERT: pre-",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": "Interspeech, 2017, pp. 1089–1093."
        },
        {
          "6. REFERENCES": "training of deep bidirectional transformers for language under-",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": ""
        },
        {
          "6. REFERENCES": "",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": "[22]\nJ. Shen, R. Pang, R. J. Weiss, M. Schuster, N. Jaitly, Z. Yang,"
        },
        {
          "6. REFERENCES": "standing,” in NAACL-HLT, 2019, pp. 4171–4186.",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": ""
        },
        {
          "6. REFERENCES": "",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": "Z. Chen, Y. Zhang, Y. Wang, R. Ryan, R. A.\nSaurous,"
        },
        {
          "6. REFERENCES": "[5] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": "Y\n. Agiomyrgiannakis, and Y. Wu, “Natural TTS synthesis by"
        },
        {
          "6. REFERENCES": "R. Soricut, “ALBERT: A lite BERT for self-supervised learn-",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": "conditioning wavenet on MEL spectrogram predictions,”\nin"
        },
        {
          "6. REFERENCES": "ing of language representations,” in ICLR, 2020.",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": "ICASSP, 2018, pp. 4779–4783."
        },
        {
          "6. REFERENCES": "[6] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy,",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": "[23] R. K. Srivastava, K. Greff, and J. Schmidhuber, “Highway net-"
        },
        {
          "6. REFERENCES": "M. Lewis, L. Zettlemoyer,\nand V. Stoyanov,\n“Roberta:\nA",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": "works,” CoRR, vol. abs/1505.00387, 2015."
        },
        {
          "6. REFERENCES": "robustly optimized BERT pretraining approach,” CoRR, vol.",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": ""
        },
        {
          "6. REFERENCES": "",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": "[24] M. J. Seo, A. Kembhavi, A. Farhadi, and H. Hajishirzi, “Bidi-"
        },
        {
          "6. REFERENCES": "abs/1907.11692, 2019.",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": ""
        },
        {
          "6. REFERENCES": "",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": "rectional attention ﬂow for machine comprehension,” in ICLR,"
        },
        {
          "6. REFERENCES": "[7]\nS.\nSchneider,\nA.\nBaevski,\nR.\nCollobert,\nand M. Auli,",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": "2017."
        },
        {
          "6. REFERENCES": "“wav2vec: Unsupervised pre-training for speech recognition,”",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": ""
        },
        {
          "6. REFERENCES": "",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": "[25] Y. Kim, “Convolutional neural networks for sentence classiﬁ-"
        },
        {
          "6. REFERENCES": "in Interspeech, 2019, pp. 3465–3469.",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": ""
        },
        {
          "6. REFERENCES": "",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": "cation,” in EMNLP, 2014, pp. 1746–1751."
        },
        {
          "6. REFERENCES": "[8] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, “wav2vec",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": ""
        },
        {
          "6. REFERENCES": "",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": "[26]\nJ. Pennington, R. Socher, and C. D. Manning, “Glove: Global"
        },
        {
          "6. REFERENCES": "2.0: A framework for self-supervised learning of speech repre-",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": ""
        },
        {
          "6. REFERENCES": "",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": "vectors for word representation,” in EMNLP, 2014, pp. 1532–"
        },
        {
          "6. REFERENCES": "sentations,” in NeurIPS, 2020.",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": ""
        },
        {
          "6. REFERENCES": "",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": "1543."
        },
        {
          "6. REFERENCES": "[9] A. Baevski, S. Schneider, and M. Auli, “vq-wav2vec:\nSelf-",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": ""
        },
        {
          "6. REFERENCES": "",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": "[27] X. Glorot, A. Bordes, and Y. Bengio, “Deep sparse rectiﬁer"
        },
        {
          "6. REFERENCES": "supervised\nlearning\nof\ndiscrete\nspeech\nrepresentations,”\nin",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": ""
        },
        {
          "6. REFERENCES": "",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": "neural networks,” in AISTATS, vol. 15, 2011, pp. 315–323."
        },
        {
          "6. REFERENCES": "ICLR, 2020.",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": ""
        },
        {
          "6. REFERENCES": "",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": "[28] X. Cai, J. Yuan, R. Zheng, L. Huang, and K. Church, “Speech"
        },
        {
          "6. REFERENCES": "[10]\nF. A. Acheampong, H. Nunoo-Mensah, and W. Chen, “Trans-",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": ""
        },
        {
          "6. REFERENCES": "",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": "emotion recognition with multi-task learning,” in Interspeech,"
        },
        {
          "6. REFERENCES": "former models for\ntext-based emotion detection:\na review of",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": ""
        },
        {
          "6. REFERENCES": "",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": "2021, pp. 4508–4512."
        },
        {
          "6. REFERENCES": "bert-based approaches,” Artif.\nIntell. Rev., vol. 54, no. 8, pp.",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": ""
        },
        {
          "6. REFERENCES": "",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": "[29] W. Dai, S. Cahyawijaya, Z. Liu, and P. Fung, “Multimodal end-"
        },
        {
          "6. REFERENCES": "5789–5829, 2021.",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": ""
        },
        {
          "6. REFERENCES": "",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": "to-end sparse model for emotion recognition,” in NAACL-HLT,"
        },
        {
          "6. REFERENCES": "[11] L. Pepino, P. Riera, and L. Ferrer, “Emotion recognition from",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": ""
        },
        {
          "6. REFERENCES": "",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": "2021, pp. 5305–5316."
        },
        {
          "6. REFERENCES": "speech using wav2vec 2.0 embeddings,” in Interspeech, 2021,",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": ""
        },
        {
          "6. REFERENCES": "",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": "[30] D. P. Kingma and J. Ba, “Adam: A method for stochastic opti-"
        },
        {
          "6. REFERENCES": "pp. 3400–3404.",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": ""
        },
        {
          "6. REFERENCES": "",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": "mization,” in ICLR, 2015."
        },
        {
          "6. REFERENCES": "[12]\nS. Siriwardhana, A. Reis, R. Weerasekera, and S. Nanayakkara,",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": ""
        },
        {
          "6. REFERENCES": "“Jointly ﬁne-tuning ”bert-like” self supervised models to im-",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": ""
        },
        {
          "6. REFERENCES": "prove multimodal speech emotion recognition,” in Interspeech,",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": ""
        },
        {
          "6. REFERENCES": "2020, pp. 3755–3759.",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": ""
        },
        {
          "6. REFERENCES": "[13]\nS. Yoon, S. Byun, and K. Jung, “Multimodal speech emotion",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": ""
        },
        {
          "6. REFERENCES": "recognition using audio and text,” in SLT, 2018, pp. 112–118.",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": ""
        },
        {
          "6. REFERENCES": "[14] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng,\nand X. Li,",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": ""
        },
        {
          "6. REFERENCES": "“Learning alignment for multimodal emotion recognition from",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": ""
        },
        {
          "6. REFERENCES": "speech,” in Interspeech, 2019, pp. 3569–3573.",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": ""
        },
        {
          "6. REFERENCES": "[15] N. Li, S. Liu, Y. Liu, S. Zhao, and M. Liu, “Neural speech",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": ""
        },
        {
          "6. REFERENCES": "synthesis with transformer network,” in IAAI, 2019, pp. 6706–",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": ""
        },
        {
          "6. REFERENCES": "6713.",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": ""
        },
        {
          "6. REFERENCES": "[16] A. Vaswani, N. Shazeer, N. Parmar,\nJ. Uszkoreit, L.\nJones,",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": ""
        },
        {
          "6. REFERENCES": "A. N. Gomez, L. Kaiser, and I. Polosukhin, “Attention is all",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": ""
        },
        {
          "6. REFERENCES": "you need,” in NIPS, 2017, pp. 5998–6008.",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": ""
        },
        {
          "6. REFERENCES": "[17] C. Busso, M. Bulut, C. Lee, A. Kazemzadeh, E. Mower,",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": ""
        },
        {
          "6. REFERENCES": "S. Kim,\nJ. N. Chang, S. Lee, and S. S. Narayanan, “IEMO-",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": ""
        },
        {
          "6. REFERENCES": "CAP:\ninteractive emotional dyadic motion capture database,”",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": ""
        },
        {
          "6. REFERENCES": "Lang. Resour. Evaluation, vol. 42, no. 4, pp. 335–359, 2008.",
          "[18] T. L. Nwe, S. W. Foo, and L. C. D. Silva, “Speech emotion": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Emotion recognition using melfrequency cepstral coefficients",
      "authors": [
        "N Sato",
        "Y Obuchi"
      ],
      "year": "2007",
      "venue": "IMT"
    },
    {
      "citation_id": "3",
      "title": "Multi-level fusion of wav2vec 2.0 and BERT for multimodal emotion recognition",
      "authors": [
        "Z Zhao",
        "Y Wang",
        "Y Wang"
      ],
      "year": "2022",
      "venue": "Multi-level fusion of wav2vec 2.0 and BERT for multimodal emotion recognition"
    },
    {
      "citation_id": "4",
      "title": "Learning fine-grained cross modality excitement for speech emotion recognition",
      "authors": [
        "H Li",
        "W Ding",
        "Z Wu",
        "Z Liu"
      ],
      "year": "2021",
      "venue": "Learning fine-grained cross modality excitement for speech emotion recognition"
    },
    {
      "citation_id": "5",
      "title": "BERT: pretraining of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "NAACL-HLT"
    },
    {
      "citation_id": "6",
      "title": "ALBERT: A lite BERT for self-supervised learning of language representations",
      "authors": [
        "Z Lan",
        "M Chen",
        "S Goodman",
        "K Gimpel",
        "P Sharma",
        "R Soricut"
      ],
      "year": "2020",
      "venue": "ALBERT: A lite BERT for self-supervised learning of language representations"
    },
    {
      "citation_id": "7",
      "title": "Roberta: A robustly optimized BERT pretraining approach",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal",
        "J Du",
        "M Joshi",
        "D Chen",
        "O Levy",
        "M Lewis",
        "L Zettlemoyer",
        "V Stoyanov"
      ],
      "year": "1907",
      "venue": "CoRR"
    },
    {
      "citation_id": "8",
      "title": "wav2vec: Unsupervised pre-training for speech recognition",
      "authors": [
        "S Schneider",
        "A Baevski",
        "R Collobert",
        "M Auli"
      ],
      "year": "2019",
      "venue": "wav2vec: Unsupervised pre-training for speech recognition"
    },
    {
      "citation_id": "9",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "wav2vec 2.0: A framework for self-supervised learning of speech representations"
    },
    {
      "citation_id": "10",
      "title": "vq-wav2vec: Selfsupervised learning of discrete speech representations",
      "authors": [
        "A Baevski",
        "S Schneider",
        "M Auli"
      ],
      "year": "2020",
      "venue": "ICLR"
    },
    {
      "citation_id": "11",
      "title": "Transformer models for text-based emotion detection: a review of bert-based approaches",
      "authors": [
        "F Acheampong",
        "H Nunoo-Mensah",
        "W Chen"
      ],
      "year": "2021",
      "venue": "Artif. Intell. Rev"
    },
    {
      "citation_id": "12",
      "title": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "authors": [
        "L Pepino",
        "P Riera",
        "L Ferrer"
      ],
      "year": "2021",
      "venue": "Emotion recognition from speech using wav2vec 2.0 embeddings"
    },
    {
      "citation_id": "13",
      "title": "Jointly fine-tuning \"bert-like\" self supervised models to improve multimodal speech emotion recognition",
      "authors": [
        "S Siriwardhana",
        "A Reis",
        "R Weerasekera",
        "S Nanayakkara"
      ],
      "venue": "Jointly fine-tuning \"bert-like\" self supervised models to improve multimodal speech emotion recognition"
    },
    {
      "citation_id": "14",
      "title": "Multimodal speech emotion recognition using audio and text",
      "authors": [
        "S Yoon",
        "S Byun",
        "K Jung"
      ],
      "year": "2018",
      "venue": "SLT"
    },
    {
      "citation_id": "15",
      "title": "Learning alignment for multimodal emotion recognition from speech",
      "authors": [
        "H Xu",
        "H Zhang",
        "K Han",
        "Y Wang",
        "Y Peng",
        "X Li"
      ],
      "year": "2019",
      "venue": "Learning alignment for multimodal emotion recognition from speech"
    },
    {
      "citation_id": "16",
      "title": "Neural speech synthesis with transformer network",
      "authors": [
        "N Li",
        "S Liu",
        "Y Liu",
        "S Zhao",
        "M Liu"
      ],
      "year": "2019",
      "venue": "IAAI"
    },
    {
      "citation_id": "17",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "NIPS"
    },
    {
      "citation_id": "18",
      "title": "IEMO-CAP: interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Lang. Resour. Evaluation"
    },
    {
      "citation_id": "19",
      "title": "Speech emotion recognition using hidden markov models",
      "authors": [
        "T Nwe",
        "S Foo",
        "L Silva"
      ],
      "year": "2003",
      "venue": "Speech Commun"
    },
    {
      "citation_id": "20",
      "title": "Speech emotion recognition using gaussian mixture vector autoregressive models",
      "authors": [
        "M Ayadi",
        "M Kamel",
        "F Karray"
      ],
      "year": "2007",
      "venue": "ICASSP"
    },
    {
      "citation_id": "21",
      "title": "A first look into a convolutional neural network for speech emotion detection",
      "authors": [
        "D Bertero",
        "P Fung"
      ],
      "year": "2017",
      "venue": "ICASSP"
    },
    {
      "citation_id": "22",
      "title": "Efficient emotion recognition from speech using deep learning on spectrograms",
      "authors": [
        "A Satt",
        "S Rozenberg",
        "R Hoory"
      ],
      "year": "2017",
      "venue": "Efficient emotion recognition from speech using deep learning on spectrograms"
    },
    {
      "citation_id": "23",
      "title": "Natural TTS synthesis by conditioning wavenet on MEL spectrogram predictions",
      "authors": [
        "J Shen",
        "R Pang",
        "R Weiss",
        "M Schuster",
        "N Jaitly",
        "Z Yang",
        "Z Chen",
        "Y Zhang",
        "Y Wang",
        "R Ryan",
        "R Saurous",
        "Y Agiomyrgiannakis",
        "Y Wu"
      ],
      "year": "2018",
      "venue": "ICASSP"
    },
    {
      "citation_id": "24",
      "title": "Highway networks",
      "authors": [
        "R Srivastava",
        "K Greff",
        "J Schmidhuber"
      ],
      "year": "2015",
      "venue": "CoRR"
    },
    {
      "citation_id": "25",
      "title": "Bidirectional attention flow for machine comprehension",
      "authors": [
        "M Seo",
        "A Kembhavi",
        "A Farhadi",
        "H Hajishirzi"
      ],
      "year": "2017",
      "venue": "ICLR"
    },
    {
      "citation_id": "26",
      "title": "Convolutional neural networks for sentence classification",
      "authors": [
        "Y Kim"
      ],
      "year": "2014",
      "venue": "EMNLP"
    },
    {
      "citation_id": "27",
      "title": "Glove: Global vectors for word representation",
      "authors": [
        "J Pennington",
        "R Socher",
        "C Manning"
      ],
      "year": "2014",
      "venue": "EMNLP"
    },
    {
      "citation_id": "28",
      "title": "Deep sparse rectifier neural networks",
      "authors": [
        "X Glorot",
        "A Bordes",
        "Y Bengio"
      ],
      "year": "2011",
      "venue": "AISTATS"
    },
    {
      "citation_id": "29",
      "title": "Speech emotion recognition with multi-task learning",
      "authors": [
        "X Cai",
        "J Yuan",
        "R Zheng",
        "L Huang",
        "K Church"
      ],
      "year": "2021",
      "venue": "Speech emotion recognition with multi-task learning"
    },
    {
      "citation_id": "30",
      "title": "Multimodal endto-end sparse model for emotion recognition",
      "authors": [
        "W Dai",
        "S Cahyawijaya",
        "Z Liu",
        "P Fung"
      ],
      "year": "2021",
      "venue": "NAACL-HLT"
    },
    {
      "citation_id": "31",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2015",
      "venue": "ICLR"
    }
  ]
}