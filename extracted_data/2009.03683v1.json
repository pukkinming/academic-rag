{
  "paper_id": "2009.03683v1",
  "title": "Rain Rendering For Evaluating And Improving Robustness To Bad Weather",
  "published": "2020-09-06T21:08:41Z",
  "authors": [
    "Maxime Tremblay",
    "Shirsendu Sukanta Halder",
    "Raoul de Charette",
    "Jean-François Lalonde"
  ],
  "keywords": [
    "Adverse weather",
    "vision and rain",
    "physicsbased rendering",
    "image to image translation",
    "GAN"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Rain fills the atmosphere with water particles, which breaks the common assumption that light travels unaltered from the scene to the camera. While it is well-known that rain affects computer vision algorithms, quantifying its impact is difficult. In this context, we present a rain rendering pipeline that enables the systematic evaluation of common computer vision algorithms to controlled amounts of rain. We present three different ways to add synthetic rain to existing images datasets: completely physic-based; completely data-driven; and a combination of both. The physic-based rain augmentation combines a physical particle simulator and accurate rain photometric modeling. We validate our rendering methods with a user study, demonstrating our rain is judged as much as 73% more realistic than the state-of-theart. Using our generated rain-augmented KITTI, Cityscapes, and nuScenes datasets, we conduct a thorough evaluation of object detection, semantic segmentation, and depth estimation algorithms and show that their performance decreases in degraded weather, on the order of 15% for object detection, 60% for semantic segmentation, and 6-fold increase in depth estimation error. Finetuning on our augmented synthetic data results in improvements of 21% on object detection, 37% on semantic segmentation, and 8% on depth estimation. Keywords Adverse weather • vision and rain • physicsbased rendering • image to image translation • GAN",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "A common assumption in computer vision is that light travels unaltered from the scene to the camera. In clear weather, this assumption is reasonable: the atmosphere behaves like a transparent medium and transmits light with very little Address(es) of author(s) should be given Object detection  [73]  Semantic segmentation  [65]  Depth estimation  [25]  Clear weather Rain (200 mm/hr) Fig.  1  Vision tasks in clear and rain-augmented images. Our synthetic rain rendering framework allows for the evaluation of computer vision algorithms in challenging bad weather scenarios. We render physically-based, realistic rain on images from the KITTI  [23]  (rows 1-2) and Cityscapes  [13]  (rows 3-4) datasets with object detection from mx-RCNN  [73]  (row 2), semantic segmentation from ESPNet  [65]  (row 4). We also present a combined data-driven and physic-based rain rendering approach which we apply to the nuScenes  [9]  (rows 5-6) dataset with depth estimation from Monodepth2  [25]  (row 6). All algorithms are quite significantly affected by rainy conditions. attenuation or scattering. However, inclement weather conditions such as rain fill the atmosphere with particles producing spatio-temporal artifacts such as attenuation or rain streaks. This creates noticeable changes to the appearance of images (see fig.  1 ), thus creating additional challenges to computer vision algorithms which must be robust to these conditions. While the influence of rain on image appearance is wellknown and understood  [19] , its impact on the performance of computer vision tasks is not. Indeed, how can one evaluate what the impact of, say, a rainfall rate of 100 mm/hour (a typical autumn shower) is on the performance of an object detector, when our existing databases all contain images overwhelmingly captured under clear weather conditions? To measure this effect, one would need a labeled object detection dataset where all the images have been captured under 100 mm/hour rain. Needless to say, such a \"rain-calibrated\" dataset does not exist, and capturing one is prohibitive. Indeed, datasets with bad weather information are few and sparse, and typically include only high-level tags (rain or not) without mentioning how much rain is falling. While they can be used to improve vision algorithms under adverse conditions by including rainy images in the training set, they cannot help us in systematically evaluating performance degradation under increasing amounts of rain.\n\nAlternatively, one can attempt to remove its effects from images-i.e., create a \"clear weather\" version of the imageprior to applying subsequent algorithms. For example, rain can be detected and attenuated from images  [21, 3, 74, 79, 50] . We experiment with this approach in sec. 7.4. An alternative approach is to employ programmable lighting to reduce the rain visibility, by shining light between raindrops  [11] . Unfortunately, these solutions either add significant processing times to already constrained time budgets, or require custom hardware. Instead, if we could systematically study the effect of weather on images, we could better understand the robustness of existing algorithms, and, potentially, increase their robustness afterwards.\n\nIn this paper, we propose methods to realistically augment existing image databases with rainy conditions. We rely on well-understood physical models as well as on recent image-to-image translations to generate visually convincing results. First, we experiment with our novel physicsbased approach, which is the first to allow controlling the amount of rain in order to generate arbitrary amounts, ranging from very light rain (5 mm/hour rainfall) to very heavy storms (200+ mm/hour). This key feature allows us to produce weather-augmented datasets, where the rainfall rate is known and calibrated. Subsequently, we augment two existing datasets (KITTI  [23]  and Cityscapes  [13] ) with rain, and evaluate the robustness of popular object detection and segmentation algorithms on these augmented databases. Second, we experiment with a combination of physics-and learningbased approaches, where a popular unpaired image-to-image translation method  [83]  is used to convey a sense of \"wetness\" to the scene, and physics-based rain is subsequently composited on the resulting image. Here, we augment the nuScenes dataset  [9] , and use it to evaluate the robustness of object detection and depth estimation algorithms. Finally, we also use the latter to refine algorithms using curriculum learning  [5] , and demonstrate improved robustness on real rainy images.\n\nIn short, we make the following contributions. First, we present two different realistic rain rendering approaches: the first is a purely physic-based method and the second is a combination of a GAN-based approach and this physic-based framework. Second, we augment KITTI  [23] , Cityscapes  [13] , and nuScenes  [9]  datasets with rain. Third, we present a methodology for systematically evaluating the performance of 13 popular algorithms-for object detection, semantic segmentation and depth estimation-on rainy images. Our findings indicate that rain affects all algorithms: performance drops of 15% mAP for object detection, 60% AP for semantic segmentation, and a 6-fold increase in depth estimation error. Finally, our augmented database can also be used to finetune these same algorithms in order to improve their performance in real-world rainy conditions. This paper significantly extends an earlier version of this work published in  [27] , by combining physics-based rendering with learning-based image-to-image translation methods, conducting a novel, more in-depth user study, evaluating depth estimation algorithms, comparing to a deraining approach, and by providing a more extensive evaluation of the performance improvement on real images. Our framework is readily usable to augment existing image with realistic rainy conditions. Code and data are available at the following URL: https://team.inria.fr/rits/ computer-vision/weather-augment/.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Rain Modeling",
      "text": "In their series of influential papers, Garg and Nayar provided a comprehensive overview of the appearance models required for understanding  [21]  and synthesizing  [20]  realistic rain. In particular, they propose an image-based rain streak database  [20]  modeling the drop oscillations, which we exploit in our physics-based rendering framework. Other streak appearance models were proposed in  [71, 3]  using a frequency model. Realistic rendering was also obtained with ray-tracing  [64]  or artistic-based techniques  [69, 14]  but on synthetic data as they require complete 3D knowledge of the scene including accurate light estimation. Numerous works also studied generation of raindrops on-screen, with 3D modeling and ray-casting  [62, 63, 28, 29]  or normal maps  [57]  some also accounting for focus blur.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Rain Removal",
      "text": "Due to the problems it creates on computer vision algorithms, rain removal in images got a lot of attention initially focusing on photometric models  [18] . For this, several techniques have been proposed, ranging from frequency space analysis  [3]  to deep networks  [74] . Sparse coding and layers priors were also important axes of research  [46, 51, 12]  due to their facility to encode streak patches. Recently, dual residual networks have been employed  [50] . Alternatively, camera parameters  [19]  or programmable light sources  [11]  can also be adjusted to limit the impact of rain on the image formation process. Additional proposals were made for the specific task of raindrops removal on windows  [17]  or windshields  [28, 57, 29] .\n\nUnpaired image translation An interesting solution to weather augmentation is the use of data-driven unpaired image translation frameworks. Zhang et al.  [79]  proposed to use conditional GANs for rain removal. By proposing to add a cyclic loss to the learning process, CycleGAN  [83]  became a significant paper for unpaired image translation; they produced interesting results in weather and season translation. DualGAN  [75]  uses similar ideas with differences in the network models. The UNIT  [47] , MUNIT  [33] , and FUNIT  [48]  frameworks all, in one way or another, propose to perform image translation with the common idea that data from different sets have a shared latent space. They showed interesting results on adding and removing weather effect to images. Since the information in clear and rainy images is symmetrical, many unsupervised image translation approaches could produce decent visual results. In  [56] , Pizzati et al. learn to disentangle the scene from lens occlusions such as raindrops, which improves both realism and physical accuracy of the translations. Another strategy for better qualitative translations is to rely on semantic consistency  [44, 68] .\n\nWeather databases In computer vision, few images databases have precise labeled weather information. Of note for mobile robotics, the BDD100K  [76] , the Oxford dataset  [52] , and Wilddash  [77]  provide data recorded in various weather conditions, including rain. Other stationary camera datasets such as AMOS  [35] , the transient attributes dataset  [40] , the Webcam Clip Art dataset  [41] , or the WILD dataset  [55]  are sparsely labeled with weather information. The relatively new nuScenes dataset  [9]  have multiple labeled scenes containing rainy images, but variation in rain intensities are not indicated. Gruber et al.  [26]  recently released a dataset with dense depth labels under a variety of real weather conditions produced by a controlled weather chamber, which inherently limits the variety of scenes (limited to four common scenarios) in the dataset. Note that  [6]  also announced-but at the time of writing, not yet fully available-a promising dataset including heavy snow and rain events. Still, datasets with rainy data are too small to train algorithms and there exists no dataset with systematically recorded rainfall rates and object/scene labels. The closest systematic works in spirit  [38, 39]  evaluated the effect of simulating weather on vision, but did so in purely virtual environments (GTA and CARLA, respectively) whereas we augment real images. Of particular relevance to our work, Sakaridis et al.  [66]  propose a framework for rendering fog into images from the Cityscapes  [13]  dataset. Their approach assumes a homogeneous fog model, which is rendered from the depth estimated from stereo. Existing scene segmentation models and object detectors are then adapted to fog. In our work, we employ the similar idea of rendering realistic weather on top of existing images, but we focus on rain rather than fog.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Rain Augmentation",
      "text": "Broadly speaking, synthesizing rain on images can be achieved using two seemingly antagonistic methods: 1) physics-based rendering (PBR) methods  [28, 20] , which explicitly model the dynamics and the radiometry of rain drops in images; or 2) learning-based image-to-image translation approaches  [83, 33] , which train deep neural networks to \"translate\" an image into its rainy version. While completely different, we argue both these methods offer complementary advantages. On one hand, physics-based approaches are accurate, controllable, can simulate a wide variety of imaging conditions, and do not require any training data. On the other hand, learning-based approaches can realistically simulate important visual cues such as wetness, cloud cover, and overall gloominess typically associated with rainy images.\n\nIn this paper, we propose to first explore the use of both techniques independently, then to combine them into a hybrid approach. This section thus first describes our PBR approach (sec. 3.1), followed by the image-to-image translation with a GAN (sec. 3.2), and concludes with the hybrid combination of the two GAN+PBR (sec. 3.3).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Physics-Based Rendering (Pbr)",
      "text": "Taking inspiration from the vast literature on rain physics,  [53, 7, 21, 54]  we simulate the rain appearance in an arbitrary image with the approach summarized in fig.  2 . Based on the estimated scene depth, a fog-like attenuation layer is first generated. Individual rain streaks are subsequently generated, and composited with the fog-like layer. The final result is blended in the original image to create a realistic, physicsbased and controllable rainfall rate.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Fog-Like Rain",
      "text": "Following the definition of  [21] , fog-like rain is the set of drops that are too far away and that project on an area smaller than 1 pixel. In this case, a pixel may even be imaging a large number of drops, which causes optical attenuation  [21] .\n\nIn practice, most drops in a rainfall are actually imaged as fog-like rain 1 , though their visual effect is less dominant. We render the volumetric attenuation using the model described in  [71]  where the per-pixel attenuation I att is expressed as the sum of the extinction L ext caused by the volume of rain and the airlight scattering A in that results of the environmental lighting. Using equations from  [71]  to model the attenuation image at pixel x we get\n\nwhere\n\nHere, R denotes the rainfall rate R (in mm/hr), d(x) the pixel depth, β HG the standard Heynyey-Greenstein coefficient, and Ēsun the average sun irradiance which we estimate from the image-radiance relation  [32] . 1 Assuming a stationary camera with KITTI calibration  [22] , we computed that only 1.24% of the drops project on 1+ pixel in 50 mm/hr rain, and 0.7% at 5 mm/hr. This follows logic: the heavier the rain, the higher the probability of having large drops.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Simulating The Physics Of Raindrops",
      "text": "We use the particles simulator of  [11]  to compute the position and dynamics of all raindrops greater than 1 mm for a given fall rate  2  . The simulator outputs the position and dynamics (start and end points of streaks) of all visible rain drops in both world and image space, and accounts for intrinsic and extrinsic camera calibration.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Rendering The Appearance Of A Rain Streak",
      "text": "While ray casting allows exact modeling of drops photometry, this comes at very high processing cost and is virtually only possible in synthetic scenes where the geometry and surface materials are perfectly known  [64, 28] . What is more, drops oscillate as they fall, which creates further complications in modeling the light interaction. Instead, we rely on the raindrop appearance database of Garg and Nayar  [21] , which contains the individual rain streaks radiance when imaged by a stationary camera. For each drop, the streak database also models 10 oscillations due to the airflow, which accounts for much greater realism than Gaussian modeling  [3] .\n\nTo render a raindrop, we first select a rain streak S ∈ S from the streak database S of  [20] , which contains 20 different streaks (each with 10 different oscillations) stored in an image format. We select the streak that best matches the final drop dimensions (computed from the output of the physical simulator), and randomly select an oscillation.\n\nThe selected rain streak S is subsequently warped to match the drop dynamics from the physical simulator:\n\nwhere H(•) is the homography computed from the start and end points in image space given by the physical simulator and the corresponding points in the database streak image.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Computing The Photometry Of A Rain Streak",
      "text": "Computing the photometry of a rain streak from a single image is impractical because drops have a much larger field of view than common cameras (165 • vs approx. 70-100 • ).\n\nTo render a drop accurately, we must therefore estimate the environment map (spherical lighting representation) around that drop. Sophisticated methods could be used  [31, 30, 80]  but we employ  [10]  which approximates the environment map through a series of simple operations on the image.\n\nFrom each camera relative 3D drop position, we compute the intersection F of the drop field of view with the environment map E, assuming a 10m constant scene distance. The process is depicted in fig.  3 , and geometrical details are provided in appendix A. Note that geometrically exact drop field of view estimation requires location-dependent environment maps, centered on each drop. However, we consider the impact negligible since drops are relatively close to the camera center compared to the sphere radius used 3 . Since a drop refracts 94% of its field of view radiance and reflects 6% of the entire environment map radiance  [21] , we multiply the streak appearance with a per-channel weight:\n\nwhere F is the mean of the intersection region F , and Ē is the mean of the environment map E.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Compositing A Single Rain Streak On The Image",
      "text": "Now that the streak position and photometry were determined from the physical simulator and the environment map respectively, we can composite it onto the original image. First, to account for the camera depth of field, we apply a defocus effect following  [58] , convolving the streak image S with the circle of confusion 4 C, that is: S = S * C. 3 We computed that, for KITTI, 98.7% of the drops are within 4 m from the camera center in a 50 mm/hr rainfall rate. Therefore, computing location-dependent environment maps would not be significantly more accurate, while being of very high processing cost. 4 The circle of confusion C of an object at distance d, is defined as:\n\nd(fp-f )fN with f p the focus plane, f the focal and f N the lens f-number. f and f N are from intrinsic calibration, and f p is set to 6 m.\n\nWe then blend the rendered drop with the attenuated background image I att using the photometric blending model from  [21] . Because the streak database and the image I are likely to be imaged with different exposures, we need to correct the exposure to match the imaging system used in I. Suppose a pixel x of the image I and x the overlapping coordinates in streak S , the composite is obtained with\n\nwhere S α is the alpha channel 5  of the rendered streak, τ 0 = √ 10 -3 /50 is the time for which the drop remained on one pixel in the streak database, and τ 1 the same measure according to our physical simulator. We refer to appendix B for details.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Compositing Rainfall On The Image",
      "text": "The rendering of rainfall of arbitrary rates in an image is done in three main steps: 1) the fog-like attenuated image I att is rendered (eq. 1), 2) the drops outputted by the physical simulator are rendered individually on the image (eq. 5), 3) the global luminosity average of the rainy image denoted I rain is adjusted. While rainy events usually occur in cloudy weather which consequently decreases the scene radiance, a typical camera imaging system adjusts its exposure to restore the luminosity. Consequently, we adjust the global luminosity factor so as to restore the mean radiance, and preserves the relation Ī = Īrain , where the overbar denotes the intensity average.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Photometric Validation.",
      "text": "A limitation of our physical pipeline is the lighting estimation which impacts the photometry of the rain. To measure its effect, in fig.  4  we compare the same rain rendered with our estimated environment map or ground truth illumination obtained from high dynamic range panoramas  [30] . Overall, our estimation differs from ground truth when the scene is not radially symmetric but we observe that it produces visually similar rain in images.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Image-To-Image Translation (Gan)",
      "text": "While our physic-based rendering generates realistic rain streaks and fog-like rain effect, it ignores major rainy characteristics such as wetness, reflections, clouds and thus may fail at conveying the overall look of a rainy scene. Conversely, generative adversarial networks (GAN) excel at learning such visual characteristics as they constitute strong signals for the discriminator in the learning process. Fig.  5  GAN+PBR rain-augmentation architecture. In this hybrid approach, clear images are first translated into rain with CycleGAN  [83]  and subsequently augmented with rain streaks with our PBR pipeline (see fig.  2 ).\n\nWe further learn the clear → rain mapping with Cycle-GAN  [83]  from a set of unpaired clear/rain images. We train our model with the 256 × 256 architecture from  [83]  on images of input size 448 × 256. The generator is similar to  [37]  with 2 downsampling blocks followed by 9 ResNet blocks and 2 upsampling blocks. The discriminator is a simple 3 hidden layers ConvNet similar to the one used in PatchGAN  [34] . The model is optimized for 40 epochs with Adam, using batch size of 1, a learning rate of 0.0002, and β = {0.5, 0.999}.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Combining Gan And Pbr",
      "text": "We combine both the PBR-and the GAN-based rain generation methods together, simply by first translating the image to its rainy version with the GAN, then compositing the rain layer onto the resulting image using PBR (see fig.  5 ). The sun irradiance estimation Ēsun (sec. 3.1.1) of the \"translated\" image is typically darker, which, in turn, makes the fog-like rain more realistic. The estimated environment map is also darker and, consequently, so is its mean value Ē. The appearance of rain streaks will thus remain coherent with their environment.\n\nSince rain streaks smaller than 1px in diameter are ignored before the rendering and instead generated as fog-like rain (sec. 3.1.1), we need to apply the PBR renderer at full resolution by upsampling the output of the GAN to the image original size. Once the rain rendering is complete, we downsample the augmented image at 448 × 256. We further refer to this hybrid rendering as GAN+PBR.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Validating Rain Appearance",
      "text": "We now validate the appearance of our synthetic rainy images when using either of our rain augmentation pipelines. We observe visual results and quantify their perceptual realism by comparing them to existing rain augmentation approaches.\n\n4.1 Qualitative evaluation Fig.  6  presents real photographs captured under heavy rain, qualitative results of our rain renderings on images from nuScenes  [9]  and representative results from 3 recent synthetic rain augmented approaches  [78, 74, 79] . From the real  rain photographs, it is noticeable that rainy scenes have complex visual appearance, and that the streaks visibility is greatly affected by the imaging device and background.\n\nOur PBR approach is able to reproduce the complex pattern of streaks with orientation consistent with camera motion and photometry consistent with background and depth. As in the real photographs, the streaks are sparse and only visible on darker backgrounds. The veiling effect caused by the rain volume (i.e. fog-like rain) is visible where the scene depth is larger (i.e. image center, sky) and nearby streaks are accurately defocused. Still, the absence of visible wetness arguably affects the rainy feeling.\n\nConversely, the GAN believably renders the wetness appearance of rainy scenes. While some reflections are geometrically incorrect (e.g., a pole is reflected in the middle of the street in the left column of fig.  6  yet no pole is present), the overall appearance is visually pleasant and the global illumination matches that of real photographs. A noticeable artifact caused by GAN is the blurry appearance of images, whereas real rain images are only blurred in the distance. This is explained by the inability of the GAN to disentangle the scene from the lens drops present in the \"rainy\" training images. This leads to blurring the whole image being an easy learning optimum, as highlighted in  [56] . Another limitation we already mentioned is that GAN does not allow to control the amount of rain in the image.\n\nThis limitation is circumvented by our GAN+PBR approach which renders controllable rain streaks while preserving the global wetness appearance learned with image translation. Despite the naive GAN and PBR compositing strategy, the drops naturally blend in the scene.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "User Study",
      "text": "To evaluate the perceptual quality of our rain renderings, we conducted two user studies. In the first, users were shown one image at a time, and asked to rate if rain looks realistic on a 5-point Likert scale. A total of 42 images were shown, that is 6 for each of the following: real rainy photographs, ours (PBR, GAN, GAN+PBR), and previous approaches  [74, 78, 79] . Answers were obtained for a total of 67 participants, aged from 22 to 75 (avg 37.0, std 14.2), with 32.8% females.\n\nFrom the Mean Opinion Score (MOS) in fig.  7 , all our rain augmentation approaches are judged to be more realistic than any of the previous approaches. Specifically, when converting ratings to the [0, 1] interval, the mean rain realism is 0.77 for real photos, 0.44 for PBR, 0.68 for GAN, 0.52 for GAN+PBR, and 0.30/0.23/0.08 for  [78] /  [79] /  [74]  respectively. Despite physical and geometrical inconsistencies, the users consistently judged GAN images to be more realistic. This is in favor of using image-to-image translation rather than physics-based rendering for realism purposes. However, Fig.  8  User study on images characteristics conveying rain. The yaxis displays ratings to the statement \"Which of these qualities help in determining the realism of the rain\".\n\nfor benchmarking or physical accuracy purposes, GAN+PBR allows us to have an arbitrary control on the rain amount at the cost of slightly lower realism.\n\nIn the second study, we asked respondents who participated in the first study to determine, for each of the same images as before (excluding images from the previous work), which visual characteristics influenced their decisions. 52 of the original participants responded, aged from 22 to 72 (avg 36.6, std 13.9) including 28.9% females. Results are reported in fig.  8 . We note that while falling rain and wetness are the main characteristics in real rain, GAN-judged the most realistic approach-fails to convey the falling rain appearance but excels at rendering wetness. The opposite is observed with PBR, though few users indicated wetness (despite its absence). The GAN+PBR offers a trade-off balancing all characteristics.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Evaluating The Impact Of Real Rain",
      "text": "We now aim at quantifying the impact of rain on three computer vision tasks: object detection, semantic segmentation, and depth estimation. These tasks are critical for any outdoor vision systems such as mobile robotics, autonomous driving, or surveillance. Before we experiment on rain-augmented images with our PBR, GAN, and GAN+PBR approaches, we first experiment on real rainy photographs.\n\nFor this, we use the nuScenes dataset  [9] . Benefiting from coarse frame-wise weather annotations in the dataset, we split Input YOLOv2\n\n[59]\n\nPSPNet  [82]  Monodepth2  [25]  Fig.  9  Qualitative results on real rainy images from the nuScenes dataset. Shown for different tasks: object detection (top line), semantic segmentation (middle line), and depth estimation (bottom line) nuScenes images  6  in two subsets: \"nuScenes-clear\" (images without rain) and \"nuScenes-rain\" (rainy images). Due to the noisy weather labels, we cross-validated each frame with a historic weather database  7  using GPS location and time, and only kept frames where the nuScenes label agreed with the weather database. This resulted in sets of 24134 images for nuScenes-clear, and 6028 for nuScenes-rain. In this dataset, rain images are dark, gloomy, the sky is heavily covered, and no falling rain is visible but there are unfocused raindrops occlusions on the lens. We experiment on these sets of real images with algorithms from each task: YOLOv2  [59]  for object detection, PSPNet  [82]  for semantic segmentation, and Mon-odepth2  [25]  for depth estimation. The nuScenes-clear set is split into train/test subsets of 19685/4449 images from 491/110 scenes. The nuScenes-rain is also split into train/test subsets of 5419/609 images from 134/15 scenes. Here, 1000 images from the nuScenes-clear(test) and all images from the nuScenes-rain(test) subset are used for evaluation (train will be used for the GAN in sec. 6.1). These training and testing sets and subsets are all displayed in table 3 in appendix C. Note that a large number of images were needed to train the CycleGAN for image translation and, to avoid any overlap in image sequences, it unfortunately left a relatively small subset of nuScenes for the evaluation on real rainy images.\n\nFor object detection and depth estimation, we first pretrain each algorithm on ImageNet (Darknet53, 448 × 448) and KITTI (monocular, 1024 × 320) respectively, and further finetune them on the nuScenes-clear(train) subset to limit the domain gap between datasets. We then evaluate their performance on the aforementioned test subsets. For segmentation, since semantic labels are not provided on nuScenes, we carefully annotated 25 images from both nuScenes-clear(test) and nuScenes-rain(test). Since we do not have enough labeled data for finetuning, we use our model pretrained on Cityscapes  [13] , with the caveat that there may be a significative domain gap between training and evaluation.\n\nTable  1  reports the results of this experiment. The performance on real rainy images compared to clear images for all three tasks are a mAP of 16.30% instead of 32.53% for object detection, an AP of 18.7% instead of 40.8% for semantic segmentation and a square relative error of 3.53% instead of 2.96% for depth estimation. Corresponding qualitative results are displayed in fig.  9 . As expected, real rain deteriorates the performance of all algorithms on all tasks. However, we cannot evaluate how rain intensity affects these algorithms since it would require the accurate measurement of the rainfall rate at the time of capture.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Evaluating The Impact Of Synthetic Rain",
      "text": "To study how vision algorithms perform under increasing amounts of rain, we leverage our rain synthesis pipeline and augment popular clear-weather datasets. Specifically, our PBR and GAN+PBR frameworks allow us to measure the performance of these algorithms in controlled rain settings.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Rain Generation Setup",
      "text": "We augment all three of the KITTI, Cityscapes, and nuScenes-clear datasets with PBR. We generate rainfall rates ranging from light to heavy storm R = {0, 5, 25, 50, 100, 200} mm/hr. Only the nuScenes-clear dataset is augmented with GAN and GAN+PBR, since neither KITTI nor Cityscapes contain rainy images to train the GAN. Our PBR and GAN+PBR rain augmentation require some preparation as they rely on calibration, depth and camera motion. Our GAN and GAN+PBR rain augmentation require the training of a CycleGAN. These preparations are described below.\n\nCalibration. For the realistic physical simulator (sec. 3.1.2) and the rain streaks photometric simulation (sec. 3.1.4), intrinsic and extrinsic calibration are used to replicate the imaging sensor. We used frame-wise or sequence-wise calibration for KITTI and nuScenes. In addition, we use 6mm focal and 2ms exposure for KITTI  [23, 22]  and assumed 5ms exposure for nuScenes. As Cityscapes does not provide calibration, we use intrinsic from the camera manufacturer with 5ms exposure and extrinsic is assumed similar to KITTI.\n\nDepth. The scene geometry (pixel depth) is also required to model accurately the light-particle interaction and the fog optical extinction. We estimate KITTI depth maps from RGB+Lidar with  [36] , and Cityscapes/nuScenes from monocular RGB with Monodepth  [24, 25] . While absolute depth is not required, we aim to avoid the critical artifacts along edges, and thus further align RGB with depth using guided filter  [4] .\n\nCamera motion. We mimic the camera ego motion in the physical simulator to ensure realistic rain streak orientation on still images and preserve temporal consistency in sequences. Ego speed is extracted from GPS data when provided (KITTI and nuScenes), or drawn uniformly in the [0, 50] km/hr interval for Cityscapes semantics and in the [0, 100] km/hr interval for KITTI object to reflect the urban and semi-urban scenarios, respectively. pixel-semantic segmentation performance on our weather augmented Cityscapes dataset, and (c) depth estimation performance on our weather augmented nuScenes dataset, all of them as a function of rainfall rate. The object detection plot shows the Coco mAP@[.1:.1:.9] (%) across cars and pedestrians, the semantic segmentation plot shows the AP (%), and the depth estimation shows the squared relative error (%). As opposed to object detection which exhibits some robustness, the segmentation and depth estimation tasks are strongly affected by the rain.\n\nCycleGAN. A CycleGAN is trained for image-to-image rain translation on the train subsets of nuScenes-clear and nuScenes-rain (sec. 5). In order to make sure that no image is used to both train and evaluate the GAN simultaneously, we use the 4449 images from the nuScenes-clear(test) subset, resize them to 448 × 256, and perform image-to-image translation to generate GAN-augmented rain images. We dub this new set of images \"nuScenes-augment\" for clarity, and will also use this for the GAN+PBR rain augmentation.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Evaluating Pbr Rain Augmentation",
      "text": "We compare the performance on PBR-augmented images for 6 object detection algorithms on KITTI  [23]  (7481 images), 6 segmentation algorithms on Cityscapes  [13]  (2995 images) and 2 depth estimation algorithm on nuScenesaugment (4449 images). For all of the algorithms, the clear version always serves as a baseline to which we compare the performance on synthetic rain translations.\n\nObject detection. We evaluate the 6 PBR augmented weathers on KITTI for 6 car/pedestrian pre-trained detection algorithms (with IoU ≥ .7): DSOD  [67] , Faster R-CNN  [60] , R-FCN  [15] , SSD  [49] , MX-RCNN  [73] , and YOLOv2  [59] .\n\nQuantitative results for the Coco mAP@[.1:.1:.9] metric across classes are shown in fig.  10a . Relative to their clearweather performance the 200 mm/hr rain is always at least 12% worse and even drops to 25-30% for R-FCN, SSD, and MX-RCNN, whereas Faster R-CNN and DSOD are the most robust to changes in fog and rain.\n\nRepresentative qualitative results on PBR images are shown in fig.  11  for 4 out of 6 algorithms to preserve space. All algorithms are strongly affected by the rain; it has a chaotic effect on object detection results because there can be large variance of occlusion level for objects populating the image. Also, as in real-life, far away objects (which are generally small objects) are more likely to disappear behind fog-like rain.\n\nSemantic segmentation. For semantic segmentation, the PBR augmented Cityscapes is evaluated for: AdaptSegNet  [70] , ERFNet  [61] , ESPNet  [65] , ICNet  [81] , PSPNet  [82]  and PSPNet(50)  [82] . Quantitative results are reported in fig.  10b . As opposed to object detection algorithms which demonstrated significant robustness to moderately high rainfall rates, here the algorithms seem to breakdown in similar conditions. Indeed, all techniques see their performance drop by a minimum of 30% under heavy fog, and almost 60% under strong rain. Interestingly, some curves cross, which indicates that different algorithms behave differently under rain. ESPNet for example, ranks among the top 3 in clear weather but drops relatively by a staggering 85% and ranks last in stormy conditions (200mm/hr). Corresponding qualitative results are shown in fig.  12  for 4 out of 6 algorithms to preserve space. Although the effect of rain may appear minimal visually, it greatly affects the output of all segmentation algorithms evaluated.\n\nDepth estimation. We evaluate the performance of the recent Monodepth2  [25]  and BTS  [43]  on the nuScenes-augment subset augmented with our PBR method. We report the standard squared relative error in fig.  10c  and note that the error seems to increase linearly with rain. In the extreme 200mm/hr rain conditions, we measure an error of 3x that of clear images. Qualitative results are shown in fig.  13 . It can be observed that performance drops when raindrops block the view or fog-like limits the visibility in the image.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Evaluating Gan And Gan+Pbr Rain Augmentations",
      "text": "Next, we ascertain the effect of the rain augmentation with our GAN and GAN+PBR augmentation strategies. As mentioned above, semantic segmentation algorithms are not eval-  Fig.  14  Performance with varying rain intensities and augmentation techniques. Opposed to PBR, GAN does not allow to control the rain intensity and is reported as dashed line, as for clear performance. Increasing rain intensity translates as a performance drop for (a) object detection with YOLOv2  [59]  and (b) depth estimation with Monodepth2  [25] .\n\nuated due to the lack of semantic labels for training. The evaluation is performed on the nuScenes-augment subset.\n\nObject detection. YOLOv2  [59]  is evaluated, and the resulting mAP as a function of rainfall rates are reported in fig.  14a . Qualitative results are shown in fig.  15 . We note that GAN augmented images have similar performance than PBR 100mm/hr images and that performance deterioration is stronger and steeper with GAN+PBR images.\n\nObserve that the particles physical simulation is the same in both PBR and GAN+PBR. Still, it is interesting to notice that the decrease is different with GAN+PBR compared to PBR (i.e. curves are shifted but also exhibit different slopes). This may be the result of the two cumulative domain shifts (i.e. wetness + streaks) leading to a non-linear effect.\n\nDepth estimation. We evaluate the performance of the recent Monodepth2  [25]  on the nuScenes-augment subset augmented with our GAN and GAN+PBR methods. As reported in fig.  14b , for the same rain intensity between PBR and GAN+PBR images, the error is worse by a factor of 80-100%. The same behavior was observed on other standard depth estimation metrics (absolute square error, RMSE, log RMSE), not reported here. Interestingly, the GAN augmentation affects only slightly the performance on depth estimation which might be because GAN translation keeps occlusion of the image to a minimum. Qualitative results are shown in fig.  16  for GAN and GAN+PBR.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Improving The Robustness To Rain",
      "text": "We now wish to demonstrate the usefulness of our rain rendering pipeline for improving robustness to rain through extensive evaluations on synthetic and real rain databases. For the sake of coherence, the improvements are shown on the same tasks, algorithms, and test data from sec. 5.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Training Methodology",
      "text": "While the ultimate goal is to improve robustness to rain, we aim at training a single model which performs well across a wide variety of rainfall rates (including clear weather). Having a single model is beneficial over employing, e.g., intensity-specific encoders  [57] , since it removes the need for determining rain intensity from the input image. Because rain significantly alters the appearance of the scene, we found that training from scratch with heavy rain or random rainfall rates fails to converge. Instead, we refine our untuned models using curriculum learning  [5]  on rain intensity in ascending order (25, then 50, and finally 100mm/hr rain). The final model is referred as finetuned and is evaluated against various weather conditions. Note that, for hybrid augmentation finetuning, the curriculum starts with the refinement on GAN images first and then go through the ascending rain intensities. The same images are used for all steps of the curriculum.\n\nIn order to avoid training and testing on the same set of images, in this section, we further divide the nuScenesaugment into train/test subsets of 1000 images each (ensuring they are taken from different scenes). Each algorithm is thus refined on the 1000 images from nuScenes-augment(train), and undergo a specific training process. For object detection, YOLOv2  [59]  is trained each step at a learning rate of 0.0001 and a momentum of 0.9 for 10 epochs with a burn-in of 5 epochs. For semantic segmentation, PSPNet  [82]  is trained with a learning rate of 0.0004 and a momentum of 0.9 for 10 epochs. Finally, for depth estimation, Monodepth2  [25]  is trained on triplets of consecutive images using a learning rate of 0.00001 with the Adam optimizer for 10 epochs with β = {0.5, 0.999}.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Improvement On Synthetic Rain",
      "text": "The synthetic evaluation is conducted on the set of 1000 images from nuScenes-augment(test), with rain up to 200mm/hr. Note again, for our hybrid GAN+PBR, 0mm/hr of rain correspond to the GAN-only augmented results. Fig.  17  shows the performance of our untuned and finetuned model for the three vision tasks on different augmented dataset. Fig.  17a  and fig.  17b  are for object detection (YOLOv2  [59] ) and depth estimation (Monodepth2  [25] ) on nuScenes-clear augmented data while fig.  17c  is for the semantic segmentation (PSPNet  [82] ) on Cityscapes. We observe a significant improvement in both tasks and additional increase in robustness even in clear weather when refined using our augmented rain. Of interest, we also improve at the unseen 200mm/hr rain though the network was only trained with rain up to 100mm/hr. The intuition here is that when facing adverse weather, the network learns to focus on strongest relevant features for all tasks and thus gain robustness.   PBR. For YOLOv2, the finetuned detection performance stays higher than its clear untuned counterpart in the 0-200mm/hr interval. Explicitly, it goes from 34.5% to 31.0% whereas the untuned model starts at 34.6% and finishes at 20.4%. For PSPNet, the segmentation exhibits a significative improvement when refined although at 100mm/hr the model is not fully able to compensate the effect of rain and drops to 54.0% versus 52.0% when untuned. Monodepth2 finetuning helps only for higher rain intensity level (+25mm/hr) and the error differences between 100mm/hr and 200mm/hr stay in the same ballpark (~1.2%). This makes sense considering that since the occlusion created by rain streaks is minimum with a low rain intensity, the untuned model would not be strongly affected.\n\nGAN+PBR. In the case of YOLOv2, we notice a major difference between the hybrid GAN+PBR untuned and finetuned performances. Indeed, the hybrid finetuned performance at 100mm/hr is at 21.7% and only at a measly 7.5% for the untuned model. The same goes for Monodepth2 for hybrid images performance with 5.7% and 8.9% at 100mm/hr for finetuned and untuned respectively. It is interesting to note that, for all tasks, performance evaluated on finetuned hybrid image decrease slower than for untuned models. This demonstrates again that more robust models are learned when finetuning with our rain translations. Table  2  Improving performance of computer vision tasks on real nuScenes  [9]  images. These tasks are object detection (YOLOv2  [59] ), semantic segmentation (PSPNet  [82] ), and depth estimation (Mon-odepth2  [25] ). The last line shows performance with the untuned models after the de-raining  [50]  process.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Improvement On Real Rain",
      "text": "We evaluate the performance on real rain, using our nuScenesrain(test) subset of images (see sec. 5). Table  2  shows that our finetuning leads to performance increase in real rainy scenes compared to untuned performance in rain. We note for object detection (PBR: +20.7%, GAN: +10.9%, GAN+PBR: +21.0%), for semantic segmentation (PBR: +36.9%), and for depth estimation (PBR: 0.0%, GAN: +3.8%, GAN+PBR: +8.2%) tasks. In clear weather, our finetuned model performs on par with the untuned version, sometimes even better. This boost in performance could be seen as the network learning to rely on more robust features, somehow invariant to rain streaks. Depth estimation underperformance for PBR finetuning can be explained by the learning loss of Monodepth2 which is, in short, a reprojection error which would not fare well with rain streaks as they do not reproject in consecutive frames. Interestingly, this problem does not seem to affect the GAN or GAN+PBR finetuned model, possibly because the GAN is being trained on split of nuScenes subsequently leading to finetuning images that are more resembling of the test set. These results demonstrate the usefulness of our different rain rendering frameworks for real rain scenarios.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "De-Raining Comparison",
      "text": "We now compare to the strategy of de-raining images first and then running un-tuned vision algorithms. To this end, we used the state-of-the-art de-raining method DualResNet  [50] , finetuned using nuScenes-clear augmented with GAN+PBR to accommodate for the domain gap.\n\nDuring the de-raining fine-tuning process, random batches of {25, 50, 100, 200}mm/hr paired with their nonaugmented counterpart are generated. Except for a smaller learning rate (10 -5 ), we used the DualResNet default hyper- Fig.  18  Performance with varying rain intensities on de-rained GAN+PBR synthetic images. The de-raining is performed with  [50] .\n\nWe see that the performance on both tasks decrease linearly for both (a) object detection with YOLOv2  [59]  and (b) depth estimation with Monodepth2  [25] . Performance on both tasks are lower at low rain intensities (and the opposite at high rain intensities) compared to models finetuned with GAN+PBR synthetic images (cf. fig.  17 ).\n\nparameters (Adam optimizer, batch and crop size of 40 and 64 respectively). With this de-raining finetuned model, we compare the performance of our \"untuned\" object detection (YOLOv2  [59] ) and depth estimation (Monodepth2  [25] ) models. Fig.  18  shows the performance of the de-raining strategy compared to our \"rain-aware\" GAN+PBR finetuned models. Here, we observe that the rain-aware models offer improved performance for object detection over de-raining, while the latter improves depth estimation. This is likely due to the fact that streaks occlude the scene background, while de-raining acts as prior impainting thus easing depth estimation.\n\nWe also applied the same de-raining strategy to the real nuScenes images and report performance in the last row of table 2. Again, for object detection on rainy images our rainaware models perform better than de-raining. However, for depth estimation, the de-raining strategy is better for both clear and rainy images. This is consistent with the results obtained on synthetic data.\n\nThese experiments illustrate that de-raining is also a valid strategy that may even outperform \"rain-aware\" algorithms. However, this comes at the cost of having to perform two tasks, which may limit practical applications. On the long term, we believe rain-robust algorithms offer an exciting new research paradigm while avoiding the in-filling of occluded areas.",
      "page_start": 14,
      "page_end": 15
    },
    {
      "section_name": "Discussion",
      "text": "In this paper, we presented the first intensity-controlled physical framework for augmenting existing image databases with realistic rain. This allows us to systematically study the impact of rain on existing computer vision algorithms on three important tasks: object detection, semantic segmentation, and depth estimation.\n\nLimitations and future work. While we demonstrated highly realistic rain rendering results, our approach still has limitations that set the stage for future work.\n\nFor our PBR approach, the approximation of the lighting conditions 3.1.1 yielded reasonable results (fig.  3 ), but it may under/over estimate the scene radiance when the sky is not/too visible. This approximation is more visible when streaks are imaged against a darker sky. More robust approaches for outdoor lighting estimation could potentially be used  [30, 80] . Second, we make an explicit distinction between fog-like rain and drops imaged on more than 1 pixel, individually rendered as streaks. While this distinction is widely used in the literature  [21, 19, 11, 45] , it causes an inappropriate sharp distinction between fog-like and streaks. A possible solution would be to render all drops as streaks weighting them as a function of their imaging surface. However, our experiments show it comes at a prohibitive computation cost. Finally, rain streaks are added to image irrespective of the scene contents. Here, the depth estimate could be used to mask out streaks that appear behind objects and under the ground plane. Another limitation is the computational cost of PBR. While this has no downside for benchmarking purpose as PBR may be run off-line, simulation requires increasing time with larger rainfall rates. With our current unoptimized implementation, the simulation of 1 / 25 / 50 / 100 mm/hr rainfall rates on Cityscapes requires 0.35 / 5.65 / 16.60 / 20.67 seconds respectively for the rain physics  [11]  and an additional 6.71 / 34.92 / 62.94 / 104.76 seconds for the rendering (times are per image, on single core, and averaged over 100 frames). This restricts the usage of PBR to off-line processing though significant speed up could be obtained at the cost of additional optimization efforts.\n\nThe GAN employed also has limitations. First, while PBR is well-suited for videos since the rain simulator is temporally consistent, this is not the case for CycleGAN which does not guarantee temporal smoothness. Existing approaches such as  [2]  are alternatives, but GANs are known for their non-realistic physical outcome  [72] . Second, Cycle-GAN imposes a limit on the image resolution. Here, superresolution networks such as SRResNet  [16]  or SRGAN  [42]  could potentially be used, or large-scale GANs such as Big-GAN  [8]  are also an option. More importantly, GANs tend to have difficulty in generating rain on images of datasets different than which they are trained on since the learning process does not disentangle rain from scene appearance, demonstrating a strong domain dependence. Finally, while our results demonstrate that fine-tuning on synthetically generated rain does improve performance on real rainy images (cf. sec. 7), the improvements obtained are still quite modest. Further efforts are necessary to develop algorithms that are truly robust to challenging rainy conditions.    19  Geometrical construction to compute a drop FOV. Considering X 0 and X 1 the drop position at shutter opening and closing, respectively. We assume a constant drop position X = X0+X1 2 (during the exposure time, a few milliseconds). Note that we drew only a slice of the drop FOV for simplicity but a full 3D visualization would show a full 3D cone. The drop FOV in the environment map is the projection of the 3D drop FOV on the scene sphere of constant distance (refer to text for details). Champlain grant. We gratefully thank Pierre Bourré for his priceless technical help, Aitor Gomez Torres for his initial input, and Srinivas Narasimhan for letting us reuse the physical simulator. We also thank the Nvidia corporation for the donation of the GPU used in this research.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "A Field Of View Of A Drop In A Sphere",
      "text": "We estimate the field of view (FOV) of a drop when projected on a sphere to compute the radiance and chromaticity of each streak, as detailed in sec. 3.3 of the main paper. Despite its motion, we make the assumption of a constant field of view within a given exposure time. This is acceptable due to the short exposure time used here (i.e. 2ms for KITTI, 5ms for Cityscapes). For each drop, the simulator outputs the start position (i.e. shutter opening) and end position (i.e. shutter closing) in both the 3D camera-centered and the 2D image coordinate frames.\n\nWe refer to the fig.  19  for a geometrical illustration of the following. Let us consider an imaged drop D, having 3D start position X 0 and end position X 1 . We first compute X = X0+X1 2 the assumed constant position for which we will estimate the corresponding FOV. The position being camera-centered, the drop viewing direction is therefore d = X ||X|| .\n\nWe compute the equation of the plane P going through X and orthogonal to the viewing direction d:\n\nwhere • is the dot product and select a random vector u (with ||u|| = 1) lying on P . Accounting for the field of view of the drop θ ≈ 165 • (according to  [21] ), we compute an arbitrary vector v on the viewing cone through the drop\n\nwith R u (θ/2) the 3x3 general rotation matrix of angle θ/2 about vector u. We use θ/2 because the cone being symmetric along the viewing direction, the complete cone field of view obtain is θ. The set V of vectors forming the viewing cone through the drop is obtained by the rotation of v all around the viewing direction. Formally,\n\nwith R d (α) the rotation matrix of α around vector d. In practice, V is a finite set of radially equidistant vectors (for computational reason we use |V | = 20).\n\nTo compute the coordinates of the drop FOV in the environment, we assume a projection sphere S of radius 10m. Hence, we compute the set Q = {φ(S, v ) | ∀ v ∈ V } of points where vectors intersect the environment sphere, considering only the positive viewing direction axis. Given that the sphere is centered to the camera position and all drops 3D positions are expressed in the camera referential, the intersection φ(S, v ) of a vector v and sphere S of radius S ρ is straight-forward with φ(S, v ) = v + td with,\n\nHaving computed Q, the finite set of 3D positions intersecting our environment sphere S, the set Q of spherical coordinates (azimuth, altitude) are obtained from simple Cartesian to spherical mapping, and directly translated to the environment map. Thus, Q is the projection of the drop FOV on the environment map. Accounting for implementation details, one may note that Q is a discrete representation of the drop field of view contours. In practice, a polygon filling algorithm is used to obtain the drop FOV F , which we use for computing the photometry of a rainstreak (cf. sec. 3.3.2 of the main paper).",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "B Compositing A Rain Streak With Different Exposure Time",
      "text": "In their seminal work, Garg and Nayar  [19]  demonstrated that the streak appearance is closely related to the amount of time τ a drop stays on a pixel. It is thus important to account for the difference of exposure time in the streak appearance database  [20]  when adding rain to existing images. Given that the appearance database does not provide enough calibration data to recompute the exact original τ 0 , we estimate it using observations made in appendix 10.3 of  [21] . The latter states that for a constant exposure time τ can be safely approximated with √ a/50 (a the drop diameter, in meters), which we use to compute τ 0 according to simulation settings in  [20] .\n\nUsing the notation defined in eq. (  6 ) from the main paper, the radiance of streak S is corrected with\n\nwhere τ 1 is the time the current drop stays on a pixel, as obtained in a streak-wise fashion by the physical simulator. Noteworthy,  [21]  also emphasizes that for a given streak the changes of τ across pixels are negligible, so τ can safely be assumed constant. Finally, after normalization, the alpha of each streak is scaled according to τ 1 and the targeted exposure time T . According to Garg and Nayar equations (cf. eq. (  18 ) from  [21] ), the composite rainy image is an alpha blending of the background image I bg and the rain layer I r . For pixel x corresponding to x in the streak coordinates, it leads to:\n\nKitti  [23]  Cityscapes  [13]  nuScenes-clear  [9]",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "C Experiments Data Splits",
      "text": "Table  3  contains the minutiae of the data splits of the various experimental steps of this paper.",
      "page_start": 19,
      "page_end": 19
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Vision tasks in clear and rain-augmented images. Our syn-",
      "page": 1
    },
    {
      "caption": "Figure 2: Physics-Based Rendering for rain augmentation. We use par-",
      "page": 3
    },
    {
      "caption": "Figure 3: Estimation of raindrop photometry. To estimate the photo-",
      "page": 4
    },
    {
      "caption": "Figure 4: Photometric validation of rain. Rain rendering using ground",
      "page": 5
    },
    {
      "caption": "Figure 5: GAN+PBR rain-augmentation architecture. In this hybrid",
      "page": 6
    },
    {
      "caption": "Figure 6: presents real photographs captured under heavy rain,",
      "page": 6
    },
    {
      "caption": "Figure 6: Comparison of real photographs and our renderings. Real",
      "page": 6
    },
    {
      "caption": "Figure 7: User study of rainy images realism. The y-axis displays rat-",
      "page": 7
    },
    {
      "caption": "Figure 8: User study on images characteristics conveying rain. The y-",
      "page": 7
    },
    {
      "caption": "Figure 9: Qualitative results on real rainy images from the nuScenes dataset. Shown for different tasks: object detection (top line), semantic",
      "page": 8
    },
    {
      "caption": "Figure 10: Performance using our PBR rain augmentation. (a) Object",
      "page": 9
    },
    {
      "caption": "Figure 11: Object detection on PBR rain augmentation of KITTI. From left to right, the original image (clear) and three PBR augmentations with",
      "page": 10
    },
    {
      "caption": "Figure 12: Qualitative evaluation of semantic segmentation on our PBR rain augmentation of Cityscapes. From left to right, the original image",
      "page": 11
    },
    {
      "caption": "Figure 13: Depth estimation on our PBR rain augmentation of nuScenes. From left to right, the original image (clear) and three PBR augmentations",
      "page": 11
    },
    {
      "caption": "Figure 14: Performance with varying rain intensities and augmenta-",
      "page": 12
    },
    {
      "caption": "Figure 17: shows the performance of our untuned and ﬁne-",
      "page": 12
    },
    {
      "caption": "Figure 17: a and ﬁg. 17b are for object detection",
      "page": 12
    },
    {
      "caption": "Figure 15: Object detection on our GAN+PBR augmented nuScenes. From left to right, the original image (clear), the GAN augmented image and",
      "page": 13
    },
    {
      "caption": "Figure 16: Depth estimation on our GAN+PBR augmented nuScenes. From left to right, the original image (clear), the GAN augmented image",
      "page": 13
    },
    {
      "caption": "Figure 17: Original (untuned) or ﬁnetuned performance on rain-",
      "page": 14
    },
    {
      "caption": "Figure 18: Performance with varying rain intensities on de-rained",
      "page": 15
    },
    {
      "caption": "Figure 18: shows the performance of the de-raining strategy compared",
      "page": 15
    },
    {
      "caption": "Figure 19: Geometrical construction to compute a drop FOV. Considering",
      "page": 16
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Abstract Rain ﬁlls": "",
          "the atmosphere with water particles,": "which breaks the common assumption that light travels unal-"
        },
        {
          "Abstract Rain ﬁlls": "tered from the scene to the camera. While it is well-known",
          "the atmosphere with water particles,": ""
        },
        {
          "Abstract Rain ﬁlls": "that rain affects computer vision algorithms, quantifying its",
          "the atmosphere with water particles,": ""
        },
        {
          "Abstract Rain ﬁlls": "impact is difﬁcult. In this context, we present a rain rendering",
          "the atmosphere with water particles,": ""
        },
        {
          "Abstract Rain ﬁlls": "pipeline that enables the systematic evaluation of common",
          "the atmosphere with water particles,": ""
        },
        {
          "Abstract Rain ﬁlls": "",
          "the atmosphere with water particles,": "computer vision algorithms to controlled amounts of rain."
        },
        {
          "Abstract Rain ﬁlls": "We present three different ways to add synthetic rain to ex-",
          "the atmosphere with water particles,": ""
        },
        {
          "Abstract Rain ﬁlls": "isting images datasets: completely physic-based; completely",
          "the atmosphere with water particles,": ""
        },
        {
          "Abstract Rain ﬁlls": "data-driven; and a combination of both. The physic-based",
          "the atmosphere with water particles,": ""
        },
        {
          "Abstract Rain ﬁlls": "rain augmentation combines a physical particle simulator",
          "the atmosphere with water particles,": ""
        },
        {
          "Abstract Rain ﬁlls": "and accurate rain photometric modeling. We validate our",
          "the atmosphere with water particles,": ""
        },
        {
          "Abstract Rain ﬁlls": "rendering methods with a user study, demonstrating our rain",
          "the atmosphere with water particles,": ""
        },
        {
          "Abstract Rain ﬁlls": "",
          "the atmosphere with water particles,": "is judged as much as 73% more realistic than the state-of-the-"
        },
        {
          "Abstract Rain ﬁlls": "",
          "the atmosphere with water particles,": "art. Using our generated rain-augmented KITTI, Cityscapes,"
        },
        {
          "Abstract Rain ﬁlls": "and nuScenes datasets, we conduct a thorough evaluation of",
          "the atmosphere with water particles,": ""
        },
        {
          "Abstract Rain ﬁlls": "",
          "the atmosphere with water particles,": "object detection, semantic segmentation, and depth estima-"
        },
        {
          "Abstract Rain ﬁlls": "tion algorithms and show that their performance decreases in",
          "the atmosphere with water particles,": ""
        },
        {
          "Abstract Rain ﬁlls": "",
          "the atmosphere with water particles,": "degraded weather, on the order of 15% for object detection,"
        },
        {
          "Abstract Rain ﬁlls": "60% for semantic segmentation, and 6-fold increase in depth",
          "the atmosphere with water particles,": ""
        },
        {
          "Abstract Rain ﬁlls": "estimation error. Finetuning on our augmented synthetic data",
          "the atmosphere with water particles,": ""
        },
        {
          "Abstract Rain ﬁlls": "results in improvements of 21% on object detection, 37% on",
          "the atmosphere with water particles,": ""
        },
        {
          "Abstract Rain ﬁlls": "semantic segmentation, and 8% on depth estimation.",
          "the atmosphere with water particles,": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "semantic segmentation, and 8% on depth estimation.": "Keywords Adverse weather · vision and rain · physics-"
        },
        {
          "semantic segmentation, and 8% on depth estimation.": ""
        },
        {
          "semantic segmentation, and 8% on depth estimation.": "based rendering · image to image translation · GAN"
        },
        {
          "semantic segmentation, and 8% on depth estimation.": ""
        },
        {
          "semantic segmentation, and 8% on depth estimation.": ""
        },
        {
          "semantic segmentation, and 8% on depth estimation.": ""
        },
        {
          "semantic segmentation, and 8% on depth estimation.": ""
        },
        {
          "semantic segmentation, and 8% on depth estimation.": "1 Introduction"
        },
        {
          "semantic segmentation, and 8% on depth estimation.": ""
        },
        {
          "semantic segmentation, and 8% on depth estimation.": ""
        },
        {
          "semantic segmentation, and 8% on depth estimation.": "A common assumption in computer vision is that light trav-"
        },
        {
          "semantic segmentation, and 8% on depth estimation.": "els unaltered from the scene to the camera. In clear weather,"
        },
        {
          "semantic segmentation, and 8% on depth estimation.": ""
        },
        {
          "semantic segmentation, and 8% on depth estimation.": "this assumption is reasonable: the atmosphere behaves like"
        },
        {
          "semantic segmentation, and 8% on depth estimation.": ""
        },
        {
          "semantic segmentation, and 8% on depth estimation.": "a transparent medium and transmits light with very little"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2": "attenuation or scattering. However, inclement weather condi-",
          "Maxime Tremblay et al.": "translation method [83] is used to convey a sense of “wet-"
        },
        {
          "2": "tions such as rain ﬁll the atmosphere with particles producing",
          "Maxime Tremblay et al.": "ness” to the scene, and physics-based rain is subsequently"
        },
        {
          "2": "spatio-temporal artifacts such as attenuation or rain streaks.",
          "Maxime Tremblay et al.": "composited on the resulting image. Here, we augment\nthe"
        },
        {
          "2": "This creates noticeable changes to the appearance of images",
          "Maxime Tremblay et al.": "nuScenes dataset [9], and use it\nto evaluate the robustness"
        },
        {
          "2": "(see ﬁg. 1), thus creating additional challenges to computer",
          "Maxime Tremblay et al.": "of object detection and depth estimation algorithms. Finally,"
        },
        {
          "2": "vision algorithms which must be robust to these conditions.",
          "Maxime Tremblay et al.": "we also use the latter to reﬁne algorithms using curriculum"
        },
        {
          "2": "While the inﬂuence of rain on image appearance is well-",
          "Maxime Tremblay et al.": "learning [5], and demonstrate improved robustness on real"
        },
        {
          "2": "known and understood [19], its impact on the performance",
          "Maxime Tremblay et al.": "rainy images."
        },
        {
          "2": "of computer vision tasks is not. Indeed, how can one evalu-",
          "Maxime Tremblay et al.": "In short, we make the following contributions. First, we"
        },
        {
          "2": "ate what the impact of, say, a rainfall rate of 100 mm/hour",
          "Maxime Tremblay et al.": "present\ntwo different\nrealistic rain rendering approaches:"
        },
        {
          "2": "(a typical autumn shower) is on the performance of an ob-",
          "Maxime Tremblay et al.": "the ﬁrst\nis\na\npurely\nphysic-based method\nand\nthe\nsec-"
        },
        {
          "2": "ject detector, when our existing databases all contain images",
          "Maxime Tremblay et al.": "ond is a combination of a GAN-based approach and this"
        },
        {
          "2": "overwhelmingly captured under clear weather conditions? To",
          "Maxime Tremblay et al.": "physic-based framework. Second, we augment KITTI [23],"
        },
        {
          "2": "measure this effect, one would need a labeled object detec-",
          "Maxime Tremblay et al.": "Cityscapes [13], and nuScenes [9] datasets with rain. Third,"
        },
        {
          "2": "tion dataset where all the images have been captured under",
          "Maxime Tremblay et al.": "we present a methodology for systematically evaluating the"
        },
        {
          "2": "100 mm/hour rain. Needless to say, such a \"rain-calibrated\"",
          "Maxime Tremblay et al.": "performance of 13 popular algorithms—for object detection,"
        },
        {
          "2": "dataset does not exist, and capturing one is prohibitive. In-",
          "Maxime Tremblay et al.": "semantic segmentation and depth estimation—on rainy im-"
        },
        {
          "2": "deed, datasets with bad weather\ninformation are few and",
          "Maxime Tremblay et al.": "ages. Our ﬁndings indicate that rain affects all algorithms:"
        },
        {
          "2": "sparse, and typically include only high-level\ntags (rain or",
          "Maxime Tremblay et al.": "performance drops of 15% mAP for object detection, 60%"
        },
        {
          "2": "not) without mentioning how much rain is falling. While",
          "Maxime Tremblay et al.": "AP for semantic segmentation, and a 6-fold increase in depth"
        },
        {
          "2": "they can be used to improve vision algorithms under ad-",
          "Maxime Tremblay et al.": "estimation error. Finally, our augmented database can also be"
        },
        {
          "2": "verse conditions by including rainy images in the training set,",
          "Maxime Tremblay et al.": "used to ﬁnetune these same algorithms in order to improve"
        },
        {
          "2": "they cannot help us in systematically evaluating performance",
          "Maxime Tremblay et al.": "their performance in real-world rainy conditions."
        },
        {
          "2": "degradation under increasing amounts of rain.",
          "Maxime Tremblay et al.": "This paper signiﬁcantly extends an earlier version of"
        },
        {
          "2": "Alternatively, one can attempt to remove its effects from",
          "Maxime Tremblay et al.": "this work published in [27], by combining physics-based"
        },
        {
          "2": "images—i.e., create a “clear weather” version of the image—",
          "Maxime Tremblay et al.": "rendering with learning-based image-to-image translation"
        },
        {
          "2": "prior to applying subsequent algorithms. For example, rain",
          "Maxime Tremblay et al.": "methods, conducting a novel, more in-depth user study, eval-"
        },
        {
          "2": "can be detected and attenuated from images [21,3,74,79,50].",
          "Maxime Tremblay et al.": "uating depth estimation algorithms, comparing to a derain-"
        },
        {
          "2": "We experiment with this approach in sec. 7.4. An alternative",
          "Maxime Tremblay et al.": "ing approach, and by providing a more extensive evalua-"
        },
        {
          "2": "approach is to employ programmable lighting to reduce the",
          "Maxime Tremblay et al.": "tion of the performance improvement on real\nimages. Our"
        },
        {
          "2": "rain visibility, by shining light between raindrops [11]. Un-",
          "Maxime Tremblay et al.": "framework is readily usable to augment existing image with"
        },
        {
          "2": "fortunately, these solutions either add signiﬁcant processing",
          "Maxime Tremblay et al.": "realistic rainy conditions. Code and data are available at"
        },
        {
          "2": "times to already constrained time budgets, or require custom",
          "Maxime Tremblay et al.": "the following URL: https://team.inria.fr/rits/"
        },
        {
          "2": "hardware. Instead, if we could systematically study the effect",
          "Maxime Tremblay et al.": "computer-vision/weather-augment/."
        },
        {
          "2": "of weather on images, we could better understand the robust-",
          "Maxime Tremblay et al.": ""
        },
        {
          "2": "ness of existing algorithms, and, potentially, increase their",
          "Maxime Tremblay et al.": ""
        },
        {
          "2": "robustness afterwards.",
          "Maxime Tremblay et al.": ""
        },
        {
          "2": "In this paper, we propose methods to realistically aug-",
          "Maxime Tremblay et al.": "2 Related work"
        },
        {
          "2": "ment existing image databases with rainy conditions. We",
          "Maxime Tremblay et al.": ""
        },
        {
          "2": "rely on well-understood physical models as well as on re-",
          "Maxime Tremblay et al.": "Rain modeling\nIn their series of inﬂuential papers, Garg and"
        },
        {
          "2": "cent\nimage-to-image translations to generate visually con-",
          "Maxime Tremblay et al.": "Nayar provided a comprehensive overview of the appearance"
        },
        {
          "2": "vincing results. First, we experiment with our novel physics-",
          "Maxime Tremblay et al.": "models required for understanding [21] and synthesizing [20]"
        },
        {
          "2": "based approach, which is the ﬁrst\nto allow controlling the",
          "Maxime Tremblay et al.": "realistic rain. In particular, they propose an image-based rain"
        },
        {
          "2": "amount of rain in order to generate arbitrary amounts, rang-",
          "Maxime Tremblay et al.": "streak database [20] modeling the drop oscillations, which"
        },
        {
          "2": "ing from very light rain (5 mm/hour rainfall) to very heavy",
          "Maxime Tremblay et al.": "we exploit in our physics-based rendering framework. Other"
        },
        {
          "2": "storms (200+ mm/hour). This key feature allows us to pro-",
          "Maxime Tremblay et al.": "streak appearance models were proposed in [71, 3] using a"
        },
        {
          "2": "duce weather-augmented datasets, where the rainfall rate is",
          "Maxime Tremblay et al.": "frequency model. Realistic rendering was also obtained with"
        },
        {
          "2": "known and calibrated. Subsequently, we augment two exist-",
          "Maxime Tremblay et al.": "ray-tracing [64] or artistic-based techniques [69,14] but on"
        },
        {
          "2": "ing datasets (KITTI [23] and Cityscapes [13]) with rain, and",
          "Maxime Tremblay et al.": "synthetic data as they require complete 3D knowledge of the"
        },
        {
          "2": "evaluate the robustness of popular object detection and seg-",
          "Maxime Tremblay et al.": "scene including accurate light estimation. Numerous works"
        },
        {
          "2": "mentation algorithms on these augmented databases. Second,",
          "Maxime Tremblay et al.": "also studied generation of raindrops on-screen, with 3D mod-"
        },
        {
          "2": "we experiment with a combination of physics- and learning-",
          "Maxime Tremblay et al.": "eling and ray-casting [62, 63, 28, 29] or normal maps [57]"
        },
        {
          "2": "based approaches, where a popular unpaired image-to-image",
          "Maxime Tremblay et al.": "some also accounting for focus blur."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "Rain removal Due to the problems it creates on computer vi-",
          "3": "Camera \nParticles \nProjection"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "3": "ego motions\nsimulator [11]"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "sion algorithms, rain removal in images got a lot of attention",
          "3": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "initially focusing on photometric models [18]. For this, sev-",
          "3": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "3": "Warp\nRain streak\nIllumination"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "eral techniques have been proposed, ranging from frequency",
          "3": "estimation \nphotometry"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "3": "Streak db [20]"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "space analysis [3] to deep networks [74]. Sparse coding and",
          "3": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "layers priors were also important axes of research [46,51,12]",
          "3": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "3": "Rainfall \nFog-like"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "3": "compositing"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "due to their facility to encode streak patches. Recently, dual",
          "3": "rain attenuation"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "residual networks have been employed [50]. Alternatively,",
          "3": "Rainy images\nClear images"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "camera parameters [19] or programmable light sources [11]",
          "3": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "can also be adjusted to limit\nthe impact of rain on the im-",
          "3": "Depth \nestimation"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "3": "[24,25]"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "age formation process. Additional proposals were made for",
          "3": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "3": "Depth maps"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "the speciﬁc task of raindrops removal on windows [17] or",
          "3": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "3": "Fig. 2 Physics-Based Rendering for rain augmentation. We use par-"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "windshields [28,57,29].",
          "3": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "3": "ticles simulation together with depth and illumination estimation to"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "3": "render arbitrarily controlled rainfall on clear images."
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "Unpaired\nimage\ntranslation An\ninteresting\nsolution\nto",
          "3": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "weather augmentation is the use of data-driven unpaired im-",
          "3": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "3": "train algorithms and there exists no dataset with systemat-"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "age translation frameworks. Zhang et al. [79] proposed to use",
          "3": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "3": "ically recorded rainfall\nrates and object/scene labels. The"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "conditional GANs for rain removal. By proposing to add a",
          "3": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "3": "closest systematic works in spirit [38,39] evaluated the effect"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "cyclic loss to the learning process, CycleGAN [83] became",
          "3": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "3": "of simulating weather on vision, but did so in purely virtual"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "a signiﬁcant paper for unpaired image translation; they pro-",
          "3": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "3": "environments (GTA and CARLA, respectively) whereas we"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "duced interesting results in weather and season translation.",
          "3": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "3": "augment real\nimages. Of particular relevance to our work,"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "DualGAN [75] uses similar ideas with differences in the net-",
          "3": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "3": "Sakaridis et al. [66] propose a framework for rendering fog"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "work models. The UNIT [47], MUNIT [33], and FUNIT [48]",
          "3": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "3": "into images from the Cityscapes [13] dataset. Their approach"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "frameworks all, in one way or another, propose to perform",
          "3": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "3": "assumes a homogeneous fog model, which is rendered from"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "image translation with the common idea that data from differ-",
          "3": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "3": "the depth estimated from stereo. Existing scene segmenta-"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "ent sets have a shared latent space. They showed interesting",
          "3": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "3": "tion models and object detectors are then adapted to fog. In"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "results on adding and removing weather effect\nto images.",
          "3": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "3": "our work, we employ the similar idea of rendering realistic"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "Since the information in clear and rainy images is symmetri-",
          "3": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "3": "weather on top of existing images, but we focus on rain rather"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "cal, many unsupervised image translation approaches could",
          "3": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "3": "than fog."
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "produce decent visual\nresults.\nIn [56], Pizzati et al.\nlearn",
          "3": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "to disentangle the scene from lens occlusions such as rain-",
          "3": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "drops, which improves both realism and physical accuracy",
          "3": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "3": "3 Rain Augmentation"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "of\nthe translations. Another strategy for better qualitative",
          "3": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "translations is to rely on semantic consistency [44,68].",
          "3": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "3": "Broadly\nspeaking,\nsynthesizing\nrain\non\nimages\ncan\nbe"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "3": "achieved\nusing\ntwo\nseemingly\nantagonistic methods:"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "Weather\ndatabases\nIn\ncomputer\nvision,\nfew\nimages",
          "3": "1) physics-based rendering (PBR) methods [28, 20], which"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "databases have precise\nlabeled weather\ninformation. Of",
          "3": "explicitly model\nthe dynamics and the radiometry of\nrain"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "note for mobile robotics,\nthe BDD100K [76],\nthe Oxford",
          "3": "drops in images; or 2) learning-based image-to-image transla-"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "dataset\n[52], and Wilddash [77] provide data recorded in",
          "3": "tion approaches [83, 33], which train deep neural networks to"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "various weather conditions, including rain. Other stationary",
          "3": "“translate” an image into its rainy version. While completely"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "camera datasets such as AMOS [35], the transient attributes",
          "3": "different, we argue both these methods offer complemen-"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "dataset [40], the Webcam Clip Art dataset [41], or the WILD",
          "3": "tary advantages. On one hand, physics-based approaches are"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "dataset [55] are sparsely labeled with weather information.",
          "3": "accurate, controllable, can simulate a wide variety of imag-"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "The relatively new nuScenes dataset\n[9] have multiple la-",
          "3": "ing conditions, and do not require any training data. On the"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "beled scenes containing rainy images, but variation in rain",
          "3": "other hand, learning-based approaches can realistically simu-"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "intensities are not\nindicated. Gruber et al. [26] recently re-",
          "3": "late important visual cues such as wetness, cloud cover, and"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "leased a dataset with dense depth labels under a variety of",
          "3": "overall gloominess typically associated with rainy images."
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "real weather conditions produced by a controlled weather",
          "3": "In this paper, we propose to ﬁrst explore the use of both"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "chamber, which inherently limits the variety of scenes (lim-",
          "3": "techniques independently,\nthen to combine them into a hy-"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "ited to four common scenarios) in the dataset. Note that [6]",
          "3": "brid approach. This\nsection thus ﬁrst describes our PBR"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "also announced—but at\nthe time of writing, not yet\nfully",
          "3": "approach (sec. 3.1), followed by the image-to-image trans-"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "available—a promising dataset\nincluding heavy snow and",
          "3": "lation with a GAN (sec. 3.2), and concludes with the hybrid"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "rain events. Still, datasets with rainy data are too small\nto",
          "3": "combination of the two GAN+PBR (sec. 3.3)."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4": "",
          "Maxime Tremblay et al.": "3.1.2 Simulating the physics of raindrops"
        },
        {
          "4": "",
          "Maxime Tremblay et al.": "We use the particles simulator of [11] to compute the position"
        },
        {
          "4": "",
          "Maxime Tremblay et al.": "and dynamics of all raindrops greater than 1 mm for a given"
        },
        {
          "4": "",
          "Maxime Tremblay et al.": "fall rate2. The simulator outputs the position and dynamics"
        },
        {
          "4": "",
          "Maxime Tremblay et al.": "(start and end points of streaks) of all visible rain drops in"
        },
        {
          "4": "(a) Drop FOV\n(b) Environment map estimation [10]",
          "Maxime Tremblay et al.": ""
        },
        {
          "4": "",
          "Maxime Tremblay et al.": "both world and image space, and accounts for intrinsic and"
        },
        {
          "4": "Fig. 3\nEstimation of raindrop photometry. To estimate the photo-",
          "Maxime Tremblay et al.": ""
        },
        {
          "4": "",
          "Maxime Tremblay et al.": "extrinsic camera calibration."
        },
        {
          "4": "metric radiance of a drop, we integrate the lighting environment map",
          "Maxime Tremblay et al.": ""
        },
        {
          "4": "over the 165◦ drop ﬁeld of view (a) relying on an estimate of the envi-",
          "Maxime Tremblay et al.": ""
        },
        {
          "4": "ronment map E shown in (b). The projected ﬁeld of view (F ) of the",
          "Maxime Tremblay et al.": "3.1.3 Rendering the appearance of a rain streak"
        },
        {
          "4": "drop is outlined in red.",
          "Maxime Tremblay et al.": ""
        },
        {
          "4": "",
          "Maxime Tremblay et al.": "While ray casting allows exact modeling of drops photometry,"
        },
        {
          "4": "",
          "Maxime Tremblay et al.": "this comes at very high processing cost and is virtually only"
        },
        {
          "4": "3.1 Physics-Based Rendering (PBR)",
          "Maxime Tremblay et al.": ""
        },
        {
          "4": "",
          "Maxime Tremblay et al.": "possible in synthetic scenes where the geometry and surface"
        },
        {
          "4": "",
          "Maxime Tremblay et al.": "materials are perfectly known [64,28]. What is more, drops"
        },
        {
          "4": "Taking inspiration from the vast\nliterature on rain physics,",
          "Maxime Tremblay et al.": ""
        },
        {
          "4": "",
          "Maxime Tremblay et al.": "oscillate as they fall, which creates further complications"
        },
        {
          "4": "[53, 7, 21,54] we simulate the rain appearance in an arbitrary",
          "Maxime Tremblay et al.": ""
        },
        {
          "4": "",
          "Maxime Tremblay et al.": "in modeling the light\ninteraction.\nInstead, we rely on the"
        },
        {
          "4": "image with the approach summarized in ﬁg. 2. Based on the",
          "Maxime Tremblay et al.": ""
        },
        {
          "4": "",
          "Maxime Tremblay et al.": "raindrop appearance database of Garg and Nayar [21], which"
        },
        {
          "4": "estimated scene depth, a fog-like attenuation layer is ﬁrst",
          "Maxime Tremblay et al.": ""
        },
        {
          "4": "",
          "Maxime Tremblay et al.": "contains the individual rain streaks radiance when imaged by"
        },
        {
          "4": "generated. Individual rain streaks are subsequently generated,",
          "Maxime Tremblay et al.": ""
        },
        {
          "4": "",
          "Maxime Tremblay et al.": "a stationary camera. For each drop, the streak database also"
        },
        {
          "4": "and composited with the fog-like layer. The ﬁnal result\nis",
          "Maxime Tremblay et al.": ""
        },
        {
          "4": "",
          "Maxime Tremblay et al.": "models 10 oscillations due to the airﬂow, which accounts for"
        },
        {
          "4": "blended in the original\nimage to create a realistic, physics-",
          "Maxime Tremblay et al.": ""
        },
        {
          "4": "",
          "Maxime Tremblay et al.": "much greater realism than Gaussian modeling [3]."
        },
        {
          "4": "based and controllable rainfall rate.",
          "Maxime Tremblay et al.": ""
        },
        {
          "4": "",
          "Maxime Tremblay et al.": "To render a raindrop, we ﬁrst select a rain streak S ∈"
        },
        {
          "4": "",
          "Maxime Tremblay et al.": "S from the streak database S of\n[20], which contains 20"
        },
        {
          "4": "3.1.1 Fog-like rain",
          "Maxime Tremblay et al.": "different streaks (each with 10 different oscillations) stored"
        },
        {
          "4": "",
          "Maxime Tremblay et al.": "in an image format. We select the streak that best matches"
        },
        {
          "4": "Following the deﬁnition of [21], fog-like rain is the set of",
          "Maxime Tremblay et al.": ""
        },
        {
          "4": "",
          "Maxime Tremblay et al.": "the ﬁnal drop dimensions (computed from the output of the"
        },
        {
          "4": "drops that are too far away and that project on an area smaller",
          "Maxime Tremblay et al.": ""
        },
        {
          "4": "",
          "Maxime Tremblay et al.": "physical simulator), and randomly select an oscillation."
        },
        {
          "4": "than 1 pixel.\nIn this case, a pixel may even be imaging a",
          "Maxime Tremblay et al.": ""
        },
        {
          "4": "",
          "Maxime Tremblay et al.": "The selected rain streak S is subsequently warped to"
        },
        {
          "4": "large number of drops, which causes optical attenuation [21].",
          "Maxime Tremblay et al.": ""
        },
        {
          "4": "",
          "Maxime Tremblay et al.": "match the drop dynamics from the physical simulator:"
        },
        {
          "4": "In practice, most drops in a rainfall are actually imaged as",
          "Maxime Tremblay et al.": ""
        },
        {
          "4": "fog-like rain1, though their visual effect is less dominant.",
          "Maxime Tremblay et al.": "S(cid:48) = H(S) ,\n(3)"
        },
        {
          "4": "We render the volumetric attenuation using the model",
          "Maxime Tremblay et al.": ""
        },
        {
          "4": "",
          "Maxime Tremblay et al.": "where H(·) is the homography computed from the start and"
        },
        {
          "4": "is ex-\ndescribed in [71] where the per-pixel attenuation Iatt",
          "Maxime Tremblay et al.": ""
        },
        {
          "4": "",
          "Maxime Tremblay et al.": "end points in image space given by the physical simulator"
        },
        {
          "4": "pressed as the sum of the extinction Lext caused by the vol-",
          "Maxime Tremblay et al.": ""
        },
        {
          "4": "",
          "Maxime Tremblay et al.": "and the corresponding points in the database streak image."
        },
        {
          "4": "ume of rain and the airlight scattering Ain that results of the",
          "Maxime Tremblay et al.": ""
        },
        {
          "4": "environmental lighting. Using equations from [71] to model",
          "Maxime Tremblay et al.": ""
        },
        {
          "4": "",
          "Maxime Tremblay et al.": "3.1.4 Computing the photometry of a rain streak"
        },
        {
          "4": "the attenuation image at pixel x we get",
          "Maxime Tremblay et al.": ""
        },
        {
          "4": "",
          "Maxime Tremblay et al.": "Computing the photometry of a rain streak from a single"
        },
        {
          "4": "(1)\nIatt(x) = ILext(x) + Ain(x) ,",
          "Maxime Tremblay et al.": ""
        },
        {
          "4": "",
          "Maxime Tremblay et al.": "image is impractical because drops have a much larger ﬁeld"
        },
        {
          "4": "",
          "Maxime Tremblay et al.": "of view than common cameras (165◦ vs approx. 70–100◦)."
        },
        {
          "4": "where",
          "Maxime Tremblay et al.": ""
        },
        {
          "4": "",
          "Maxime Tremblay et al.": "To render a drop accurately, we must therefore estimate the"
        },
        {
          "4": "Lext(x) = e−0.312R0.67d(x) ,",
          "Maxime Tremblay et al.": "environment map (spherical lighting representation) around"
        },
        {
          "4": "(2)",
          "Maxime Tremblay et al.": ""
        },
        {
          "4": "",
          "Maxime Tremblay et al.": "that drop. Sophisticated methods could be used [31, 30, 80]"
        },
        {
          "4": "Ain(x) = βHG(θ) ¯Esun(1 − Lext(x)) .",
          "Maxime Tremblay et al.": ""
        },
        {
          "4": "",
          "Maxime Tremblay et al.": "but we employ [10] which approximates the environment"
        },
        {
          "4": "Here, R denotes the rainfall rate R (in mm/hr), d(x) the pixel",
          "Maxime Tremblay et al.": "map through a series of simple operations on the image."
        },
        {
          "4": "depth, βHG the standard Heynyey-Greenstein coefﬁcient, and",
          "Maxime Tremblay et al.": "From each camera relative 3D drop position, we compute"
        },
        {
          "4": "¯",
          "Maxime Tremblay et al.": "the intersection F of the drop ﬁeld of view with the envi-"
        },
        {
          "4": "Esun the average sun irradiance which we estimate from the",
          "Maxime Tremblay et al.": ""
        },
        {
          "4": "image-radiance relation [32].",
          "Maxime Tremblay et al.": "ronment map E, assuming a 10m constant scene distance."
        },
        {
          "4": "",
          "Maxime Tremblay et al.": "The process is depicted in ﬁg. 3, and geometrical details are"
        },
        {
          "4": "1 Assuming a stationary camera with KITTI calibration [22], we",
          "Maxime Tremblay et al.": ""
        },
        {
          "4": "computed that only 1.24% of the drops project on 1+ pixel in 50 mm/hr",
          "Maxime Tremblay et al.": "2 The distribution and dynamics of drops vary on earth due to gravity"
        },
        {
          "4": "rain, and 0.7% at 5 mm/hr. This follows logic: the heavier the rain, the",
          "Maxime Tremblay et al.": "and atmospheric conditions. We selected here the broadly used physical"
        },
        {
          "4": "higher the probability of having large drops.",
          "Maxime Tremblay et al.": "models recorded in Ottawa, Canada [53,1]."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "5": "We then blend the rendered drop with the attenuated"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "Groundtruth",
          "5": "background image Iatt using the photometric blending model"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "5": "from [21]. Because the streak database and the image I are"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "5": "likely to be imaged with different exposures, we need to"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "5": "correct the exposure to match the imaging system used in I."
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "Ours",
          "5": "Suppose a pixel x of\nthe image I and x(cid:48)\nthe overlapping"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "5": "coordinates in streak S(cid:48), the composite is obtained with"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "5": "T − S(cid:48)\nτ1\nα(x(cid:48))τ1"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "5": ",\n(5)\nIrain(x) =\nIatt(x) + S(cid:48)(x(cid:48))"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "Groundtruth",
          "5": "T\nτ0"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "5": "where S(cid:48)\nis\nthe\nalpha\nchannel5\nof\nthe\nrendered streak,"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "5": "α\n√"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "5": "10−3/50 is the time for which the drop remained\nτ0 ="
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "5": "on one pixel in the streak database, and τ1 the same measure"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "Ours",
          "5": "according to our physical simulator. We refer to appendix B"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "5": "for details."
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "Environment map\nOur synthesized rain (50mm/hr)",
          "5": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "Fig. 4 Photometric validation of rain. Rain rendering using ground",
          "5": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "5": "3.1.6 Compositing rainfall on the image"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "truth illumination or our approximated environment map. From HDR",
          "5": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "panoramas [30], we ﬁrst extract limited ﬁeld of view crops to simulate",
          "5": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "5": "The rendering of\nrainfall of arbitrary rates in an image is"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "the point of view of a regular camera. Then, 50mm/hr rain is rendered",
          "5": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "using either (rows 1, 3) the ground truth HDR environment map or (rows",
          "5": "done in three main steps: 1) the fog-like attenuated image"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "2, 4) our environment estimation. The environment maps are shown as",
          "5": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "5": "is rendered (eq. 1), 2) the drops outputted by the physical\nIatt"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "reference on the left. While our approximated environment maps differ",
          "5": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "5": "simulator are rendered individually on the image (eq. 5),"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "from the ground truth, they are sufﬁcient to generate visually similar",
          "5": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "5": "3) the global luminosity average of the rainy image denoted"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "rain in images.",
          "5": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "5": "Irain is adjusted. While rainy events usually occur in cloudy"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "5": "weather which consequently decreases the scene radiance, a"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "provided in appendix A. Note that geometrically exact drop",
          "5": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "5": "typical camera imaging system adjusts its exposure to restore"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "ﬁeld of view estimation requires location-dependent environ-",
          "5": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "5": "the luminosity. Consequently, we adjust the global luminosity"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "ment maps, centered on each drop. However, we consider",
          "5": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "5": "factor so as to restore the mean radiance, and preserves the"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "the impact negligible since drops are relatively close to the",
          "5": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "5": "relation ¯I = ¯Irain, where the overbar denotes the intensity"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "camera center compared to the sphere radius used3.",
          "5": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "5": "average."
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "Since a drop refracts 94% of its ﬁeld of view radiance",
          "5": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "and reﬂects 6% of the entire environment map radiance [21],",
          "5": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "5": "Photometric validation. A limitation of our physical pipeline"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "we multiply the streak appearance with a per-channel weight:",
          "5": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "5": "is the lighting estimation which impacts the photometry of"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "5": "the rain. To measure its effect,\nin ﬁg. 4 we compare the"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "S(cid:48) = S(cid:48)(0.94 ¯F + 0.06 ¯E) ,\n(4)",
          "5": "same rain rendered with our estimated environment map or"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "5": "ground truth illumination obtained from high dynamic range"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "where ¯F is the mean of the intersection region F , and ¯E is",
          "5": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "5": "panoramas [30]. Overall, our estimation differs from ground"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "the mean of the environment map E.",
          "5": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "5": "truth when the scene is not radially symmetric but we observe"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "5": "that it produces visually similar rain in images."
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "3.1.5 Compositing a single rain streak on the image",
          "5": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "Now that the streak position and photometry were determined",
          "5": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "5": "3.2 Image-to-image translation (GAN)"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "from the physical simulator and the environment map respec-",
          "5": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "tively, we can composite it onto the original image. First, to",
          "5": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "5": "While our physic-based rendering generates realistic rain"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "account for the camera depth of ﬁeld, we apply a defocus",
          "5": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "5": "streaks and fog-like rain effect, it ignores major rainy charac-"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "effect following [58], convolving the streak image S(cid:48) with",
          "5": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "5": "teristics such as wetness, reﬂections, clouds and thus may fail"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "the circle of confusion4 C, that is: S(cid:48) = S(cid:48) ∗ C.",
          "5": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "5": "at conveying the overall\nlook of a rainy scene. Conversely,"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "3 We computed that, for KITTI, 98.7% of the drops are within 4 m",
          "5": "generative adversarial networks (GAN) excel at learning such"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "from the camera center in a 50 mm/hr rainfall rate. Therefore, computing",
          "5": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "5": "visual characteristics as they constitute strong signals for the"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "location-dependent environment maps would not be signiﬁcantly more",
          "5": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "5": "discriminator in the learning process."
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "accurate, while being of very high processing cost.",
          "5": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "4 The circle of confusion C of an object at distance d, is deﬁned as:",
          "5": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "5": "5 While [20] does not provide an alpha channel, the latter is easily"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "C = (d−fp)f 2\nwith fp the focus plane, f the focal and fN the lens",
          "5": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "d(fp−f )fN",
          "5": "computed since drops were rendered on black background in a white"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "f-number. f and fN are from intrinsic calibration, and fp is set to 6 m.",
          "5": "ambient lighting."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Rain photographs": ""
        },
        {
          "Rain photographs": ""
        },
        {
          "Rain photographs": ""
        },
        {
          "Rain photographs": ""
        },
        {
          "Rain photographs": ""
        },
        {
          "Rain photographs": ""
        },
        {
          "Rain photographs": ""
        },
        {
          "Rain photographs": ""
        },
        {
          "Rain photographs": ""
        },
        {
          "Rain photographs": ""
        },
        {
          "Rain photographs": "Our rain rendering"
        },
        {
          "Rain photographs": ""
        },
        {
          "Rain photographs": ""
        },
        {
          "Rain photographs": "Clear"
        },
        {
          "Rain photographs": ""
        },
        {
          "Rain photographs": ""
        },
        {
          "Rain photographs": ""
        },
        {
          "Rain photographs": "PBR\n100mm/hr"
        },
        {
          "Rain photographs": ""
        },
        {
          "Rain photographs": ""
        },
        {
          "Rain photographs": "PBR\n200mm/hr"
        },
        {
          "Rain photographs": ""
        },
        {
          "Rain photographs": ""
        },
        {
          "Rain photographs": ""
        },
        {
          "Rain photographs": ""
        },
        {
          "Rain photographs": "GAN"
        },
        {
          "Rain photographs": ""
        },
        {
          "Rain photographs": ""
        },
        {
          "Rain photographs": ""
        },
        {
          "Rain photographs": ""
        },
        {
          "Rain photographs": ""
        },
        {
          "Rain photographs": "GAN+PBR\n100mm/hr"
        },
        {
          "Rain photographs": ""
        },
        {
          "Rain photographs": ""
        },
        {
          "Rain photographs": ""
        },
        {
          "Rain photographs": ""
        },
        {
          "Rain photographs": "GAN+PBR\n200mm/hr"
        },
        {
          "Rain photographs": ""
        },
        {
          "Rain photographs": "Other physic-based rain rendering"
        },
        {
          "Rain photographs": ""
        },
        {
          "Rain photographs": ""
        },
        {
          "Rain photographs": ""
        },
        {
          "Rain photographs": "rain100H [74]\nrain800 [79]\ndid-MDN [78]"
        },
        {
          "Rain photographs": "Fig. 6 Comparison of real photographs and our renderings. Real"
        },
        {
          "Rain photographs": "photographs (source: web,\n[51],\n[9]) showing various rain intensity,"
        },
        {
          "Rain photographs": "sample output of our rain rendering (PBR, GAN, and GAN+PBR), and"
        },
        {
          "Rain photographs": "other recent rain rendering methods. Although rain appearance is highly"
        },
        {
          "Rain photographs": "camera-dependent [19], results show that both real photographs and"
        },
        {
          "Rain photographs": ""
        },
        {
          "Rain photographs": "our rain generation share volume attenuation and sparse visible streaks"
        },
        {
          "Rain photographs": ""
        },
        {
          "Rain photographs": "which correctly vary with the scene background. As opposed to the"
        },
        {
          "Rain photographs": "other rain rendering methods, our pipeline simulates physical rainfall"
        },
        {
          "Rain photographs": "(here, 100mm/hr and 200mm/hr) and valid particles photometry."
        },
        {
          "Rain photographs": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "rain photographs,\nit\nis noticeable that\nrainy scenes have",
          "7": "Strongly Disagree\nDisagree\nNeutral\nAgree\nStrongly agree"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "complex visual appearance, and that\nthe streaks visibility",
          "7": "400"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "is greatly affected by the imaging device and background.",
          "7": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "7": "300"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "Our PBR approach is able to reproduce the complex pat-",
          "7": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "tern of streaks with orientation consistent with camera motion",
          "7": "200"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "and photometry consistent with background and depth. As",
          "7": "# of ratings"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "in the real photographs, the streaks are sparse and only visi-",
          "7": "100"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "ble on darker backgrounds. The veiling effect caused by the",
          "7": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "rain volume (i.e.\nfog-like rain)\nis visible where the scene",
          "7": "0\nRain100H\nRain800\nDID-MDN\nOurs\nOurs\nOurs\nReal"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "7": "(PBR)\n(GAN)\n(GAN+PBR)"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "depth is larger (i.e. image center, sky) and nearby streaks are",
          "7": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "accurately defocused. Still,\nthe absence of visible wetness",
          "7": "Fig. 7 User study of rainy images realism. The y-axis displays rat-"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "arguably affects the rainy feeling.",
          "7": "ings to the statement \"Rain in this image looks realistic\". All of our"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "7": "approaches signiﬁcantly outperform existing techniques."
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "Conversely, the GAN believably renders the wetness ap-",
          "7": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "7": "Falling rain \nWetness \nSky cover \nGlobal illumination \nNone of the above"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "pearance of rainy scenes. While some reﬂections are geo-",
          "7": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "7": "100%"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "metrically incorrect (e.g., a pole is reﬂected in the middle of",
          "7": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "7": "75%"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "the street in the left column of ﬁg. 6 yet no pole is present),",
          "7": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "7": "50%"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "the overall appearance is visually pleasant and the global",
          "7": "% of ratings"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "illumination matches that of real photographs. A noticeable",
          "7": "25%"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "artifact caused by GAN is the blurry appearance of images,",
          "7": "0%"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "7": "Ours (PBR)\nOurs (GAN)\nOurs (GAN+PBR)\nReal"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "whereas real\nrain images are only blurred in the distance.",
          "7": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "7": "Fig. 8 User study on images characteristics conveying rain. The y-"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "This is explained by the inability of the GAN to disentangle",
          "7": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "7": "axis displays ratings to the statement \"Which of these qualities help in"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "the scene from the lens drops present in the “rainy” training",
          "7": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "7": "determining the realism of the rain\"."
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "images. This leads to blurring the whole image being an easy",
          "7": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "learning optimum, as highlighted in [56]. Another limitation",
          "7": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "we already mentioned is that GAN does not allow to control",
          "7": "for benchmarking or physical accuracy purposes, GAN+PBR"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "the amount of rain in the image.",
          "7": "allows us to have an arbitrary control on the rain amount at"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "This limitation is circumvented by our GAN+PBR ap-",
          "7": "the cost of slightly lower realism."
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "proach which renders controllable rain streaks while pre-",
          "7": "In the second study, we asked respondents who partic-"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "serving the global wetness appearance learned with image",
          "7": "ipated in the ﬁrst study to determine, for each of the same"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "translation. Despite the naive GAN and PBR compositing",
          "7": "images as before (excluding images from the previous work),"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "strategy, the drops naturally blend in the scene.",
          "7": "which visual characteristics inﬂuenced their decisions. 52 of"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "7": "the original participants responded, aged from 22 to 72 (avg"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "7": "36.6, std 13.9) including 28.9% females. Results are reported"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "7": "in ﬁg. 8. We note that while falling rain and wetness are the"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "4.2 User study",
          "7": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "7": "main characteristics in real rain, GAN—judged the most re-"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "7": "alistic approach—fails to convey the falling rain appearance"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "To evaluate the perceptual quality of our rain renderings, we",
          "7": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "7": "but excels at rendering wetness. The opposite is observed"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "conducted two user studies. In the ﬁrst, users were shown",
          "7": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "7": "with PBR,\nthough few users indicated wetness (despite its"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "one image at a time, and asked to rate if rain looks realistic",
          "7": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "7": "absence). The GAN+PBR offers a trade-off balancing all"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "on a 5-point Likert scale. A total of 42 images were shown,",
          "7": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "7": "characteristics."
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "that\nis 6 for each of the following: real rainy photographs,",
          "7": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "ours (PBR, GAN, GAN+PBR), and previous approaches [74,",
          "7": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "78, 79]. Answers were obtained for a total of 67 participants,",
          "7": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "aged from 22 to 75 (avg 37.0, std 14.2), with 32.8% females.",
          "7": "5 Evaluating the impact of real rain"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "From the Mean Opinion Score (MOS) in ﬁg. 7, all our",
          "7": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "rain augmentation approaches are judged to be more realistic",
          "7": "We now aim at quantifying the impact of rain on three com-"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "than any of the previous approaches. Speciﬁcally, when con-",
          "7": "puter vision tasks: object detection, semantic segmentation,"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "verting ratings to the [0, 1] interval,\nthe mean rain realism",
          "7": "and depth estimation. These tasks are critical for any outdoor"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "is 0.77 for real photos, 0.44 for PBR, 0.68 for GAN, 0.52",
          "7": "vision systems such as mobile robotics, autonomous driving,"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "for GAN+PBR, and 0.30/0.23/0.08 for [78]/[79]/[74] respec-",
          "7": "or surveillance. Before we experiment on rain-augmented"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "tively. Despite physical and geometrical inconsistencies, the",
          "7": "images with our PBR, GAN, and GAN+PBR approaches, we"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "users consistently judged GAN images to be more realistic.",
          "7": "ﬁrst experiment on real rainy photographs."
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "This is in favor of using image-to-image translation rather",
          "7": "For this, we use the nuScenes dataset [9]. Beneﬁting from"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "than physics-based rendering for realism purposes. However,",
          "7": "coarse frame-wise weather annotations in the dataset, we split"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Monodepth2\n[25]\nFig. 9 Qualitative results on real rainy images from the nuScenes dataset. Shown for different\ntasks: object detection (top line), semantic": "segmentation (middle line), and depth estimation (bottom line)"
        },
        {
          "Monodepth2\n[25]\nFig. 9 Qualitative results on real rainy images from the nuScenes dataset. Shown for different\ntasks: object detection (top line), semantic": "nuScenes images6 in two subsets: “nuScenes-clear” (images\nFor object detection and depth estimation, we ﬁrst pre-"
        },
        {
          "Monodepth2\n[25]\nFig. 9 Qualitative results on real rainy images from the nuScenes dataset. Shown for different\ntasks: object detection (top line), semantic": "without rain) and “nuScenes-rain” (rainy images). Due to the\ntrain each algorithm on ImageNet (Darknet53, 448 × 448)"
        },
        {
          "Monodepth2\n[25]\nFig. 9 Qualitative results on real rainy images from the nuScenes dataset. Shown for different\ntasks: object detection (top line), semantic": "noisy weather labels, we cross-validated each frame with a\nand KITTI (monocular, 1024 × 320) respectively, and further"
        },
        {
          "Monodepth2\n[25]\nFig. 9 Qualitative results on real rainy images from the nuScenes dataset. Shown for different\ntasks: object detection (top line), semantic": "historic weather database7 using GPS location and time, and\nﬁnetune them on the nuScenes-clear(train) subset to limit the"
        },
        {
          "Monodepth2\n[25]\nFig. 9 Qualitative results on real rainy images from the nuScenes dataset. Shown for different\ntasks: object detection (top line), semantic": "only kept frames where the nuScenes label agreed with the\ndomain gap between datasets. We then evaluate their perfor-"
        },
        {
          "Monodepth2\n[25]\nFig. 9 Qualitative results on real rainy images from the nuScenes dataset. Shown for different\ntasks: object detection (top line), semantic": "weather database. This resulted in sets of 24134 images for\nmance on the aforementioned test subsets. For segmentation,"
        },
        {
          "Monodepth2\n[25]\nFig. 9 Qualitative results on real rainy images from the nuScenes dataset. Shown for different\ntasks: object detection (top line), semantic": "nuScenes-clear, and 6028 for nuScenes-rain. In this dataset,\nsince semantic labels are not provided on nuScenes, we care-"
        },
        {
          "Monodepth2\n[25]\nFig. 9 Qualitative results on real rainy images from the nuScenes dataset. Shown for different\ntasks: object detection (top line), semantic": "rain images are dark, gloomy, the sky is heavily covered, and\nfully annotated 25 images from both nuScenes-clear(test)"
        },
        {
          "Monodepth2\n[25]\nFig. 9 Qualitative results on real rainy images from the nuScenes dataset. Shown for different\ntasks: object detection (top line), semantic": "no falling rain is visible but\nthere are unfocused raindrops\nand nuScenes-rain(test). Since we do not have enough la-"
        },
        {
          "Monodepth2\n[25]\nFig. 9 Qualitative results on real rainy images from the nuScenes dataset. Shown for different\ntasks: object detection (top line), semantic": "occlusions on the lens.\nbeled data for ﬁnetuning, we use our model pretrained on"
        },
        {
          "Monodepth2\n[25]\nFig. 9 Qualitative results on real rainy images from the nuScenes dataset. Shown for different\ntasks: object detection (top line), semantic": "Cityscapes [13], with the caveat that there may be a signiﬁca-"
        },
        {
          "Monodepth2\n[25]\nFig. 9 Qualitative results on real rainy images from the nuScenes dataset. Shown for different\ntasks: object detection (top line), semantic": "We experiment on these sets of\nreal\nimages with al-"
        },
        {
          "Monodepth2\n[25]\nFig. 9 Qualitative results on real rainy images from the nuScenes dataset. Shown for different\ntasks: object detection (top line), semantic": "tive domain gap between training and evaluation."
        },
        {
          "Monodepth2\n[25]\nFig. 9 Qualitative results on real rainy images from the nuScenes dataset. Shown for different\ntasks: object detection (top line), semantic": "gorithms from each task: YOLOv2 [59]\nfor object detec-"
        },
        {
          "Monodepth2\n[25]\nFig. 9 Qualitative results on real rainy images from the nuScenes dataset. Shown for different\ntasks: object detection (top line), semantic": "Table 1 reports the results of this experiment. The per-\ntion, PSPNet\n[82]\nfor\nsemantic segmentation, and Mon-"
        },
        {
          "Monodepth2\n[25]\nFig. 9 Qualitative results on real rainy images from the nuScenes dataset. Shown for different\ntasks: object detection (top line), semantic": "formance on real\nrainy images compared to clear\nimages\nodepth2 [25] for depth estimation. The nuScenes-clear set"
        },
        {
          "Monodepth2\n[25]\nFig. 9 Qualitative results on real rainy images from the nuScenes dataset. Shown for different\ntasks: object detection (top line), semantic": "for all three tasks are a mAP of 16.30% instead of 32.53%\nis split\ninto train/test subsets of 19685/4449 images from"
        },
        {
          "Monodepth2\n[25]\nFig. 9 Qualitative results on real rainy images from the nuScenes dataset. Shown for different\ntasks: object detection (top line), semantic": "for object detection, an AP of 18.7% instead of 40.8% for\n491/110 scenes. The nuScenes-rain is also split into train/test"
        },
        {
          "Monodepth2\n[25]\nFig. 9 Qualitative results on real rainy images from the nuScenes dataset. Shown for different\ntasks: object detection (top line), semantic": "semantic segmentation and a square relative error of 3.53%\nsubsets of 5419/609 images from 134/15 scenes. Here, 1000"
        },
        {
          "Monodepth2\n[25]\nFig. 9 Qualitative results on real rainy images from the nuScenes dataset. Shown for different\ntasks: object detection (top line), semantic": "instead of 2.96% for depth estimation. Corresponding qual-\nimages from the nuScenes-clear(test) and all images from the"
        },
        {
          "Monodepth2\n[25]\nFig. 9 Qualitative results on real rainy images from the nuScenes dataset. Shown for different\ntasks: object detection (top line), semantic": "itative results are displayed in ﬁg. 9. As expected, real rain\nnuScenes-rain(test) subset are used for evaluation (train will"
        },
        {
          "Monodepth2\n[25]\nFig. 9 Qualitative results on real rainy images from the nuScenes dataset. Shown for different\ntasks: object detection (top line), semantic": "deteriorates the performance of all algorithms on all\ntasks.\nbe used for the GAN in sec. 6.1). These training and testing"
        },
        {
          "Monodepth2\n[25]\nFig. 9 Qualitative results on real rainy images from the nuScenes dataset. Shown for different\ntasks: object detection (top line), semantic": "However, we cannot evaluate how rain intensity affects these\nsets and subsets are all displayed in table 3 in appendix C."
        },
        {
          "Monodepth2\n[25]\nFig. 9 Qualitative results on real rainy images from the nuScenes dataset. Shown for different\ntasks: object detection (top line), semantic": "algorithms since it would require the accurate measurement\nNote that a large number of images were needed to train the"
        },
        {
          "Monodepth2\n[25]\nFig. 9 Qualitative results on real rainy images from the nuScenes dataset. Shown for different\ntasks: object detection (top line), semantic": "of the rainfall rate at the time of capture.\nCycleGAN for image translation and, to avoid any overlap"
        },
        {
          "Monodepth2\n[25]\nFig. 9 Qualitative results on real rainy images from the nuScenes dataset. Shown for different\ntasks: object detection (top line), semantic": "in image sequences,\nit unfortunately left a relatively small"
        },
        {
          "Monodepth2\n[25]\nFig. 9 Qualitative results on real rainy images from the nuScenes dataset. Shown for different\ntasks: object detection (top line), semantic": "subset of nuScenes for the evaluation on real rainy images."
        },
        {
          "Monodepth2\n[25]\nFig. 9 Qualitative results on real rainy images from the nuScenes dataset. Shown for different\ntasks: object detection (top line), semantic": "6 Evaluating the impact of synthetic rain"
        },
        {
          "Monodepth2\n[25]\nFig. 9 Qualitative results on real rainy images from the nuScenes dataset. Shown for different\ntasks: object detection (top line), semantic": "6 Note that only front camera and annotated key frames are used for"
        },
        {
          "Monodepth2\n[25]\nFig. 9 Qualitative results on real rainy images from the nuScenes dataset. Shown for different\ntasks: object detection (top line), semantic": "To study how vision algorithms perform under increasing\nconsistency and ground truth accuracy."
        },
        {
          "Monodepth2\n[25]\nFig. 9 Qualitative results on real rainy images from the nuScenes dataset. Shown for different\ntasks: object detection (top line), semantic": "7 Weather database at https://openweathermap.org.\namounts of rain, we leverage our rain synthesis pipeline and"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "=\nfall\nrates\nranging\nfrom light\nto\nheavy\nstorm R": ""
        },
        {
          "=\nfall\nrates\nranging\nfrom light\nto\nheavy\nstorm R": "{0, 5, 25, 50, 100, 200} mm/hr. Only\nthe\nnuScenes-clear"
        },
        {
          "=\nfall\nrates\nranging\nfrom light\nto\nheavy\nstorm R": "dataset is augmented with GAN and GAN+PBR, since nei-"
        },
        {
          "=\nfall\nrates\nranging\nfrom light\nto\nheavy\nstorm R": "ther KITTI nor Cityscapes contain rainy images to train the"
        },
        {
          "=\nfall\nrates\nranging\nfrom light\nto\nheavy\nstorm R": ""
        },
        {
          "=\nfall\nrates\nranging\nfrom light\nto\nheavy\nstorm R": "GAN. Our PBR and GAN+PBR rain augmentation require"
        },
        {
          "=\nfall\nrates\nranging\nfrom light\nto\nheavy\nstorm R": ""
        },
        {
          "=\nfall\nrates\nranging\nfrom light\nto\nheavy\nstorm R": "some preparation as they rely on calibration, depth and cam-"
        },
        {
          "=\nfall\nrates\nranging\nfrom light\nto\nheavy\nstorm R": "era motion. Our GAN and GAN+PBR rain augmentation"
        },
        {
          "=\nfall\nrates\nranging\nfrom light\nto\nheavy\nstorm R": ""
        },
        {
          "=\nfall\nrates\nranging\nfrom light\nto\nheavy\nstorm R": "require the training of a CycleGAN. These preparations are"
        },
        {
          "=\nfall\nrates\nranging\nfrom light\nto\nheavy\nstorm R": ""
        },
        {
          "=\nfall\nrates\nranging\nfrom light\nto\nheavy\nstorm R": "described below."
        },
        {
          "=\nfall\nrates\nranging\nfrom light\nto\nheavy\nstorm R": ""
        },
        {
          "=\nfall\nrates\nranging\nfrom light\nto\nheavy\nstorm R": ""
        },
        {
          "=\nfall\nrates\nranging\nfrom light\nto\nheavy\nstorm R": ""
        },
        {
          "=\nfall\nrates\nranging\nfrom light\nto\nheavy\nstorm R": "Calibration. For the realistic physical simulator (sec. 3.1.2)"
        },
        {
          "=\nfall\nrates\nranging\nfrom light\nto\nheavy\nstorm R": ""
        },
        {
          "=\nfall\nrates\nranging\nfrom light\nto\nheavy\nstorm R": "and the rain streaks photometric simulation (sec. 3.1.4), intrin-"
        },
        {
          "=\nfall\nrates\nranging\nfrom light\nto\nheavy\nstorm R": ""
        },
        {
          "=\nfall\nrates\nranging\nfrom light\nto\nheavy\nstorm R": "sic and extrinsic calibration are used to replicate the imaging"
        },
        {
          "=\nfall\nrates\nranging\nfrom light\nto\nheavy\nstorm R": "sensor. We used frame-wise or sequence-wise calibration for"
        },
        {
          "=\nfall\nrates\nranging\nfrom light\nto\nheavy\nstorm R": ""
        },
        {
          "=\nfall\nrates\nranging\nfrom light\nto\nheavy\nstorm R": "KITTI and nuScenes. In addition, we use 6mm focal and 2ms"
        },
        {
          "=\nfall\nrates\nranging\nfrom light\nto\nheavy\nstorm R": ""
        },
        {
          "=\nfall\nrates\nranging\nfrom light\nto\nheavy\nstorm R": "exposure for KITTI [23, 22] and assumed 5ms exposure for"
        },
        {
          "=\nfall\nrates\nranging\nfrom light\nto\nheavy\nstorm R": ""
        },
        {
          "=\nfall\nrates\nranging\nfrom light\nto\nheavy\nstorm R": "nuScenes. As Cityscapes does not provide calibration, we use"
        },
        {
          "=\nfall\nrates\nranging\nfrom light\nto\nheavy\nstorm R": ""
        },
        {
          "=\nfall\nrates\nranging\nfrom light\nto\nheavy\nstorm R": "intrinsic from the camera manufacturer with 5ms exposure"
        },
        {
          "=\nfall\nrates\nranging\nfrom light\nto\nheavy\nstorm R": ""
        },
        {
          "=\nfall\nrates\nranging\nfrom light\nto\nheavy\nstorm R": "and extrinsic is assumed similar to KITTI."
        },
        {
          "=\nfall\nrates\nranging\nfrom light\nto\nheavy\nstorm R": ""
        },
        {
          "=\nfall\nrates\nranging\nfrom light\nto\nheavy\nstorm R": ""
        },
        {
          "=\nfall\nrates\nranging\nfrom light\nto\nheavy\nstorm R": "Depth. The scene geometry (pixel depth) is also required"
        },
        {
          "=\nfall\nrates\nranging\nfrom light\nto\nheavy\nstorm R": ""
        },
        {
          "=\nfall\nrates\nranging\nfrom light\nto\nheavy\nstorm R": "to model accurately the light-particle interaction and the"
        },
        {
          "=\nfall\nrates\nranging\nfrom light\nto\nheavy\nstorm R": ""
        },
        {
          "=\nfall\nrates\nranging\nfrom light\nto\nheavy\nstorm R": "fog optical extinction. We estimate KITTI depth maps from"
        },
        {
          "=\nfall\nrates\nranging\nfrom light\nto\nheavy\nstorm R": "RGB+Lidar with [36], and Cityscapes/nuScenes from monoc-"
        },
        {
          "=\nfall\nrates\nranging\nfrom light\nto\nheavy\nstorm R": "ular RGB with Monodepth [24, 25]. While absolute depth"
        },
        {
          "=\nfall\nrates\nranging\nfrom light\nto\nheavy\nstorm R": ""
        },
        {
          "=\nfall\nrates\nranging\nfrom light\nto\nheavy\nstorm R": "is not required, we aim to avoid the critical artifacts along"
        },
        {
          "=\nfall\nrates\nranging\nfrom light\nto\nheavy\nstorm R": "edges, and thus further align RGB with depth using guided"
        },
        {
          "=\nfall\nrates\nranging\nfrom light\nto\nheavy\nstorm R": ""
        },
        {
          "=\nfall\nrates\nranging\nfrom light\nto\nheavy\nstorm R": "ﬁlter [4]."
        },
        {
          "=\nfall\nrates\nranging\nfrom light\nto\nheavy\nstorm R": ""
        },
        {
          "=\nfall\nrates\nranging\nfrom light\nto\nheavy\nstorm R": ""
        },
        {
          "=\nfall\nrates\nranging\nfrom light\nto\nheavy\nstorm R": "Camera motion. We mimic the camera ego motion in the"
        },
        {
          "=\nfall\nrates\nranging\nfrom light\nto\nheavy\nstorm R": "physical\nsimulator\nto ensure realistic rain streak orienta-"
        },
        {
          "=\nfall\nrates\nranging\nfrom light\nto\nheavy\nstorm R": "tion on still\nimages and preserve temporal consistency in"
        },
        {
          "=\nfall\nrates\nranging\nfrom light\nto\nheavy\nstorm R": "sequences. Ego speed is extracted from GPS data when pro-"
        },
        {
          "=\nfall\nrates\nranging\nfrom light\nto\nheavy\nstorm R": "vided (KITTI and nuScenes), or drawn uniformly in the"
        },
        {
          "=\nfall\nrates\nranging\nfrom light\nto\nheavy\nstorm R": "[0, 50] km/hr\ninterval\nfor Cityscapes semantics and in the"
        },
        {
          "=\nfall\nrates\nranging\nfrom light\nto\nheavy\nstorm R": "[0, 100] km/hr interval for KITTI object to reﬂect the urban"
        },
        {
          "=\nfall\nrates\nranging\nfrom light\nto\nheavy\nstorm R": "and semi-urban scenarios, respectively."
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 11 Object detection on PBR rain augmentation of KITTI. From left to right, the original image (clear) and three PBR augmentations with": "varying rainfall rates. Images are cropped for visualization."
        },
        {
          "Fig. 11 Object detection on PBR rain augmentation of KITTI. From left to right, the original image (clear) and three PBR augmentations with": "R-FCN [15], SSD [49], MX-RCNN [73], and YOLOv2 [59]."
        },
        {
          "Fig. 11 Object detection on PBR rain augmentation of KITTI. From left to right, the original image (clear) and three PBR augmentations with": "Quantitative results\nfor\nthe Coco mAP@[.1:.1:.9] metric"
        },
        {
          "Fig. 11 Object detection on PBR rain augmentation of KITTI. From left to right, the original image (clear) and three PBR augmentations with": "across classes are shown in ﬁg. 10a. Relative to their clear-"
        },
        {
          "Fig. 11 Object detection on PBR rain augmentation of KITTI. From left to right, the original image (clear) and three PBR augmentations with": "weather performance the 200 mm/hr rain is always at least"
        },
        {
          "Fig. 11 Object detection on PBR rain augmentation of KITTI. From left to right, the original image (clear) and three PBR augmentations with": "12% worse and even drops to 25-30% for R-FCN, SSD, and"
        },
        {
          "Fig. 11 Object detection on PBR rain augmentation of KITTI. From left to right, the original image (clear) and three PBR augmentations with": "MX-RCNN, whereas Faster R-CNN and DSOD are the most"
        },
        {
          "Fig. 11 Object detection on PBR rain augmentation of KITTI. From left to right, the original image (clear) and three PBR augmentations with": "robust to changes in fog and rain."
        },
        {
          "Fig. 11 Object detection on PBR rain augmentation of KITTI. From left to right, the original image (clear) and three PBR augmentations with": "Representative qualitative results on PBR images are"
        },
        {
          "Fig. 11 Object detection on PBR rain augmentation of KITTI. From left to right, the original image (clear) and three PBR augmentations with": "shown in ﬁg. 11 for 4 out of 6 algorithms to preserve space."
        },
        {
          "Fig. 11 Object detection on PBR rain augmentation of KITTI. From left to right, the original image (clear) and three PBR augmentations with": "All algorithms are strongly affected by the rain;\nit has a"
        },
        {
          "Fig. 11 Object detection on PBR rain augmentation of KITTI. From left to right, the original image (clear) and three PBR augmentations with": ""
        },
        {
          "Fig. 11 Object detection on PBR rain augmentation of KITTI. From left to right, the original image (clear) and three PBR augmentations with": "chaotic effect on object detection results because there can"
        },
        {
          "Fig. 11 Object detection on PBR rain augmentation of KITTI. From left to right, the original image (clear) and three PBR augmentations with": ""
        },
        {
          "Fig. 11 Object detection on PBR rain augmentation of KITTI. From left to right, the original image (clear) and three PBR augmentations with": "be large variance of occlusion level for objects populating"
        },
        {
          "Fig. 11 Object detection on PBR rain augmentation of KITTI. From left to right, the original image (clear) and three PBR augmentations with": ""
        },
        {
          "Fig. 11 Object detection on PBR rain augmentation of KITTI. From left to right, the original image (clear) and three PBR augmentations with": "the image. Also, as in real-life, far away objects (which are"
        },
        {
          "Fig. 11 Object detection on PBR rain augmentation of KITTI. From left to right, the original image (clear) and three PBR augmentations with": ""
        },
        {
          "Fig. 11 Object detection on PBR rain augmentation of KITTI. From left to right, the original image (clear) and three PBR augmentations with": "generally small objects) are more likely to disappear behind"
        },
        {
          "Fig. 11 Object detection on PBR rain augmentation of KITTI. From left to right, the original image (clear) and three PBR augmentations with": ""
        },
        {
          "Fig. 11 Object detection on PBR rain augmentation of KITTI. From left to right, the original image (clear) and three PBR augmentations with": "fog-like rain."
        },
        {
          "Fig. 11 Object detection on PBR rain augmentation of KITTI. From left to right, the original image (clear) and three PBR augmentations with": ""
        },
        {
          "Fig. 11 Object detection on PBR rain augmentation of KITTI. From left to right, the original image (clear) and three PBR augmentations with": ""
        },
        {
          "Fig. 11 Object detection on PBR rain augmentation of KITTI. From left to right, the original image (clear) and three PBR augmentations with": "Semantic segmentation. For semantic segmentation, the PBR"
        },
        {
          "Fig. 11 Object detection on PBR rain augmentation of KITTI. From left to right, the original image (clear) and three PBR augmentations with": ""
        },
        {
          "Fig. 11 Object detection on PBR rain augmentation of KITTI. From left to right, the original image (clear) and three PBR augmentations with": "augmented Cityscapes is evaluated for: AdaptSegNet [70],"
        },
        {
          "Fig. 11 Object detection on PBR rain augmentation of KITTI. From left to right, the original image (clear) and three PBR augmentations with": ""
        },
        {
          "Fig. 11 Object detection on PBR rain augmentation of KITTI. From left to right, the original image (clear) and three PBR augmentations with": "ERFNet [61], ESPNet [65], ICNet [81], PSPNet [82] and"
        },
        {
          "Fig. 11 Object detection on PBR rain augmentation of KITTI. From left to right, the original image (clear) and three PBR augmentations with": "PSPNet(50) [82]. Quantitative results are reported in ﬁg. 10b."
        },
        {
          "Fig. 11 Object detection on PBR rain augmentation of KITTI. From left to right, the original image (clear) and three PBR augmentations with": "As opposed to object detection algorithms which demon-"
        },
        {
          "Fig. 11 Object detection on PBR rain augmentation of KITTI. From left to right, the original image (clear) and three PBR augmentations with": "strated signiﬁcant robustness to moderately high rainfall rates,"
        },
        {
          "Fig. 11 Object detection on PBR rain augmentation of KITTI. From left to right, the original image (clear) and three PBR augmentations with": "here the algorithms seem to breakdown in similar conditions."
        },
        {
          "Fig. 11 Object detection on PBR rain augmentation of KITTI. From left to right, the original image (clear) and three PBR augmentations with": "Indeed, all techniques see their performance drop by a mini-"
        },
        {
          "Fig. 11 Object detection on PBR rain augmentation of KITTI. From left to right, the original image (clear) and three PBR augmentations with": "mum of 30% under heavy fog, and almost 60% under strong"
        },
        {
          "Fig. 11 Object detection on PBR rain augmentation of KITTI. From left to right, the original image (clear) and three PBR augmentations with": "rain. Interestingly, some curves cross, which indicates that"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "ERFNet\nInput\n[61]\nOriginal",
          "11": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "ESPNet\nICNet\nPSPNet\n[65]\n[81]\n[82]\nClear weather\nModerate rain",
          "11": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "(50 mm/hr)",
          "11": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "11": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "(clear) and three PBR augmentations with varying rainfall rates.",
          "11": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "Input\n[25]\nOriginal",
          "11": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "Monodepth2\nBTS\n[43]\nClear weather\nModerate rain",
          "11": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "(50 mm/hr)",
          "11": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "11": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "with varying rainfall rates.",
          "11": ""
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "12": "",
          "Maxime Tremblay et al.": "7.1 Training methodology"
        },
        {
          "12": "Light\nMedium\nAutumn\nRaining Cats\nLight\nMedium\nAutumn\nRaining Cats",
          "Maxime Tremblay et al.": ""
        },
        {
          "12": "Rain\nRain\nShower\nand Dogs\nRain\nRain\nShower\nand Dogs",
          "Maxime Tremblay et al.": ""
        },
        {
          "12": "35\n12.5",
          "Maxime Tremblay et al.": ""
        },
        {
          "12": "30",
          "Maxime Tremblay et al.": "While the ultimate goal is to improve robustness to rain, we"
        },
        {
          "12": "10.0",
          "Maxime Tremblay et al.": ""
        },
        {
          "12": "25",
          "Maxime Tremblay et al.": ""
        },
        {
          "12": "",
          "Maxime Tremblay et al.": "aim at training a single model which performs well across"
        },
        {
          "12": "7.5\n20",
          "Maxime Tremblay et al.": ""
        },
        {
          "12": "mAP (%)\nSq Rel (%)",
          "Maxime Tremblay et al.": "a wide variety of\nrainfall\nrates (including clear weather)."
        },
        {
          "12": "15",
          "Maxime Tremblay et al.": ""
        },
        {
          "12": "5.0",
          "Maxime Tremblay et al.": ""
        },
        {
          "12": "PBR\n10",
          "Maxime Tremblay et al.": "Having a single model\nis beneﬁcial over employing, e.g.,"
        },
        {
          "12": "GAN+PBR",
          "Maxime Tremblay et al.": ""
        },
        {
          "12": "2.5",
          "Maxime Tremblay et al.": ""
        },
        {
          "12": "05\nClear\nGAN",
          "Maxime Tremblay et al.": "intensity-speciﬁc encoders [57], since it removes the need for"
        },
        {
          "12": "0.0",
          "Maxime Tremblay et al.": ""
        },
        {
          "12": "0\n50\n100\n150\n200\n0\n50\n100\n150\n200",
          "Maxime Tremblay et al.": "determining rain intensity from the input image. Because rain"
        },
        {
          "12": "Rain intensity (mm/hr)\nRain intensity (mm/hr)",
          "Maxime Tremblay et al.": ""
        },
        {
          "12": "",
          "Maxime Tremblay et al.": "signiﬁcantly alters the appearance of the scene, we found that"
        },
        {
          "12": "(a) Object detection [59]\n(b) Depth estimation [25]",
          "Maxime Tremblay et al.": ""
        },
        {
          "12": "",
          "Maxime Tremblay et al.": "training from scratch with heavy rain or random rainfall rates"
        },
        {
          "12": "Fig. 14 Performance with varying rain intensities and augmenta-",
          "Maxime Tremblay et al.": ""
        },
        {
          "12": "",
          "Maxime Tremblay et al.": "fails to converge. Instead, we reﬁne our untuned models using"
        },
        {
          "12": "tion techniques. Opposed to PBR, GAN does not allow to control",
          "Maxime Tremblay et al.": ""
        },
        {
          "12": "the rain intensity and is reported as dashed line, as for clear perfor-",
          "Maxime Tremblay et al.": "curriculum learning [5] on rain intensity in ascending order"
        },
        {
          "12": "mance. Increasing rain intensity translates as a performance drop for",
          "Maxime Tremblay et al.": "(25, then 50, and ﬁnally 100mm/hr rain). The ﬁnal model is"
        },
        {
          "12": "(a) object detection with YOLOv2 [59] and (b) depth estimation with",
          "Maxime Tremblay et al.": ""
        },
        {
          "12": "",
          "Maxime Tremblay et al.": "referred as ﬁnetuned and is evaluated against various weather"
        },
        {
          "12": "Monodepth2 [25].",
          "Maxime Tremblay et al.": ""
        },
        {
          "12": "",
          "Maxime Tremblay et al.": "conditions. Note that, for hybrid augmentation ﬁnetuning,"
        },
        {
          "12": "",
          "Maxime Tremblay et al.": "the curriculum starts with the reﬁnement on GAN images"
        },
        {
          "12": "",
          "Maxime Tremblay et al.": "ﬁrst and then go through the ascending rain intensities. The"
        },
        {
          "12": "uated due to the lack of semantic labels for\ntraining. The",
          "Maxime Tremblay et al.": ""
        },
        {
          "12": "",
          "Maxime Tremblay et al.": "same images are used for all steps of the curriculum."
        },
        {
          "12": "evaluation is performed on the nuScenes-augment subset.",
          "Maxime Tremblay et al.": ""
        },
        {
          "12": "",
          "Maxime Tremblay et al.": "In order\nto avoid training and testing on the same set"
        },
        {
          "12": "",
          "Maxime Tremblay et al.": "of images,\nin this section, we further divide the nuScenes-"
        },
        {
          "12": "Object detection. YOLOv2 [59]\nis evaluated, and the re-",
          "Maxime Tremblay et al.": ""
        },
        {
          "12": "",
          "Maxime Tremblay et al.": "augment into train/test subsets of 1000 images each (ensuring"
        },
        {
          "12": "sulting mAP as a function of rainfall rates are reported in",
          "Maxime Tremblay et al.": ""
        },
        {
          "12": "",
          "Maxime Tremblay et al.": "they are taken from different scenes). Each algorithm is thus"
        },
        {
          "12": "ﬁg. 14a. Qualitative results are shown in ﬁg. 15. We note",
          "Maxime Tremblay et al.": ""
        },
        {
          "12": "",
          "Maxime Tremblay et al.": "reﬁned on the 1000 images from nuScenes-augment(train),"
        },
        {
          "12": "that GAN augmented images have similar performance than",
          "Maxime Tremblay et al.": ""
        },
        {
          "12": "",
          "Maxime Tremblay et al.": "and undergo a speciﬁc training process. For object detection,"
        },
        {
          "12": "PBR 100mm/hr images and that performance deterioration is",
          "Maxime Tremblay et al.": ""
        },
        {
          "12": "",
          "Maxime Tremblay et al.": "YOLOv2 [59] is trained each step at a learning rate of 0.0001"
        },
        {
          "12": "stronger and steeper with GAN+PBR images.",
          "Maxime Tremblay et al.": ""
        },
        {
          "12": "",
          "Maxime Tremblay et al.": "and a momentum of 0.9 for 10 epochs with a burn-in of 5"
        },
        {
          "12": "Observe that the particles physical simulation is the same",
          "Maxime Tremblay et al.": ""
        },
        {
          "12": "",
          "Maxime Tremblay et al.": "epochs. For semantic segmentation, PSPNet [82] is trained"
        },
        {
          "12": "in both PBR and GAN+PBR. Still, it is interesting to notice",
          "Maxime Tremblay et al.": ""
        },
        {
          "12": "",
          "Maxime Tremblay et al.": "with a learning rate of 0.0004 and a momentum of 0.9 for"
        },
        {
          "12": "that the decrease is different with GAN+PBR compared to",
          "Maxime Tremblay et al.": ""
        },
        {
          "12": "",
          "Maxime Tremblay et al.": "10 epochs. Finally, for depth estimation, Monodepth2 [25]"
        },
        {
          "12": "PBR (i.e. curves are shifted but also exhibit different slopes).",
          "Maxime Tremblay et al.": ""
        },
        {
          "12": "",
          "Maxime Tremblay et al.": "is trained on triplets of consecutive images using a learning"
        },
        {
          "12": "This may be the result of the two cumulative domain shifts",
          "Maxime Tremblay et al.": ""
        },
        {
          "12": "",
          "Maxime Tremblay et al.": "rate of 0.00001 with the Adam optimizer for 10 epochs with"
        },
        {
          "12": "(i.e. wetness + streaks) leading to a non-linear effect.",
          "Maxime Tremblay et al.": ""
        },
        {
          "12": "",
          "Maxime Tremblay et al.": "β = {0.5, 0.999}."
        },
        {
          "12": "Depth estimation. We evaluate the performance of the re-",
          "Maxime Tremblay et al.": ""
        },
        {
          "12": "cent Monodepth2 [25] on the nuScenes-augment subset aug-",
          "Maxime Tremblay et al.": ""
        },
        {
          "12": "mented with our GAN and GAN+PBR methods. As reported",
          "Maxime Tremblay et al.": "7.2 Improvement on synthetic rain"
        },
        {
          "12": "in ﬁg. 14b,\nfor\nthe same rain intensity between PBR and",
          "Maxime Tremblay et al.": ""
        },
        {
          "12": "",
          "Maxime Tremblay et al.": "The synthetic evaluation is conducted on the set of 1000 im-"
        },
        {
          "12": "GAN+PBR images,\nthe error\nis worse by a factor of 80–",
          "Maxime Tremblay et al.": ""
        },
        {
          "12": "",
          "Maxime Tremblay et al.": "ages from nuScenes-augment(test), with rain up to 200mm/hr."
        },
        {
          "12": "100%. The same behavior was observed on other standard",
          "Maxime Tremblay et al.": ""
        },
        {
          "12": "",
          "Maxime Tremblay et al.": "Note again, for our hybrid GAN+PBR, 0mm/hr of rain corre-"
        },
        {
          "12": "depth estimation metrics (absolute square error, RMSE, log",
          "Maxime Tremblay et al.": ""
        },
        {
          "12": "",
          "Maxime Tremblay et al.": "spond to the GAN-only augmented results."
        },
        {
          "12": "RMSE), not reported here. Interestingly, the GAN augmenta-",
          "Maxime Tremblay et al.": ""
        },
        {
          "12": "tion affects only slightly the performance on depth estimation",
          "Maxime Tremblay et al.": "Fig. 17 shows the performance of our untuned and ﬁne-"
        },
        {
          "12": "which might be because GAN translation keeps occlusion",
          "Maxime Tremblay et al.": "tuned model\nfor\nthe three vision tasks on different aug-"
        },
        {
          "12": "of the image to a minimum. Qualitative results are shown in",
          "Maxime Tremblay et al.": "mented dataset. Fig. 17a and ﬁg. 17b are for object detection"
        },
        {
          "12": "ﬁg. 16 for GAN and GAN+PBR.",
          "Maxime Tremblay et al.": "(YOLOv2 [59]) and depth estimation (Monodepth2 [25])"
        },
        {
          "12": "",
          "Maxime Tremblay et al.": "on nuScenes-clear augmented data while ﬁg. 17c is for the"
        },
        {
          "12": "",
          "Maxime Tremblay et al.": "semantic segmentation (PSPNet [82]) on Cityscapes. We ob-"
        },
        {
          "12": "7 Improving the robustness to rain",
          "Maxime Tremblay et al.": "serve a signiﬁcant improvement in both tasks and additional"
        },
        {
          "12": "",
          "Maxime Tremblay et al.": "increase in robustness even in clear weather when reﬁned"
        },
        {
          "12": "We now wish to demonstrate the usefulness of our rain ren-",
          "Maxime Tremblay et al.": "using our augmented rain. Of interest, we also improve at the"
        },
        {
          "12": "dering pipeline for\nimproving robustness to rain through",
          "Maxime Tremblay et al.": "unseen 200mm/hr rain though the network was only trained"
        },
        {
          "12": "extensive evaluations on synthetic and real rain databases.",
          "Maxime Tremblay et al.": "with rain up to 100mm/hr. The intuition here is that when fac-"
        },
        {
          "12": "For the sake of coherence, the improvements are shown on",
          "Maxime Tremblay et al.": "ing adverse weather, the network learns to focus on strongest"
        },
        {
          "12": "the same tasks, algorithms, and test data from sec. 5.",
          "Maxime Tremblay et al.": "relevant features for all tasks and thus gain robustness."
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "YOLOv2\nInput\n[59]\nOriginal",
          "13": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "YOLOv2\nInput\n[59]\nClear weather\nGAN only",
          "13": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "13": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "13": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "three GAN+PBR images.",
          "13": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "Monodepth2\nInput\n[25]\nOriginal",
          "13": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "Monodepth2\nInput\n[25]\nClear weather\nGAN only",
          "13": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "13": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "13": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "and three GAN+PBR images.",
          "13": ""
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "14": "Light",
          "Maxime Tremblay et al.": "Depth est. [25]"
        },
        {
          "14": "",
          "Maxime Tremblay et al.": ""
        },
        {
          "14": "Rain",
          "Maxime Tremblay et al.": ""
        },
        {
          "14": "",
          "Maxime Tremblay et al.": "Sq. err. (%) ↓"
        },
        {
          "14": "40",
          "Maxime Tremblay et al.": ""
        },
        {
          "14": "35",
          "Maxime Tremblay et al.": ""
        },
        {
          "14": "",
          "Maxime Tremblay et al.": "Rain\nClear\nRain"
        },
        {
          "14": "30",
          "Maxime Tremblay et al.": ""
        },
        {
          "14": "25",
          "Maxime Tremblay et al.": ""
        },
        {
          "14": "",
          "Maxime Tremblay et al.": "18.7\n2.96\n3.53"
        },
        {
          "14": "mAP (%)\n20",
          "Maxime Tremblay et al.": ""
        },
        {
          "14": "",
          "Maxime Tremblay et al.": "25.6\n3.15\n3.54"
        },
        {
          "14": "15",
          "Maxime Tremblay et al.": ""
        },
        {
          "14": "",
          "Maxime Tremblay et al.": "2.89\n*\n3.40"
        },
        {
          "14": "10",
          "Maxime Tremblay et al.": ""
        },
        {
          "14": "",
          "Maxime Tremblay et al.": "3.29\n*\n3.01"
        },
        {
          "14": "05",
          "Maxime Tremblay et al.": ""
        },
        {
          "14": "",
          "Maxime Tremblay et al.": ""
        },
        {
          "14": "0",
          "Maxime Tremblay et al.": "*\n2.25\n3.09"
        },
        {
          "14": "",
          "Maxime Tremblay et al.": ""
        },
        {
          "14": "",
          "Maxime Tremblay et al.": ""
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "GAN+PBR (untuned)": "05\nGAN+PBR (finetuned)",
          "19.73\n3.29\nFinetuned (GAN+PBR)\n30.59\n*\n*\n3.01": ""
        },
        {
          "GAN+PBR (untuned)": "0.0",
          "19.73\n3.29\nFinetuned (GAN+PBR)\n30.59\n*\n*\n3.01": ""
        },
        {
          "GAN+PBR (untuned)": "0\n50\n100\n150\n200\n0\n50\n100\n150\n200",
          "19.73\n3.29\nFinetuned (GAN+PBR)\n30.59\n*\n*\n3.01": "De-rained DualResNet\n32.60\n18.30\n*\n*\n2.25\n3.09"
        },
        {
          "GAN+PBR (untuned)": "Rain intensity (mm/hr)\nRain intensity (mm/hr)",
          "19.73\n3.29\nFinetuned (GAN+PBR)\n30.59\n*\n*\n3.01": ""
        },
        {
          "GAN+PBR (untuned)": "",
          "19.73\n3.29\nFinetuned (GAN+PBR)\n30.59\n*\n*\n3.01": "* Not evaluated due to lack of semantic labels for GAN training."
        },
        {
          "GAN+PBR (untuned)": "(a) Object detection [59]\n(b) Depth estimation [25]",
          "19.73\n3.29\nFinetuned (GAN+PBR)\n30.59\n*\n*\n3.01": ""
        },
        {
          "GAN+PBR (untuned)": "",
          "19.73\n3.29\nFinetuned (GAN+PBR)\n30.59\n*\n*\n3.01": "Table 2 Improving performance of computer vision tasks on real"
        },
        {
          "GAN+PBR (untuned)": "Autumn\nLight\nMedium",
          "19.73\n3.29\nFinetuned (GAN+PBR)\n30.59\n*\n*\n3.01": ""
        },
        {
          "GAN+PBR (untuned)": "Rain\nRain\nShower",
          "19.73\n3.29\nFinetuned (GAN+PBR)\n30.59\n*\n*\n3.01": "nuScenes [9] images. These tasks are object detection (YOLOv2 [59]),"
        },
        {
          "GAN+PBR (untuned)": "100",
          "19.73\n3.29\nFinetuned (GAN+PBR)\n30.59\n*\n*\n3.01": ""
        },
        {
          "GAN+PBR (untuned)": "",
          "19.73\n3.29\nFinetuned (GAN+PBR)\n30.59\n*\n*\n3.01": "semantic segmentation (PSPNet\n[82]), and depth estimation (Mon-"
        },
        {
          "GAN+PBR (untuned)": "80",
          "19.73\n3.29\nFinetuned (GAN+PBR)\n30.59\n*\n*\n3.01": "odepth2 [25]). The last line shows performance with the untuned models"
        },
        {
          "GAN+PBR (untuned)": "",
          "19.73\n3.29\nFinetuned (GAN+PBR)\n30.59\n*\n*\n3.01": "after the de-raining [50] process."
        },
        {
          "GAN+PBR (untuned)": "AP (%)\n60",
          "19.73\n3.29\nFinetuned (GAN+PBR)\n30.59\n*\n*\n3.01": ""
        },
        {
          "GAN+PBR (untuned)": "40",
          "19.73\n3.29\nFinetuned (GAN+PBR)\n30.59\n*\n*\n3.01": ""
        },
        {
          "GAN+PBR (untuned)": "20",
          "19.73\n3.29\nFinetuned (GAN+PBR)\n30.59\n*\n*\n3.01": "7.3 Improvement on real rain"
        },
        {
          "GAN+PBR (untuned)": "0",
          "19.73\n3.29\nFinetuned (GAN+PBR)\n30.59\n*\n*\n3.01": ""
        },
        {
          "GAN+PBR (untuned)": "0\n25\n50\n75\n100",
          "19.73\n3.29\nFinetuned (GAN+PBR)\n30.59\n*\n*\n3.01": ""
        },
        {
          "GAN+PBR (untuned)": "Rain intensity (mm/hr)",
          "19.73\n3.29\nFinetuned (GAN+PBR)\n30.59\n*\n*\n3.01": ""
        },
        {
          "GAN+PBR (untuned)": "",
          "19.73\n3.29\nFinetuned (GAN+PBR)\n30.59\n*\n*\n3.01": "We evaluate the performance on real rain, using our nuScenes-"
        },
        {
          "GAN+PBR (untuned)": "(c) Semantic segmentation [82]",
          "19.73\n3.29\nFinetuned (GAN+PBR)\n30.59\n*\n*\n3.01": ""
        },
        {
          "GAN+PBR (untuned)": "",
          "19.73\n3.29\nFinetuned (GAN+PBR)\n30.59\n*\n*\n3.01": "rain(test) subset of images (see sec. 5). Table 2 shows that"
        },
        {
          "GAN+PBR (untuned)": "Fig.\n17 Original\n(untuned)\nor ﬁnetuned performance\non rain-",
          "19.73\n3.29\nFinetuned (GAN+PBR)\n30.59\n*\n*\n3.01": "our ﬁnetuning leads to performance increase in real rainy"
        },
        {
          "GAN+PBR (untuned)": "augmented versions of nuScenes\n(a)-(b) and Cityscapes\n(c). Not",
          "19.73\n3.29\nFinetuned (GAN+PBR)\n30.59\n*\n*\n3.01": "scenes compared to untuned performance in rain. We note for"
        },
        {
          "GAN+PBR (untuned)": "only the ﬁnetuned models signiﬁcantly outperform untuned models,",
          "19.73\n3.29\nFinetuned (GAN+PBR)\n30.59\n*\n*\n3.01": ""
        },
        {
          "GAN+PBR (untuned)": "",
          "19.73\n3.29\nFinetuned (GAN+PBR)\n30.59\n*\n*\n3.01": "object detection (PBR: +20.7%, GAN: +10.9%, GAN+PBR:"
        },
        {
          "GAN+PBR (untuned)": "but\nthey exhibit a lower decrease with rain intensity, demonstrating",
          "19.73\n3.29\nFinetuned (GAN+PBR)\n30.59\n*\n*\n3.01": ""
        },
        {
          "GAN+PBR (untuned)": "",
          "19.73\n3.29\nFinetuned (GAN+PBR)\n30.59\n*\n*\n3.01": "+21.0%), for semantic segmentation (PBR: +36.9%), and for"
        },
        {
          "GAN+PBR (untuned)": "increased robustness to both rain and clear weather.",
          "19.73\n3.29\nFinetuned (GAN+PBR)\n30.59\n*\n*\n3.01": ""
        },
        {
          "GAN+PBR (untuned)": "",
          "19.73\n3.29\nFinetuned (GAN+PBR)\n30.59\n*\n*\n3.01": "depth estimation (PBR: 0.0%, GAN: +3.8%, GAN+PBR:"
        },
        {
          "GAN+PBR (untuned)": "",
          "19.73\n3.29\nFinetuned (GAN+PBR)\n30.59\n*\n*\n3.01": "+8.2%) tasks. In clear weather, our ﬁnetuned model performs"
        },
        {
          "GAN+PBR (untuned)": "PBR. For YOLOv2,\nthe ﬁnetuned detection performance",
          "19.73\n3.29\nFinetuned (GAN+PBR)\n30.59\n*\n*\n3.01": "on par with the untuned version, sometimes even better. This"
        },
        {
          "GAN+PBR (untuned)": "stays higher\nthan its clear untuned counterpart\nin the 0-",
          "19.73\n3.29\nFinetuned (GAN+PBR)\n30.59\n*\n*\n3.01": "boost in performance could be seen as the network learning"
        },
        {
          "GAN+PBR (untuned)": "200mm/hr interval. Explicitly, it goes from 34.5% to 31.0%",
          "19.73\n3.29\nFinetuned (GAN+PBR)\n30.59\n*\n*\n3.01": "to rely on more robust features, somehow invariant\nto rain"
        },
        {
          "GAN+PBR (untuned)": "whereas the untuned model starts at 34.6% and ﬁnishes at",
          "19.73\n3.29\nFinetuned (GAN+PBR)\n30.59\n*\n*\n3.01": "streaks."
        },
        {
          "GAN+PBR (untuned)": "20.4%. For PSPNet, the segmentation exhibits a signiﬁcative",
          "19.73\n3.29\nFinetuned (GAN+PBR)\n30.59\n*\n*\n3.01": "Depth estimation underperformance for PBR ﬁnetuning"
        },
        {
          "GAN+PBR (untuned)": "improvement when reﬁned although at 100mm/hr the model",
          "19.73\n3.29\nFinetuned (GAN+PBR)\n30.59\n*\n*\n3.01": "can be explained by the learning loss of Monodepth2 which is,"
        },
        {
          "GAN+PBR (untuned)": "is not fully able to compensate the effect of rain and drops to",
          "19.73\n3.29\nFinetuned (GAN+PBR)\n30.59\n*\n*\n3.01": "in short, a reprojection error which would not fare well with"
        },
        {
          "GAN+PBR (untuned)": "54.0% versus 52.0% when untuned. Monodepth2 ﬁnetuning",
          "19.73\n3.29\nFinetuned (GAN+PBR)\n30.59\n*\n*\n3.01": "rain streaks as they do not reproject in consecutive frames."
        },
        {
          "GAN+PBR (untuned)": "helps only for higher rain intensity level (+25mm/hr) and the",
          "19.73\n3.29\nFinetuned (GAN+PBR)\n30.59\n*\n*\n3.01": "Interestingly, this problem does not seem to affect the GAN"
        },
        {
          "GAN+PBR (untuned)": "error differences between 100mm/hr and 200mm/hr stay in",
          "19.73\n3.29\nFinetuned (GAN+PBR)\n30.59\n*\n*\n3.01": "or GAN+PBR ﬁnetuned model, possibly because the GAN"
        },
        {
          "GAN+PBR (untuned)": "the same ballpark (~1.2%). This makes sense considering",
          "19.73\n3.29\nFinetuned (GAN+PBR)\n30.59\n*\n*\n3.01": "is being trained on split of nuScenes subsequently leading"
        },
        {
          "GAN+PBR (untuned)": "that since the occlusion created by rain streaks is minimum",
          "19.73\n3.29\nFinetuned (GAN+PBR)\n30.59\n*\n*\n3.01": "to ﬁnetuning images that are more resembling of the test set."
        },
        {
          "GAN+PBR (untuned)": "with a low rain intensity,\nthe untuned model would not be",
          "19.73\n3.29\nFinetuned (GAN+PBR)\n30.59\n*\n*\n3.01": "These results demonstrate the usefulness of our different rain"
        },
        {
          "GAN+PBR (untuned)": "strongly affected.",
          "19.73\n3.29\nFinetuned (GAN+PBR)\n30.59\n*\n*\n3.01": "rendering frameworks for real rain scenarios."
        },
        {
          "GAN+PBR (untuned)": "GAN+PBR.\nIn the case of YOLOv2, we notice a major differ-",
          "19.73\n3.29\nFinetuned (GAN+PBR)\n30.59\n*\n*\n3.01": "7.4 De-raining comparison"
        },
        {
          "GAN+PBR (untuned)": "ence between the hybrid GAN+PBR untuned and ﬁnetuned",
          "19.73\n3.29\nFinetuned (GAN+PBR)\n30.59\n*\n*\n3.01": ""
        },
        {
          "GAN+PBR (untuned)": "performances. Indeed, the hybrid ﬁnetuned performance at",
          "19.73\n3.29\nFinetuned (GAN+PBR)\n30.59\n*\n*\n3.01": "We now compare to the strategy of de-raining images ﬁrst"
        },
        {
          "GAN+PBR (untuned)": "100mm/hr\nis at 21.7% and only at a measly 7.5% for\nthe",
          "19.73\n3.29\nFinetuned (GAN+PBR)\n30.59\n*\n*\n3.01": "and then running un-tuned vision algorithms. To this end, we"
        },
        {
          "GAN+PBR (untuned)": "untuned model. The same goes for Monodepth2 for hybrid",
          "19.73\n3.29\nFinetuned (GAN+PBR)\n30.59\n*\n*\n3.01": "used the state-of-the-art de-raining method DualResNet [50],"
        },
        {
          "GAN+PBR (untuned)": "images performance with 5.7% and 8.9% at 100mm/hr for",
          "19.73\n3.29\nFinetuned (GAN+PBR)\n30.59\n*\n*\n3.01": "ﬁnetuned using nuScenes-clear augmented with GAN+PBR"
        },
        {
          "GAN+PBR (untuned)": "ﬁnetuned and untuned respectively. It is interesting to note",
          "19.73\n3.29\nFinetuned (GAN+PBR)\n30.59\n*\n*\n3.01": "to accommodate for the domain gap."
        },
        {
          "GAN+PBR (untuned)": "that, for all\ntasks, performance evaluated on ﬁnetuned hy-",
          "19.73\n3.29\nFinetuned (GAN+PBR)\n30.59\n*\n*\n3.01": "During\nthe\nde-raining\nﬁne-tuning\nprocess,\nrandom"
        },
        {
          "GAN+PBR (untuned)": "brid image decrease slower than for untuned models. This",
          "19.73\n3.29\nFinetuned (GAN+PBR)\n30.59\n*\n*\n3.01": "batches of {25, 50, 100, 200}mm/hr paired with their non-"
        },
        {
          "GAN+PBR (untuned)": "demonstrates again that more robust models are learned when",
          "19.73\n3.29\nFinetuned (GAN+PBR)\n30.59\n*\n*\n3.01": "augmented counterpart are generated. Except for a smaller"
        },
        {
          "GAN+PBR (untuned)": "ﬁnetuning with our rain translations.",
          "19.73\n3.29\nFinetuned (GAN+PBR)\n30.59\n*\n*\n3.01": "learning rate (10−5), we used the DualResNet default hyper-"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "Light\nMedium\nAutumn\nRaining Cats",
          "15": "Limitations and future work. While we demonstrated highly"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "Light\nMedium\nAutumn\nRaining Cats",
          "15": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "Rain\nRain\nShower\nand Dogs",
          "15": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "Rain\nRain\nShower\nand Dogs\n40\n12.5",
          "15": "realistic rain rendering results, our approach still has limita-"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "35",
          "15": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "10.0",
          "15": "tions that set the stage for future work."
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "30",
          "15": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "25",
          "15": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "mAP (%)\nSq Rel (%)\n7.5",
          "15": "For our PBR approach,\nthe approximation of the light-"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "20",
          "15": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "5.0\n15",
          "15": "ing conditions 3.1.1 yielded reasonable results (ﬁg. 3), but"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "10",
          "15": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "2.5\nGAN+PBR (finetuned)",
          "15": "it may under/over estimate the scene radiance when the sky"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "De-rain (GAN+PBR)",
          "15": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "05\n0.0\n0\n50\n100\n150\n200",
          "15": "is not/too visible. This approximation is more visible when"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "0\n50\n100\n150\n200",
          "15": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "Rain intensity (mm/hr)",
          "15": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "Rain intensity (mm/hr)",
          "15": "streaks are imaged against a darker sky. More robust ap-"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "(a) Object detection [59]\n(b) Depth estimation [25]",
          "15": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "15": "proaches for outdoor lighting estimation could potentially"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "Fig. 18 Performance with varying rain intensities on de-rained",
          "15": "be used [30, 80]. Second, we make an explicit distinction"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "GAN+PBR synthetic images. The de-raining is performed with [50].",
          "15": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "15": "between fog-like rain and drops imaged on more than 1 pixel,"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "We see that the performance on both tasks decrease linearly for both",
          "15": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "15": "individually rendered as streaks. While this distinction is"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "(a) object detection with YOLOv2 [59] and (b) depth estimation with",
          "15": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "15": "widely used in the literature [21, 19, 11, 45], it causes an in-"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "Monodepth2 [25]. Performance on both tasks are lower at low rain in-",
          "15": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "tensities (and the opposite at high rain intensities) compared to models",
          "15": "appropriate sharp distinction between fog-like and streaks."
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "ﬁnetuned with GAN+PBR synthetic images (cf. ﬁg. 17).",
          "15": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "15": "A possible solution would be to render all drops as streaks"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "15": "weighting them as a function of their imaging surface. How-"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "15": "ever, our experiments show it comes at a prohibitive computa-"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "parameters (Adam optimizer, batch and crop size of 40 and",
          "15": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "15": "tion cost. Finally, rain streaks are added to image irrespective"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "64 respectively).",
          "15": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "15": "of the scene contents. Here, the depth estimate could be used"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "With this de-raining ﬁnetuned model, we compare the per-",
          "15": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "15": "to mask out streaks that appear behind objects and under"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "formance of our “untuned” object detection (YOLOv2 [59])",
          "15": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "15": "the ground plane. Another limitation is the computational"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "and depth estimation (Monodepth2 [25]) models. Fig. 18",
          "15": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "15": "cost of PBR. While this has no downside for benchmarking"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "shows the performance of the de-raining strategy compared",
          "15": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "15": "purpose as PBR may be run off-line, simulation requires"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "to our “rain-aware” GAN+PBR ﬁnetuned models. Here, we",
          "15": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "15": "increasing time with larger rainfall rates. With our current"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "observe that\nthe rain-aware models offer improved perfor-",
          "15": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "15": "unoptimized implementation, the simulation of 1 / 25 / 50 /"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "mance for object detection over de-raining, while the latter",
          "15": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "15": "100 mm/hr rainfall rates on Cityscapes requires 0.35 / 5.65 /"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "improves depth estimation. This is likely due to the fact that",
          "15": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "15": "16.60 / 20.67 seconds respectively for the rain physics [11]"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "streaks occlude the scene background, while de-raining acts",
          "15": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "15": "and an additional 6.71 / 34.92 / 62.94 / 104.76 seconds for the"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "as prior impainting thus easing depth estimation.",
          "15": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "15": "rendering (times are per image, on single core, and averaged"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "We also applied the same de-raining strategy to the real",
          "15": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "15": "over 100 frames). This restricts the usage of PBR to off-line"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "nuScenes images and report performance in the last row of",
          "15": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "15": "processing though signiﬁcant speed up could be obtained at"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "table 2. Again, for object detection on rainy images our rain-",
          "15": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "15": "the cost of additional optimization efforts."
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "aware models perform better than de-raining. However, for",
          "15": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "depth estimation,\nthe de-raining strategy is better for both",
          "15": "The GAN employed also has limitations. First, while"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "clear and rainy images. This is consistent with the results",
          "15": "PBR is well-suited for videos since the rain simulator\nis"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "obtained on synthetic data.",
          "15": "temporally consistent,\nthis is not\nthe case for CycleGAN"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "15": "which does not guarantee temporal smoothness. Existing ap-"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "These experiments illustrate that de-raining is also a valid",
          "15": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "15": "proaches such as [2] are alternatives, but GANs are known"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "strategy that may even outperform “rain-aware” algorithms.",
          "15": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "15": "for their non-realistic physical outcome [72]. Second, Cycle-"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "However,\nthis comes at\nthe cost of having to perform two",
          "15": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "15": "GAN imposes a limit on the image resolution. Here, super-"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "tasks, which may limit practical applications. On the long",
          "15": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "15": "resolution networks such as SRResNet [16] or SRGAN [42]"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "term, we believe rain-robust algorithms offer an exciting new",
          "15": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "15": "could potentially be used, or large-scale GANs such as Big-"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "research paradigm while avoiding the in-ﬁlling of occluded",
          "15": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "15": "GAN [8] are also an option. More importantly, GANs tend"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "areas.",
          "15": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "15": "to have difﬁculty in generating rain on images of datasets"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "15": "different\nthan which they are trained on since the learning"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "15": "process does not disentangle rain from scene appearance,"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "8 Discussion",
          "15": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "15": "demonstrating a strong domain dependence."
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "In this paper, we presented the ﬁrst intensity-controlled physi-",
          "15": "Finally, while our results demonstrate that ﬁne-tuning"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "cal framework for augmenting existing image databases with",
          "15": "on synthetically generated rain does improve performance"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "realistic rain. This allows us to systematically study the im-",
          "15": "on real rainy images (cf. sec. 7), the improvements obtained"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "pact of rain on existing computer vision algorithms on three",
          "15": "are still quite modest. Further efforts are necessary to de-"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "important tasks: object detection, semantic segmentation, and",
          "15": "velop algorithms that are truly robust\nto challenging rainy"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "depth estimation.",
          "15": "conditions."
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "16": "",
          "Maxime Tremblay et al.": "vectors forming the viewing cone through the drop is obtained by the"
        },
        {
          "16": "",
          "Maxime Tremblay et al.": "rotation of v all around the viewing direction. Formally,"
        },
        {
          "16": "",
          "Maxime Tremblay et al.": "(8)\nV (cid:48) = {v · Rd(α) | ∀ α ∈ [0, 2π[} ,"
        },
        {
          "16": "Drop viewing cone",
          "Maxime Tremblay et al.": "with Rd(α) the rotation matrix of α around vector d. In practice, V (cid:48)"
        },
        {
          "16": "",
          "Maxime Tremblay et al.": "is a ﬁnite set of radially equidistant vectors (for computational reason"
        },
        {
          "16": "",
          "Maxime Tremblay et al.": "we use |V (cid:48)| = 20)."
        },
        {
          "16": "D\nd\nr\nu",
          "Maxime Tremblay et al.": "To compute the coordinates of the drop FOV in the environment,"
        },
        {
          "16": "o\nr\np\ni",
          "Maxime Tremblay et al.": ""
        },
        {
          "16": "n\ng\nm",
          "Maxime Tremblay et al.": ""
        },
        {
          "16": "o\ne",
          "Maxime Tremblay et al.": ""
        },
        {
          "16": "t\ni\nx\no\np\nn\no",
          "Maxime Tremblay et al.": "we assume a projection sphere S of radius 10m. Hence, we compute the"
        },
        {
          "16": "s\nu",
          "Maxime Tremblay et al.": ""
        },
        {
          "16": "r\ne",
          "Maxime Tremblay et al.": ""
        },
        {
          "16": "Camera",
          "Maxime Tremblay et al.": "set Q = {φ(S, v(cid:48)) | ∀ v(cid:48) ∈ V (cid:48)} of points where vectors intersect the"
        },
        {
          "16": "",
          "Maxime Tremblay et al.": "environment sphere, considering only the positive viewing direction axis."
        },
        {
          "16": "Fig. 19 Geometrical construction to compute a drop FOV. Considering",
          "Maxime Tremblay et al.": "Given that the sphere is centered to the camera position and all drops"
        },
        {
          "16": "X0 and X1 the drop position at shutter opening and closing, respec-",
          "Maxime Tremblay et al.": "3D positions are expressed in the camera referential, the intersection"
        },
        {
          "16": "tively. We assume a constant drop position X = X0+X1\n(during the",
          "Maxime Tremblay et al.": "φ(S, v(cid:48)) of a vector v(cid:48) and sphere S of radius Sρ is straight-forward"
        },
        {
          "16": "2",
          "Maxime Tremblay et al.": ""
        },
        {
          "16": "exposure time, a few milliseconds). Note that we drew only a slice of",
          "Maxime Tremblay et al.": "with"
        },
        {
          "16": "the drop FOV for simplicity but a full 3D visualization would show a",
          "Maxime Tremblay et al.": ""
        },
        {
          "16": "",
          "Maxime Tremblay et al.": "φ(S, v(cid:48)) = v(cid:48) + td with,"
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "Kitti [23]\nCityscapes [13] nuScenes-clear [9] nuScenes-rain [9]",
          "17": "12. Chen, Y.L., Hsu, C.T.: A generalized low-rank appearance model"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "7,481\n2,995\n24,134\n4,996",
          "17": "for spatio-temporally correlated rain streaks. In: IEEE International"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "17": "Conference on Computer Vision (2013) 3"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "Effect of real rain",
          "17": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "-\n-\n1,000 (test)\n*609 / 25 (test)",
          "17": "13. Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M.,"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "(sec. 5)",
          "17": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "17": "Benenson, R., Franke, U., Roth, S., Schiele, B.: The Cityscapes"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "Effect of synth.",
          "17": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "7,481 (test)\n2,995 (test)\n4,449 (test)\n-",
          "17": "dataset for semantic urban scene understanding.\nIn: IEEE Confer-"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "rain (sec. 6)",
          "17": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "GAN training\n19,685 (train)\n5,419 (train)",
          "17": "ence on Computer Vision and Pattern Recognition (2016) 1, 2, 3,"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "-\n-",
          "17": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "(sec. 6.1)\n4,449 (test)\n609 (test)",
          "17": "8, 9, 17"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "Improving synth.",
          "17": "14. Creus, C., Patow, G.A.: R4: Realistic rain rendering in realtime."
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "1,000 (train)\n1,000 (train)\n1,000 (train)",
          "17": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "robustness",
          "17": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "-",
          "17": "Computers & Graphics 37(1-2), 33–40 (2013) 2"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "1,000 (test)\n1,000 (test)\n1,000 (test)",
          "17": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "(sec. 7.2)",
          "17": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "17": "15. Dai, J., Li, Y., He, K., Sun, J.: R-FCN: Object detection via region-"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "Improving real",
          "17": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "1,000 (train)",
          "17": "based fully convolutional networks.\nIn: Advances in Neural Infor-"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "robustness\n-\n-\n*609 / 25 (test)",
          "17": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "*1,000 / 25 (test)",
          "17": "mation Processing Systems (2016) 10"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "(sec. 7.3)",
          "17": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "17": "16. Dong, C., Loy, C.C., He, K., Tang, X.:\nImage super-resolution"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "*Number of images for object&depth / semantics",
          "17": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "17": "using deep convolutional networks.\nIEEE Transactions on Pattern"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "Table 3 Data splits for all experiments. The splits are separated per",
          "17": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "17": "Analysis and Machine Intelligence 38(2), 295–307 (2015) 15"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "context and datasets ; orange indicates our\nrain augmented images.",
          "17": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "17": "17. Eigen, D., Krishnan, D., Fergus, R.: Restoring an image taken"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "Notably, both the train and the test sets for improving the robustness to",
          "17": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "17": "through a window covered with dirt or rain.\nIn: IEEE International"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "synthetic and real rain are sampled from the test set of the GAN training",
          "17": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "17": "Conference on Computer Vision (2013) 3"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "phase; this is valid since the computer tasks trained with these subsets",
          "17": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "17": "18. Garg, K., Nayar, S.K.: Detection and removal of rain from videos."
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "have not seen images from neither set.",
          "17": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "17": "In: IEEE Conference on Computer Vision and Pattern Recognition"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "17": "(2004) 3"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "17": "In: IEEE\n19. Garg, K., Nayar, S.K.: When does a camera see rain?"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "17": "International Conference on Computer Vision (2005) 2, 3, 6, 15,"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "C Experiments data splits",
          "17": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "17": "16"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "17": "20. Garg, K., Nayar, S.K.: Photorealistic rendering of\nrain streaks."
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "Table 3 contains the minutiae of the data splits of the various experi-",
          "17": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "17": "ACM Transactions on Graphics (SIGGRAPH) 25(3), 996–1002"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "mental steps of this paper.",
          "17": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "17": "(2006) 2, 3, 4, 5, 16"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "17": "21. Garg, K., Nayar, S.K.: Vision and rain.\nInternational Journal of"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "17": "Computer Vision 75(1), 3–27 (2007) 2, 4, 5, 15, 16"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "References",
          "17": "22. Geiger, A., Lenz, P., Stiller, C., Urtasun, R.: Vision meets robotics:"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "",
          "17": "The KITTI dataset.\nInternational Journal of Robotics Research"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "1. Atlas, D., Srivastava, R., Sekhon, R.S.: Doppler radar characteris-",
          "17": "32(11), 1231–1237 (2013) 4, 9"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "tics of precipitation at vertical incidence. Reviews of Geophysics",
          "17": "23. Geiger, A., Lenz, P., Urtasun, R.: Are we ready for autonomous"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "11(1), 1–35 (1973) 4",
          "17": "driving? The KITTI vision benchmark suite.\nIn: IEEE Conference"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "2. Bansal, A., Ma, S., Ramanan, D., Sheikh, Y.: Recycle-gan: Unsu-",
          "17": "on Computer Vision and Pattern Recognition (2012) 1, 2, 9, 17"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "pervised video retargeting.\nIn: European Conference on Computer",
          "17": "24. Godard, C., Mac Aodha, O., Brostow, G.J.: Unsupervised monocu-"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "Vision (2018) 15",
          "17": "lar depth estimation with left-right consistency.\nIn: IEEE Confer-"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "3. Barnum, P.C., Narasimhan, S., Kanade, T.: Analysis of rain and",
          "17": "ence on Computer Vision and Pattern Recognition (2017) 9"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "snow in frequency space. International Journal of Computer Vision",
          "17": "25. Godard, C., Mac Aodha, O., Firman, M., Brostow, G.J.: Digging"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "86(2-3), 256 (2010) 2, 3, 4",
          "17": "into self-supervised monocular depth estimation.\nIn: IEEE Interna-"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "4. Barron, J.T., Poole, B.: The fast bilateral solver.\nIn: European",
          "17": "tional Conference on Computer Vision (2019) 1, 8, 9, 10, 11, 12,"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "Conference on Computer Vision (2016) 9",
          "17": "13, 14, 15"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "5. Bengio, Y., Louradour, J., Collobert, R., Weston, J.: Curriculum",
          "17": "26. Gruber, T., Bijelic, M., Heide, F., Ritter, W., Dietmayer, K.: Pixel-"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "learning. In: International Conference on Machine Learning (2009)",
          "17": "accurate depth evaluation in realistic driving scenarios.\nIn: Interna-"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "2, 12",
          "17": "tional Conference on 3D Vision (2019) 3"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "6. Bijelic, M., Mannan, F., Gruber, T., Ritter, W., Dietmayer, K.,",
          "17": "27. Halder, S.S., Lalonde, J.F., Charette, R.d.: Physics-based rendering"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "Heide, F.: Seeing through fog without seeing fog: Deep sensor",
          "17": "for improving robustness to rain. In: IEEE International Conference"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "fusion in the absence of labeled training data.\nIn: IEEE Conference",
          "17": "on Computer Vision (2019) 2"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "on Computer Vision and Pattern Recognition (2020) 3",
          "17": "28. Halimeh, J.C., Roser, M.: Raindrop detection on car windshields us-"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "7.\nvan Boxel, J.H., et al.: Numerical model for the fall speed of rain",
          "17": "ing geometric-photometric environment construction and intensity-"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "drops in a rain fall simulator.\nIn: Workshop on wind and water",
          "17": "based correlation.\nIn: IEEE Intelligent Vehicles Symposium (2009)"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "erosion (1997) 4",
          "17": "2, 3, 4"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "8. Brock, A., Donahue,\nJ., Simonyan, K.: Large scale gan train-",
          "17": "29. Hao, Z., You, S., Li, Y., Li, K., Lu, F.: Learning from synthetic"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "ing for high ﬁdelity natural\nimage\nsynthesis.\narXiv preprint",
          "17": "photorealistic raindrop for single image raindrop removal. In: IEEE"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "arXiv:1809.11096 (2018) 15",
          "17": "International Conference on Computer Vision Workshops (2019)"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "9. Caesar, H., Bankiti, V., Lang, A.H., Vora, S., Liong, V.E., Xu,",
          "17": "2, 3"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "Q., Krishnan, A., Pan, Y., Baldan, G., Beijbom, O.: nuScenes: A",
          "17": "30. Hold-Geoffroy, Y., Athawale, A., Lalonde, J.F.: Deep sky modeling"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "multimodal dataset for autonomous driving.\nIn: IEEE Conference",
          "17": "for single image outdoor lighting estimation.\nIn: IEEE Conference"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "on Computer Vision and Pattern Recognition (2020) 1, 2, 3, 6, 7,",
          "17": "on Computer Vision and Pattern Recognition (2019) 4, 5, 15"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "9, 14, 17",
          "17": "31. Hold-Geoffroy, Y., Sunkavalli, K., Hadap, S., Gambaretto, E.,"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "10. Cameron, C.: Hallucinating environment maps from single images.",
          "17": "Lalonde, J.F.: Deep outdoor\nillumination estimation.\nIn:\nIEEE"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "Tech. rep. (2005) 4",
          "17": "Conference on Computer Vision and Pattern Recognition (2017) 4"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "de Charette, R., Tamburo, R., Barnum, P.C., Rowe, A., Kanade, T.,\n11.",
          "17": "32. Horn, B., Klaus, B., Horn, P.: Robot vision. MIT press (1986) 4"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "Narasimhan, S.G.: Fast reactive control for illumination through",
          "17": "33. Huang, X., Liu, M.Y., Belongie, S., Kautz, J.: Multimodal unsu-"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "rain and snow. In: IEEE International Conference on Computational",
          "17": "pervised image-to-image translation.\nIn: European Conference on"
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "Photography (2012) 2, 3, 4, 15",
          "17": "Computer Vision (2018) 3"
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "18": "34.\nIsola, P., Zhu, J.Y., Zhou, T., Efros, A.A.: Image-to-image transla-",
          "Maxime Tremblay et al.": "55. Narasimhan, S.G., Wang, C., Nayar, S.K.: All\nthe images of an"
        },
        {
          "18": "tion with conditional adversarial networks.\nIn: IEEE Conference",
          "Maxime Tremblay et al.": "outdoor scene.\nIn: European Conference on Computer Vision"
        },
        {
          "18": "on Computer Vision and Pattern Recognition (2017) 6",
          "Maxime Tremblay et al.": "(2002) 3"
        },
        {
          "18": "35.\nJacobs, N., Roman, N., Pless, R.: Consistent temporal variations in",
          "Maxime Tremblay et al.": "56.\nPizzati, F., Cerri, P., de Charette, R.: Model-based occlusions disen-"
        },
        {
          "18": "many outdoor scenes.\nIn: IEEE Conference on Computer Vision",
          "Maxime Tremblay et al.": "tanglement for image-to-image translation. European Conference"
        },
        {
          "18": "and Pattern Recognition (2007) 3",
          "Maxime Tremblay et al.": "on Computer Vision (2020) 3, 7"
        },
        {
          "18": "36.\nJaritz, M., de Charette, R., Wirbel, E., Perrotton, X., Nashashibi, F.:",
          "Maxime Tremblay et al.": "57.\nPorav, H., Bruls, T., Newman, P.: I can see clearly now: Image"
        },
        {
          "18": "Sparse and dense data with CNNs: Depth completion and semantic",
          "Maxime Tremblay et al.": "restoration via de-raining.\nIn: IEEE International Conference on"
        },
        {
          "18": "segmentation.\nIn: International Conference on 3D Vision (2018) 9",
          "Maxime Tremblay et al.": "Robotics and Automation (2019) 2, 3, 12"
        },
        {
          "18": "37.\nJohnson, J., Alahi, A., Fei-Fei, L.: Perceptual losses for real-time",
          "Maxime Tremblay et al.": "58.\nPotmesil, M., Chakravarty, I.: A lens and aperture camera model"
        },
        {
          "18": "style transfer and super-resolution.\nIn: European Conference on",
          "Maxime Tremblay et al.": "for synthetic image generation. ACM Transactions on Graphics"
        },
        {
          "18": "Computer Vision (2016) 6",
          "Maxime Tremblay et al.": "(SIGGRAPH) 15(3), 297–305 (1981) 5"
        },
        {
          "18": "38.\nJohnson-Roberson, M., Barto, C., Mehta, R., Sridhar, S.N., Rosaen,",
          "Maxime Tremblay et al.": "59. Redmon, J., Farhadi, A.: YOLO9000: Better, faster, stronger.\nIn:"
        },
        {
          "18": "K., Vasudevan, R.: Driving in the matrix: Can virtual worlds replace",
          "Maxime Tremblay et al.": "IEEE Conference on Computer Vision and Pattern Recognition"
        },
        {
          "18": "human-generated annotations for real world tasks? In: International",
          "Maxime Tremblay et al.": "(2017) 8, 9, 10, 12, 13, 14, 15"
        },
        {
          "18": "Conference on Robotics and Automation (2016) 3",
          "Maxime Tremblay et al.": "60. Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: Towards real-"
        },
        {
          "18": "39. Khan, S., Phan, B., Salay, R., Czarnecki, K.: Procsy: Procedural",
          "Maxime Tremblay et al.": "time object detection with region proposal networks.\nIn: Advances"
        },
        {
          "18": "synthetic dataset generation towards inﬂuence factor studies of se-",
          "Maxime Tremblay et al.": "in Neural Information Processing Systems (2015) 9, 10"
        },
        {
          "18": "mantic segmentation networks.\nIn: IEEE Conference on Computer",
          "Maxime Tremblay et al.": "61. Romera, E., Alvarez, J.M., Bergasa, L.M., Arroyo, R.: ERFNet:"
        },
        {
          "18": "Vision and Pattern Recognition Workshops (2019) 3",
          "Maxime Tremblay et al.": "Efﬁcient residual factorized convnet for real-time semantic segmen-"
        },
        {
          "18": "40. Laffont, P.Y., Ren, Z., Tao, X., Qian, C., Hays, J.: Transient at-",
          "Maxime Tremblay et al.": "tation.\nIEEE Transactions on Intelligent Transportation Systems"
        },
        {
          "18": "tributes for high-level understanding and editing of outdoor scenes.",
          "Maxime Tremblay et al.": "19(1), 263–272 (2018) 10, 11"
        },
        {
          "18": "ACM Transactions on Graphics (SIGGRAPH) 33(4), 1–11 (2014)",
          "Maxime Tremblay et al.": "62. Roser, M., Geiger, A.: Video-based raindrop detection for improved"
        },
        {
          "18": "3",
          "Maxime Tremblay et al.": "image registration. In: IEEE International Conference on Computer"
        },
        {
          "18": "41. Lalonde, J.F., Efros, A.A., Narasimhan, S.G.: Webcam clip art:",
          "Maxime Tremblay et al.": "Vision Workshops (2009) 2"
        },
        {
          "18": "Appearance and illuminant\ntransfer\nfrom time-lapse sequences.",
          "Maxime Tremblay et al.": "63. Roser, M., Kurz, J., Geiger, A.: Realistic modeling of water droplets"
        },
        {
          "18": "ACM Transactions on Graphics (SIGGRAPH) 28(5), 1–10 (2009)",
          "Maxime Tremblay et al.": "for monocular adherent raindrop recognition using bezier curves."
        },
        {
          "18": "3",
          "Maxime Tremblay et al.": "In: Asian Conference on Computer Vision (2010) 2"
        },
        {
          "18": "42. Ledig, C., Theis, L., Huszár, F., Caballero, J., Cunningham, A.,",
          "Maxime Tremblay et al.": "64. Rousseau, P., Jolivet, V., Ghazanfarpour, D.: Realistic real-time"
        },
        {
          "18": "Acosta, A., Aitken, A., Tejani, A., Totz, J., Wang, Z., et al.: Photo-",
          "Maxime Tremblay et al.": "rain rendering. Computers & Graphics 30(4), 507–518 (2006) 2, 4"
        },
        {
          "18": "realistic single image super-resolution using a generative adversar-",
          "Maxime Tremblay et al.": "65.\nSachin Mehta, M.R., Caspi, A., Shapiro, L., Hajishirzi, H.: ESPNet:"
        },
        {
          "18": "ial network.\nIn: IEEE Conference on Computer Vision and Pattern",
          "Maxime Tremblay et al.": "Efﬁcient spatial pyramid of dilated convolutions for semantic seg-"
        },
        {
          "18": "Recognition (2017) 15",
          "Maxime Tremblay et al.": "mentation.\nIn: European Conference on Computer Vision (2018)"
        },
        {
          "18": "43. Lee, J.H., Han, M.K., Ko, D.W., Suh,\nI.H.: From big to small:",
          "Maxime Tremblay et al.": "1, 10, 11"
        },
        {
          "18": "Multi-scale local planar guidance for monocular depth estimation.",
          "Maxime Tremblay et al.": "66.\nSakaridis, C., Dai, D., Van Gool, L.: Semantic foggy scene under-"
        },
        {
          "18": "arXiv preprint arXiv:1907.10326v5 (2020) 10, 11",
          "Maxime Tremblay et al.": "standing with synthetic data.\nInternational Journal of Computer"
        },
        {
          "18": "44. Li, P., Liang, X.,\nJia, D., Xing, E.P.: Semantic-aware\ngrad-",
          "Maxime Tremblay et al.": "Vision 126(9), 973–992 (2018) 3"
        },
        {
          "18": "gan for virtual-to-real urban scene\nadaption.\narXiv preprint",
          "Maxime Tremblay et al.": "67.\nShen, Z., Liu, Z., Li, J., Jiang, Y.G., Chen, Y., Xue, X.: DSOD:"
        },
        {
          "18": "arXiv:1801.01726 (2018) 3",
          "Maxime Tremblay et al.": "Learning deeply supervised object detectors from scratch. In: IEEE"
        },
        {
          "18": "45. Li, R., Tan, R.T., Cheong, L.F.: Robust optical ﬂow in rainy scenes.",
          "Maxime Tremblay et al.": "International Conference on Computer Vision (2017) 9"
        },
        {
          "18": "In: European Conference on Computer Vision (2018) 15",
          "Maxime Tremblay et al.": "68. Tasar, O., Happy, S., Tarabalka, Y., Alliez, P.: Semi2i: Semantically"
        },
        {
          "18": "46. Li, Y., Tan, R.T., Guo, X., Lu, J., Brown, M.S.: Rain streak removal",
          "Maxime Tremblay et al.": "consistent\nimage-to-image translation for domain adaptation of"
        },
        {
          "18": "using layer priors.\nIn: IEEE Conference on Computer Vision and",
          "Maxime Tremblay et al.": "remote sensing data. arXiv preprint arXiv:2002.05925 (2020) 3"
        },
        {
          "18": "Pattern Recognition (2016) 3",
          "Maxime Tremblay et al.": "69. Tatarchuk, N.: Artist-directable real-time rain rendering in city"
        },
        {
          "18": "47. Liu, M.Y., Breuel, T., Kautz, J.: Unsupervised image-to-image",
          "Maxime Tremblay et al.": "environments.\nIn: SIGGRAPH 2006 Courses, pp. 23–64. ACM"
        },
        {
          "18": "translation networks.\nIn: Advances in Neural Information Process-",
          "Maxime Tremblay et al.": "(2006) 2"
        },
        {
          "18": "ing Systems (2017) 3",
          "Maxime Tremblay et al.": "70. Tsai, Y.H., Hung, W.C., Schulter, S., Sohn, K., Yang, M.H., Chan-"
        },
        {
          "18": "48. Liu, M.Y., Huang, X., Mallya, A., Karras, T., Aila, T., Lehtinen, J.,",
          "Maxime Tremblay et al.": "draker, M.: Learning to adapt structured output space for semantic"
        },
        {
          "18": "Kautz, J.: Few-shot unsupervised image-to-image translation.\nIn:",
          "Maxime Tremblay et al.": "segmentation.\nIn: IEEE Conference on Computer Vision and Pat-"
        },
        {
          "18": "IEEE International Conference on Computer Vision (2019) 3",
          "Maxime Tremblay et al.": "tern Recognition (2018) 10"
        },
        {
          "18": "49. Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.Y.,",
          "Maxime Tremblay et al.": "71. Weber, Y., Jolivet, V., Gilet, G., Ghazanfarpour, D.: A multiscale"
        },
        {
          "18": "Berg, A.C.: SSD: Single shot multibox detector.\nIn: European",
          "Maxime Tremblay et al.": "model for rain rendering in real-time. Computers & Graphics 50,"
        },
        {
          "18": "Conference on Computer Vision (2016) 10",
          "Maxime Tremblay et al.": "61–70 (2015) 2, 4"
        },
        {
          "18": "50. Liu, X., Suganuma, M., Sun, Z., Okatani, T.: Dual residual net-",
          "Maxime Tremblay et al.": "72. Xie, Y., Franz, E., Chu, M., Thuerey, N.: tempogan: A temporally"
        },
        {
          "18": "works\nleveraging the potential of paired operations\nfor\nimage",
          "Maxime Tremblay et al.": "coherent, volumetric gan for super-resolution ﬂuid ﬂow. ACM"
        },
        {
          "18": "restoration.\nIn: IEEE Conference on Computer Vision and Pat-",
          "Maxime Tremblay et al.": "Transactions on Graphics (SIGGRAPH) 37(4), 1–15 (2018) 15"
        },
        {
          "18": "tern Recognition (2019) 2, 3, 14, 15",
          "Maxime Tremblay et al.": "73. Yang, F., Choi, W., Lin, Y.: Exploit all the layers: Fast and accurate"
        },
        {
          "18": "51. Luo, Y., Xu, Y., Ji, H.: Removing rain from a single image via",
          "Maxime Tremblay et al.": "cnn object detector with scale dependent pooling and cascaded"
        },
        {
          "18": "discriminative sparse coding.\nIn: IEEE International Conference",
          "Maxime Tremblay et al.": "rejection classiﬁers.\nIn: IEEE Conference on Computer Vision and"
        },
        {
          "18": "on Computer Vision (2015) 3, 6",
          "Maxime Tremblay et al.": "Pattern Recognition (2016) 1, 10"
        },
        {
          "18": "52. Maddern, W., Pascoe, G., Linegar, C., Newman, P.: 1 Year, 1000km:",
          "Maxime Tremblay et al.": "74. Yang, W., Tan, R.T., Feng, J., Liu, J., Guo, Z., Yan, S.: Deep"
        },
        {
          "18": "The Oxford RobotCar Dataset.\nInternational Journal of Robotics",
          "Maxime Tremblay et al.": "joint rain detection and removal from a single image.\nIn: IEEE"
        },
        {
          "18": "Research 36(1), 3–15 (2017) 3",
          "Maxime Tremblay et al.": "Conference on Computer Vision and Pattern Recognition (2017) 2,"
        },
        {
          "18": "53. Marshall, J.S., Palmer, W.M.K.: The distribution of raindrops with",
          "Maxime Tremblay et al.": "3, 6, 7"
        },
        {
          "18": "size. Journal of meteorology 5(4), 165–166 (1948) 4",
          "Maxime Tremblay et al.": "75. Yi, Z., Zhang, H., Tan, P., Gong, M.: Dualgan: Unsupervised dual"
        },
        {
          "18": "54. Narasimhan, S.G., Nayar, S.K.: Vision and the atmosphere.\nInter-",
          "Maxime Tremblay et al.": "learning for image-to-image translation.\nIn: IEEE International"
        },
        {
          "18": "national Journal of Computer Vision 48(3), 233–254 (2002) 4",
          "Maxime Tremblay et al.": "Conference on Computer vision (2017) 3"
        }
      ],
      "page": 18
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "76. Yu, F., Xian, W., Chen, Y., Liu, F., Liao, M., Madhavan, V., Dar-",
          "19": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "rell, T.: Bdd100k: A diverse driving video database with scalable",
          "19": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "annotation tooling. arXiv preprint arXiv:1805.04687 (2018) 3",
          "19": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "77. Zendel, O., Honauer, K., Murschitz, M., Steininger, D., Fernan-",
          "19": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "dez Dominguez, G.: Wilddash-creating hazard-aware benchmarks.",
          "19": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "In: European Conference on Computer Vision (2018) 3",
          "19": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "78. Zhang, H., Patel, V.M.: Density-aware single image de-raining",
          "19": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "using a multi-stream dense network.\nIn:\nIEEE Conference on",
          "19": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "Computer Vision and Pattern Recognition (2018) 6, 7",
          "19": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "79. Zhang, H., Sindagi, V., Patel, V.M.: Image de-raining using a con-",
          "19": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "ditional generative adversarial network.\nIEEE Transactions on",
          "19": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "Circuits and Systems for Video Technology (2019) 2, 3, 6, 7",
          "19": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "80. Zhang, J., Sunkavalli, K., Hold-Geoffroy, Y., Hadap, S., Eisenman,",
          "19": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "J., Lalonde, J.F.: All-weather deep outdoor lighting estimation.\nIn:",
          "19": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "IEEE Conference on Computer Vision and Pattern Recognition",
          "19": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "(2019) 4, 15",
          "19": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "81. Zhao, H., Qi, X., Shen, X., Shi, J., Jia, J.:\nICNet\nfor\nreal-time",
          "19": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "semantic segmentation on high-resolution images.\nIn: European",
          "19": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "Conference on Computer Vision (2018) 10, 11",
          "19": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "82. Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J.: Pyramid scene parsing",
          "19": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "network.\nIn: IEEE Conference on Computer Vision and Pattern",
          "19": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "Recognition (2017) 8, 9, 10, 11, 12, 14",
          "19": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "83. Zhu, J.Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image",
          "19": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "translation using cycle-consistent adversarial networks.\nIn: IEEE",
          "19": ""
        },
        {
          "Rain rendering for evaluating and improving robustness to bad weather": "International Conference on Computer Vision (2017) 2, 3, 6",
          "19": ""
        }
      ],
      "page": 19
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Doppler radar characteristics of precipitation at vertical incidence",
      "authors": [
        "D Atlas",
        "R Srivastava",
        "R Sekhon"
      ],
      "year": "1973",
      "venue": "Reviews of Geophysics"
    },
    {
      "citation_id": "2",
      "title": "Recycle-gan: Unsupervised video retargeting",
      "authors": [
        "A Bansal",
        "S Ma",
        "D Ramanan",
        "Y Sheikh"
      ],
      "year": "2018",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "3",
      "title": "Analysis of rain and snow in frequency space",
      "authors": [
        "P Barnum",
        "S Narasimhan",
        "T Kanade"
      ],
      "year": "2010",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "4",
      "title": "The fast bilateral solver",
      "authors": [
        "J Barron",
        "B Poole"
      ],
      "year": "2016",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "5",
      "title": "Curriculum learning",
      "authors": [
        "Y Bengio",
        "J Louradour",
        "R Collobert",
        "J Weston"
      ],
      "year": "2009",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "6",
      "title": "Seeing through fog without seeing fog: Deep sensor fusion in the absence of labeled training data",
      "authors": [
        "M Bijelic",
        "F Mannan",
        "T Gruber",
        "W Ritter",
        "K Dietmayer",
        "F Heide"
      ],
      "year": "2020",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "7",
      "title": "Numerical model for the fall speed of rain drops in a rain fall simulator",
      "authors": [
        "J Van Boxel"
      ],
      "year": "1997",
      "venue": "Workshop on wind and water erosion"
    },
    {
      "citation_id": "8",
      "title": "Large scale gan training for high fidelity natural image synthesis",
      "authors": [
        "A Brock",
        "J Donahue",
        "K Simonyan"
      ],
      "year": "2018",
      "venue": "Large scale gan training for high fidelity natural image synthesis",
      "arxiv": "arXiv:1809.11096"
    },
    {
      "citation_id": "9",
      "title": "nuScenes: A multimodal dataset for autonomous driving",
      "authors": [
        "H Caesar",
        "V Bankiti",
        "A Lang",
        "S Vora",
        "V Liong",
        "Q Xu",
        "A Krishnan",
        "Y Pan",
        "G Baldan",
        "O Beijbom"
      ],
      "year": "2009",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "10",
      "title": "Hallucinating environment maps from single images",
      "authors": [
        "C Cameron"
      ],
      "year": "2005",
      "venue": "Hallucinating environment maps from single images"
    },
    {
      "citation_id": "11",
      "title": "Fast reactive control for illumination through rain and snow",
      "authors": [
        "R De Charette",
        "R Tamburo",
        "P Barnum",
        "A Rowe",
        "T Kanade",
        "S Narasimhan"
      ],
      "year": "2012",
      "venue": "IEEE International Conference on Computational Photography"
    },
    {
      "citation_id": "12",
      "title": "A generalized low-rank appearance model for spatio-temporally correlated rain streaks",
      "authors": [
        "Y Chen",
        "C Hsu"
      ],
      "year": "2013",
      "venue": "IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "13",
      "title": "The Cityscapes dataset for semantic urban scene understanding",
      "authors": [
        "M Cordts",
        "M Omran",
        "S Ramos",
        "T Rehfeld",
        "M Enzweiler",
        "R Benenson",
        "U Franke",
        "S Roth",
        "B Schiele"
      ],
      "year": "2016",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "14",
      "title": "R4: Realistic rain rendering in realtime",
      "authors": [
        "C Creus",
        "G Patow"
      ],
      "year": "2013",
      "venue": "R4: Realistic rain rendering in realtime"
    },
    {
      "citation_id": "15",
      "title": "R-FCN: Object detection via regionbased fully convolutional networks",
      "authors": [
        "J Dai",
        "Y Li",
        "K He",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "16",
      "title": "Image super-resolution using deep convolutional networks",
      "authors": [
        "C Dong",
        "C Loy",
        "K He",
        "X Tang"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "17",
      "title": "Restoring an image taken through a window covered with dirt or rain",
      "authors": [
        "D Eigen",
        "D Krishnan",
        "R Fergus"
      ],
      "year": "2013",
      "venue": "IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "18",
      "title": "Detection and removal of rain from videos",
      "authors": [
        "K Garg",
        "S Nayar"
      ],
      "year": "2004",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "19",
      "title": "When does a camera see rain?",
      "authors": [
        "K Garg",
        "S Nayar"
      ],
      "year": "2005",
      "venue": "IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "20",
      "title": "Photorealistic rendering of rain streaks",
      "authors": [
        "K Garg",
        "S Nayar"
      ],
      "year": "2006",
      "venue": "ACM Transactions on Graphics (SIGGRAPH)"
    },
    {
      "citation_id": "21",
      "title": "Vision and rain",
      "authors": [
        "K Garg",
        "S Nayar"
      ],
      "year": "2007",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "22",
      "title": "Vision meets robotics: The KITTI dataset",
      "authors": [
        "A Geiger",
        "P Lenz",
        "C Stiller",
        "R Urtasun"
      ],
      "year": "2013",
      "venue": "International Journal of Robotics Research"
    },
    {
      "citation_id": "23",
      "title": "Are we ready for autonomous driving? The KITTI vision benchmark suite",
      "authors": [
        "A Geiger",
        "P Lenz",
        "R Urtasun"
      ],
      "year": "2012",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "24",
      "title": "Unsupervised monocular depth estimation with left-right consistency",
      "authors": [
        "C Godard",
        "O Mac Aodha",
        "G Brostow"
      ],
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "25",
      "title": "Digging into self-supervised monocular depth estimation",
      "authors": [
        "C Godard",
        "O Mac Aodha",
        "M Firman",
        "G Brostow"
      ],
      "year": "2019",
      "venue": "IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "26",
      "title": "Pixelaccurate depth evaluation in realistic driving scenarios",
      "authors": [
        "T Gruber",
        "M Bijelic",
        "F Heide",
        "W Ritter",
        "K Dietmayer"
      ],
      "year": "2019",
      "venue": "International Conference on 3D Vision"
    },
    {
      "citation_id": "27",
      "title": "Physics-based rendering for improving robustness to rain",
      "authors": [
        "S Halder",
        "J Lalonde",
        "R Charette"
      ],
      "year": "2019",
      "venue": "IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "28",
      "title": "Raindrop detection on car windshields using geometric-photometric environment construction and intensitybased correlation",
      "authors": [
        "J Halimeh",
        "M Roser"
      ],
      "year": "2009",
      "venue": "IEEE Intelligent Vehicles Symposium"
    },
    {
      "citation_id": "29",
      "title": "Learning from synthetic photorealistic raindrop for single image raindrop removal",
      "authors": [
        "Z Hao",
        "S You",
        "Y Li",
        "K Li",
        "F Lu"
      ],
      "year": "2019",
      "venue": "IEEE International Conference on Computer Vision Workshops"
    },
    {
      "citation_id": "30",
      "title": "Deep sky modeling for single image outdoor lighting estimation",
      "authors": [
        "Y Hold-Geoffroy",
        "A Athawale",
        "J Lalonde"
      ],
      "year": "2019",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "31",
      "title": "Deep outdoor illumination estimation",
      "authors": [
        "Y Hold-Geoffroy",
        "K Sunkavalli",
        "S Hadap",
        "E Gambaretto",
        "J Lalonde"
      ],
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "32",
      "title": "Robot vision",
      "authors": [
        "B Horn",
        "B Klaus",
        "P Horn"
      ],
      "year": "1986",
      "venue": "Robot vision"
    },
    {
      "citation_id": "33",
      "title": "Multimodal unsupervised image-to-image translation",
      "authors": [
        "X Huang",
        "M Liu",
        "S Belongie",
        "J Kautz"
      ],
      "year": "2018",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "34",
      "title": "Image-to-image translation with conditional adversarial networks",
      "authors": [
        "P Isola",
        "J Zhu",
        "T Zhou",
        "A Efros"
      ],
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "35",
      "title": "Consistent temporal variations in many outdoor scenes",
      "authors": [
        "N Jacobs",
        "N Roman",
        "R Pless"
      ],
      "year": "2007",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "36",
      "title": "Sparse and dense data with CNNs: Depth completion and semantic segmentation",
      "authors": [
        "M Jaritz",
        "R De Charette",
        "E Wirbel",
        "X Perrotton",
        "F Nashashibi"
      ],
      "year": "2018",
      "venue": "International Conference on 3D Vision"
    },
    {
      "citation_id": "37",
      "title": "Perceptual losses for real-time style transfer and super-resolution",
      "authors": [
        "J Johnson",
        "A Alahi",
        "L Fei-Fei"
      ],
      "year": "2016",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "38",
      "title": "Driving in the matrix: Can virtual worlds replace human-generated annotations for real world tasks",
      "authors": [
        "M Johnson-Roberson",
        "C Barto",
        "R Mehta",
        "S Sridhar",
        "K Rosaen",
        "R Vasudevan"
      ],
      "year": "2016",
      "venue": "International Conference on Robotics and Automation"
    },
    {
      "citation_id": "39",
      "title": "Procsy: Procedural synthetic dataset generation towards influence factor studies of semantic segmentation networks",
      "authors": [
        "S Khan",
        "B Phan",
        "R Salay",
        "K Czarnecki"
      ],
      "year": "2019",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "40",
      "title": "Transient attributes for high-level understanding and editing of outdoor scenes",
      "authors": [
        "P Laffont",
        "Z Ren",
        "X Tao",
        "C Qian",
        "J Hays"
      ],
      "year": "2014",
      "venue": "ACM Transactions on Graphics (SIGGRAPH)"
    },
    {
      "citation_id": "41",
      "title": "Webcam clip art: Appearance and illuminant transfer from time-lapse sequences",
      "authors": [
        "J Lalonde",
        "A Efros",
        "S Narasimhan"
      ],
      "year": "2009",
      "venue": "ACM Transactions on Graphics (SIGGRAPH)"
    },
    {
      "citation_id": "42",
      "title": "Photorealistic single image super-resolution using a generative adversarial network",
      "authors": [
        "C Ledig",
        "L Theis",
        "F Huszár",
        "J Caballero",
        "A Cunningham",
        "A Acosta",
        "A Aitken",
        "A Tejani",
        "J Totz",
        "Z Wang"
      ],
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "43",
      "title": "From big to small: Multi-scale local planar guidance for monocular depth estimation",
      "authors": [
        "J Lee",
        "M Han",
        "D Ko",
        "I Suh"
      ],
      "year": "2020",
      "venue": "From big to small: Multi-scale local planar guidance for monocular depth estimation",
      "arxiv": "arXiv:1907.10326v5"
    },
    {
      "citation_id": "44",
      "title": "Semantic-aware gradgan for virtual-to-real urban scene adaption",
      "authors": [
        "P Li",
        "X Liang",
        "D Jia",
        "E Xing"
      ],
      "year": "2018",
      "venue": "Semantic-aware gradgan for virtual-to-real urban scene adaption",
      "arxiv": "arXiv:1801.01726"
    },
    {
      "citation_id": "45",
      "title": "Robust optical flow in rainy scenes",
      "authors": [
        "R Li",
        "R Tan",
        "L Cheong"
      ],
      "year": "2018",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "46",
      "title": "Rain streak removal using layer priors",
      "authors": [
        "Y Li",
        "R Tan",
        "X Guo",
        "J Lu",
        "M Brown"
      ],
      "year": "2016",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "47",
      "title": "Unsupervised image-to-image translation networks",
      "authors": [
        "M Liu",
        "T Breuel",
        "J Kautz"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "48",
      "title": "Few-shot unsupervised image-to-image translation",
      "authors": [
        "M Liu",
        "X Huang",
        "A Mallya",
        "T Karras",
        "T Aila",
        "J Lehtinen",
        "J Kautz"
      ],
      "year": "2019",
      "venue": "Few-shot unsupervised image-to-image translation"
    },
    {
      "citation_id": "49",
      "title": "SSD: Single shot multibox detector",
      "authors": [
        "W Liu",
        "D Anguelov",
        "D Erhan",
        "C Szegedy",
        "S Reed",
        "C Fu"
      ],
      "year": "2016",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "50",
      "title": "Dual residual networks leveraging the potential of paired operations for image restoration",
      "authors": [
        "X Liu",
        "M Suganuma",
        "Z Sun",
        "T Okatani"
      ],
      "year": "2019",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "51",
      "title": "Removing rain from a single image via discriminative sparse coding",
      "authors": [
        "Y Luo",
        "Y Xu",
        "H Ji"
      ],
      "year": "2015",
      "venue": "IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "52",
      "title": "1 Year, 1000km: The Oxford RobotCar Dataset",
      "authors": [
        "W Maddern",
        "G Pascoe",
        "C Linegar",
        "P Newman"
      ],
      "year": "2017",
      "venue": "International Journal of Robotics Research"
    },
    {
      "citation_id": "53",
      "title": "The distribution of raindrops with size",
      "authors": [
        "J Marshall",
        "W Palmer"
      ],
      "year": "1948",
      "venue": "Journal of meteorology"
    },
    {
      "citation_id": "54",
      "title": "Vision and the atmosphere",
      "authors": [
        "S Narasimhan",
        "S Nayar"
      ],
      "year": "2002",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "55",
      "title": "All the images of an outdoor scene",
      "authors": [
        "S Narasimhan",
        "C Wang",
        "S Nayar"
      ],
      "year": "2002",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "56",
      "title": "Model-based occlusions disentanglement for image-to-image translation",
      "authors": [
        "F Pizzati",
        "P Cerri",
        "R De Charette"
      ],
      "year": "2020",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "57",
      "title": "I can see clearly now: Image restoration via de-raining",
      "authors": [
        "H Porav",
        "T Bruls",
        "P Newman"
      ],
      "year": "2019",
      "venue": "IEEE International Conference on Robotics and Automation"
    },
    {
      "citation_id": "58",
      "title": "A lens and aperture camera model for synthetic image generation",
      "authors": [
        "M Potmesil",
        "I Chakravarty"
      ],
      "year": "1981",
      "venue": "ACM Transactions on Graphics (SIGGRAPH)"
    },
    {
      "citation_id": "59",
      "title": "YOLO9000: Better, faster, stronger",
      "authors": [
        "J Redmon",
        "A Farhadi"
      ],
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "60",
      "title": "Faster R-CNN: Towards realtime object detection with region proposal networks",
      "authors": [
        "S Ren",
        "K He",
        "R Girshick",
        "J Sun"
      ],
      "year": "2015",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "61",
      "title": "ERFNet: Efficient residual factorized convnet for real-time semantic segmentation",
      "authors": [
        "E Romera",
        "J Alvarez",
        "L Bergasa",
        "R Arroyo"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Intelligent Transportation Systems"
    },
    {
      "citation_id": "62",
      "title": "Video-based raindrop detection for improved image registration",
      "authors": [
        "M Roser",
        "A Geiger"
      ],
      "year": "2009",
      "venue": "IEEE International Conference on Computer Vision Workshops"
    },
    {
      "citation_id": "63",
      "title": "Realistic modeling of water droplets for monocular adherent raindrop recognition using bezier curves",
      "authors": [
        "M Roser",
        "J Kurz",
        "A Geiger"
      ],
      "year": "2010",
      "venue": "Asian Conference on Computer Vision"
    },
    {
      "citation_id": "64",
      "title": "Realistic real-time rain rendering",
      "authors": [
        "P Rousseau",
        "V Jolivet",
        "D Ghazanfarpour"
      ],
      "year": "2006",
      "venue": "Computers & Graphics"
    },
    {
      "citation_id": "65",
      "title": "ESPNet: Efficient spatial pyramid of dilated convolutions for semantic segmentation",
      "authors": [
        "Sachin Mehta",
        "M Caspi",
        "A Shapiro",
        "L Hajishirzi"
      ],
      "year": "2018",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "66",
      "title": "Semantic foggy scene understanding with synthetic data",
      "authors": [
        "C Sakaridis",
        "D Dai",
        "L Van Gool"
      ],
      "year": "2018",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "67",
      "title": "DSOD: Learning deeply supervised object detectors from scratch",
      "authors": [
        "Z Shen",
        "Z Liu",
        "J Li",
        "Y Jiang",
        "Y Chen",
        "X Xue"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "68",
      "title": "Semi2i: Semantically consistent image-to-image translation for domain adaptation of remote sensing data",
      "authors": [
        "O Tasar",
        "S Happy",
        "Y Tarabalka",
        "P Alliez"
      ],
      "year": "2020",
      "venue": "Semi2i: Semantically consistent image-to-image translation for domain adaptation of remote sensing data",
      "arxiv": "arXiv:2002.05925"
    },
    {
      "citation_id": "69",
      "title": "Artist-directable real-time rain rendering in city environments",
      "authors": [
        "N Tatarchuk"
      ],
      "year": "2006",
      "venue": "SIGGRAPH 2006 Courses"
    },
    {
      "citation_id": "70",
      "title": "Learning to adapt structured output space for semantic segmentation",
      "authors": [
        "Y Tsai",
        "W Hung",
        "S Schulter",
        "K Sohn",
        "M Yang",
        "M Chandraker"
      ],
      "year": "2018",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "71",
      "title": "A multiscale model for rain rendering in real-time",
      "authors": [
        "Y Weber",
        "V Jolivet",
        "G Gilet",
        "D Ghazanfarpour"
      ],
      "year": "2015",
      "venue": "Computers & Graphics"
    },
    {
      "citation_id": "72",
      "title": "tempogan: A temporally coherent, volumetric gan for super-resolution fluid flow",
      "authors": [
        "Y Xie",
        "E Franz",
        "M Chu",
        "N Thuerey"
      ],
      "year": "2018",
      "venue": "ACM Transactions on Graphics (SIGGRAPH)"
    },
    {
      "citation_id": "73",
      "title": "Exploit all the layers: Fast and accurate cnn object detector with scale dependent pooling and cascaded rejection classifiers",
      "authors": [
        "F Yang",
        "W Choi",
        "Y Lin"
      ],
      "year": "2016",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "74",
      "title": "Deep joint rain detection and removal from a single image",
      "authors": [
        "W Yang",
        "R Tan",
        "J Feng",
        "J Liu",
        "Z Guo",
        "S Yan"
      ],
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "75",
      "title": "Dualgan: Unsupervised dual learning for image-to-image translation",
      "authors": [
        "Z Yi",
        "H Zhang",
        "P Tan",
        "M Gong"
      ],
      "year": "2017",
      "venue": "Dualgan: Unsupervised dual learning for image-to-image translation"
    },
    {
      "citation_id": "76",
      "title": "Bdd100k: A diverse driving video database with scalable annotation tooling",
      "authors": [
        "F Yu",
        "W Xian",
        "Y Chen",
        "F Liu",
        "M Liao",
        "V Madhavan",
        "T Darrell"
      ],
      "year": "2018",
      "venue": "Bdd100k: A diverse driving video database with scalable annotation tooling",
      "arxiv": "arXiv:1805.04687"
    },
    {
      "citation_id": "77",
      "title": "Wilddash-creating hazard-aware benchmarks",
      "authors": [
        "O Zendel",
        "K Honauer",
        "M Murschitz",
        "D Steininger",
        "G Fernandez Dominguez"
      ],
      "year": "2018",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "78",
      "title": "Density-aware single image de-raining using a multi-stream dense network",
      "authors": [
        "H Zhang",
        "V Patel"
      ],
      "year": "2018",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "79",
      "title": "Image de-raining using a conditional generative adversarial network",
      "authors": [
        "H Zhang",
        "V Sindagi",
        "V Patel"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "80",
      "title": "All-weather deep outdoor lighting estimation",
      "authors": [
        "J Zhang",
        "K Sunkavalli",
        "Y Hold-Geoffroy",
        "S Hadap",
        "J Eisenman",
        "J Lalonde"
      ],
      "year": "2019",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "81",
      "title": "ICNet for real-time semantic segmentation on high-resolution images",
      "authors": [
        "H Zhao",
        "X Qi",
        "X Shen",
        "J Shi",
        "J Jia"
      ],
      "year": "2018",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "82",
      "title": "Pyramid scene parsing network",
      "authors": [
        "H Zhao",
        "J Shi",
        "X Qi",
        "X Wang",
        "J Jia"
      ],
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "83",
      "title": "Unpaired image-to-image translation using cycle-consistent adversarial networks",
      "authors": [
        "J Zhu",
        "T Park",
        "P Isola",
        "A Efros"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Computer Vision"
    }
  ]
}