{
  "paper_id": "2403.13731v2",
  "title": "Emotion Recognition Using Transformers With Masked Learning",
  "published": "2024-03-19T12:26:53Z",
  "authors": [
    "Seongjae Min",
    "Junseok Yang",
    "Sangjun Lim",
    "Junyong Lee",
    "Sangwon Lee",
    "Sejoon Lim"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In recent years, deep learning has achieved innovative advancements in various fields, including the analysis of human emotions and behaviors. Initiatives such as the Affective Behavior Analysis in-the-wild (ABAW) competition have been particularly instrumental in driving research in this area by providing diverse and challenging datasets that enable precise evaluation of complex emotional states. This study leverages the Vision Transformer (ViT) and Transformer models to focus on the estimation of Valence-Arousal (VA), which signifies the positivity and intensity of emotions, recognition of various facial expressions, and detection of Action Units (AU) representing fundamental muscle movements. This approach transcends traditional Convolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) based methods, proposing a new Transformerbased framework that maximizes the understanding of temporal and spatial features. The core contributions of this research include the introduction of a learning technique through random frame masking and the application of Focal loss adapted for imbalanced data, enhancing the accuracy and applicability of emotion and behavior analysis in real-world settings. This approach is expected to contribute to the advancement of emotional computing and deep learning methodologies. The code is here.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Recently, deep learning has undergone significant changes in various fields such as computer vision, natural language processing, and especially in analyzing human emotions * Sejoon Lim is the corresponding author. and behaviors. One of the key developments in this field is the Affective Behavior Analysis in-the-wild (ABAW) competition held by Kollias et al.  [6] [7] [8] [9] [10] [11] [12] [13] [14] [15] [16] [17] 23]  These competitions facilitate research by providing diverse and challenging datasets such as AffWild2, C-EXPR-DB, and Hume-Vidmimic2, encouraging the development of models capable of accurately assessing complex emotional states. These models provide keys through Valence-Arousal (VA) estimation, facial expression recognition, and Action Unit (AU) detection, which are essential components in understanding human emotions.\n\nIn the field of emotional analysis, VA estimation provides the foundation by quantifying the positivity (Valence) and intensity (Arousal) of emotions, while facial expression recognition focuses on classifying facial expressions into distinct emotions. Furthermore, Action Unit detection emphasizes identifying the basic muscle movements that constitute these expressions, offering finer details in interpreting emotional states.\n\nRecent studies have embraced various deep learning approaches, including Convolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM), achieving notable success. Additionally, the emergence of transformer models has introduced a new paradigm in understanding temporal and spatial features, expanding the limits of how machines can interpret human emotions and states.\n\nThis research builds on these advancements, proposing a new learning framework that utilizes temporally ordered pairs of masked features derived from facial expressions, Action Units, and valence-arousal indicators. By integrating advancements in feature extraction and sequence modeling, we aim to refine the accuracy and applicability of emotional and behavioral analysis in real-world environments and contribute to the evolving landscape of emotion computing and deep learning methodologies.\n\nThe main contributions of this study are as follows:\n\n• Introduction of random frame masking learning technique: This study proposes a new learning method that improves the generalization ability of emotion recognition models by randomly masking selected frames.\n\n• Application of Focal loss to imbalanced data: By using Focal loss, we have significantly improved the performance of the model in addressing the imbalance problem in facial expression recognition and Action Unit detection.\n\nThe advancement of deep learning has brought significant changes to the study of human emotional behavior as well. The Affective Behavior Analysis in-the-wild (ABAW) competition has been a tremendous contribution to driving such needed changes and pushing the field forward. ABAW provides a wide variety of datasets, including Aff-Wild2 and C-EXPR-DB, for challenges and research opportunities. In this direction, apart from the Hume-Vidmimic2 dataset, it proposes a challenge with a number of tasks.\n\nValence-Arousal Estimation is a type of emotional analysis that gives an emphasis on forecasting the Valence and Arousal of the persons. Valence is referred to as a characteristic of either positivity or negativity of the emotions. An increase in Valence will symbolize an increase in positive emotion, while a reduction in Valence will show negative emotions. Greater Arousal would indicate that the emotions were more actively energized, while lesser Arousal would mean that the emotions were cool and composed. Recent studies have been doing quite well with performance in  [20]  using CNNs and LSTMs. Some recent progress has been reported in the application of transformer models as well.\n\nThe task for Expression Recognition is a mutually exclusive class recognition problem. Each frame of the video should be classified to one of the defined categories: Neutral, Anger, Disgust, Fear, Happiness, Sadness, Surprise, Other. The research has been carried out using visual and audio information where there exists, to a greater extent, emotional content. References include  [24] [25] [26] . Nguyen et al.  [19]  proposed to use only images, and for each of them, a feature vector is extracted using a pretrained network and then supplied to a transformer encoder.\n\nIn Action Unit Recognition, the determination of specific Action Units (AU) based on the human face's features is done in every frame of the video. It requires facial motion analysis down to the last detail. Yu et al.  [22]  proposed a feature fusion module based on self-attention, which is responsible for integrating overall facial characteristic and relationship feature between AUs. Zhang et al.  [25]  and Wang et al.  [21]  initialized a Masked Autoencoder This enabled the extraction of various general features associated with the face.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Approach",
      "text": "In this paper, we suggest a network that can learn human expressions, action units (AU), valence-arousal (VA), and temporally masked features for each frame. So, the first step is the feature extraction for each of the input images. Section 2.1 details the feature extraction step. After the extraction of the features has been acquired, they are masked randomly, put together into temporal pairs, and finally input into the transformer encoder. This is followed by an FC layer to produce the final output. we describe the functioning principle of the transformer classifier module at 2.2. Section 2.3 describes the loss function used for learning. In Figure  1 , we show the schema of our whole network.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Feature Extractor",
      "text": "Pretrained Vision Transformer (ViT)  [4]  network in order to extract useful features Instead of using the 'cls' token from ViT's final output in the conventional manner, we apply average pooling to the output of the last layer based on the method put forth in  [1] . This saves computational power required during training time by pre-extracting features for the Aff-Wild2 dataset. Instead, the use of a large-scale pretrained network allows for the extraction of generalized representations better adapted to the diverse contexts of the image and enhancement of its ability in both processing and analyzing the input image's complex emotional expressions and related action units.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Transformer Classifier",
      "text": "Masked inputs in the Transformer model have been validated in different parts of the Transformer Classifier: GPT  [2] , Bert  [3] , MAE  [5]  Motivated from the works discussed above, we propose to design a Transformer Classifier with features processed in the order of time and an input mask. The proposed encoder is designed to realize the self-attention mechanism that can process efficient sequences of image data. However, this approach improves to a great extent the knowledge of changes in facial expression in a temporal image sequence, something very important in the correct recognition of emotion and AU. During learning, the temporal feature pairs are given as input with a certain probability p by making them partially masked beforehand. This ensures that overfitting is totally avoided and, in turn, increases the generalization performance.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Loss Function",
      "text": "For AU and Expression, Focal loss  [18] , which is strong against the imbalanced distribution of data. Focal loss performs very well while learning the model in severe class imbalanced datasets. It is defined as follows: To avert the risk of overfitting, these extracted features from each frame are randomly masked. In the final step, a transformer classifier sequentially processes these randomly masked frame features to predict the outcome ŷ\n\nHere, p t denotes the predicted probability, and α and γ are tuning parameters. These hyperparameters assign more importance to hard samples while reducing their importance for easier ones, so that the model gets to focus more on the part it struggles with in the learning process. This, in turn, is a performance booster for the focal loss on imbalanced datasets.\n\nFor VA measurement, CCC (Concordance Correlation Coefficient) loss was used. It is computed as follows:\n\nHere, ρ is the correlation coefficient between the two variables, and σ xy , σ 2\n\nx , σ 2 y represent their respective averages. The CCC loss function measures the concordance between the predicted and actual values, making it a suitable loss function for predicting emotional states. The foregoing greater importance to the difficult samples and reducing the importance of easy samples allows the model to focus more on parts that should be more concentrating in the learning process. Then, Focal loss will be the best function that will substantially improve the performance in a highly imbalanced dataset. CCC loss function, on the other hand, is best used in predicting emotional states because it gives a way that makes it possible to quantify the agreement between the predicted and the target values.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Setup",
      "text": "In the current study, the researcher used the ImageNet21k and Aff-Wild2 datasets. ImageNet21k refers to a largescale dataset that consists of approximately 21,000 classes and has around 14 million images, which was used in pretraining the feature extractor. The Aff-Wild2 input model was only used if the cropped image was available and was utilized for the Transformer Encoder training.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Implementation",
      "text": "The feature extractor uses ViT Base. The Transformer Classifier utilizes 8 heads, 6 layers, and a dropout rate of 0.2. The batch size is set to 512, and the temporal length is set to 100. The optimizer used is AdamW with a learning rate of 0.0001 and a fixed weight decay of 0.001. The parameters for Focal loss are set with alpha at 0.  25",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Results",
      "text": "1 presents the test results of our methodology on the validation set for VA, EXPR, and AU, compared with the Baseline. For VA Estimation, we used the average of the Concordance Correlation Coefficients for Valence and Arousal.\n\nFor EXPR Recognition and AU Detection, we used the F1 Score as the evaluation metric for each, respectively.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , we show the schema of our whole network.",
      "page": 2
    },
    {
      "caption": "Figure 1: illustrates the comprehensive pipeline of the our model. Initially, A pretrained vision transformer individually extracts features",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Kookmin University, Seoul, Korea": "lim@kookmin.ac.kr"
        },
        {
          "Kookmin University, Seoul, Korea": "Abstract"
        },
        {
          "Kookmin University, Seoul, Korea": ""
        },
        {
          "Kookmin University, Seoul, Korea": "In recent years, deep learning has achieved innovative"
        },
        {
          "Kookmin University, Seoul, Korea": "advancements\nin various fields,\nincluding the analysis of"
        },
        {
          "Kookmin University, Seoul, Korea": "human emotions and behaviors.\nInitiatives such as the Af-"
        },
        {
          "Kookmin University, Seoul, Korea": "fective Behavior Analysis in-the-wild (ABAW) competition"
        },
        {
          "Kookmin University, Seoul, Korea": "have been particularly instrumental\nin driving research in"
        },
        {
          "Kookmin University, Seoul, Korea": "this area by providing diverse and challenging datasets that"
        },
        {
          "Kookmin University, Seoul, Korea": "enable precise evaluation of complex emotional states. This"
        },
        {
          "Kookmin University, Seoul, Korea": "study leverages\nthe Vision Transformer\n(ViT) and Trans-"
        },
        {
          "Kookmin University, Seoul, Korea": "former models to focus on the estimation of Valence-Arousal"
        },
        {
          "Kookmin University, Seoul, Korea": "(VA), which signifies\nthe positivity and intensity of emo-"
        },
        {
          "Kookmin University, Seoul, Korea": "tions, recognition of various facial expressions, and detec-"
        },
        {
          "Kookmin University, Seoul, Korea": "tion of Action Units (AU) representing fundamental muscle"
        },
        {
          "Kookmin University, Seoul, Korea": "movements. This approach transcends traditional Convolu-"
        },
        {
          "Kookmin University, Seoul, Korea": "tional Neural Networks (CNNs) and Long Short-Term Mem-"
        },
        {
          "Kookmin University, Seoul, Korea": "ory (LSTM) based methods, proposing a new Transformer-"
        },
        {
          "Kookmin University, Seoul, Korea": "based framework that maximizes the understanding of tem-"
        },
        {
          "Kookmin University, Seoul, Korea": "poral and spatial\nfeatures.\nThe core contributions of\nthis"
        },
        {
          "Kookmin University, Seoul, Korea": "research include the introduction of a learning technique"
        },
        {
          "Kookmin University, Seoul, Korea": ""
        },
        {
          "Kookmin University, Seoul, Korea": "through random frame masking and the application of Fo-"
        },
        {
          "Kookmin University, Seoul, Korea": ""
        },
        {
          "Kookmin University, Seoul, Korea": "cal\nloss adapted for\nimbalanced data, enhancing the ac-"
        },
        {
          "Kookmin University, Seoul, Korea": ""
        },
        {
          "Kookmin University, Seoul, Korea": "curacy and applicability of emotion and behavior analy-"
        },
        {
          "Kookmin University, Seoul, Korea": ""
        },
        {
          "Kookmin University, Seoul, Korea": "sis\nin real-world settings.\nThis approach is expected to"
        },
        {
          "Kookmin University, Seoul, Korea": ""
        },
        {
          "Kookmin University, Seoul, Korea": "contribute to the advancement of emotional computing and"
        },
        {
          "Kookmin University, Seoul, Korea": ""
        },
        {
          "Kookmin University, Seoul, Korea": "https:\ndeep learning methodologies. The code is here."
        },
        {
          "Kookmin University, Seoul, Korea": ""
        },
        {
          "Kookmin University, Seoul, Korea": "//github.com/msjae/ABAW"
        },
        {
          "Kookmin University, Seoul, Korea": ""
        },
        {
          "Kookmin University, Seoul, Korea": ""
        },
        {
          "Kookmin University, Seoul, Korea": ""
        },
        {
          "Kookmin University, Seoul, Korea": "1. Introduction"
        },
        {
          "Kookmin University, Seoul, Korea": ""
        },
        {
          "Kookmin University, Seoul, Korea": ""
        },
        {
          "Kookmin University, Seoul, Korea": "Recently, deep learning has undergone significant changes"
        },
        {
          "Kookmin University, Seoul, Korea": ""
        },
        {
          "Kookmin University, Seoul, Korea": "in various fields such as computer vision, natural\nlanguage"
        },
        {
          "Kookmin University, Seoul, Korea": ""
        },
        {
          "Kookmin University, Seoul, Korea": "processing,\nand especially in analyzing human emotions"
        },
        {
          "Kookmin University, Seoul, Korea": ""
        },
        {
          "Kookmin University, Seoul, Korea": "*Sejoon Lim is the corresponding author."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "The main contributions of this study are as follows:": "•\nIntroduction\nof\nrandom frame masking\nlearning\ntech-",
          "2. Approach": ""
        },
        {
          "The main contributions of this study are as follows:": "",
          "2. Approach": "In this paper, we suggest a network that can learn human"
        },
        {
          "The main contributions of this study are as follows:": "nique: This study proposes a new learning method that",
          "2. Approach": ""
        },
        {
          "The main contributions of this study are as follows:": "",
          "2. Approach": "expressions, action units (AU), valence-arousal\n(VA), and"
        },
        {
          "The main contributions of this study are as follows:": "improves the generalization ability of emotion recogni-",
          "2. Approach": ""
        },
        {
          "The main contributions of this study are as follows:": "",
          "2. Approach": "temporally masked features\nfor each frame.\nSo,\nthe first"
        },
        {
          "The main contributions of this study are as follows:": "tion models by randomly masking selected frames.",
          "2. Approach": ""
        },
        {
          "The main contributions of this study are as follows:": "",
          "2. Approach": "step is the feature extraction for each of the input\nimages."
        },
        {
          "The main contributions of this study are as follows:": "",
          "2. Approach": "Section 2.1 details the feature extraction step. After the ex-"
        },
        {
          "The main contributions of this study are as follows:": "• Application of Focal\nloss to imbalanced data: By using",
          "2. Approach": ""
        },
        {
          "The main contributions of this study are as follows:": "",
          "2. Approach": "traction of the features has been acquired,\nthey are masked"
        },
        {
          "The main contributions of this study are as follows:": "Focal\nloss, we have significantly improved the perfor-",
          "2. Approach": ""
        },
        {
          "The main contributions of this study are as follows:": "",
          "2. Approach": "randomly, put\ntogether\ninto temporal pairs, and finally in-"
        },
        {
          "The main contributions of this study are as follows:": "mance of the model in addressing the imbalance problem",
          "2. Approach": ""
        },
        {
          "The main contributions of this study are as follows:": "",
          "2. Approach": "put into the transformer encoder. This is followed by an FC"
        },
        {
          "The main contributions of this study are as follows:": "in facial expression recognition and Action Unit detec-",
          "2. Approach": ""
        },
        {
          "The main contributions of this study are as follows:": "",
          "2. Approach": "layer to produce the final output. we describe the function-"
        },
        {
          "The main contributions of this study are as follows:": "tion.",
          "2. Approach": ""
        },
        {
          "The main contributions of this study are as follows:": "",
          "2. Approach": "ing principle of\nthe transformer classifier module at\n2.2."
        },
        {
          "The main contributions of this study are as follows:": "The advancement of deep learning has brought signifi-",
          "2. Approach": "Section\n2.3 describes the loss function used for\nlearning."
        },
        {
          "The main contributions of this study are as follows:": "cant changes to the study of human emotional behavior as",
          "2. Approach": "In Figure\n1, we show the schema of our whole network."
        },
        {
          "The main contributions of this study are as follows:": "well. The Affective Behavior Analysis in-the-wild (ABAW)",
          "2. Approach": ""
        },
        {
          "The main contributions of this study are as follows:": "",
          "2. Approach": "2.1. Feature Extractor"
        },
        {
          "The main contributions of this study are as follows:": "competition has been a tremendous contribution to driving",
          "2. Approach": ""
        },
        {
          "The main contributions of this study are as follows:": "such needed changes and pushing the field forward. ABAW",
          "2. Approach": ""
        },
        {
          "The main contributions of this study are as follows:": "",
          "2. Approach": "Pretrained Vision Transformer (ViT) [4] network in order to"
        },
        {
          "The main contributions of this study are as follows:": "provides a wide variety of datasets,\nincluding Aff-Wild2",
          "2. Approach": ""
        },
        {
          "The main contributions of this study are as follows:": "",
          "2. Approach": "extract useful features Instead of using the ’cls’ token from"
        },
        {
          "The main contributions of this study are as follows:": "and C-EXPR-DB,\nfor challenges and research opportuni-",
          "2. Approach": ""
        },
        {
          "The main contributions of this study are as follows:": "",
          "2. Approach": "ViT’s final output in the conventional manner, we apply av-"
        },
        {
          "The main contributions of this study are as follows:": "ties.\nIn this direction,\napart\nfrom the Hume-Vidmimic2",
          "2. Approach": ""
        },
        {
          "The main contributions of this study are as follows:": "",
          "2. Approach": "erage pooling to the output of\nthe last\nlayer based on the"
        },
        {
          "The main contributions of this study are as follows:": "dataset, it proposes a challenge with a number of tasks.",
          "2. Approach": ""
        },
        {
          "The main contributions of this study are as follows:": "",
          "2. Approach": "method put forth in\n[1]. This saves computational power"
        },
        {
          "The main contributions of this study are as follows:": "Valence-Arousal Estimation is a type of emotional anal-",
          "2. Approach": "required during training time by pre-extracting features for"
        },
        {
          "The main contributions of this study are as follows:": "ysis that gives an emphasis on forecasting the Valence and",
          "2. Approach": "the Aff-Wild2 dataset.\nInstead, the use of a large-scale pre-"
        },
        {
          "The main contributions of this study are as follows:": "Arousal of the persons. Valence is referred to as a charac-",
          "2. Approach": "trained network allows for the extraction of generalized rep-"
        },
        {
          "The main contributions of this study are as follows:": "teristic of either positivity or negativity of the emotions. An",
          "2. Approach": "resentations better adapted to the diverse contexts of the im-"
        },
        {
          "The main contributions of this study are as follows:": "increase in Valence will symbolize an increase in positive",
          "2. Approach": "age and enhancement of\nits ability in both processing and"
        },
        {
          "The main contributions of this study are as follows:": "emotion, while a reduction in Valence will show negative",
          "2. Approach": "analyzing the input image’s complex emotional expressions"
        },
        {
          "The main contributions of this study are as follows:": "emotions. Greater Arousal would indicate that the emotions",
          "2. Approach": "and related action units."
        },
        {
          "The main contributions of this study are as follows:": "were more actively energized, while lesser Arousal would",
          "2. Approach": ""
        },
        {
          "The main contributions of this study are as follows:": "",
          "2. Approach": "2.2. Transformer Classifier"
        },
        {
          "The main contributions of this study are as follows:": "mean that\nthe emotions were cool and composed. Recent",
          "2. Approach": ""
        },
        {
          "The main contributions of this study are as follows:": "studies have been doing quite well with performance in [20]",
          "2. Approach": ""
        },
        {
          "The main contributions of this study are as follows:": "",
          "2. Approach": "Masked inputs in the Transformer model have been vali-"
        },
        {
          "The main contributions of this study are as follows:": "using CNNs and LSTMs.\nSome recent progress has been",
          "2. Approach": ""
        },
        {
          "The main contributions of this study are as follows:": "",
          "2. Approach": "dated in different parts of the Transformer Classifier: GPT"
        },
        {
          "The main contributions of this study are as follows:": "reported in the application of transformer models as well.",
          "2. Approach": ""
        },
        {
          "The main contributions of this study are as follows:": "",
          "2. Approach": "[2], Bert\n[3], MAE\n[5] Motivated from the works dis-"
        },
        {
          "The main contributions of this study are as follows:": "The task for Expression Recognition is a mutually ex-",
          "2. Approach": "cussed above, we propose to design a Transformer Clas-"
        },
        {
          "The main contributions of this study are as follows:": "clusive class recognition problem. Each frame of the video",
          "2. Approach": "sifier with features processed in the order of\ntime and an"
        },
        {
          "The main contributions of this study are as follows:": "should be classified to one of the defined categories: Neu-",
          "2. Approach": "input mask.\nThe proposed encoder\nis designed to realize"
        },
        {
          "The main contributions of this study are as follows:": "tral, Anger, Disgust, Fear, Happiness, Sadness, Surprise,",
          "2. Approach": "the self-attention mechanism that can process efficient se-"
        },
        {
          "The main contributions of this study are as follows:": "Other. The research has been carried out using visual and",
          "2. Approach": "quences of image data. However, this approach improves to"
        },
        {
          "The main contributions of this study are as follows:": "audio information where there exists,\nto a greater extent,",
          "2. Approach": "a great extent the knowledge of changes in facial expression"
        },
        {
          "The main contributions of this study are as follows:": "emotional content. References include [24–26]. Nguyen et",
          "2. Approach": "in a temporal image sequence, something very important in"
        },
        {
          "The main contributions of this study are as follows:": "al.[19] proposed to use only images, and for each of them,",
          "2. Approach": "the correct recognition of emotion and AU. During learning,"
        },
        {
          "The main contributions of this study are as follows:": "a feature vector is extracted using a pretrained network and",
          "2. Approach": "the temporal feature pairs are given as input with a certain"
        },
        {
          "The main contributions of this study are as follows:": "then supplied to a transformer encoder.",
          "2. Approach": "probability p by making them partially masked beforehand."
        },
        {
          "The main contributions of this study are as follows:": "",
          "2. Approach": "This ensures that overfitting is totally avoided and,\nin turn,"
        },
        {
          "The main contributions of this study are as follows:": "In Action Unit Recognition,\nthe determination of\nspe-",
          "2. Approach": ""
        },
        {
          "The main contributions of this study are as follows:": "",
          "2. Approach": "increases the generalization performance."
        },
        {
          "The main contributions of this study are as follows:": "cific Action Units (AU) based on the human face’s features",
          "2. Approach": ""
        },
        {
          "The main contributions of this study are as follows:": "is done in every frame of the video.\nIt requires facial mo-",
          "2. Approach": ""
        },
        {
          "The main contributions of this study are as follows:": "",
          "2. Approach": "2.3. Loss function"
        },
        {
          "The main contributions of this study are as follows:": "tion analysis down to the last detail. Yu et al.[22] proposed",
          "2. Approach": ""
        },
        {
          "The main contributions of this study are as follows:": "a feature fusion module based on self-attention, which is re-",
          "2. Approach": "For AU and Expression, Focal\nloss\n[18], which is strong"
        },
        {
          "The main contributions of this study are as follows:": "sponsible for integrating overall facial characteristic and re-",
          "2. Approach": "against\nthe imbalanced distribution of data. Focal\nloss per-"
        },
        {
          "The main contributions of this study are as follows:": "lationship feature between AUs. Zhang et al.[25] and Wang",
          "2. Approach": "forms very well while learning the model\nin severe class"
        },
        {
          "The main contributions of this study are as follows:": "et al.\n[21]\ninitialized a Masked Autoencoder This enabled",
          "2. Approach": "imbalanced datasets. It is defined as follows:"
        },
        {
          "The main contributions of this study are as follows:": "the extraction of various general\nfeatures associated with",
          "2. Approach": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 3: 2.Implementation",
      "data": [
        {
          "Figure 1.\nillustrates the comprehensive pipeline of the our model.\nInitially, A pretrained vision transformer individually extracts features": "from each input\nframe image (where b stands for batch size, and n represents sequential\nlength), ensuring a detailed analysis of every"
        },
        {
          "Figure 1.\nillustrates the comprehensive pipeline of the our model.\nInitially, A pretrained vision transformer individually extracts features": "frame. To avert\nthe risk of overfitting,\nthese extracted features from each frame are randomly masked.\nIn the final step, a transformer"
        },
        {
          "Figure 1.\nillustrates the comprehensive pipeline of the our model.\nInitially, A pretrained vision transformer individually extracts features": "classifier sequentially processes these randomly masked frame features to predict the outcome ˆy"
        },
        {
          "Figure 1.\nillustrates the comprehensive pipeline of the our model.\nInitially, A pretrained vision transformer individually extracts features": "that makes it possible to quantify the agreement between\nHere, pt denotes the predicted probability, and α and γ are"
        },
        {
          "Figure 1.\nillustrates the comprehensive pipeline of the our model.\nInitially, A pretrained vision transformer individually extracts features": "tuning parameters. These hyperparameters assign more im-\nthe predicted and the target values."
        },
        {
          "Figure 1.\nillustrates the comprehensive pipeline of the our model.\nInitially, A pretrained vision transformer individually extracts features": "portance to hard samples while reducing their\nimportance"
        },
        {
          "Figure 1.\nillustrates the comprehensive pipeline of the our model.\nInitially, A pretrained vision transformer individually extracts features": "for easier ones, so that the model gets to focus more on the"
        },
        {
          "Figure 1.\nillustrates the comprehensive pipeline of the our model.\nInitially, A pretrained vision transformer individually extracts features": "3. Experiments"
        },
        {
          "Figure 1.\nillustrates the comprehensive pipeline of the our model.\nInitially, A pretrained vision transformer individually extracts features": "part\nit struggles with in the learning process. This,\nin turn,"
        },
        {
          "Figure 1.\nillustrates the comprehensive pipeline of the our model.\nInitially, A pretrained vision transformer individually extracts features": "is a performance booster\nfor\nthe focal\nloss on imbalanced"
        },
        {
          "Figure 1.\nillustrates the comprehensive pipeline of the our model.\nInitially, A pretrained vision transformer individually extracts features": "3.1. Experimental Setup"
        },
        {
          "Figure 1.\nillustrates the comprehensive pipeline of the our model.\nInitially, A pretrained vision transformer individually extracts features": "datasets."
        },
        {
          "Figure 1.\nillustrates the comprehensive pipeline of the our model.\nInitially, A pretrained vision transformer individually extracts features": "For VA measurement, CCC (Concordance Correlation\nIn the current study,\nthe researcher used the ImageNet21k"
        },
        {
          "Figure 1.\nillustrates the comprehensive pipeline of the our model.\nInitially, A pretrained vision transformer individually extracts features": "Coefficient) loss was used. It is computed as follows:\nand Aff-Wild2 datasets.\nImageNet21k refers\nto a large-"
        },
        {
          "Figure 1.\nillustrates the comprehensive pipeline of the our model.\nInitially, A pretrained vision transformer individually extracts features": "scale dataset\nthat consists of approximately 21,000 classes"
        },
        {
          "Figure 1.\nillustrates the comprehensive pipeline of the our model.\nInitially, A pretrained vision transformer individually extracts features": "2ρσxy"
        },
        {
          "Figure 1.\nillustrates the comprehensive pipeline of the our model.\nInitially, A pretrained vision transformer individually extracts features": "(2)\nLccc = 1 −\nand has around 14 million images, which was used in pre-"
        },
        {
          "Figure 1.\nillustrates the comprehensive pipeline of the our model.\nInitially, A pretrained vision transformer individually extracts features": "σ2\nx + σ2\ny + (¯x − ¯y)2"
        },
        {
          "Figure 1.\nillustrates the comprehensive pipeline of the our model.\nInitially, A pretrained vision transformer individually extracts features": "training the feature extractor. The Aff-Wild2 input model"
        },
        {
          "Figure 1.\nillustrates the comprehensive pipeline of the our model.\nInitially, A pretrained vision transformer individually extracts features": "Here, ρ is\nthe correlation coefficient between the two\nwas only used if the cropped image was available and was"
        },
        {
          "Figure 1.\nillustrates the comprehensive pipeline of the our model.\nInitially, A pretrained vision transformer individually extracts features": "their\nrespective aver-\nutilized for the Transformer Encoder training.\nvariables, and σxy, σ2\nx, σ2\nyrepresent"
        },
        {
          "Figure 1.\nillustrates the comprehensive pipeline of the our model.\nInitially, A pretrained vision transformer individually extracts features": "ages. The CCC loss function measures the concordance be-"
        },
        {
          "Figure 1.\nillustrates the comprehensive pipeline of the our model.\nInitially, A pretrained vision transformer individually extracts features": "tween the predicted and actual values, making it a suitable"
        },
        {
          "Figure 1.\nillustrates the comprehensive pipeline of the our model.\nInitially, A pretrained vision transformer individually extracts features": "3.2. Implementation"
        },
        {
          "Figure 1.\nillustrates the comprehensive pipeline of the our model.\nInitially, A pretrained vision transformer individually extracts features": "loss function for predicting emotional states. The foregoing"
        },
        {
          "Figure 1.\nillustrates the comprehensive pipeline of the our model.\nInitially, A pretrained vision transformer individually extracts features": "greater importance to the difficult samples and reducing the\nThe feature extractor uses ViT Base. The Transformer Clas-"
        },
        {
          "Figure 1.\nillustrates the comprehensive pipeline of the our model.\nInitially, A pretrained vision transformer individually extracts features": "importance of easy samples allows the model to focus more\nsifier utilizes 8 heads, 6 layers, and a dropout\nrate of 0.2."
        },
        {
          "Figure 1.\nillustrates the comprehensive pipeline of the our model.\nInitially, A pretrained vision transformer individually extracts features": "on parts that should be more concentrating in the learning\nThe batch size is set\nto 512, and the temporal\nlength is set"
        },
        {
          "Figure 1.\nillustrates the comprehensive pipeline of the our model.\nInitially, A pretrained vision transformer individually extracts features": "process. Then, Focal loss will be the best function that will\nto 100. The optimizer used is AdamW with a learning rate"
        },
        {
          "Figure 1.\nillustrates the comprehensive pipeline of the our model.\nInitially, A pretrained vision transformer individually extracts features": "substantially improve the performance in a highly imbal-\nof 0.0001 and a fixed weight decay of 0.001. The parame-"
        },
        {
          "Figure 1.\nillustrates the comprehensive pipeline of the our model.\nInitially, A pretrained vision transformer individually extracts features": "anced dataset. CCC loss function, on the other hand, is best\nters for Focal\nloss are set with alpha at 0.25 and gamma at"
        },
        {
          "Figure 1.\nillustrates the comprehensive pipeline of the our model.\nInitially, A pretrained vision transformer individually extracts features": "used in predicting emotional states because it gives a way\n2."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "tive behavior in the second abaw2 competition.\nIn Proceed-": "ings of\nthe IEEE/CVF International Conference on Com-"
        },
        {
          "tive behavior in the second abaw2 competition.\nIn Proceed-": ""
        },
        {
          "tive behavior in the second abaw2 competition.\nIn Proceed-": "puter Vision, pages 3652–3660, 2021."
        },
        {
          "tive behavior in the second abaw2 competition.\nIn Proceed-": "[12] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing"
        },
        {
          "tive behavior in the second abaw2 competition.\nIn Proceed-": "affective behavior\nin the first abaw 2020 competition.\nIn"
        },
        {
          "tive behavior in the second abaw2 competition.\nIn Proceed-": ""
        },
        {
          "tive behavior in the second abaw2 competition.\nIn Proceed-": "2020\n15th\nIEEE International Conference\non Automatic"
        },
        {
          "tive behavior in the second abaw2 competition.\nIn Proceed-": "Face and Gesture Recognition (FG 2020)(FG), pages 794–"
        },
        {
          "tive behavior in the second abaw2 competition.\nIn Proceed-": "800."
        },
        {
          "tive behavior in the second abaw2 competition.\nIn Proceed-": "[13] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos"
        },
        {
          "tive behavior in the second abaw2 competition.\nIn Proceed-": "Zafeiriou.\nFace\nbehavior\na\nla\ncarte:\nExpressions,\naf-"
        },
        {
          "tive behavior in the second abaw2 competition.\nIn Proceed-": "arXiv preprint\nfect and action units\nin a single network."
        },
        {
          "tive behavior in the second abaw2 competition.\nIn Proceed-": ""
        },
        {
          "tive behavior in the second abaw2 competition.\nIn Proceed-": "arXiv:1910.11111, 2019."
        },
        {
          "tive behavior in the second abaw2 competition.\nIn Proceed-": ""
        },
        {
          "tive behavior in the second abaw2 competition.\nIn Proceed-": "[14] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,"
        },
        {
          "tive behavior in the second abaw2 competition.\nIn Proceed-": "Athanasios Papaioannou, Guoying Zhao, Bj¨orn Schuller,"
        },
        {
          "tive behavior in the second abaw2 competition.\nIn Proceed-": ""
        },
        {
          "tive behavior in the second abaw2 competition.\nIn Proceed-": "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Challenge\nMetric\nMethod\nResult": "VA\nCCC\nOurs\n0.32",
          "the IEEE/CVF Conference on Computer Vision and Pattern": "Recognition, pages 5589–5598, 2023."
        },
        {
          "Challenge\nMetric\nMethod\nResult": "(CCCv:0.23, CCCa:0.41)",
          "the IEEE/CVF Conference on Computer Vision and Pattern": "[9] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,"
        },
        {
          "Challenge\nMetric\nMethod\nResult": "Baseline\n0.22",
          "the IEEE/CVF Conference on Computer Vision and Pattern": "action unit\nrecognition: Aff-wild2, multi-task learning and"
        },
        {
          "Challenge\nMetric\nMethod\nResult": "",
          "the IEEE/CVF Conference on Computer Vision and Pattern": "arcface. arXiv preprint arXiv:1910.04855, 2019."
        },
        {
          "Challenge\nMetric\nMethod\nResult": "(CCCv:0.24, CCCa:0.20)",
          "the IEEE/CVF Conference on Computer Vision and Pattern": ""
        },
        {
          "Challenge\nMetric\nMethod\nResult": "",
          "the IEEE/CVF Conference on Computer Vision and Pattern": "[10] Dimitrios Kollias and Stefanos Zafeiriou.\nAffect analysis"
        },
        {
          "Challenge\nMetric\nMethod\nResult": "EXPR\nF1-Score\nOurs\n0.29",
          "the IEEE/CVF Conference on Computer Vision and Pattern": ""
        },
        {
          "Challenge\nMetric\nMethod\nResult": "",
          "the IEEE/CVF Conference on Computer Vision and Pattern": "in-the-wild: Valence-arousal, expressions, action units and a"
        },
        {
          "Challenge\nMetric\nMethod\nResult": "Baseline\n0.25",
          "the IEEE/CVF Conference on Computer Vision and Pattern": ""
        },
        {
          "Challenge\nMetric\nMethod\nResult": "",
          "the IEEE/CVF Conference on Computer Vision and Pattern": "unified framework. arXiv preprint arXiv:2103.15792, 2021."
        },
        {
          "Challenge\nMetric\nMethod\nResult": "AU\nF1-Score\nOurs\n0.40",
          "the IEEE/CVF Conference on Computer Vision and Pattern": ""
        },
        {
          "Challenge\nMetric\nMethod\nResult": "",
          "the IEEE/CVF Conference on Computer Vision and Pattern": "[11] Dimitrios Kollias and Stefanos Zafeiriou. Analysing affec-"
        },
        {
          "Challenge\nMetric\nMethod\nResult": "Baseline\n0.39",
          "the IEEE/CVF Conference on Computer Vision and Pattern": ""
        },
        {
          "Challenge\nMetric\nMethod\nResult": "",
          "the IEEE/CVF Conference on Computer Vision and Pattern": "tive behavior in the second abaw2 competition.\nIn Proceed-"
        },
        {
          "Challenge\nMetric\nMethod\nResult": "",
          "the IEEE/CVF Conference on Computer Vision and Pattern": "ings of\nthe IEEE/CVF International Conference on Com-"
        },
        {
          "Challenge\nMetric\nMethod\nResult": "Table 1. Results on Validation set of Aff-Wild2",
          "the IEEE/CVF Conference on Computer Vision and Pattern": ""
        },
        {
          "Challenge\nMetric\nMethod\nResult": "",
          "the IEEE/CVF Conference on Computer Vision and Pattern": "puter Vision, pages 3652–3660, 2021."
        },
        {
          "Challenge\nMetric\nMethod\nResult": "",
          "the IEEE/CVF Conference on Computer Vision and Pattern": "[12] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing"
        },
        {
          "Challenge\nMetric\nMethod\nResult": "",
          "the IEEE/CVF Conference on Computer Vision and Pattern": "affective behavior\nin the first abaw 2020 competition.\nIn"
        },
        {
          "Challenge\nMetric\nMethod\nResult": "3.3. Results",
          "the IEEE/CVF Conference on Computer Vision and Pattern": ""
        },
        {
          "Challenge\nMetric\nMethod\nResult": "",
          "the IEEE/CVF Conference on Computer Vision and Pattern": "2020\n15th\nIEEE International Conference\non Automatic"
        },
        {
          "Challenge\nMetric\nMethod\nResult": "1 presents the test results of our methodology on the vali-",
          "the IEEE/CVF Conference on Computer Vision and Pattern": "Face and Gesture Recognition (FG 2020)(FG), pages 794–"
        },
        {
          "Challenge\nMetric\nMethod\nResult": "dation set for VA, EXPR, and AU, compared with the Base-",
          "the IEEE/CVF Conference on Computer Vision and Pattern": "800."
        },
        {
          "Challenge\nMetric\nMethod\nResult": "line. For VA Estimation, we used the average of the Con-",
          "the IEEE/CVF Conference on Computer Vision and Pattern": "[13] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos"
        },
        {
          "Challenge\nMetric\nMethod\nResult": "cordance Correlation Coefficients for Valence and Arousal.",
          "the IEEE/CVF Conference on Computer Vision and Pattern": "Zafeiriou.\nFace\nbehavior\na\nla\ncarte:\nExpressions,\naf-"
        },
        {
          "Challenge\nMetric\nMethod\nResult": "",
          "the IEEE/CVF Conference on Computer Vision and Pattern": "arXiv preprint\nfect and action units\nin a single network."
        },
        {
          "Challenge\nMetric\nMethod\nResult": "For EXPR Recognition and AU Detection, we used the F1",
          "the IEEE/CVF Conference on Computer Vision and Pattern": ""
        },
        {
          "Challenge\nMetric\nMethod\nResult": "",
          "the IEEE/CVF Conference on Computer Vision and Pattern": "arXiv:1910.11111, 2019."
        },
        {
          "Challenge\nMetric\nMethod\nResult": "Score as the evaluation metric for each, respectively.",
          "the IEEE/CVF Conference on Computer Vision and Pattern": ""
        },
        {
          "Challenge\nMetric\nMethod\nResult": "",
          "the IEEE/CVF Conference on Computer Vision and Pattern": "[14] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,"
        },
        {
          "Challenge\nMetric\nMethod\nResult": "",
          "the IEEE/CVF Conference on Computer Vision and Pattern": "Athanasios Papaioannou, Guoying Zhao, Bj¨orn Schuller,"
        },
        {
          "Challenge\nMetric\nMethod\nResult": "References",
          "the IEEE/CVF Conference on Computer Vision and Pattern": ""
        },
        {
          "Challenge\nMetric\nMethod\nResult": "",
          "the IEEE/CVF Conference on Computer Vision and Pattern": "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction"
        },
        {
          "Challenge\nMetric\nMethod\nResult": "[1] Lucas Beyer, Xiaohua Zhai, and Alexander Kolesnikov. Bet-",
          "the IEEE/CVF Conference on Computer Vision and Pattern": "in-the-wild: Aff-wild database and challenge, deep architec-"
        },
        {
          "Challenge\nMetric\nMethod\nResult": "arXiv\npreprint\nter\nplain\nvit\nbaselines\nfor\nimagenet-1k.",
          "the IEEE/CVF Conference on Computer Vision and Pattern": "tures, and beyond. International Journal of Computer Vision,"
        },
        {
          "Challenge\nMetric\nMethod\nResult": "arXiv:2205.01580, 2022. 2",
          "the IEEE/CVF Conference on Computer Vision and Pattern": "pages 1–23, 2019."
        },
        {
          "Challenge\nMetric\nMethod\nResult": "[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-",
          "the IEEE/CVF Conference on Computer Vision and Pattern": "[15] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos"
        },
        {
          "Challenge\nMetric\nMethod\nResult": "biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-",
          "the IEEE/CVF Conference on Computer Vision and Pattern": "Zafeiriou.\nDistribution matching for heterogeneous multi-"
        },
        {
          "Challenge\nMetric\nMethod\nResult": "tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-",
          "the IEEE/CVF Conference on Computer Vision and Pattern": "arXiv\npreprint\ntask\nlearning:\na\nlarge-scale\nface\nstudy."
        },
        {
          "Challenge\nMetric\nMethod\nResult": "in-\nguage models are few-shot\nlearners. Advances in neural",
          "the IEEE/CVF Conference on Computer Vision and Pattern": "arXiv:2105.03790, 2021."
        },
        {
          "Challenge\nMetric\nMethod\nResult": "formation processing systems, 33:1877–1901, 2020. 2",
          "the IEEE/CVF Conference on Computer Vision and Pattern": "[16] Dimitrios Kollias, Panagiotis Tzirakis, Alice Baird, Alan"
        },
        {
          "Challenge\nMetric\nMethod\nResult": "[3]\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina",
          "the IEEE/CVF Conference on Computer Vision and Pattern": "Cowen, and Stefanos Zafeiriou. Abaw: Valence-arousal esti-"
        },
        {
          "Challenge\nMetric\nMethod\nResult": "Toutanova.\nBert:\nPre-training\nof\ndeep\nbidirectional",
          "the IEEE/CVF Conference on Computer Vision and Pattern": "mation, expression recognition, action unit detection & emo-"
        },
        {
          "Challenge\nMetric\nMethod\nResult": "arXiv preprint\ntransformers\nfor\nlanguage understanding.",
          "the IEEE/CVF Conference on Computer Vision and Pattern": "tional reaction intensity estimation challenges.\nIn Proceed-"
        },
        {
          "Challenge\nMetric\nMethod\nResult": "arXiv:1810.04805, 2018. 2",
          "the IEEE/CVF Conference on Computer Vision and Pattern": "ings of\nthe IEEE/CVF Conference on Computer Vision and"
        },
        {
          "Challenge\nMetric\nMethod\nResult": "[4] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,",
          "the IEEE/CVF Conference on Computer Vision and Pattern": "Pattern Recognition, pages 5888–5897, 2023."
        },
        {
          "Challenge\nMetric\nMethod\nResult": "Dirk Weissenborn,\nXiaohua\nZhai,\nThomas Unterthiner,",
          "the IEEE/CVF Conference on Computer Vision and Pattern": "[17] Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Ste-"
        },
        {
          "Challenge\nMetric\nMethod\nResult": "Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-",
          "the IEEE/CVF Conference on Computer Vision and Pattern": "fanos Zafeiriou, Chunchang Shao, and Guanyu Hu. The 6th"
        },
        {
          "Challenge\nMetric\nMethod\nResult": "vain Gelly, et al. An image is worth 16x16 words: Trans-",
          "the IEEE/CVF Conference on Computer Vision and Pattern": "affective behavior analysis in-the-wild (abaw) competition."
        },
        {
          "Challenge\nMetric\nMethod\nResult": "arXiv\npreprint\nformers\nfor\nimage\nrecognition\nat\nscale.",
          "the IEEE/CVF Conference on Computer Vision and Pattern": "arXiv preprint arXiv:2402.19344, 2024. 1"
        },
        {
          "Challenge\nMetric\nMethod\nResult": "arXiv:2010.11929, 2020. 2",
          "the IEEE/CVF Conference on Computer Vision and Pattern": "[18] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and"
        },
        {
          "Challenge\nMetric\nMethod\nResult": "[5] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr",
          "the IEEE/CVF Conference on Computer Vision and Pattern": "Piotr Doll´ar. Focal\nloss for dense object detection.\nIn Pro-"
        },
        {
          "Challenge\nMetric\nMethod\nResult": "Doll´ar, and Ross Girshick. Masked autoencoders are scalable",
          "the IEEE/CVF Conference on Computer Vision and Pattern": "ceedings of\nthe IEEE international conference on computer"
        },
        {
          "Challenge\nMetric\nMethod\nResult": "vision learners.\nIn Proceedings of the IEEE/CVF conference",
          "the IEEE/CVF Conference on Computer Vision and Pattern": "vision, pages 2980–2988, 2017. 2"
        },
        {
          "Challenge\nMetric\nMethod\nResult": "on computer vision and pattern recognition, pages 16000–",
          "the IEEE/CVF Conference on Computer Vision and Pattern": "[19] Dang-Khanh Nguyen, Ngoc-Huynh Ho, Sudarshan Pant, and"
        },
        {
          "Challenge\nMetric\nMethod\nResult": "16009, 2022. 2",
          "the IEEE/CVF Conference on Computer Vision and Pattern": "Hyung-Jeong Yang. A transformer-based approach to video"
        },
        {
          "Challenge\nMetric\nMethod\nResult": "[6] Dimitrios Kollias.\nAbaw: Valence-arousal estimation, ex-",
          "the IEEE/CVF Conference on Computer Vision and Pattern": "frame-level prediction in affective behaviour analysis in-the-"
        },
        {
          "Challenge\nMetric\nMethod\nResult": "pression\nrecognition,\naction\nunit\ndetection & multi-task",
          "the IEEE/CVF Conference on Computer Vision and Pattern": "wild. arXiv preprint arXiv:2303.09293, 2023. 2"
        },
        {
          "Challenge\nMetric\nMethod\nResult": "the IEEE/CVF Con-\nlearning challenges.\nIn Proceedings of",
          "the IEEE/CVF Conference on Computer Vision and Pattern": "[20] Geesung Oh, Euiseok Jeong, and Sejoon Lim.\nCausal af-"
        },
        {
          "Challenge\nMetric\nMethod\nResult": "ference on Computer Vision and Pattern Recognition, pages",
          "the IEEE/CVF Conference on Computer Vision and Pattern": "fect prediction model using a past facial image sequence.\nIn"
        },
        {
          "Challenge\nMetric\nMethod\nResult": "2328–2336, 2022. 1",
          "the IEEE/CVF Conference on Computer Vision and Pattern": "Proceedings of\nthe IEEE/CVF International Conference on"
        },
        {
          "Challenge\nMetric\nMethod\nResult": "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &",
          "the IEEE/CVF Conference on Computer Vision and Pattern": "Computer Vision, pages 3550–3556, 2021. 2"
        },
        {
          "Challenge\nMetric\nMethod\nResult": "multi-task learning challenges.\nIn European Conference on",
          "the IEEE/CVF Conference on Computer Vision and Pattern": "[21] Zihan Wang, Siyang Song, Cheng Luo, Yuzhi Zhou, Shiling"
        },
        {
          "Challenge\nMetric\nMethod\nResult": "Computer Vision, pages 157–172. Springer, 2023.",
          "the IEEE/CVF Conference on Computer Vision and Pattern": "Wu, Weicheng Xie, and Linlin Shen.\nSpatio-temporal au"
        },
        {
          "Challenge\nMetric\nMethod\nResult": "[8] Dimitrios Kollias. Multi-label compound expression recog-",
          "the IEEE/CVF Conference on Computer Vision and Pattern": "relational graph representation learning for facial action units"
        },
        {
          "Challenge\nMetric\nMethod\nResult": "nition:\nC-expr database & network.\nIn Proceedings of",
          "the IEEE/CVF Conference on Computer Vision and Pattern": "detection. arXiv preprint arXiv:2303.10644, 2023. 2"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[22]\nJun Yu,\nRenda\nLi,\nZhongpeng\nCai,\nGongpeng\nZhao,": "Guochen Xie, Jichao Zhu, Wangyuan Zhu, Qiang Ling, Lei"
        },
        {
          "[22]\nJun Yu,\nRenda\nLi,\nZhongpeng\nCai,\nGongpeng\nZhao,": "Wang, Cong Wang, et al.\nLocal\nregion perception and re-"
        },
        {
          "[22]\nJun Yu,\nRenda\nLi,\nZhongpeng\nCai,\nGongpeng\nZhao,": "lationship learning combined with feature fusion for\nfacial"
        },
        {
          "[22]\nJun Yu,\nRenda\nLi,\nZhongpeng\nCai,\nGongpeng\nZhao,": "action unit detection.\nIn Proceedings of the IEEE/CVF Con-"
        },
        {
          "[22]\nJun Yu,\nRenda\nLi,\nZhongpeng\nCai,\nGongpeng\nZhao,": "ference on Computer Vision and Pattern Recognition, pages"
        },
        {
          "[22]\nJun Yu,\nRenda\nLi,\nZhongpeng\nCai,\nGongpeng\nZhao,": "5784–5791, 2023. 2"
        },
        {
          "[22]\nJun Yu,\nRenda\nLi,\nZhongpeng\nCai,\nGongpeng\nZhao,": "[23]\nStefanos Zafeiriou, Dimitrios Kollias, Mihalis A Nicolaou,"
        },
        {
          "[22]\nJun Yu,\nRenda\nLi,\nZhongpeng\nCai,\nGongpeng\nZhao,": "Athanasios Papaioannou, Guoying Zhao,\nand\nIrene Kot-"
        },
        {
          "[22]\nJun Yu,\nRenda\nLi,\nZhongpeng\nCai,\nGongpeng\nZhao,": "sia. Aff-wild: Valence and arousal\n‘in-the-wild’challenge."
        },
        {
          "[22]\nJun Yu,\nRenda\nLi,\nZhongpeng\nCai,\nGongpeng\nZhao,": "and Pattern Recognition Workshops\nIn Computer Vision"
        },
        {
          "[22]\nJun Yu,\nRenda\nLi,\nZhongpeng\nCai,\nGongpeng\nZhao,": "(CVPRW), 2017 IEEE Conference on,\npages 1980–1987."
        },
        {
          "[22]\nJun Yu,\nRenda\nLi,\nZhongpeng\nCai,\nGongpeng\nZhao,": "IEEE, 2017. 1"
        },
        {
          "[22]\nJun Yu,\nRenda\nLi,\nZhongpeng\nCai,\nGongpeng\nZhao,": "[24]\nSu Zhang, Ziyuan Zhao, and Cuntai Guan. Multimodal con-"
        },
        {
          "[22]\nJun Yu,\nRenda\nLi,\nZhongpeng\nCai,\nGongpeng\nZhao,": "tinuous emotion recognition: A technical report for abaw5."
        },
        {
          "[22]\nJun Yu,\nRenda\nLi,\nZhongpeng\nCai,\nGongpeng\nZhao,": "the IEEE/CVF Conference on Computer\nIn Proceedings of"
        },
        {
          "[22]\nJun Yu,\nRenda\nLi,\nZhongpeng\nCai,\nGongpeng\nZhao,": "Vision and Pattern Recognition, pages 5763–5768, 2023. 2"
        },
        {
          "[22]\nJun Yu,\nRenda\nLi,\nZhongpeng\nCai,\nGongpeng\nZhao,": "[25] Wei Zhang, Bowen Ma, Feng Qiu,\nand Yu Ding. Multi-"
        },
        {
          "[22]\nJun Yu,\nRenda\nLi,\nZhongpeng\nCai,\nGongpeng\nZhao,": "modal facial affective analysis based on masked autoencoder."
        },
        {
          "[22]\nJun Yu,\nRenda\nLi,\nZhongpeng\nCai,\nGongpeng\nZhao,": "the IEEE/CVF Conference on Computer\nIn Proceedings of"
        },
        {
          "[22]\nJun Yu,\nRenda\nLi,\nZhongpeng\nCai,\nGongpeng\nZhao,": "Vision and Pattern Recognition, pages 5792–5801, 2023. 2"
        },
        {
          "[22]\nJun Yu,\nRenda\nLi,\nZhongpeng\nCai,\nGongpeng\nZhao,": "[26] Weiwei Zhou,\nJiada Lu, Zhaolong Xiong,\nand Weifeng"
        },
        {
          "[22]\nJun Yu,\nRenda\nLi,\nZhongpeng\nCai,\nGongpeng\nZhao,": "Wang. Leveraging tcn and transformer for effective visual-"
        },
        {
          "[22]\nJun Yu,\nRenda\nLi,\nZhongpeng\nCai,\nGongpeng\nZhao,": "audio fusion in continuous emotion recognition. In Proceed-"
        },
        {
          "[22]\nJun Yu,\nRenda\nLi,\nZhongpeng\nCai,\nGongpeng\nZhao,": "ings of\nthe IEEE/CVF Conference on Computer Vision and"
        },
        {
          "[22]\nJun Yu,\nRenda\nLi,\nZhongpeng\nCai,\nGongpeng\nZhao,": "Pattern Recognition, pages 5755–5762, 2023. 2"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Better plain vit baselines for imagenet-1k",
      "authors": [
        "Lucas Beyer",
        "Xiaohua Zhai",
        "Alexander Kolesnikov"
      ],
      "year": "2022",
      "venue": "Better plain vit baselines for imagenet-1k",
      "arxiv": "arXiv:2205.01580"
    },
    {
      "citation_id": "2",
      "title": "Language models are few-shot learners. Advances in neural information processing systems",
      "authors": [
        "Tom Brown",
        "Benjamin Mann",
        "Nick Ryder",
        "Melanie Subbiah",
        "Jared Kaplan",
        "Prafulla Dhariwal",
        "Arvind Neelakantan",
        "Pranav Shyam",
        "Girish Sastry",
        "Amanda Askell"
      ],
      "year": "2020",
      "venue": "Language models are few-shot learners. Advances in neural information processing systems"
    },
    {
      "citation_id": "3",
      "title": "Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova",
        "Bert"
      ],
      "year": "2018",
      "venue": "Pre-training of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "4",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "Alexey Dosovitskiy",
        "Lucas Beyer",
        "Alexander Kolesnikov",
        "Dirk Weissenborn",
        "Xiaohua Zhai",
        "Thomas Unterthiner",
        "Mostafa Dehghani",
        "Matthias Minderer",
        "Georg Heigold",
        "Sylvain Gelly"
      ],
      "year": "2020",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "arxiv": "arXiv:2010.11929"
    },
    {
      "citation_id": "5",
      "title": "Masked autoencoders are scalable vision learners",
      "authors": [
        "Kaiming He",
        "Xinlei Chen",
        "Saining Xie",
        "Yanghao Li",
        "Piotr Dollár",
        "Ross Girshick"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "6",
      "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & multi-task learning challenges",
      "authors": [
        "Dimitrios Kollias"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "7",
      "title": "Abaw: learning from synthetic data & multi-task learning challenges",
      "authors": [
        "Dimitrios Kollias"
      ],
      "year": "2023",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "8",
      "title": "Multi-label compound expression recognition: C-expr database & network",
      "authors": [
        "Dimitrios Kollias"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "9",
      "title": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2019",
      "venue": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "arxiv": "arXiv:1910.04855"
    },
    {
      "citation_id": "10",
      "title": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "arxiv": "arXiv:2103.15792"
    },
    {
      "citation_id": "11",
      "title": "Analysing affective behavior in the second abaw2 competition",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "12",
      "title": "Analysing affective behavior in the first abaw 2020 competition",
      "authors": [
        "Kollias",
        "E Schulc",
        "Hajiyev",
        "Zafeiriou"
      ],
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)(FG)"
    },
    {
      "citation_id": "13",
      "title": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "authors": [
        "Dimitrios Kollias",
        "Viktoriia Sharmanska",
        "Stefanos Zafeiriou"
      ],
      "year": "2019",
      "venue": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "arxiv": "arXiv:1910.11111"
    },
    {
      "citation_id": "14",
      "title": "Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "A Mihalis",
        "Athanasios Nicolaou",
        "Guoying Papaioannou",
        "Björn Zhao",
        "Irene Schuller",
        "Stefanos Kotsia",
        "Zafeiriou"
      ],
      "year": "2019",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "15",
      "title": "Distribution matching for heterogeneous multitask learning: a large-scale face study",
      "authors": [
        "Dimitrios Kollias",
        "Viktoriia Sharmanska",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Distribution matching for heterogeneous multitask learning: a large-scale face study",
      "arxiv": "arXiv:2105.03790"
    },
    {
      "citation_id": "16",
      "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & emotional reaction intensity estimation challenges",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "Alice Baird",
        "Alan Cowen",
        "Stefanos Zafeiriou"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "17",
      "title": "The 6th affective behavior analysis in-the-wild (abaw) competition",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "Alan Cowen",
        "Stefanos Zafeiriou",
        "Chunchang Shao",
        "Guanyu Hu"
      ],
      "year": "2024",
      "venue": "The 6th affective behavior analysis in-the-wild (abaw) competition",
      "arxiv": "arXiv:2402.19344"
    },
    {
      "citation_id": "18",
      "title": "Focal loss for dense object detection",
      "authors": [
        "Tsung-Yi Lin",
        "Priya Goyal",
        "Ross Girshick",
        "Kaiming He",
        "Piotr Dollár"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE international conference on computer vision"
    },
    {
      "citation_id": "19",
      "title": "A transformer-based approach to video frame-level prediction in affective behaviour analysis in-thewild",
      "authors": [
        "Dang-Khanh Nguyen",
        "Ngoc-Huynh Ho",
        "Sudarshan Pant",
        "Hyung-Jeong Yang"
      ],
      "year": "2023",
      "venue": "A transformer-based approach to video frame-level prediction in affective behaviour analysis in-thewild",
      "arxiv": "arXiv:2303.09293"
    },
    {
      "citation_id": "20",
      "title": "Causal affect prediction model using a past facial image sequence",
      "authors": [
        "Geesung Oh",
        "Euiseok Jeong",
        "Sejoon Lim"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "21",
      "title": "Spatio-temporal au relational graph representation learning for facial action units detection",
      "authors": [
        "Zihan Wang",
        "Siyang Song",
        "Cheng Luo",
        "Yuzhi Zhou",
        "Shiling Wu",
        "Weicheng Xie",
        "Linlin Shen"
      ],
      "year": "2023",
      "venue": "Spatio-temporal au relational graph representation learning for facial action units detection",
      "arxiv": "arXiv:2303.10644"
    },
    {
      "citation_id": "22",
      "title": "Local region perception and relationship learning combined with feature fusion for facial action unit detection",
      "authors": [
        "Jun Yu",
        "Renda Li",
        "Zhongpeng Cai",
        "Gongpeng Zhao",
        "Guochen Xie",
        "Jichao Zhu",
        "Wangyuan Zhu",
        "Qiang Ling",
        "Lei Wang",
        "Cong Wang"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "23",
      "title": "Aff-wild: Valence and arousal 'in-the-wild'challenge",
      "authors": [
        "Stefanos Zafeiriou",
        "Dimitrios Kollias",
        "A Mihalis",
        "Athanasios Nicolaou",
        "Guoying Papaioannou",
        "Irene Zhao",
        "Kotsia"
      ],
      "year": "2017",
      "venue": "Computer Vision and Pattern Recognition Workshops (CVPRW), 2017 IEEE Conference on"
    },
    {
      "citation_id": "24",
      "title": "Multimodal continuous emotion recognition: A technical report for abaw5",
      "authors": [
        "Su Zhang",
        "Ziyuan Zhao",
        "Cuntai Guan"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "25",
      "title": "Multimodal facial affective analysis based on masked autoencoder",
      "authors": [
        "Wei Zhang",
        "Bowen Ma",
        "Feng Qiu",
        "Yu Ding"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "26",
      "title": "Leveraging tcn and transformer for effective visualaudio fusion in continuous emotion recognition",
      "authors": [
        "Weiwei Zhou",
        "Jiada Lu",
        "Zhaolong Xiong",
        "Weifeng Wang"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    }
  ]
}