{
  "paper_id": "2104.02643v2",
  "title": "The Arousal Video Game Annotation (Again) Dataset",
  "published": "2021-04-06T16:27:21Z",
  "authors": [
    "David Melhart",
    "Antonios Liapis",
    "Georgios N. Yannakakis"
  ],
  "keywords": [
    "Emotional corpora",
    "arousal",
    "human-computer interaction",
    "affective computing",
    "games"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "How can we model affect in a general fashion, across dissimilar tasks, and to which degree are such general representations of affect even possible? To address such questions and enable research towards general affective computing, this paper introduces The Arousal video Game AnnotatIoN (AGAIN) dataset. AGAIN is a large-scale affective corpus that features over 1,100 in-game videos (with corresponding gameplay data) from nine different games, which are annotated for arousal from 124 participants in a first-person continuous fashion. Even though AGAIN is created for the purpose of investigating the generality of affective computing across dissimilar tasks, affect modelling can be studied within each of its 9 specific interactive games. To the best of our knowledge AGAIN is the largest-over 37 hours of annotated video and game logs-and most diverse publicly available affective dataset based on games as interactive affect elicitors.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "A core challenge of affective computing (AC) is the investigation of generality in the ways emotions are elicited and manifested, in the annotation protocols designed, and ultimately in the affect models created. To examine the degree to which general representations of affect are possible and meaningful, AC research requires access to corpora containing affect responses and annotations across dissimilar tasks, participants and annotators. Traditional large-scale AC datasets feature affect annotation of static images, videos, sounds and speech files within a narrow context through which affect is elicited from a particular task. Even when the various tasks under annotation may vary, those are still limited to a very specific context-such as viewing a set of social interactions under a theme or playing sessions of the same game.\n\nThis paper identifies games as a unique opportunity in AC to observe emergent emotions in a well-defined but highly interactive environment. Interactivity is especially important for the future of AC research as emotions permeate our daily interactions-not just with each other, but with our environment and computers as well. Affective states arising from these interactions impact our behaviour and decision making on a fundamental level  [1] ,  [2] . Therefore, modelling emotions that emerge from interactions is becoming paramount to AC research.\n\nMotivated by the lack of corpora for the study of general properties of affect across tasks and participants, in this paper we introduce The Arousal video Game AnnotatIoN (AGAIN) dataset, which contains data from over 120 participants who played and annotated over 1, 000 gameplay sessions. AGAIN is accessible online  1  and features data collected from nine games of three dissimilar genres, which were developed specifically for the purposes of the dataset (see Fig.  1 ). As shown in Table  I , along with game telemetry and selfannotated arousal labels, the dataset also features a video database of unique gameplay sessions with over 37 hours of in-game footage. The diverse nature of the AGAIN affect elicitors (games) provides a testbed for general affect detection in games  [3] ,  [4]  and broadens the horizons for research on general-purpose AI representations  [5] ,  [6]  and artificial general intelligence.\n\nWhile AC datasets in general rely on collecting peripheral physiological signals in laboratory settings, the AGAIN dataset moves data collection to an online setting. On the one hand, this setup only allows us to collect behavioural data in a reliable way. However, since the tools and pipelines employed to collect the dataset emphasise a simple crowdsourced setup, the AGAIN database is much more flexible, extensible and scalable. The design and creation of AGAIN was indeed guided by the following factors: a) accessibility, which is achieved through an online crowdsourcing framework; b) scalability: AGAIN is utilising the PAGAN online annotation framework  [7]  and, hence, one can easily populate the AGAIN database with more participants and annotators; c) extensibility: more affect dimensions and categories can be considered and integrated to the existing dataset through the customisable PAGAN annotation tool; d) generality: any additional online game or interactive session can be easily integrated to the experimental protocol of AGAIN. While at the time of writing the database hosts 9 games annotated for arousal, AGAIN is designed with all aforementioned factors in mind so that it can host data from more games and user modalities, considering alternative affective labels. The AGAIN dataset is unique in a number of ways. First, it is the largest and most diverse publicly available affective dataset based on games as interactive elicitors. Given the breadth of elicitors offered, the dataset can be used for testing specific affect models on one particular task (i.e. a particular game) all the way to general models of affect across tasks (game genres and games in general). Second, the dataset is annotated with the core affective dimension of arousal, linking dominant annotation practices in affective computing with player modelling and game user research. Finally, it employs a novel annotation framework  [8]  which captures subjective annotations in a continuous and unbounded manner that can be further processed as labels for regression, classification or ordinal learning affect modelling tasks  [9] ,  [10] .\n\nThe remainder of the paper is structured as follows. Section II contextualises the dataset within the fields of affective computing and affect modelling in games while Section III offers a systematic review of existing audiovisual datasets. The games used as the affect elicitors of AGAIN are described in Section IV. Section V details the AGAIN dataset by describing the protocol followed, the characteristics of the participants, the data types collected, and the annotation framework used. Section VI offers a detailed yet preliminary data analysis of the dataset. Limitations and extensions of AGAIN are discussed in Section VII and the paper concludes with Section VIII.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Background",
      "text": "AGAIN is an accessible dataset offered for research in affective computing at large and player modelling in particular. This background section discusses the importance of arousal within the field of affect representation (Section II-A) and reviews studies for modelling the affect of game users (i.e. players) in Section II-B.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Arousal As Affect Representation",
      "text": "While there are different approaches to affect representation including categorical  [11] ,  [12] , dimensional  [13] , and mixed  [14]  frameworks, the AGAIN dataset uses a dimensional representation based on the Pleasure-Arousal-Dominance (PAD) model of affect  [15]  and the Circumplex Model of Emotions  [13] . In contrast to categorical frameworks, which assume a clear division between emotional responses, these models propose a more ambiguous and general representation. Instead of complex emotions, the PAD model focuses on basic affective states represented across three dimensions. Pleasure is associated with the valence of the emotion; psychological arousal describes the intensity of the emotion; and finally dominance describes the agency or level of autonomy during the emotional episode. One can place different emotions within this 3D continuous space without explicitly categorising them, reducing the chance of misrepresenting how a subject feels. This type of evaluation lends itself better for continuous and subjective annotation  [9] ,  [10] .\n\nWhile the Circumplex model and the PAD model represent affect across two and three dimensions, respectively, in the AGAIN dataset we focus currently on soliciting annotations based on the dimension of arousal. Selecting and investigating arousal first-instead of other affect dimensions-is relevant for games, the core domain of AGAIN. Arousal is present and dominant as an emotional manifestation in game affect interactions and has been associated with challenge  [16] , cognitive and affective engagement  [17] , tension  [18] , fun  [19] , frustration  [20]  and flow  [21] , as well as positive post-game outcomes, such as increased creativity  [22]  and working memory  [23]  performance. Focusing on one affect dimension reduces the cognitive load of the annotation task  [7] , which in turn increases the reliability of our data; however, it limits the expressive range of affect annotation in the dataset. Moreover, the focus on arousal assists the research community to build, extend upon and advance research that already has benchmarked the study of arousal in games  [4] ,  [5] ,  [8] .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Affect Modelling In Games",
      "text": "Player modelling is the study of video game play both in terms of behavioural and affective patterns  [24] . It relies heavily on artificial intelligence methods for building predictive models of player behaviour  [25] ,  [26] , playtime  [27] , churn  [28] ,  [29] , or player experience  [5] ,  [9] ,  [30] . It is naturally characterised by dynamic representations and modelling of data, thereby providing even moment-to-moment predictions of a game's elicited experience  [31] . A key limitation of player modelling, as with any other data-driven approach, is that it is data hungry. In particular, studies that focus on affective aspects of player experience require ground-truth affect labels which are often costly to collect  [32] ,  [33] .\n\nTo address the above challenge, an increasing number of studies focus on approaches that could realise aspects of general player modelling  [3] . General player modelling features methods that are able to predict a player's affective state on unseen games. While early studies such as that of Martinez et al.  [34]  investigated game-independent features of the playing experience, such as heart rate and skin conductance, later studies put an emphasis on finding general gameplay features either manually  [35]  or through algorithmic feature mapping  [36] . More recently, Camilleri et al. investigated general gameplay features and generalised metrics of player experience across three dissimilar games  [4] . Their study used high-level features such as goal-oriented and goal-opposed gameplay events and relative metrics of arousal to moderate success, showing the difficulty of creating general player models. Similarly, Bonometti et al. used high-level general features to characterise the gameplay context (such as activity count and activity diversity) to model engagement across six games published by Square Enix Ltd.  [37] .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iii. Audiovisual Affective Datasets",
      "text": "The availability of large-scale corpora comprising affect manifestations that are elicited through appropriate stimuli is a necessity for affect modelling. Creating datasets that are annotated with reliable affect information is, therefore, instrumental to the field of AC at large. In this section we review representative affective corpora that rely on audiovisual elicitors and discuss the contribution of AGAIN to the current list of datasets that are enriched with affect labels. Table  II  presents the outcome of our survey 2  . We follow a systematic approach for reviewing the state of the art in affect corpora and examine the following factors that distinguish the surveyed datasets: the interactivity and the type of the provided elicitors, the number of possible elicitor items, and the overall size of the available video database (see second to fifth column of Table  II ), the number of participants and their recorded modalities (see columns six and seven of Table  II ), the annotation protocol in terms of perspective and type of annotation (see columns eight and nine of Table  II ), the affect labels (see column ten of Table  II ), and finally the number of annotators (if different from the number of participants and/or not selfreported) and number of tasks each annotator had to complete (see the last two columns of Table  II ).\n\nIt is apparent from Table  II  that affective datasets have gradually-over the last decade or so-drifted away from traditional induced elicitation and posed expressions, and instead turned towards soliciting spontaneous emotion manifestations. New datasets have been focusing on elicitation through naturalistic expressions. While some of these datasets use their own elicitors, many rely on popular media, using video clips and still images from music videos and movies  [39] ,  [41] ,  [42] . This method has proved to be reliable, cost-effective, and easy to set up, which subsequently led to a widespread adoption in the field. Compared to staged videos and images, interactive elicitors provide more organic stimuli. While reactions to noninteractive media can produce spontaneous expressions, interactive elicitation can increase the participants' involvement with the elicitor and reveal emotional reactions that might be hard to elicit with pre-recorded videos and images alone. Subsequently, there has been a growing body of research dedicated to enrich the set of affective corpora with interactive elicitors. These datasets use a wide-range of methods including dyadic tasks  [43] ,  [44] , group tasks  [45] , board games  [46] , and video games  [48] ,  [49] . These interactive tasks provide a more complex and multifaceted affective stimulus, while organically structuring the participants' experience.\n\nMost traditional affective computing databases surveyed capture affective dimensions such as arousal and valence, with some datasets offering labels for additional dimensionssuch as dominance-and categorical labels (see the annotation/labels column in Table  II ). However, datasets using interactive elicitors tend to have a wider focus. While some of these datasets collect affective and emotional labels, their primary focus is task-related emotional outcomes. In datasets focusing on gameplay, this generally includes gameplay experience  [46]  and other game-related outcomes-such as frustration, perceived challenge  [47] ,  [48] , engagement  [48] , and fun  [47] ,  [49] . Studies that use the same affective labels are easier to compare and their lessons easier to transfer to new data than studies using a more diverse set of labels (i.e. fun, engagement, challenge, etc.). On one hand, the mapping between labels such as \"fun\" and \"challenge\" can be uncertain; and on the other hand, it can be difficult to reliably translate outcomes such as \"engagement\" to more traditional affective computing concepts such as arousal or valence.\n\nThe affective datasets we survey appear to be rather split in terms of annotation type used. While some (e.g. DEAP  [39] , MANHOB-HCI  [38] ) opt for self-reporting (first-person annotation), many databases (e.g. RELOCA  [44] , SEWA  [43] ) use only a few expert annotators in a third-person manner. There is a clear trade-off between these approaches. Firstperson annotations (i.e. self-reported labels) are ideal for capturing the subjective appraisal of emotional content, while third-person annotations are better at labelling emotion manifestation through inter-rater agreement  [50] . Interestingly, most datasets using an interactive elicitor also opt for first-person annotation through self-reports. A possible explanation is the higher degree of participant involvement, which makes the experience more unique to the participant. Such a highly subjective experience is better captured via first-person reporting. There is a definite trade-off, however, between interactive and non-interactive elicitation. Using multimedia clips for elicitation offers a cost-effective solution, which leads to large and varied datasets. On the other hand, interactive elicitors can stimulate emergent emotions more naturally.\n\nDatasets using non-interactive elicitors are generally larger, while the cost associated with using interactive elicitation limits these datasets. Table  II  shows that interactive datasets often focus on fewer elicitor items. However, this lack of variety is often offset by the different ways participants can interact with these elicitors, producing more diverse data. Despite this diversity, there are not many datasets that feature multiple different interactive elicitors. The handful of examples either combine dissimilar datasets  [4] -which comes with its own challenges in reconciling different data formats-or use a small set of very similar elicitors-e.g. the FUNii dataset  [49]  features two similar games from the same franchise. There are some exceptions: for instance, the MUMBAI dataset  [46]   features six board games, although the data here is not selfreported but labelled by third-person annotators. AGAIN addresses the aforementioned limitations by offering a large-scale corpus that is based on a set of dissimilar interactive affect elicitors that are annotated through a firstperson protocol. While the dataset at the time of writing is limited to 9 games and their annotated arousal, the dataset is planned to be augmented through more affective dimensions and enriched through more games. The resulting dataset leverages the strength of active emotion elicitation while producing data in volumes comparable to databases featuring non-interactive affect stimuli. Moreover AGAIN provides a diverse database for general player affect modelling research that is not possible within any of the existing corpora.\n\nWe position AGAIN at the intersection of traditional affective computing corpora and datasets with a focus on interactive emotion elicitation. By focusing on a core affect dimension (i.e. arousal) instead of task-related complex emotional outcomes, we aim to make the dataset more relevant to traditional AC research. We argue that the use of video games as interactive elicitors combined with traditional affective labels can also help bridge the gap between AC and games user research. As games are highly interactive media, the captured data and annotations encode not merely player affect but also behaviour and game context. We focus on self-reported labels to better capture the subjective intricacies of gameplay. Finally, we choose to record continuous unbounded traces of arousal using RankTrace  [8]  via the PAGAN online annotation framework  [7] . Such traces can be processed and machine learned in a number of ways including regression, classification and relational learning  [9] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iv. Games",
      "text": "Nine games, across three different genres, were designed and developed as affect elicitors specifically for the AGAIN dataset. We put careful consideration to create software which is aesthetically pleasing, representative of popular sub-genres of games, can be understood immediately with a basic level of game literacy  [51] , and produces a coherent and consistent dataset without the need of heavy pre-processing. The game genres were selected (racing, shooters, platformers) because they represent a good cross-section of the game genres  [52]  and are among the most popular among gamers  [24] ,  [53] , but also because they have simple enough controls and clear mechanics so that players can pick them up quickly. Opposed to other genres, such as role playing or strategy games, that require longer time investment and players to learn the specific mechanics, strategies and synergies, the games in the dataset relied on fast-paced genres and popular tropes to communicate the game rules as fast as possible. Specific games were designed under each genre are representative of the genre.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Racing",
      "text": "The racing genre is characterised by fast-paced driving against a number of opponents in a given track. The dynamics of the experience is partly dictated by the limited interaction with opponent vehicles (e.g. pushing into each other) but mainly defined by the track itself. In the AGAIN database, three games are representing distinct sub-genres of racing games. TinyCars is an arcade-style racer with an isometric view (see Fig  2a ). Its controls are the hardest to master due to the drifting of the player's car. Solid is a more traditional rally game, with more realistic handling (see Fig  2b ). As the player sees the track from the driver's seat, adapting to the turns of the track is more challenging. ApexSpeed is a speed-racer type game, with minimalist controls (see Fig  2c ). While the player only has to change lanes (the vehicle accelerates and follows the track automatically), the game has a faster pace than other racing games and additional elements are complicating the track (i.e. speed boost platforms and obstacles).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Shooter",
      "text": "The shooter genre focuses on eliminating opponents using projectile weapons. The gameplay dynamic of these games builds on hand-eye coordination and it is characterised by periods of suspense and periods of engagement with the ingame opponents. Shooter games in the AGAIN dataset provide examples of different shooter sub-genres. Heist! is a typical first-person shooter game with similar mechanics to modern shooters (see Fig  2d ). Because the player has to wait for their health to regenerate, the play experience is broken up into smaller engagements. In contrast, TopDown has a top-down view, an automatic weapon, and health pickups (see Fig  2e ). This provides a more action-packed environment as the player is not encouraged to stop if they are low on health. These two games also feature less linear maps compared to other games in the dataset. Shootout on the other hand does not feature traversal at all. In this game the player can only aim and shoot as the screen is filled with more and more enemies (see Fig  2f ). This dynamic of even-increasing intensity is typical of arcadestyle games (including shooters).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Platformer",
      "text": "The platformer genre focuses on traversal and often requires precision and dexterity. The platformer games featured in the AGAIN dataset are the most diverse set of games. Endless is an endless-runner, a popular mobile-game genre. In these games, the player moves forward automatically at an everincreasing pace while they have to attack or dodge incoming obstacles (see Fig  2g ). Subsequently, Endless is one of the most frantic games in the dataset. Pirates! is a classical platformer, akin to Super Mario Bros (Nintendo, 1985) (see Fig  2h ). This game has a more relaxed pace as the gameplay is focused on light platform puzzles and simple traversal. Finally, Run'N'Gun is a shoot-em up game, which has the characteristics of both a platformer and shooter game (see Fig  2i ). Thanks to the shooting mechanics, number of enemies, and enemy projectiles, this game has a more intense gameplay loop compared to the other platformer games.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "V. Again Dataset",
      "text": "Games in the AGAIN dataset were built for the WebGL platform and are played in a web-browser. The games were integrated into the PAGAN annotation platform  [7] , which allowed the large-scale crowd-sourcing of both the game playing and annotation tasks.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Protocol",
      "text": "The collection procedure took anywhere between 45 to 55 minutes and followed by a stimulated recall protocol  [54] . Participants were invited through Amazon's Mechanical Turk service  3  and were compensated with 10 USD for their time. The only criterion for participation was prior purchase of video games, in order to filter out potential subjects who might not have the game literacy required to play the games. Participants were greeted with an introduction screen (see Fig.  3 ), which informed them about the overall task and explained arousal as a feeling of tension, excitement, exhilaration or readiness and the opposite of boredom, calmness or relaxation. The experiment consisted of 9 rounds, each round consisting of 2 minutes of game-play followed by 2 minutes of annotation. Due to the high cognitive load of video game play, the annotations could not be collected at the same time as the game play telemetry. To mitigate this issue, a stimulated recall technique was used. Participants' gameplay was captured and played back to them during the annotation process. The collection procedure was set up in an iterative manner with participants playing for 2 minutes, then annotating their gameplay video for 2 minutes. The order of the games was randomised and this procedure was repeated until all games were played and annotated. After the experiment, participants filled in a simple exit-survey recording their biographical data and gaming habits.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "B. Participants",
      "text": "Through the procedure presented in Section V-A, we collected data from 124 participants  4  which include 1, 116 gameplay sessions (124 sessions per game) with detailed telemetry and over 37 hours of gameplay videos. Out of the 124 participants, one identified as non-binary, 43 as female, and 80 as male. Participants' age varied between 19 and 55 years old (average of 33). Most participants were from the USA (82%); the remaining 22 participants came from Brazil (10 participants), Italy (3), Canada (2), India (2), Czech Republic (1), Germany (1), and Romania  (1) . Most participants identified as casual gamers (57%) or hard-core gamers (36%). Reflectively, the majority of participants (87%) were playing daily or weekly. All participants had either a PC or a gaming console or both, with the most popular platform being PC. Participants played very diverse games in their free time across different genres: from casual games through platformers, sports simulators, shooters, to role-playing games. The anonymised demographic data is included in the dataset.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "C. Game Footage Videos",
      "text": "For realising first person annotation, the gameplay footage of players had to be recorded and annotated by the players themselves. As a result the raw AGAIN dataset features 1, 116 videos of around 2 mins each (i.e. over 37 hours of game footage). The video database contains more than 3×10 6 frames of video, which are recorded at 24 FPS and have a resolution of 960 × 600 pixels. Such data can enable future research that employs computer vision and deep-learning to directly map pixels to emotions  [5] . Previous studies have shown promise in using general-purpose representations such as pixel data from game footage  [55] , and utilising audiovisual data for learning through privileged information  [56] .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "D. Game Context Features",
      "text": "In addition to the raw video game footage, AGAIN features a number of hand-crafted attributes for each game. Inspired by advances in machine learning with privileged information  [56] ,  [57]  we view telemetry data as privileged information and we include such ad-hoc features in the dataset. Privileged information here means information that pertains to an experience but not readily available to an observer. This kind of information generally encodes domain specific or hard-to-attain data. The associated cost of learning the information makes the data valuable in building expert systems, but poses some limitation to data-hungry machine learning approaches. When it comes to video games, privileged information can include player physiology or game telemetry based on expert heuristics  [56] . Fusing gameplay features with other user modalities has also been a dominant practice in game-based affective computing  [58] ,  [59] . The game context features described in this section are considered in the preliminary data analysis of the dataset in Section V.\n\nAll AGAIN games implement the same data-logging strategy and use a similar method for recording telemetry. Games within the same genre share the same feature labels. Not all features, however, have a qualitative meaning for all games within a genre-for instance, players move in Heist! but are immobile in Shootout. To ease the data collection and aggregation process, when features are absent from a game they are given values with zero-variance (zeroes or ones, depending on the feature). For example, a looping racetrack is only present in the Solid game (see Figure  2b ), therefore the visible_loop_count feature is always zero in the other racing games.\n\nTable  III  shows the number of features we have extracted per game with the zero-variance features removed. Recorded game telemetry encodes control events initiated by the player (e.g. player_steering), player status (e.g. player_health), gameplay events outside of the player's control (e.g. bot_aim_at_player), bot status (e.g. bot_offroad), and the proximal and general game context (e.g bot_player_distance and pickups_visible).\n\nGameplay is recorded at approximately 4Hz (every 250ms). Due to limitations of the Unity engine and the WebGL format, the logging rate is not consistent. To mitigate this issue, the logging script aggregates multiple ticks of the engine's update In addition to the features enumerated in Table  III , the dataset includes 14 general gameplay features. These general features are ad-hoc designed and derived from the gamespecific events and are based on contemporary studies of general player modelling  [4] ,  [37] . Events which require expert evaluation of the game such as the goal-oriented and goal-opposed events of Camilleri et al.  [4]  are omitted from these general features of AGAIN, but may be considered as additional features. Table IV lists these features alongside their explanation.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "E. Annotation",
      "text": "The annotation task was administered through the PAGAN platform  [7] , using the RankTrace annotation method  [8] . PAGAN is an annotation platform developed to be an easy-to-use software for crowdsourcing annotation tasks with a focus on one-dimensional time-continuous annotation using three different methods. RankTrace  [8] , an ordinal annotation framework, GTrace  [60] , a bounded annotation scale which gathers continuous data that can be converted to a Likertlike format, and BTrace, which is a binary annotation tool for both time-continuous and discrete annotation, inspired by AffectRank  [61] . We have chosen RankTrace as our annotation framework for this dataset.\n\nRankTrace allowed us to collect data in an unbounded fashion (see Fig.  4 ). Due this collection method, the value range of the annotation is not bounded between 0 and 1, which can make it significantly harder to use the data for certain tasks-for example applying regression. However, this type of data is best interpreted as subjective, ordinal labels as it preserves the relative relationships between datapoints  [9] , and therefore is well suited for preference learning tasks. The unbounded trace means that users can always adjust their annotations higher or lower than previous values, which alleviates much of the guesswork compared to when users annotate on an absolute and objective scale  [59] . The ordinal nature of the annotation follows the cognitive process of human evaluation, as it provides a trace which factors in habituation  [62] , anchoring bias  [2] ,  [63]  and recency-effects  [64] . To preserve this subjectivity encoded in the annotation, we apply data transformation (i.e. normalisation and pairwise transformation) based on individual sessions.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "F. Data Cleaning",
      "text": "To ease any subsequent analysis and future studies based on the dataset, in this section we propose a preprocessing pipeline which removes 10.8% of the dataset as outliers. AGAIN contains both the raw and the cleaned data that result from the process outlined here.\n\nSince PAGAN only records annotations when there is a change in the signal and the Unity engine loop is affected by hardware performance, as a first step we resample the whole dataset at 4Hz to get a consistent signal. We remove duplicate values from the dataset, as well as sessions which are either too short (less than 1 minute) or too long (more than 3 minutes) due to software or technical errors during crowdsourcing. We also prune sessions which have less than 10 annotation points, assuming that the participant was unresponsive. This initial cleanup phase removes 24 sessions (2.1% of the data).",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Table V Preliminary Analysis Of The Clean Again Dataset. The Table Lists The Number Of Game Sessions And Their Corresponding Data Points On A Frame-By-Frame Basis (250 Ms). The Table Also Lists The Number Of 3S Time Windows Within Which The Arousal Value Increases (↑), Decreases (↓) Or Stays Stable Within A 10%",
      "text": "THRESHOLD BOUND (-).",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Arousal (3 S Interval) Game",
      "text": "Sessions Data (•10 Inspired by Makantasis et al.  [55] , we apply Dynamic Time Warping (DTW) to clean the dataset of irregular annotations. DTW is used extensively in time-series analysis as a distance measure  [65] -  [67] . DTW is an elastic measure of distance between two signals, which can be of different length and sampled at different rates  [65] ,  [68] . The DTW distance is calculated based on the similarity matrix between two timeseries, where every point of the two sequences is matched to each other-with one-to-many mapping where necessary  [68] . While in signal processing DTW is often applied to synchronise different signals, the cumulative distance-calculated when finding the warping path between signals-provides a reliable similarity measure between the dynamics of the time-series in question  [65] ,  [66] . We apply the cumulative DTW distance as a similarity measure between arousal traces, in order to remove irregular annotation patterns; we do not transform any of the signals. It should be noted that the signals have been synchronised and resampled, and thus the length and frequency of all annotation traces is the same.\n\nAs a first step in the cleanup process, we calculate the cumulative DTW distance to an artificial flat baseline (arousal annotations at 0 in all time windows). The resulting score provides us with a similarity measure to an artificial session where the participant performed no annotation; this allows us to remove unresponsive outliers. We remove all sessions which fall more than two standard deviations closer to zero from the average cumulative distance (the left tail of the distribution). This step removes 28 additional sessions from the dataset (2.5%). Since games in the dataset are quite short and players encounter similar situations, we assume that their experience would be somewhat similar. Therefore, we remove sessions where the annotation traces are too far from other traces in the dataset. To this end, we apply the cumulative DTW distance metric between each datapoint and sum up the resulting distances. This metric shows us the relative similarity of a session to every other session. We remove all sessions which fall more than two standard deviations away from the average summed cumulative distance (see Fig.  5 ). This step removes an additional 69 sessions (6.2%). This last step removes annotations which are too dissimilar from the general trends of participants' annotations; we presume that either the annotation was improper or that this session's elicitor was somehow not in line with how other players played the same game. Observing outliers empirically affirms that most participants whose annotation traces were atypical encountered issues with game controls, experienced slow-down and other glitches, or in most cases annotated positive and negative events instead of high and low arousal.\n\nAt the end of the cleaning process, 121 sessions-including all data from 2 participants-are removed (10.8%). Around 40% of the outliers are removed due to inactivity or incompleteness, while the rest is held out due to unusual annotation patterns. The cleaning process proposed in this section is conservative due to the limitations of the online collection process, where there is less control over the quality of annotation. However, the raw dataset is also made available, which provides opportunities for different processing methods. The clean dataset consists of 122 participants and 995 sessions; details on the clean dataset are provided in Section VI.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Vi. Again Analysis",
      "text": "Following the cleanup process presented in Section V-F, this Section performs a preliminary analysis of the clean version of the AGAIN dataset, focusing on patterns in the arousal annotations and the AGAIN game context features (see Section VI-A). Section VI-B describes an initial set of affect modelling experiments with this dataset, serving as baseline for future studies. While some games receive more aggressive data cleaning than others (TinyCars, Solid, and Shootout), overall there is an even distribution of data and sessions across genres as shown in Table  V .",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "A. Trends In The Data",
      "text": "Figure  6  shows the average annotation trace as calculated by averaging values in time windows of 250 ms of all sessions' traces. The gameplay sessions have been normalised to show the relative trend in the data. It is evident that arousal annotation tends to have an upwards tendency. This is not surprising, as most games considered are action-oriented with an everincreasing challenge; for instance, Endless keeps increasing the speed of the game which evidently makes it both harder and more arousing as time passes. Racing games (top row of Figure  6 ), on the other hand, tend to have arousal converging to a maximum mean value after the first 30 seconds. This is likely because the player is initially rushing to overtake the opponents' cars (players always start last); after this initial excitement the race becomes repetitive, with players trying to either maintain the lead or slowly catch up to the leader.\n\nObserving the twelve general gameplay features shared across all nine games, one can detect some notable differences between games. In terms of the player's input (control), games with more complex interaction schemes appear to have higher input diversity and input intensity (see Table  IV  for details on these features). Even accounting for the games' different control schemes (i.e. the number of controls the player has available), ApexSpeed, Shootout, and Endless have the lowest intensity (number of keypresses) and diversity (number of unique keypresses) while Pirates! and TinyCars have the highest diversity. This discrepancy could point to an easier control scheme for the former games, but it could also point to a more frantic and engaging interaction in the latter games. The idle time and activity features corroborate this observation, as racing games have less idle time without keypresses (since in two of the games the player needs to constantly press a button to move forward). In contrast, games where participants mainly reacted to stimuli (e.g. in Shootout players react to opponents popping up and in Endless players move only when a gap or obstacle is near) featured much higher idle times. In terms of other features, the number of bots (opponents) visible on the screen varied wildly between games, with Tiny Cars and Shootout having the highest number of visible enemies on average. Perhaps due to the many enemies present, Shootout had the highest number of events (event intensity in Table  IV ), while Solid had the fewest events per time window.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "B. Preliminary Arousal Models",
      "text": "In this section we provide an initial modelling approach for the AGAIN dataset, serving as a baseline study for future research with this dataset. As a preliminary step, we process the clean AGAIN dataset to predict arousal. To this end, we split the annotation traces into 3-second time windowscomputing the mean of the window-and introduce a 1-second lag to the annotation trace. Our choice of time windows and lag is motivated by best practices established by a long line of prior research  [4] ,  [8] ,  [61] ,  [69] -  [72] , as well as empirical results of studies into AC research design. It has been shown that a 3-second window size is well-suited to capture valence and arousal changes  [73] . In their experiment on the DEAP dataset  [39] , Ayata et al. have shown that affective data processed at this granularity leads to a higher model performance  [73] . Mariooryad and Busso have shown that while an optimal input lag value can be found algorithmically, an ad-hoc value between 1 to 3 seconds gives a good approximation of human input lag in AC annotation tasks  [74] . Here we chose a 1second lag to conform to the aforementioned body of research. All features (including arousal values) are normalised on a persession basis to a [0, 1] range. This means that feature values of 0 and 1 are indicating the minimum and maximum intensity of a given feature only within a session. This processing method gives weight to the relative dynamics of features instead of focusing on the absolute values.\n\nWhile in the published dataset both clean and raw data is available for the application of different machine learning techniques, we treat arousal modelling in AGAIN as a preference learning task  [9] ,  [10] ,  [75]  and focus on predicting arousal change from a 3-second time window to the next. We apply preference learning through a pairwise transformation. During this transformation we observe consecutive datapoints within sessions in pairs and create a new representation of the dataset. By describing the difference between arousal values of time windows, this new representation reformulates the preference learning problem as binary classification (arousal increasing or decreasing). For every (x i , x j ) ∈ X pair of game data we observe the relationship of their affect output (y i , y j ) ∈ Y . If y i is preferred to y j , we can label the distance between the corresponding data points (x i -x j ) and 1. Conversely, we can label the reverse of this observation (x j -x i ) as 0. While either one of these observations is sufficient to describe the relationship between x i and x j , by keeping both observations (λ xi-xj = 1 and λ xj -xi = 0), we can maintain a 50% baseline accuracy in the post-transformation dataset independently of the trends in the dataset before the transformation. While this method creates redundancies in the training data, it mitigates some of the issues that arise from the strong temporal patterns discussed in Section VI-A, as the algorithm is trained on both increasing and decreasing examples. To reduce experimental noise from trivial changes within the arousal trace, we omit all consecutive time windows between which the arousal change is less than 10% of the total amplitude of the session's arousal value. While this 10% threshold is based on prior experiments in similar problems  [18] ,  [76] , a more extensive analysis could explore the impact of the threshold value on prediction accuracy and the volume of data lost.\n\nAs mentioned above, applying this pairwise transformation to consecutive time windows reformulates the preference learning paradigm as binary classification. To construct accessible and simple models of arousal, this initial study employs a Random Forest Classifier. A Random Forest (RF) is an ensemble learning method, which operates by constructing a number of randomly initialised decision trees and uses the mode of their independent predictions as its output. Decision trees are simple learning algorithms, which operate through an acyclical network of nodes that split the decision process along smaller feature sets and model the prediction as a tree of decisions  [77] . In this paper we are using the RF implementation in the Scikit-learn Python library  [78] . We initialise RFs with their default parameters. For controlling overfitting we set the number of estimators in the RF to 100 and the maximum depth of each tree to 10. This experimental setup is meant to provide a simple baseline prediction performance for the dataset, and thus we are not tuning the hyperparameters of the algorithm.\n\nTo examine the validity of the general features discussed in Section V-D, models are constructed for each game based on three different feature sets: 1) game-specific features excluding general features 2) general features across games shown on Table  IV  and 3 ) all features combined. Due to the pairwise transformation discussed above, the baseline accuracy of all experiments is 50%. Because RFs are stochastic algorithms, we run each experiment 5 times and we report the 10-fold cross validation accuracy. Note that each fold contains the data of 10 to 12 participants and no two folds contain data from the same participant. The reported statistical significance is measured with two-tailed Student's t-tests with α = 0.05, adjusted with the Bonferroni correction where applicable.\n\nFigure  7  shows the performance of the RF models. Prediction accuracy varies between 58.06% and 82.50% across games. The results reveal that arousal appears to be easier to predict in some games (e.g. ApexSpeed, TopDown, and Endless) than others (e.g. TinyCars, Shootout, and Run'N'Gun). In the racing and platformer genres, games with fewer input options and an automatic progression system (ApexSpeed and Endless respectively) are tied to higher model performance. An explanation could be that games with more internal structure (due to the sparsity of actions the player can take and automatic progression through the game with minimal input) present a simpler problem. An exception to this observation is Shootout, in which the controls are limited (only looking around and shooting) and enemies appearing in an everincreasing speed, but despite these similarities with ApexSpeed and Endless, Shootout models are struggling to reach 60% accuracy (the lowest performance across all games).\n\nLooking at individual games across different feature sets, we observe that the general features manage to perform comparably to the specific features independently of the game tested. Game-specific features yield significantly higher performances than general features only in 4 games (TinyCars, Solid, Endless, and Pirates!). Moreover, the combination of both specific and general features yields significantly more accurate arousal models than either the game-specific or general features (or both) in 5 games: Solid, Heist!, TopDown, Endless, and Pirates!. These results demonstrate the robustness of the general features presented in Section V-D and show that there is little to no trade-off in representing the presented games in a more abstract and general manner.\n\nThe arousal model performances presented in this section highlight a number of challenges for future research. Firstly, the differences in performances between games show that the complexity of the affect modelling task is dependent on the characteristics of the elicitor and the game context. Finding new processing methods, data treatment, algorithms, and model architectures which perform equally well across different games is an open problem. Secondly, the robustness demonstrated by the general features proposed in this paper point towards the possibility of general player affect modelling across games. While research has already been investigating general player modelling in video games  [4] , early results showed only moderate success. The dataset and baselines presented in this paper provide a large open source database of games with robust enough general features to continue the exploration of general player modelling.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Vii. Discussion",
      "text": "This paper presented the AGAIN dataset, a database for affect modelling in video games. The dataset contains data from 124 players and includes game telemetry, gameplay videos, and arousal annotations of 1, 116 gameplay sessions. The paper also presented the dataset, discussed the underlying trends in the data, and showcased some preliminary preference learning models. In this section, we discuss some of the limitations and propose avenues for future work, before concluding the paper.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "A. Limitations",
      "text": "While the crowd-sourcing protocol for data collection enabled a larger dataset with high extensibility potential, most of the limitations of AGAIN stem from the same crowd-sourced protocol. Collected data lacks modalities traditionally associated with AC datasets. Neither physiological signals nor facial expression data is collected, as the online procedure focused on behavioural telemetry instead. While AGAIN features no peripheral signals, the dataset also contains over 37 hours of gameplay video footage, which can support a number of computer vision-based applications  [5] ,  [55] .\n\nWhereas many affective datasets are composed of multiple affective labels-with arousal and valence being the most common-the AGAIN dataset focuses only on arousal. As the game-playing task is already a lengthy and involved process, annotating multiple affective dimensions was infeasible during data collection. The choice of arousal was motivated by this affective dimension's strong connection to the dynamics of gameplay. This is especially important for first-person annotations, as games already encode positive and negative events (in the form of helpful and detrimental effects to the goal of the game) which can be assessed by third-person annotators later. However, the subjectively perceived dynamics of the game might differ from an observer's impressions.\n\nWhile each game elicits similar playstyles across different participants, the database features unique videos with selfannotated arousal traces. AGAIN puts an emphasis on selfreported labels as it is expected to yield ground truths of affect that are closer to the experience  [9] ,  [69] ,  [79] . The existing in-game footage of AGAIN, however, can be used directly for third-person annotation in future studies. Regardless of the annotation scheme used (first vs. third person) AGAIN annotations are captured in an unbounded fashion which eliminates high degrees of reporting bias  [9] ,  [10] .\n\nA necessary but limiting factor is that collection of affect labels is not concurrent with the collection of video and telemetry data. Collecting reliable first-person affect labels simultaneously to video game play is impossible due to the high cognitive demand of the task; however, the stimulated recall technique applied here does pose some limitations to the annotation process as certain temporal biases can arise.\n\nFinally, many game sessions in the dataset have an overall increasing intensity, which is reflected in the corresponding affective labels as well. While this is a limitation of the dataset, it is also a limitation of the domain. Although the same increasing intensity is true to even high-budget console and PC games on the macro level, this dynamic is especially true to short casual and mobile games played over short periods.\n\nWe note that the presented machine learning models are quite preliminary, aiming to showcase a use-case for the dataset along with a proposed cleaning and modelling pipeline. Future studies should look into training and tuning more complex models on AGAIN. However, as the published dataset contains both the clean and raw data, future work can propose different processing methods. While the presented models show some level of robustness, they do not use all of the data the dataset has to offer, such as the captured videos.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "B. Future Work",
      "text": "The AGAIN dataset was created to facilitate games user modelling through the lens of affective computing. The dataset allows for the adoption of machine learning techniques that use game telemetry and video data to model player arousal. While a more traditional approach was presented here, future studies should utilise the available video database and apply deep leaning methods to create more complex models. As the dataset contains a large set of games, AGAIN is especially useful for research into general affect models. Future studies should focus on the transferability of models across different games and genres in the dataset  [71] . The current dataset only encodes one affective dimension, arousal, across videos from nine games; AGAIN, however is easily scalable to more affective dimensions and more game-based affect stimuli. Future work will focus on expanding the labels with expert annotations of valence and dominance to match the format of other affective computing databases  [39] ,  [41] ,  [43] ,  [44] . Its accessibility and its unobtrusive data collection via crowdsourcing make AGAIN easily extendable to more affect labels, affect elicitors and participants.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Viii. Conclusion",
      "text": "This paper introduced a new database for affect modelling, the AGAIN dataset. AGAIN is the largest and most diverse publicly available dataset coupling gameplay context, gameplay videos, and annotated affect to date. It includes a variety of interactive elicitors, in the form of nine games from three popular yet dissimilar game genres. In particular, the dataset consists of 37 hours of video footage accompanied by telemetry and self-annotated arousal labels from 1, 116 gameplay sessions played by 124 participants. The motivation behind the construction of this dataset is to facilitate and further advance research on general player modelling through a clean, largescale, diverse (elicitor-wise) and accessible database.\n\nInspired by recent work on the importance of gameplay context as a predictor of affect  [5] , the user modalities of AGAIN are currently limited to in-game video footage and behavioural telemetry data. In addition, the protocol of AGAIN limits the user modalities available so that crowdsourcing of selfreported affect annotations is both feasible and efficient. While AGAIN puts an emphasis on accessibility-soliciting game context and behavioural data from users as its modalities-the AGAIN games can be used for small-scale, lab-based affect studies that incorporate more user modalities including visual and auditory player cues (e.g.  [48] ,  [55] ).\n\nGiven the characteristics of a unique set of diverse elicitors, a large participant count, first-person annotations and a largescale video and game telemetry database, AGAIN couples important aspects of affective computing with core aspects of game user modelling-thereby enabling research in the area of general player modelling, in games and beyond.",
      "page_start": 12,
      "page_end": 13
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: All games featured in the AGAIN dataset currently. The dataset",
      "page": 1
    },
    {
      "caption": "Figure 2: a). Its controls are the hardest to master due to",
      "page": 5
    },
    {
      "caption": "Figure 2: b). As the player",
      "page": 5
    },
    {
      "caption": "Figure 2: c). While the player",
      "page": 5
    },
    {
      "caption": "Figure 2: d). Because the player has to wait for their",
      "page": 5
    },
    {
      "caption": "Figure 2: g). Subsequently, Endless is one of the",
      "page": 5
    },
    {
      "caption": "Figure 2: h). This game has a more relaxed pace as the gameplay",
      "page": 5
    },
    {
      "caption": "Figure 2: i). Thanks to the shooting mechanics, number of enemies,",
      "page": 5
    },
    {
      "caption": "Figure 2: Start screens of the nine games included in the AGAIN dataset, showing the game’s rules and players’ controls.",
      "page": 6
    },
    {
      "caption": "Figure 3: Introduction screen of the experiment.",
      "page": 6
    },
    {
      "caption": "Figure 2: b), therefore the",
      "page": 7
    },
    {
      "caption": "Figure 4: The PAGAN RankTrace annotation interface. The gameplay video",
      "page": 8
    },
    {
      "caption": "Figure 4: ). Due this collection method, the value",
      "page": 8
    },
    {
      "caption": "Figure 5: Distribution of summed cumulative DTW distance values of each",
      "page": 8
    },
    {
      "caption": "Figure 6: shows the average annotation trace as calculated by",
      "page": 9
    },
    {
      "caption": "Figure 6: ), on the other hand, tend to have arousal converging",
      "page": 9
    },
    {
      "caption": "Figure 6: Average annotation traces (normalised per session) showing an",
      "page": 10
    },
    {
      "caption": "Figure 7: Performance of random forest models of arousal for each game with",
      "page": 11
    },
    {
      "caption": "Figure 7: shows the performance of the RF models. Pre-",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Number of Participants": "Number of Gameplay Videos",
          "124\n122": "1116\n995"
        },
        {
          "Number of Participants": "Number of Game-telemetry Logs",
          "124\n122": "1116\n995"
        },
        {
          "Number of Participants": "Video database size",
          "124\n122": "37+ hours\n33+ hours"
        },
        {
          "Number of Participants": "Number of Elicitors",
          "124\n122": "9 games (3 genres)"
        },
        {
          "Number of Participants": "Gameplay/Video duration",
          "124\n122": "2 min"
        },
        {
          "Number of Participants": "Annotation Perspective",
          "124\n122": "First-person"
        },
        {
          "Number of Participants": "Annotation Type",
          "124\n122": "Continuous unbounded"
        },
        {
          "Number of Participants": "Affective Labels",
          "124\n122": "Arousal"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MAHNOB-\nHCI\n[38]": "DEAP\n[39]",
          "No": "No",
          "Video": "Video",
          "20\nvideos": "40\nvideos",
          "20\nhours": "40\nmins",
          "30": "32",
          "EEG, ECG, EDA,\ntemp.,\nresp.,\nface\nand body video,\ngaze, audio": "EEG, BVP, EDA,\nEMG,\ntemp.,\nresp.,\nface video",
          "First-person": "First-person",
          "Discrete\n(9-step)": "Discrete\n(5-step)",
          "Arousal, valence,\ndominance, emotional\nkeywords, predictability": "Arousal, valence,\ndominance,\nliking,\nfamiliarity",
          "self-report": "self-report",
          "20": "40"
        },
        {
          "MAHNOB-\nHCI\n[38]": "LIRIS-\nACCEDE\n[40]",
          "No": "No",
          "Video": "Video",
          "20\nvideos": "9, 800\nvideos",
          "20\nhours": "27\nhours",
          "30": "N/A",
          "EEG, ECG, EDA,\ntemp.,\nresp.,\nface\nand body video,\ngaze, audio": "N/A",
          "First-person": "First-person",
          "Discrete\n(9-step)": "Pairwise",
          "Arousal, valence,\ndominance, emotional\nkeywords, predictability": "Arousal, valence",
          "self-report": "1517(arousal)\n2442(valence)",
          "20": "UNK"
        },
        {
          "MAHNOB-\nHCI\n[38]": "Aff-Wild\n[41]",
          "No": "No",
          "Video": "Video",
          "20\nvideos": "298\nvideos",
          "20\nhours": "30\nhours",
          "30": "200",
          "EEG, ECG, EDA,\ntemp.,\nresp.,\nface\nand body video,\ngaze, audio": "N/A",
          "First-person": "Third-person",
          "Discrete\n(9-step)": "Continuous\nbounded",
          "Arousal, valence,\ndominance, emotional\nkeywords, predictability": "Arousal, valence",
          "self-report": "6-8",
          "20": "298"
        },
        {
          "MAHNOB-\nHCI\n[38]": "AffectNet\n[42]",
          "No": "No",
          "Video": "Image",
          "20\nvideos": "450, 000\nimages",
          "20\nhours": "N/A",
          "30": "N/A",
          "EEG, ECG, EDA,\ntemp.,\nresp.,\nface\nand body video,\ngaze, audio": "N/A",
          "First-person": "Third-person",
          "Discrete\n(9-step)": "Continuous\nbounded,\ncategorical",
          "Arousal, valence,\ndominance, emotional\nkeywords, predictability": "Arousal, valence, 8\nemotion categories",
          "self-report": "12",
          "20": "137, 500"
        },
        {
          "MAHNOB-\nHCI\n[38]": "Sonancia\n[18]",
          "No": "No",
          "Video": "Audio",
          "20\nvideos": "1280\nsounds",
          "20\nhours": "N/A",
          "30": "N/A",
          "EEG, ECG, EDA,\ntemp.,\nresp.,\nface\nand body video,\ngaze, audio": "N/A",
          "First-person": "First-person",
          "Discrete\n(9-step)": "Pairwise",
          "Arousal, valence,\ndominance, emotional\nkeywords, predictability": "Arousal, valence,\ntension",
          "self-report": "UNK",
          "20": "10"
        },
        {
          "MAHNOB-\nHCI\n[38]": "SEWA\nDB [43]",
          "No": "Yes",
          "Video": "Video",
          "20\nvideos": "4\nvideos\n1 task",
          "20\nhours": "27\nhours\n17\nhours",
          "30": "398",
          "EEG, ECG, EDA,\ntemp.,\nresp.,\nface\nand body video,\ngaze, audio": "Facial\nlandmarks,\nFAU, hand and head\ngestures",
          "First-person": "Third-person",
          "Discrete\n(9-step)": "Continuous\nbounded",
          "Arousal, valence,\ndominance, emotional\nkeywords, predictability": "Arousal, valence (dis)liking\nintensity, agreement,\nmimicry",
          "self-report": "5",
          "20": "90"
        },
        {
          "MAHNOB-\nHCI\n[38]": "RELOCA\n[44]",
          "No": "Yes",
          "Video": "Video",
          "20\nvideos": "1 task",
          "20\nhours": "4\nhours",
          "30": "46",
          "EEG, ECG, EDA,\ntemp.,\nresp.,\nface\nand body video,\ngaze, audio": "ECG, EDA,\nface\nvideo, audio",
          "First-person": "Third-person",
          "Discrete\n(9-step)": "Continuous\nbounded",
          "Arousal, valence,\ndominance, emotional\nkeywords, predictability": "Arousal, valence",
          "self-report": "6",
          "20": "23"
        },
        {
          "MAHNOB-\nHCI\n[38]": "GAME-\nON\n[45]",
          "No": "Yes",
          "Video": "Social game",
          "20\nvideos": "1\ngame",
          "20\nhours": "11.5\nhours",
          "30": "51",
          "EEG, ECG, EDA,\ntemp.,\nresp.,\nface\nand body video,\ngaze, audio": "Video, audio, and\nmotion capture data",
          "First-person": "First-person",
          "Discrete\n(9-step)": "Discrete\n(5–9-step)",
          "Arousal, valence,\ndominance, emotional\nkeywords, predictability": "Emotions, cohesion,\nwarmth, competence,\ncompetitivity,\nleadership,\nand motivation",
          "self-report": "self-report",
          "20": "5"
        },
        {
          "MAHNOB-\nHCI\n[38]": "MUMBAI\n[46]",
          "No": "Yes",
          "Video": "Board-game",
          "20\nvideos": "6\ngames",
          "20\nhours": "46\nhours",
          "30": "58",
          "EEG, ECG, EDA,\ntemp.,\nresp.,\nface\nand body video,\ngaze, audio": "Gameplay,\nfacial\nvideo, and facial\naction units",
          "First-person": "First-person\nand\nThird-person",
          "Discrete\n(9-step)": "Discrete\nlabels",
          "Arousal, valence,\ndominance, emotional\nkeywords, predictability": "Valence, attention,\ngameplay experience,\npersonality",
          "self-report": "56\n(Third-person)\n58\n(First-person)",
          "20": "6"
        },
        {
          "MAHNOB-\nHCI\n[38]": "MazeBall\n[47]",
          "No": "Yes",
          "Video": "Videogame",
          "20\nvideos": "1\ngame",
          "20\nhours": "N/A",
          "30": "36",
          "EEG, ECG, EDA,\ntemp.,\nresp.,\nface\nand body video,\ngaze, audio": "BVP(HRV), EDA,\ngame telemetry",
          "First-person": "First-person",
          "Discrete\n(9-step)": "Pairwise",
          "Arousal, valence,\ndominance, emotional\nkeywords, predictability": "Fun, challenge,\nfrustration,\nanxiety, boredom,\nexcitement,\nrelaxation",
          "self-report": "self-report",
          "20": "1"
        },
        {
          "MAHNOB-\nHCI\n[38]": "PED [48]",
          "No": "Yes",
          "Video": "Videogame",
          "20\nvideos": "1\ngame",
          "20\nhours": "6\nhours",
          "30": "58",
          "EEG, ECG, EDA,\ntemp.,\nresp.,\nface\nand body video,\ngaze, audio": "Gaze, head position,\ngame telemetry",
          "First-person": "First-person",
          "Discrete\n(9-step)": "Discrete\n(5-step),\npairwise",
          "Arousal, valence,\ndominance, emotional\nkeywords, predictability": "Engagement,\nfrustration,\nchallenge",
          "self-report": "self-report",
          "20": "1"
        },
        {
          "MAHNOB-\nHCI\n[38]": "FUNii\n[49]",
          "No": "Yes",
          "Video": "Videogame",
          "20\nvideos": "2\ngames",
          "20\nhours": "N/A",
          "30": "190",
          "EEG, ECG, EDA,\ntemp.,\nresp.,\nface\nand body video,\ngaze, audio": "ECG, EDA, gaze\nand head position,\ncontroller\ninput",
          "First-person": "First-person",
          "Discrete\n(9-step)": "Continuous,\ndiscrete",
          "Arousal, valence,\ndominance, emotional\nkeywords, predictability": "Fun (cont.),\nfun, difﬁculty,\nworkload,\nimmersion, UX",
          "self-report": "self-report",
          "20": "2"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Racing": "Shooter",
          "Arcade-Racing\nRally\nSpeed-Racer": "First-Person Shooter\nTop-down Shooter\nArcade-Shooter",
          "TinyCars\nSolid\nApexSpeed": "Heist!\nTopDown\nShootout",
          "33\n34\n34": "37\n38\n23"
        },
        {
          "Racing": "Platform",
          "Arcade-Racing\nRally\nSpeed-Racer": "Endless Runner\nMario-Clone\nShoot’Em’Up",
          "TinyCars\nSolid\nApexSpeed": "Endless\nPirates!\nRun’N’Gun",
          "33\n34\n34": "33\n39\n47"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Gut reactions: A perceptual theory of emotion",
      "authors": [
        "J Prinz"
      ],
      "year": "2004",
      "venue": "Gut reactions: A perceptual theory of emotion"
    },
    {
      "citation_id": "2",
      "title": "Descartes' error: Emotion, rationality and the human brain",
      "authors": [
        "A Damasio"
      ],
      "year": "1994",
      "venue": "Descartes' error: Emotion, rationality and the human brain"
    },
    {
      "citation_id": "3",
      "title": "General general game AI",
      "authors": [
        "J Togelius",
        "G Yannakakis"
      ],
      "year": "2016",
      "venue": "Proc. of the IEEE Conference on Computational Intelligence and Games (CIG)"
    },
    {
      "citation_id": "4",
      "title": "Towards general models of player affect",
      "authors": [
        "E Camilleri",
        "G Yannakakis",
        "A Liapis"
      ],
      "year": "2017",
      "venue": "Proc. of the Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "5",
      "title": "From pixels to affect: a study on games and player experience",
      "authors": [
        "K Makantasis",
        "A Liapis",
        "G Yannakakis"
      ],
      "year": "2019",
      "venue": "Proc. of the Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "6",
      "title": "Human-level control through deep reinforcement learning",
      "authors": [
        "V Mnih",
        "K Kavukcuoglu",
        "D Silver",
        "A Rusu",
        "J Veness",
        "M Bellemare",
        "A Graves",
        "M Riedmiller",
        "A Fidjeland",
        "G Ostrovski"
      ],
      "year": "2015",
      "venue": "Nature"
    },
    {
      "citation_id": "7",
      "title": "PAGAN: Video affect annotation made easy",
      "authors": [
        "D Melhart",
        "A Liapis",
        "G Yannakakis"
      ],
      "year": "2019",
      "venue": "Proc. of the Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "8",
      "title": "Ranktrace: Relative and unbounded affect annotation",
      "authors": [
        "P Lopes",
        "G Yannakakis",
        "A Liapis"
      ],
      "year": "2017",
      "venue": "Proc. of the Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "9",
      "title": "The ordinal nature of emotions: An emerging approach",
      "authors": [
        "G Yannakakis",
        "R Cowie",
        "C Busso"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "10",
      "title": "The ordinal nature of emotions",
      "year": "2017",
      "venue": "Proc. of the Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "11",
      "title": "The Ekman 60 faces test as a diagnostic instrument in frontotemporal dementia",
      "authors": [
        "J Diehl-Schmid",
        "C Pohl",
        "C Ruprecht",
        "S Wagenpfeil",
        "H Foerstl",
        "A Kurz"
      ],
      "year": "2007",
      "venue": "Archives of Clinical Neuropsychology"
    },
    {
      "citation_id": "12",
      "title": "Avoid violence, rioting, and outrage; approach celebration, delight, and strength: Using large text corpora to compute valence, arousal, and the basic emotions",
      "authors": [
        "C Westbury",
        "J Keith",
        "B Briesemeister",
        "M Hofmann",
        "A Jacobs"
      ],
      "year": "2015",
      "venue": "The Quarterly Journal of Experimental Psychology"
    },
    {
      "citation_id": "13",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "14",
      "title": "The hourglass of emotions",
      "authors": [
        "E Cambria",
        "A Livingstone",
        "A Hussain"
      ],
      "year": "2012",
      "venue": "Cognitive behavioural systems"
    },
    {
      "citation_id": "15",
      "title": "Basic dimensions for a general psychological theory implications for personality, social, environmental, and developmental studies",
      "authors": [
        "A Mehrabian"
      ],
      "year": "1980",
      "venue": "Basic dimensions for a general psychological theory implications for personality, social, environmental, and developmental studies"
    },
    {
      "citation_id": "16",
      "title": "Psychophysiology of challenge in play: EDA and self-reported arousal",
      "authors": [
        "M Klarkowski",
        "D Johnson",
        "P Wyeth",
        "C Phillips",
        "S Smith"
      ],
      "year": "2016",
      "venue": "Proc. of the CHI Conference Extended Abstracts on Human Factors in Computing Systems"
    },
    {
      "citation_id": "17",
      "title": "An empirical validation of consumer video game engagement: A playfulconsumption experience approach",
      "authors": [
        "A Abbasi",
        "D Ting",
        "H Hlavacs",
        "L Costa",
        "A Veloso"
      ],
      "year": "2019",
      "venue": "Entertainment Computing"
    },
    {
      "citation_id": "18",
      "title": "Modelling affect for horror soundscapes",
      "authors": [
        "P Lopes",
        "A Liapis",
        "G Yannakakis"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "19",
      "title": "Biometrics and classifier fusion to predict the fun-factor in video gaming",
      "authors": [
        "A Clerico",
        "C Chamberland",
        "M Parent",
        "P.-E Michon",
        "S Tremblay",
        "T Falk",
        "J.-C Gagnon",
        "P Jackson"
      ],
      "year": "2016",
      "venue": "Proc. of the IEEE Conference on Computational Intelligence and Games (CIG)"
    },
    {
      "citation_id": "20",
      "title": "Towards a comprehensive model of mediating frustration in videogames",
      "authors": [
        "D Melhart"
      ],
      "year": "2018",
      "venue": "Game Studies"
    },
    {
      "citation_id": "21",
      "title": "Personality correlates of psychological flow states in videogame play",
      "authors": [
        "J Seger",
        "R Potts"
      ],
      "year": "2012",
      "venue": "Current Psychology"
    },
    {
      "citation_id": "22",
      "title": "Exploring the effects of videogame play on creativity performance and emotional responses",
      "authors": [
        "-H Yeh"
      ],
      "year": "2015",
      "venue": "Computers in Human Behavior"
    },
    {
      "citation_id": "23",
      "title": "Effects of valence and arousal on working memory performance in virtual reality gaming",
      "authors": [
        "D Gabana",
        "L Tokarchuk",
        "E Hannon",
        "H Gunes"
      ],
      "year": "2017",
      "venue": "Proc. of the Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "24",
      "title": "Artificial Intelligence and Games",
      "authors": [
        "G Yannakakis",
        "J Togelius"
      ],
      "year": "2018",
      "venue": "Artificial Intelligence and Games"
    },
    {
      "citation_id": "25",
      "title": "Player behavioural modelling for video games",
      "authors": [
        "S Bakkes",
        "P Spronck",
        "G Van Lankveld"
      ],
      "year": "2012",
      "venue": "Entertainment Computing"
    },
    {
      "citation_id": "26",
      "title": "Deep player behavior models: Evaluating a novel take on dynamic difficulty adjustment",
      "authors": [
        "J Pfau",
        "J Smeddinck",
        "R Malaka"
      ],
      "year": "2019",
      "venue": "Proc. of the CHI Conference Extended Abstracts on Human Factors in Computing Systems"
    },
    {
      "citation_id": "27",
      "title": "Predicting player behavior in Tomb Raider: Underworld",
      "authors": [
        "T Mahlmann",
        "A Drachen",
        "J Togelius",
        "A Canossa",
        "G Yannakakis"
      ],
      "year": "2010",
      "venue": "Proc. of the IEEE Symposium on Computational Intelligence and Games (CIG)"
    },
    {
      "citation_id": "28",
      "title": "Churn prediction in mobile social games: towards a complete assessment using survival ensembles",
      "authors": [
        "Á Periáñez",
        "A Saas",
        "A Guitart",
        "C Magne"
      ],
      "year": "2016",
      "venue": "Proc. of the Conference on Data Science and Advanced Analytics"
    },
    {
      "citation_id": "29",
      "title": "Playtime measurement with survival analysis",
      "authors": [
        "M Viljanen",
        "A Airola",
        "J Heikkonen",
        "T Pahikkala"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Games"
    },
    {
      "citation_id": "30",
      "title": "Your gameplay says it all: Modelling motivation in Tom Clancy's The Division",
      "authors": [
        "D Melhart",
        "A Azadvar",
        "A Canossa",
        "A Liapis",
        "G Yannakakis"
      ],
      "year": "2019",
      "venue": "Proc. of the IEEE Conference on Games (CoG)"
    },
    {
      "citation_id": "31",
      "title": "Moment-to-moment engagement prediction through the eyes of the observer: Pubg streaming on twitch",
      "authors": [
        "D Melhart",
        "D Gravina",
        "G Yannakakis"
      ],
      "year": "2020",
      "venue": "Proc. of the Conference on the Foundations of Digital Games (FDG)"
    },
    {
      "citation_id": "32",
      "title": "Review of affective computing in education/learning: Trends and challenges",
      "authors": [
        "C.-H Wu",
        "Y.-M Huang",
        "J.-P Hwang"
      ],
      "year": "2016",
      "venue": "British Journal of Educational Technology"
    },
    {
      "citation_id": "33",
      "title": "The affective computing approach to affect measurement",
      "authors": [
        "S Mello",
        "A Kappas",
        "J Gratch"
      ],
      "year": "2018",
      "venue": "Emotion Review"
    },
    {
      "citation_id": "34",
      "title": "Generic physiological features as predictors of player experience",
      "authors": [
        "H Martínez",
        "M Garbarino",
        "G Yannakakis"
      ],
      "year": "2011",
      "venue": "Proc. of the Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "35",
      "title": "Towards generic models of player experience",
      "authors": [
        "N Shaker",
        "M Shaker",
        "M Abou-Zleikha"
      ],
      "year": "2015",
      "venue": "Proc. of the Conference on Artificial Intelligence and Interactive Digital Entertainment (AIIDE)"
    },
    {
      "citation_id": "36",
      "title": "Transfer learning for cross-game prediction of player experience",
      "authors": [
        "N Shaker",
        "M Abou-Zleikha"
      ],
      "year": "2016",
      "venue": "Proc. of the IEEE Conference on Computational Intelligence and Games (CIG)"
    },
    {
      "citation_id": "37",
      "title": "From theory to behaviour: Towards a general model of engagement",
      "authors": [
        "V Bonometti",
        "C Ringer",
        "M Ruiz",
        "A Wade",
        "A Drachen"
      ],
      "year": "2020",
      "venue": "From theory to behaviour: Towards a general model of engagement",
      "arxiv": "arXiv:2004.12644"
    },
    {
      "citation_id": "38",
      "title": "A multimodal database for affect recognition and implicit tagging",
      "authors": [
        "M Soleymani",
        "J Lichtenauer",
        "T Pun",
        "M Pantic"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "39",
      "title": "DEAP: A database for emotion analysis using physiological signals",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani",
        "J.-S Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "40",
      "title": "LIRIS-ACCEDE: a video database for affective content analysis",
      "authors": [
        "Y Baveye",
        "E Dellandrea",
        "C Chamaret",
        "L Chen"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "41",
      "title": "Aff-wild: Valence and arousal 'in-the-wild' challenge",
      "authors": [
        "S Zafeiriou",
        "D Kollias",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "I Kotsia"
      ],
      "year": "2017",
      "venue": "Proc. of the IEEE Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "42",
      "title": "AffectNet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "43",
      "title": "SEWA DB: A rich database for audio-visual emotion and sentiment research in the wild",
      "authors": [
        "J Kossaifi",
        "R Walecki",
        "Y Panagakis",
        "J Shen",
        "M Schmitt",
        "F Ringeval",
        "J Han",
        "V Pandit",
        "A Toisoul",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "44",
      "title": "Introducing the RECOLA multimodal corpus of remote collaborative and affective interactions",
      "authors": [
        "F Ringeval",
        "A Sonderegger",
        "J Sauer",
        "D Lalanne"
      ],
      "year": "2013",
      "venue": "Proc. of the IEEE Conference and workshops on automatic face and gesture recognition"
    },
    {
      "citation_id": "45",
      "title": "GAME-ON: a multimodal dataset for cohesion and group analysis",
      "authors": [
        "L Maman",
        "E Ceccaldi",
        "N Lehmann-Willenbrock",
        "L Likforman-Sulem",
        "M Chetouani",
        "G Volpe",
        "G Varni"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "46",
      "title": "MUMBAI: multiperson, multimodal board game affect and interaction analysis dataset",
      "authors": [
        "M Doyran",
        "A Schimmel",
        "P Baki",
        "K Ergin",
        "B Türkmen",
        "A Salah",
        "S Bakkes",
        "H Kaya",
        "R Poppe",
        "A Salah"
      ],
      "year": "2021",
      "venue": "Journal on Multimodal User Interfaces"
    },
    {
      "citation_id": "47",
      "title": "Towards affective camera control in games",
      "authors": [
        "G Yannakakis",
        "H Martínez",
        "A Jhala"
      ],
      "year": "2010",
      "venue": "User Modeling and User-Adapted Interaction"
    },
    {
      "citation_id": "48",
      "title": "The platformer experience dataset",
      "authors": [
        "K Karpouzis",
        "G Yannakakis",
        "N Shaker",
        "S Asteriadis"
      ],
      "year": "2015",
      "venue": "Proc. of the Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "49",
      "title": "The FUNii database: A physiological, behavioral, demographic and subjective video game database for affective gaming and player experience research",
      "authors": [
        "N Beaudoin-Gagnon",
        "A Fortin-Côté",
        "C Chamberland",
        "L Lefebvre",
        "J Bergeron-Boucher",
        "A Campeau-Lecours",
        "S Tremblay",
        "P Jackson"
      ],
      "year": "2019",
      "venue": "Proc. of the Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "50",
      "title": "Natural affect data: Collection and annotation",
      "authors": [
        "S Afzal",
        "P Robinson"
      ],
      "year": "2011",
      "venue": "New perspectives on affect and learning technologies"
    },
    {
      "citation_id": "51",
      "title": "Game literacy in theory and practice",
      "authors": [
        "D Buckingham",
        "A Burn"
      ],
      "year": "2007",
      "venue": "Journal of Educational Multimedia and Hypermedia"
    },
    {
      "citation_id": "52",
      "title": "Making sense of genre: The logic of video game genre organization",
      "authors": [
        "J Vargas-Iglesias"
      ],
      "year": "2020",
      "venue": "Games and Culture"
    },
    {
      "citation_id": "53",
      "title": "Video game genres and advancing quantitative video game research with the genre diversity score",
      "authors": [
        "R Sevin",
        "W Decamp"
      ],
      "year": "2020",
      "venue": "The Computer Games Journal"
    },
    {
      "citation_id": "54",
      "title": "",
      "authors": [
        "P Lankoski",
        "S Björk"
      ],
      "year": "2015",
      "venue": ""
    },
    {
      "citation_id": "55",
      "title": "The pixels and sounds of emotion: General-purpose representations of arousal in games",
      "authors": [
        "K Makantasis",
        "A Liapis",
        "G Yannakakis"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "56",
      "title": "Privileged information for modeling affect in the wild",
      "authors": [
        "K Makantasis",
        "D Melhart",
        "A Liapis",
        "G Yannakakis"
      ],
      "venue": "Proc. of the Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "57",
      "title": "A new learning paradigm: Learning using privileged information",
      "authors": [
        "V Vapnik",
        "A Vashist"
      ],
      "year": "2009",
      "venue": "Neural networks"
    },
    {
      "citation_id": "58",
      "title": "Deep multimodal fusion: Combining discrete events and continuous signals",
      "authors": [
        "H Martínez",
        "G Yannakakis"
      ],
      "year": "2014",
      "venue": "Proc. of the Conference on Multimodal Interaction"
    },
    {
      "citation_id": "59",
      "title": "Don't classify ratings of affect; rank them",
      "authors": [
        "H Martinez",
        "G Yannakakis",
        "J Hallam"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "60",
      "title": "Gtrace: General trace program compatible with EmotionML",
      "authors": [
        "R Cowie",
        "M Sawey",
        "C Doherty",
        "J Jaimovich",
        "C Fyans",
        "P Stapleton"
      ],
      "year": "2013",
      "venue": "Proc. of the Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "61",
      "title": "Grounding truth via ordinal annotation",
      "authors": [
        "G Yannakakis",
        "H Martinez"
      ],
      "year": "2015",
      "venue": "Proc. of the Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "62",
      "title": "An opponent-process theory of motivation: I. temporal dynamics of affect",
      "authors": [
        "R Solomon",
        "J Corbit"
      ],
      "year": "1974",
      "venue": "Psychological Review"
    },
    {
      "citation_id": "63",
      "title": "Anchors, scales and the relative coding of value in the brain",
      "authors": [
        "B Seymour",
        "S Mcclure"
      ],
      "year": "2008",
      "venue": "Current Opinion in Neurobiology"
    },
    {
      "citation_id": "64",
      "title": "Emotional context modulates subsequent memory effect",
      "authors": [
        "S Erk",
        "M Kiefer",
        "J Grothe",
        "A Wunderlich",
        "M Spitzer",
        "H Walter"
      ],
      "year": "2003",
      "venue": "Neuroimage"
    },
    {
      "citation_id": "65",
      "title": "Experimental comparison of representation methods and distance measures for time series data",
      "authors": [
        "X Wang",
        "A Mueen",
        "H Ding",
        "G Trajcevski",
        "P Scheuermann",
        "E Keogh"
      ],
      "year": "2013",
      "venue": "Data Mining and Knowledge Discovery"
    },
    {
      "citation_id": "66",
      "title": "Faster and more accurate classification of time series by exploiting a novel dynamic time warping averaging algorithm",
      "authors": [
        "F Petitjean",
        "G Forestier",
        "G Webb",
        "A Nicholson",
        "Y Chen",
        "E Keogh"
      ],
      "year": "2016",
      "venue": "Knowledge and Information Systems"
    },
    {
      "citation_id": "67",
      "title": "Adaptively constrained dynamic time warping for time series classification and clustering",
      "authors": [
        "H Li",
        "J Liu",
        "Z Yang",
        "R Liu",
        "K Wu",
        "Y Wan"
      ],
      "year": "2020",
      "venue": "Information Sciences"
    },
    {
      "citation_id": "68",
      "title": "Using dynamic time warping to find patterns in time series",
      "authors": [
        "D Berndt",
        "J Clifford"
      ],
      "year": "1994",
      "venue": "Proce. of the AAA1 Workshop on Knowledge Discovery in Databases"
    },
    {
      "citation_id": "69",
      "title": "Annotation and processing of continuous emotional attributes: Challenges and opportunities",
      "authors": [
        "A Metallinou",
        "S Narayanan"
      ],
      "year": "2013",
      "venue": "Proc. of the IEEE Conference and workshops on automatic face and gesture recognition"
    },
    {
      "citation_id": "70",
      "title": "I Feel I Feel You: A theory of mindexperiment in games",
      "authors": [
        "D Melhart",
        "A Liapis",
        "G Yannakakis"
      ],
      "year": "2019",
      "venue": "Künstliche Intelligenz"
    },
    {
      "citation_id": "71",
      "title": "Towards general models of player experience: A study within genres",
      "venue": "Proc. of the IEEE Conference on Games (CoG)"
    },
    {
      "citation_id": "72",
      "title": "Trace it like you believe it: Time-continuous believability prediction",
      "authors": [
        "C Pacheco",
        "D Melhart",
        "A Liapis",
        "G Yannakakis",
        "D Perez-Liebana"
      ],
      "venue": "Proc. of the Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "73",
      "title": "Emotion recognition via random forest and galvanic skin response: Comparison of time based feature sets, window sizes and wavelet approaches",
      "authors": [
        "D Ayata",
        "Y Yaslan",
        "M Kamas"
      ],
      "year": "2016",
      "venue": "Proc. of the IEEE Congress on Medical Technologies National"
    },
    {
      "citation_id": "74",
      "title": "Analysis and compensation of the reaction lag of evaluators in continuous emotional annotations",
      "authors": [
        "S Mariooryad",
        "C Busso"
      ],
      "year": "2013",
      "venue": "Proc. of the Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "75",
      "title": "Preference learning",
      "authors": [
        "J Fürnkranz",
        "E Hüllermeier"
      ],
      "year": "2011",
      "venue": "Encyclopedia of Machine Learning"
    },
    {
      "citation_id": "76",
      "title": "A study on affect model validity: Nominal vs ordinal labels",
      "authors": [
        "D Melhart",
        "K Sfikas",
        "G Giannakakis",
        "G Yannakakis",
        "A Liapis"
      ],
      "year": "2020",
      "venue": "Proc. of the IJCAI workshop on AI and Affective Computing"
    },
    {
      "citation_id": "77",
      "title": "An introduction to classification and regression tree (CART) analysis",
      "authors": [
        "R Lewis"
      ],
      "year": "2000",
      "venue": "Proc. of the Society for Academic Emergency Medicine Annual Meeting"
    },
    {
      "citation_id": "78",
      "title": "Scikit-learn: Machine learning in Python",
      "authors": [
        "F Pedregosa",
        "G Varoquaux",
        "A Gramfort",
        "V Michel",
        "B Thirion",
        "O Grisel",
        "M Blondel",
        "P Prettenhofer",
        "R Weiss",
        "V Dubourg",
        "J Vanderplas",
        "A Passos",
        "D Cournapeau",
        "M Brucher",
        "M Perrot",
        "E Duchesnay"
      ],
      "year": "2011",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "79",
      "title": "Emotion data collection and its implications for affective computing",
      "authors": [
        "S Afzal",
        "P Robinson"
      ],
      "year": "2014",
      "venue": "The Oxford handbook of affective computing"
    },
    {
      "citation_id": "80",
      "title": "Antonios Liapis is a Senior Lecturer at the Institute of Digital Games, University of Malta, where he bridges the gap between game technology and game design in courses focusing on human-computer creativity, digital prototyping and game development. He received the Ph.D. degree in Information Technology from the IT University of Copenhagen in 2014",
      "year": "2018",
      "venue": "Game Research from the University of Malta in 2021. His research focuses on Machine Learning, Affective Computing, and Games User Modelling"
    }
  ]
}