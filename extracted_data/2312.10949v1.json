{
  "paper_id": "2312.10949v1",
  "title": "Leveraged Mel Spectrograms Using Harmonic And Percussive Components In Speech Emotion Recognition",
  "published": "2023-12-18T05:55:46Z",
  "authors": [
    "David Hason Rudd",
    "Huan Huo",
    "Guandong Xu"
  ],
  "keywords": [
    "Speech Emotion Recognition (SER)",
    "Mel spectrogram",
    "Convolutional Neural Network (CNN)",
    "voice signal processing",
    "acoustic features"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech Emotion Recognition (SER) affective technology enables the intelligent embedded devices to interact with sensitivity. Similarly, call centre employees recognise customers' emotions from their pitch, energy, and tone of voice so as to modify their speech for a highquality interaction with customers. This work explores, for the first time, the effects of the harmonic and percussive components of Mel spectrograms in SER. We attempt to leverage the Mel spectrogram by decomposing distinguishable acoustic features for exploitation in our proposed architecture, which includes a novel feature map generator algorithm, a CNN-based network feature extractor and a multi-layer perceptron (MLP) classifier. This study specifically focuses on effective data augmentation techniques for building an enriched hybrid-based feature map. This process results in a function that outputs a 2D image so that it can be used as input data for a pre-trained CNN-VGG16 feature extractor. Furthermore, we also investigate other acoustic features such as MFCCs, chromagram, spectral contrast, and the tonnetz to assess our proposed framework. A test accuracy of 92.79% on the Berlin EMO-DB database is achieved. Our result is higher than previous works using CNN-VGG16.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The general motivation of SER systems is to recognize specific features of a speaker's voice in different emotional situations to provide a more personal and often superior user experience  [6] . For example, a Customer Relationship Management (CRM) team can use SER to determine a customer's satisfaction by their voice during a call. Emotions are universal, although their understandings, interpretations and reflections are particular and partially associated with culture  [1] . Unlike speech recognition, there is no standard or integrated approach for recognising emotions and analysing them through human voices  [30] .\n\nThe fundamental challenge of SER is the extraction of discriminative and robust features from speech signals. Features used for SER are generally categorized as prosodic, acoustic, and linguistic features. The prosodic features include pitch, energy, and zero-crossings of the speech signal  [16, 19, 28] . The acoustic features describe speech wave properties including linear predictor coefficients (LPC), mel-scaled power spectrograms (Mel), linear predictor cepstral coefficients (LPCC), power spectral analysis (FFT), power spectrogram chroma (Chroma), and mel-frequency cepstral coefficients (MFCC)  [5] . In SER, the Mel spectrogram, MFCC, and chromagram are the most effective in decoding emotion from a signal  [22] .\n\nAmong the most common speech feature extraction techniques, this paper addresses a principal question in Emotion Recognition (ER): How can we maximise the advantage of the Mel spectrogram feature to improve SER? This study presents a novel implementation of emotion detection from speech signals by processing harmonic and percussive components of Mel spectrograms and combining the result with the log Mel spectrogram feature. Our primary contribution is the introduction of an effective hybrid acoustic feature map technique that improves SER. First, we employ CNN-VGG16 as a feature extractor of emotion identifier, then utilise the MLP networks for classification task. Furthermore, we tune the MLP network parameters using the random search model hyperparameter technique to obtain the best model. Based on empirical experiments, we assert that a data augmentation strategy using an efficient prosodic and acoustic feature combination analysis is the key to obtaining state-of-the-art results since input data represents more diversity with enriched features; these characteristics lead to better model generalisation.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Works",
      "text": "Early traditional SER models relied on modification and optimisation of Support Vector Machine (SVM) classifiers to predict emotions such as anger, happiness, and sadness, among others  [25, 23, 15] . Wu et al.  [31]  implemented a traditional machine learning method based on EMO-DB  [3]  database. The authors proposed novel sound features named Modulation Spectral Features (MSFs) that combined prosodic features, and they ultimately obtained 85.8% validation accuracy for speaker-independent classification using a multi-class Linear Discriminant Analysis (LDA) classifier. Similarly, Milton et al.  [21]  proposed another classical machine learning method for SER by using a combination of three SVMs to classify emotions in the Berlin EMO-DB. Furthermore, Huang et al.  [13]  introduced a hybrid model called a semi-CNN, which used a deep CNN to learn feature maps and a classic machine learning SVM to classify seven emotions from EMO-DB. The authors utilised spectrograms as the input for their proposed model and achieved 88.3% and 85.2% test accuracy for speaker-dependent and speaker-independent classification, respectively.\n\nThe idea of exploiting pre-trained CNN image classifiers  [7]  for other tasks involves leveraging transfer learning methods in SER. Surprisingly, using speech-based spectrograms as the input images for pre-trained image classifiers produced competitive results when compared with other well-known traditional methods. Badshah et al.  [2]  extracted spectrogram speech features, which were then visualised in 2D images and passed to a CNN; this approach achieved a 52% test accuracy on EMO-DB. Demircan and Kahramanli  [8]  developed several different classifiers and obtained test accuracies 92.86%, 92.86%, and 90%, respectively on SVM, KNN and ANN. Additionally, Wang et al.  [29]  worked on MFCCs feature and proposed an acoustic feature called the Fourier Parameter (FP) , which obtained 73.3% average accuracy with an SVM classifier. Furthermore, many similar studies were conducted on different databases. Popova et al.  [24]  used a fine-tuned DNN and CNN-VGG16 classifier to extract the Mel spectrogram features in the RAVDESS dataset  [17]  and obtained an accuracy of 71%  [24] . Satt et al.  [27]  presented another multi-modal LSTM-CNN and proposed a novel feature extraction method based on the paralingual data from spectrograms. The authors obtained 68% accuracy on the IMOCAP  [4]  database.\n\nIn recent years, some works proposed the use of hybrid feature map techniques as input data for CNN-based networks. Meng et al.  [20]  proposed a feature extraction strategy for Log-Mel spectrograms that extracted a 3D voice feature representation map by combining log Mel spectrograms with the first and second derivatives of the log MelSpec of the raw speech signal. The authors proposed a CNN with a multimodal dilated architecture that used a residual block and BiLSTM (ADRNN) to improve the classifier accuracy. In addition, the ADRNN further enhanced the extraction of speech features using the proposed attention mechanism approach. The model achieved a remarkable performance of 74.96% and the 90.78% accuracy of the IEMOCAP and EMO-DB databases. On the other hand, Hajarolasvadi et al. introduced a 3D feature frame technique for use as input data to the network by extracting an 88-dimensional vector of voice features including MFCCs, intensity, and pitch. The model can reduce speech signal feature frames by applying k-means clustering on the extracted features and selecting the k most discriminant frames as keyframes. Then, the feature data placed in the keyframe sequence were encapsulated in a 3D tensor, which produced a final extracted feature map for use as input data for a 3D-CNN-based classifier that used the 10-fold cross-validation method. The authors achieved a weighted accuracy of 72.21% on EMO-DB. Zhao et al.  [33]  proposed a multimodal 2D CNN-LSTM network and extracted the log of the Mel-spectrograms from the speech signals for use as input data. The outcome of their work is stateof-the-art with the accuracy of 95.89% for speaker-independent classification on the Berlin EMO-DB.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Methodology",
      "text": "This section explains the work procedures used to build the hybrid feature map representation in our model. We compute the average of the signal's harmonic and percussive components and combine the result with the log Mel spectrogram feature. The proposed hybrid feature map method can be generalised with other supervised classifiers to obtain better prediction accuracy.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Proposed Hybrid Features: Harmonic And Percussive Components Of Mel Spectrogram",
      "text": "Essential features in speech signal processing are the spectrograms on the Mel scale, chromograms  [12] , spectral contrast, the tonnetz  [12]  and MFCCs  [32] .\n\nSince the average length of the recorded voice samples are four seconds, we digitise each original utterance signal at an 88KHz sample rate using the Hanning window function  [11]  shown in  (1)  to provide sufficient frequency resolution and spectral leakage protection. Next, we apply Mel filter banks to the spectrogram by shifting 0.4ms in a window time of 23ms so that the output is a group of FFTs located next to one another. The Hanning window is described in  (1) ,\n\nwhere M represents the number of points in the output window, which is set to 128 and n denote the number of specific sample point from the signal. Finally, we construct the Mel spectrogram by multiplying the obtained energy matrix of the Mel scaled static with the STFT results formulated in  (2) ,\n\nwhere |X(k)| 2 represents the energy spectrum in the kth energy block, H(k) is a Mel-spaced filter bank function, m represents the number of filter banks, and k points to the number of FFT coefficients. LM S represents the log Mel spectrogram. To perform Mel spectrogram feature extraction, we use Librosa tools  [18]  to set the size of Mel filterbanks as 128, the window size as 2048 and hop length as 512. Fig.  1  shows the Mel spectrogram of sample voices exhibiting five emotions from the EMO-DB dataset. It is clear that the amplitude and frequency of each emotion image have a high distinction from other samples.\n\nThe first feature map is built by applying a decomposition process to the Mel spectrum using the popular method in  [9] . The decomposition method can be formulated such that the harmonic s h and percussive s p components are separated from the input signal s by applying a STFT on the frames to obtain spectrogram S of signal s as shown in definitions (  3 ) and (  4 ), where S denotes a spectrum of signal s in k th Fourier coefficient on the m th time frame, ω : [0 : N -1] := {0, 1, ..., N -1} is a sine windowing function that represents the window length N , H represents the hop size value, n indicates current frame number and N is the length of the discrete Fourier transform. We can obtain the harmonic and percussive components of the spectrum by applying a median filter in the horizontal (time-domain) and vertical (frequency-domain) direction on spectrum S. Finally, we extract the first feature map by obtaining the mean of both components as shown in the following summarised formulas in (  5 ), (  6 ) and (  7 ),\n\nand\n\nobtained by\n\nwhere denotes the multiplication element of the median filter in M H , which is the horizontal direction filtering used to obtain the H harmonic components of the original spectrogram S. Subsequently, M P represents the vertical median filtering results M P , which is the percussive component of the original spectrogram, S shown in (4). Fig.  2  shows the harmonic and percussive components as two distinctive spectrograms in the 128 Mel filterbank.\n\nThe second feature map is extracted by applying the log of the Mel spectrogram obtained in (2) to measure the sensitivity of the Mel spectrogram output value fluctuation concerning changes in the voice signal amplitude. A sample 2D hybrid feature representation in our work is visualised in Fig.  3 , which clearly shows that each sample feature map is combined in a two-dimensional image. This specific feature combination improves the prediction accuracy in a simple full contact neural network classifier based on our empirical experiments.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Model Architecture And Training",
      "text": "We use the CNN-VGG16  [26]  as a feature extractor to learn from high dimensional feature maps since the network can learn from small variations that occur in the extracted features maps. However, the high-capacity memory storage requirements for a simple classification task can be considered a partial limitation of VGG16 applications.\n\nThe details of the proposed architecture are shown in Fig.  4 ; the architecture consists of an VGG16 and MLP network, which serve as an feature extractor and emotion classifier, respectively. First, the subsamples are extracted from a fixed window size and then feature maps are built using the proposed feature map function. Therefore, the input to the VGG16 feature extractor is a 2-D feature map in the dimension of (128 x 128 x 2). The input to the MLP classifier is a 2048 one-dimensional vector generated by VGG16. The MLP classifier includes four fully connected layers with the ReLU activation function and softmax in the output layer. Dense 1 and 2 have a 1024 input with a 0.5 dropout value, and dense 3 and 4 are set to 512 input with 0.3 dropouts. The ADAM optimiser with a learning rate of 0.0001 is selected for our architecture design. Fig.  4 : Model architecture, which includes a 2D hybrid feature map built using the harmonic and percussive components, as well as the log of the Mel spectrogram in the feature map generator function. The features are extracted using a CNN-VGG16 network. Finally, the MLP network classifies seven emotions.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Experimental Analysis",
      "text": "This section analyses the experimental configuration and the result of the feature extractor and MLP classifier on EMO-DB  [3] . The sample voices are randomly partitioned and 80% are used for the training set and 10% for the validation and test set for the speaker-independent classification task. We apply an oversampling strategy to compensate the minority classes and increase the voice samples before feeding them to the feature extractor network during the pre-processing phase. The classifier is trained on 128 epochs with a batch size of 128 and used an Nvidia GPU. The window size is set to 2048 with (128 x 128) bands and frames to obtain each subsample length = 2.9 sec. Then, the subsamples are created in each defined data frame. Finally, 167426 signal subsamples and 9717 feature maps are obtained from a sample rate of 88KHz. Based on the time-frequency trade-off, large frame size is chosen to obtain high-frequency resolution rather than time resolution since analysing the frequency of speech signal enables us to decode emotion. Several time-consuming experiments are conducted to assess the effectiveness of the proposed hybrid feature, which aims to find the best data augmentation through feature combination.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Results Analysis",
      "text": "To assess our enriched feature representation method in the MLP classifier, the result of evaluation metrics such as the confusion matrix and test accuracy are observed on different sample rates and feature map dimensions (bands and frames). We also evaluate our model output based on the setting of various parameters in the feature map function. For example, the prediction accuracy results based on some different parameters setting are shown in Table  1 . These results indicate that the superior result is achieved on feature map dimensions of 128 x 128 with a sample rate of 88200, Since the highest subsample length of 2.9seconds is achieved and more sample points can contribute in each subsample. We examine the effect of the different number of subsamples from the signal by increasing the window size and sample rate on ten different feature map representations, including 1D, 2D, and 3D maps, and we then compare their results with our hybrid feature extraction method. With respect to the primary research question, it is found that we can take maximum advantage of the powerful Mel spectrogram feature through harmonic and percussive components in emotion recognition.\n\nAs shown in Table  2 , the proposed hybrid feature map representation achieves better results than other well-known feature combinations techniques. Furthermore, the results in Table  2 . indicate that the accuracy increases in the high range of the sample rate and window size in most represented methods since the feature map generator function handles more data points via a higher overlapping between frames. Consequently, for most feature extraction methods, the VGG16 network can learn from better-enriched features when the sample rate is higher. In contrast, an increased number of data points in the subsamples requires a memory capacity in the gigabyte range to store the base, train, validation, test feature map files in the pkl format. For instance, in our model, a signal sampling rate of 88KHz and a window size of 2048 occupy an approximately 3-gigabyte memory space to store the pkl files for analysing the whole voice files in the EMO-DB; this requirement can limit its application.\n\nThe fluctuation in the prediction accuracy per emotion class is illustrated for various feature representation methods in Fig.  5 . The boxplot graph shows that the model output is more reliable and stable when predicting seven emotions using our proposed hybrid feature extraction \"2D-log-MSS+Avg.HP\" and two more feature representations built by combining the delta of the Mel spectrogram (MSS) and log Mel spectrogram or MFCCs features.\n\nThe model's confusion matrix in Table  3 . shows that the network performs better when recognising specific emotions (anger, sadness, happiness, and fear)",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Model Comparison With Previous Works On Emo-Db",
      "text": "As shown in Table  4 , our method achieves superior results compared with most previous studies except for two works in terms of accuracy that are not significantly higher than our results. However, their work frame is more sophisticated than our proposed model. Zhao et al.  [33]  combined two 1-D and 2-D LSTM CNN networks in the feature learning process. Demirican et al.  [8]  used a model with three classifiers KNN, SVM and ANN to improve the prediction accuracy. Nevertheless, the major advantage of our architecture comes from its simplicity",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "The key research question in this study focuses on leveraging Mel spectrogram components in a hybrid-based feature engineering technique as well as proposing a novel acoustic feature extraction method to improve emotion recognition. The proposed feature map generator function extracts the harmonic and percussive components by applying a median filter on the horizontal (time-domain) and vertical (frequency-domain) directions of the spectrum, and is implemented with a four-layer MLP classifier to predict emotions in the human voice. The performance of the proposed hybrid feature technique is tested on the Berlin EMO-DB and compared with other 1D, 2D, and 3D feature extraction methods.\n\nTo the best of our knowledge, this is the first study on speech emotion recognition that combines this specific component of the spectrogram. The results show that our work significantly outperforms most previous works due to its achievement of a 92.79% test accuracy which is also a superior result in VGG16 feature learning methods. In future investigations, facial expression analysis and linguistic features can be embedded into the framework to improve the emotion recognition as an acoustic-only method is not constant across different languages and cultures.",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: shows the Mel spectrogram of sample voices exhibiting",
      "page": 4
    },
    {
      "caption": "Figure 1: The above sample Mel spectrograms clearly illustrate the distinction be-",
      "page": 5
    },
    {
      "caption": "Figure 2: shows the harmonic and percussive components as",
      "page": 5
    },
    {
      "caption": "Figure 3: , which clearly",
      "page": 5
    },
    {
      "caption": "Figure 2: The harmonic and percussive components of the Mel spectrograms for a",
      "page": 6
    },
    {
      "caption": "Figure 3: Visualising an achieved 2D hybrid feature maps from the Berlin EMO-DB",
      "page": 6
    },
    {
      "caption": "Figure 4: ; the architecture",
      "page": 6
    },
    {
      "caption": "Figure 4: Model architecture, which includes a 2D hybrid feature map built using",
      "page": 7
    },
    {
      "caption": "Figure 5: The boxplot graph shows that",
      "page": 8
    },
    {
      "caption": "Figure 5: Variation in the prediction accuracy per emotion class for different feature",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "{david.hasonrudd@student,\nhuan.huo,\nguandong.xu}@uts.edu.au": "∗corresponding author"
        },
        {
          "{david.hasonrudd@student,\nhuan.huo,\nguandong.xu}@uts.edu.au": "Abstract. Speech Emotion Recognition (SER) affective technology en-"
        },
        {
          "{david.hasonrudd@student,\nhuan.huo,\nguandong.xu}@uts.edu.au": "ables the intelligent embedded devices to interact with sensitivity. Sim-"
        },
        {
          "{david.hasonrudd@student,\nhuan.huo,\nguandong.xu}@uts.edu.au": "ilarly,\ncall\ncentre\nemployees\nrecognise\ncustomers’\nemotions\nfrom their"
        },
        {
          "{david.hasonrudd@student,\nhuan.huo,\nguandong.xu}@uts.edu.au": "pitch, energy, and tone of voice so as to modify their speech for a high-"
        },
        {
          "{david.hasonrudd@student,\nhuan.huo,\nguandong.xu}@uts.edu.au": "quality interaction with customers. This work explores, for the first time,"
        },
        {
          "{david.hasonrudd@student,\nhuan.huo,\nguandong.xu}@uts.edu.au": "the effects of\nthe harmonic and percussive components of Mel spectro-"
        },
        {
          "{david.hasonrudd@student,\nhuan.huo,\nguandong.xu}@uts.edu.au": "grams in SER. We attempt to leverage the Mel spectrogram by decom-"
        },
        {
          "{david.hasonrudd@student,\nhuan.huo,\nguandong.xu}@uts.edu.au": "posing distinguishable acoustic features for exploitation in our proposed"
        },
        {
          "{david.hasonrudd@student,\nhuan.huo,\nguandong.xu}@uts.edu.au": "architecture, which includes a novel\nfeature map generator algorithm,"
        },
        {
          "{david.hasonrudd@student,\nhuan.huo,\nguandong.xu}@uts.edu.au": "a CNN-based network feature\nextractor and a multi-layer perceptron"
        },
        {
          "{david.hasonrudd@student,\nhuan.huo,\nguandong.xu}@uts.edu.au": "(MLP) classifier. This\nstudy specifically focuses on effective data aug-"
        },
        {
          "{david.hasonrudd@student,\nhuan.huo,\nguandong.xu}@uts.edu.au": "mentation techniques for building an enriched hybrid-based feature map."
        },
        {
          "{david.hasonrudd@student,\nhuan.huo,\nguandong.xu}@uts.edu.au": "This process results in a function that outputs a 2D image so that it can"
        },
        {
          "{david.hasonrudd@student,\nhuan.huo,\nguandong.xu}@uts.edu.au": "be used as input data for a pre-trained CNN-VGG16 feature extractor."
        },
        {
          "{david.hasonrudd@student,\nhuan.huo,\nguandong.xu}@uts.edu.au": "Furthermore, we also investigate other acoustic features such as MFCCs,"
        },
        {
          "{david.hasonrudd@student,\nhuan.huo,\nguandong.xu}@uts.edu.au": "chromagram, spectral contrast, and the tonnetz to assess our proposed"
        },
        {
          "{david.hasonrudd@student,\nhuan.huo,\nguandong.xu}@uts.edu.au": "framework. A test accuracy of 92.79% on the Berlin EMO-DB database"
        },
        {
          "{david.hasonrudd@student,\nhuan.huo,\nguandong.xu}@uts.edu.au": "is achieved. Our result is higher than previous works using CNN-VGG16."
        },
        {
          "{david.hasonrudd@student,\nhuan.huo,\nguandong.xu}@uts.edu.au": "Keywords: Speech Emotion Recognition (SER)\n· Mel\nspectrogram ·"
        },
        {
          "{david.hasonrudd@student,\nhuan.huo,\nguandong.xu}@uts.edu.au": "Convolutional Neural Network (CNN) · voice signal processing · acoustic"
        },
        {
          "{david.hasonrudd@student,\nhuan.huo,\nguandong.xu}@uts.edu.au": "features"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1": "The general motivation of SER systems",
          "Introduction": ""
        },
        {
          "1": "speaker’s voice in different emotional situations to provide a more personal and",
          "Introduction": ""
        },
        {
          "1": "often superior user experience [6]. For example, a Customer Relationship Man-",
          "Introduction": ""
        },
        {
          "1": "agement",
          "Introduction": "(CRM)"
        },
        {
          "1": "their voice during a call. Emotions are universal, although their understandings,",
          "Introduction": ""
        },
        {
          "1": "interpretations and reflections are particular and partially associated with cul-",
          "Introduction": ""
        },
        {
          "1": "ture [1]. Unlike speech recognition, there is no standard or integrated approach",
          "Introduction": ""
        },
        {
          "1": "for recognising emotions and analysing them through human voices [30].",
          "Introduction": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2\nD. Hason Rudd et al.": "The fundamental challenge of SER is\nthe extraction of discriminative and"
        },
        {
          "2\nD. Hason Rudd et al.": "robust\nfeatures\nfrom speech signals. Features used for SER are generally cat-"
        },
        {
          "2\nD. Hason Rudd et al.": "egorized as prosodic, acoustic,\nand linguistic\nfeatures. The prosodic\nfeatures"
        },
        {
          "2\nD. Hason Rudd et al.": "include pitch,\nenergy, and zero-crossings of\nthe\nspeech signal\n[16,19,28]. The"
        },
        {
          "2\nD. Hason Rudd et al.": "acoustic features describe speech wave properties including linear predictor co-"
        },
        {
          "2\nD. Hason Rudd et al.": "efficients (LPC), mel-scaled power spectrograms (Mel),\nlinear predictor cepstral"
        },
        {
          "2\nD. Hason Rudd et al.": "coefficients (LPCC), power spectral analysis (FFT), power spectrogram chroma"
        },
        {
          "2\nD. Hason Rudd et al.": "(Chroma), and mel-frequency cepstral coefficients (MFCC) [5]. In SER, the Mel"
        },
        {
          "2\nD. Hason Rudd et al.": "spectrogram, MFCC, and chromagram are the most effective in decoding emo-"
        },
        {
          "2\nD. Hason Rudd et al.": "tion from a signal\n[22]."
        },
        {
          "2\nD. Hason Rudd et al.": "Among the most common speech feature extraction techniques,\nthis paper"
        },
        {
          "2\nD. Hason Rudd et al.": "addresses a principal question in Emotion Recognition (ER): How can we max-"
        },
        {
          "2\nD. Hason Rudd et al.": "imise the advantage of the Mel spectrogram feature to improve SER? This study"
        },
        {
          "2\nD. Hason Rudd et al.": "presents a novel implementation of emotion detection from speech signals by pro-"
        },
        {
          "2\nD. Hason Rudd et al.": "cessing harmonic and percussive components of Mel spectrograms and combining"
        },
        {
          "2\nD. Hason Rudd et al.": "the result with the log Mel spectrogram feature. Our primary contribution is the"
        },
        {
          "2\nD. Hason Rudd et al.": "introduction of an effective hybrid acoustic feature map technique that improves"
        },
        {
          "2\nD. Hason Rudd et al.": "SER. First, we employ CNN-VGG16 as a feature extractor of emotion identifier,"
        },
        {
          "2\nD. Hason Rudd et al.": "then utilise the MLP networks for classification task. Furthermore, we tune the"
        },
        {
          "2\nD. Hason Rudd et al.": "MLP network parameters using the random search model hyperparameter tech-"
        },
        {
          "2\nD. Hason Rudd et al.": "nique to obtain the best model. Based on empirical experiments, we assert that"
        },
        {
          "2\nD. Hason Rudd et al.": "a data augmentation strategy using an efficient prosodic and acoustic feature"
        },
        {
          "2\nD. Hason Rudd et al.": "combination analysis is the key to obtaining state-of-the-art results since input"
        },
        {
          "2\nD. Hason Rudd et al.": "data represents more diversity with enriched features; these characteristics lead"
        },
        {
          "2\nD. Hason Rudd et al.": "to better model generalisation."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Title Suppressed Due to Excessive Length\n3": "based spectrograms as the input images for pre-trained image classifiers produced"
        },
        {
          "Title Suppressed Due to Excessive Length\n3": "competitive results when compared with other well-known traditional methods."
        },
        {
          "Title Suppressed Due to Excessive Length\n3": "Badshah et al.\n[2] extracted spectrogram speech features, which were then visu-"
        },
        {
          "Title Suppressed Due to Excessive Length\n3": "alised in 2D images and passed to a CNN;\nthis approach achieved a 52% test"
        },
        {
          "Title Suppressed Due to Excessive Length\n3": "accuracy on EMO-DB. Demircan and Kahramanli [8] developed several different"
        },
        {
          "Title Suppressed Due to Excessive Length\n3": "classifiers and obtained test accuracies 92.86%, 92.86%, and 90%, respectively on"
        },
        {
          "Title Suppressed Due to Excessive Length\n3": "SVM, KNN and ANN. Additionally, Wang et al.\n[29] worked on MFCCs feature"
        },
        {
          "Title Suppressed Due to Excessive Length\n3": "and proposed an acoustic\nfeature\ncalled the Fourier Parameter\n(FP)\n, which"
        },
        {
          "Title Suppressed Due to Excessive Length\n3": "obtained 73.3% average accuracy with an SVM classifier. Furthermore, many"
        },
        {
          "Title Suppressed Due to Excessive Length\n3": "similar\nstudies were conducted on different databases. Popova et al.\n[24] used"
        },
        {
          "Title Suppressed Due to Excessive Length\n3": "a fine-tuned DNN and CNN-VGG16 classifier\nto extract\nthe Mel\nspectrogram"
        },
        {
          "Title Suppressed Due to Excessive Length\n3": "features in the RAVDESS dataset\n[17] and obtained an accuracy of 71% [24]."
        },
        {
          "Title Suppressed Due to Excessive Length\n3": "Satt et al. [27] presented another multi-modal LSTM-CNN and proposed a novel"
        },
        {
          "Title Suppressed Due to Excessive Length\n3": "feature extraction method based on the paralingual data from spectrograms. The"
        },
        {
          "Title Suppressed Due to Excessive Length\n3": "authors obtained 68% accuracy on the IMOCAP [4] database."
        },
        {
          "Title Suppressed Due to Excessive Length\n3": "In recent years,\nsome works proposed the use of hybrid feature map tech-"
        },
        {
          "Title Suppressed Due to Excessive Length\n3": "niques as input data for CNN-based networks. Meng et al. [20] proposed a feature"
        },
        {
          "Title Suppressed Due to Excessive Length\n3": "extraction strategy for Log-Mel spectrograms that extracted a 3D voice feature"
        },
        {
          "Title Suppressed Due to Excessive Length\n3": "representation map by combining log Mel spectrograms with the first and second"
        },
        {
          "Title Suppressed Due to Excessive Length\n3": "derivatives of the log MelSpec of the raw speech signal. The authors proposed"
        },
        {
          "Title Suppressed Due to Excessive Length\n3": "a CNN with a multimodal dilated architecture that used a residual block and"
        },
        {
          "Title Suppressed Due to Excessive Length\n3": "BiLSTM (ADRNN) to improve the classifier accuracy. In addition, the ADRNN"
        },
        {
          "Title Suppressed Due to Excessive Length\n3": "further enhanced the extraction of speech features using the proposed attention"
        },
        {
          "Title Suppressed Due to Excessive Length\n3": "mechanism approach. The model achieved a remarkable performance of 74.96%"
        },
        {
          "Title Suppressed Due to Excessive Length\n3": "and the 90.78% accuracy of\nthe IEMOCAP and EMO-DB databases. On the"
        },
        {
          "Title Suppressed Due to Excessive Length\n3": "other hand, Hajarolasvadi et al.\nintroduced a 3D feature frame technique for use"
        },
        {
          "Title Suppressed Due to Excessive Length\n3": "as\ninput data to the network by extracting an 88-dimensional vector of voice"
        },
        {
          "Title Suppressed Due to Excessive Length\n3": "features\nincluding MFCCs,\nintensity, and pitch. The model can reduce speech"
        },
        {
          "Title Suppressed Due to Excessive Length\n3": "signal\nfeature frames by applying k-means clustering on the extracted features"
        },
        {
          "Title Suppressed Due to Excessive Length\n3": "and selecting the k most discriminant\nframes as keyframes. Then,\nthe feature"
        },
        {
          "Title Suppressed Due to Excessive Length\n3": "data placed in the keyframe sequence were encapsulated in a 3D tensor, which"
        },
        {
          "Title Suppressed Due to Excessive Length\n3": "produced a final extracted feature map for use as input data for a 3D-CNN-based"
        },
        {
          "Title Suppressed Due to Excessive Length\n3": "classifier that used the 10-fold cross-validation method. The authors achieved a"
        },
        {
          "Title Suppressed Due to Excessive Length\n3": "weighted accuracy of 72.21% on EMO-DB. Zhao et al.\n[33] proposed a multi-"
        },
        {
          "Title Suppressed Due to Excessive Length\n3": "modal 2D CNN-LSTM network and extracted the log of the Mel-spectrograms"
        },
        {
          "Title Suppressed Due to Excessive Length\n3": "from the speech signals for use as input data. The outcome of their work is state-"
        },
        {
          "Title Suppressed Due to Excessive Length\n3": "of-the-art with the accuracy of 95.89% for speaker-independent classification on"
        },
        {
          "Title Suppressed Due to Excessive Length\n3": "the Berlin EMO-DB."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4\nD. Hason Rudd et al.": "feature. The proposed hybrid feature map method can be generalised with other"
        },
        {
          "4\nD. Hason Rudd et al.": "supervised classifiers to obtain better prediction accuracy."
        },
        {
          "4\nD. Hason Rudd et al.": "3.1\nProposed hybrid features: Harmonic and Percussive"
        },
        {
          "4\nD. Hason Rudd et al.": "components of Mel spectrogram"
        },
        {
          "4\nD. Hason Rudd et al.": "Essential\nfeatures in speech signal processing are the spectrograms on the Mel"
        },
        {
          "4\nD. Hason Rudd et al.": "scale, chromograms\n[12],\nspectral contrast,\nthe tonnetz [12] and MFCCs\n[32]."
        },
        {
          "4\nD. Hason Rudd et al.": "Since\nthe average\nlength of\nthe\nrecorded voice\nsamples are\nfour\nseconds, we"
        },
        {
          "4\nD. Hason Rudd et al.": "digitise each original utterance signal at an 88KHz sample rate using the Hanning"
        },
        {
          "4\nD. Hason Rudd et al.": "window function [11] shown in (1) to provide sufficient frequency resolution and"
        },
        {
          "4\nD. Hason Rudd et al.": "spectral\nleakage protection. Next, we apply Mel filter banks to the spectrogram"
        },
        {
          "4\nD. Hason Rudd et al.": "by shifting 0.4ms\nin a window time of 23ms\nso that\nthe output\nis a group of"
        },
        {
          "4\nD. Hason Rudd et al.": "FFTs located next to one another. The Hanning window is described in (1),"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Title Suppressed Due to Excessive Length\n5": "Fig. 1: The above sample Mel spectrograms clearly illustrate the distinction be-"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "tween amplitude and frequency in each emotion. The red colours represent fre-"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "quencies that contribute more than orange and white colours."
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "where S denotes a spectrum of signal s in kth Fourier coefficient on the mth"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "time frame, ω :\n[0 : N − 1]\n:= {0, 1, ..., N − 1} is a sine windowing function that"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "represents\nthe window length N , H represents\nthe hop size value, n indicates"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "current frame number and N is the length of the discrete Fourier transform. We"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "can obtain the harmonic and percussive components of the spectrum by applying"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "a median filter in the horizontal (time-domain) and vertical (frequency-domain)"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "direction on spectrum S. Finally, we extract the first feature map by obtaining"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "the mean of both components as shown in the following summarised formulas in"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "(5), (6) and (7),"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "(5)\nH = (cid:98)S (cid:79) M H"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "and"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "(6)\nP = (cid:98)S (cid:79) M P"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "obtained by"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "H + (cid:98)P"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "(7)\nF2(LMS) ="
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "2"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "where (cid:78) denotes the multiplication element of the median filter in M H , which"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "is the horizontal direction filtering used to obtain the\nH harmonic components"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "of the original spectrogram (cid:98)S. Subsequently, M P represents the vertical median"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "filtering results M P , which is the percussive component of the original spectro-"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "gram, (cid:98)S shown in (4). Fig. 2 shows the harmonic and percussive components as"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "two distinctive spectrograms in the 128 Mel filterbank."
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "The second feature map is extracted by applying the log of the Mel spectro-"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "gram obtained in (2) to measure the sensitivity of the Mel spectrogram output"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "value fluctuation concerning changes in the voice signal amplitude. A sample 2D"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "hybrid feature representation in our work is visualised in Fig. 3, which clearly"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "shows\nthat each sample feature map is combined in a two-dimensional\nimage."
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "This specific feature combination improves the prediction accuracy in a simple"
        },
        {
          "Title Suppressed Due to Excessive Length\n5": "full contact neural network classifier based on our empirical experiments."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6\nD. Hason Rudd et al.": "Fig. 2: The harmonic and percussive components of the Mel spectrograms for a"
        },
        {
          "6\nD. Hason Rudd et al.": "sample neutral emotion"
        },
        {
          "6\nD. Hason Rudd et al.": "Fig. 3: Visualising an achieved 2D hybrid feature maps from the Berlin EMO-DB"
        },
        {
          "6\nD. Hason Rudd et al.": "3.2\nModel architecture and training"
        },
        {
          "6\nD. Hason Rudd et al.": "We use the CNN-VGG16 [26] as a feature extractor to learn from high dimen-"
        },
        {
          "6\nD. Hason Rudd et al.": "sional feature maps since the network can learn from small variations that occur"
        },
        {
          "6\nD. Hason Rudd et al.": "in the extracted features maps. However, the high-capacity memory storage re-"
        },
        {
          "6\nD. Hason Rudd et al.": "quirements for a simple classification task can be considered a partial\nlimitation"
        },
        {
          "6\nD. Hason Rudd et al.": "of VGG16 applications."
        },
        {
          "6\nD. Hason Rudd et al.": "The details of the proposed architecture are shown in Fig. 4; the architecture"
        },
        {
          "6\nD. Hason Rudd et al.": "consists of an VGG16 and MLP network, which serve as an feature extractor and"
        },
        {
          "6\nD. Hason Rudd et al.": "emotion classifier, respectively. First, the subsamples are extracted from a fixed"
        },
        {
          "6\nD. Hason Rudd et al.": "window size and then feature maps are built using the proposed feature map"
        },
        {
          "6\nD. Hason Rudd et al.": "function. Therefore, the input to the VGG16 feature extractor is a 2-D feature"
        },
        {
          "6\nD. Hason Rudd et al.": "map in the dimension of (128 x 128 x 2). The input to the MLP classifier is a"
        },
        {
          "6\nD. Hason Rudd et al.": "2048 one-dimensional vector generated by VGG16. The MLP classifier includes"
        },
        {
          "6\nD. Hason Rudd et al.": "four\nfully connected layers with the ReLU activation function and softmax in"
        },
        {
          "6\nD. Hason Rudd et al.": "the output\nlayer. Dense 1 and 2 have a 1024 input with a 0.5 dropout value,"
        },
        {
          "6\nD. Hason Rudd et al.": "and dense 3 and 4 are set to 512 input with 0.3 dropouts. The ADAM optimiser"
        },
        {
          "6\nD. Hason Rudd et al.": "with a learning rate of 0.0001 is selected for our architecture design."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Title Suppressed Due to Excessive Length\n7": "Fig. 4: Model architecture, which includes a 2D hybrid feature map built using"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "the harmonic and percussive components, as well as the log of the Mel spectro-"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "gram in the feature map generator function. The features are extracted using a"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "CNN-VGG16 network. Finally, the MLP network classifies seven emotions."
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "4\nExperimental analysis"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "This section analyses the experimental configuration and the result of the feature"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "extractor and MLP classifier on EMO-DB [3]. The sample voices are randomly"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "partitioned and 80% are used for the training set and 10% for the validation and"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "test\nset\nfor\nthe speaker-independent classification task. We apply an oversam-"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "pling strategy to compensate the minority classes and increase the voice samples"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "before feeding them to the feature extractor network during the pre-processing"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "phase. The classifier is trained on 128 epochs with a batch size of 128 and used an"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "Nvidia GPU. The window size is set to 2048 with (128 x 128) bands and frames"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "to obtain each subsample length = 2.9 sec. Then,\nthe subsamples are created"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "in each defined data frame. Finally, 167426 signal subsamples and 9717 feature"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "maps are obtained from a sample rate of 88KHz. Based on the time-frequency"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "trade-off,\nlarge frame size is chosen to obtain high-frequency resolution rather"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "than time resolution since analysing the frequency of\nspeech signal enables us"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "to decode emotion. Several time-consuming experiments are conducted to assess"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "the effectiveness of the proposed hybrid feature, which aims to find the best data"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "augmentation through feature combination."
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "4.1\nResults Analysis"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "To\nassess\nour\nenriched feature\nrepresentation method in the MLP classifier,"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "the\nresult of\nevaluation metrics\nsuch as\nthe\nconfusion matrix and test accu-"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "racy are observed on different sample rates and feature map dimensions (bands"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 1: The emotion classifier accuracy based on different feature map repre-",
      "data": [
        {
          "8": "and frames). We also evaluate our model output based on the setting of various",
          "D. Hason Rudd et al.": ""
        },
        {
          "8": "parameters",
          "D. Hason Rudd et al.": ""
        },
        {
          "8": "results based on some different parameters setting are shown in Table 1. These",
          "D. Hason Rudd et al.": ""
        },
        {
          "8": "results indicate that the superior result is achieved on feature map dimensions",
          "D. Hason Rudd et al.": ""
        },
        {
          "8": "of 128 x 128 with a sample rate of 88200, Since the highest subsample length of",
          "D. Hason Rudd et al.": ""
        },
        {
          "8": "2.9seconds is achieved and more sample points can contribute in each subsample.",
          "D. Hason Rudd et al.": ""
        },
        {
          "8": "Table 1: The emotion classifier accuracy based on different feature map repre-",
          "D. Hason Rudd et al.": ""
        },
        {
          "8": "sentation dimensions and signal sampling ratios",
          "D. Hason Rudd et al.": ""
        },
        {
          "8": "",
          "D. Hason Rudd et al.": "Band"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 1: The emotion classifier accuracy based on different feature map repre-",
      "data": [
        {
          "Table 1: The emotion classifier accuracy based on different feature map repre-": "sentation dimensions and signal sampling ratios"
        },
        {
          "Table 1: The emotion classifier accuracy based on different feature map repre-": "Band"
        },
        {
          "Table 1: The emotion classifier accuracy based on different feature map repre-": "32"
        },
        {
          "Table 1: The emotion classifier accuracy based on different feature map repre-": "128"
        },
        {
          "Table 1: The emotion classifier accuracy based on different feature map repre-": "128"
        },
        {
          "Table 1: The emotion classifier accuracy based on different feature map repre-": "32"
        },
        {
          "Table 1: The emotion classifier accuracy based on different feature map repre-": "64"
        },
        {
          "Table 1: The emotion classifier accuracy based on different feature map repre-": "128"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 2: Evaluation of the prediction accuracy based on the different feature",
      "data": [
        {
          "Table 2: Evaluation of": "extraction methods, sample rate and window size",
          "the prediction accuracy based on the different": ""
        },
        {
          "Table 2: Evaluation of": "Window Size",
          "the prediction accuracy based on the different": "512"
        },
        {
          "Table 2: Evaluation of": "Sample Rate",
          "the prediction accuracy based on the different": "22050"
        },
        {
          "Table 2: Evaluation of": "Feature extraction methods",
          "the prediction accuracy based on the different": "Acc %"
        },
        {
          "Table 2: Evaluation of": "1D-MFCCs",
          "the prediction accuracy based on the different": "65.81"
        },
        {
          "Table 2: Evaluation of": "1D-Mel spectrogram",
          "the prediction accuracy based on the different": "75.48"
        },
        {
          "Table 2: Evaluation of": "1D-Chromagram",
          "the prediction accuracy based on the different": "80.01"
        },
        {
          "Table 2: Evaluation of": "1D-Tonnets",
          "the prediction accuracy based on the different": "56.77"
        },
        {
          "Table 2: Evaluation of": "1D-Spectral",
          "the prediction accuracy based on the different": "54.84"
        },
        {
          "Table 2: Evaluation of": "2D-MFCCs+Chromagram",
          "the prediction accuracy based on the different": "83.87"
        },
        {
          "Table 2: Evaluation of": "2D-Mel spectrogram+MFCCs",
          "the prediction accuracy based on the different": "88.39"
        },
        {
          "Table 2: Evaluation of": "2D-Mel spectrogram+Spectral",
          "the prediction accuracy based on the different": "82.01"
        },
        {
          "Table 2: Evaluation of": "3D-Mel spectrogram+MFCCs+Chromagram",
          "the prediction accuracy based on the different": "83.87"
        },
        {
          "Table 2: Evaluation of": "2D-log-MSS+Avg.HP(proposed)",
          "the prediction accuracy based on the different": "92.02"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 2: Another advantage of the architecture is the capability of storing the",
      "data": [
        {
          "Table 3: Confusion matrix (%) of the model with an average accuracy of 92.71%": "on the EMO-DB dataset"
        },
        {
          "Table 3: Confusion matrix (%) of the model with an average accuracy of 92.71%": "Emotion:"
        },
        {
          "Table 3: Confusion matrix (%) of the model with an average accuracy of 92.71%": "Anger"
        },
        {
          "Table 3: Confusion matrix (%) of the model with an average accuracy of 92.71%": "Boredom"
        },
        {
          "Table 3: Confusion matrix (%) of the model with an average accuracy of 92.71%": "Disgust"
        },
        {
          "Table 3: Confusion matrix (%) of the model with an average accuracy of 92.71%": "Fear"
        },
        {
          "Table 3: Confusion matrix (%) of the model with an average accuracy of 92.71%": "Happiness"
        },
        {
          "Table 3: Confusion matrix (%) of the model with an average accuracy of 92.71%": "Neutral"
        },
        {
          "Table 3: Confusion matrix (%) of the model with an average accuracy of 92.71%": "Sadness"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 2: Another advantage of the architecture is the capability of storing the",
      "data": [
        {
          "Table 4: Model comparison with previous works on EMO-DB": "Learner"
        },
        {
          "Table 4: Model comparison with previous works on EMO-DB": "CNN"
        },
        {
          "Table 4: Model comparison with previous works on EMO-DB": "VGG16"
        },
        {
          "Table 4: Model comparison with previous works on EMO-DB": "[10] CNN"
        },
        {
          "Table 4: Model comparison with previous works on EMO-DB": "SVM"
        },
        {
          "Table 4: Model comparison with previous works on EMO-DB": "CNN"
        },
        {
          "Table 4: Model comparison with previous works on EMO-DB": "VGG16"
        },
        {
          "Table 4: Model comparison with previous works on EMO-DB": "CNN-LSTM"
        },
        {
          "Table 4: Model comparison with previous works on EMO-DB": "SVM"
        },
        {
          "Table 4: Model comparison with previous works on EMO-DB": ""
        },
        {
          "Table 4: Model comparison with previous works on EMO-DB": "[8] SVM"
        },
        {
          "Table 4: Model comparison with previous works on EMO-DB": "CNN-LSTM"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Title Suppressed Due to Excessive Length": "",
          "11": ""
        },
        {
          "Title Suppressed Due to Excessive Length": "",
          "11": ""
        },
        {
          "Title Suppressed Due to Excessive Length": "",
          "11": ""
        },
        {
          "Title Suppressed Due to Excessive Length": "",
          "11": ""
        },
        {
          "Title Suppressed Due to Excessive Length": "",
          "11": ""
        },
        {
          "Title Suppressed Due to Excessive Length": "",
          "11": ""
        },
        {
          "Title Suppressed Due to Excessive Length": "",
          "11": ""
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "number: DP22010371, LE220100078, DP200101374 and LP170100891": "References"
        },
        {
          "number: DP22010371, LE220100078, DP200101374 and LP170100891": "1. Alu, D., Zoltan, E., Stoica, I.C.: Voice based emotion recognition with convolutional"
        },
        {
          "number: DP22010371, LE220100078, DP200101374 and LP170100891": "neural networks for companion robots. Science and Technology 20, 222–240"
        },
        {
          "number: DP22010371, LE220100078, DP200101374 and LP170100891": "2. Badshah, A.M., Ahmad, J., Rahim, N., Baik, S.W.: Speech emotion recognition"
        },
        {
          "number: DP22010371, LE220100078, DP200101374 and LP170100891": "from spectrograms with deep convolutional neural network. In: 2017 international"
        },
        {
          "number: DP22010371, LE220100078, DP200101374 and LP170100891": "conference on platform technology and service (PlatCon). pp. 1–5 (2017)"
        },
        {
          "number: DP22010371, LE220100078, DP200101374 and LP170100891": "3. Burkhardt, F., Paeschke, A., Rolfes, M., Sendlmeier, W.F., Weiss, B., et al.: A"
        },
        {
          "number: DP22010371, LE220100078, DP200101374 and LP170100891": "database of german emotional speech. In: Interspeech. vol. 5, pp. 1517–1520 (2005)"
        },
        {
          "number: DP22010371, LE220100078, DP200101374 and LP170100891": "4. Busso, C., Bulut, M., Lee, C.C., Kazemzadeh, A., Mower, E., Kim, S., Chang, J.N.,"
        },
        {
          "number: DP22010371, LE220100078, DP200101374 and LP170100891": "Lee, S., Narayanan, S.S.:\nIemocap:\nInteractive emotional dyadic motion capture"
        },
        {
          "number: DP22010371, LE220100078, DP200101374 and LP170100891": "database. Language resources and evaluation 42(4), 335–359 (2008)"
        },
        {
          "number: DP22010371, LE220100078, DP200101374 and LP170100891": "5. Chu, S., Narayanan, S., Kuo, C.C.J.: Environmental sound recognition with time–"
        },
        {
          "number: DP22010371, LE220100078, DP200101374 and LP170100891": "frequency audio features.\nIEEE Transactions on Audio, Speech, and Language"
        },
        {
          "number: DP22010371, LE220100078, DP200101374 and LP170100891": "Processing 17(6), 1142–1158 (2009)"
        },
        {
          "number: DP22010371, LE220100078, DP200101374 and LP170100891": "6. Cowie, R., Douglas-Cowie, E., Tsapatsoulis, N., Votsis, G., Kollias, S., Fellenz, W.,"
        },
        {
          "number: DP22010371, LE220100078, DP200101374 and LP170100891": "Taylor, J.G.: Emotion recognition in human-computer\ninteraction.\nIEEE Signal"
        },
        {
          "number: DP22010371, LE220100078, DP200101374 and LP170100891": "processing magazine 18(1), 32–80 (2001)"
        },
        {
          "number: DP22010371, LE220100078, DP200101374 and LP170100891": "7. Cummins, N., Amiriparian, S., Hagerer, G., Batliner, A., Steidl, S., Schuller, B.W.:"
        },
        {
          "number: DP22010371, LE220100078, DP200101374 and LP170100891": "An image-based deep spectrum feature representation for the recognition of emo-"
        },
        {
          "number: DP22010371, LE220100078, DP200101374 and LP170100891": "tional speech. In: Proceedings of the 25th ACM international conference on Mul-"
        },
        {
          "number: DP22010371, LE220100078, DP200101374 and LP170100891": "timedia. pp. 478–484 (2017)"
        },
        {
          "number: DP22010371, LE220100078, DP200101374 and LP170100891": "8. Demircan, S., Kahramanli, H.: Application of\nfuzzy c-means clustering algorithm"
        },
        {
          "number: DP22010371, LE220100078, DP200101374 and LP170100891": "to spectral\nfeatures for emotion classification from speech. Neural Computing and"
        },
        {
          "number: DP22010371, LE220100078, DP200101374 and LP170100891": "Applications 29(8), 59–66 (2018)"
        },
        {
          "number: DP22010371, LE220100078, DP200101374 and LP170100891": "9. Fitzgerald, D.: Harmonic/percussive\nseparation using median filtering.\nIn: Pro-"
        },
        {
          "number: DP22010371, LE220100078, DP200101374 and LP170100891": "ceedings of the International Conference on Digital Audio Effects (DAFx). vol. 13,"
        },
        {
          "number: DP22010371, LE220100078, DP200101374 and LP170100891": "pp. 1–4 (2010)"
        },
        {
          "number: DP22010371, LE220100078, DP200101374 and LP170100891": "10. Hajarolasvadi, N., Demirel, H.: 3d cnn-based speech emotion recognition using"
        },
        {
          "number: DP22010371, LE220100078, DP200101374 and LP170100891": "k-means clustering and spectrograms. Entropy 21(5), 479–495 (2019)"
        },
        {
          "number: DP22010371, LE220100078, DP200101374 and LP170100891": "11. Harris, F.J.: On the use of windows for harmonic analysis with the discrete fourier"
        },
        {
          "number: DP22010371, LE220100078, DP200101374 and LP170100891": "transform. Proceedings of the IEEE 66(1), 51–83 (1978)"
        },
        {
          "number: DP22010371, LE220100078, DP200101374 and LP170100891": "12. Harte, C., Sandler, M., Gasser, M.: Detecting harmonic change in musical audio. In:"
        },
        {
          "number: DP22010371, LE220100078, DP200101374 and LP170100891": "Proceedings of the 1st ACM workshop on Audio and music computing multimedia."
        },
        {
          "number: DP22010371, LE220100078, DP200101374 and LP170100891": "pp. 21–26 (2006)"
        },
        {
          "number: DP22010371, LE220100078, DP200101374 and LP170100891": "13. Huang, Z., Dong, M., Mao, Q., Zhan, Y.: Speech emotion recognition using cnn. In:"
        },
        {
          "number: DP22010371, LE220100078, DP200101374 and LP170100891": "Proceedings of the 22nd ACM international conference media. pp. 801–804 (2014)"
        },
        {
          "number: DP22010371, LE220100078, DP200101374 and LP170100891": "14.\nIssa, D., Demirci, M.F., Yazici, A.: Speech emotion recognition with deep convo-"
        },
        {
          "number: DP22010371, LE220100078, DP200101374 and LP170100891": "lutional neural networks. Biomedical Signal Processing and Control 59, 101894–"
        },
        {
          "number: DP22010371, LE220100078, DP200101374 and LP170100891": "101904 (2020)"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "12\nD. Hason Rudd et al.": "15.\nJin, Q., Li, C., Chen, S., Wu, H.: Speech emotion recognition with acoustic and"
        },
        {
          "12\nD. Hason Rudd et al.": "lexical\nfeatures.\nIn: 2015 IEEE international conference on acoustics, speech and"
        },
        {
          "12\nD. Hason Rudd et al.": "signal processing (ICASSP). pp. 4749–4753 (2015)"
        },
        {
          "12\nD. Hason Rudd et al.": "16. Li, M., Han, K.J., Narayanan, S.: Automatic speaker age and gender recognition"
        },
        {
          "12\nD. Hason Rudd et al.": "using acoustic and prosodic level information fusion. Computer Speech & Language"
        },
        {
          "12\nD. Hason Rudd et al.": "27(1), 151–167 (2013)"
        },
        {
          "12\nD. Hason Rudd et al.": "17. Livingstone, S.R., Russo, F.A.: The\nryerson audio-visual database of\nemotional"
        },
        {
          "12\nD. Hason Rudd et al.": "speech and song (ravdess): A dynamic, multimodal set of\nfacial and vocal expres-"
        },
        {
          "12\nD. Hason Rudd et al.": "sions in north american english. PloS one 13(5), 1–35 (2018)"
        },
        {
          "12\nD. Hason Rudd et al.": "18. McFee, B., Raffel, C., Liang, D., Ellis, D.P., McVicar, M., Battenberg, E., Nieto,"
        },
        {
          "12\nD. Hason Rudd et al.": "O.:\nlibrosa: Audio and music signal analysis in python. In: Proceedings of the 14th"
        },
        {
          "12\nD. Hason Rudd et al.": "python in science conference. vol. 8, pp. 18–25 (2015)"
        },
        {
          "12\nD. Hason Rudd et al.": "19. Meinedo, H., Trancoso,\nI.: Age and gender classification using fusion of acoustic"
        },
        {
          "12\nD. Hason Rudd et al.": "and prosodic features.\nIn: Eleventh annual conference of the international speech"
        },
        {
          "12\nD. Hason Rudd et al.": "communication association. pp. 1–4 (2010)"
        },
        {
          "12\nD. Hason Rudd et al.": "20. Meng, H., Yan, T., Yuan, F., Wei, H.: Speech emotion recognition from 3d log-mel"
        },
        {
          "12\nD. Hason Rudd et al.": "spectrograms with deep learning network. IEEE access 7, 125868–125881 (2019)"
        },
        {
          "12\nD. Hason Rudd et al.": "21. Milton, A., Roy, S.S., Selvi, S.T.: Svm scheme for speech emotion recognition using"
        },
        {
          "12\nD. Hason Rudd et al.": "mfcc feature. International Journal of Computer Applications 69(9) (2013)"
        },
        {
          "12\nD. Hason Rudd et al.": "22. Motlıcek, P.: Feature extraction in speech coding and recognition. Tech. rep., Tech-"
        },
        {
          "12\nD. Hason Rudd et al.": "nical Report of PhD research internship in ASP Group (pp 1-50,2002)"
        },
        {
          "12\nD. Hason Rudd et al.": "23. P´erez-Rosas, V., Mihalcea, R., Morency, L.P.: Utterance-level multimodal\nsenti-"
        },
        {
          "12\nD. Hason Rudd et al.": "ment analysis.\nIn: Proceedings of the 51st Annual Meeting of the Association for"
        },
        {
          "12\nD. Hason Rudd et al.": "Computational Linguistics (Volume 1: Long Papers). pp. 973–982 (2013)"
        },
        {
          "12\nD. Hason Rudd et al.": "24. Popova, A.S., Rassadin, A.G., Ponomarenko, A.A.: Emotion recognition in sound."
        },
        {
          "12\nD. Hason Rudd et al.": "In: International Conference on Neuroinformatics. pp. 117–124 (2017)"
        },
        {
          "12\nD. Hason Rudd et al.": "25. Rozgi´c, V., Ananthakrishnan, S., Saleem, S., Kumar, R., Prasad, R.: Ensemble of"
        },
        {
          "12\nD. Hason Rudd et al.": "svm trees for multimodal emotion recognition. In: Proceedings of the 2012 Asia Pa-"
        },
        {
          "12\nD. Hason Rudd et al.": "cific signal and information processing association annual summit and conference."
        },
        {
          "12\nD. Hason Rudd et al.": "pp. 1–4 (2012)"
        },
        {
          "12\nD. Hason Rudd et al.": "26. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,"
        },
        {
          "12\nD. Hason Rudd et al.": "Karpathy, A., Khosla, A., Bernstein, M., et al.: Imagenet large scale visual recog-"
        },
        {
          "12\nD. Hason Rudd et al.": "nition challenge. International\njournal of computer vision 115(3), 211–252 (2015)"
        },
        {
          "12\nD. Hason Rudd et al.": "27. Satt, A., Rozenberg, S., Hoory, R.: Efficient emotion recognition from speech using"
        },
        {
          "12\nD. Hason Rudd et al.": "deep learning on spectrograms. In: Interspeech. pp. 1089–1093 (2017)"
        },
        {
          "12\nD. Hason Rudd et al.": "28. Shriberg, E., Ferrer, L., Kajarekar, S., Venkataraman, A., Stolcke, A.: Modeling"
        },
        {
          "12\nD. Hason Rudd et al.": "prosodic feature sequences for speaker recognition. Speech communication 46(3-4),"
        },
        {
          "12\nD. Hason Rudd et al.": "455–472 (2005)"
        },
        {
          "12\nD. Hason Rudd et al.": "29. Wang, K., An, N., Li, B.N., Zhang, Y., Li, L.: Speech emotion recognition using"
        },
        {
          "12\nD. Hason Rudd et al.": "fourier parameters. IEEE Transactions on affective computing 6(1), 69–75 (2015)"
        },
        {
          "12\nD. Hason Rudd et al.": "30. Weninger, F., W¨ollmer, M., Schuller, B.: Emotion recognition in naturalistic speech"
        },
        {
          "12\nD. Hason Rudd et al.": "and language—a survey.\nIn book: Emotion Recognition: A Pattern Analysis Ap-"
        },
        {
          "12\nD. Hason Rudd et al.": "proach pp. 237–267 (2015)"
        },
        {
          "12\nD. Hason Rudd et al.": "31. Wu, S., Falk, T.H., Chan, W.Y.: Automatic speech emotion recognition using mod-"
        },
        {
          "12\nD. Hason Rudd et al.": "ulation spectral\nfeatures. Speech communication 53(5), 768–785 (2011)"
        },
        {
          "12\nD. Hason Rudd et al.": "32. Xu, M., Duan, L.Y., Cai, J., Chia, L.T., Xu, C., Tian, Q.: Hmm-based audio"
        },
        {
          "12\nD. Hason Rudd et al.": "keyword generation. In: Pacific-Rim Conference on Multimedia. pp. 566–574 (2004)"
        },
        {
          "12\nD. Hason Rudd et al.": "33. Zhao, J., Mao, X., Chen, L.: Speech emotion recognition using deep 1d & 2d cnn"
        },
        {
          "12\nD. Hason Rudd et al.": "lstm networks. Biomedical Signal Processing and Control 47, 312–323 (2019)"
        }
      ],
      "page": 12
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Voice based emotion recognition with convolutional neural networks for companion robots",
      "authors": [
        "D Alu",
        "E Zoltan",
        "I Stoica"
      ],
      "venue": "Science and Technology"
    },
    {
      "citation_id": "2",
      "title": "Speech emotion recognition from spectrograms with deep convolutional neural network",
      "authors": [
        "A Badshah",
        "J Ahmad",
        "N Rahim",
        "S Baik"
      ],
      "year": "2017",
      "venue": "2017 international conference on platform technology and service (PlatCon)"
    },
    {
      "citation_id": "3",
      "title": "A database of german emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "Interspeech"
    },
    {
      "citation_id": "4",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "5",
      "title": "Environmental sound recognition with timefrequency audio features",
      "authors": [
        "S Chu",
        "S Narayanan",
        "C Kuo"
      ],
      "year": "2009",
      "venue": "IEEE Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "6",
      "title": "Emotion recognition in human-computer interaction",
      "authors": [
        "R Cowie",
        "E Douglas-Cowie",
        "N Tsapatsoulis",
        "G Votsis",
        "S Kollias",
        "W Fellenz",
        "J Taylor"
      ],
      "year": "2001",
      "venue": "IEEE Signal processing magazine"
    },
    {
      "citation_id": "7",
      "title": "An image-based deep spectrum feature representation for the recognition of emotional speech",
      "authors": [
        "N Cummins",
        "S Amiriparian",
        "G Hagerer",
        "A Batliner",
        "S Steidl",
        "B Schuller"
      ],
      "year": "2017",
      "venue": "Proceedings of the 25th ACM international conference on Multimedia"
    },
    {
      "citation_id": "8",
      "title": "Application of fuzzy c-means clustering algorithm to spectral features for emotion classification from speech",
      "authors": [
        "S Demircan",
        "H Kahramanli"
      ],
      "year": "2018",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "9",
      "title": "Harmonic/percussive separation using median filtering",
      "authors": [
        "D Fitzgerald"
      ],
      "year": "2010",
      "venue": "Proceedings of the International Conference on Digital Audio Effects (DAFx)"
    },
    {
      "citation_id": "10",
      "title": "3d cnn-based speech emotion recognition using k-means clustering and spectrograms",
      "authors": [
        "N Hajarolasvadi",
        "H Demirel"
      ],
      "year": "2019",
      "venue": "Entropy"
    },
    {
      "citation_id": "11",
      "title": "On the use of windows for harmonic analysis with the discrete fourier transform",
      "authors": [
        "F Harris"
      ],
      "year": "1978",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "12",
      "title": "Detecting harmonic change in musical audio",
      "authors": [
        "C Harte",
        "M Sandler",
        "M Gasser"
      ],
      "year": "2006",
      "venue": "Proceedings of the 1st ACM workshop on Audio and music computing multimedia"
    },
    {
      "citation_id": "13",
      "title": "Speech emotion recognition using cnn",
      "authors": [
        "Z Huang",
        "M Dong",
        "Q Mao",
        "Y Zhan"
      ],
      "year": "2014",
      "venue": "Proceedings of the 22nd ACM international conference media"
    },
    {
      "citation_id": "14",
      "title": "Speech emotion recognition with deep convolutional neural networks",
      "authors": [
        "D Issa",
        "M Demirci",
        "A Yazici"
      ],
      "year": "2020",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "15",
      "title": "Speech emotion recognition with acoustic and lexical features",
      "authors": [
        "Q Jin",
        "C Li",
        "S Chen",
        "H Wu"
      ],
      "year": "2015",
      "venue": "IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "16",
      "title": "Automatic speaker age and gender recognition using acoustic and prosodic level information fusion",
      "authors": [
        "M Li",
        "K Han",
        "S Narayanan"
      ],
      "year": "2013",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "17",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "18",
      "title": "librosa: Audio and music signal analysis in python",
      "authors": [
        "B Mcfee",
        "C Raffel",
        "D Liang",
        "D Ellis",
        "M Mcvicar",
        "E Battenberg",
        "O Nieto"
      ],
      "year": "2015",
      "venue": "Proceedings of the 14th python in science conference"
    },
    {
      "citation_id": "19",
      "title": "Age and gender classification using fusion of acoustic and prosodic features",
      "authors": [
        "H Meinedo",
        "I Trancoso"
      ],
      "year": "2010",
      "venue": "Eleventh annual conference of the international speech communication association"
    },
    {
      "citation_id": "20",
      "title": "Speech emotion recognition from 3d log-mel spectrograms with deep learning network",
      "authors": [
        "H Meng",
        "T Yan",
        "F Yuan",
        "H Wei"
      ],
      "year": "2019",
      "venue": "IEEE access"
    },
    {
      "citation_id": "21",
      "title": "Svm scheme for speech emotion recognition using mfcc feature",
      "authors": [
        "A Milton",
        "S Roy",
        "S Selvi"
      ],
      "year": "2013",
      "venue": "International Journal of Computer Applications"
    },
    {
      "citation_id": "22",
      "title": "Feature extraction in speech coding and recognition",
      "authors": [
        "P Motlıcek"
      ],
      "year": "2002",
      "venue": "Technical Report of PhD research internship in ASP Group"
    },
    {
      "citation_id": "23",
      "title": "Utterance-level multimodal sentiment analysis",
      "authors": [
        "V Pérez-Rosas",
        "R Mihalcea",
        "L Morency"
      ],
      "year": "2013",
      "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "24",
      "title": "Emotion recognition in sound",
      "authors": [
        "A Popova",
        "A Rassadin",
        "A Ponomarenko"
      ],
      "year": "2017",
      "venue": "International Conference on Neuroinformatics"
    },
    {
      "citation_id": "25",
      "title": "Ensemble of svm trees for multimodal emotion recognition",
      "authors": [
        "V Rozgić",
        "S Ananthakrishnan",
        "S Saleem",
        "R Kumar",
        "R Prasad"
      ],
      "year": "2012",
      "venue": "Proceedings of the 2012 Asia Pacific signal and information processing association annual summit and conference"
    },
    {
      "citation_id": "26",
      "title": "Imagenet large scale visual recognition challenge",
      "authors": [
        "O Russakovsky",
        "J Deng",
        "H Su",
        "J Krause",
        "S Satheesh",
        "S Ma",
        "Z Huang",
        "A Karpathy",
        "A Khosla",
        "M Bernstein"
      ],
      "year": "2015",
      "venue": "International journal of computer vision"
    },
    {
      "citation_id": "27",
      "title": "Efficient emotion recognition from speech using deep learning on spectrograms",
      "authors": [
        "A Satt",
        "S Rozenberg",
        "R Hoory"
      ],
      "year": "2017",
      "venue": "Efficient emotion recognition from speech using deep learning on spectrograms"
    },
    {
      "citation_id": "28",
      "title": "Modeling prosodic feature sequences for speaker recognition",
      "authors": [
        "E Shriberg",
        "L Ferrer",
        "S Kajarekar",
        "A Venkataraman",
        "A Stolcke"
      ],
      "year": "2005",
      "venue": "Speech communication"
    },
    {
      "citation_id": "29",
      "title": "Speech emotion recognition using fourier parameters",
      "authors": [
        "K Wang",
        "N An",
        "B Li",
        "Y Zhang",
        "L Li"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on affective computing"
    },
    {
      "citation_id": "30",
      "title": "Emotion recognition in naturalistic speech and language-a survey",
      "authors": [
        "F Weninger",
        "M Wöllmer",
        "B Schuller"
      ],
      "year": "2015",
      "venue": "book: Emotion Recognition: A Pattern Analysis Approach"
    },
    {
      "citation_id": "31",
      "title": "Automatic speech emotion recognition using modulation spectral features",
      "authors": [
        "S Wu",
        "T Falk",
        "W Chan"
      ],
      "year": "2011",
      "venue": "Speech communication"
    },
    {
      "citation_id": "32",
      "title": "Hmm-based audio keyword generation",
      "authors": [
        "M Xu",
        "L Duan",
        "J Cai",
        "L Chia",
        "C Xu",
        "Q Tian"
      ],
      "year": "2004",
      "venue": "Pacific-Rim Conference on Multimedia"
    },
    {
      "citation_id": "33",
      "title": "Speech emotion recognition using deep 1d & 2d cnn lstm networks",
      "authors": [
        "J Zhao",
        "X Mao",
        "L Chen"
      ],
      "year": "2019",
      "venue": "Biomedical Signal Processing and Control"
    }
  ]
}