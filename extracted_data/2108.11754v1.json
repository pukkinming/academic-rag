{
  "paper_id": "2108.11754v1",
  "title": "Training And Profiling A Pediatric Emotion Recognition Classifier On Mobile Devices",
  "published": "2021-08-22T01:48:53Z",
  "authors": [
    "Agnik Banerjee",
    "Peter Washington",
    "Cezmi Mutlu",
    "Aaron Kline",
    "Dennis P. Wall"
  ],
  "keywords": [
    "edge computing",
    "autism spectrum disorder",
    "mobile health",
    "computer vision"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Implementing automated emotion recognition on mobile devices could provide an accessible diagnostic and therapeutic tool for those who struggle to recognize emotion, including children with developmental behavioral conditions such as autism. Although recent advances have been made in building more accurate emotion classifiers, existing models are too computationally expensive to be deployed on mobile devices. In this study, we optimized and profiled various machine learning models designed for inference on edge devices and were able to match previous state of the art results for emotion recognition on children. Our best model, a MobileNet-V2 network pre-trained on ImageNet, achieved 65.11% balanced accuracy and 64.19% F1-score on CAFE, while achieving a 45-millisecond inference latency on a Motorola Moto G6 phone. This balanced accuracy is only 1.79% less than the current state of the art for CAFE, which used a model that contains 26.62x more parameters and was unable to run on the Moto G6, even when fully optimized. This work validates that with specialized design and optimization techniques, machine learning models can become lightweight enough for deployment on mobile devices and still achieve high accuracies on difficult image classification tasks.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Autism Spectrum Disorder (ASD) affects 1 in 54 children and is the fastest growing developmental disability in the U.S., with its prevalence having increased by 178% since 2000  [1] . While research has shown that early detection and therapy is vital in order to treat ASD  [2] [3] , a lack of access to clinical practitioners, particularly among lower-income families  [4] [5] , results in 27% of children over the age of 8 remaining undiagnosed and too old to respond as effectively to treatment in the future  [6] .\n\nDue to lowering prices, digital technologies are becoming widely available in almost all socioeconomic levels  [7] , even in developing nations where ASD clinicians are far scarcer than the U.S.  [8] . Thus, it is conceivable that mobile apps on smartphones could be used as an alternative medium for autism diagnosis and treatment that is easily accessible and highly affordable.\n\nMachine learning models utilized in digital therapeutics have shown significant diagnostic  [9] [10] [11] [12] [13] [14] [15] [16] [17] [18] [19] [20] [21] [22] [23] [24] [25]  and treatment  [26] [27] [28] [29] [30] [31] [32]  capabilities for children with ASD. These models rely on large volumes of data, however, and with children being severely underrepresented in the few datasets available, there is a necessity for more behavioral data for both neurotypical and autistic children  [34] . To address this issue, we have previously developed a Charades-style mobile game named GuessWhat  [35] [36] [37] [38] [39] , which challenges children with ASD to improve their social interactions while simultaneously collecting structured data for diagnostic and therapeutic AI development.\n\nBecause children with ASD and similar developmental behavioral conditions often display significant impairment in both the understanding and imitation of facial emotion  [68] [69] , we explored the deployment of emotion recognition classifiers on mobile devices in this study. We optimized for both classification performance and efficiency on a Motorola Moto G6 phone. Our best model was able to match previous state of the art results on emotion recognition for children while being lightweight enough to perform inference on the Moto G6 in real-time. This work can be used for a variety of applications such as mobile health therapies for ASD which provide targeted emotion treatment based on the affective profile of the user.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work",
      "text": "The fields of facial emotion recognition and development of machine learning models for edge devices are vast. Prior work relevant to this study can be divided into three categories: (A) facial emotion recognition, (B) neural architecture search, and (C) model compression techniques.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Facial Emotion Recognition",
      "text": "Facial emotion recognition (FER) is a widely researched field with a large library of datasets and classifiers. Early techniques introduced by Kharat et al.  [40]  involved extracting facial key points from faces and passing them through standard models such as support vector machines. Although initial results using this method were promising and computationally inexpensive, these classifiers were evaluated against small, well-structured datasets such as the Extended Cohn-Kanade (CK+) dataset  [41]  and the Japanese Female Facial Expression (JAFFE) dataset  [42] . When tested against more heterogeneous data of images taken from a variety of orientations, such as the Face Expression Recognition 2013 (FER-2013) dataset  [43] , models received much lower scores  [44] .\n\nOut of all existing techniques, convolutional neural networks (CNNs) have shown the greatest potential in both accuracy and generalizability due to their powerful automatic feature extraction.  [45] [46] . Thus, CNNs are presently the most widely used technique in FER, with an ensemble of CNNs with residual masking blocks leveraged by Luan et al. to achieve current state of the art results  [47] .\n\nWhile CNNs have seen results in FER improve consistently throughout recent years and have been used in similar applications such as eye gaze detection  [70] [71] , there are few endeavors involving classification on children's faces. The Child Affective Facial Expression Set (CAFE)  [48]  currently is the largest dataset of facial expressions from children and a standard benchmark in the field of FER on children. The current state of the art on this dataset, achieved by Washington et al., was able to achieve 69% accuracy using a ResNet152-V2 architecture pre-trained on ImageNet weights  [34] .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Neural Architecture Search",
      "text": "Neural architecture search (NAS) is a paradigm for several techniques to automate network architecture engineering. NAS can be used to find efficient deep neural network architectures that can be used for facial emotion recognition. While NAS takes a large amount of computational power to find the optimal network, it can be easily tailored to find the best model for a specific use case. For instance, Lee et al. used NAS to build EmotionNet Nano  [49] , which was able to outperform other state-of-the-art models at FER while optimizing for speed and energy. Although we were unable to pursue NAS in this study due to computational limitations, we highlight this field as an interesting area of potential research, especially when paired with model compression techniques.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Model Compression Techniques",
      "text": "With CNNs being both computationally and memory intensive, several model compression techniques have been developed to make these models more lightweight post-training. Han et al. proposes three techniques to increase inference speed while decreasing memory overhead and energy consumption: weight pruning, weight clustering, and quantization  [50] .\n\nWeight pruning involves gradually zeroing the magnitudes of the weights, making the model sparser by effectively removing weights that have the least significance in the model's predictions. When weight pruning is used with weight clustering, which groups homogeneous weights together to share common values, model size can be decreased by as much as 9x to 13x with negligible accuracy loss  [50] . By quantizing the standard 32-bit weights of a model to a lower bit representation, models can be further compressed and even deployed on specialized edge hardware for faster inference  [51] . In this study, we use all these techniques in conjunction to improve our models' performance.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Methods",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Data Collection",
      "text": "We leveraged images from ten relatively small yet well controlled datasets in order to train our models: NIMH Child Emotional Faces Picture Set (NIMH-ChEFS)  [53] , Facial Expression Phoenix (FePh)  [52] , Karolinska Directed Emotional Faces (KDEF)  [54] , Averaged KDEF (AKDEF)  [54] , Dartmouth Database of Children's Faces (Dartmouth)  [55] , Extended Cohn-Kanade Dataset (CK+)  [41] , Japanese Female Facial Expression (JAFFE)  [42] , Radboud Faces Dataset (RaFD)  [56] , NimStim Set of Facial Expressions (NimStim)  [57] , and the Tsinghua Facial Expression Database (Tsinghua-FED)  [58] .\n\nWe also used the Face Expression Recognition 2013 (FER-2013) dataset  [44]  and a ~1600 subset of images from Expression in-the-Wild (ExpW)  [59] , a large library of web scraped images of faces, to balance the ratio of samples of each emotion. In total, 57884 images were used to train each model. Although we utilized a wider variety of datasets than the current state of the art, we used ~15000 fewer images  [34] .\n\nWhile detailed background information was not provided for the subjects present in FePH, FER-2013 and ExpW, we were able to compile demographics for the remaining datasets, which are shown in the table below. Despite collecting a large and diverse collection of datasets for our study, children were still heavily underrepresented in our training data with only NIMH-ChEFS, Dartmouth, and a small portion of RaFD containing children's faces. Although subjects came from a wide background of ethnicities, there still was an overwhelmingly large number of Caucasians and little to no presence of subjects from African or South-Asian descent. In the future, we hope that subsequent datasets manage to capture these underrepresented groups.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Data Preprocessing",
      "text": "Before training our models, faces were cropped from all images using the Oxford VGGFace model  [60]  with a ResNet50 backbone. Images were then resized to 224x224 pixels and grayscale images were converted to three color channels. All images were then normalized to a range from -1 to 1.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Model Training",
      "text": "We trained and compared five existing CNN architectures designed for deployment on mobile devices: MobileNetV3-Small 1.0x  [61] , MobileNetV2 1.0x  [62] , EfficientNet-B0  [63] , MobileNetV3 1.0x  [62] , and NasNetMobile  [64] , all of which were pre-trained on ImageNet  [65] . We retrained each layer of each network using categorical cross entropy loss and an Adam optimizer  [66]  with a learning rate of 1e-5. During training, all images were subject to a potential horizontal flip, zoomed in/out by a factor up to 0.15, rotated between -45 degrees and 45 degrees, shifted by a factor up to 0.10, and brightened by a factor between 0.80 and 1.20. We assumed the model converged and thus interrupted training once the validation loss did not improve for five consecutive epochs.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "D. Model Evaluation",
      "text": "We evaluated our models against the Child Affective Facial Expression Set (CAFE)  [67] , a large dataset consisting of facial expressions for children. CAFE is an excellent benchmark for this study as the children present are from a wide range of ethnic backgrounds, as shown in the figure below. CAFE's subjects are also aged between 2 to 8 years, the same range where an autism diagnosis is most vital  [6] . Children in this dataset express seven emotions: happiness, sadness, surprise, fear, anger, disgust, and neutrality. Gender and Age of CAFE Subjects Fig.  2 .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Ethnicity Of Cafe Subjects",
      "text": "We additionally evaluated Subset A and Subset B of CAFE to observe our models' performance against faces that even human annotators have difficulty classifying. Subset A contains faces that were identified with 60% accuracy or above by 100 adult participants. In contrast, Subset B contains faces with substantially greater variability for each emotion, resulting in a Cronbach's alpha internal consistency score that is 0.052 lower than Subset A.  [48] .\n\nWe profiled all models on a Motorola Moto G6 Phone using the Tensorflow Lite benchmark API. We also deployed our models on an Android demo app we built that performs real-time image classification on a live video feed to ensure our models matched the results we received from the benchmark tool.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "E. Model Optimization And Reevaluation",
      "text": "After evaluating on CAFE, we performed weight pruning before fine tuning the network until the validation loss did not improve for five consecutive epochs. We then applied weight clustering before fine tuning the network again in an identical fashion. We finally performed quantized-aware training before evaluating the fully optimized model against CAFE. If the model was unable to undergo quantized-aware training, we applied post-training quantization instead.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iv. Results",
      "text": "Upon evaluation, our best model was the MobileNetV2 1.0x, which achieved 65.11% balanced accuracy and 64.19% F1-score on CAFE (confusion matrix in Figure  3 ). This performance increases to 79.54% balanced accuracy and 79.43% F1-score on Subset A of CAFE (confusion matrix in Figure  4 ). When evaluated on CAFE Subset B, the MobileNetV2 model achieves 63.94% balanced accuracy and 62.32% F1-score (confusion matrix on Figure  5 ), attaining accuracies higher than those that even human annotators could achieve  [48] . Although models with more complex architectures than the MobileNetV2 were used, they performed slightly worse when evaluated on CAFE despite having higher validation accuracies. We believe that this issue occurs because the validation dataset still contains dissimilarities between the training dataset, even though we used the most diverse library of facial emotion images ever to train on CAFE. Thus, models with significantly more complexity overfit on the validation set and do worse when tested. Despite the discrepancy between the validation and testing set, all models still achieved impressive results with accuracies and F1-scores above 61%, nearly matching state of the art results while being far more lightweight. We profiled all five models on our Motorola Moto G6 phone and measured the memory consumption and latency when it performed inference on an image. We were able to decrease memory consumption by 4x and latency by ~1.3x using weight pruning, weight clustering, and quantization. These improvements are significant considering how few refinements could be made to these specific networks, as they already started out incredibly well-optimized simply by their architecture. We also experimented with the optimal number of threads that should be added to the app process to get the best performance, and found that using seven threads on the Moto G6 yielded the lowest model latency for most models. Relationship between Model Latency and CPU Threads",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "V. Discussion",
      "text": "In this study, we trained several machine learning models to recognize emotion on childrens' faces. Using various optimization techniques, we were able to match state of the art accuracy while ensuring each model is able to perform real-time inference on a mobile device. These models are thus lightweight and accurate enough to be deployed in mobile health therapies such as targeted emotion treatment to children with ASD based on their affective profile, although specificity will need to be assessed in future studies. We also show that with specialized training, machine learning models designed for running on edge devices can achieve state of the art results on difficult classification tasks.\n\nAlthough these results are promising, there were several limitations to this study. Due to a lack of computational resources, we were unable to create a custom model using neural architecture search. As a result, our model's backbone was not tailored specifically for the task of emotion classification on children and could still contain redundant layers despite being well optimized. In addition, while our models evaluated well on seven emotions, larger models may be needed to generalize to data with multiple and/or more emotions.\n\nFuture work includes the further collection of childrens' faces, which are still heavily underrepresented in facial emotion datasets and caused imbalance between the validation and testing datasets in this study. Another area of promise is deploying these models for on-device training. Once achieved, copies of a model can be sent to multiple decentralized mobile devices, before being concatenated as a global model once each model is individually trained on data captured by its device. This technique, known as federated learning, is an active field of research and could be leveraged to further improve these models in a privacy-preserving manner while they simultaneously provide therapeutic diagnosis and treatment of ASD.",
      "page_start": 5,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Gender and Age of CAFE Subjects",
      "page": 3
    },
    {
      "caption": "Figure 2: Ethnicity of CAFE Subjects",
      "page": 3
    },
    {
      "caption": "Figure 5: ), attaining",
      "page": 4
    },
    {
      "caption": "Figure 3: Confusion Matrix for Entirety of CAFE",
      "page": 4
    },
    {
      "caption": "Figure 4: Confusion Matrix for Subset A of CAFE",
      "page": 4
    },
    {
      "caption": "Figure 5: Confusion Matrix for Subset B of CAFE",
      "page": 4
    },
    {
      "caption": "Figure 6: Relationship between Model Latency and CPU Threads",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset": "NIMH-ChEFS",
          "#\nSubjects": "59",
          "Age": "M=13.57\nSD=1.66",
          "Ethnicity": "Mostly Caucasian",
          "%\nFemale": "66.1"
        },
        {
          "Dataset": "KDEF/AKDEF",
          "#\nSubjects": "70",
          "Age": "M=23.73\nSD=7.24",
          "Ethnicity": "Latino",
          "%\nFemale": "50.0"
        },
        {
          "Dataset": "Dartmouth",
          "#\nSubjects": "80",
          "Age": "M=9.84\nSD=2.33",
          "Ethnicity": "Caucasian",
          "%\nFemale": "50.0"
        },
        {
          "Dataset": "CK+",
          "#\nSubjects": "123",
          "Age": "98.14",
          "Ethnicity": "81% Caucasian;\n13% African",
          "%\nFemale": "69.00"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "American; 6%\nOther": "Asian"
        },
        {
          "American; 6%\nOther": "Caucasian"
        },
        {
          "American; 6%\nOther": "58% Caucasian;\n23%\nAfro-American;\n14% Asian;\n5% Latino"
        },
        {
          "American; 6%\nOther": "Asian"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Size": "Params\n(M)",
          "Balanced Accuracy (%)": "Subset\nA"
        },
        {
          "Size": "1.54",
          "Balanced Accuracy (%)": "75.11"
        },
        {
          "Size": "2.27",
          "Balanced Accuracy (%)": "79.54"
        },
        {
          "Size": "4.06",
          "Balanced Accuracy (%)": "78.49"
        },
        {
          "Size": "4.24",
          "Balanced Accuracy (%)": "76.97"
        },
        {
          "Size": "4.27",
          "Balanced Accuracy (%)": "76.32"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Original": "Latency\n(ms)",
          "Optimized": "Latency\n(ms)"
        },
        {
          "Original": "52.33",
          "Optimized": "45.48"
        },
        {
          "Original": "62.33",
          "Optimized": "45.61"
        },
        {
          "Original": "415.04",
          "Optimized": "301.62"
        },
        {
          "Original": "124.47",
          "Optimized": "98.14"
        },
        {
          "Original": "218.07",
          "Optimized": "192.85"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Prevalence of Autism Spectrum Disorder Among Children Aged 8 Years -Autism and Developmental Disabilities Monitoring Network, 11 Sites",
      "authors": [
        "Matthew Maenner"
      ],
      "year": "2016",
      "venue": "MMWR Surveillance Summaries"
    },
    {
      "citation_id": "2",
      "title": "Early Behavioral Intervention, Brain Plasticity, and the Prevention of Autism Spectrum Disorder",
      "authors": [
        "Geraldine Dawson"
      ],
      "year": "2008",
      "venue": "Development and Psychopathology"
    },
    {
      "citation_id": "3",
      "title": "Social and Communication Development in Toddlers with Early and Later Diagnosis of Autism Spectrum Disorders",
      "authors": [
        "Rebecca Landa",
        "C Katherine",
        "Elizabeth Holman",
        "Garrett-Mayer"
      ],
      "year": "2007",
      "venue": "Archives of General Psychiatry"
    },
    {
      "citation_id": "4",
      "title": "Autism Diagnostic Interview-Revised: A Revised Version of a Diagnostic Interview for Caregivers of Individuals with Possible Pervasive Developmental Disorders",
      "authors": [
        "C Lord",
        "M Rutter",
        "A Couteur"
      ],
      "year": "1994",
      "venue": "Journal of Autism and Developmental Disorders"
    },
    {
      "citation_id": "5",
      "title": "The Importance of Early Identification and Intervention for Children with or at Risk for Autism Spectrum Disorders",
      "authors": [
        "Lynn Koegel",
        "Robert Kern",
        "Kristen Koegel",
        "Jessica Ashbaugh",
        "Bradshaw"
      ],
      "year": "2014",
      "venue": "International Journal of Speech-Language Pathology"
    },
    {
      "citation_id": "6",
      "title": "Diagnostic and Statistical Manual of Mental Disorders 5: A Quick Glance",
      "authors": [
        "Vahia",
        "N Vihang"
      ],
      "year": "2013",
      "venue": "Indian Journal of Psychiatry"
    },
    {
      "citation_id": "7",
      "title": "On the Relationship between Socio-Economic Factors and Cell Phone Usage",
      "authors": [
        "Vanessa Frias-Martinez",
        "Jesus Virseda"
      ],
      "year": "2012",
      "venue": "Proceedings of the Fifth International Conference on Information and Communication Technologies and Development -ICTD '12"
    },
    {
      "citation_id": "8",
      "title": "Cell-Phone Medicine Brings Care to Patients in Developing Nations",
      "authors": [
        "J Feder",
        "Lester"
      ],
      "year": "2010",
      "venue": "Health Affairs (Project Hope)"
    },
    {
      "citation_id": "9",
      "title": "Feature replacement methods enable reliable home video analysis for machine learning detection of autism",
      "authors": [
        "Emilie Leblanc",
        "Peter Washington",
        "Maya Varma",
        "Kaitlyn Dunlap",
        "Yordan Penev",
        "Aaron Kline",
        "Dennis Wall"
      ],
      "year": "2020",
      "venue": "Scientific reports"
    },
    {
      "citation_id": "10",
      "title": "Data-driven diagnostics and the potential of mobile artificial intelligence for digital therapeutic phenotyping in computational psychiatry",
      "authors": [
        "Peter Washington",
        "Natalie Park",
        "Parishkrita Srivastava",
        "Catalin Voss",
        "Aaron Kline",
        "Maya Varma",
        "Qandeel Tariq"
      ],
      "year": "2020",
      "venue": "Biological Psychiatry: Cognitive Neuroscience and Neuroimaging"
    },
    {
      "citation_id": "11",
      "title": "Enhancing diagnosis of autism with optimized machine learning models and personal characteristic data",
      "authors": [
        "Milan Parikh",
        "Hailong Li",
        "Lili He"
      ],
      "year": "2019",
      "venue": "Frontiers in computational neuroscience"
    },
    {
      "citation_id": "12",
      "title": "Precision telemedicine through crowdsourced machine learning: testing variability of crowd workers for video-based autism feature recognition",
      "authors": [
        "Peter Washington",
        "Emilie Leblanc",
        "Kaitlyn Dunlap",
        "Yordan Penev",
        "Aaron Kline",
        "Kelley Paskov",
        "Min Sun"
      ],
      "year": "2020",
      "venue": "Journal of personalized medicine"
    },
    {
      "citation_id": "13",
      "title": "Feature selection and dimension reduction of social autism data",
      "authors": [
        "Peter Washington",
        "Kelley Paskov",
        "Haik Kalantarian",
        "Nathaniel Stockham",
        "Catalin Voss",
        "Aaron Kline",
        "Ritik Patnaik"
      ],
      "year": "2019",
      "venue": "PACIFIC SYMPOSIUM ON BIOCOMPUTING 2020"
    },
    {
      "citation_id": "14",
      "title": "Validity of online screening for autism: crowdsourcing study comparing paid and unpaid diagnostic tasks",
      "authors": [
        "Peter Washington",
        "Haik Kalantarian",
        "Qandeel Tariq",
        "Jessey Schwartz",
        "Kaitlyn Dunlap",
        "Brianna Chrisman",
        "Maya Varma"
      ],
      "year": "2019",
      "venue": "Journal of medical Internet research"
    },
    {
      "citation_id": "15",
      "title": "Detecting developmental delay and autism through machine learning models using home videos of Bangladeshi children: Development and validation study",
      "authors": [
        "Qandeel Tariq",
        "Scott Fleming",
        "Jessey Schwartz",
        "Kaitlyn Dunlap",
        "Conor Corbin",
        "Peter Washington",
        "Haik Kalantarian",
        "Z Naila",
        "Gary Khan",
        "Dennis Darmstadt",
        "Wall"
      ],
      "year": "2019",
      "venue": "Journal of medical Internet research"
    },
    {
      "citation_id": "16",
      "title": "Selection of trustworthy crowd workers for telemedical diagnosis of pediatric autism spectrum disorder",
      "authors": [
        "Peter Washington",
        "Emilie Leblanc",
        "Kaitlyn Dunlap",
        "Yordan Penev",
        "Maya Varma",
        "Jae-Yoon Jung",
        "Brianna Chrisman"
      ],
      "year": "2020",
      "venue": "BIOCOMPUTING 2021: Proceedings of the Pacific Symposium"
    },
    {
      "citation_id": "17",
      "title": "Crowdsourced privacy-preserved feature tagging of short home videos for machine learning ASD detection",
      "authors": [
        "Peter Washington",
        "Qandeel Tariq",
        "Emilie Leblanc",
        "Brianna Chrisman",
        "Kaitlyn Dunlap",
        "Aaron Kline",
        "Haik Kalantarian"
      ],
      "year": "2021",
      "venue": "Scientific reports"
    },
    {
      "citation_id": "18",
      "title": "Crowd Annotations Can Approximate Clinical Autism Impressions from Short Home Videos with Privacy Protections",
      "authors": [
        "Peter Washington",
        "Emilie Leblanc",
        "Kaitlyn Dunlap",
        "Aaron Kline",
        "Cezmi Mutlu",
        "Brianna Chrisman",
        "Nate Stockham",
        "Kelley Paskov",
        "Dennis Wall"
      ],
      "year": "2021",
      "venue": "medRxiv"
    },
    {
      "citation_id": "19",
      "title": "Identification of social engagement indicators associated with autism spectrum disorder using a game-based mobile application",
      "authors": [
        "Maya Varma",
        "Peter Washington",
        "Brianna Chrisman",
        "Aaron Kline",
        "Emilie Leblanc",
        "Kelley Paskov",
        "Nate Stockham",
        "Jae-Yoon Jung",
        "Min Sun",
        "Dennis Wall"
      ],
      "year": "2021",
      "venue": "medRxiv"
    },
    {
      "citation_id": "20",
      "title": "Crowdsourced feature tagging for scalable and privacy-preserved autism diagnosis",
      "authors": [
        "Peter Washington",
        "Qandeel Tariq",
        "Emilie Leblanc",
        "Brianna Chrisman",
        "Kaitlyn Dunlap",
        "Aaron Kline",
        "Haik Kalantarian"
      ],
      "year": "2020",
      "venue": "medRxiv"
    },
    {
      "citation_id": "21",
      "title": "Use of artificial intelligence to shorten the be",
      "authors": [
        "Dennis Wall",
        "Rebecca Dally",
        "Rhiannon Luyster",
        "Jae-Yoon Jung",
        "Todd Deluca"
      ],
      "venue": "Use of artificial intelligence to shorten the be"
    },
    {
      "citation_id": "22",
      "title": "Multi-modular AI approach to streamline autism diagnosis in young children",
      "authors": [
        "Halim Abbas",
        "Ford Garberson",
        "Stuart Liu-Mayo",
        "Eric Glover",
        "Dennis Wall"
      ],
      "year": "2020",
      "venue": "Scientific reports"
    },
    {
      "citation_id": "23",
      "title": "Use of machine learning to shorten observation-based screening and diagnosis of autism",
      "authors": [
        "Dennis Wall",
        "J Paul",
        "T Kosmicki",
        "E Deluca",
        "Vincent Harstad",
        "Fusaro"
      ],
      "year": "2012",
      "venue": "Translational psychiatry"
    },
    {
      "citation_id": "24",
      "title": "Clinical evaluation of a novel and mobile autism risk assessment",
      "authors": [
        "Marlena Duda",
        "Jena Daniels",
        "Dennis Wall"
      ],
      "year": "2016",
      "venue": "Journal of autism and developmental disorders"
    },
    {
      "citation_id": "25",
      "title": "The potential of accelerating early detection of autism through content analysis of YouTube videos",
      "authors": [
        "Vincent Fusaro",
        "Jena Daniels",
        "Marlena Duda",
        "Todd Deluca",
        "D' Olivia",
        "Jenna Angelo",
        "James Tamburello",
        "Dennis Maniscalco",
        "Wall"
      ],
      "year": "2014",
      "venue": "PLOS one"
    },
    {
      "citation_id": "26",
      "title": "Superpower glass",
      "authors": [
        "Aaron Kline",
        "Catalin Voss",
        "Peter Washington",
        "Nick Haber",
        "Hessey Schwartz",
        "Qandeel Tariq",
        "Terry Winograd",
        "Carl Feinstein",
        "Dennis Wall"
      ],
      "year": "2019",
      "venue": "GetMobile: Mobile Computing and Communications"
    },
    {
      "citation_id": "27",
      "title": "Superpower glass: delivering unobtrusive real-time social cues in wearable systems",
      "authors": [
        "Catalin Voss",
        "Peter Washington",
        "Nick Haber",
        "Aaron Kline",
        "Jena Daniels",
        "Azar Fazel",
        "Titas De"
      ],
      "year": "2016",
      "venue": "Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct"
    },
    {
      "citation_id": "28",
      "title": "Effect of wearable digital intervention for improving socialization in children with autism spectrum disorder: a randomized clinical trial",
      "authors": [
        "Catalin Voss",
        "Jessey Schwartz",
        "Jena Daniels",
        "Aaron Kline",
        "Nick Haber",
        "Peter Washington",
        "Qandeel Tariq"
      ],
      "year": "2019",
      "venue": "JAMA pediatrics"
    },
    {
      "citation_id": "29",
      "title": "Making emotions transparent: Google Glass helps autistic kids understand facial expressions through augmented-reality therapy",
      "authors": [
        "Nick Haber",
        "Catalin Voss",
        "Dennis Wall"
      ],
      "year": "2020",
      "venue": "IEEE Spectrum"
    },
    {
      "citation_id": "30",
      "title": "Exploratory study examining the at-home feasibility of a wearable tool for social-affective learning in children with autism",
      "authors": [
        "Jena Daniels",
        "Jessey Schwartz",
        "Catalin Voss",
        "Nick Haber",
        "Azar Fazel",
        "Aaron Kline",
        "Peter Washington",
        "Carl Feinstein",
        "Terry Winograd",
        "Dennis Wall"
      ],
      "year": "2018",
      "venue": "NPJ digital medicine"
    },
    {
      "citation_id": "31",
      "title": "Effect of wearable digital intervention for improving socialization in children with autism spectrum disorder: a randomized clinical trial",
      "authors": [
        "Catalin Voss",
        "Jessey Schwartz",
        "Jena Daniels",
        "Aaron Kline",
        "Nick Haber",
        "Peter Washington",
        "Qandeel Tariq"
      ],
      "year": "2019",
      "venue": "JAMA pediatrics"
    },
    {
      "citation_id": "32",
      "title": "The Potential for Machine Learning-Based Wearables to Improve Socialization in Teenagers and Adults With Autism Spectrum Disorder-Reply",
      "authors": [
        "Catalin Voss",
        "Nick Haber",
        "Dennis Wall"
      ],
      "year": "2019",
      "venue": "JAMA pediatrics"
    },
    {
      "citation_id": "33",
      "title": "Exploratory Study Examining the At-Home Feasibility of a Wearable Tool for Social-Affective Learning in Children with Autism",
      "authors": [
        "Jena Daniels",
        "Jessey Schwartz",
        "Catalin Voss",
        "Nick Haber",
        "Azar Fazel",
        "Aaron Kline",
        "Peter Washington",
        "Carl Feinstein",
        "Terry Winograd",
        "Dennis Wall"
      ],
      "year": "2018",
      "venue": "Npj Digital Medicine"
    },
    {
      "citation_id": "34",
      "title": "Training an Emotion Detection Classifier Using Frames from a Mobile Therapeutic Game for Children with Developmental Disorders",
      "authors": [
        "Peter Washington",
        "Haik Kalantarian",
        "Jack Kent",
        "Arman Husic",
        "Aaron Kline",
        "Emilie Leblanc",
        "Cathy Hou"
      ],
      "year": "2020",
      "venue": "Training an Emotion Detection Classifier Using Frames from a Mobile Therapeutic Game for Children with Developmental Disorders"
    },
    {
      "citation_id": "35",
      "title": "Guess what?",
      "authors": [
        "Haik Kalantarian",
        "Peter Washington",
        "Jessey Schwartz",
        "Jena Daniels",
        "Nick Haber",
        "Dennis Wall"
      ],
      "year": "2019",
      "venue": "Journal of healthcare informatics research"
    },
    {
      "citation_id": "36",
      "title": "The performance of emotion classifiers for children with parent-reported autism: quantitative feasibility study",
      "authors": [
        "Haik Kalantarian",
        "Khaled Jedoui",
        "Kaitlyn Dunlap",
        "Jessey Schwartz",
        "Peter Washington",
        "Arman Husic",
        "Qandeel Tariq",
        "Michael Ning",
        "Aaron Kline",
        "Dennis Wall"
      ],
      "year": "2020",
      "venue": "JMIR mental health"
    },
    {
      "citation_id": "37",
      "title": "A gamified mobile system for crowdsourcing video for autism research",
      "authors": [
        "Haik Kalantarian",
        "Peter Washington",
        "Jessey Schwartz",
        "Jena Daniels",
        "Nick Haber",
        "Dennis Wall"
      ],
      "year": "2018",
      "venue": "2018 IEEE international conference on healthcare informatics (ICHI)"
    },
    {
      "citation_id": "38",
      "title": "Labeling images with facial emotion and the potential for pediatric healthcare",
      "authors": [
        "Haik Kalantarian",
        "Khaled Jedoui",
        "Peter Washington",
        "Qandeel Tariq",
        "Kaiti Dunlap",
        "Jessey Schwartz",
        "Dennis Wall"
      ],
      "year": "2019",
      "venue": "Artificial intelligence in medicine"
    },
    {
      "citation_id": "39",
      "title": "A mobile game for automatic emotion-labeling of images",
      "authors": [
        "Haik Kalantarian",
        "Khaled Jedoui",
        "Peter Washington",
        "Dennis Wall"
      ],
      "year": "2018",
      "venue": "IEEE transactions on games"
    },
    {
      "citation_id": "40",
      "title": "Human Emotion Recognition System Using Optimally Designed SVM with Different Facial Feature Extraction Techniques",
      "authors": [
        "G Kharat",
        "S Dudul"
      ],
      "year": "2008",
      "venue": "WSEAS Transactions on Computers"
    },
    {
      "citation_id": "41",
      "title": "The Extended Cohn-Kanade Dataset (CK+): A Complete Dataset for Action Unit and Emotion-Specified Expression",
      "authors": [
        "Patrick Lucey",
        "Jeffrey Cohn",
        "Takeo Kanade",
        "Jason Saragih",
        "Zara Ambadar",
        "Iain Matthews"
      ],
      "year": "2010",
      "venue": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition -Workshops"
    },
    {
      "citation_id": "42",
      "title": "Coding Facial Expressions with Gabor Wavelets",
      "authors": [
        "M Lyons",
        "S Akamatsu",
        "M Kamachi",
        "J Gyoba"
      ],
      "year": "2002",
      "venue": "Proceedings Third IEEE International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "43",
      "title": "Challenges in Representation Learning: A Report on Three Machine Learning Contests",
      "authors": [
        "Ian Goodfellow",
        "Pierre Dumitru Erhan",
        "Aaron Carrier",
        "Mehdi Courville",
        "Ben Mirza",
        "Will Hamner",
        "Cukierski"
      ],
      "year": "2015",
      "venue": "Challenges in Representation Learning: A Report on Three Machine Learning Contests"
    },
    {
      "citation_id": "44",
      "title": "Facial Expression Recognition Using Convolutional Neural Networks: State of the Art",
      "authors": [
        "Christopher Pramerdorfer",
        "Martin Kampel"
      ],
      "year": "2016",
      "venue": "ArXiv"
    },
    {
      "citation_id": "45",
      "title": "A Survey on Human Face Expression Recognition Techniques",
      "authors": [
        "I Revina",
        "W Michael",
        "Emmanuel Sam"
      ],
      "year": "2018",
      "venue": "Journal of King Saud University -Computer and Information Sciences"
    },
    {
      "citation_id": "46",
      "title": "A Brief Review of Facial Emotion Recognition Based on Visual Information",
      "authors": [
        "Byoung Ko",
        "Chul"
      ],
      "year": "2018",
      "venue": "Sensors"
    },
    {
      "citation_id": "47",
      "title": "Facial Expression Recognition Using Residual Masking Network",
      "authors": [
        "Pham Luan"
      ],
      "year": "2020",
      "venue": "IEEE 25th International Conference on Pattern Recognition"
    },
    {
      "citation_id": "48",
      "title": "The Child Affective Facial Expression (CAFE) Set: Validity and Reliability from Untrained Adults",
      "authors": [
        "Vanessa Lobue",
        "Cat Thrasher"
      ],
      "year": "2014",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "49",
      "title": "EmotionNet Nano: An Efficient Deep Convolutional Neural Network Design for Real-Time Facial Expression Recognition",
      "authors": [
        "James Lee",
        "Linda Ren",
        "Alexander Wang",
        "Wong"
      ],
      "year": "2020",
      "venue": "Frontiers in Artificial Intelligence"
    },
    {
      "citation_id": "50",
      "title": "Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding",
      "authors": [
        "Song Han",
        "Huizi Mao",
        "William Dally"
      ],
      "year": "2015",
      "venue": "ArXiv"
    },
    {
      "citation_id": "51",
      "title": "High-Speed, Real-Time, Spike-Based Object Tracking and Path Prediction on Google Edge TPU",
      "authors": [
        "Jonah Sengupta",
        "Rajkumar Kubendran",
        "Emre Neftci",
        "Andreas Andreou"
      ],
      "year": "2020",
      "venue": "2020 2nd IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS)"
    },
    {
      "citation_id": "52",
      "title": "Facial Expression Phoenix (FePh): An Annotated Sequenced Dataset for Facial and Emotion-Specified Expressions in Sign Language",
      "authors": [
        "Marie Alaghband",
        "Niloofar Yousefi",
        "Ivan Garibay"
      ],
      "year": "2020",
      "venue": "ArXiv"
    },
    {
      "citation_id": "53",
      "title": "The NIMH Child Emotional Faces Picture Set (NIMH-ChEFS): A New Set of Children's Facial Emotion Stimuli: Child Emotional Faces Stimuli",
      "authors": [
        "Helen Egger",
        "Daniel Link",
        "Eric Pine",
        "Ellen Nelson",
        "Monique Leibenluft",
        "Kenneth Ernst",
        "Adrian Towbin",
        "Angold"
      ],
      "year": "2011",
      "venue": "International Journal of Methods in Psychiatric Research"
    },
    {
      "citation_id": "54",
      "title": "The Karolinska Directed Emotional Faces: A Validation Study",
      "authors": [
        "Ellen Goeleven",
        "Rudi De Raedt",
        "Lemke Leyman",
        "Bruno Verschuere"
      ],
      "year": "2008",
      "venue": "Cognition & Emotion"
    },
    {
      "citation_id": "55",
      "title": "The Dartmouth Database of Children's Faces: Acquisition and Validation of a New Face Stimulus Set",
      "authors": [
        "Kirsten Dalrymple",
        "Jesse Gomez",
        "Brad Duchaine"
      ],
      "year": "2013",
      "venue": "PloS One"
    },
    {
      "citation_id": "56",
      "title": "Presentation and Validation of the Radboud Faces Database",
      "authors": [
        "Oliver Langner",
        "Ron Dotsch",
        "Gijsbert Bijlstra",
        "H Daniel",
        "Skyler Wigboldus",
        "Ad Hawk",
        "Van Knippenberg"
      ],
      "year": "2010",
      "venue": "Cognition & Emotion"
    },
    {
      "citation_id": "57",
      "title": "The NimStim Set of Facial Expressions: Judgments from Untrained Research Participants",
      "authors": [
        "Tottenham",
        "James Nim",
        "Andrew Tanaka",
        "Thomas Leon",
        "Marcella Mccarry",
        "Todd Nurse",
        "David Hare",
        "Alissa Marcus",
        "B Westerlund",
        "Charles Casey",
        "Nelson"
      ],
      "year": "2009",
      "venue": "The NimStim Set of Facial Expressions: Judgments from Untrained Research Participants"
    },
    {
      "citation_id": "58",
      "title": "Tsinghua Facial Expression Database -A Database of Facial Expressions in Chinese Young and Older Women and Men: Development and Validation",
      "authors": [
        "Tao Yang",
        "Zeyun Yang",
        "Guangzheng Xu",
        "Duoling Gao",
        "Ziheng Zhang",
        "Hui Wang",
        "Shiyu Liu"
      ],
      "year": "2020",
      "venue": "PloS One"
    },
    {
      "citation_id": "59",
      "title": "From Facial Expression Recognition to Interpersonal Relation Prediction",
      "authors": [
        "Zhanpeng Zhang",
        "Ping Luo"
      ],
      "year": "2018",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "60",
      "title": "Deep Face Recognition",
      "authors": [
        "M Omkar",
        "Andrea Parkhi",
        "Andrew Vedaldi",
        "Zisserman"
      ],
      "year": "2015",
      "venue": "Proceedings of the British Machine Vision Conference (BMVC)"
    },
    {
      "citation_id": "61",
      "title": "Searching for MobileNetV3",
      "authors": [
        "Andrew Howard",
        "Mark Sandler",
        "Grace Chu",
        "Liang-Chieh Chen",
        "Bo Chen",
        "Mingxing Tan",
        "Weijun Wang"
      ],
      "year": "2019",
      "venue": "ArXiv"
    },
    {
      "citation_id": "62",
      "title": "MobileNetV2: Inverted Residuals and Linear Bottlenecks",
      "authors": [
        "Mark Sandler",
        "Andrew Howard",
        "Menglong Zhu",
        "Andrey Zhmoginov",
        "Liang-Chieh Chen"
      ],
      "year": "2018",
      "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "63",
      "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
      "authors": [
        "Mingxing Tan",
        "V Quoc",
        "Le"
      ],
      "year": "2019",
      "venue": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"
    },
    {
      "citation_id": "64",
      "title": "Learning transferable architectures for scalable image recognition",
      "authors": [
        "Barret Zoph",
        "Vijay Vasudevan",
        "Jonathon Shlens",
        "Quoc Le"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "65",
      "title": "ImageNet: A Large-Scale Hierarchical Image Database",
      "authors": [
        "Jia Deng",
        "Wei Dong",
        "Richard Socher",
        "Li-Jia Li",
        "Kai Li",
        "Li Fei-Fei"
      ],
      "year": "2009",
      "venue": "2009 IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "66",
      "title": "Adam: A Method for Stochastic Optimization",
      "authors": [
        "Diederik Kingma",
        "Jimmy Ba"
      ],
      "year": "2014",
      "venue": "ArXiv"
    },
    {
      "citation_id": "67",
      "title": "Mobile detection of autism through machine learning on home video: A development and prospective validation study",
      "authors": [
        "Qandeel Tariq",
        "Jena Daniels",
        "Jessey Schwartz",
        "Peter Washington",
        "Haik Kalantarian",
        "Dennis Wall"
      ],
      "year": "2018",
      "venue": "PLoS medicine"
    },
    {
      "citation_id": "68",
      "title": "Alexithymia, but not autism spectrum disorder, may be related to the production of emotional facial expressions",
      "authors": [
        "D Trevisan",
        "M Bowering",
        "E Birmingham"
      ],
      "year": "2016",
      "venue": "Molecular Autism"
    },
    {
      "citation_id": "69",
      "title": "Automatic Recognition of Posed Facial Expression of Emotion in Individuals with Autism Spectrum Disorder",
      "authors": [
        "J Manfredonia ; Manfredonia",
        "A Bangerter",
        "N Manyakov",
        "S Ness",
        "D Lewin",
        "A Skalkin",
        "G Pandina"
      ],
      "year": "2019",
      "venue": "Journal of Autism and Developmental Disorders"
    },
    {
      "citation_id": "70",
      "title": "Detection of eye contact with deep neural networks is as accurate as human experts",
      "authors": [
        "E Chong",
        "E Clark-Whitney",
        "A Southerland",
        "E Stubbs",
        "C Miller",
        "E Ajodan",
        "J Rehg"
      ],
      "year": "2020",
      "venue": "Nature Communications"
    },
    {
      "citation_id": "71",
      "title": "DEEP eye contact detector: Robust eye contact bid detection using convolutional neural network",
      "authors": [
        "Y Mitsuzumi",
        "A Nakazawa",
        "T Nishida"
      ],
      "year": "2017",
      "venue": "Proceedings of the British Machine Vision Conference"
    }
  ]
}