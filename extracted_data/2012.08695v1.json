{
  "paper_id": "2012.08695v1",
  "title": "Dialogxl: All-In-One Xlnet For Multi-Party Conversation Emotion Recognition",
  "published": "2020-12-16T01:50:46Z",
  "authors": [
    "Weizhou Shen",
    "Junqing Chen",
    "Xiaojun Quan",
    "Zhixian Xie"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This paper presents our pioneering effort for emotion recognition in conversation (ERC) with pre-trained language models. Unlike regular documents, conversational utterances appear alternately from different parties and are usually organized as hierarchical structures in previous work. Such structures are not conducive to the application of pre-trained language models such as XLNet. To address this issue, we propose an all-in-one XLNet model, namely DialogXL, with enhanced memory to store longer historical context and dialog-aware self-attention to deal with the multi-party structures. Specifically, we first modify the recurrence mechanism of XLNet from segment-level to utterance-level in order to better model the conversational data. Second, we introduce dialog-aware self-attention in replacement of the vanilla self-attention in XLNet to capture useful intra-and interspeaker dependencies. Extensive experiments are conducted on four ERC benchmarks with mainstream models presented for comparison. The experimental results show that the proposed model outperforms the baselines on all the datasets. Several other experiments such as ablation study and error analysis are also conducted and the results confirm the role of the critical modules of DialogXL.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition in conversation (ERC) is an emerging task in natural language processing (NLP) that aims to identify the emotion of each utterance in a conversation. It can be regarded as an extension of traditional emotion detection from text, or an arising problem in dialogue systems that helps generate emotion-aware dialogues  (Zhou et al. 2017) . Empirical evidence shows that the conversational context of an utterance plays an indispensable role in this task  (Poria et al. 2019) . Moreover, the emotion also tends to stay unchanged within a short context of the conversation. It is thus very critical to effectively model the alternate utterances by different parties.\n\nTo solve this problem, many recent works focus on deep neural networks with hierarchical structures to model the conversational data  (Majumder et al. 2019; Ghosal et al. 2019; Jiao et al. 2019; Zhong, Wang, and Miao 2019) . In these works, each utterance is firstly encoded separately into an utterance representation, which is then modeled sequentially and hierarchically. Although the structures seem to comply with the organization of utterances, they ignore the direct dependencies between words in different utterances. In addition, they are not conducive to the application of pretrained language models such as BERT  (Devlin et al. 2018)  and XLNet  (Yang et al. 2019) , which have achieved superior performance in many dialogue system tasks other than ERC  (Madotto, Wu, and Fung 2018; Bao et al. 2020; Henderson et al. 2019) .\n\nThere are two main challenges to directly apply these pre-trained language models to ERC. First, conversations in ERC are usually multi-party and there can be intra-and inter-speaker dependencies  (Ghosal et al. 2019) . Existing pre-trained language models are not readily feasible to encode these dependencies. Second, almost all language models are constrained by the input length. When the input sequence exceeds the limit, it has to be truncated, which may lead to loss of information in distant historical utterances  (Majumder et al. 2019; Ghosal et al. 2019 ).\n\nTo cope with the above challenges, we introduce an allin-one XLNet model, namely DialogXL, for emotion recognition in multi-turn multi-party conversation. DialogXL intends to apply a strong pre-trained language model to ERC without constructing a complicated, hierarchical model in processing the conversational data. Specifically, it first replaces XLNet's segment recurrence by a more flexible and memory-saving utterance recurrence to utilize historical utterances. Utterance recurrence stores the hidden states of historical utterances in a memory bank and reuses them while identifying a query utterance. Next, the selfattention in XLNet's Transformer layers is substituted for dialog-aware self-attention, which consists of four different types of attention, namely local self-attention, global selfattention, speaker self-attention, and listener self-attention. Dialog-aware self-attention allows DialogXL to model the inter-and intra-speaker dependencies under different reception fields in the historical context. We conduct extensive experiments on four ERC benchmarks and the results show that the proposed model, DialogXL, outperforms all the baselines on the datasets. Furthermore, several studies are conducted to verify the modules of DialogXL, and an error analysis is used to delve into the reasons behind the errors.\n\nTo conclude, our contributions are as follows:\n\n• DialogXL is the first effort of pre-trained language models designed for emotion recognition in conversation (ERC).\n\n• We propose a memory-saving utterance recurrence to replace XLNet's segment recurrence. The new approach allows DialogXL to cache up to 1000 historical words of a conversation, which is more powerful than the vanilla XLNet model.\n\n• Unlike the original self-attention that merely computes attention weights between words, our dialog-aware selfattention computes them by different reception fields and party roles, allowing us to capture useful intra-and interspeaker dependencies.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work Emotion Recognition In Conversation",
      "text": "Emotion recognition in conversation (ERC) has emerged as an important problem in recent years and has attracted numerous interests from the NLP community. The availability of large conversational datasets  (Busso et al. 2008; Schuller et al. 2012; Li et al. 2017; Chen et al. 2018; Poria et al. 2019)  account partly for this phenomenon, and the increasing interests in dialogue systems may also explain it.\n\nRecent works on ERC generally resort to deep learning models. For example, CMN  (Hazarika et al. 2018 ) and ICON  (Hazarika et al. 2018 ) both utilize gated recurrent unit (GRU) and memory networks.  Majumder et al. (2019)  propose a recurrent-based model to model the party state, global state and emotional dynamics.  Jiao et al. (2019)  propose a hierarchical GRU structure that trains utterance-level and conversation-level encoders jointly.  Ghosal et al. (2019)  propose a graph neural network based model to encode speaker dependencies and temporal information.  Zhong, Wang, and Miao (2019)  incorporate external knowledge bases to support the identification.  Hazarika et al. (2019)  introduce transfer learning from utterance generation to ERC.\n\nThe modalities of data used in the above works are not the same. Specifically,  (Hazarika et al. 2018; Hazarika et al. 2018; Majumder et al. 2019 ) utilize textual, audio and video modalities, while the latest research  (Jiao et al. 2019; Ghosal et al. 2019; Zhong, Wang, and Miao 2019; Hazarika et al. 2019)  tends to use only the textual modality.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Pre-Trained Language Models",
      "text": "The effectiveness of large pre-trained language models  (Devlin et al. 2018; Yang et al. 2019; Liu et al. 2019; Conneau et al. 2020 ) has been well exhibited in many NLP tasks such as machine reading comprehension, text classification, machine translation. Among the language models, BERT  (Devlin et al. 2018 ) utilizes bi-directional Transformer encoders as well as pre-training schemes of masked language modeling and next sentence prediction. XLNet  (Yang et al. 2019 ) is another powerful pre-trained language model, which excels at processing long documents with the segment recurrence mechanism. In addition, it combines the strengths of both auto-encoding and auto-regressive language modeling. There have been some recent works that apply pre-trained language models to dialog-related tasks  (Bao et al. 2020; Ham et al. 2020; Henderson et al. 2019 ), but they have yet to be applied to emotion recognition in conversation.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Methodology",
      "text": "There are two challenges to overcome in order to apply pretrained language models to emotion recognition in conversation (ERC). The first challenge is how to encode a long historical context with hundreds of words. The second is how to model the intra-and inter-speaker dependencies of different parties. Instead of building a hierarchical network as previous work, we propose DialogXL 1  to address these two challenges on the basis of XLNet with two improvements.\n\nThe",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Problem Definition",
      "text": "In ERC, a conversation is defined as a list of utterances {u 1 , u 2 , ..., u N }, where N is the number of utterances. Each utterance u i consists of n i tokens, namely u i = {w i1 , w i2 , ..., w ini }. A discrete value y i ∈ S is used to denote the emotion label of u i , where S is the set of emotion labels. The speaker is denoted by a function p(•). For example, p(u i ) ∈ P denotes the speaker of u i and P is the collection of all speaker roles in an ERC dataset. The objective of this task is to output the emotion label y t for a given query utterance u t based on its historical context {u 1 , u 2 , ..., u t-1 } and the corresponding speaker information.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Model Input",
      "text": "At each time step t, the query utterance u t is prepended with the special token \"[CLS]\":\n\n(1)\n\nThe utterance is then passed to the embedding layer. In Di-alogXL, this layer consists of only word embedding. The output of the embedding layer is treated as input hidden states to the first Transformer layer:\n\nUtterance Recurrence\n\nXLNet  (Yang et al. 2019 ) and Transformer-XL  (Dai et al. 2019)  address the limitation of input size by a mechanism named segment recurrence, which caches previous hidden states in a memory bank and revisits them in future computations. However, this mechanism is ineffective when directly applied to conversational emotion recognition for two reasons. First, the \"segment\" in XLNet refers to a fixed-length sequence rather than a linguistic unit such as a sentence. The conversation in ERC is defined in terms of utterances, which are typically full sentences or paragraphs. Therefore, it is essential to keep the utterances complete rather than segmented into pieces. Second, segment recurrence constrains segments in the same training batch to have the same length, which results in too many paddings stored in memory. By contrast, the proposed utterance recurrence stores the historical context in memory without paddings, allowing the memory to store a longer historical context. The memory, denoted by m, works like a stack. Every time a new set of hidden states are generated for a query utterance, they are concatenated with the current memory. To prevent from introducing noises into the memory, only the hidden states of the utterance tokens are stored, with the hidden states of the \"[CLS]\" and padding positions ignored. Formally, for the t-th utterance, at each Transformer layer l the new memory m l is updated as:\n\nwhere denotes the concatenation operation. This update strategy is useful especially during the batching operation. As illustrated in Figure  2 , updating memory with only the hidden states of utterance tokens makes the memory more compact, for the noises introduced by padding are mostly eliminated and more space is freed to cache a longer context.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Dialog-Aware Self-Attention",
      "text": "Utterances occur alternately by different parties in a conversation, and the vanilla self-attention in XLNet cannot be directly applied to the multi-party setting. To this end, we replace the self-attention by dialog-aware self-attention, which enables our model to encode conversation contexts in a multi-turn multi-party setting. The new self-attention consists of four types of self-attention: global self-attention and local self-attention for different sizes of receptive fields, and speaker self-attention and listener self-attention for intra-and inter-speaker dependencies. We implement the dialog-aware self-attention by skillfully changing the masking strategies of self-attention, without the need to add any extra embeddings or parameters, as illustrated in Figure  3 .\n\nDialog-aware self-attention is multi-headed. For each attention head of the l-th Transformer layer, the attention output o l t is computed as follows:\n\n(8) where W l q , W l k , and W l v are trainable parameters for each attention head, and RelPosAttn(•) are the relative position attention adopted from Transformer-XL and XLNet.\n\nThe attention mask s in Equation (  7 ) is a matrix with the same shape as the attention weights a l t . The value of s ij is set to +∞ only when the attention between the i-th vector in q l t and j-th vector in k l t is masked, and set to 0 otherwise. For the sake of convenience, we denote Equation (4) to Equation (  8 ) by a function f (•): The utterance to be identified is u t , while the hidden states of u t-3 , u t-2 , and u t-1 are cached in memory. The speaker identities are: p(u t ) = p(u t-2 ) and p(u t-1 ) = p(u t-3 ). The window size of local self-attention is 2. The upper part of each subgraph is the attention mask for u t and other utterances, with masked attention weights colored grey. The lower part of each subgraph is the attention flows between two consecutive Transformer layers, where solid blue lines represent self-attention within u t and dashed orange lines represent attention between u t and the memory m.\n\nGlobal Self-Attention Global self-attention takes all the historical context and the query utterance as the reception field. It is the same as the vanilla self-attention, in which the query utterance pays attention to the whole context. This setting allows our model to attend to previously distant utterances which may also be useful  (Majumder et al. 2019 ). Thus, no masking is made for global self-attention:\n\nLocal Self-Attention Local self-attention only has a reception field of ω latest historical utterances, where ω is a hyperparameter. The motivation for this attention is that intuitively speaker's emotion is mostly influenced by the recent utterances. In local self-attention, we mask the attentions between the query utterance and the historical utterances outside the reception field:\n\nwhere Idx(U) is a function that maps the utterance tokens in U to the corresponding positions in the key matrix k l t . Speaker Self-Attention Speaker self-attention considers only the historical context spoken by the present speaker. It intends to model the intra-speaker dependency  (Ghosal et al. 2019 ) by identifying emotional clues in the speaker's historical utterances. In speaker self-attention, we mask the attentions between the query utterance and the utterances spoken by other speakers:\n\nListener Self-Attention Listener self-attention considers only the historical utterances spoken by other speakers. It intends to model the inter-speaker dependency  (Ghosal et al. 2019) , meaning that the present speaker's emotion may be influence by other speakers' words. In listener self-attention, we mask the attentions between the query utterance and the utterances made by the present speaker:\n\nThe outputs of the four types of self-attention are concatenated and passed through a normalization layer followed by a feed-forward network to generate the output for this Transformer layer:\n\nwhere K is the number of self-attention heads, and c k ∈ {global, local, speaker, listener} is the corresponding type of dialog-aware attention for the k-th attention head.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Model Training",
      "text": "We take the hidden state of \"[CLS]\" at the last layer as the final encoding of the query utterance and the historical context, and pass it through a feed-forward neural network to get the predicted emotion:\n\nFor the training of our model, we use the standard crossentropy loss as the loss function:\n\nwhere M is the number of conversations in the training set, and θ is the collection of trainable parameters in DialogXL.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experimental Settings",
      "text": "In this section, we present the experimental settings such as implementation details, datasets, metrics, and baselines.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Implementation Details",
      "text": "We initialize the proposed DialogXL by pre-trained XLNet-Base  (Yang et al. 2019)   The evaluation metrics are chosen as micro-F1 for DailyDialog 5  and weighted-F1 for the other datasets.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Baseline Methods",
      "text": "We compare DialogXL with the following baselines: Previous methods: CMN  (Hazarika et al. 2018) , Dia-logueRNN  (Majumder et al. 2019) , HiGRU  (Jiao et al. 2019 ), DialogueGCN(Ghosal et al. 2019) , TL-ERC  (Hazarika et al. 2019) , and KET  (Zhong, Wang, and Miao 2019) . BERT  (Devlin et al. 2018 ): The BERT baseline for ERC, initialized with the pre-trained parameters of BERT-base. We concatenate historical utterances and the query utterance in order and then feed them into BERT for classification. The hyperparameters are tuned the same as DialogXL. XLNet  (Yang et al. 2019 ): The XLNet baseline with the original segment recurrence and vanilla self-attention, initialized with the pre-trained parameters of XLNet-base. The hyperparameters are tuned the same as DialogXL.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Results And Analysis",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Overall Results",
      "text": "The overall results of our DialogXL and the baselines are reported in Table  2 . We can clearly note that DialogXL reaches a new state of the art on all of the four datasets. Besides, we can make another two observations as follows, which help to understand the ERC task and the pros and cons of DialogXL.\n\nFirst, in general, there are considerable improvements for the pre-trained language models over the others on MELD, DailyDialog, and EmoryNLP. However, the improvements of DialogXL over BERT and XLNet are not significant on these datasets. After delving into the datasets, we found that the dialogues in these datasets are relatively short (mostly 5 to 9 utterances). So the current language models, BERT and XLNet, can already encode the entire historical context and the query utterance in most cases. On these short dialogues, however, the advantages of DialogXL are not shown up completely.\n\nSecond, while inferior performance of BERT and XLNet is observed to the other baselines on IEMOCAP, the improvements of DialogXL over BERT and XLNet are significant. After examining the dataset, we realized the dialogues in IEMOCAP are much longer (around 70 utterances per dialog) than the other datasets. In this case, BERT and XL-Net cannot encode too much historical context effectively, while such baselines as DialogueRNN and DialogueGCN can reach distant utterances and also encode other key features such as speaker information. Moreover, our DialogXL can both encode the historical context effectively by the utterance recurrence and capture the speaker information by the dialog-aware self-attention, allowing it to achieve superior performance to all the baselines.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Effect Of The Enhanced Memory",
      "text": "One of the contributions of DialogXL lies in the enhanced memory with utterance recurrence. Here, we study how the Table  2 : Overall performance on the four datasets. The scores marked by \"*\" is based on our re-implementation, because of the differences in evaluation metrics and data statistics between the corresponding work and ours. utterance recurrence and the maximum memory length contribute to the final results. We change the maximum memory length from 100 to 1000 with an interval of 100 and plot the test scores on IEMOCAP, which has sufficient utterances in each conversation. The memory waste rate of segment recurrence in XLNet is also plotted in terms of the percentage of paddings in memory. Since the proposed utterance recurrence has 0 memory waste in theory, its memory waste rate is not plotted. Three models are studied for this experiment: XLNet with the original segment recurrence, XLNet with utterance recurrence, and DialogXL.\n\nThe results are shown in Figure  4 . We can note that segment recurrence always leads to a memory waste rate of over 60% for each different memory length. The rate drops with the memory length increases, along with the growth of the three models. When the memory length exceeds 700, their performance generally stops improving any more, which indicates that increasing the maximum memory length only contributes to the test results within a certain range.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Ablation Study",
      "text": "In this ablation study, we analyze the impact of dialog-aware self-attention by removing each type of dialog-aware selfattention from DialogXL. The results on two representative datasets, IEMOCAP and MELD, are presented in Table  3   We can observe that the performance of DialogXL drops on both IEMOCAP and MELD when any type of the selfattention is removed, suggesting that all these self-attentions contribute to the improvement of DialogXL. Nevertheless, their contributions can be distinguished. When speaker selfattention or listener self-attention is removed, considerable drops are observed. But when they are both removed, the drops are more obvious. This implies the importance of the inter-/intra-speaker dependency  (Ghosal et al. 2019) .\n\nMoreover, when local self-attention is removed, the F1 score drops the most on IEMOCAP, which contains long utterances (around 70) for each conversation. This indicates that the historical context near a query utterance is more important for this dataset. The drop on MELD is not as obvious as on IEMOCAP, because MELD has much shorter conversations (5 to 9 utterances per conversation). Finally, the removal of global self-attention leads to the least performance degradation. The reason could be twofold. First, global utterances are not as important as local utterances. Second, the speaker self-attention and listener self-attention already capture some useful information from distant utterances.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Speaker Role Embedding",
      "text": "Our speaker self-attention and listener self-attention model the speaker dependencies  (Ghosal et al. 2019 ) by directly letting the model know which part of the utterances should be attended to. Another way to let a pre-trained language model understand the speaker dependencies in dialog is speaker role embedding  (Bao et al. 2020; Ham et al. 2020) , which maps each participant to a trainable embedding vector. Here, we make a simple comparison between the two approaches of embedding different parties on IEMOCAP and DailyDialog. To this end, we replace the speaker selfattention and listener self-attention of DialogXL with the speaker role embeddings, and refer to the resulting model as DialogXL-emb. The results of comparison are shown in Table  4 . We can observe that our explicit speaker&listencer self-attention is more effective than the speaker role embedding approach. As a result, the proposed attention mechanism can be potentially applied to other dialog tasks as well.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Error Study",
      "text": "Although our DialogXL has a novel framework and achieves a new state of the art, we still want to figure out its possible shortcomings to motivate the future research. Therefore, we carry out an error study on IEMOCAP. In short, we found that DialogXL's powerful capability of directly capturing word-level features in the historical context can be a  Table  4 : Results of comparison between direct speaker role embedding and our speaker&listener self-attention approach on the IEMOCAP and DailyDialog datasets.\n\ndouble-edge sword. As illustrated in Figure  5 , the word-level attention mechanism based on semantic relevance can help make a good prediction (Case #1), but it may also lead to a mistake by focusing too much on the semantic relevance between the query utterance and historical utterances (Case #2). As a result, it seems to be necessary to combine with other mechanisms rather than merely relying on the popular attention to carry out the emotion recognition in dialogues. Besides, we also observe from our bad cases that some of them are mentioned in previous works, such as emotional shifts (i.e., the emotion labels of two consecutive utterances from a same speaker are different)  (Hazarika et al. 2018; Majumder et al. 2019) . Roughly, our model commits mistakes for 45% of these cases, which calls for further investigations.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we proposed an all-in-one XLNet model, namely DialogXL, for emotion recognition in conversation (ERC). To model the multi-turn multi-party conversational data, DialogXL contributes two improvements on the basis of XLNet and Transformer-XL. First, an enhanced memory was introduced to replace XLNet's vanilla memory to store historical contexts more effectively. Second, a dialogaware self-attention mechanism was proposed to deal with the multi-turn multi-party data structures. Extensive experiments were conducted on four ERC benchmarks and the results show that the proposed model outperforms all the baselines on the datasets. The effectiveness of the two improvements is also confirmed by extensive analyses. Furthermore, we have the following three findings. First, the original segment recurrence mechanism stores more than 60% paddings in memory, making it ineffective to encode the historical contexts for ERC. Second, the traditional speaker role embedding strategy is not as effective as our speaker&listener self-attention, which could also be applied to other dialog tasks. Finally, an error analysis reveals that merely relying on the attention mechanism may mislead the model.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The architecture of our DialogXL.",
      "page": 3
    },
    {
      "caption": "Figure 2: Illustration of the memory update strategies by ut-",
      "page": 3
    },
    {
      "caption": "Figure 2: , updating memory with only the",
      "page": 3
    },
    {
      "caption": "Figure 3: Dialog-aware self-attention is multi-headed. For each at-",
      "page": 3
    },
    {
      "caption": "Figure 3: Demonstration of dialog-aware self-attention: (a) global self-attention, (b) local self-attention, (c) speaker self-",
      "page": 4
    },
    {
      "caption": "Figure 4: The results of vanilla XLNet, XLNet with utter-",
      "page": 6
    },
    {
      "caption": "Figure 4: We can note that seg-",
      "page": 6
    },
    {
      "caption": "Figure 5: Results of error analysis, where two query utterances are provided, along with the visualization of attention weights",
      "page": 7
    },
    {
      "caption": "Figure 5: , the word-level",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table 2: Overall performance on the four datasets. The",
      "data": [
        {
          "Method": "",
          "F1 score": "IEMOCAP"
        },
        {
          "Method": "DialogXL\n- speaker self-attention\n- listener self-attention\n- speaker&listener self-attention\n- local self-attention\n- global self-attention",
          "F1 score": "65.94\n62.30 (↓3.64)\n62.87 (↓3.07)\n61.71 (↓4.23)\n61.66 (↓4.28)\n63.34 (↓2.60)"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "PLATO: Pre-trained Dialogue Generation Model with Discrete Latent Variable",
      "authors": [
        "S Bao",
        "H He",
        "F Wang",
        "H Wu",
        "H Wang"
      ],
      "year": "2020",
      "venue": "ACL 2020: 58th annual meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "2",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "3",
      "title": "EmotionLines: An Emotion Corpus of Multi-Party Conversations",
      "authors": [
        "S.-Y Chen",
        "C.-C Hsu",
        "C.-C Kuo",
        "; Ting-Hao",
        "; Huang",
        "L.-W Ku"
      ],
      "year": "2018",
      "venue": "11th International Conference on Language Resources and Evaluation"
    },
    {
      "citation_id": "4",
      "title": "Unsupervised Cross-lingual Representation Learning at Scale",
      "authors": [
        "A Conneau",
        "K Khandelwal",
        "N Goyal",
        "V Chaudhary",
        "G Wenzek",
        "F Guzmán",
        "E Grave",
        "M Ott",
        "L Zettlemoyer",
        "V Stoyanov"
      ],
      "year": "2020",
      "venue": "ACL 2020: 58th annual meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "5",
      "title": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context",
      "authors": [
        "Z Dai",
        "Z Yang",
        "Y Yang",
        "J Carbonell",
        "Q Le",
        "R Salakhutdinov"
      ],
      "year": "2019",
      "venue": "ACL 2019 : The 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "6",
      "title": "DialogueGCN: A Graph Convolutional Neural Network for Emotion Recognition in Conversation",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova",
        "D Ghosal",
        "N Majumder",
        "S Poria",
        "N Chhaya",
        "A Gelbukh"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "7",
      "title": "Endto-End Neural Pipeline for Goal-Oriented Dialogue Systems using GPT-2",
      "authors": [
        "D Ham",
        "J.-G Lee",
        "Y Jang",
        "K.-E Kim"
      ],
      "year": "2020",
      "venue": "ACL 2020: 58th annual meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "8",
      "title": "Icon: Interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "D Hazarika",
        "S Poria",
        "R Mihalcea",
        "E Cambria",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "9",
      "title": "Conversational Memory Network for Emotion Recognition in Dyadic Dialogue Videos",
      "authors": [
        "D Hazarika",
        "S Poria",
        "A Zadeh",
        "E Cambria",
        "L.-P Morency",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "10",
      "title": "Emotion Recognition in Conversations with Transfer Learning from Generative Conversation Modeling. arXiv: Computation and Language",
      "authors": [
        "D Hazarika",
        "S Poria",
        "R Zimmermann",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "Emotion Recognition in Conversations with Transfer Learning from Generative Conversation Modeling. arXiv: Computation and Language"
    },
    {
      "citation_id": "11",
      "title": "ConveRT: Efficient and accurate conversational representations from transformers",
      "authors": [
        "M Henderson",
        "I Casanueva",
        "N Mrkšić",
        "P.-H Su",
        "I Vulić"
      ],
      "year": "2019",
      "venue": "ConveRT: Efficient and accurate conversational representations from transformers",
      "arxiv": "arXiv:1911.03688"
    },
    {
      "citation_id": "12",
      "title": "Hi-GRU: Hierarchical Gated Recurrent Units for Utterance-Level Emotion Recognition",
      "authors": [
        "W Jiao",
        "H Yang",
        "I King",
        "M Lyu"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "13",
      "title": "DailyDialog: A Manually Labelled Multi-turn Dialogue Dataset",
      "authors": [
        "Y Li",
        "H Su",
        "X Shen",
        "W Li",
        "Z Cao",
        "S Niu"
      ],
      "year": "2017",
      "venue": "Proceedings of the Eighth International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "14",
      "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal",
        "J Du",
        "M Joshi",
        "D Chen",
        "O Levy",
        "M Lewis",
        "L Zettlemoyer",
        "V Stoyanov"
      ],
      "year": "2019",
      "venue": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "15",
      "title": "Fixing Weight Decay Regularization in Adam",
      "authors": [
        "I Loshchilov",
        "F Hutter"
      ],
      "year": "2018",
      "venue": "Fixing Weight Decay Regularization in Adam"
    },
    {
      "citation_id": "16",
      "title": "Mem2seq: Effectively incorporating knowledge bases into endto-end task-oriented dialog systems",
      "authors": [
        "A Madotto",
        "C.-S Wu",
        "P Fung"
      ],
      "year": "2018",
      "venue": "Mem2seq: Effectively incorporating knowledge bases into endto-end task-oriented dialog systems",
      "arxiv": "arXiv:1804.08217"
    },
    {
      "citation_id": "17",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "N Majumder",
        "S Poria",
        "D Hazarika",
        "R Mihalcea",
        "A Gelbukh",
        "E Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "18",
      "title": "MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "R Mihalcea",
        "G Naik",
        "E Cambria"
      ],
      "year": "2019",
      "venue": "ACL 2019 : The 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "19",
      "title": "Emotion recognition in conversation: Research challenges, datasets, and recent advances",
      "authors": [
        "S Poria",
        "N Majumder",
        "R Mihalcea",
        "E Hovy"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "20",
      "title": "AVEC 2012: the continuous audio/visual emotion challenge",
      "authors": [
        "B Schuller",
        "M Valster",
        "F Eyben",
        "R Cowie",
        "M Pantic"
      ],
      "year": "2012",
      "venue": "Proceedings of the 14th ACM international conference on Multimodal interaction"
    },
    {
      "citation_id": "21",
      "title": "Attention is All You Need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Proceedings of the 31st International Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "22",
      "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
      "authors": [
        "Z Yang",
        "Z Dai",
        "Y Yang",
        "J Carbonell",
        "R Salakhutdinov",
        "Q Le"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "23",
      "title": "Emotion Detection on TV Show Transcripts with Sequence-based Convolutional Neural Networks",
      "authors": [
        "S Zahiri",
        "J Choi"
      ],
      "year": "2017",
      "venue": "AAAI Workshops"
    },
    {
      "citation_id": "24",
      "title": "Knowledge-Enriched Transformer for Emotion Detection in Textual Conversations",
      "authors": [
        "P Zhong",
        "D Wang",
        "C Miao"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "25",
      "title": "Emotional chatting machine: Emotional conversation generation with internal and external memory",
      "authors": [
        "H Zhou",
        "M Huang",
        "T Zhang",
        "X Zhu",
        "B Liu"
      ],
      "year": "2017",
      "venue": "Emotional chatting machine: Emotional conversation generation with internal and external memory",
      "arxiv": "arXiv:1704.01074"
    }
  ]
}