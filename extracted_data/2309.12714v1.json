{
  "paper_id": "2309.12714v1",
  "title": "Unsupervised Representations Improve Supervised Learning In Speech Emotion Recognition",
  "published": "2023-09-22T08:54:06Z",
  "authors": [
    "Amirali Soltani Tehrani",
    "Niloufar Faridani",
    "Ramin Toosi"
  ],
  "keywords": [
    "Speech Emotion Recognition",
    "Self-supervised Learning",
    "Convolutional Neural Network"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech Emotion Recognition (SER) plays a pivotal role in enhancing human-computer interaction by enabling a deeper understanding of emotional states across a wide range of applications, contributing to more empathetic and effective communication. This study proposes an innovative approach that integrates self-supervised feature extraction with supervised classification for emotion recognition from small audio segments. In the preprocessing step, to eliminate the need of crafting audio features, we employed a self-supervised feature extractor, based on the Wav2Vec model, to capture acoustic features from audio data. Then, the output feature-maps of the preprocessing step are fed to a custom designed Convolutional Neural Network (CNN)-based model to perform emotion classification. Utilizing the ShEMO dataset as our testing ground, the proposed method surpasses two baseline methods, i.e. support vector machine classifier and transfer learning of a pretrained CNN. comparing the propose method to the state-of-the-art methods in SER task indicates the superiority of the proposed method. Our findings underscore the pivotal role of deep unsupervised feature learning in elevating the landscape of SER, offering enhanced emotional comprehension in the realm of human-computer interactions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Communication establishes the foundation for strong connections between individuals. A community where people feel heard, appreciated, and supported is highly influenced by thoughtful speaking and listening, as well as by taking into account how our words affect others. Speech is one of the most important means through which people communicate with their surroundings. Speaking remains the most natural, prevalent, and effective method of human engagement, despite the fact that modern communication increasingly takes place via keyboards and displays  [1] . When we communicate, using the right emotions may affect how people hear and understand what we are saying while also demonstrating our interest, commitment, and sincerity. This makes our genuine intentions clear and makes it possible for receivers to react in a suitable way. Designing technologies for human usage seems to benefit from using emotion detection  [34]  to improve interaction given that human-computer interaction is being used in a variety of applications  [35] . Therefore, a technique and tool for extracting and identifying emotions in speech would be useful. † These authors contributed equally to this work Speech emotion recognition (SER) seeks to automatically identify an individual's emotional or physical state from their voice. Despite not altering linguistic content, emotional state has a big impact on communication since it gives feedback in different situations  [2] . The growing number of fields that have profited from SER research, including as contact centers, video games, and lie detection, has increased interest in this area  [3] . SER can be used in interactive online learning as well. Computer systems can adaptively choose the most effective methods for presenting the remaining course material by detecting a student's emotional state  [8] . In therapeutic settings, SER enables professionals to better understand patients' mental states and perhaps uncover hidden emotions  [9] . Virtual voice assistants like Siri, Alexa, Cortana, and Google Assistant have proliferated in popularity as human interaction interfaces in recent years  [19] . Systems run the danger of being viewed as cold, socially awkward, unreliable, and incompetent if they are unable to recognize or effectively respond to a user's emotional condition  [20] . SER is therefore becoming a more crucial capability  [19] . To represent emotional cues in voice signals, the SER approach primarily uses feature extraction, which is followed by feature classification to categorize emotions  [4] . The first stage of SER is feature extraction, which entails finding emotional cue representations in speech signals. A major problem in SER continues to be identifying the best features to achieve improved performance  [15] . Certain aspects must successfully communicate essential emotional information. Source-based excitation features, prosodic features, vocal tract factors, and hybrid feature sets are just a few of the feature types that researchers have developed for speech processing  [5] .Speech contains linguistic and auditory information that conveys emotion. Previous research produced embeddings for challenges involving spoken language using methods such as phone representations  [21]  and Mel-frequency approaches  [22] . Energy-related features, pitch frequency features, formant frequencies  [26] ,  [27] , zero crossing rate (ZCR), linear prediction coefficients (LPC), linear prediction cepstral features, mel-frequency cepstrum coefficients (MFCCs) and its first derivative  [28] , RASTA-PLP features  [29] , and others have all been studied to improve emotion classification.Pitch, intensity, and spectral characteristics were used as audio features in  [18]  to extract emotion data. For deep learning challenges, new feature learning methods have recently emerged. Given the lack of labeled data for the majority of the 7,000 languages spoken worldwide, many modern language models use selfsupervised rather than supervised learning. For the most part, there aren't many hours worth of annotated voice data. As a result, self-supervised approaches facilitate cross-lingual transfer and language learning for low-resource languages  [14] ,  [16] . In order to improve voice recognition, unsupervised pre-training techniques like Wav2Vec have been developed  [14] . The 1,000 hours that are typically found in supervised academic datasets have been significantly surpassed by these strategies, which have scaled quickly to 1,000,000 hours of training data  [17] . Because models automatically learn from unlabeled voices, this extensive scale is possible. Multilingual transcription systems and other speech communities have long been interested in cross-lingual learning, which tries to create models that use data from different languages to improve performance  [12] ,  [13] .unsupervised pre-training methods have advanced stateof-the-art performance when fine-tuned on benchmark tasks, especially in low-resource environments  [14] .\n\nFeature classification using both linear and nonlinear classifiers is applied in the second stage of SER systems  [6] . Significant interest in voice recognition has been generated by a proposal to improve discrete language models using neural networks  [7] . Speech emotion identification has made revolutionary advances in machine learning recently because of Deep Neural Networks (DNNs). Convolutional Neural Networks (CNNs) use frame-level features to identify patterns and local context. CNNs enable integrated feature representation and classification to be trained end-to-end in one optimization process. CNNs and long short-term memory (LSTM) networks are two common DNN architectures that recently emerged for voice emotion recognition  [23] . A three-layer LSTM + DNN model for emotional computing was proposed in an early, significant study and trained on the functionals of LLDs  [24] . Stuhlsatz et al.  [25]  used generalized discriminant analysis (GerDA) and restricted Boltzmann machines (RBMs) to directly extract discriminative features from raw data. in  [10] , After a deep neural network feature extractor, a support vector machine (SVM) was used as the final classifier for emotion recognition. Across multiple acoustic feature groups, this architecture intended to learn relevant representations. The proposed model's assertion 64% accuracy demonstrated better recognition performance. In contrast to conventional methods that rely on speech segmentation, the fully CNN with attention mechanisms presented in  [30]  can process variable-length speech directly. The accuracy of this design in recognizing emotions was 70.4%. To overcome restrictions caused by insufficient training data, the authors investigated transfer learning. Speech analysis should use careful attention mechanisms since silent speech contains prosodic clues and tonal dynamics. On the ShEMO dataset, a CNN + BLSTM architecture acting on low-level acoustic descriptors extracted from voice frames obtained 78.29% accuracy  [11] .\n\nIn  [19] , transfer learning for SER is investigated by using representations from a pre-trained wav2vec 2.0 model and basic neural networks. To combine outputs across layers of the pre-trained model, the authors provide trainable weights that are learned along with the downstream model. Using the IEMOCAP and RAVDESS emotion datasets, the performance of wav2vec 2.0 variations with and without speech recognition fine-tuning is examined\n\nIn this study, we extract acoustic features from audio data using a wav2vec feature extractor. Then, a novel audio classification model inspired by CNN is fed these features. The ShEMO emotion recognition dataset serves as the training and evaluating on the ShEMO On this dataset, our proposed method performs well while requiring little to no preprocessing of the raw audio.The adaption of CNN architectures for feature learning from auditory inputs is a significant contribution of our work. Our results show that this CNN-based model outperforms baseline approaches and is successful at extracting emotional cues from speech. In addition, our model offers a generalizable method for speech emotion recognition by relying on deep feature learning as opposed to hand-crafted audio features.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Proposed Method",
      "text": "The three main steps of our suggested methodology are preprocessing, feature extraction, and classification. To prepare the raw audio data, the preprocessing phase first applies initial transformations. Then, a feature extractor identifies representative cues that encode emotional data. In order to predict the target emotions, classification models subsequently examine the auditory representations. Three different classifier architectures are evaluated. The data can be preprocessed before being fed into the classifiers for emotion recognition using this staged pipeline. Support vector machines (SVMs)  [45] , transfer learning using prominent pre-trained models, and customized deep neural networks are all used in our categorization experiments. The pipeline is shown in Fig.  1 . We wanted to compare performance and accuracy on the voice emotion recognition task by evaluating various classifier architectures. The main goal was to categorize the emotional state that was exhibited in each input audio clip from our dataset.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Preprocessing",
      "text": "Through zero-padding, our preprocessing standardized all audio clips to 7 seconds. The underrepresented 'fear' class was excluded. All audios are resampled to 16KHz. XLSR-53  [46] , a self-supervised multi-lingual speech representation model that has been pre-trained on 53 languages, was used to extract features. We considered the output of 24-layer feature map with 349 x 1024 feature arrays produced by XLSR-53. A sample first layer output for \"sadness\" is shown in Fig.  2 . We used the mean feature vector from the first layer for the SVM classifier. We Utilized the full 349 x 1024 first layer feature map from XLSR-53 for deep neural network classifiers. CNNs received this altered data as an input in the form of a 300 x 300 2D representation. With the help of data preprocessing and the extraction of informative voice features, we improved  The preprocessing made it possible to use deep neural networks, which typically anticipate 2D inputs, for the task of recognizing speech emotions. Specifically, the audio data was converted into a format that was compatible with CNNs by reshaping the 349x1024 XLSR feature array into a 300x300 2D representation. These models could be used by matching the input structure to anticipated CNN architectures. Modern deep learning algorithms can be used to solve our audio emotion classification problem by first converting the audio to a matrix-shape format. The audio input was transformed by our preprocessing pipeline into a format that CNNs can use effectively.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Classification",
      "text": "Reshaping the audio features into a 300x300 matrix representation enabled leveraging deep learning techniques designed for image inputs. CNNs are well-suited for this 2D structured data, as they employ 2D convolutions to extract spatial patterns and relationships. Moreover, transfer learning can be utilized by fine-tuning models pre-trained on image datasets. Additionally, we establish baseline performance by using SVMs on the mean audio features from the first XLSR layer, as a baseline method. Although deep learning algorithms are not anticipated to match the SVM, it offers a good linear benchmark for comparison. Hierarchical feature learning in CNN is expected to outperform the SVM model. However, the SVM offers a fundamental baseline for quantifying gains made by incorporating deep networks into our audio emotion recognition problem.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "1) Support Vector Machines (Svm):",
      "text": "As a baseline model, we utilized a Support Vector Machine (SVM) classifier. The input representation comprised the mean feature vector from the first layer of the XLSR model, summarizing information across time similarly to the original audio We standardized the data to have zero mean, then applied a radial basis function (RBF) kernel to map the data into a higher dimensional space where a linear decision boundary could separate it. The RBF kernel allowed the SVM model to handle any nonlinear relationships between the audio features and emotion labels. By comparing deep learning approaches to this nonlinear SVM baseline, we aimed to demonstrate the benefits of learned hierarchical representations. The SVM provides a reasonable benchmark for accuracy from a non-deep model applied directly to the raw audio features.  3) Proposed CNN Model: Finally, we propose a customdesigned CNN architecture specifically created for speech emotion classification, achieving improved performance. The model comprises seven 2D convolutional layers with progressively decreasing filter sizes, enabling the network to learn higher-level feature representations from the input spectrograms in a hierarchical manner. Batch normalization was used to increase training stability after each convolutional layer. In order to downsample the feature maps and uncover strong spatial patterns, max-pooling layers were carefully placed. Following layer pooling, dropout, and regularization were also used to reduce overfitting on the training set of data. Before a softmax classification layer, the final convolution layer outputs were flattened and routed through fully connected dense layers to compress the features shown in TABLEI. Our CNN model can learn a deep hierarchy of emotionally significant audio elements by increasing the filter depths and modifying the overall architecture. Our particular design seeks to achieve greater accuracy compared to traditional CNNs and transfer learning models by adjusting the network topology and hyperparameters to the specifics of our innovative preprocessed spectrogram input representation. In addition to the SVM baseline, the customized model offers a powerful deep learning strategy for generalizing across various human emotions inside the audio dataset. Fig.  3  depicts the suggested CNN model architecture.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iii. Experiment And Result",
      "text": "A. Experimental Setup 1) Dataset: The ShEMO dataset  [31]  is made up of dialogues from 87 individuals that were taken from the radio and spans a duration of around 3 hours and 25 minutes. Of these speakers, 31 are women and 56 are males. This dataset has been made of 3000 utterances in the mono channel which accurance of each emotion is in TABLEII, and at the frequency of 44.1 kHz. We employed the following five emotions in this work: surprise, happiness, sadness, neutral, and anger. We also excluded waveforms with fear labels since there were only 38 of them (1.26 % of the total). Additionally, audio waves longer than 7 seconds were clipped to this duration, while those shorter than 7 seconds were zero-padded to provide uniform sequence lengths, while a sample of each emotion for 3 seconds are illustrated in Fig.  4 . 2) Parameters: In the SVM experiment, we employed the RBF kernel  [40]  and the gamma was set to auto. For optimization, we employed stochastic gradient descent (SGD)  [37]  to train the EfficientNet-B3 network  [41]  and Adam  [38]  with a fixed learning rate of 10-3 and scheduler in our proposed model experiments. A batch size of 4 was used during training. Model evaluation was performed on full audio recordings using wav2vec representations as input, without relying on segmentation. Additionally, regularization was employed to avoid overfitting and dropout  [39] -which was applied with probabilities of 0.1, 0.2, 0.3, 0.4, and 0.6-was used after the max-pooling layers. Finally, for the loss function, we used categorical cross-entropy due to the labels being tagged first.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Results",
      "text": "The transfer learning loss and accuracy curves are shown in Fig.  5 . Training loss decreased as the model fit improved over epochs. However, validation loss remained nearly constant before slightly increasing after 100 epochs, indicating potential overfitting. Accuracy followed comparable trends, with training accuracy quickly reaching 77.8% then plateauing as validation fluctuated without notable gains. The curves suggest modifications are needed to further enhance performance.\n\nAs shown in Fig.  6 , the CNN model exhibited decreasing training and validation loss over epochs, indicating successful learning from the data. The validation loss remained lower than training loss throughout training, suggesting effective generalization to new data. Both training and validation accuracy steadily increased over epochs. In later epochs, validation accuracy surpassed 80%, outperforming SVM and transfer learning models. These curves demonstrate our proposed architecture achieved higher precision and lower loss on training and validation sets compared to prior benchmarks. Moreover, superior validation over training performance highlights model generalizability.\n\nWe compare our results to prior work by Keesing et al.  [42] , which utilized wav2vec features on the ShEMO dataset. Our hypotheses are that expanding model capacity and depth can improve performance. As shown in TABLEIII, our proposed model achieves superior accuracy on the ShEMO validation and test sets compared to  [42] . Specifically, our test set recall improves by an absolute value of approximately 13%.\n\nWe also compare against prior work utilizing hand-crafted features on ShEMO, summarized in Table  2 . Notably, Yazdani et al.  [44]  achieved state-of-the-art accuracy using this approach. Our model surpasses their accuracy by 3.41%. Critically, unlike Yazdani et al.'s reliance on manually engineered features like MFCCs, our model utilizes an end-toend architecture, operating directly on raw waveform inputs independent of temporal or spectral characteristics.    [44]  presents the most recent benchmark on the full ShEMO dataset, achieving state-of-the-art results. Their reported test set weighted average recall is 0.862, just 0.8% absolutely lower than our model utilizing wav2vec features. For completeness, their F1 score is 0.860 compared to 0.870 for our proposed method, a 0.01 absolute improvement. By incorporating wav2vec representations into a deeper CNN, our approach advances the state-of-the-art in this dimension.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Iv. Conclusion",
      "text": "In this study, we introduced a novel approach to SER by combining self-supervised feature extraction with a CNN architecture. Firstly, by harnessing wav2vec feature extraction and a costume designed CNN-based model, we achieved substantial improvements over prior SER methods. Notably, our proposed approach outperformed the SVM-based model and transfer learning methods, showcasing superior generalization capabilities. Comparing our model to prior work, we observed substantial performance enhancements. In particular, we surpassed the accuracy achieved by models relying on handcrafted features, such as MFCCs, underscoring the potential of end-to-end architectures that operate directly on raw waveform inputs. Furthermore, our approach demonstrated competitive results when benchmarked against the most recent state-of-theart methods. Our findings contribute to the evolving landscape of SER, emphasizing the importance of data-driven approaches in advancing the understanding of emotional cues in speech.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Structure of the model for SER paradigm",
      "page": 3
    },
    {
      "caption": "Figure 2: Output of preprocessing step",
      "page": 3
    },
    {
      "caption": "Figure 3: Proposed CNN Model Architecture",
      "page": 5
    },
    {
      "caption": "Figure 4: Random waveforms of six emotions from ShEMO",
      "page": 5
    },
    {
      "caption": "Figure 5: Loss and accuracy of EfficientNet model",
      "page": 5
    },
    {
      "caption": "Figure 6: Loss and accuracy of proposed model",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table 2: Notably, Yaz-",
      "data": [
        {
          "layer name": "conv1",
          "output size": "300 x 300 x 64",
          "Proposed Model": "3 x 3, 64, stride 1"
        },
        {
          "layer name": "conv2\nx",
          "output size": "150 x 150 x 128",
          "Proposed Model": "3 x 3 max pool, stride 1\n[3 x 3, 128] x 2\n[3 x 3, 128] x 2"
        },
        {
          "layer name": "conv3\nx",
          "output size": "75 x 75 x 256",
          "Proposed Model": "[3 x 3, 256] x 2\n[3 x 3, 256] x 2"
        },
        {
          "layer name": "conv4\nx",
          "output size": "37 x 37 x 512",
          "Proposed Model": "[3 x 3, 512] x 2\n[3 x 3, 512] x 2"
        },
        {
          "layer name": "conv5\nx",
          "output size": "18 x 18 x 512",
          "Proposed Model": "[3 x 3, 512] x 2"
        },
        {
          "layer name": "",
          "output size": "1 x 1 x 1024",
          "Proposed Model": "average pool"
        },
        {
          "layer name": "Fully Connected",
          "output size": "64",
          "Proposed Model": "64 x 1024"
        },
        {
          "layer name": "softmax",
          "output size": "5",
          "Proposed Model": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Accuracy": "-",
          "F1 score": "-",
          "Recall": "0.640"
        },
        {
          "Accuracy": "78.29%",
          "F1 score": "-",
          "Recall": "-"
        },
        {
          "Accuracy": "-",
          "F1 score": "0.860",
          "Recall": "0.862"
        },
        {
          "Accuracy": "76%",
          "F1 score": "0.73",
          "Recall": "0.76"
        },
        {
          "Accuracy": "78.7%",
          "F1 score": "0.75",
          "Recall": "0.77"
        },
        {
          "Accuracy": "81.7%",
          "F1 score": "0.87",
          "Recall": "0.870"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Automatic speech recognition: a review",
      "authors": [
        "S Arora",
        "R Singh"
      ],
      "year": "2012",
      "venue": "International Journal of Computer Applications"
    },
    {
      "citation_id": "2",
      "title": "Emotional speech recognition: Resources, features, and methods",
      "authors": [
        "D Ververidis",
        "C Kotropoulos"
      ],
      "year": "2006",
      "venue": "Speech communication"
    },
    {
      "citation_id": "3",
      "title": "Deep learning for emotional speech recognition",
      "authors": [
        "M Sánchez-Gutiérrez"
      ],
      "year": "2014",
      "venue": "Pattern Recognition: 6th Mexican Conference"
    },
    {
      "citation_id": "4",
      "title": "Emotion recognition from speech: a review",
      "authors": [
        "G Shashidhar",
        "K Koolagudi",
        "R Sreenivasa"
      ],
      "year": "2012",
      "venue": "Springer Science+ Business Media"
    },
    {
      "citation_id": "5",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "M El Ayadi",
        "M Kamel",
        "F Karray"
      ],
      "year": "2011",
      "venue": "Pattern recognition"
    },
    {
      "citation_id": "6",
      "title": "Speech emotion recognition using deep learning techniques: A review",
      "authors": [
        "R Khalil"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "7",
      "title": "Deep learning: methods and applications",
      "authors": [
        "L Deng",
        "D Yu"
      ],
      "year": "2014",
      "venue": "Foundations and trends in signal processing"
    },
    {
      "citation_id": "8",
      "title": "Speech emotion recognition combining acoustic features and linguistic information in a hybrid support vector machine-belief network architecture",
      "authors": [
        "B Schuller",
        "G Rigoll",
        "M Lang"
      ],
      "year": "2004",
      "venue": "IEEE international conference on acoustics, speech, and signal processing"
    },
    {
      "citation_id": "9",
      "title": "Acoustical properties of speech as indicators of depression and suicidal risk",
      "authors": [
        "D France"
      ],
      "year": "2000",
      "venue": "IEEE transactions on Biomedical Engineering"
    },
    {
      "citation_id": "10",
      "title": "Speech emotion recognition with heterogeneous feature unification of deep neural network",
      "authors": [
        "W Jiang"
      ],
      "year": "2019",
      "venue": "Sensors"
    },
    {
      "citation_id": "11",
      "title": "Emotion recognition in persian speech using deep neural networks",
      "authors": [
        "A Yazdani",
        "H Simchi",
        "Y Shekofteh"
      ],
      "year": "2021",
      "venue": "11th International Conference on Computer Engineering and Knowledge (ICCKE)"
    },
    {
      "citation_id": "12",
      "title": "Multilingual acoustic modeling for speech recognition based on subspace Gaussian mixture models",
      "authors": [
        "L Burget",
        "P Schwarz",
        "M Agarwal",
        "P Akyazi",
        "K Feng",
        "A Ghoshal",
        ". Thomas"
      ],
      "year": "2010",
      "venue": "2010 IEEE international conference on acoustics, speech and signal processing"
    },
    {
      "citation_id": "13",
      "title": "Unsupervised cross-lingual representation learning for speech recognition",
      "authors": [
        "A Conneau",
        "A Baevski",
        "R Collobert",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Unsupervised cross-lingual representation learning for speech recognition",
      "arxiv": "arXiv:2006.13979"
    },
    {
      "citation_id": "14",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "15",
      "title": "High-level feature representation using recurrent neural network for speech emotion recognition",
      "authors": [
        "J Lee",
        "I Tashev"
      ],
      "year": "2015",
      "venue": "High-level feature representation using recurrent neural network for speech emotion recognition"
    },
    {
      "citation_id": "16",
      "title": "Ethnologue: Languages of the world",
      "authors": [
        "M Lewis",
        "G Simon",
        "C Fennig"
      ],
      "year": "2016",
      "venue": "Ethnologue: Languages of the world"
    },
    {
      "citation_id": "17",
      "title": "Bigssl: Exploring the frontier of large-scale semi-supervised learning for automatic speech recognition",
      "authors": [
        "Y Zhang",
        "D Park",
        "W Han",
        "J Qin",
        "A Gulati",
        "J Shor",
        ". Wu"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "18",
      "title": "Acoustic profiles in vocal emotion expression",
      "authors": [
        "R Banse",
        "K Scherer"
      ],
      "year": "1996",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "19",
      "title": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "authors": [
        "L Pepino",
        "P Riera",
        "L Ferrer"
      ],
      "year": "2021",
      "venue": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "arxiv": "arXiv:2104.03502"
    },
    {
      "citation_id": "20",
      "title": "Emotion in human-computer interaction",
      "authors": [
        "S Brave",
        "C Nass"
      ],
      "year": "2007",
      "venue": "The human-computer interaction handbook"
    },
    {
      "citation_id": "21",
      "title": "Universal phone recognition with a multilingual allophone system",
      "authors": [
        "X Li",
        "S Dalmia",
        "J Li",
        "M Lee",
        "P Littell",
        "J Yao",
        ". Metze"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "22",
      "title": "librosa: Audio and music signal analysis in python",
      "authors": [
        "B Mcfee",
        "C Raffel",
        "D Liang",
        "D Ellis",
        "M Mcvicar",
        "E Battenberg",
        "O Nieto"
      ],
      "year": "2015",
      "venue": "Proceedings of the 14th python in science conference"
    },
    {
      "citation_id": "23",
      "title": "End-to-end speech emotion recognition using deep neural networks",
      "authors": [
        "P Tzirakis",
        "J Zhang",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "2018 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "24",
      "title": "Abandoning emotion classes-towards continuous emotion recognition with modelling of long-range dependencies",
      "authors": [
        "M Wöllmer",
        "F Eyben",
        "S Reiter",
        "B Schuller",
        "C Cox",
        "E Douglas-Cowie",
        "R Cowie"
      ],
      "year": "2008",
      "venue": "Abandoning emotion classes-towards continuous emotion recognition with modelling of long-range dependencies"
    },
    {
      "citation_id": "25",
      "title": "Deep neural networks for acoustic emotion recognition: Raising the benchmarks",
      "authors": [
        "A Stuhlsatz",
        "C Meyer",
        "F Eyben",
        "T Zielke",
        "G Meier",
        "B Schuller"
      ],
      "year": "2011",
      "venue": "2011 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "26",
      "title": "Automatic emotional speech classification",
      "authors": [
        "D Ververidis",
        "C Kotropoulos",
        "I Pitas"
      ],
      "year": "2004",
      "venue": "2004 IEEE international conference on acoustics, speech, and signal processing"
    },
    {
      "citation_id": "27",
      "title": "Features extraction and selection for emotional speech classification",
      "authors": [
        "Z Xiao",
        "E Dellandrea",
        "W Dou",
        "L Chen"
      ],
      "year": "2005",
      "venue": "IEEE Conference on Advanced Video and Signal Based Surveillance"
    },
    {
      "citation_id": "28",
      "title": "Feature extraction and selection in speech emotion recognition",
      "authors": [
        "Y Pan",
        "P Shen",
        "L Shen"
      ],
      "year": "2005",
      "venue": "IEEE Conference on Advanced Video and Signal Based Surveillance (AVSS 2005)"
    },
    {
      "citation_id": "29",
      "title": "Relative spectralperceptual linear prediction (RASTA-PLP) speech signals analysis using singular value decomposition (SVD)",
      "authors": [
        "M Zulkifly",
        "N Yahya"
      ],
      "year": "2017",
      "venue": "2017 IEEE 3rd International Symposium in Robotics and Manufacturing Automation (ROMA)"
    },
    {
      "citation_id": "30",
      "title": "Attention based fully convolutional network for speech emotion recognition",
      "authors": [
        "Y Zhang"
      ],
      "year": "2018",
      "venue": "Asia-Pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "31",
      "title": "ShEMO: a large-scale validated database for Persian speech emotion detection",
      "authors": [
        "Mohamad Nezami",
        "P Lou",
        "M Karami"
      ],
      "year": "2019",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "32",
      "title": "An introduction to convolutional neural networks",
      "authors": [
        "K O'shea",
        "R Nash"
      ],
      "year": "2015",
      "venue": "An introduction to convolutional neural networks",
      "arxiv": "arXiv:1511.08458"
    },
    {
      "citation_id": "33",
      "title": "",
      "authors": [
        "Elgeish"
      ],
      "year": "2020",
      "venue": ""
    },
    {
      "citation_id": "34",
      "title": "Emotion in human-computer interaction. Expanding the frontiers of visual analytics and visualization",
      "authors": [
        "C Peter",
        "B Urban"
      ],
      "year": "2012",
      "venue": "Emotion in human-computer interaction. Expanding the frontiers of visual analytics and visualization"
    },
    {
      "citation_id": "35",
      "title": "A brief history of human-computer interaction technology",
      "authors": [
        "B Myers"
      ],
      "year": "1998",
      "venue": "interactions"
    },
    {
      "citation_id": "36",
      "title": "",
      "authors": [
        "A Babu",
        "C Wang",
        "A Tjandra",
        "K Lakhotia",
        "Q Xu",
        "N Goyal"
      ],
      "venue": ""
    },
    {
      "citation_id": "37",
      "title": "XLS-R: Self-supervised cross-lingual speech representation learning at scale",
      "authors": [
        "M Auli"
      ],
      "year": "2021",
      "venue": "XLS-R: Self-supervised cross-lingual speech representation learning at scale",
      "arxiv": "arXiv:2111.09296"
    },
    {
      "citation_id": "38",
      "title": "An overview of gradient descent optimization algorithms",
      "authors": [
        "S Ruder"
      ],
      "year": "2016",
      "venue": "An overview of gradient descent optimization algorithms",
      "arxiv": "arXiv:1609.04747"
    },
    {
      "citation_id": "39",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "40",
      "title": "Dropout: a simple way to prevent neural networks from overfitting",
      "authors": [
        "N Srivastava",
        "G Hinton",
        "A Krizhevsky",
        "I Sutskever",
        "R Salakhutdinov"
      ],
      "year": "2014",
      "venue": "The journal of machine learning research"
    },
    {
      "citation_id": "41",
      "title": "SVM kernel functions for classification",
      "authors": [
        "A Patle",
        "D Chouhan"
      ],
      "year": "2013",
      "venue": "2013 International conference on advances in technology and engineering (ICATE)"
    },
    {
      "citation_id": "42",
      "title": "Efficientnet: Rethinking model scaling for convolutional neural networks",
      "authors": [
        "M Tan",
        "Q Le"
      ],
      "year": "2019",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "43",
      "title": "Acoustic Features and Neural Representations for Categorical Emotion Recognition from Speech",
      "authors": [
        "A Keesing",
        "Y Koh",
        "M Witbrock"
      ],
      "year": "2021",
      "venue": "Interspeech"
    },
    {
      "citation_id": "44",
      "title": "Emotions Understanding Model from Spoken Language using Deep Neural Networks and Mel-Frequency Cepstral Coefficients",
      "authors": [
        "M De Pinto",
        "M Polignano",
        "P Lops",
        "G Semeraro"
      ],
      "year": "2020",
      "venue": "2020 IEEE Conference on Evolving and Adaptive Intelligent Systems (EAIS)"
    },
    {
      "citation_id": "45",
      "title": "Multi-lingual multi-task speech emotion recognition using wav2vec 2.0. ICASSP 2022",
      "authors": [
        "M Sharma"
      ],
      "year": "2022",
      "venue": "2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "46",
      "title": "Support-vector networks",
      "authors": [
        "C Cortes",
        "V Vapnik"
      ],
      "year": "1995",
      "venue": "Machine learning"
    },
    {
      "citation_id": "47",
      "title": "",
      "authors": [
        "A Babu",
        "C Wang",
        "A Tjandra",
        "K Lakhotia",
        "Q Xu",
        "N Goyal"
      ],
      "venue": ""
    },
    {
      "citation_id": "48",
      "title": "XLS-R: Self-supervised cross-lingual speech representation learning at scale",
      "authors": [
        "M Auli"
      ],
      "year": "2021",
      "venue": "XLS-R: Self-supervised cross-lingual speech representation learning at scale",
      "arxiv": "arXiv:2111.09296"
    }
  ]
}