{
  "paper_id": "2404.16905v1",
  "title": "Samsung Research China-Beijing At Semeval-2024 Task 3: A Multi-Stage Framework For Emotion-Cause Pair Extraction In Conversations",
  "published": "2024-04-25T11:52:21Z",
  "authors": [
    "Shen Zhang",
    "Haojie Zhang",
    "Jing Zhang",
    "Xudong Zhang",
    "Yimeng Zhuang",
    "Jinting Wu"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In human-computer interaction, it is crucial for agents to respond to human by understanding their emotions. Unraveling the causes of emotions is more challenging. A new task named Multimodal Emotion-Cause Pair Extraction in Conversations is responsible for recognizing emotion and identifying causal expressions. In this study, we propose a multi-stage framework to generate emotion and extract the emotion causal pairs given the target emotion. In the first stage, Llama-2-based InstructERC is utilized to extract the emotion category of each utterance in a conversation. After emotion recognition, a two-stream attention model is employed to extract the emotion causal pairs given the target emotion for subtask 2 while MuTEC is employed to extract causal span for subtask 1. Our approach achieved first place for both of the two subtasks in the competition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Comprehending emotions plays a vital role in developing artificial intelligence with human-like capabilities, as emotions are inherent to humans and exert a substantial impact on our thinking, choices, and social engagements  (Wang et al., 2023b) . Dialogues, being a fundamental mode of human communication, abound with a variety of emotions  (C. et al., 2008; Poria et al., 2019; Zahiri and Choi, 2017; Li et al., 2017; Xia and Ding, 2019; Ding et al., 2020; Wei et al., 2020; Fan et al., 2020) . Going beyond simple emotion identification, unraveling the underlying catalysts of these emotions within conversations represents a more complex and less-explored challenge  (Wang et al., 2023b) . Hence,  (Wang et al., 2023a (Wang et al., , 2024 ) introduces a ⋆: equal contributions. : Corresponding Author.  Shen  Zhang is in charge of the basic subtask-emotion recognition in conversation (ERC) and Haojie Zhang is responsible for the pipeline framework and causal pair extraction and causal span extraction subtasks. novel undertaking known as Recognizing Emotion Cause in Emotion-Cause-in-Friends (ECF). ECF contains 1,344 conversations and 13,509 utterances where 9,272 emotion-cause pairs are annotated, covering textual, visual, and acoustic modalities. All utterances are annotated by one of the seven emotion labels, which are neutral, surprise, fear, sadness, joy, disgust, and anger. Within ECF, a significant task is identified as Emotion-Cause Pair Extraction in Conversations (ECPEC). ECPEC is responsible for identifying causal expressions related to a specific utterance in conversations where the emotion is implicitly expressed. ECPEC provides two Multimodal Emotion Cause Analysis in Conversations (ECAC) subtasks:\n\n• Subtask 1: Textual Emotion-Cause Pair Extraction in Conversations. Given a conversation containing the speaker and the text of each utterance U = [U 1 , U 2 , ...U n ], the model is aim to predict emotion-cause pairs, which include emotion utterance's emotion category and the textual cause span in a specific cause utterance (e.g. U3_joy, U2_\"You made up!\").\n\n• Subtask 2: Multimodal Emotion Cause Analysis in Conversations. Given a conversation including the speaker, text and audio-visual clip for each utterance, the model is aim to predict emotion-cause pairs, which include emotion category and a cause utterance (e.g. U5_Disgust, U5).\n\nTo address the above problem,  Wang et al. (2023a)  proposed a two-step approach. First, they extract the emotional utterances and causal utterances by a multi-task learning framework and then pair and filter them.  Zhao et al. (2023)  proposes an end-to-end method by leveraging multi-task learning in a pipeline manner. However, these methods still suffer from low evaluation performances.\n\nMotivated by the phenomenon that the performance of the emotion recognition of utterances in a conversation harnessed by the traditional manner is generally poor, we design a new pipeline framework. Firstly we utilize the Llama-2-based Instruc-tERC  (Lei et al., 2023a)  to extract the emotion category of each utterance in a conversation. Then we consider the emotion causal pair extraction as the causal emotion entailment subtask and employ a two-stream attention model to extract the emotion causal pairs given the target emotion. For the causal span extraction, we employ MuTEC  (Bhat and Modi, 2023)  which is an end-to-end multi-task learning framework.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Works",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Emotion Recognition In Conversation",
      "text": "Emotion recognition in conversation (ERC), which is a task to predict emotions of utterances during conversations, is crucial in both of the two ECAC subtasks. The existing methods can be divided into graph-based, RNN-based, Transformer-based, LLM-based, and knowledge-injecting methods.\n\nGraph-based methods  (Shen et al., 2021b; Li et al., 2024; Zhang et al., 2019; Taichi et al., 2020; Ghosal et al., 2019)  aims to represent the correlations between emotions of utterances and speakers in the conversations. RNN-based methods  (Hu et al., 2023; Lei et al., 2023c; Majumder et al., 2019; Hazarika et al., 2018; Poria et al., 2017)  using GRU and LSTM  (Wang et al., 2020)  to capture the dependency of interlocutors and emotions of utterances. To model the emotional states during long-range context, Transformerbased methods  (Song et al., 2022; Liu et al., 2023b; Chudasama et al., 2022; Shen et al., 2021a; Hu et al., 2022)  utilize encoder-decoder framework or encoder-only models, such as BERT  (Li et al., 2020)  and RoBERTa  (Kim and Vossen, 2021) , to establish the correlation between long-range emotional states during conversations. Considering more than seven utterances in single conversation input, InstructERC  (Lei et al., 2023b)  defines the ERC task as a generative task based on LLMs, which unifies emotion labels between three common ERC datasets and utilizes auxiliary tasks (speaker identification and emotion prediction) by using instruction template to capture speaker relationships and emotional states in future utterances. Knowledge-injecting methods  (Freudenthaler et al., 2022; Ghosal et al., 2020; Zhong et al., 2019; Zhu et al., 2021; Lei et al., 2023b)  use external knowledge to analyze conversation scenarios.  Poria et al. (2021)  introduces the task of recognizing emotion causes in conversations and introduce two novel sub-tasks: Causal Span Extraction (CSE) and Causal Emotion Entailment (CEE), designed to identify the emotion cause at the span-level and utterance-level, respectively.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Emotion Causes In Conversations",
      "text": "Causal Emotion Entailment  Poria et al. (2021)  define CEE as a classification task for utterance pairs and establish robust Transformer-based baselines for it.  Wang et al. (2023a)  introduces a multimodality conversation dataset Emotion-Cause-in-Friends (ECF) and propose a two-step approach to extract the causal pairs. They first extract the emotion utterances and the potential causal utterances individually and then pair and filter them.  Li et al. (2022)  introduce the social commonsense knowledge to propagate causal clues between utterances.  Zhao et al. (2023)  propose the Knowledge-Bridged Causal Interaction Network (KBCIN), which integrates commonsense knowledge (CSK) as three bridges called semantics-level bridge, emotionlevel bridge and action-level bridge.\n\nCausal Span Extraction involves identifying the causal span (emotion cause) for a given non-neutral utterance.  Poria et al. (2021)  first introduces the subtask and employs the pre-trained Transformerbased model to formulate the Causal Span Extraction as the Machine Reading Comprehension (MRC).  Bhat and Modi (2023)  propose a multitask learning framework to extract the causal pairs and causal span in an utterance in a joint end-toend manner. Besides, they also propose a two-step approach consisting of Emotion Prediction (EP), followed by Causal Span (CSE).\n\n3 System Overview",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "System Architecture",
      "text": "The overview of the architecture of our proposed model is shown in Figure  1 . The InstructERC aims to extract the emotion of utterances. TSAM model is a two-stream attention model utilized to extract the causal pairs given the predicted emotion utterance. The MuTEC is an end-to-end network designed to extract the causal span based on the causal pair extraction.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Emotion Recognition In Conversations",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Instructerc For Emotion Recognition",
      "text": "InstructERC  (Lei et al., 2023b)  reformulate the ERC task from a discriminative framework to a generative framework and design a prompt template which comprises job description, historical utterance window, label set and emotional domain retrieval module. Besides emotion recognition task, InstructERC also utilizes speaker identification and emotion prediction tasks for ERC task. The performance of emotional domain retrieval module, which is based on Sentence BERT  (Reimers and Gurevych, 2019) , rely on the abundance of corpus. Taking into account that no additional data can be used, we only retain job description, historical utterance window and label statement in the instruct template.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Hierarchical Emotion Label",
      "text": "The hierarchical classification structure is shown in Figure  2 . The emotion labels in dataset can be split into three categories: neutral, positive and negative, which positive set consists of surprise and joy while negative set includes fear, sadness, disgust and anger.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Auxiliary Tasks And Instruct Design",
      "text": "Auxiliary tasks are proven as one of the efficient data augment methods  (Lei et al., 2023b) . Besides emotion recognition and speaker identification tasks, we add three auxiliary tasks in training data: sub-label recognition, positive recognition, and negative recognition tasks. The instruct template is depicted in Figure  3 .\n\nFor emotion recognition and speaker identification task, we follow the format of instruct template in InstructERC, which consists of job description, historical content and label statement. For sublabel recognition (SR), positive recognition (PR) and negative recognition (NR) tasks, we utilize the corresponding label set which is mentioned in Section 3.2.2 to replace the label statement separately. The number of Speakers in the dataset is 304. The number of utterances from other speakers except the protagonist is far lower than the number of protagonists. Therefore, we unified all speakers other than the protagonist into 'Others'.\n\nVisual data also plays an essential role in ERC. For video clips, we utilize LLaVA to generate descriptions of background, speaker movement and personal state. Therefore, we add background description, movement description and personal state description in instruct template. The background exhibits the information of scene in the conversation. The movement description depicts the action of speakers during corresponding utterances. The personal state description provides the observation of speakers' facial expressions. Considering the influence of the context, we have generated two sets of descriptions. The input of the first group only includes the clips corresponding to the utterances, while the second group adds the clips sequence corresponding to the historical utterances to the input of second group.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Emotion Cause Span Extraction",
      "text": "Emotion cause span extraction aims to extract the start position and end position of the causal utterance in a conversation. Typically, we can utilize a pipeline framework which firstly predicts the emotion and then predicts the cause span. For the cause span predictor, we can use SpanBERT  (Joshi et al., 2020) , RoBERTa  (Liu et al., 2019)  as the feature extractor and employ two heads on the top of them to extract the start and end positions given the causal utterance. The two-step model offers an advantage in its modularity, allowing the application of distinct architectures for the emotion predictor and cause span predictor. However, it comes with two drawbacks: 1) Errors in the first step can propagate to the next, and 2) This approach assumes that emotion prediction and cause-span prediction are mutually exclusive tasks. In our system, we follow MuTEC  Bhat and Modi (2023)  and use an end-to-end framework in a joint multi-task learning manner to extract the causal span in a conversation.\n\nDuring the training period, the input comprises the target utterance U t , the candidate causes utterance U i , and the historical context. MuTEC employs a pre-trained model (PLM) to extract the context representations. For emotion recognition, which is an auxiliary task, it employs a classification head on the top of the PLM. The end position is predicted by the prediction head of the concatenated representations of the given start index and the sequence output from the PLM. In this stage, the golden start index is used as the start index.\n\nThe training loss is a linear combination of the loss for cause-span prediction and emotion prediction:\n\nDuring the inference period, as the start index is unknown, it uses top k start indices as the candidate start indices and gets k candidate end indices. Finally, it gets the final start-end indices by argmaxing the k × k start-end pairs.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Emotion-Cause Pair Extraction",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Tsam Model",
      "text": "In our pipeline framework, for Subtask2, we first extract the emotion of the utterance and then ex-tract the causal pairs given the emotional utterance in a conversation. The causal pairs extraction is typically modelled as the causal emotion entailment (CEE) task. In our system, we employ TSAM model from  Zhang et al. (2022)  as the causal pair extractor. TSAM mainly comprises three modules: Speaker Attention Network (SAN), Emotion Attention Network (EAN), and Interaction Network (IN). The EAN and SAN integrate emotion and speaker information simultaneously, and the subsequent interaction module efficiently exchanges pertinent information between the EAN and SAN through a mutual BiAffine transformation  (Dozat and Manning, 2016) .\n\nContextual Utterance Representation The pretrained RoBERTa is employed as the utterance encoder, and we obtain contextual utterance representations by inputting the entire conversational history U t , into the RoBERTa  (Liu et al., 2019) , separated by a special token  [CLS] , where i = 0, 1, 2, ..., t. We use the representation of  [CLS]  as the contextual representation of the utterance, which can be denoted as\n\nEmotion Attention Network To represent emotions, the EAN utilizes an emotion embedding network as the extractor of emotion representations, X k e = Embedding(e k ), where e k represents k-th emotion label. The embedding network can be considered as the lookup-table operation. The emotion embedding matrix is initialized using a random initializer and is fine-tuned throughout the training process. Employing a multi-head attention mechanism  (Devlin et al., 2018) , the EAN treats utterance representations as query vectors and emotion representations as key and value vectors. The calculation process of the EAN mirrors that of a typical multi-head self-attention module (MHSA).\n\nwhere\n\nSpeaker Attention Network The SAN facilitates interactions between utterances to incorporate speaker information by applying attention over the speaker relation graph. There are two types of relation edges: (1) Intra-relation type, which signifies how the utterance influences other utterances, including itself, expressed by the same speaker;\n\n(2) Inter-relation type, indicating how the utterance influences those expressed by other speakers.\n\nThe speaker representation given a relationship can be formulated by the graphical attention mechanism  (Zhang et al., 2022) .\n\nInteraction Network To efficiently exchange pertinent information between the EAN and SAN, a mutual Bi-Affine transformation is applied as a bridge  (Dozat and Manning, 2016) . In our Interaction Network, we integrate a masking mechanism to accommodate the existence of empty utterance speakers in some instances, which differs from the original approach. We denote this approach as the Masking Interaction Network (MIN).\n\nCause Predictor The ultimate utterance representation for U i is acquired by concatenating the output Ḣe and Ḣs from the L-layer TSAM. Subsequently, the concatenated vector undergoes classification using a fully-connected network. Given the target utterance U i , the causal probability of the U j can be formulated as follows:\n\nMulti-task Learning Auxiliary Task (MTLA) One drawback of the pipeline framework is that the extraction of utterance emotion and causal information are treated as separate tasks, potentially limiting the exploration of implicit relationships between them. Therefore, we incorporate emotion prediction as an auxiliary task within a multi-task learning framework. For emotion prediction, we utilize a classification head atop the Transformerbased model and apply the Dice loss  (Li et al., 2019)  as the multi-category classification loss.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Infusion Of Video And Audio Information",
      "text": "The video data potentially carries rich knowledge for emotion analysis and existing research  (Caridakis et al., 2007)  has underscored the significance of multi-modal information in augmenting the semantic prediction capabilities of models. Our study leverages the visual and auditory cues present in conversational contexts with the aim of bolstering the efficacy of our language models in emotion analysis tasks.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Embedding And Concating Strategy",
      "text": "We set up specific embedding and fusion strategies for different language models. For BERT, we use the concatenation of textual and multi-modal features in the hidden layer. For Large Language Models (LLMs), our approach is characterized by the utilization of visual captions as supportive prompts, thereby furnishing the LLMs with an enriched informational context.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Extract Audio Feature Set",
      "text": "Audio data contains valuable information for emotion analysis, including tone, pitch, speed, and intensity of speech, as well as non-linguistic sounds and pauses, which together convey rich emotional cues. We use openSMILE  (Eyben et al., 2010)  to extract two comprehensive feature sets: GeMAPS  (Eyben et al., 2016)  and ComParE  (Schuller and Batliner, 2013) . GeMAPS is proposed for its effectiveness in capturing emotion-relevant vocal characteristics and ComParE encompasses a wide range of descriptors.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Video Image To Text",
      "text": "Integrating multi-modal features directly into the hidden layers of Large Language Models (LLMs) presents a significant challenge, primarily due to the prohibitive requirements for data and computational resources, such as GPUs. Although some finetuning strategies like prompt tuning could achieve it by addiing features to the input layer, we convert video to text with captioning where we can leverage our well-trained ERC model. The performance of image captioning has been further enhanced with the outstanding NLU ability of LLMs. Large VLMs like LLaVA  (Liu et al., 2023a)  provide GPT-4 level multi-modal capability by visual instruction tuning. Furthermore, the Audio-Visual Language Model, Video-Llama  (Zhang et al., 2023a) , integrates both visual and audio encoders, enabling the comprehensive fusion of entire video content into LLMs. Without further training the VLMs as lack data, a well-designed prompt instructs the model to generate an emotionrelated description. Our prompt asks the model to generate information from the front-ground event and place to character movements, the main character, facial expression, and finally emotion. The use of Chain-of-Thought  (Wei et al., 2022)  prompting further guides the model through a step-by-step process to derive the final emotion label. The output generated at each step is then incorporated into the ERC model, enriching it with a more detailed informational context.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Video Image To Face Embedding",
      "text": "The faces in the video images contain rich emotionrelated information, so pre-trained models are used to extract the face embeddings and correspond the identity of the face to the speaker in the text. The framework of the face module is shown in Figure  4 .\n\nFirstly, the Multi-Task Convolutional Neural Network (MTCNN)  (Zhang et al., 2016)  is used to detect the bounding boxes and key points of the faces. Next, the face images are affine transformed to a forward and intermediate state, and the faces are cropped and resized. The cropped images are then used for two subtasks: face matching and Face Emotion Recognition (FER). During face matching, two images of each protagonist are selected to build a matching database. With the help of Mo-bileFaceNets  (Chen et al., 2018) , the embeddings of the face images are extracted, and the identity of each face image is obtained by calculating its similarity with the embeddings of faces in the matching database. During FER, the emotion-related embedding of the face image corresponding to the speaker is extracted by VGG19  (Simonyan and Zisserman, 2015)  for subsequent multimodal analysis. When the speaker is a supporting character that is not included in the matching database, the features of the face image with the largest area are selected. When no face is detected or the speaker cannot be matched, the output features are filled with 0.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Model Ensemble",
      "text": "Ensembling models has been proven to be effective in boosting system performance across various tasks  (Zhang et al., 2023b) . For the extraction of  causal pairs, we utilize various models for ensemble learning. We utilize a majority voting mechanism to determine the final prediction, aiming for optimal performance on the test dataset.\n\n4 Experimental Setup",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Training Data",
      "text": "The split of dataset is same as SHARK  (Wang et al., 2023b) . The ECF dataset is divided into training, validation and test sets, which incclude 9966, 1087, 2566 utterances.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Training Details",
      "text": "For ERC task, we use InstructERC with Llama-2-7B-chat and LLamMA2-13B-chat, which retain default parameters. We finetune ERC model by peft on single A100 with batch size 8. The length of historical window is 12.\n\nFor both the causal emotion entailment subtask and the causal span extraction subtask, we adopt the default hyperparameter settings from the respective original papers. We found that conducting a hyperparameter grid search did not yield any additional performance improvements.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Results And Discuss",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Emotion Recognition",
      "text": "We use weight average F1 score and accuracy to evaluate the performance of the model. It should be noted that according to the rules of the competition, we removed the neutral utterances when computing F1 score and accuracy. The result of ERC on test set is shown in Table  1 . All models is trained on four auxiliary tasks mentioned by in Section 3.2.3. The best weight average F1 score is 58.64, which is achieved by Llama-2-13B with historical clips descriptions. The descriptions which contains information with the emotions of speakers improve 0.79 (from 57.85 to 58.64). As for accuracy, the Llama-2-13B without video clips descriptions achieves the highest score of 61.45. Compared with InstructERC's training data strategy, we have added additional auxiliary tasks and improve 12.52 on accuracy.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Emotion Cause Span Extraction",
      "text": "We utilize an end-to-end framework for cause span extraction and achieve a final performance of 32.23 in weighted average proportional F1 score on the official evaluation dataset as is shown in the Table  2 . Our result significantly surpasses the result of 26.40 above ∼ +6.0 achieved by the second-place participant. Furthermore, our results achieved the highest scores across all other official evaluation metrics, validating the effectiveness of our approach for subtask 1.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Causal Emotion Entailment",
      "text": "In our initial experiments focusing solely on text modality, we utilize the TSAM model as our baseline for the causal pair extraction subtask. As is shown in Table  2 , After incorporating the MIN, our positive F1 score improves by +1.2. Furthermore, with the introduction of emotional multitask learning as an auxiliary task, our result sees an additional improvement of +0.4. Furthermore, we achieve an additional improvement of approximately ∼ +1.1 in the official final evaluation dataset through model ensembling.\n\nWe also conduct experiments involving other modalities, including audio and vision, as is show in Table  3 . For both audio and vision features, we concatenate them with the pure textual features. Regarding audio, we experiment with two public feature sets: GeMAPS and ComParE. The GeMAPS feature has a dimension of 62, while the ComParE feature has a dimension of 6373. For the ComParE features, we employ an L1-based logistic regression classifier for feature selection, and we find that the best performance is achieved with a feature selection dimension of 128, resulting in a performance of 73.9. For the vision modality, we achieve a performance of 74.8, which is comparable to the result of the audio modality. However, upon introducing either audio or visual modalities, we observe a decreasing trend compared to the pure textual modality. This observation inspires us to develop a more reasonable approach to incorporate multi-modality in conversation analysis.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we propose a joint pipeline framework for Subtask1 and Subtask2. Firstly, we utilize the Llama-2-based Instruct ERC model to extract the emotional content of utterances in a conversation. Next, we employ a two-stream attention model to identify causal pairs based on the predicted emotional states of the utterances. Lastly, we adopt an end-to-end framework using a multi-task learning approach to extract causal spans within a conversation. Our approach achieved first place in the competition, and the effectiveness of our approach is further confirmed by the ablation study.\n\nIn future work, we plan to explore the integration of audio and visual modalities to enhance the performance of the task.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The InstructERC",
      "page": 2
    },
    {
      "caption": "Figure 1: The overview of proposed model framework.",
      "page": 3
    },
    {
      "caption": "Figure 2: The emotion labels in dataset can",
      "page": 3
    },
    {
      "caption": "Figure 2: The Hierarchical Structure of Emotion labels.",
      "page": 3
    },
    {
      "caption": "Figure 3: For emotion recognition and speaker identifica-",
      "page": 3
    },
    {
      "caption": "Figure 3: The Schematic of Instruct Template for ERC.",
      "page": 4
    },
    {
      "caption": "Figure 4: The framework of the face module.",
      "page": 5
    },
    {
      "caption": "Figure 4: Firstly, the Multi-Task Convolutional Neural",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Results of ERC task on test set without neutral utterances.",
      "page": 6
    },
    {
      "caption": "Table 2: Results of our models for the causal emotion entailment subtask.",
      "page": 7
    },
    {
      "caption": "Table 1: . All mod-",
      "page": 7
    },
    {
      "caption": "Table 2: Our result significantly surpasses the result of 26.40",
      "page": 7
    },
    {
      "caption": "Table 2: , After incorporating the MIN,",
      "page": 7
    },
    {
      "caption": "Table 3: For both audio and vision features, we",
      "page": 7
    },
    {
      "caption": "Table 3: Results of multi-modality experiments for the causal emotion entailment subtask.",
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Multi-task learning framework for extracting emotion cause span and entailment in conversations",
      "authors": [
        "Ashwani Bhat",
        "Ashutosh Modi"
      ],
      "year": "2023",
      "venue": "Transfer Learning for Natural Language Processing Workshop"
    },
    {
      "citation_id": "2",
      "title": "IEMO-CAP: Interactive emotional dyadic motion capture database. Language resources and evaluation",
      "authors": [
        "C Busso",
        "M Bulut"
      ],
      "year": "2008",
      "venue": "IEMO-CAP: Interactive emotional dyadic motion capture database. Language resources and evaluation"
    },
    {
      "citation_id": "3",
      "title": "Multimodal emotion recognition from expressive faces, body gestures and speech",
      "authors": [
        "George Caridakis",
        "Ginevra Castellano",
        "Loic Kessous",
        "Amaryllis Raouzaiou",
        "Lori Malatesta",
        "Stelios Asteriadis",
        "Kostas Karpouzis"
      ],
      "year": "2007",
      "venue": "Artificial Intelligence and Innovations 2007: from Theory to Applications"
    },
    {
      "citation_id": "4",
      "title": "Mobilefacenets: Efficient cnns for accurate real-time face verification on mobile devices",
      "authors": [
        "Sheng Chen",
        "Yang Liu",
        "Xiang Gao",
        "Zhen Han"
      ],
      "year": "2018",
      "venue": "Biometric Recognition: 13th Chinese Conference"
    },
    {
      "citation_id": "5",
      "title": "Pankaj Wasnik, and Naoyuki Onoe. 2022. M2fnet: Multi-modal fusion network for emotion recognition in conversation",
      "authors": [
        "Purbayan Vishal Chudasama",
        "Ashish Kar",
        "Nirmesh Gudmalwar",
        "Shah"
      ],
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops"
    },
    {
      "citation_id": "6",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2018",
      "venue": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "7",
      "title": "ECPE-2D: Emotion-cause pair extraction based on joint twodimensional representation, interaction and prediction",
      "authors": [
        "Zixiang Ding",
        "Rui Xia",
        "Jianfei Yu"
      ],
      "year": "2020",
      "venue": "Association for Computational Linguistics (ACL)"
    },
    {
      "citation_id": "8",
      "title": "Deep biaffine attention for neural dependency parsing",
      "authors": [
        "Timothy Dozat",
        "Christopher Manning"
      ],
      "year": "2016",
      "venue": "Deep biaffine attention for neural dependency parsing",
      "arxiv": "arXiv:1611.01734"
    },
    {
      "citation_id": "9",
      "title": "The geneva minimalistic acoustic parameter set (gemaps) for voice research and affective computing",
      "authors": [
        "Florian Eyben",
        "Klaus Scherer",
        "Björn Schuller",
        "Johan Sundberg",
        "Elisabeth André",
        "Carlos Busso",
        "Laurence Devillers",
        "Julien Epps",
        "Petri Laukka",
        "S Shrikanth",
        "Khiet Narayanan",
        "Truong"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2015.2457417"
    },
    {
      "citation_id": "10",
      "title": "Opensmile: the munich versatile and fast opensource audio feature extractor",
      "authors": [
        "Florian Eyben",
        "Martin Wöllmer",
        "Björn Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM International Conference on Multimedia, MM '10",
      "doi": "10.1145/1873951.1874246"
    },
    {
      "citation_id": "11",
      "title": "Transition-based directed graph construction for emotion-cause pair extraction",
      "authors": [
        "Chuang Fan",
        "Chaofa Yuan",
        "Jiachen Du",
        "Lin Gui",
        "Min Yang",
        "Ruifeng Xu"
      ],
      "year": "2020",
      "venue": "Association for Computational Linguistics (ACL)"
    },
    {
      "citation_id": "12",
      "title": "KI-Net: Ai-based optimization in industrial manufacturing-a project overview",
      "authors": [
        "Bernhard Freudenthaler",
        "Jorge Martinez-Gil",
        "Anna Fensel",
        "Kai Höfig",
        "Stefan Huber",
        "Dirk Jacob"
      ],
      "year": "2022",
      "venue": "International Conference on Computer Aided Systems Theory"
    },
    {
      "citation_id": "13",
      "title": "Alexander Gelbukh, Rada Mihalcea, and Soujanya Poria",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder"
      ],
      "year": "2020",
      "venue": "Commonsense knowledge for emotion identification in conversations",
      "arxiv": "arXiv:2010.02795"
    },
    {
      "citation_id": "14",
      "title": "Dia-logueGCN: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Soujanya Poria",
        "Niyati Chhaya",
        "Alexander Gelbukh"
      ],
      "year": "2019",
      "venue": "Dia-logueGCN: A graph convolutional neural network for emotion recognition in conversation",
      "arxiv": "arXiv:1908.11540"
    },
    {
      "citation_id": "15",
      "title": "ICON: Interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Rada Mihalcea",
        "Erik Cambria",
        "Roger Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/D18-1280"
    },
    {
      "citation_id": "16",
      "title": "Supervised adversarial contrastive learning for emotion recognition in conversations",
      "authors": [
        "Dou Hu",
        "Yinan Bao",
        "Lingwei Wei",
        "Wei Zhou",
        "Songlin Hu"
      ],
      "year": "2023",
      "venue": "Supervised adversarial contrastive learning for emotion recognition in conversations",
      "arxiv": "arXiv:2306.01505"
    },
    {
      "citation_id": "17",
      "title": "UniMSE: Towards unified multimodal sentiment analysis and emotion recognition",
      "authors": [
        "Guimin Hu",
        "Ting-En Lin",
        "Yi Zhao",
        "Guangming Lu",
        "Yuchuan Wu",
        "Yongbin Li"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "18",
      "title": "Spanbert: Improving pre-training by representing and predicting spans",
      "authors": [
        "Mandar Joshi",
        "Danqi Chen",
        "Yinhan Liu",
        "Luke Daniel S Weld",
        "Omer Zettlemoyer",
        "Levy"
      ],
      "year": "2020",
      "venue": "Transactions of the association for computational linguistics"
    },
    {
      "citation_id": "19",
      "title": "EmoBERTa: Speaker-aware emotion recognition in conversation with roberta",
      "authors": [
        "Taewoon Kim",
        "Piek Vossen"
      ],
      "year": "2021",
      "venue": "EmoBERTa: Speaker-aware emotion recognition in conversation with roberta"
    },
    {
      "citation_id": "20",
      "title": "Instructerc: Reforming emotion recognition in conversation with a retrieval multi-task llms framework",
      "authors": [
        "Shanglin Lei",
        "Guanting Dong",
        "Xiaoping Wang",
        "Keheng Wang",
        "Sirui Wang"
      ],
      "year": "2023",
      "venue": "Instructerc: Reforming emotion recognition in conversation with a retrieval multi-task llms framework",
      "arxiv": "arXiv:2309.11911"
    },
    {
      "citation_id": "21",
      "title": "InstructERC: Reforming emotion recognition in conversation with a retrieval multi-task llms framework",
      "authors": [
        "Shanglin Lei",
        "Guanting Dong",
        "Xiaoping Wang",
        "Keheng Wang",
        "Sirui Wang"
      ],
      "year": "2023",
      "venue": "InstructERC: Reforming emotion recognition in conversation with a retrieval multi-task llms framework",
      "doi": "10.48550/arXiv.2309.11911",
      "arxiv": "arXiv:2309.11911"
    },
    {
      "citation_id": "22",
      "title": "2023c. Watch the speakers: A hybrid continuous attribution network for emotion recognition in conversation with emotion disentanglement",
      "authors": [
        "Shanglin Lei",
        "Xiaoping Wang",
        "Guanting Dong",
        "Jiang Li",
        "Yingjian Liu"
      ],
      "venue": "2023 IEEE 35th International Conference on Tools with Artificial Intelligence (ICTAI)",
      "doi": "10.1109/ICTAI59109.2023.00133"
    },
    {
      "citation_id": "23",
      "title": "GraphCFC: A directed graph based cross-modal feature complementation approach for multimodal conversational emotion recognition",
      "authors": [
        "Jiang Li",
        "Xiaoping Wang",
        "Guoqing Lv",
        "Zhigang Zeng"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Multimedia",
      "doi": "10.1109/TMM.2023.3260635"
    },
    {
      "citation_id": "24",
      "title": "Neutral utterances are also causes: Enhancing conversational causal emotion entailment with social commonsense knowledge",
      "authors": [
        "Jiangnan Li",
        "Fandong Meng",
        "Zheng Lin",
        "Rui Liu",
        "Peng Fu",
        "Yanan Cao",
        "Weiping Wang",
        "Jie Zhou"
      ],
      "year": "2022",
      "venue": "Neutral utterances are also causes: Enhancing conversational causal emotion entailment with social commonsense knowledge",
      "arxiv": "arXiv:2205.00759"
    },
    {
      "citation_id": "25",
      "title": "Multi-task learning with auxiliary speaker identification for conversational emotion recognition",
      "authors": [
        "Jingye Li",
        "Meishan Zhang",
        "Donghong Ji",
        "Yijiang Liu"
      ],
      "year": "2020",
      "venue": "Multi-task learning with auxiliary speaker identification for conversational emotion recognition",
      "arxiv": "arXiv:2003.01478"
    },
    {
      "citation_id": "26",
      "title": "Dice loss for data-imbalanced nlp tasks",
      "authors": [
        "Xiaoya Li",
        "Xiaofei Sun",
        "Yuxian Meng",
        "Junjun Liang",
        "Fei Wu",
        "Jiwei Li"
      ],
      "year": "2019",
      "venue": "Dice loss for data-imbalanced nlp tasks",
      "arxiv": "arXiv:1911.02855"
    },
    {
      "citation_id": "27",
      "title": "DailyDialog a manually labelled multi-turn dialogue dataset",
      "authors": [
        "Yanran Li",
        "Hui Su",
        "Xiaoyu Shen",
        "Wenjie Li",
        "Ziqiang Cao",
        "Shuzi Niu"
      ],
      "year": "2017",
      "venue": "DailyDialog a manually labelled multi-turn dialogue dataset",
      "doi": "10.48550/arXiv.1710.03957",
      "arxiv": "arXiv:1710.03957"
    },
    {
      "citation_id": "28",
      "title": "",
      "authors": [
        "Haotian Liu",
        "Chunyuan Li",
        "Qingyang Wu",
        "Yong Jae Lee"
      ],
      "year": "2023",
      "venue": ""
    },
    {
      "citation_id": "29",
      "title": "Fuzhao Xue, and Yang You. 2023b. Hierarchical dialogue understanding with special tokens and turn-level attention",
      "authors": [
        "Xiao Liu",
        "Jian Zhang",
        "Heng Zhang"
      ],
      "venue": "Fuzhao Xue, and Yang You. 2023b. Hierarchical dialogue understanding with special tokens and turn-level attention",
      "arxiv": "arXiv:2305.00262"
    },
    {
      "citation_id": "30",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "31",
      "title": "DialogueRNN: An attentive rnn for emotion detection in conversations",
      "authors": [
        "N Majumder",
        "S Poria",
        "D Hazarika",
        "R Mihalcea",
        "A Gelbukh",
        "E Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": "10.1609/aaai.v33i01.33016818"
    },
    {
      "citation_id": "32",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P17-1081"
    },
    {
      "citation_id": "33",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika"
      ],
      "year": "2019",
      "venue": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "doi": "10.48550/arXiv.1810.02508",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "34",
      "title": "Recognizing emotion cause in conversations",
      "authors": [
        "Soujanya Poria",
        "Navonil Majumder",
        "Devamanyu Hazarika",
        "Deepanway Ghosal",
        "Rishabh Bhardwaj",
        "Samson Yu Bai Jian",
        "Pengfei Hong",
        "Romila Ghosh",
        "Abhinaba Roy",
        "Niyati Chhaya"
      ],
      "year": "2021",
      "venue": "Cognitive Computation"
    },
    {
      "citation_id": "35",
      "title": "Sentence-bert: Sentence embeddings using siamese bert-networks",
      "authors": [
        "Nils Reimers",
        "Iryna Gurevych"
      ],
      "year": "2019",
      "venue": "Sentence-bert: Sentence embeddings using siamese bert-networks",
      "arxiv": "arXiv:1908.10084"
    },
    {
      "citation_id": "36",
      "title": "Computational Paralinguistics: Emotion, Affect and Personality in Speech and Language Processing, 1st edition",
      "authors": [
        "Bjorn Schuller",
        "Anton Batliner"
      ],
      "year": "2013",
      "venue": "Computational Paralinguistics: Emotion, Affect and Personality in Speech and Language Processing, 1st edition"
    },
    {
      "citation_id": "37",
      "title": "2021a. DialogXL: All-in-one xlnet for multiparty conversation emotion recognition",
      "authors": [
        "Weizhou Shen",
        "Junqing Chen",
        "Xiaojun Quan",
        "Zhixian Xie"
      ],
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": "10.1609/aaai.v35i15.17625"
    },
    {
      "citation_id": "38",
      "title": "2021b. Directed acyclic graph network for conversational emotion recognition",
      "authors": [
        "Weizhou Shen",
        "Siyue Wu",
        "Yunyi Yang",
        "Xiaojun Quan"
      ],
      "venue": "2021b. Directed acyclic graph network for conversational emotion recognition",
      "arxiv": "arXiv:2105.12907"
    },
    {
      "citation_id": "39",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "Karen Simonyan",
        "Andrew Zisserman"
      ],
      "year": "2015",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "40",
      "title": "Supervised prototypical contrastive learning for emotion recognition in conversation",
      "authors": [
        "Xiaohui Song",
        "Longtao Huang",
        "Hui Xue",
        "Songlin Hu"
      ],
      "year": "2022",
      "venue": "Supervised prototypical contrastive learning for emotion recognition in conversation",
      "arxiv": "arXiv:2210.08713"
    },
    {
      "citation_id": "41",
      "title": "Relation-aware graph attention networks with relational position encodings for emotion recognition in conversations",
      "authors": [
        "Ishiwatari Taichi",
        "Yasuda Yuki",
        "Miyazaki Taro"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
      "doi": "10.18653/v1/2020.emnlp-main.597"
    },
    {
      "citation_id": "42",
      "title": "2023a. Multimodal emotion-cause pair extraction in conversations",
      "authors": [
        "Fanfan Wang",
        "Zixiang Ding",
        "Rui Xia",
        "Zhaoyu Li",
        "Jianfei Yu"
      ],
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "43",
      "title": "Semeval-2024 task 3: Multimodal emotion cause analysis in conversations",
      "authors": [
        "Fanfan Wang",
        "Heqing Ma",
        "Rui Xia",
        "Jianfei Yu",
        "Erik Cambria"
      ],
      "year": "2024",
      "venue": "Proceedings of the 18th International Workshop on Semantic Evaluation (SemEval-2024)"
    },
    {
      "citation_id": "44",
      "title": "2023b. Generative emotion cause triplet extraction in conversations with commonsense knowledge",
      "authors": [
        "Fanfan Wang",
        "Jianfei Yu",
        "Rui Xia"
      ],
      "venue": "2023b. Generative emotion cause triplet extraction in conversations with commonsense knowledge"
    },
    {
      "citation_id": "45",
      "title": "Contextualized emotion recognition in conversation as sequence tagging",
      "authors": [
        "Yan Wang",
        "Jiayu Zhang",
        "Jun Ma",
        "Shaojun Wang",
        "Jing Xiao"
      ],
      "year": "2020",
      "venue": "Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue",
      "doi": "10.18653/v1/2020.sigdial-1.23"
    },
    {
      "citation_id": "46",
      "title": "Chain of thought prompting elicits reasoning in large language models",
      "authors": [
        "Jason Wei",
        "Xuezhi Wang",
        "Dale Schuurmans",
        "Maarten Bosma",
        "Ed Huai Hsin Chi",
        "F Xia",
        "Quoc Le",
        "Denny Zhou"
      ],
      "year": "2022",
      "venue": "Chain of thought prompting elicits reasoning in large language models"
    },
    {
      "citation_id": "47",
      "title": "Effective inter-clause modeling for end-to-end emotioncause pair extraction",
      "authors": [
        "Penghui Wei",
        "Jiahao Zhao",
        "Wenji Mao"
      ],
      "year": "2020",
      "venue": "Association for Computational Linguistics (ACL)"
    },
    {
      "citation_id": "48",
      "title": "Emotion-cause pair extraction: A new task to emotion analysis in texts",
      "authors": [
        "Rui Xia",
        "Zixiang Ding"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "49",
      "title": "Emotion detection on tv show transcripts with sequencebased convolutional neural networks",
      "authors": [
        "M Sayyed",
        "Jinho Zahiri",
        "Choi"
      ],
      "year": "2017",
      "venue": "Emotion detection on tv show transcripts with sequencebased convolutional neural networks",
      "doi": "10.48550/arXiv.1708.04299",
      "arxiv": "arXiv:1708.04299"
    },
    {
      "citation_id": "50",
      "title": "Modeling both context-and speaker-sensitive dependence for emotion detection in multi-speaker conversations",
      "authors": [
        "Dong Zhang",
        "Liangqing Wu",
        "Changlong Sun"
      ],
      "year": "2019",
      "venue": "IJCAI"
    },
    {
      "citation_id": "51",
      "title": "Tsam: A two-stream attention model for causal emotion entailment",
      "authors": [
        "Duzhen Zhang",
        "Zhen Yang",
        "Fandong Meng",
        "Xiuyi Chen",
        "Jie Zhou"
      ],
      "year": "2022",
      "venue": "Tsam: A two-stream attention model for causal emotion entailment",
      "arxiv": "arXiv:2203.00819"
    },
    {
      "citation_id": "52",
      "title": "2023a. Video-LLaMA: An instruction-tuned audio-visual language model for video understanding",
      "authors": [
        "Hang Zhang",
        "Xin Li",
        "Lidong Bing"
      ],
      "venue": "2023a. Video-LLaMA: An instruction-tuned audio-visual language model for video understanding",
      "arxiv": "arXiv:2306.02858"
    },
    {
      "citation_id": "53",
      "title": "2023b. Samsung research china-beijing at semeval-2023 task 2: An al-r model for multilingual complex named entity recognition",
      "authors": [
        "Haojie Zhang",
        "Xiao Li",
        "Renhua Gu",
        "Xiaoyan Qu",
        "Xiangfeng Meng",
        "Shuo Hu",
        "Song Liu"
      ],
      "venue": "Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)"
    },
    {
      "citation_id": "54",
      "title": "Joint face detection and alignment using multitask cascaded convolutional networks",
      "authors": [
        "Kaipeng Zhang",
        "Zhanpeng Zhang",
        "Zhifeng Li",
        "Yu Qiao"
      ],
      "year": "2016",
      "venue": "IEEE signal processing letters"
    },
    {
      "citation_id": "55",
      "title": "Knowledge-bridged causal interaction network for causal emotion entailment",
      "authors": [
        "Weixiang Zhao",
        "Yanyan Zhao",
        "Zhuojun Li",
        "Bing Qin"
      ],
      "year": "2023",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "56",
      "title": "Knowledge-enriched transformer for emotion detection in textual conversations",
      "authors": [
        "Peixiang Zhong",
        "Di Wang",
        "Chunyan Miao"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
      "doi": "10.18653/v1/D19-1016"
    },
    {
      "citation_id": "57",
      "title": "Topic-driven and knowledgeaware transformer for dialogue emotion detection",
      "authors": [
        "Lixing Zhu",
        "Gabriele Pergola",
        "Lin Gui",
        "Deyu Zhou",
        "Yulan He"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
      "doi": "10.18653/v1/2021.acl-long.125"
    }
  ]
}