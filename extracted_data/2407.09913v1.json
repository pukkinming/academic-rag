{
  "paper_id": "2407.09913v1",
  "title": "Emotion Detection Through Body Gesture And Face",
  "published": "2024-07-13T15:15:50Z",
  "authors": [
    "Haoyang Liu"
  ],
  "keywords": [
    "Aff-Wild2",
    "DFEW",
    "ResNet",
    "DenseNet",
    "ANN",
    "Valence and Arousal"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The project leverages advanced machine and deep learning techniques to address the challenge of emotion recognition by focusing on non-facial cues, specifically hands, body gestures, and gestures. Traditional emotion recognition systems mainly rely on facial expression analysis and often ignore the rich emotional information conveyed through body language. To bridge this gap, this method leverages the Aff-Wild2 and DFEW databases to train and evaluate a model capable of recognizing seven basic emotions (angry, disgust, fear, happiness, sadness, surprise, and neutral) and estimating valence and continuous scales wakeup descriptor. Leverage OpenPose for pose estimation to extract detailed body posture and posture features from images and videos. These features serve as input to stateof-the-art neural network architectures, including ResNet, and ANN for emotion classification, and fully connected layers for valence arousal regression analysis. This bifurcation strategy can solve classification and regression problems in the field of emotion recognition. The project aims to contribute to the field of affective computing by enhancing the ability of machines to interpret and respond to human emotions in a more comprehensive and nuanced way. By integrating multimodal data and cutting-edge computational models, I aspire to develop a system that not only enriches human-computer interaction but also has potential applications in areas as diverse as mental health support, educational technology, and autonomous vehicle systems.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Background",
      "text": "Human-computer interaction (HCI) has expanded dramatically with the emergence of machine and deep learning technologies, aiming to create more natural and intuitive ways for machines to understand and respond to human needs.\n\nA key aspect of this interaction is emotion recognition, which is the process of recognizing human emotions through various means, including facial expressions, speech, and, as the focus of this project, body gestures. Historically, facial expressions have been the primary conduit for emotion recognition systems, exploiting the nuances of the human face to map to universally recognized basic emotions: anger, disgust, fear, happiness, sadness, surprise, and neutral states. However, this approach ignores the rich nonverbal cues humans display through body language.\n\nRecent explorations in the field of affective computing have shown that body gestures can tell as much, if not more, about a person's emotional state as facial expressions alone. These non-facial cues can provide additional context and improve the accuracy of emotion recognition systems, especially when facial expressions are ambiguous or social conditions mask true feelings. Therefore, emotion recognition is crucial in the application areas such as marketing, human-robot interaction, healthcare, mental health monitoring, and security  [1] . The study of emotions for healthcare includes vast neurological disorders like sleep disorders  [2] , evaluation of sleep quality  [3] .\n\nThis project aims to harness the untapped potential of body language as a medium for emotion recognition. By leveraging advanced machine and deep learning techniques, such as OpenPose for pose estimation, and complex neural networks, such as ResNet, DenseNet, and Visual Transformers (ViT), the project aims to classify of seven basic emotions. And estimate the valence and arousal emotion descriptors are derived from human body gestures.\n\nThe databases that will be used, Aff-Wild2  [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]  and DFEW, provide rich datasets on which models can be trained to account for continuous emotional states, ranging from highly positive to very negative (valence), and from very passive to highly active (arousal). Classification of basic emotions will serve as a discrete problem-solving task, while valence arousal estimation will solve a more complex regression problem.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Problem Statement",
      "text": "In the rapidly developing field of human-computer interaction, the accurate recognition and interpretation of human emotions plays a crucial role. Traditional emotion recognition systems focus primarily on facial expressions, which, while effective, cannot provide a complete picture of the human emotion spectrum. This limitation becomes especially apparent in situations where facial cues are invisible or influenced by social and cultural norms (like wear mask). Therefore, there is a huge gap in exploiting the full potential of non-verbal cues, especially body gestures, which are often overlooked in emotion recognition systems.\n\nFurthermore, most existing emotion recognition models are build around discrete emotional states, mainly the seven basic emotions. However, human emotions are more subtle and often exist on a spectrum, especially when con-sidering the dimensions of valence and arousal. The lack of models that accurately explain and quantify these dimensions limits the applicability of emotion recognition to domains that require a more complex understanding of human emotions, such as mental health assessment and adaptive learning systems.\n\nThe challenge, therefore, is to develop a comprehensive emotion recognition system that goes beyond only facial analysis to incorporate the subtleties of body gesture to provide a more complete and accurate understanding of human emotions. Such a system must not only classify basic emotional states but must also leverage advanced machine and deep learning techniques to estimate complex emotion descriptors of valence and arousal. The effective integration of these technologies has the potential to revolutionize the field of affective computing and expand its applications in different fields.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Aim",
      "text": "The purpose of this project is to develop an emotion recognition system that can identify human emotions with high accuracy by analysing body gestures and facial expression. The system utilizes machine and deep learning methods and is designed to surpass the capabilities of traditional facial recognition models. It will cover the seven basic emotional states, valence and arousal. With this innovation, we seek to capture the subtlety and complexity of human emotions and promote better understanding and interaction within the field of affective computing. • Emotion classification: To construct a classify model capable of 7 basic emotion states. Inputs are from face expressions and body gestures, through the model to determine the specific emotional state among the seven basic emotions.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Objectives",
      "text": "• Valence-Arousal Analysis: To construct a regression model capable of estimating valence and arousal levels, providing a mapping of emotional states.\n\n• Performance Evaluation: To rigorously evaluate the performance of the models using metrics such as accuracy, precision, and recall.\n\n• System Optimization: To fine-tune the system for real-time processing and ensure it is robust against variations in environmental conditions.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Literature Review",
      "text": "Emotion recognition has become a popular research topic in the past decade.\n\nWhile works based on facial expressions or speech abound, identifying the effects of body posture remains a less explored topic  [34] , This chapter provides a detailed analysis of the impact of associate technology and provides information on existing technologies in place in order to gather requirements for the proposed solution. Then what lessons can be learned from these approaches?",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Emotion Recognition Techniques",
      "text": "There are several traditional approaches to process emotion recognition. Like from speech and facial expressions are the most employed mechanisms for emotion identification among physical signals  [35] . After that, Physiological Response Measurement is a good approach to recognize emotion. For example, electroencephalogram (EEG), electrocardiogram (ECG), electromyogram (EMG), galvanic skin response (GSR), respiration (RSP), skin temperature, photoplethysmography, and eye tracking (ET) are the most employed physiological signals for emotion recognition  [35] . Among physiological signals, the most often utilized modalities for detecting human emotions are EEG, GSR, ECG, and ET  [36] . Furthermore, body gesture. As a crucial component of body language, also a good approach to recognize emotion.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Emotion Recognition From Body Gesture And Pose",
      "text": "Psychological research further shows that body gestures conveys nonverbal emotional cues, include some part face and voice cannot express. However, recognizing emotion based on body gestures remains less explored. Emotion recognition from body gesture is a field of study in artificial intelligence and human-computer interaction focusing on the ability of machines to identify and interpret human emotions based on physical expressions and movements. This technology uses sensors, computer vision, and machine learning algorithms to analyse the posture, movement, and gestures of a person.\n\nThe Multi-view emotional expressions dataset using 2D pose estimation as featured in Nature emphasizes the potency of body expressions in conveying emotional shifts, sometimes even surpassing facial or vocal expressions in efficacy  [37] .\n\nThe exploration of body gesture as a component of body language in emotion recognition is less explored compared to face expression-based and speech-based approaches. However, the works such as those presented in IEEE Xplore suggest that deep learning can successfully be employed to recognize emotions from body gestures, using large datasets of multi-view RGB videos for training  [38] . This burgeoning interest in body gesture and pose for emotion recognition signifies a pivotal shift towards more comprehensive and possibly more naturalistic modes of human affect analysis. As such, it holds considerable promise for applications in areas ranging from interactive gaming to psychological analysis, and from assistive technologies to security systems.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Machine And Deep Learning In Emotion Recognition",
      "text": "Machine learning is like teaching a computer to make decisions or predictions based on past examples. You give the computer data, and it learns patterns from that data. While deep learning is a special way of doing machine learning that involves layers of processing to understand complex patterns. Think of it like stacking several filters to refine what you learn from the data, each layer focusing on learning more detail.\n\nNeural Networks are the foundation of deep learning. They are computing systems vaguely inspired by the biological neural networks that constitute animal brains. An artificial neural network consists of a collection of connected units called neurons, which process data in layers. The first layer takes in the input data, and each subsequent layer builds a more abstract representation of the data, with the final layer producing the output.\n\nIn recent years, the rapid development of machine learning (ML) and deep learning (DL) and information fusion has made it possible to give machines/computers the ability to understand, identify and analyse emotions. Emotion recognition has attracted increasing interest from researchers in different fields. Human emotions can be recognized through facial expressions, speech, behaviour (gestures/postures), or physiological signals.\n\nThe SVM-based ML decision-making has been proven the most effective and preferred emotion recognition model. The ability of DL models to automatically extract and classify deep features is gaining popularity and has been increasing in the usage of CNN models  [36] .",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Openpose And Its Contributions",
      "text": "OpenPose has revolutionized the field of human-computer interaction by providing a tool for real-time multi-person 2D pose estimation. Developed at Carnegie Mellon University, it has been instrumental in advancing research in various domains, including emotion recognition. The original work introducing OpenPose offered a method uses a nonparametric representation, which referred as Part Affinity Fields (PAFs), to learn to associate body parts with individuals in the image, allowing for effective pose estimation in complex scenes with multiple people  [39] .\n\nBecause OpenPose can identify subtle body gestures and poses that are indicative of emotional states, its application in emotion recognition is compelling. Its use extends beyond the capabilities of facial emotion recognition, particularly in scenarios where the face may not be visible or expressions are subtle and thus harder to detect. For instance, research has employed OpenPose to analyse body gestures in conjunction with deep learning models for emotion recognition, a novel approach is proposed to use deep neural network fuse skeleton and RGB features only using single-modality RGB video data. Experimental results show them approach achieves substantial improvements both in individual categories and overall and is provided with stronger generalization capability as well  [38] .\n\nBased on the capability of OpenPose, I can get the keypoints of a person in picture. OpenPose can process each frames in a video, therefore I can get images from each video's frames. Then use OpenPose to process each images to get the keypoints of person in the images. The keypoints will shows the body poses with lines with different colours and also detect the outline of their face. Thus I can get the images that contains the body pose with the face outlines and disable the original image just keep the keypoints, which will become dataset for training and validating. Figure  1  shows a example of processing image via OpenPose.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Databases For Emotion Recognition",
      "text": "The Aff-Wild2 database, an extension of the original Aff-Wild dataset, is the first to capture real-world affect in-the-wild. It offers a substantial volume of data, including videos download from Youtube with associated annotations for valence and arousal. The database's annotations of the seven basic emotions and continuous valence-arousal labels allow for both classification and regression problem formulations in emotion recognition tasks  [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 40] .\n\nThis dataset consisting of more than 500 videos, having 2,000,000 more frames, all of person in video which have been annotated. The videos in the dataset are captured in unconstrained settings, with varying illumination conditions, head poses, occlusions, and backgrounds, making it more challenging and realistic than controlled laboratory settings.\n\nDFEW (Dynamic Facial Expression in the Wild) is another prominent dataset that provides a collection of facial expressions captured in natural settings. With its large-scale, DFEW can be served as a benchmark for researchers to develop and evaluate their methods for dealing with dynamic facial expression recognition (FER) in the wild  [27] .\n\nThis dataset contains over 16000 videos from thousands of movies, these video clips contain various challenging interference in practical scenarios such as extreme illumination, occlusions, and capricious pose changes  [27] . These videos will provide lots of images with different emotions. The videos in the DFEW dataset are collected from popular movies, which means that the facial expressions are captured in various unconstrained settings, with different illumination conditions, occlusions, and head poses.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Emotional States",
      "text": "The classification of emotions into discrete categories, particularly the basic emotions of anger, disgust, fear, happiness, sadness, surprise, and neutral, has served as a foundational element in the field of affective computing. This traditional approach makes use of the classification of emotions into these seven discrete states in order to expedite the creation of computational models that are capable of identifying and interpreting human emotions. Supervised learning approaches, a subset of machine learning techniques, are ideally suited to this classification job. Supervised learning algorithms are best suited in scenarios where labelled datasets are abundant since they require a dataset containing labelled examples from which to learn.\n\nIn the context of emotion recognition, datasets often consist of videos or images of individuals expressing various emotions. These expressions are annotated with labels that correspond to the basic emotions. For example, annotations are assigned numerical values from 1 to 7, each number representing a different basic emotion. The mapping of numbers to emotions is typically as follows: 1 for happiness, 2 for sadness, 3 for neutral (no strong emotion displayed), 4 for anger, 5 for surprise, 6 for disgust, and 7 for fear. This numerical labelling system streamlines the process of training supervised learning models by providing a clear, structured format for the emotion data.\n\nFurthermore, there's a new way to determine the emotion which is valence and arousal. This way gives each emotion two numbers between -1 to 1. These two numbers represent two axes. Valence represents the emotion is positive or negative, whereas arousal refers to its intensity, from low to high. For example, an emotion with high arousal and positive valence will be happy, an emotion with low arousal and negative valence will be sad. Based on the value of valence and arousal, we can classify the 7 basic emotion states out.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Resnet",
      "text": "Deep convolutional neural networks have led to a series of breakthroughs for image classification. Deep networks naturally integrate low / mid / high level features and classifiers in an end-to-end multilayer fashion, and the \"levels\" of features can be enriched by the number of stacked layers (depth). Driven by the significance of depth, a question arises: Is learning better networks as easy as stacking more layers? An obstacle to answering this question was the notorious problem of vanishing/exploding gradients, which hamper convergence from the beginning  [41] .\n\nResNet provide a brilliant method to prevent vanishing/exploding gradients with deep convolution neural networks.\n\nTo integrate the learned residual g(x) with the input x, ResNet uses a shortcut connection (also known as a skip connection) that bypasses one or more layers. The output of the residual block is then f(x) = g(x) + x, where g(x) is the output from the learned residual function and x is the original input. This mechanism allows the network to adjust the magnitude of the residual, and if the input is already close to the desired output, the network can effectively push g(x) towards 0, making f(x)≈ x.\n\nThis residual learning framework has a profound implication: it allows the neural network to be significantly deeper without the training becoming unmanageable due to vanishing or exploding gradients. If g(x) is approximately to 0, Figure  3 : Figure  3  Architecture of DenseNet the f(x) is approximated, indicating that the added layers do not contribute significant changes to the input. This capability not only facilitates the training of much deeper networks by effectively alleviating the vanishing gradient problem but also ensuring that the additional layers do not degrade the network's performance. Since it publishes, ResNet has become a foundational architecture for many computer vision tasks, setting new benchmarks for performance and efficiency. Its principle of learning residual functions has been adapted and extended to various other types of neural networks, illustrating the versatility and impact of this approach.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Densenet",
      "text": "DenseNet introduces direct connections between any two layers with the same feature-map size. The paper showed that DenseNets scale naturally to hundreds of layers, while ex-habiting no optimization difficulties  [42] .\n\nDense Connection is a connection method in the DenseNet network architecture that allows each layer to receive feature maps as input from all previous layers and pass its own learned feature maps to all subsequent layers. Refer to figure above, if a layer is an X2 layer, it will receive the feature maps of all previous layers X1, X0 as input. This layer performs convolution operations and other operations on the basis of its input feature map to obtain a new feature map. The output feature map of this layer is directly passed to all subsequent layers X3, X4 as part of the input feature map.\n\nThis dense connection method is different from the connection between layers of traditional convolutional networks, where the previous layer is connected to the next layer. Dense connections maximize the reuse of features, and the model can more efficiently combine all features learned in previous layers. So DenseNet enhanced feature propagation helps to better propagate gradients, thereby alleviating the vanishing gradient problem. For the same computing resources, narrower and deeper networks can be built to achieve better accuracy with it.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "What Have Been Done",
      "text": "Classifying correct emotions from different data sources such as text, images, videos, and speech has been an area of research for researchers in various disciplines. Automatic emotion detection from videos and images is one of the most challenging tasks to analyse using supervised and unsupervised machine learning methods.\n\nIn a paper, facial and body features extracted by the OpenPose tool have been used for detecting basic 6, 7 and 9 emotions from videos and images by a novel deep neural network framework which combines the Gaussian mixture model with CNN, LSTM and Transformer to generate the CNN-LSTM model and CNN-Transformer model with and without Gaussian centres  [43] . This paper using two benchmark datasets, namely FABO and CK+. It reported over 90% accuracy for most combinations of features for both datasets.\n\nAnother paper, In this paper, therefore, we report the multi-view emotional expressions dataset (MEED) using 2D pose estimation  [37] . They got twentytwo actors presented six emotional (anger, disgust, fear, happiness, sadness, surprise) and neutral body movements from three viewpoints (left, front, right). A total of 4102 videos were captured.\n\nA paper introduce emotional body gestures as a component of what is commonly known as \"body language\" and comment general aspects as gender differences and culture dependence  [34] . They also discuss multi-modal approaches that combine speech or face with body gestures for improved emotion recognition.\n\nTogether, these studies illustrate the dynamic evolution of emotion detection technologies, showcasing a trend towards more integrated, multi-modal systems that can adapt to and accurately interpret the complex interplay of human expressions and gestures across diverse settings.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Summary",
      "text": "In order to comprehend human emotions, emotion recognition has developed into a sophisticated field that integrates multiple technologies. Due to the direct display of emotions in speech and facial expressions, traditional emotion identification techniques mostly use these methods. Furthermore, because physiological signals reliably indicate subconscious emotional states, such as EEG, ECG, GSR, and ET, they are used to quantify emotional responses.\n\nDespite advancements in recognizing emotions from facial and physiological signals cues, the role of body gestures remains less explored. Recent studies, like the use of 2D pose estimation, have demonstrated that body gestures can sometimes express emotional information more effectively than facial expressions or speech. OpenPose has notably advanced this area by enabling the analysis of body gestures, which is critical for emotion recognition, especially in settings where facial expressions are not visible.\n\nThe application of machine learning and deep learning has been critical in enhancing emotion recognition systems. Techniques such as Support Vector Machines (SVM) and Convolutional Neural Networks (CNN) are particularly effective in processing and classifying image data. The introduction of ResNet has further improved deep learning models by mitigating the gradient vanishing problem, thus facilitating the training of deeper neural networks.\n\nImportant contributions have also been made by the specialised databases DFEW and Aff-Wild2, which provide extensive video datasets captured in videos and movies. These resources are invaluable for training and validating emotion recognition algorithms under realistic conditions. The classification of emotions into 7 discrete categories such as anger, disgust, and happiness, in other hand newer methods that assess emotional valence and arousal, provides an extensive framework for understanding and interpreting human emotions.\n\nIn general, the field of emotion recognition is moving towards more integrated, multimodal systems that use traditional and innovative methods to capture the subtle and complex aspects of human emotions. This will enhancing applications across various domains including interactive gaming, psychological analysis, assistive technologies, and security systems.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Openpose",
      "text": "OpenPose is a versatile computer vision tool that specializes in real time multiple person keypoint detection. This technology can analyse videos to identify human body, face, hand, and foot keypoints, providing detailed positional data in keypoints for each frame. The results from OpenPose can be outputted in two distinct formats: images and JSON files.\n\nIn the case of the image format, OpenPose generates image where the detected keypoints are highlighted with coloured points and lines on a black background. This approach focuses only on the keypoints, discarding the original background and other non-keypoint elements of the video frame. This method helps in enhancing the keypoints data, making it easier to analyse face expression and body gesture without the distraction of the original frame's content.\n\nFor the JSON format output, OpenPose provides a structured data file containing detailed information about the keypoints, including their coordinates in each video frame. This JSON output is particularly useful for further computational analysis and can be seamlessly integrated as input into various neural network models.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Pre-Processing",
      "text": "Initially, video files are processed using OpenPose, it will extract human face and body keypoints from video frames. This results in each video being broken down into a series of individual frame images, stored in separate folders corresponding to each video name.\n\nTwo different datasets are involved, each providing numerical labels that correspond to seven different emotions expressed in the videos. Based on these labels, the frame images from each video are then categorized into one of seven designated folders. Each folder represents one of the seven emotions being trained.\n\nOnce the images are sorted into these emotion-specific folders, it's important to locate the issue of redundancy of the data. Because the original video frames are captured in quick succession, consecutive frames often display very similar gestures and expressions due to the continuous nature of video recording. This high similarity between nearby frames can lead to redundancy, which is not ideal for training machine learning models that perform better with diverse data samples.\n\nTo mitigate this, a subsampling technique need to be utilized. Specifically, I chose to retain only every tenth frame from each video. This approach significantly reduces the total number of images while ensuring that the remaining frames are spaced out enough to capture varied gestures and face expressions, thus maintaining the diversity necessary for effective model training.\n\nBy restructuring the data in this way-organizing it into emotion-specific categories and reducing redundancy through subsampling-the pre-processed data is better suited for subsequent analytical tasks.\n\nAfter the subsampling, frame images will be categorized to seven different emotion folders based on the labels.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Inputting Datasets",
      "text": "For importing the images, start by defining the location of the training dataset and the validation dataset. Utilize the dataset.ImageFolder() function to import these datasets while applying data augmentation techniques simultaneously such as rotation, horizontal flip, and normalization. This approach not only simplifies the process of handling dataset but also enhances the dataset by introducing variability and improving model generalization capabilities.\n\nFor importing the JSON files, particularly those containing keypoints data, the initial step is to examine the JSON structure to locate the 'people' key. Subsequently, focus on extracting 'pose keypoints 2d' and 'face keypoints 2d' from the 'people' key. These keys contain valuable data on the positional coordinates of body and facial keypoints, which are essential for tasks that require precise human gestures and facial expression analysis.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Showing Images",
      "text": "To effectively manage and inspect a dataset of images labeled with emotional expressions, it is beneficial to display the images alongside their labels using two distinct labeling systems.\n\nThe first system categorizes 7 emotions into one of seven basic categories from label 0-6. This categorical system simplifies the complexity of human emotions into easily 7 distinguishable groups, useful for basic emotional analysis. The second labeling system employs a two-dimensional approach where each image is associated with two values which are valence and arousal. Valence measures the pleasantness of the emotion, ranging from negative to positive, whereas arousal measures the intensity of the emotion, from low to high. This system provides a more nuanced understanding of emotional states, capturing the subtleties in how emotions are experienced.\n\nWhen importing these datasets, it is critical to visually inspect the images alongside their labels to check if the labeling correct and ensure that there are no discrepancies or errors in the dataset and label. Any mismatches identified during this inspection can then be addressed to improve the reliability and usability of the dataset for further emotional analysis.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Defining Model",
      "text": "In this project, I used three types of neural network architectures: Artificial Neural Network (ANN), ResNet18 with a modified structure and ResNet18 pretrained on ImageNet, and a pre-trained ResNet50 model trained on ImageNet.\n\nFor the ANN, I designed the architecture to include four linear fully connected layers. These layers were connected using ReLU (Rectified Linear Unit) activation functions, which import non-linear function into the network and help prevent the vanishing gradient problem during training. The purpose of this design was to ensure a robust feature extraction from the input data, culminating in the final layer. This last layer is crucial as it outputs the probabilities across seven different emotions, hence the output channel was set to seven. This setup is particularly aimed at facilitating the model's capability to perform multi-class classification of emotional states from the input data.\n\nMoving to the ResNet18 architecture, I implemented a variant of the standard model to better suit the specific requirements. Each residual block of the network contains two convolutional layers. These convolutional layers are followed by batch normalization layers, which stabilize and accelerate the training process by normalizing the inputs to each layer. A key feature of ResNet architectures is the inclusion of skip connections that help in alleviating the problem of training deep networks by allowing gradients to flow through the network without significant vanishing. The network structure is completed by sequentially connecting these residual blocks with additional convolutional layers and a classification layer, which collectively function to effectively capture and interpret the hierarchical features of the data.\n\nLastly, for the ResNet50 model, I utilized a pre-trained version available through the PyTorch 'models' module by executing the following command: models.resnet50(pretrained=True). This model is initially trained on ImageNet, a large visual database designed for use in visual object recognition software research. By utilizing a pre-trained model, we can use the learned weights as a starting point, thus saving significant training time and computational resources. To fits this model to my task of emotion prediction, I modified the final layer to have an output size of seven to correspond to my seven emotion categories. This adaptation allows the ResNet50 model to output refined predictions specifically for my dataset.",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "Hyperparameter",
      "text": "In the process of fine-tuning the hyperparameters for different machine learning tasks, distinct strategies were employed for classification and regression problems, primarily concerning the choice of loss functions. For the classification tasks, I opted to use the CrossEntropy loss function, which is well-suited for problems involving multiple classes as it effectively handles the probabilistic interpretation of class predictions. Here's a formula of CrossEntropy below:\n\nH represents CrossEntropy, p is real label of classes while q is the predict output of this classes. Each predicted class will compare to real label of 0 or 1. The calculated loss penalizes the probability based on how far it is from the expected value. Then the penalty is logarithmic, which means there's a large score for more difference close to 1 and small score for less difference close to 0.\n\nConversely, for regression tasks, the Mean Squared Error (MSE) loss was utilized, which quantifies the average of the squares of the errors-that is, the average squared difference between the estimated values and the actual value. Here's a formula of MSELoss below:\n\nIt calculates the squared difference between the predicted value and the actual value. Then then averages these squared differences across N which is number of samples. MSE is great for ensuring that the model we train does not have outlier predictions with huge errors because MSE gives more weight to these errors due to the squared part of the function.\n\nAdjustments to the learning rate followed the selection of the loss function, as this parameter critically influences the convergence speed and quality of the training process. An inappropriate learning rate can either lead to very slow convergence or cause the training to oscillate without stabilizing.\n\nAdditionally, the choice of optimizer played a crucial role in the network's performance. Various optimizers were tested, including Stochastic Gradient Descent (SGD), Adam, and Adamax. Among these, Adam yielded the best results, likely due to its adaptive learning rate capabilities, which help in handling sparse gradients on noisy problems.\n\nAdam is an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. Furthermore, AdaMax is a variant of Adam based on the infinity norm  [44] .\n\nLastly, learning rate schedulers were implemented to further optimize the training process. Specifically, the OneCycleLR and ReduceLROnPlateau strategies were tried. OneCycleLR aims to adjust the learning rate according to a one-cycle policy that oscillates between a lower bound and an upper bound, potentially leading to faster convergence. The ReduceLROnPlateau scheduler decreases the learning rate when a metric has stopped improving, which ideally reduces overfitting and leads to a more robust model. However, these schedulers did not result in significant improvements in the context of the tasks, suggesting that the benefits of these approaches might be context-dependent or require further adjustment to the hyperparameters.",
      "page_start": 14,
      "page_end": 15
    },
    {
      "section_name": "Implementation",
      "text": "",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Artificial Neural Network (Ann)",
      "text": "The artificial neural network (ANN) I implemented consists of a simple architecture with four linear layers. The network is designed to process input data provided in a JSON file, which mainly contains keypoints.\n\nAs data goes through the network, it through four different linear layers. The main goal of these layers is to classify input data into one of seven basic emotional states. This classification is based on patterns and relationships learned by the network from training data involving emotional cues in the keypoints.\n\nIn addition to classifying emotional states, the network can also evaluates valence and arousal, two dimensions commonly used to describe emotions. To achieve this, the input data is first passed through three separate linear layers dedicated to processing these specific features. Thereafter, the processed data are further refined through two additional linear layers. The output of these two layers is simply output a value for each layer representing valence and arousal. The specific task of these layers is to predict valence and arousal values based on input keypoints.\n\nUsing multiple linear layers in this way enables artificial neural networks to learn from complex high-dimensional data sets and make accurate predictions of emotional state, valence, and arousal. This structured approach enables the network to effectively differentiate between different emotions and assess their intensity and pleasantness, providing valuable insights for applications requiring emotion recognition.\n\nResidual Neural Network (ResNet)\n\nThe architecture I've implemented known as ResNet18. Comprises a total of 18 distinct layers designed to enhance the model's performance and efficiency in processing visual data. The initial layer, the Layer0, is a convolutional layer that serves as the entry point for the input images. This layer is crucial as it performs the initial processing and transformation of the input.\n\nBetween the initial and final layers, the model is structured into four main layers-Layer1 through Layer4. Each of these layers contains two residual blocks, making up the core mechanism that enables ResNet models to train more effectively and avoid issues related to deep networks such as vanishing gradients. A residual block is specially designed to allow activations to propagate through the network more effectively by incorporating skip connections that skip two layers in my model. Within each residual block from Layer1 to Layer4, there are two convolutional layers. These layers are responsible for extracting and refining features from the input data, with each subsequent layer building on the output of the previous layer. The design of these blocks and their shortcut connections help reduce information loss during training, ensuring that the network learns effectively even as it deepens.\n\nFollowing Layer4, the architecture includes a fully connected layer, which typically comes towards the end of the network. This layer is essential for integrating and interpreting the features extracted by the convolutional layers, making high-level decisions about the input data based on the learned features.\n\nIn classification task, it should be 7 which represent 7 basic emotion states. Then for valence and arousal, the output channel of fully connected layer should be 2 which are values of valence and arousal.\n\nSumming up all the convolutional layers from Layer1 to Layer4 results in 16 convolutional layers. When combined with the initial Layer0 and the fully connected layer, the total comes to 18 layers, hence the name ResNet18. This architecture is known for its robustness and efficacy in handling complex visual tasks by leveraging deep learning capabilities while maintaining manageable computational costs.\n\nFor ResNet50, which is quite similar with ResNet18. The difference is ResNet50 get deeper and more layers. ResNet50 use more residual blocks which means more convolutional layers. But it also requests higher computational resource and has a bigger model size. Due to this, it generally achieves better performance on both classify and regression tasks due to its increased depth and capacity to learn more intricate features. So, it will get better result than ResNet18.\n\nFor DenseNet121, compared to ResNet18 and ResNet50, DenseNet121 is a much deeper network, with more layers and a different connectivity pattern. While ResNet use skip connections to mitigate the vanishing gradient problem, DenseNet employ dense connectivity, which has proven to be an effective way to train very deep neural networks. In terms of performance, DenseNet121 generally outperforms ResNet18 and ResNet50 in my project. The F1 score is a measure of a model's performance, commonly used in fields like machine learning and information retrieval. It combines the precision and recall metrics into a single score. Precision measures how many of the predicted positive cases were correct. Recall measures how many of the actual positive cases were correctly identified. Then use F1 = 2 * (precision * recall) / (precision + recall) to calculate the F1 score. The CCC (Concordance Correlation Coefficient) score is a metric used to evaluate the performance of regression models by measuring the agreement between predicted and real values.\n\nσ 2 means variance and µ means mean in this equation.\n\nIn general, deeper model get better result. And Adam always gets best result faster than SGD. And Adam usually gets better result because Adam can adjust the learning rates based on the average of the second moments of the gradients. So the pre-trained DenseNet121 get the best result. Lastly the effectiveness of SGD can be notably enhanced by fine-tuning the learning rate. So sometimes SGD can get better result by adjusting learning rate.",
      "page_start": 15,
      "page_end": 18
    },
    {
      "section_name": "Evaluation",
      "text": "",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Strength Of Projects",
      "text": "The project use OpenPose, an advanced tool capable of extracting keypoints from videos of individual person, OpenPose will focus exclusively on the body and facial keypoints while discarding other potentially distracting elements like the original image. The extraction is done in two distinct ways: firstly, by rendering coloured keypoints over a plain black background in an image format, and secondly, by saving the keypoints' coordinates in JSON file. These two approaches not only streamline the data extracting but also enhances processing efficiency.\n\nTo enhance the richness of the dataset, I implement two large datasets, Aff-Wild2 and DFEW, which provide a total of approximately 4,000,000 video frames covering sufficient variation in the seven basic emotional states. Such a large amount of data ensures a comprehensive sampling of human emotional expressions, thereby facilitating robust model training.\n\nIn the pre-processing phase, I employed techniques such as subsampling and classification of the images into seven distinct categories. This methodological categorization helps minimize redundancy and prevents potential I/O errors when handling large volumes of image and JSON data. Following preprocessing, I utilized the images with coloured keypoints for training a convolutional neural network using ResNet architectures (specifically ResNet18 and ResNet50). For the JSON data, a three-layer artificial neural network was used. By comparing the effectiveness of two optimizers, SGD and Adam, within the context of the Cross Entropy(classify) and MSE (regression) loss function, it was determined that the Adam optimizer exhibited superior capability in feature learning, hence optimizing our neural network training.",
      "page_start": 18,
      "page_end": 19
    },
    {
      "section_name": "Weakness Of Projects",
      "text": "The project is reliance on OpenPose brings inherent limitations, particularly in terms of accuracy and completeness of extracted keypoints. Sometimes Open-Pose is unable to detect certain parts of the body or face, resulting in an in-complete representation of the data, such as frames with complete loss of key keypoints resulting in a completely black picture.\n\nProcessing video frame by frame using OpenPose also results in a quite high similarity between consecutive frames. This similarity poses a significant challenge in maintaining the diversity of the training dataset, as many frames do not differ enough from one another. To alleviate this problem, I implemented secondary sampling, selecting every ten frames to reduce redundancy and enhance dataset diversity. However, challenges remain even after secondary sampling, mainly because many of the frames derived from successive video sequences contain transitional or less expressive emotional states that are less obvious and therefore less useful for training.\n\nThe project initially divided emotions into seven categories, consistent with traditional seven emotion states. However, due to the consecutive frames cause highly similarity between nearby frames and due to OpenPose there're lots of invalid frames. To address these challenges, I turned my focus to using valence and arousal as a dimensional approach to continuous output, turning the problem into a regression task. This change allows the network to predict two values, which are then used to set thresholds for seven basic emotions, providing a potentially more nuanced understanding of emotional states than just categorical categories.",
      "page_start": 18,
      "page_end": 19
    },
    {
      "section_name": "Summary Of Evaluation",
      "text": "According to the results from model, the main issue is low accuracy. That's because high similarity among consecutive frames, processing video frames sequentially with OpenPose resulted in a high degree of similarity among them, which hindered the diversity of the training dataset. Despite attempts to mitigate this issue through subsampling (selecting every tenth frame), the problem persisted. Many frames did not sufficiently differ from each other, particularly those depicting transitional or less expressive emotional states.\n\nInitially, the project categorized emotions into seven discrete classes based on traditional emotion theory. This approach faces difficulties due to the indistinguishable and large proportion of emotion transitions between frames, resulting in low verification accuracy. So many similar images in different classes make the ResNet and DenseNet model cannot identify the difference, making it difficult to differentiate and correctly classify the emotions.\n\nTo solve this problem, I change my model from a classification task to a regression task, categorizing emotions based on valence and arousal as continuous outputs. This shift aimed to enhance the model's capability to capture a more accurate and more nuanced spectrum of emotional expressions. However, the there're lots of similar images, it still potentially improve the accuracy.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Conclusion",
      "text": "This project provided a good opportunity to use my background in computer science to dive into new areas across disciplines. Through this initiative, I was able to take a hands-on approach to the development of computer vision technology, methodically addressing challenges at each step of the process. As the project progressed, my passion for the area of computer vision grew further.\n\nThe main goal of the project is to develop a predictive model capable of recognizing seven different emotions: happiness, sadness, anger, fear, surprise, disgust and neutral states. I use OpenPose for image processing to extract features required for emotion recognition. The method first pre-processes the images and then organizes them into seven different folders corresponding to each emotion to facilitate the classification task. I also explore subsampling techniques to enhance the model's ability to handle regression tasks. However, due to the nature of the video-derived dataset (which mainly consists of sequential images), the sample lacks diversity even after a subsampling effort.\n\nIn the comparative analysis, I found that ResNet and DenseNet get better performance than traditional artificial neural networks (ANN) on both regression and classification tasks. Performance differences can be attributed to different datasets and structural differences in the models. In the classification task, it is clear that the model can more accurately predict certain emotions, such as \"neutral\" and \"happiness.\" This may be because these emotions exhibit unique and consistent cues that are easier for models to find out. In contrast, detect emotions such as fear and anger pose more significant difficult due to subtle differences in expression.\n\nFurthermore, when performance was assessed in terms of valence (the intrinsic attractiveness or aversiveness of an event) and arousal (the physiological and psychological state of being aroused or aroused), it was observed that these models generally exhibited lower loss rates, suggesting a greater sensitivity to emotional intensity.\n\nOverall, the application of ResNet and DenseNet in this project proved they are very effective for emotion recognition tasks. The use of the Adam optimizer enhances the performance of them, achieving the best results in both regression and classification of emotional states. I think that by adjusting the diversity of training data and further tuning model parameters, the accuracy of valence and arousal predictions may improve.",
      "page_start": 20,
      "page_end": 21
    },
    {
      "section_name": "Further Work",
      "text": "Possible further work could implement advanced subsampling techniques to selectively retain representative images from sequences of continuous actions., in order to reduce the similarity of images between different classes. Possible is using a better computer which will provide a better GPU or multiple GPUs rather than single GPU, can significantly reduce training and inference times. This could also include exploring cloud-based solutions or specialized hardware like TPU. Possible to use another CNN, like DenseNet201, which features densely connected convolutional networks, or Vision Transformer (ViT), which applies transformer mechanisms to image recognition, might yield better or faster results. Implementing techniques like grid search, random search, or Bayesian optimization to automate the selection of model hyperparameters, which could enhance model performance without extensive massive manual experimentation.\n\nThe proposed directions for further research in project systems focus on three areas to overcome the weakness of project.\n\nFirst, optimizing data handling, for example, subsampling gets developing more efficient methods for processing and managing data, which can lead to faster response time and reduce cost of computational resource. This might include the implementation of advanced data subsampling or the use of more complex data pre-processing techniques to enhance the quality of the data.\n\nSecond, experimenting with more advance neural network architectures is crucial for improving the accuracy of this system. This might involve utilizing new forms of deep learning models, such as recurrent neural networks (RNN) or Generative Pre-trained Transformer (GPT), By modifying existing architectures or creating new ones, model can be better capturing the nuances of emotional expressions, potentially leading to breakthroughs with this project.\n\nFinally, these strategies aim to optimize the performance of project by making them faster, more accurate, and capable of processing larger and more diverse datasets. These improvements are particularly important for applications that demand real-time processing and high levels of precision, such as customer service analysis, and healthcare monitoring, where rapid and reliable emotion detection is crucial.",
      "page_start": 20,
      "page_end": 21
    },
    {
      "section_name": "Acknowledgements",
      "text": "A special thank you to my supervisor Dr Dimitrios Kollias, for his guidance, expertise, and encouragement throughout the project.\n\nI wish to extend a special thanks to my parents and my little sister for the constant love, inspiration, and support throughout. Which enabled me to dedicate my time and effort to this project. As well as my friends for continuous solace and motivation.",
      "page_start": 21,
      "page_end": 21
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Figure 1 Processed Frame Image by OpenPose",
      "page": 6
    },
    {
      "caption": "Figure 1: shows a example of processing image via",
      "page": 6
    },
    {
      "caption": "Figure 2: Figure 2 Architecture of Residual Block",
      "page": 8
    },
    {
      "caption": "Figure 3: Figure 3 Architecture of DenseNet",
      "page": 9
    },
    {
      "caption": "Figure 4: Figure 4 Detailed Architecture of ResNet18",
      "page": 16
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1\nAbstract": "The project leverages advanced machine and deep learning techniques to address"
        },
        {
          "1\nAbstract": "the challenge of emotion recognition by focusing on non-facial cues, specifically"
        },
        {
          "1\nAbstract": "hands, body gestures, and gestures. Traditional emotion recognition systems"
        },
        {
          "1\nAbstract": "mainly rely on facial expression analysis and often ignore the rich emotional"
        },
        {
          "1\nAbstract": "information conveyed through body language. To bridge this gap, this method"
        },
        {
          "1\nAbstract": "leverages\nthe Aff-Wild2 and DFEW databases\nto train and evaluate a model"
        },
        {
          "1\nAbstract": "capable of\nrecognizing seven basic\nemotions\n(angry, disgust,\nfear, happiness,"
        },
        {
          "1\nAbstract": "sadness,\nsurprise, and neutral) and estimating valence and continuous\nscales"
        },
        {
          "1\nAbstract": "wakeup descriptor."
        },
        {
          "1\nAbstract": "Leverage OpenPose for pose estimation to extract detailed body posture and"
        },
        {
          "1\nAbstract": "posture features from images and videos. These features serve as input to state-"
        },
        {
          "1\nAbstract": "of-the-art neural network architectures, including ResNet, and ANN for emotion"
        },
        {
          "1\nAbstract": "classification, and fully connected layers for valence arousal regression analysis."
        },
        {
          "1\nAbstract": "This bifurcation strategy can solve classification and regression problems in the"
        },
        {
          "1\nAbstract": "field of emotion recognition."
        },
        {
          "1\nAbstract": "The project aims\nto contribute to the field of affective computing by en-"
        },
        {
          "1\nAbstract": "hancing the ability of machines to interpret and respond to human emotions in"
        },
        {
          "1\nAbstract": "a more comprehensive and nuanced way. By integrating multimodal data and"
        },
        {
          "1\nAbstract": "cutting-edge computational models, I aspire to develop a system that not only"
        },
        {
          "1\nAbstract": "enriches human-computer interaction but also has potential applications in areas"
        },
        {
          "1\nAbstract": "as diverse as mental health support, educational\ntechnology, and autonomous"
        },
        {
          "1\nAbstract": "vehicle systems."
        },
        {
          "1\nAbstract": "Keywords – Aff-Wild2, DFEW, ResNet, DenseNet, ANN, Valence and Arousal"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "and intuitive ways\nfor machines\nto understand and respond to human needs.": "A key aspect of\nthis\ninteraction is\nemotion recognition, which is\nthe process"
        },
        {
          "and intuitive ways\nfor machines\nto understand and respond to human needs.": "of recognizing human emotions through various means,\nincluding facial expres-"
        },
        {
          "and intuitive ways\nfor machines\nto understand and respond to human needs.": "sions, speech, and, as the focus of this project, body gestures. Historically, facial"
        },
        {
          "and intuitive ways\nfor machines\nto understand and respond to human needs.": "expressions have been the primary conduit for emotion recognition systems, ex-"
        },
        {
          "and intuitive ways\nfor machines\nto understand and respond to human needs.": "ploiting the nuances of the human face to map to universally recognized basic"
        },
        {
          "and intuitive ways\nfor machines\nto understand and respond to human needs.": "emotions: anger, disgust,\nfear, happiness, sadness, surprise, and neutral states."
        },
        {
          "and intuitive ways\nfor machines\nto understand and respond to human needs.": "However, this approach ignores the rich nonverbal cues humans display through"
        },
        {
          "and intuitive ways\nfor machines\nto understand and respond to human needs.": "body language."
        },
        {
          "and intuitive ways\nfor machines\nto understand and respond to human needs.": "Recent explorations in the field of affective computing have shown that body"
        },
        {
          "and intuitive ways\nfor machines\nto understand and respond to human needs.": "gestures can tell as much,\nif not more, about a person’s emotional state as facial"
        },
        {
          "and intuitive ways\nfor machines\nto understand and respond to human needs.": "expressions alone.\nThese non-facial\ncues\ncan provide additional\ncontext and"
        },
        {
          "and intuitive ways\nfor machines\nto understand and respond to human needs.": "improve\nthe accuracy of\nemotion recognition systems,\nespecially when facial"
        },
        {
          "and intuitive ways\nfor machines\nto understand and respond to human needs.": "expressions are ambiguous or\nsocial conditions mask true feelings. Therefore,"
        },
        {
          "and intuitive ways\nfor machines\nto understand and respond to human needs.": "emotion recognition is crucial\nin the application areas\nsuch as marketing, hu-"
        },
        {
          "and intuitive ways\nfor machines\nto understand and respond to human needs.": "man–robot interaction, healthcare, mental health monitoring, and security [1]."
        },
        {
          "and intuitive ways\nfor machines\nto understand and respond to human needs.": "The study of emotions\nfor healthcare includes vast neurological disorders\nlike"
        },
        {
          "and intuitive ways\nfor machines\nto understand and respond to human needs.": "sleep disorders [2], evaluation of sleep quality [3]."
        },
        {
          "and intuitive ways\nfor machines\nto understand and respond to human needs.": "This project aims\nto harness\nthe untapped potential of body language as"
        },
        {
          "and intuitive ways\nfor machines\nto understand and respond to human needs.": "a medium for emotion recognition. By leveraging advanced machine and deep"
        },
        {
          "and intuitive ways\nfor machines\nto understand and respond to human needs.": "learning techniques,\nsuch as OpenPose for pose estimation, and complex neu-"
        },
        {
          "and intuitive ways\nfor machines\nto understand and respond to human needs.": "ral networks,\nsuch as ResNet, DenseNet, and Visual Transformers\n(ViT),\nthe"
        },
        {
          "and intuitive ways\nfor machines\nto understand and respond to human needs.": "project aims to classify of seven basic emotions. And estimate the valence and"
        },
        {
          "and intuitive ways\nfor machines\nto understand and respond to human needs.": "arousal emotion descriptors are derived from human body gestures."
        },
        {
          "and intuitive ways\nfor machines\nto understand and respond to human needs.": "The databases that will be used, Aff-Wild2 [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,"
        },
        {
          "and intuitive ways\nfor machines\nto understand and respond to human needs.": "15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33] and DFEW,"
        },
        {
          "and intuitive ways\nfor machines\nto understand and respond to human needs.": "provide rich datasets on which models can be trained to account for continuous"
        },
        {
          "and intuitive ways\nfor machines\nto understand and respond to human needs.": "emotional states,\nranging from highly positive to very negative (valence), and"
        },
        {
          "and intuitive ways\nfor machines\nto understand and respond to human needs.": "from very passive to highly active (arousal).\nClassification of basic emotions"
        },
        {
          "and intuitive ways\nfor machines\nto understand and respond to human needs.": "will serve as a discrete problem-solving task, while valence arousal estimation"
        },
        {
          "and intuitive ways\nfor machines\nto understand and respond to human needs.": "will solve a more complex regression problem."
        },
        {
          "and intuitive ways\nfor machines\nto understand and respond to human needs.": "2.2\nProblem Statement"
        },
        {
          "and intuitive ways\nfor machines\nto understand and respond to human needs.": "In the\nrapidly developing field of human-computer\ninteraction,\nthe accurate"
        },
        {
          "and intuitive ways\nfor machines\nto understand and respond to human needs.": "recognition and interpretation of human emotions plays a crucial role. Tradi-"
        },
        {
          "and intuitive ways\nfor machines\nto understand and respond to human needs.": "tional emotion recognition systems focus primarily on facial expressions, which,"
        },
        {
          "and intuitive ways\nfor machines\nto understand and respond to human needs.": "while effective, cannot provide a complete picture of the human emotion spec-"
        },
        {
          "and intuitive ways\nfor machines\nto understand and respond to human needs.": "trum.\nThis\nlimitation becomes especially apparent\nin situations where facial"
        },
        {
          "and intuitive ways\nfor machines\nto understand and respond to human needs.": "cues are invisible or influenced by social and cultural norms (like wear mask)."
        },
        {
          "and intuitive ways\nfor machines\nto understand and respond to human needs.": "Therefore, there is a huge gap in exploiting the full potential of non-verbal cues,"
        },
        {
          "and intuitive ways\nfor machines\nto understand and respond to human needs.": "especially body gestures, which are often overlooked in emotion recognition sys-"
        },
        {
          "and intuitive ways\nfor machines\nto understand and respond to human needs.": "tems."
        },
        {
          "and intuitive ways\nfor machines\nto understand and respond to human needs.": "Furthermore, most\nexisting emotion recognition models are build around"
        },
        {
          "and intuitive ways\nfor machines\nto understand and respond to human needs.": "discrete emotional\nstates, mainly the seven basic emotions. However, human"
        },
        {
          "and intuitive ways\nfor machines\nto understand and respond to human needs.": "emotions are more subtle and often exist on a spectrum, especially when con-"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "sidering the dimensions of valence and arousal. The lack of models that accu-": "rately explain and quantify these dimensions limits the applicability of emotion"
        },
        {
          "sidering the dimensions of valence and arousal. The lack of models that accu-": "recognition to domains\nthat\nrequire a more complex understanding of human"
        },
        {
          "sidering the dimensions of valence and arousal. The lack of models that accu-": "emotions, such as mental health assessment and adaptive learning systems."
        },
        {
          "sidering the dimensions of valence and arousal. The lack of models that accu-": "The challenge, therefore,\nis to develop a comprehensive emotion recognition"
        },
        {
          "sidering the dimensions of valence and arousal. The lack of models that accu-": "system that goes beyond only facial analysis\nto incorporate\nthe\nsubtleties of"
        },
        {
          "sidering the dimensions of valence and arousal. The lack of models that accu-": "body gesture to provide a more complete and accurate understanding of human"
        },
        {
          "sidering the dimensions of valence and arousal. The lack of models that accu-": "emotions.\nSuch a system must not only classify basic\nemotional\nstates but"
        },
        {
          "sidering the dimensions of valence and arousal. The lack of models that accu-": "must also leverage advanced machine and deep learning techniques to estimate"
        },
        {
          "sidering the dimensions of valence and arousal. The lack of models that accu-": "complex emotion descriptors of valence and arousal. The effective integration"
        },
        {
          "sidering the dimensions of valence and arousal. The lack of models that accu-": "of\nthese\ntechnologies has\nthe potential\nto revolutionize\nthe field of affective"
        },
        {
          "sidering the dimensions of valence and arousal. The lack of models that accu-": "computing and expand its applications in different fields."
        },
        {
          "sidering the dimensions of valence and arousal. The lack of models that accu-": "2.3\nAim"
        },
        {
          "sidering the dimensions of valence and arousal. The lack of models that accu-": "The purpose of\nthis project\nis\nto develop an emotion recognition system that"
        },
        {
          "sidering the dimensions of valence and arousal. The lack of models that accu-": "can identify human emotions with high accuracy by analysing body gestures and"
        },
        {
          "sidering the dimensions of valence and arousal. The lack of models that accu-": "facial expression. The system utilizes machine and deep learning methods and"
        },
        {
          "sidering the dimensions of valence and arousal. The lack of models that accu-": "is designed to surpass the capabilities of traditional\nfacial recognition models."
        },
        {
          "sidering the dimensions of valence and arousal. The lack of models that accu-": "It will cover\nthe seven basic emotional states, valence and arousal. With this"
        },
        {
          "sidering the dimensions of valence and arousal. The lack of models that accu-": "innovation, we seek to capture the subtlety and complexity of human emotions"
        },
        {
          "sidering the dimensions of valence and arousal. The lack of models that accu-": "and promote better understanding and interaction within the field of affective"
        },
        {
          "sidering the dimensions of valence and arousal. The lack of models that accu-": "computing."
        },
        {
          "sidering the dimensions of valence and arousal. The lack of models that accu-": "2.4\nObjectives"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2.4\nObjectives": "• Pose Estimation Integration: To integrate OpenPose to accurately capture"
        },
        {
          "2.4\nObjectives": "body gestures from visual data.\nIt capable of delineating individual body"
        },
        {
          "2.4\nObjectives": "parts and joint movements, which are\ncritical\nfor\ninterpreting physical"
        },
        {
          "2.4\nObjectives": "expressions related to emotional states."
        },
        {
          "2.4\nObjectives": "• Model Development: To develop and train deep learning models, such as"
        },
        {
          "2.4\nObjectives": "artificial neural networks (ANNs), ResNet, and Visual Transformers,\nfor"
        },
        {
          "2.4\nObjectives": "the classification of the seven basic emotions."
        },
        {
          "2.4\nObjectives": "• Emotion classification:\nTo construct a classify model\ncapable of 7 ba-"
        },
        {
          "2.4\nObjectives": "sic emotion states.\nInputs are from face expressions and body gestures,"
        },
        {
          "2.4\nObjectives": "through the model\nto determine the specific emotional\nstate among the"
        },
        {
          "2.4\nObjectives": "seven basic emotions."
        },
        {
          "2.4\nObjectives": "• Valence-Arousal Analysis: To construct a regression model capable of es-"
        },
        {
          "2.4\nObjectives": "timating valence and arousal\nlevels, providing a mapping of\nemotional"
        },
        {
          "2.4\nObjectives": "states."
        },
        {
          "2.4\nObjectives": "• Performance Evaluation: To rigorously evaluate the performance of\nthe"
        },
        {
          "2.4\nObjectives": "models using metrics such as accuracy, precision, and recall."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "and ensure it is robust against variations in environmental conditions.": "3\nLiterature Review"
        },
        {
          "and ensure it is robust against variations in environmental conditions.": "Emotion recognition has become a popular\nresearch topic in the past decade."
        },
        {
          "and ensure it is robust against variations in environmental conditions.": "While works based on facial expressions or speech abound, identifying the effects"
        },
        {
          "and ensure it is robust against variations in environmental conditions.": "of body posture\nremains a less\nexplored topic\n[34], This\nchapter provides a"
        },
        {
          "and ensure it is robust against variations in environmental conditions.": "detailed analysis of the impact of associate technology and provides information"
        },
        {
          "and ensure it is robust against variations in environmental conditions.": "on existing technologies in place in order to gather requirements for the proposed"
        },
        {
          "and ensure it is robust against variations in environmental conditions.": "solution. Then what lessons can be learned from these approaches?"
        },
        {
          "and ensure it is robust against variations in environmental conditions.": "3.1\nEmotion Recognition Techniques"
        },
        {
          "and ensure it is robust against variations in environmental conditions.": "There are several\ntraditional approaches to process emotion recognition. Like"
        },
        {
          "and ensure it is robust against variations in environmental conditions.": "from speech and facial expressions are the most employed mechanisms for emo-"
        },
        {
          "and ensure it is robust against variations in environmental conditions.": "tion identification among physical\nsignals\n[35].\nAfter\nthat, Physiological Re-"
        },
        {
          "and ensure it is robust against variations in environmental conditions.": "sponse Measurement\nis\na\ngood approach to\nrecognize\nemotion.\nFor\nexam-"
        },
        {
          "and ensure it is robust against variations in environmental conditions.": "ple,\nelectroencephalogram (EEG),\nelectrocardiogram (ECG),\nelectromyogram"
        },
        {
          "and ensure it is robust against variations in environmental conditions.": "(EMG), galvanic\nskin response\n(GSR),\nrespiration (RSP),\nskin temperature,"
        },
        {
          "and ensure it is robust against variations in environmental conditions.": "photoplethysmography, and eye tracking (ET) are the most employed physio-"
        },
        {
          "and ensure it is robust against variations in environmental conditions.": "logical\nsignals\nfor emotion recognition [35]. Among physiological\nsignals,\nthe"
        },
        {
          "and ensure it is robust against variations in environmental conditions.": "most often utilized modalities\nfor detecting human emotions are EEG, GSR,"
        },
        {
          "and ensure it is robust against variations in environmental conditions.": "ECG, and ET [36].\nFurthermore, body gesture.\nAs a crucial\ncomponent of"
        },
        {
          "and ensure it is robust against variations in environmental conditions.": "body language, also a good approach to recognize emotion."
        },
        {
          "and ensure it is robust against variations in environmental conditions.": "3.2\nEmotion Recognition from Body Gesture and Pose"
        },
        {
          "and ensure it is robust against variations in environmental conditions.": "Psychological research further shows that body gestures conveys nonverbal emo-"
        },
        {
          "and ensure it is robust against variations in environmental conditions.": "tional cues,\ninclude some part face and voice cannot express. However, recog-"
        },
        {
          "and ensure it is robust against variations in environmental conditions.": "nizing emotion based on body gestures remains less explored."
        },
        {
          "and ensure it is robust against variations in environmental conditions.": "Emotion recognition from body gesture is a field of study in artificial\nintel-"
        },
        {
          "and ensure it is robust against variations in environmental conditions.": "ligence and human-computer interaction focusing on the ability of machines to"
        },
        {
          "and ensure it is robust against variations in environmental conditions.": "identify and interpret human emotions based on physical expressions and move-"
        },
        {
          "and ensure it is robust against variations in environmental conditions.": "ments.\nThis\ntechnology uses\nsensors,\ncomputer vision, and machine learning"
        },
        {
          "and ensure it is robust against variations in environmental conditions.": "algorithms to analyse the posture, movement, and gestures of a person."
        },
        {
          "and ensure it is robust against variations in environmental conditions.": "The Multi-view emotional expressions dataset using 2D pose estimation as"
        },
        {
          "and ensure it is robust against variations in environmental conditions.": "featured in Nature emphasizes\nthe potency of body expressions\nin conveying"
        },
        {
          "and ensure it is robust against variations in environmental conditions.": "emotional shifts, sometimes even surpassing facial or vocal expressions in efficacy"
        },
        {
          "and ensure it is robust against variations in environmental conditions.": "[37]."
        },
        {
          "and ensure it is robust against variations in environmental conditions.": "The exploration of body gesture as a component of body language in emotion"
        },
        {
          "and ensure it is robust against variations in environmental conditions.": "recognition is less explored compared to face expression-based and speech-based"
        },
        {
          "and ensure it is robust against variations in environmental conditions.": "approaches. However, the works such as those presented in IEEE Xplore suggest"
        },
        {
          "and ensure it is robust against variations in environmental conditions.": "that deep learning can successfully be employed to recognize emotions from body"
        },
        {
          "and ensure it is robust against variations in environmental conditions.": "gestures, using large datasets of multi-view RGB videos for training [38]."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "This burgeoning interest in body gesture and pose for emotion recognition": "signifies a pivotal shift towards more comprehensive and possibly more natural-"
        },
        {
          "This burgeoning interest in body gesture and pose for emotion recognition": "istic modes of human affect analysis. As such,\nit holds considerable promise for"
        },
        {
          "This burgeoning interest in body gesture and pose for emotion recognition": "applications in areas ranging from interactive gaming to psychological analysis,"
        },
        {
          "This burgeoning interest in body gesture and pose for emotion recognition": "and from assistive technologies to security systems."
        },
        {
          "This burgeoning interest in body gesture and pose for emotion recognition": "3.3\nMachine and Deep Learning in Emotion Recognition"
        },
        {
          "This burgeoning interest in body gesture and pose for emotion recognition": "Machine learning is like teaching a computer to make decisions or predictions"
        },
        {
          "This burgeoning interest in body gesture and pose for emotion recognition": "based on past examples. You give the computer data, and it\nlearns patterns"
        },
        {
          "This burgeoning interest in body gesture and pose for emotion recognition": "from that data. While deep learning is a special way of doing machine learning"
        },
        {
          "This burgeoning interest in body gesture and pose for emotion recognition": "that involves layers of processing to understand complex patterns. Think of\nit"
        },
        {
          "This burgeoning interest in body gesture and pose for emotion recognition": "like stacking several filters\nto refine what you learn from the data, each layer"
        },
        {
          "This burgeoning interest in body gesture and pose for emotion recognition": "focusing on learning more detail."
        },
        {
          "This burgeoning interest in body gesture and pose for emotion recognition": "Neural Networks are the foundation of deep learning. They are computing"
        },
        {
          "This burgeoning interest in body gesture and pose for emotion recognition": "systems vaguely inspired by the biological neural networks that constitute an-"
        },
        {
          "This burgeoning interest in body gesture and pose for emotion recognition": "imal brains. An artificial neural network consists of a collection of connected"
        },
        {
          "This burgeoning interest in body gesture and pose for emotion recognition": "units called neurons, which process data in layers. The first layer takes in the"
        },
        {
          "This burgeoning interest in body gesture and pose for emotion recognition": "input data, and each subsequent layer builds a more abstract representation of"
        },
        {
          "This burgeoning interest in body gesture and pose for emotion recognition": "the data, with the final\nlayer producing the output."
        },
        {
          "This burgeoning interest in body gesture and pose for emotion recognition": "In recent years, the rapid development of machine learning (ML) and deep"
        },
        {
          "This burgeoning interest in body gesture and pose for emotion recognition": "learning (DL) and information fusion has made it possible to give machines/computers"
        },
        {
          "This burgeoning interest in body gesture and pose for emotion recognition": "the ability to understand,\nidentify and analyse emotions. Emotion recognition"
        },
        {
          "This burgeoning interest in body gesture and pose for emotion recognition": "has attracted increasing interest\nfrom researchers\nin different fields.\nHuman"
        },
        {
          "This burgeoning interest in body gesture and pose for emotion recognition": "emotions can be recognized through facial expressions, speech, behaviour (ges-"
        },
        {
          "This burgeoning interest in body gesture and pose for emotion recognition": "tures/postures), or physiological signals."
        },
        {
          "This burgeoning interest in body gesture and pose for emotion recognition": "The SVM-based ML decision-making has been proven the most effective and"
        },
        {
          "This burgeoning interest in body gesture and pose for emotion recognition": "preferred emotion recognition model. The ability of DL models to automatically"
        },
        {
          "This burgeoning interest in body gesture and pose for emotion recognition": "extract and classify deep features is gaining popularity and has been increasing"
        },
        {
          "This burgeoning interest in body gesture and pose for emotion recognition": "in the usage of CNN models [36]."
        },
        {
          "This burgeoning interest in body gesture and pose for emotion recognition": "3.4\nOpenPose and Its Contributions"
        },
        {
          "This burgeoning interest in body gesture and pose for emotion recognition": "OpenPose has revolutionized the field of human-computer interaction by provid-"
        },
        {
          "This burgeoning interest in body gesture and pose for emotion recognition": "ing a tool for real-time multi-person 2D pose estimation. Developed at Carnegie"
        },
        {
          "This burgeoning interest in body gesture and pose for emotion recognition": "Mellon University,\nit has been instrumental\nin advancing research in various do-"
        },
        {
          "This burgeoning interest in body gesture and pose for emotion recognition": "mains,\nincluding emotion recognition. The original work introducing OpenPose"
        },
        {
          "This burgeoning interest in body gesture and pose for emotion recognition": "offered a method uses a nonparametric representation, which referred as Part"
        },
        {
          "This burgeoning interest in body gesture and pose for emotion recognition": "Affinity Fields (PAFs), to learn to associate body parts with individuals in the"
        },
        {
          "This burgeoning interest in body gesture and pose for emotion recognition": "image, allowing for effective pose estimation in complex scenes with multiple"
        },
        {
          "This burgeoning interest in body gesture and pose for emotion recognition": "people [39]."
        },
        {
          "This burgeoning interest in body gesture and pose for emotion recognition": "Because OpenPose can identify subtle body gestures and poses that are in-"
        },
        {
          "This burgeoning interest in body gesture and pose for emotion recognition": "dicative of emotional states, its application in emotion recognition is compelling."
        },
        {
          "This burgeoning interest in body gesture and pose for emotion recognition": "Its use extends beyond the capabilities of\nfacial emotion recognition, particu-"
        },
        {
          "This burgeoning interest in body gesture and pose for emotion recognition": "larly in scenarios where the face may not be visible or expressions are subtle and"
        },
        {
          "This burgeoning interest in body gesture and pose for emotion recognition": "thus harder to detect. For instance, research has employed OpenPose to analyse"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 1: Figure 1 Processed Frame Image by OpenPose": "body gestures in conjunction with deep learning models for emotion recognition,"
        },
        {
          "Figure 1: Figure 1 Processed Frame Image by OpenPose": "a novel approach is proposed to use deep neural network fuse skeleton and RGB"
        },
        {
          "Figure 1: Figure 1 Processed Frame Image by OpenPose": "features only using single-modality RGB video data. Experimental results show"
        },
        {
          "Figure 1: Figure 1 Processed Frame Image by OpenPose": "them approach achieves substantial\nimprovements both in individual categories"
        },
        {
          "Figure 1: Figure 1 Processed Frame Image by OpenPose": "and overall and is provided with stronger generalization capability as well\n[38]."
        },
        {
          "Figure 1: Figure 1 Processed Frame Image by OpenPose": "Based on the capability of OpenPose,\nI can get\nthe keypoints of a person"
        },
        {
          "Figure 1: Figure 1 Processed Frame Image by OpenPose": "in picture. OpenPose can process each frames\nin a video,\ntherefore I can get"
        },
        {
          "Figure 1: Figure 1 Processed Frame Image by OpenPose": "images from each video’s frames. Then use OpenPose to process each images to"
        },
        {
          "Figure 1: Figure 1 Processed Frame Image by OpenPose": "get the keypoints of person in the images. The keypoints will shows the body"
        },
        {
          "Figure 1: Figure 1 Processed Frame Image by OpenPose": "poses with lines with different colours and also detect the outline of their face."
        },
        {
          "Figure 1: Figure 1 Processed Frame Image by OpenPose": "Thus I can get the images that contains the body pose with the face outlines and"
        },
        {
          "Figure 1: Figure 1 Processed Frame Image by OpenPose": "disable the original\nimage just keep the keypoints, which will become dataset"
        },
        {
          "Figure 1: Figure 1 Processed Frame Image by OpenPose": "for training and validating. Figure 1 shows a example of processing image via"
        },
        {
          "Figure 1: Figure 1 Processed Frame Image by OpenPose": "OpenPose."
        },
        {
          "Figure 1: Figure 1 Processed Frame Image by OpenPose": "3.5\nDatabases for Emotion Recognition"
        },
        {
          "Figure 1: Figure 1 Processed Frame Image by OpenPose": "The Aff-Wild2 database, an extension of\nthe original Aff-Wild dataset,\nis\nthe"
        },
        {
          "Figure 1: Figure 1 Processed Frame Image by OpenPose": "first\nto capture real-world affect\nin-the-wild.\nIt offers a substantial volume of"
        },
        {
          "Figure 1: Figure 1 Processed Frame Image by OpenPose": "data,\nincluding videos download from Youtube with associated annotations for"
        },
        {
          "Figure 1: Figure 1 Processed Frame Image by OpenPose": "valence and arousal. The database’s annotations of\nthe seven basic emotions"
        },
        {
          "Figure 1: Figure 1 Processed Frame Image by OpenPose": "and continuous valence- arousal labels allow for both classification and regression"
        },
        {
          "Figure 1: Figure 1 Processed Frame Image by OpenPose": "problem formulations in emotion recognition tasks [4, 5, 6, 7, 8, 9, 10, 11, 12,"
        },
        {
          "Figure 1: Figure 1 Processed Frame Image by OpenPose": "13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 40]."
        },
        {
          "Figure 1: Figure 1 Processed Frame Image by OpenPose": "This dataset\nconsisting of more\nthan 500 videos, having 2,000,000 more"
        },
        {
          "Figure 1: Figure 1 Processed Frame Image by OpenPose": "frames, all of person in video which have been annotated. The videos\nin the"
        },
        {
          "Figure 1: Figure 1 Processed Frame Image by OpenPose": "dataset are captured in unconstrained settings, with varying illumination con-"
        },
        {
          "Figure 1: Figure 1 Processed Frame Image by OpenPose": "ditions, head poses, occlusions, and backgrounds, making it more challenging"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "and realistic than controlled laboratory settings.": "DFEW (Dynamic Facial Expression in the Wild) is another prominent dataset"
        },
        {
          "and realistic than controlled laboratory settings.": "that provides a collection of facial expressions captured in natural settings. With"
        },
        {
          "and realistic than controlled laboratory settings.": "its large- scale, DFEW can be served as a benchmark for researchers to develop"
        },
        {
          "and realistic than controlled laboratory settings.": "and evaluate their methods for dealing with dynamic facial expression recogni-"
        },
        {
          "and realistic than controlled laboratory settings.": "tion (FER) in the wild [27]."
        },
        {
          "and realistic than controlled laboratory settings.": "This dataset\ncontains over 16000 videos\nfrom thousands of movies,\nthese"
        },
        {
          "and realistic than controlled laboratory settings.": "video clips contain various challenging interference in practical\nscenarios\nsuch"
        },
        {
          "and realistic than controlled laboratory settings.": "as extreme illumination, occlusions, and capricious pose changes\n[27].\nThese"
        },
        {
          "and realistic than controlled laboratory settings.": "videos will provide lots of\nimages with different emotions. The videos\nin the"
        },
        {
          "and realistic than controlled laboratory settings.": "DFEW dataset are collected from popular movies, which means\nthat\nthe fa-"
        },
        {
          "and realistic than controlled laboratory settings.": "cial expressions are captured in various unconstrained settings, with different"
        },
        {
          "and realistic than controlled laboratory settings.": "illumination conditions, occlusions, and head poses."
        },
        {
          "and realistic than controlled laboratory settings.": "3.6\nEmotional States"
        },
        {
          "and realistic than controlled laboratory settings.": "The\nclassification of\nemotions\ninto discrete\ncategories, particularly the basic"
        },
        {
          "and realistic than controlled laboratory settings.": "emotions of anger, disgust,\nfear, happiness, sadness, surprise, and neutral, has"
        },
        {
          "and realistic than controlled laboratory settings.": "served as a foundational element in the field of affective computing. This tra-"
        },
        {
          "and realistic than controlled laboratory settings.": "ditional approach makes use of\nthe classification of emotions\ninto these seven"
        },
        {
          "and realistic than controlled laboratory settings.": "discrete states in order to expedite the creation of computational models that"
        },
        {
          "and realistic than controlled laboratory settings.": "are capable of\nidentifying and interpreting human emotions. Supervised learn-"
        },
        {
          "and realistic than controlled laboratory settings.": "ing approaches, a subset of machine learning techniques, are ideally suited to"
        },
        {
          "and realistic than controlled laboratory settings.": "this classification job. Supervised learning algorithms are best suited in scenar-"
        },
        {
          "and realistic than controlled laboratory settings.": "ios where labelled datasets are abundant since they require a dataset containing"
        },
        {
          "and realistic than controlled laboratory settings.": "labelled examples from which to learn."
        },
        {
          "and realistic than controlled laboratory settings.": "In the\ncontext of\nemotion recognition, datasets often consist of videos or"
        },
        {
          "and realistic than controlled laboratory settings.": "images of\nindividuals expressing various emotions. These expressions are anno-"
        },
        {
          "and realistic than controlled laboratory settings.": "tated with labels\nthat correspond to the basic emotions.\nFor example, anno-"
        },
        {
          "and realistic than controlled laboratory settings.": "tations are assigned numerical values from 1 to 7, each number representing a"
        },
        {
          "and realistic than controlled laboratory settings.": "different basic emotion. The mapping of numbers\nto emotions\nis\ntypically as"
        },
        {
          "and realistic than controlled laboratory settings.": "follows:\n1 for happiness, 2 for\nsadness, 3 for neutral\n(no strong emotion dis-"
        },
        {
          "and realistic than controlled laboratory settings.": "played), 4 for anger, 5 for surprise, 6 for disgust, and 7 for fear. This numerical"
        },
        {
          "and realistic than controlled laboratory settings.": "labelling system streamlines the process of training supervised learning models"
        },
        {
          "and realistic than controlled laboratory settings.": "by providing a clear, structured format for the emotion data."
        },
        {
          "and realistic than controlled laboratory settings.": "Furthermore, there’s a new way to determine the emotion which is valence"
        },
        {
          "and realistic than controlled laboratory settings.": "and arousal. This way gives each emotion two numbers between -1 to 1. These"
        },
        {
          "and realistic than controlled laboratory settings.": "two numbers represent two axes. Valence represents the emotion is positive or"
        },
        {
          "and realistic than controlled laboratory settings.": "negative, whereas arousal refers to its intensity,\nfrom low to high. For example,"
        },
        {
          "and realistic than controlled laboratory settings.": "an emotion with high arousal and positive valence will be happy, an emotion"
        },
        {
          "and realistic than controlled laboratory settings.": "with low arousal and negative valence will be sad. Based on the value of valence"
        },
        {
          "and realistic than controlled laboratory settings.": "and arousal, we can classify the 7 basic emotion states out."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 2: Figure 2 Architecture of Residual Block": "3.7\nResNet"
        },
        {
          "Figure 2: Figure 2 Architecture of Residual Block": "Deep convolutional neural networks have led to a series of breakthroughs\nfor"
        },
        {
          "Figure 2: Figure 2 Architecture of Residual Block": "image classification. Deep networks naturally integrate low / mid / high level"
        },
        {
          "Figure 2: Figure 2 Architecture of Residual Block": "features and classifiers in an end-to-end multilayer fashion, and the “levels” of"
        },
        {
          "Figure 2: Figure 2 Architecture of Residual Block": "features can be enriched by the number of stacked layers (depth). Driven by the"
        },
        {
          "Figure 2: Figure 2 Architecture of Residual Block": "significance of depth, a question arises:\nIs learning better networks as easy as"
        },
        {
          "Figure 2: Figure 2 Architecture of Residual Block": "stacking more layers? An obstacle to answering this question was the notorious"
        },
        {
          "Figure 2: Figure 2 Architecture of Residual Block": "problem of vanishing/exploding gradients, which hamper convergence from the"
        },
        {
          "Figure 2: Figure 2 Architecture of Residual Block": "beginning [41]."
        },
        {
          "Figure 2: Figure 2 Architecture of Residual Block": "ResNet provide a brilliant method to prevent vanishing/exploding gradients"
        },
        {
          "Figure 2: Figure 2 Architecture of Residual Block": "with deep convolution neural networks."
        },
        {
          "Figure 2: Figure 2 Architecture of Residual Block": "To integrate the learned residual g(x) with the input x, ResNet uses a short-"
        },
        {
          "Figure 2: Figure 2 Architecture of Residual Block": "cut connection (also known as a skip connection)\nthat bypasses one or more"
        },
        {
          "Figure 2: Figure 2 Architecture of Residual Block": "layers. The output of the residual block is then f(x) = g(x) + x, where g(x) is"
        },
        {
          "Figure 2: Figure 2 Architecture of Residual Block": "the output from the learned residual\nfunction and x is the original\ninput. This"
        },
        {
          "Figure 2: Figure 2 Architecture of Residual Block": "mechanism allows the network to adjust the magnitude of the residual, and if"
        },
        {
          "Figure 2: Figure 2 Architecture of Residual Block": "the input is already close to the desired output, the network can effectively push"
        },
        {
          "Figure 2: Figure 2 Architecture of Residual Block": "g(x) towards 0, making f(x)≈ x."
        },
        {
          "Figure 2: Figure 2 Architecture of Residual Block": "This residual\nlearning framework has a profound implication:\nit allows the"
        },
        {
          "Figure 2: Figure 2 Architecture of Residual Block": "neural network to be significantly deeper without the training becoming unman-"
        },
        {
          "Figure 2: Figure 2 Architecture of Residual Block": "ageable due to vanishing or exploding gradients.\nIf g(x) is approximately to 0,"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 3: Figure 3 Architecture of DenseNet": "the\nf(x)\nis approximated,\nindicating that\nthe added layers do not\ncontribute"
        },
        {
          "Figure 3: Figure 3 Architecture of DenseNet": "significant changes to the input. This capability not only facilitates the training"
        },
        {
          "Figure 3: Figure 3 Architecture of DenseNet": "of much deeper networks by effectively alleviating the vanishing gradient prob-"
        },
        {
          "Figure 3: Figure 3 Architecture of DenseNet": "lem but also ensuring that the additional\nlayers do not degrade the network’s"
        },
        {
          "Figure 3: Figure 3 Architecture of DenseNet": "performance.\nSince it publishes, ResNet has become a foundational architec-"
        },
        {
          "Figure 3: Figure 3 Architecture of DenseNet": "ture for many computer vision tasks, setting new benchmarks for performance"
        },
        {
          "Figure 3: Figure 3 Architecture of DenseNet": "and efficiency.\nIts principle of\nlearning residual\nfunctions has been adapted and"
        },
        {
          "Figure 3: Figure 3 Architecture of DenseNet": "extended to various other types of neural networks,\nillustrating the versatility"
        },
        {
          "Figure 3: Figure 3 Architecture of DenseNet": "and impact of this approach."
        },
        {
          "Figure 3: Figure 3 Architecture of DenseNet": "3.8\nDenseNet"
        },
        {
          "Figure 3: Figure 3 Architecture of DenseNet": "DenseNet introduces direct connections between any two layers with the same"
        },
        {
          "Figure 3: Figure 3 Architecture of DenseNet": "feature- map size. The paper showed that DenseNets scale naturally to hundreds"
        },
        {
          "Figure 3: Figure 3 Architecture of DenseNet": "of\nlayers, while ex-habiting no optimization difficulties [42]."
        },
        {
          "Figure 3: Figure 3 Architecture of DenseNet": "Dense Connection is a connection method in the DenseNet network archi-"
        },
        {
          "Figure 3: Figure 3 Architecture of DenseNet": "tecture that allows each layer to receive feature maps as input from all previous"
        },
        {
          "Figure 3: Figure 3 Architecture of DenseNet": "layers and pass\nits own learned feature maps\nto all\nsubsequent\nlayers. Refer"
        },
        {
          "Figure 3: Figure 3 Architecture of DenseNet": "to figure above,\nif a layer is an X2 layer,\nit will receive the feature maps of all"
        },
        {
          "Figure 3: Figure 3 Architecture of DenseNet": "previous layers X1, X0 as input. This layer performs convolution operations and"
        },
        {
          "Figure 3: Figure 3 Architecture of DenseNet": "other operations on the basis of\nits input feature map to obtain a new feature"
        },
        {
          "Figure 3: Figure 3 Architecture of DenseNet": "map. The output feature map of this layer is directly passed to all subsequent"
        },
        {
          "Figure 3: Figure 3 Architecture of DenseNet": "layers X3, X4 as part of the input feature map."
        },
        {
          "Figure 3: Figure 3 Architecture of DenseNet": "This dense connection method is different from the connection between layers"
        },
        {
          "Figure 3: Figure 3 Architecture of DenseNet": "of traditional convolutional networks, where the previous layer is connected to"
        },
        {
          "Figure 3: Figure 3 Architecture of DenseNet": "the next\nlayer.\nDense\nconnections maximize\nthe\nreuse of\nfeatures,\nand the"
        },
        {
          "Figure 3: Figure 3 Architecture of DenseNet": "model can more efficiently combine all\nfeatures learned in previous layers.\nSo"
        },
        {
          "Figure 3: Figure 3 Architecture of DenseNet": "DenseNet\nenhanced feature propagation helps\nto better propagate gradients,"
        },
        {
          "Figure 3: Figure 3 Architecture of DenseNet": "thereby alleviating the vanishing gradient problem.\nFor\nthe same computing"
        },
        {
          "Figure 3: Figure 3 Architecture of DenseNet": "resources, narrower and deeper networks can be built to achieve better accuracy"
        },
        {
          "Figure 3: Figure 3 Architecture of DenseNet": "with it."
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3.9 What Have Been Done": "Classifying correct emotions\nfrom different data sources\nsuch as\ntext,\nimages,"
        },
        {
          "3.9 What Have Been Done": "videos, and speech has been an area of research for researchers in various dis-"
        },
        {
          "3.9 What Have Been Done": "ciplines.\nAutomatic emotion detection from videos and images\nis one of\nthe"
        },
        {
          "3.9 What Have Been Done": "most challenging tasks to analyse using supervised and unsupervised machine"
        },
        {
          "3.9 What Have Been Done": "learning methods."
        },
        {
          "3.9 What Have Been Done": "In a paper,\nfacial and body features extracted by the OpenPose tool have"
        },
        {
          "3.9 What Have Been Done": "been used for detecting basic 6, 7 and 9 emotions from videos and images by"
        },
        {
          "3.9 What Have Been Done": "a novel deep neural network framework which combines the Gaussian mixture"
        },
        {
          "3.9 What Have Been Done": "model with CNN, LSTM and Transformer to generate the CNN-LSTM model"
        },
        {
          "3.9 What Have Been Done": "and CNN-Transformer model with and without Gaussian centres\n[43].\nThis"
        },
        {
          "3.9 What Have Been Done": "paper using two benchmark datasets, namely FABO and CK+.\nIt\nreported"
        },
        {
          "3.9 What Have Been Done": "over 90% accuracy for most combinations of\nfeatures for both datasets."
        },
        {
          "3.9 What Have Been Done": "Another paper, In this paper, therefore, we report the multi-view emotional"
        },
        {
          "3.9 What Have Been Done": "expressions dataset (MEED) using 2D pose estimation [37]. They got twenty-"
        },
        {
          "3.9 What Have Been Done": "two actors presented six emotional\n(anger, disgust,\nfear, happiness,\nsadness,"
        },
        {
          "3.9 What Have Been Done": "surprise) and neutral body movements from three viewpoints (left, front, right)."
        },
        {
          "3.9 What Have Been Done": "A total of 4102 videos were captured."
        },
        {
          "3.9 What Have Been Done": "A paper introduce emotional body gestures as a component of what is com-"
        },
        {
          "3.9 What Have Been Done": "monly known as ”body language” and comment general aspects as gender differ-"
        },
        {
          "3.9 What Have Been Done": "ences and culture dependence [34]. They also discuss multi-modal approaches"
        },
        {
          "3.9 What Have Been Done": "that combine speech or face with body gestures for improved emotion recogni-"
        },
        {
          "3.9 What Have Been Done": "tion."
        },
        {
          "3.9 What Have Been Done": "Together, these studies illustrate the dynamic evolution of emotion detection"
        },
        {
          "3.9 What Have Been Done": "technologies, showcasing a trend towards more integrated, multi-modal systems"
        },
        {
          "3.9 What Have Been Done": "that\ncan adapt\nto and accurately interpret\nthe\ncomplex interplay of human"
        },
        {
          "3.9 What Have Been Done": "expressions and gestures across diverse settings."
        },
        {
          "3.9 What Have Been Done": "3.10\nSummary"
        },
        {
          "3.9 What Have Been Done": "In order\nto comprehend human emotions,\nemotion recognition has developed"
        },
        {
          "3.9 What Have Been Done": "into a sophisticated field that integrates multiple technologies. Due to the di-"
        },
        {
          "3.9 What Have Been Done": "rect display of emotions\nin speech and facial expressions,\ntraditional emotion"
        },
        {
          "3.9 What Have Been Done": "identification techniques mostly use these methods. Furthermore, because phys-"
        },
        {
          "3.9 What Have Been Done": "iological signals reliably indicate subconscious emotional states, such as EEG,"
        },
        {
          "3.9 What Have Been Done": "ECG, GSR, and ET, they are used to quantify emotional responses."
        },
        {
          "3.9 What Have Been Done": "Despite advancements in recognizing emotions from facial and physiological"
        },
        {
          "3.9 What Have Been Done": "signals cues,\nthe role of body gestures\nremains\nless explored. Recent\nstudies,"
        },
        {
          "3.9 What Have Been Done": "like the use of 2D pose estimation, have demonstrated that body gestures can"
        },
        {
          "3.9 What Have Been Done": "sometimes express emotional information more effectively than facial expressions"
        },
        {
          "3.9 What Have Been Done": "or speech. OpenPose has notably advanced this area by enabling the analysis"
        },
        {
          "3.9 What Have Been Done": "of body gestures, which is critical for emotion recognition, especially in settings"
        },
        {
          "3.9 What Have Been Done": "where facial expressions are not visible."
        },
        {
          "3.9 What Have Been Done": "The application of machine learning and deep learning has been critical\nin"
        },
        {
          "3.9 What Have Been Done": "enhancing emotion recognition systems.\nTechniques\nsuch as Support Vector"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Machines\n(SVM) and Convolutional Neural Networks\n(CNN) are particularly": "effective in processing and classifying image data. The introduction of ResNet"
        },
        {
          "Machines\n(SVM) and Convolutional Neural Networks\n(CNN) are particularly": "has further improved deep learning models by mitigating the gradient vanishing"
        },
        {
          "Machines\n(SVM) and Convolutional Neural Networks\n(CNN) are particularly": "problem, thus facilitating the training of deeper neural networks."
        },
        {
          "Machines\n(SVM) and Convolutional Neural Networks\n(CNN) are particularly": "Important contributions have also been made by the specialised databases"
        },
        {
          "Machines\n(SVM) and Convolutional Neural Networks\n(CNN) are particularly": "DFEW and Aff-Wild2, which provide\nextensive\nvideo datasets\ncaptured in"
        },
        {
          "Machines\n(SVM) and Convolutional Neural Networks\n(CNN) are particularly": "videos and movies. These resources are invaluable for\ntraining and validating"
        },
        {
          "Machines\n(SVM) and Convolutional Neural Networks\n(CNN) are particularly": "emotion recognition algorithms under realistic conditions. The classification of"
        },
        {
          "Machines\n(SVM) and Convolutional Neural Networks\n(CNN) are particularly": "emotions\ninto 7 discrete\ncategories\nsuch as anger, disgust, and happiness,\nin"
        },
        {
          "Machines\n(SVM) and Convolutional Neural Networks\n(CNN) are particularly": "other hand newer methods that assess emotional valence and arousal, provides"
        },
        {
          "Machines\n(SVM) and Convolutional Neural Networks\n(CNN) are particularly": "an extensive framework for understanding and interpreting human emotions."
        },
        {
          "Machines\n(SVM) and Convolutional Neural Networks\n(CNN) are particularly": "In general,\nthe field of\nemotion recognition is moving towards more\ninte-"
        },
        {
          "Machines\n(SVM) and Convolutional Neural Networks\n(CNN) are particularly": "grated, multimodal systems that use traditional and innovative methods to cap-"
        },
        {
          "Machines\n(SVM) and Convolutional Neural Networks\n(CNN) are particularly": "ture the subtle and complex aspects of human emotions. This will enhancing"
        },
        {
          "Machines\n(SVM) and Convolutional Neural Networks\n(CNN) are particularly": "applications across various domains including interactive gaming, psychological"
        },
        {
          "Machines\n(SVM) and Convolutional Neural Networks\n(CNN) are particularly": "analysis, assistive technologies, and security systems."
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Two different datasets are\ninvolved,\neach providing numerical\nlabels\nthat": "correspond to seven different emotions expressed in the videos. Based on these"
        },
        {
          "Two different datasets are\ninvolved,\neach providing numerical\nlabels\nthat": "labels, the frame images from each video are then categorized into one of seven"
        },
        {
          "Two different datasets are\ninvolved,\neach providing numerical\nlabels\nthat": "designated folders.\nEach folder\nrepresents\none\nof\nthe\nseven emotions being"
        },
        {
          "Two different datasets are\ninvolved,\neach providing numerical\nlabels\nthat": "trained."
        },
        {
          "Two different datasets are\ninvolved,\neach providing numerical\nlabels\nthat": "Once the images are sorted into these emotion-specific folders,\nit’s important"
        },
        {
          "Two different datasets are\ninvolved,\neach providing numerical\nlabels\nthat": "to locate the issue of redundancy of the data. Because the original video frames"
        },
        {
          "Two different datasets are\ninvolved,\neach providing numerical\nlabels\nthat": "are captured in quick succession, consecutive frames often display very similar"
        },
        {
          "Two different datasets are\ninvolved,\neach providing numerical\nlabels\nthat": "gestures and expressions due to the continuous nature of video recording. This"
        },
        {
          "Two different datasets are\ninvolved,\neach providing numerical\nlabels\nthat": "high similarity between nearby frames\ncan lead to redundancy, which is not"
        },
        {
          "Two different datasets are\ninvolved,\neach providing numerical\nlabels\nthat": "ideal\nfor\ntraining machine\nlearning models\nthat perform better with diverse"
        },
        {
          "Two different datasets are\ninvolved,\neach providing numerical\nlabels\nthat": "data samples."
        },
        {
          "Two different datasets are\ninvolved,\neach providing numerical\nlabels\nthat": "To mitigate this, a subsampling technique need to be utilized. Specifically,"
        },
        {
          "Two different datasets are\ninvolved,\neach providing numerical\nlabels\nthat": "I chose to retain only every tenth frame from each video. This approach signif-"
        },
        {
          "Two different datasets are\ninvolved,\neach providing numerical\nlabels\nthat": "icantly reduces the total number of\nimages while ensuring that\nthe remaining"
        },
        {
          "Two different datasets are\ninvolved,\neach providing numerical\nlabels\nthat": "frames are spaced out enough to capture varied gestures and face expressions,"
        },
        {
          "Two different datasets are\ninvolved,\neach providing numerical\nlabels\nthat": "thus maintaining the diversity necessary for effective model training."
        },
        {
          "Two different datasets are\ninvolved,\neach providing numerical\nlabels\nthat": "By restructuring the data in this way—organizing it\ninto emotion-specific"
        },
        {
          "Two different datasets are\ninvolved,\neach providing numerical\nlabels\nthat": "categories and reducing redundancy through subsampling—the pre-processed"
        },
        {
          "Two different datasets are\ninvolved,\neach providing numerical\nlabels\nthat": "data is better suited for subsequent analytical tasks."
        },
        {
          "Two different datasets are\ninvolved,\neach providing numerical\nlabels\nthat": "After the subsampling,\nframe images will be categorized to seven different"
        },
        {
          "Two different datasets are\ninvolved,\neach providing numerical\nlabels\nthat": "emotion folders based on the labels."
        },
        {
          "Two different datasets are\ninvolved,\neach providing numerical\nlabels\nthat": "4.3\nInputting Datasets"
        },
        {
          "Two different datasets are\ninvolved,\neach providing numerical\nlabels\nthat": "For importing the images, start by defining the location of the training dataset"
        },
        {
          "Two different datasets are\ninvolved,\neach providing numerical\nlabels\nthat": "and the validation dataset. Utilize the dataset.ImageFolder() function to import"
        },
        {
          "Two different datasets are\ninvolved,\neach providing numerical\nlabels\nthat": "these datasets while applying data augmentation techniques simultaneously such"
        },
        {
          "Two different datasets are\ninvolved,\neach providing numerical\nlabels\nthat": "as rotation, horizontal flip, and normalization. This approach not only simplifies"
        },
        {
          "Two different datasets are\ninvolved,\neach providing numerical\nlabels\nthat": "the process of handling dataset but also enhances\nthe dataset by introducing"
        },
        {
          "Two different datasets are\ninvolved,\neach providing numerical\nlabels\nthat": "variability and improving model generalization capabilities."
        },
        {
          "Two different datasets are\ninvolved,\neach providing numerical\nlabels\nthat": "For importing the JSON files, particularly those containing keypoints data,"
        },
        {
          "Two different datasets are\ninvolved,\neach providing numerical\nlabels\nthat": "the initial step is to examine the JSON structure to locate the ’people’ key. Sub-"
        },
        {
          "Two different datasets are\ninvolved,\neach providing numerical\nlabels\nthat": "from\nsequently,\nfocus on extracting ’pose keypoints 2d’ and ’face keypoints 2d’"
        },
        {
          "Two different datasets are\ninvolved,\neach providing numerical\nlabels\nthat": "the ’people’ key. These keys contain valuable data on the positional coordinates"
        },
        {
          "Two different datasets are\ninvolved,\neach providing numerical\nlabels\nthat": "of body and facial keypoints, which are essential\nfor tasks that require precise"
        },
        {
          "Two different datasets are\ninvolved,\neach providing numerical\nlabels\nthat": "human gestures and facial expression analysis."
        },
        {
          "Two different datasets are\ninvolved,\neach providing numerical\nlabels\nthat": "4.4\nShowing Images"
        },
        {
          "Two different datasets are\ninvolved,\neach providing numerical\nlabels\nthat": "To effectively manage and inspect a dataset of\nimages labeled with emotional"
        },
        {
          "Two different datasets are\ninvolved,\neach providing numerical\nlabels\nthat": "expressions,\nit is beneficial to display the images alongside their labels using two"
        },
        {
          "Two different datasets are\ninvolved,\neach providing numerical\nlabels\nthat": "distinct labeling systems."
        },
        {
          "Two different datasets are\ninvolved,\neach providing numerical\nlabels\nthat": "The first\nsystem categorizes 7 emotions\ninto one of\nseven basic categories"
        },
        {
          "Two different datasets are\ninvolved,\neach providing numerical\nlabels\nthat": "from label 0-6.\nThis\ncategorical\nsystem simplifies\nthe\ncomplexity of human"
        },
        {
          "Two different datasets are\ninvolved,\neach providing numerical\nlabels\nthat": "emotions into easily 7 distinguishable groups, useful for basic emotional analysis."
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "The second labeling system employs a two-dimensional approach where each": "image\nis associated with two values which are valence and arousal.\nValence"
        },
        {
          "The second labeling system employs a two-dimensional approach where each": "measures\nthe pleasantness of\nthe emotion,\nranging from negative to positive,"
        },
        {
          "The second labeling system employs a two-dimensional approach where each": "whereas arousal measures the intensity of the emotion,\nfrom low to high. This"
        },
        {
          "The second labeling system employs a two-dimensional approach where each": "system provides a more nuanced understanding of emotional states, capturing"
        },
        {
          "The second labeling system employs a two-dimensional approach where each": "the subtleties in how emotions are experienced."
        },
        {
          "The second labeling system employs a two-dimensional approach where each": "When importing these datasets,\nit is critical to visually inspect the images"
        },
        {
          "The second labeling system employs a two-dimensional approach where each": "alongside their labels to check if the labeling correct and ensure that there are"
        },
        {
          "The second labeling system employs a two-dimensional approach where each": "no discrepancies or errors in the dataset and label. Any mismatches identified"
        },
        {
          "The second labeling system employs a two-dimensional approach where each": "during this\ninspection can then be addressed to improve\nthe\nreliability and"
        },
        {
          "The second labeling system employs a two-dimensional approach where each": "usability of the dataset for further emotional analysis."
        },
        {
          "The second labeling system employs a two-dimensional approach where each": "4.5\nDefining Model"
        },
        {
          "The second labeling system employs a two-dimensional approach where each": "In this project,\nI used three types of neural network architectures: Artificial"
        },
        {
          "The second labeling system employs a two-dimensional approach where each": "Neural Network (ANN), ResNet18 with a modified structure and ResNet18 pre-"
        },
        {
          "The second labeling system employs a two-dimensional approach where each": "trained on ImageNet, and a pre-trained ResNet50 model trained on ImageNet."
        },
        {
          "The second labeling system employs a two-dimensional approach where each": "For\nthe ANN,\nI designed the architecture to include four\nlinear\nfully con-"
        },
        {
          "The second labeling system employs a two-dimensional approach where each": "nected layers. These layers were connected using ReLU (Rectified Linear Unit)"
        },
        {
          "The second labeling system employs a two-dimensional approach where each": "activation functions, which import non-linear function into the network and help"
        },
        {
          "The second labeling system employs a two-dimensional approach where each": "prevent the vanishing gradient problem during training. The purpose of this de-"
        },
        {
          "The second labeling system employs a two-dimensional approach where each": "sign was to ensure a robust feature extraction from the input data, culminating"
        },
        {
          "The second labeling system employs a two-dimensional approach where each": "in the final\nlayer. This last layer is crucial as it outputs the probabilities across"
        },
        {
          "The second labeling system employs a two-dimensional approach where each": "seven different emotions, hence the output channel was set to seven. This setup"
        },
        {
          "The second labeling system employs a two-dimensional approach where each": "is particularly aimed at facilitating the model’s capability to perform multi-class"
        },
        {
          "The second labeling system employs a two-dimensional approach where each": "classification of emotional states from the input data."
        },
        {
          "The second labeling system employs a two-dimensional approach where each": "Moving to the ResNet18 architecture,\nI implemented a variant of the stan-"
        },
        {
          "The second labeling system employs a two-dimensional approach where each": "dard model\nto better\nsuit\nthe\nspecific\nrequirements.\nEach residual block of"
        },
        {
          "The second labeling system employs a two-dimensional approach where each": "the network contains two convolutional\nlayers. These convolutional\nlayers are"
        },
        {
          "The second labeling system employs a two-dimensional approach where each": "followed by batch normalization layers, which stabilize and accelerate the train-"
        },
        {
          "The second labeling system employs a two-dimensional approach where each": "ing process by normalizing the inputs to each layer. A key feature of ResNet"
        },
        {
          "The second labeling system employs a two-dimensional approach where each": "architectures\nis\nthe\ninclusion of\nskip connections\nthat help in alleviating the"
        },
        {
          "The second labeling system employs a two-dimensional approach where each": "problem of\ntraining deep networks by allowing gradients\nto flow through the"
        },
        {
          "The second labeling system employs a two-dimensional approach where each": "network without significant vanishing. The network structure is completed by"
        },
        {
          "The second labeling system employs a two-dimensional approach where each": "sequentially connecting these residual blocks with additional convolutional\nlay-"
        },
        {
          "The second labeling system employs a two-dimensional approach where each": "ers and a classification layer, which collectively function to effectively capture"
        },
        {
          "The second labeling system employs a two-dimensional approach where each": "and interpret the hierarchical\nfeatures of the data."
        },
        {
          "The second labeling system employs a two-dimensional approach where each": "Lastly,\nfor\nthe ResNet50 model,\nI utilized a pre-trained version available"
        },
        {
          "The second labeling system employs a two-dimensional approach where each": "through the PyTorch ’models’ module by executing the following command:"
        },
        {
          "The second labeling system employs a two-dimensional approach where each": "models.resnet50(pretrained=True). This model is initially trained on ImageNet,"
        },
        {
          "The second labeling system employs a two-dimensional approach where each": "a large visual database designed for use in visual object recognition software re-"
        },
        {
          "The second labeling system employs a two-dimensional approach where each": "search.\nBy utilizing a pre-trained model, we\ncan use\nthe\nlearned weights as"
        },
        {
          "The second labeling system employs a two-dimensional approach where each": "a starting point,\nthus\nsaving significant\ntraining time and computational\nre-"
        },
        {
          "The second labeling system employs a two-dimensional approach where each": "sources. To fits\nthis model\nto my task of emotion prediction,\nI modified the"
        },
        {
          "The second labeling system employs a two-dimensional approach where each": "final\nlayer\nto have an output\nsize of\nseven to correspond to my seven emo-"
        },
        {
          "The second labeling system employs a two-dimensional approach where each": "tion categories. This adaptation allows the ResNet50 model to output refined"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "predictions specifically for my dataset.": "4.5.1\nHyperparameter"
        },
        {
          "predictions specifically for my dataset.": "In the process of fine-tuning the hyperparameters for different machine learning"
        },
        {
          "predictions specifically for my dataset.": "tasks, distinct\nstrategies were employed for classification and regression prob-"
        },
        {
          "predictions specifically for my dataset.": "lems, primarily concerning the choice of\nloss\nfunctions.\nFor\nthe classification"
        },
        {
          "predictions specifically for my dataset.": "tasks,\nI opted to use the CrossEntropy loss\nfunction, which is well-suited for"
        },
        {
          "predictions specifically for my dataset.": "problems\ninvolving multiple\nclasses as\nit\neffectively handles\nthe probabilistic"
        },
        {
          "predictions specifically for my dataset.": "interpretation of class predictions. Here’s a formula of CrossEntropy below:"
        },
        {
          "predictions specifically for my dataset.": "(cid:88)"
        },
        {
          "predictions specifically for my dataset.": "H(p, q) = −\n(p(x) log q(x))\n(1)"
        },
        {
          "predictions specifically for my dataset.": "x=classes"
        },
        {
          "predictions specifically for my dataset.": "H represents CrossEntropy, p is\nreal\nlabel of classes while q is\nthe predict"
        },
        {
          "predictions specifically for my dataset.": "output of\nthis classes. Each predicted class will compare to real\nlabel of 0 or"
        },
        {
          "predictions specifically for my dataset.": "1. The calculated loss penalizes the probability based on how far it is from the"
        },
        {
          "predictions specifically for my dataset.": "expected value. Then the penalty is logarithmic, which means there’s a large"
        },
        {
          "predictions specifically for my dataset.": "score for more difference close to 1 and small score for less difference close to 0."
        },
        {
          "predictions specifically for my dataset.": "Conversely,\nfor\nregression tasks,\nthe Mean Squared Error\n(MSE)\nloss was"
        },
        {
          "predictions specifically for my dataset.": "utilized, which quantifies the average of the squares of the errors—that is, the"
        },
        {
          "predictions specifically for my dataset.": "average squared difference between the estimated values and the actual value."
        },
        {
          "predictions specifically for my dataset.": "Here’s a formula of MSELoss below:"
        },
        {
          "predictions specifically for my dataset.": "1 N\nN(cid:88) i\n(2)\nM SE =\n(yi − ˆyi)2"
        },
        {
          "predictions specifically for my dataset.": "=1"
        },
        {
          "predictions specifically for my dataset.": "It\ncalculates\nthe\nsquared difference between the predicted value and the"
        },
        {
          "predictions specifically for my dataset.": "actual value. Then then averages\nthese squared differences across N which is"
        },
        {
          "predictions specifically for my dataset.": "number of\nsamples. MSE is great\nfor ensuring that\nthe model we train does"
        },
        {
          "predictions specifically for my dataset.": "not have outlier predictions with huge errors because MSE gives more weight"
        },
        {
          "predictions specifically for my dataset.": "to these errors due to the squared part of the function."
        },
        {
          "predictions specifically for my dataset.": "Adjustments to the learning rate followed the selection of the loss function,"
        },
        {
          "predictions specifically for my dataset.": "as this parameter critically influences the convergence speed and quality of the"
        },
        {
          "predictions specifically for my dataset.": "training process. An inappropriate learning rate can either\nlead to very slow"
        },
        {
          "predictions specifically for my dataset.": "convergence or cause the training to oscillate without stabilizing."
        },
        {
          "predictions specifically for my dataset.": "Additionally,\nthe choice of optimizer played a crucial role in the network’s"
        },
        {
          "predictions specifically for my dataset.": "performance. Various optimizers were tested,\nincluding Stochastic Gradient De-"
        },
        {
          "predictions specifically for my dataset.": "scent (SGD), Adam, and Adamax. Among these, Adam yielded the best results,"
        },
        {
          "predictions specifically for my dataset.": "likely due to its adaptive learning rate capabilities, which help in handling sparse"
        },
        {
          "predictions specifically for my dataset.": "gradients on noisy problems."
        },
        {
          "predictions specifically for my dataset.": "Adam is an algorithm for first-order gradient-based optimization of stochas-"
        },
        {
          "predictions specifically for my dataset.": "tic objective functions, based on adaptive estimates of\nlower-order moments."
        },
        {
          "predictions specifically for my dataset.": "The method is straightforward to implement,\nis computationally efficient, has"
        },
        {
          "predictions specifically for my dataset.": "little memory requirements,\nis invariant to diagonal rescaling of the gradients,"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "and is well suited for problems that are large in terms of data and/or param-": "eters. Furthermore, AdaMax is a variant of Adam based on the infinity norm"
        },
        {
          "and is well suited for problems that are large in terms of data and/or param-": "[44]."
        },
        {
          "and is well suited for problems that are large in terms of data and/or param-": "Lastly,"
        },
        {
          "and is well suited for problems that are large in terms of data and/or param-": "training process. Specifically, the OneCycleLR and ReduceLROnPlateau strate-"
        },
        {
          "and is well suited for problems that are large in terms of data and/or param-": "gies were tried. OneCycleLR aims"
        },
        {
          "and is well suited for problems that are large in terms of data and/or param-": "one-cycle policy that oscillates between a lower bound and an upper bound,"
        },
        {
          "and is well suited for problems that are large in terms of data and/or param-": "potentially leading to faster convergence. The ReduceLROnPlateau scheduler"
        },
        {
          "and is well suited for problems that are large in terms of data and/or param-": "decreases the learning rate when a metric has stopped improving, which ideally"
        },
        {
          "and is well suited for problems that are large in terms of data and/or param-": "reduces overfitting and leads to a more robust model. However, these schedulers"
        },
        {
          "and is well suited for problems that are large in terms of data and/or param-": "did not result in significant improvements in the context of the tasks, suggest-"
        },
        {
          "and is well suited for problems that are large in terms of data and/or param-": "ing that the benefits of these approaches might be context-dependent or require"
        },
        {
          "and is well suited for problems that are large in terms of data and/or param-": "further adjustment to the hyperparameters."
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 4: Figure 4 Detailed Architecture of ResNet18": "layers—Layer1\nthrough Layer4.\nEach of\nthese\nlayers\ncontains\ntwo\nresidual"
        },
        {
          "Figure 4: Figure 4 Detailed Architecture of ResNet18": "blocks, making up the\ncore mechanism that\nenables ResNet models\nto train"
        },
        {
          "Figure 4: Figure 4 Detailed Architecture of ResNet18": "more\neffectively and avoid issues\nrelated to deep networks\nsuch as vanishing"
        },
        {
          "Figure 4: Figure 4 Detailed Architecture of ResNet18": "gradients. A residual block is\nspecially designed to allow activations\nto prop-"
        },
        {
          "Figure 4: Figure 4 Detailed Architecture of ResNet18": "agate through the network more effectively by incorporating skip connections"
        },
        {
          "Figure 4: Figure 4 Detailed Architecture of ResNet18": "that skip two layers in my model."
        },
        {
          "Figure 4: Figure 4 Detailed Architecture of ResNet18": "Within each residual block from Layer1 to Layer4,\nthere are two convolu-"
        },
        {
          "Figure 4: Figure 4 Detailed Architecture of ResNet18": "tional\nlayers. These layers are responsible for extracting and refining features"
        },
        {
          "Figure 4: Figure 4 Detailed Architecture of ResNet18": "from the input data, with each subsequent layer building on the output of the"
        },
        {
          "Figure 4: Figure 4 Detailed Architecture of ResNet18": "previous layer. The design of these blocks and their shortcut connections help"
        },
        {
          "Figure 4: Figure 4 Detailed Architecture of ResNet18": "reduce information loss during training, ensuring that the network learns effec-"
        },
        {
          "Figure 4: Figure 4 Detailed Architecture of ResNet18": "tively even as it deepens."
        },
        {
          "Figure 4: Figure 4 Detailed Architecture of ResNet18": "Following Layer4,\nthe architecture includes a fully connected layer, which"
        },
        {
          "Figure 4: Figure 4 Detailed Architecture of ResNet18": "typically comes\ntowards\nthe\nend of\nthe network.\nThis\nlayer\nis\nessential\nfor"
        },
        {
          "Figure 4: Figure 4 Detailed Architecture of ResNet18": "integrating and interpreting the features extracted by the convolutional\nlayers,"
        },
        {
          "Figure 4: Figure 4 Detailed Architecture of ResNet18": "making high-level decisions about the input data based on the learned features."
        },
        {
          "Figure 4: Figure 4 Detailed Architecture of ResNet18": "In classification task, it should be 7 which represent 7 basic emotion states. Then"
        },
        {
          "Figure 4: Figure 4 Detailed Architecture of ResNet18": "for valence and arousal, the output channel of\nfully connected layer should be 2"
        },
        {
          "Figure 4: Figure 4 Detailed Architecture of ResNet18": "which are values of valence and arousal."
        },
        {
          "Figure 4: Figure 4 Detailed Architecture of ResNet18": "Summing up all\nthe convolutional\nlayers\nfrom Layer1 to Layer4 results\nin"
        },
        {
          "Figure 4: Figure 4 Detailed Architecture of ResNet18": "16 convolutional\nlayers. When combined with the initial Layer0 and the fully"
        },
        {
          "Figure 4: Figure 4 Detailed Architecture of ResNet18": "connected layer, the total comes to 18 layers, hence the name ResNet18. This"
        },
        {
          "Figure 4: Figure 4 Detailed Architecture of ResNet18": "architecture is known for its robustness and efficacy in handling complex visual"
        },
        {
          "Figure 4: Figure 4 Detailed Architecture of ResNet18": "tasks by leveraging deep learning capabilities while maintaining manageable"
        },
        {
          "Figure 4: Figure 4 Detailed Architecture of ResNet18": "computational costs."
        },
        {
          "Figure 4: Figure 4 Detailed Architecture of ResNet18": "For ResNet50, which is\nquite\nsimilar with ResNet18.\nThe difference\nis"
        },
        {
          "Figure 4: Figure 4 Detailed Architecture of ResNet18": "ResNet50 get deeper and more layers. ResNet50 use more residual blocks which"
        },
        {
          "Figure 4: Figure 4 Detailed Architecture of ResNet18": "means more\nconvolutional\nlayers.\nBut\nit also requests higher\ncomputational"
        },
        {
          "Figure 4: Figure 4 Detailed Architecture of ResNet18": "resource and has a bigger model size. Due to this,\nit generally achieves better"
        },
        {
          "Figure 4: Figure 4 Detailed Architecture of ResNet18": "performance on both classify and regression tasks due to its\nincreased depth"
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "for Classification with 7 Emotion": "SGD"
        },
        {
          "for Classification with 7 Emotion": "0.18"
        },
        {
          "for Classification with 7 Emotion": "0.26"
        },
        {
          "for Classification with 7 Emotion": "0.14"
        },
        {
          "for Classification with 7 Emotion": "N/A"
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2 × covariance(x, y)": "CCC ="
        },
        {
          "2 × covariance(x, y)": "σ2\nx + σ2\ny + (µx + µy)2"
        },
        {
          "2 × covariance(x, y)": "σ2 means variance and µ means mean in this equation."
        },
        {
          "2 × covariance(x, y)": ""
        },
        {
          "2 × covariance(x, y)": "faster than SGD. And Adam usually gets better result because Adam can adjust"
        },
        {
          "2 × covariance(x, y)": "the learning rates based on the average of the second moments of the gradients."
        },
        {
          "2 × covariance(x, y)": "So the pre-trained DenseNet121 get the best result. Lastly the effectiveness of"
        },
        {
          "2 × covariance(x, y)": "SGD can be notably enhanced by fine-tuning the learning rate."
        },
        {
          "2 × covariance(x, y)": "SGD can get better result by adjusting learning rate."
        }
      ],
      "page": 18
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "complete representation of the data,\nsuch as frames with complete loss of key": "keypoints resulting in a completely black picture."
        },
        {
          "complete representation of the data,\nsuch as frames with complete loss of key": "Processing video frame by frame using OpenPose also results in a quite high"
        },
        {
          "complete representation of the data,\nsuch as frames with complete loss of key": "similarity between consecutive frames. This similarity poses a significant chal-"
        },
        {
          "complete representation of the data,\nsuch as frames with complete loss of key": "lenge in maintaining the diversity of the training dataset, as many frames do not"
        },
        {
          "complete representation of the data,\nsuch as frames with complete loss of key": "differ enough from one another. To alleviate this problem,\nI\nimplemented sec-"
        },
        {
          "complete representation of the data,\nsuch as frames with complete loss of key": "ondary sampling, selecting every ten frames to reduce redundancy and enhance"
        },
        {
          "complete representation of the data,\nsuch as frames with complete loss of key": "dataset diversity. However, challenges\nremain even after\nsecondary sampling,"
        },
        {
          "complete representation of the data,\nsuch as frames with complete loss of key": "mainly because many of\nthe\nframes derived from successive video sequences"
        },
        {
          "complete representation of the data,\nsuch as frames with complete loss of key": "contain transitional or less expressive emotional states that are less obvious and"
        },
        {
          "complete representation of the data,\nsuch as frames with complete loss of key": "therefore less useful\nfor training."
        },
        {
          "complete representation of the data,\nsuch as frames with complete loss of key": "The project initially divided emotions into seven categories, consistent with"
        },
        {
          "complete representation of the data,\nsuch as frames with complete loss of key": "traditional seven emotion states. However, due to the consecutive frames cause"
        },
        {
          "complete representation of the data,\nsuch as frames with complete loss of key": "highly similarity between nearby frames and due to OpenPose there’re lots of"
        },
        {
          "complete representation of the data,\nsuch as frames with complete loss of key": "invalid frames. To address these challenges, I turned my focus to using valence"
        },
        {
          "complete representation of the data,\nsuch as frames with complete loss of key": "and arousal as a dimensional approach to continuous output, turning the prob-"
        },
        {
          "complete representation of the data,\nsuch as frames with complete loss of key": "lem into a regression task. This change allows the network to predict two values,"
        },
        {
          "complete representation of the data,\nsuch as frames with complete loss of key": "which are then used to set thresholds for seven basic emotions, providing a po-"
        },
        {
          "complete representation of the data,\nsuch as frames with complete loss of key": "tentially more nuanced understanding of emotional states than just categorical"
        },
        {
          "complete representation of the data,\nsuch as frames with complete loss of key": "categories."
        },
        {
          "complete representation of the data,\nsuch as frames with complete loss of key": "7.3\nSummary of Evaluation"
        },
        {
          "complete representation of the data,\nsuch as frames with complete loss of key": "According to the results\nfrom model,\nthe main issue is\nlow accuracy. That’s"
        },
        {
          "complete representation of the data,\nsuch as frames with complete loss of key": "because high similarity among consecutive frames, processing video frames se-"
        },
        {
          "complete representation of the data,\nsuch as frames with complete loss of key": "quentially with OpenPose resulted in a high degree of similarity among them,"
        },
        {
          "complete representation of the data,\nsuch as frames with complete loss of key": "which hindered the diversity of the training dataset. Despite attempts to miti-"
        },
        {
          "complete representation of the data,\nsuch as frames with complete loss of key": "gate this issue through subsampling (selecting every tenth frame), the problem"
        },
        {
          "complete representation of the data,\nsuch as frames with complete loss of key": "persisted. Many frames did not sufficiently differ from each other, particularly"
        },
        {
          "complete representation of the data,\nsuch as frames with complete loss of key": "those depicting transitional or less expressive emotional states."
        },
        {
          "complete representation of the data,\nsuch as frames with complete loss of key": "Initially, the project categorized emotions into seven discrete classes based on"
        },
        {
          "complete representation of the data,\nsuch as frames with complete loss of key": "traditional emotion theory. This approach faces difficulties due to the indistin-"
        },
        {
          "complete representation of the data,\nsuch as frames with complete loss of key": "guishable and large proportion of emotion transitions between frames, resulting"
        },
        {
          "complete representation of the data,\nsuch as frames with complete loss of key": "in low verification accuracy. So many similar images in different classes make the"
        },
        {
          "complete representation of the data,\nsuch as frames with complete loss of key": "ResNet and DenseNet model cannot identify the difference, making it difficult"
        },
        {
          "complete representation of the data,\nsuch as frames with complete loss of key": "to differentiate and correctly classify the emotions."
        },
        {
          "complete representation of the data,\nsuch as frames with complete loss of key": "To solve this problem, I change my model\nfrom a classification task to a re-"
        },
        {
          "complete representation of the data,\nsuch as frames with complete loss of key": "gression task, categorizing emotions based on valence and arousal as continuous"
        },
        {
          "complete representation of the data,\nsuch as frames with complete loss of key": "outputs. This shift aimed to enhance the model’s capability to capture a more"
        },
        {
          "complete representation of the data,\nsuch as frames with complete loss of key": "accurate and more nuanced spectrum of emotional expressions. However,\nthe"
        },
        {
          "complete representation of the data,\nsuch as frames with complete loss of key": "there’re lots of similar images,\nit still potentially improve the accuracy."
        }
      ],
      "page": 19
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "8\nConclusion": "This project provided a good opportunity to use my background in computer"
        },
        {
          "8\nConclusion": "science\nto dive\ninto new areas across disciplines.\nThrough this\ninitiative,\nI"
        },
        {
          "8\nConclusion": "was able to take a hands-on approach to the development of computer vision"
        },
        {
          "8\nConclusion": "technology, methodically addressing challenges at each step of the process. As"
        },
        {
          "8\nConclusion": "the project progressed, my passion for the area of computer vision grew further."
        },
        {
          "8\nConclusion": "The main goal of\nthe project\nis\nto develop a predictive model capable of"
        },
        {
          "8\nConclusion": "recognizing seven different emotions: happiness, sadness, anger,\nfear, surprise,"
        },
        {
          "8\nConclusion": "disgust and neutral\nstates.\nI use OpenPose\nfor\nimage processing to extract"
        },
        {
          "8\nConclusion": "features\nrequired for emotion recognition. The method first pre-processes\nthe"
        },
        {
          "8\nConclusion": "images and then organizes\nthem into seven different\nfolders corresponding to"
        },
        {
          "8\nConclusion": "each emotion to facilitate\nthe\nclassification task.\nI also explore\nsubsampling"
        },
        {
          "8\nConclusion": "techniques to enhance the model’s ability to handle regression tasks. However,"
        },
        {
          "8\nConclusion": "due to the nature of the video-derived dataset (which mainly consists of sequen-"
        },
        {
          "8\nConclusion": "tial\nimages), the sample lacks diversity even after a subsampling effort."
        },
        {
          "8\nConclusion": "In the comparative analysis,\nI\nfound that ResNet and DenseNet get better"
        },
        {
          "8\nConclusion": "performance than traditional artificial neural networks (ANN) on both regres-"
        },
        {
          "8\nConclusion": "sion and classification tasks. Performance differences can be attributed to dif-"
        },
        {
          "8\nConclusion": "ferent datasets and structural differences\nin the models.\nIn the classification"
        },
        {
          "8\nConclusion": "task,\nit\nis clear\nthat\nthe model can more accurately predict certain emotions,"
        },
        {
          "8\nConclusion": "such as ”neutral” and ”happiness.” This may be because these emotions exhibit"
        },
        {
          "8\nConclusion": "unique and consistent cues that are easier for models to find out.\nIn contrast,"
        },
        {
          "8\nConclusion": "detect emotions\nsuch as\nfear and anger pose more significant difficult due to"
        },
        {
          "8\nConclusion": "subtle differences in expression."
        },
        {
          "8\nConclusion": "Furthermore, when performance was assessed in terms of valence (the in-"
        },
        {
          "8\nConclusion": "trinsic attractiveness or aversiveness of an event) and arousal (the physiological"
        },
        {
          "8\nConclusion": "and psychological state of being aroused or aroused),\nit was observed that these"
        },
        {
          "8\nConclusion": "models generally exhibited lower loss rates,\nsuggesting a greater sensitivity to"
        },
        {
          "8\nConclusion": "emotional\nintensity."
        },
        {
          "8\nConclusion": "Overall, the application of ResNet and DenseNet in this project proved they"
        },
        {
          "8\nConclusion": "are very effective for emotion recognition tasks. The use of the Adam optimizer"
        },
        {
          "8\nConclusion": "enhances the performance of them, achieving the best results in both regression"
        },
        {
          "8\nConclusion": "and classification of emotional states.\nI think that by adjusting the diversity of"
        },
        {
          "8\nConclusion": "training data and further tuning model parameters, the accuracy of valence and"
        },
        {
          "8\nConclusion": "arousal predictions may improve."
        },
        {
          "8\nConclusion": "8.1\nFurther Work"
        },
        {
          "8\nConclusion": "Possible further work could implement advanced subsampling techniques to se-"
        },
        {
          "8\nConclusion": "lectively retain representative images from sequences of continuous actions.,\nin"
        },
        {
          "8\nConclusion": "order to reduce the similarity of images between different classes. Possible is us-"
        },
        {
          "8\nConclusion": "ing a better computer which will provide a better GPU or multiple GPUs rather"
        },
        {
          "8\nConclusion": "than single GPU, can significantly reduce training and inference times. This"
        },
        {
          "8\nConclusion": "could also include exploring cloud-based solutions or specialized hardware like"
        },
        {
          "8\nConclusion": "TPU. Possible to use another CNN,\nlike DenseNet201, which features densely"
        }
      ],
      "page": 20
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "connected convolutional networks, or Vision Transformer (ViT), which applies": "transformer mechanisms to image recognition, might yield better or faster re-"
        },
        {
          "connected convolutional networks, or Vision Transformer (ViT), which applies": "sults.\nImplementing techniques\nlike grid search,\nrandom search, or Bayesian"
        },
        {
          "connected convolutional networks, or Vision Transformer (ViT), which applies": "optimization to automate the selection of model hyperparameters, which could"
        },
        {
          "connected convolutional networks, or Vision Transformer (ViT), which applies": "enhance model performance without extensive massive manual experimentation."
        },
        {
          "connected convolutional networks, or Vision Transformer (ViT), which applies": "The proposed directions\nfor\nfurther\nresearch in project\nsystems\nfocus on"
        },
        {
          "connected convolutional networks, or Vision Transformer (ViT), which applies": "three areas to overcome the weakness of project."
        },
        {
          "connected convolutional networks, or Vision Transformer (ViT), which applies": "First, optimizing data handling,\nfor example,\nsubsampling gets developing"
        },
        {
          "connected convolutional networks, or Vision Transformer (ViT), which applies": "more efficient methods\nfor processing and managing data, which can lead to"
        },
        {
          "connected convolutional networks, or Vision Transformer (ViT), which applies": "faster\nresponse\ntime and reduce\ncost of\ncomputational\nresource.\nThis might"
        },
        {
          "connected convolutional networks, or Vision Transformer (ViT), which applies": "include the implementation of advanced data subsampling or\nthe use of more"
        },
        {
          "connected convolutional networks, or Vision Transformer (ViT), which applies": "complex data pre-processing techniques to enhance the quality of the data."
        },
        {
          "connected convolutional networks, or Vision Transformer (ViT), which applies": "Second,\nexperimenting with more advance neural network architectures\nis"
        },
        {
          "connected convolutional networks, or Vision Transformer (ViT), which applies": "crucial\nfor improving the accuracy of this system. This might involve utilizing"
        },
        {
          "connected convolutional networks, or Vision Transformer (ViT), which applies": "new forms of deep learning models, such as recurrent neural networks (RNN) or"
        },
        {
          "connected convolutional networks, or Vision Transformer (ViT), which applies": "Generative Pre-trained Transformer (GPT), By modifying existing architectures"
        },
        {
          "connected convolutional networks, or Vision Transformer (ViT), which applies": "or creating new ones, model can be better capturing the nuances of emotional"
        },
        {
          "connected convolutional networks, or Vision Transformer (ViT), which applies": "expressions, potentially leading to breakthroughs with this project."
        },
        {
          "connected convolutional networks, or Vision Transformer (ViT), which applies": "Finally, these strategies aim to optimize the performance of project by mak-"
        },
        {
          "connected convolutional networks, or Vision Transformer (ViT), which applies": "ing them faster, more accurate, and capable of processing larger and more di-"
        },
        {
          "connected convolutional networks, or Vision Transformer (ViT), which applies": "verse datasets. These improvements are particularly important for applications"
        },
        {
          "connected convolutional networks, or Vision Transformer (ViT), which applies": "that demand real-time processing and high levels of precision, such as customer"
        },
        {
          "connected convolutional networks, or Vision Transformer (ViT), which applies": "service analysis, and healthcare monitoring, where rapid and reliable emotion"
        },
        {
          "connected convolutional networks, or Vision Transformer (ViT), which applies": "detection is crucial."
        }
      ],
      "page": 21
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[3]\nI. B. Mauss, A. S. Troy, and M. K. LeBourgeois, “Poorer sleep quality is": "associated with lower emotion-regulation ability in a laboratory paradigm,”"
        },
        {
          "[3]\nI. B. Mauss, A. S. Troy, and M. K. LeBourgeois, “Poorer sleep quality is": "Cognition & emotion, vol. 27, no. 3, pp. 567–576, 2013."
        },
        {
          "[3]\nI. B. Mauss, A. S. Troy, and M. K. LeBourgeois, “Poorer sleep quality is": "[4] D. Kollias,\nP. Tzirakis, M. A. Nicolaou, A. Papaioannou, G.\nZhao,"
        },
        {
          "[3]\nI. B. Mauss, A. S. Troy, and M. K. LeBourgeois, “Poorer sleep quality is": "B. Schuller,\nI. Kotsia,\nand S. Zafeiriou,\n“Deep affect prediction in-the-"
        },
        {
          "[3]\nI. B. Mauss, A. S. Troy, and M. K. LeBourgeois, “Poorer sleep quality is": "wild: Aff-wild database and challenge, deep architectures, and beyond,”"
        },
        {
          "[3]\nI. B. Mauss, A. S. Troy, and M. K. LeBourgeois, “Poorer sleep quality is": "International Journal of Computer Vision, vol. 127, no. 6, pp. 907–929,"
        },
        {
          "[3]\nI. B. Mauss, A. S. Troy, and M. K. LeBourgeois, “Poorer sleep quality is": "2019."
        },
        {
          "[3]\nI. B. Mauss, A. S. Troy, and M. K. LeBourgeois, “Poorer sleep quality is": "[5] D. Kollias\nand\nS.\nZafeiriou,\n“Expression,\naffect,\naction\nunit\nrecog-"
        },
        {
          "[3]\nI. B. Mauss, A. S. Troy, and M. K. LeBourgeois, “Poorer sleep quality is": "nition:\nAff-wild2,\nmulti-task\nlearning\nand\narcface,”\narXiv\npreprint"
        },
        {
          "[3]\nI. B. Mauss, A. S. Troy, and M. K. LeBourgeois, “Poorer sleep quality is": "arXiv:1910.04855, 2019."
        },
        {
          "[3]\nI. B. Mauss, A. S. Troy, and M. K. LeBourgeois, “Poorer sleep quality is": "[6] D. Kollias, “Abaw:\nLearning from synthetic data & multi-task learning"
        },
        {
          "[3]\nI. B. Mauss, A. S. Troy, and M. K. LeBourgeois, “Poorer sleep quality is": "challenges,”\nin European Conference on Computer Vision, pp. 157–172,"
        },
        {
          "[3]\nI. B. Mauss, A. S. Troy, and M. K. LeBourgeois, “Poorer sleep quality is": "Springer, 2022."
        },
        {
          "[3]\nI. B. Mauss, A. S. Troy, and M. K. LeBourgeois, “Poorer sleep quality is": "[7] D. Kollias, P. Tzirakis, A. Baird, A. Cowen,\nand S. Zafeiriou,\n“Abaw:"
        },
        {
          "[3]\nI. B. Mauss, A. S. Troy, and M. K. LeBourgeois, “Poorer sleep quality is": "Valence-arousal\nestimation,\nexpression recognition, action unit detection"
        },
        {
          "[3]\nI. B. Mauss, A. S. Troy, and M. K. LeBourgeois, “Poorer sleep quality is": "& emotional\nreaction intensity estimation challenges,”\nin Proceedings of"
        },
        {
          "[3]\nI. B. Mauss, A. S. Troy, and M. K. LeBourgeois, “Poorer sleep quality is": "the IEEE/CVF Conference on Computer Vision and Pattern Recognition,"
        },
        {
          "[3]\nI. B. Mauss, A. S. Troy, and M. K. LeBourgeois, “Poorer sleep quality is": "pp. 5888–5897, 2023."
        },
        {
          "[3]\nI. B. Mauss, A. S. Troy, and M. K. LeBourgeois, “Poorer sleep quality is": "[8] D. Kollias, K. Vendal, P. Gadhavi, and S. Russom, “Btdnet: A multi-modal"
        },
        {
          "[3]\nI. B. Mauss, A. S. Troy, and M. K. LeBourgeois, “Poorer sleep quality is": "approach for brain tumor\nradiogenomic classification,” Applied Sciences,"
        },
        {
          "[3]\nI. B. Mauss, A. S. Troy, and M. K. LeBourgeois, “Poorer sleep quality is": "vol. 13, no. 21, p. 11984, 2023."
        },
        {
          "[3]\nI. B. Mauss, A. S. Troy, and M. K. LeBourgeois, “Poorer sleep quality is": "[9] D. Kollias,\nA.\nPsaroudakis,\nA. Arsenos,\nand\nP. Theofilou,\n“Facer-"
        },
        {
          "[3]\nI. B. Mauss, A. S. Troy, and M. K. LeBourgeois, “Poorer sleep quality is": "net:\na\nfacial\nexpression\nintensity\nestimation\nnetwork,”\narXiv\npreprint"
        },
        {
          "[3]\nI. B. Mauss, A. S. Troy, and M. K. LeBourgeois, “Poorer sleep quality is": "arXiv:2303.00180, 2023."
        },
        {
          "[3]\nI. B. Mauss, A. S. Troy, and M. K. LeBourgeois, “Poorer sleep quality is": "[10] D. Kollias, “Multi-label compound expression recognition: C-expr database"
        },
        {
          "[3]\nI. B. Mauss, A. S. Troy, and M. K. LeBourgeois, “Poorer sleep quality is": "& network,” in Proceedings of\nthe\nIEEE/CVF Conference on Computer"
        },
        {
          "[3]\nI. B. Mauss, A. S. Troy, and M. K. LeBourgeois, “Poorer sleep quality is": "Vision and Pattern Recognition, pp. 5589–5598, 2023."
        },
        {
          "[3]\nI. B. Mauss, A. S. Troy, and M. K. LeBourgeois, “Poorer sleep quality is": "[11] D. Kollias, P. Tzirakis, A. Cowen,\nS. Zafeiriou, C. Shao,\nand G. Hu,"
        },
        {
          "[3]\nI. B. Mauss, A. S. Troy, and M. K. LeBourgeois, “Poorer sleep quality is": "“The 6th affective behavior analysis in-the-wild (abaw) competition,” arXiv"
        },
        {
          "[3]\nI. B. Mauss, A. S. Troy, and M. K. LeBourgeois, “Poorer sleep quality is": "preprint arXiv:2402.19344, 2024."
        },
        {
          "[3]\nI. B. Mauss, A. S. Troy, and M. K. LeBourgeois, “Poorer sleep quality is": "[12] D. Kollias, V. Sharmanska, and S. Zafeiriou, “Distribution matching for"
        },
        {
          "[3]\nI. B. Mauss, A. S. Troy, and M. K. LeBourgeois, “Poorer sleep quality is": "multi-task learning of classification tasks:\na large-scale study on faces &"
        },
        {
          "[3]\nI. B. Mauss, A. S. Troy, and M. K. LeBourgeois, “Poorer sleep quality is": "beyond,” arXiv preprint arXiv:2401.01219, 2024."
        },
        {
          "[3]\nI. B. Mauss, A. S. Troy, and M. K. LeBourgeois, “Poorer sleep quality is": "[13] D. Kollias, A. Arsenos, and S. Kollias, “Domain adaptation, explainability"
        },
        {
          "[3]\nI. B. Mauss, A. S. Troy, and M. K. LeBourgeois, “Poorer sleep quality is": "& fairness in ai\nfor medical\nimage analysis: Diagnosis of covid-19 based on"
        },
        {
          "[3]\nI. B. Mauss, A. S. Troy, and M. K. LeBourgeois, “Poorer sleep quality is": "3-d chest ct-scans,” arXiv preprint arXiv:2403.02192, 2024."
        }
      ],
      "page": 22
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[14] D. Kollias, S. Cheng, E. Ververas, I. Kotsia, and S. Zafeiriou, “Deep neural": "network augmentation: Generating faces for affect analysis,” International"
        },
        {
          "[14] D. Kollias, S. Cheng, E. Ververas, I. Kotsia, and S. Zafeiriou, “Deep neural": "Journal of Computer Vision, pp. 1–30, 2020."
        },
        {
          "[14] D. Kollias, S. Cheng, E. Ververas, I. Kotsia, and S. Zafeiriou, “Deep neural": "[15] S. Zafeiriou, D. Kollias, M. A. Nicolaou, A. Papaioannou, G. Zhao, and"
        },
        {
          "[14] D. Kollias, S. Cheng, E. Ververas, I. Kotsia, and S. Zafeiriou, “Deep neural": "I. Kotsia,\n“Aff-wild:\nvalence and arousal’in-the-wild’challenge,”\nin Pro-"
        },
        {
          "[14] D. Kollias, S. Cheng, E. Ververas, I. Kotsia, and S. Zafeiriou, “Deep neural": "ceedings of the IEEE conference on computer vision and pattern recognition"
        },
        {
          "[14] D. Kollias, S. Cheng, E. Ververas, I. Kotsia, and S. Zafeiriou, “Deep neural": "workshops, pp. 34–41, 2017."
        },
        {
          "[14] D. Kollias, S. Cheng, E. Ververas, I. Kotsia, and S. Zafeiriou, “Deep neural": "[16] G. Hu, E. Papadopoulou, D. Kollias, P. Tzouveli, J. Wei, and X. Yang,"
        },
        {
          "[14] D. Kollias, S. Cheng, E. Ververas, I. Kotsia, and S. Zafeiriou, “Deep neural": "“Bridging the gap: Protocol\ntowards\nfair and consistent affect analysis,”"
        },
        {
          "[14] D. Kollias, S. Cheng, E. Ververas, I. Kotsia, and S. Zafeiriou, “Deep neural": "arXiv preprint arXiv:2405.06841, 2024."
        },
        {
          "[14] D. Kollias, S. Cheng, E. Ververas, I. Kotsia, and S. Zafeiriou, “Deep neural": "[17] A. Psaroudakis and D. Kollias,\n“Mixaugment & mixup:\nAugmentation"
        },
        {
          "[14] D. Kollias, S. Cheng, E. Ververas, I. Kotsia, and S. Zafeiriou, “Deep neural": "methods for facial expression recognition,” in Proceedings of the IEEE/CVF"
        },
        {
          "[14] D. Kollias, S. Cheng, E. Ververas, I. Kotsia, and S. Zafeiriou, “Deep neural": "Conference on Computer Vision and Pattern Recognition, pp. 2367–2375,"
        },
        {
          "[14] D. Kollias, S. Cheng, E. Ververas, I. Kotsia, and S. Zafeiriou, “Deep neural": "2022."
        },
        {
          "[14] D. Kollias, S. Cheng, E. Ververas, I. Kotsia, and S. Zafeiriou, “Deep neural": "[18] D. Kollias, A. Schulc, E. Hajiyev, and S. Zafeiriou, “Analysing affective be-"
        },
        {
          "[14] D. Kollias, S. Cheng, E. Ververas, I. Kotsia, and S. Zafeiriou, “Deep neural": "havior in the first abaw 2020 competition,” in 2020 15th IEEE International"
        },
        {
          "[14] D. Kollias, S. Cheng, E. Ververas, I. Kotsia, and S. Zafeiriou, “Deep neural": "Conference on Automatic Face and Gesture Recognition (FG 2020)(FG),"
        },
        {
          "[14] D. Kollias, S. Cheng, E. Ververas, I. Kotsia, and S. Zafeiriou, “Deep neural": "pp. 794–800, 2020."
        },
        {
          "[14] D. Kollias, S. Cheng, E. Ververas, I. Kotsia, and S. Zafeiriou, “Deep neural": "[19] D. Kollias, V. Sharmanska, and S. Zafeiriou, “Distribution matching for"
        },
        {
          "[14] D. Kollias, S. Cheng, E. Ververas, I. Kotsia, and S. Zafeiriou, “Deep neural": "heterogeneous multi-task learning: a large-scale face study,” arXiv preprint"
        },
        {
          "[14] D. Kollias, S. Cheng, E. Ververas, I. Kotsia, and S. Zafeiriou, “Deep neural": "arXiv:2105.03790, 2021."
        },
        {
          "[14] D. Kollias, S. Cheng, E. Ververas, I. Kotsia, and S. Zafeiriou, “Deep neural": "[20] D. Kollias and S. Zafeiriou, “Affect analysis in-the-wild: Valence-arousal,"
        },
        {
          "[14] D. Kollias, S. Cheng, E. Ververas, I. Kotsia, and S. Zafeiriou, “Deep neural": "expressions,\naction\nunits\nand\na\nunified\nframework,”\narXiv\npreprint"
        },
        {
          "[14] D. Kollias, S. Cheng, E. Ververas, I. Kotsia, and S. Zafeiriou, “Deep neural": "arXiv:2103.15792, 2021."
        },
        {
          "[14] D. Kollias, S. Cheng, E. Ververas, I. Kotsia, and S. Zafeiriou, “Deep neural": "[21] D. Kollias, V. Sharmanska, and S. Zafeiriou, “Face behavior a la carte:"
        },
        {
          "[14] D. Kollias, S. Cheng, E. Ververas, I. Kotsia, and S. Zafeiriou, “Deep neural": "Expressions, affect and action units\nin a single network,” arXiv preprint"
        },
        {
          "[14] D. Kollias, S. Cheng, E. Ververas, I. Kotsia, and S. Zafeiriou, “Deep neural": "arXiv:1910.11111, 2019."
        },
        {
          "[14] D. Kollias, S. Cheng, E. Ververas, I. Kotsia, and S. Zafeiriou, “Deep neural": "[22] D. Kollias and S. Zafeiriou, “Analysing affective behavior\nin the\nsecond"
        },
        {
          "[14] D. Kollias, S. Cheng, E. Ververas, I. Kotsia, and S. Zafeiriou, “Deep neural": "abaw2 competition,” in Proceedings of\nthe IEEE/CVF International Con-"
        },
        {
          "[14] D. Kollias, S. Cheng, E. Ververas, I. Kotsia, and S. Zafeiriou, “Deep neural": "ference on Computer Vision, pp. 3652–3660, 2021."
        },
        {
          "[14] D. Kollias, S. Cheng, E. Ververas, I. Kotsia, and S. Zafeiriou, “Deep neural": "[23] D. Kollias, A. Arsenos, and S. Kollias, “Ai-enabled analysis of 3-d ct scans"
        },
        {
          "[14] D. Kollias, S. Cheng, E. Ververas, I. Kotsia, and S. Zafeiriou, “Deep neural": "for diagnosis of covid-19 & its severity,” in 2023 IEEE International Confer-"
        },
        {
          "[14] D. Kollias, S. Cheng, E. Ververas, I. Kotsia, and S. Zafeiriou, “Deep neural": "ence on Acoustics, Speech, and Signal Processing Workshops (ICASSPW),"
        },
        {
          "[14] D. Kollias, S. Cheng, E. Ververas, I. Kotsia, and S. Zafeiriou, “Deep neural": "pp. 1–5, IEEE, 2023."
        },
        {
          "[14] D. Kollias, S. Cheng, E. Ververas, I. Kotsia, and S. Zafeiriou, “Deep neural": "[24] D. Kollias, A. Arsenos, and S. Kollias, “A deep neural architecture for har-"
        },
        {
          "[14] D. Kollias, S. Cheng, E. Ververas, I. Kotsia, and S. Zafeiriou, “Deep neural": "monizing 3-d input data analysis and decision making in medical imaging,”"
        },
        {
          "[14] D. Kollias, S. Cheng, E. Ververas, I. Kotsia, and S. Zafeiriou, “Deep neural": "Neurocomputing, vol. 542, p. 126244, 2023."
        }
      ],
      "page": 23
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[25] A. Arsenos, A. Davidhi, D. Kollias, P. Prassopoulos, and S. Kollias, “Data-": "driven covid-19 detection through medical imaging,” in 2023 IEEE Interna-"
        },
        {
          "[25] A. Arsenos, A. Davidhi, D. Kollias, P. Prassopoulos, and S. Kollias, “Data-": "tional Conference on Acoustics, Speech, and Signal Processing Workshops"
        },
        {
          "[25] A. Arsenos, A. Davidhi, D. Kollias, P. Prassopoulos, and S. Kollias, “Data-": "(ICASSPW), pp. 1–5, IEEE, 2023."
        },
        {
          "[25] A. Arsenos, A. Davidhi, D. Kollias, P. Prassopoulos, and S. Kollias, “Data-": "[26] D. Gerogiannis, A. Arsenos, D. Kollias, D. Nikitopoulos, and S. Kollias,"
        },
        {
          "[25] A. Arsenos, A. Davidhi, D. Kollias, P. Prassopoulos, and S. Kollias, “Data-": "“Covid-19 computer-aided diagnosis\nthrough ai-assisted ct\nimaging anal-"
        },
        {
          "[25] A. Arsenos, A. Davidhi, D. Kollias, P. Prassopoulos, and S. Kollias, “Data-": "ysis: Deploying a medical ai\nsystem,” arXiv preprint arXiv:2403.06242,"
        },
        {
          "[25] A. Arsenos, A. Davidhi, D. Kollias, P. Prassopoulos, and S. Kollias, “Data-": "2024."
        },
        {
          "[25] A. Arsenos, A. Davidhi, D. Kollias, P. Prassopoulos, and S. Kollias, “Data-": "[27] X. Jiang, Y. Zong, W. Zheng, C. Tang, W. Xia, C. Lu, and J. Liu, “Dfew: A"
        },
        {
          "[25] A. Arsenos, A. Davidhi, D. Kollias, P. Prassopoulos, and S. Kollias, “Data-": "large-scale database for recognizing dynamic facial expressions in the wild,”"
        },
        {
          "[25] A. Arsenos, A. Davidhi, D. Kollias, P. Prassopoulos, and S. Kollias, “Data-": "in Proceedings of\nthe 28th ACM international conference on multimedia,"
        },
        {
          "[25] A. Arsenos, A. Davidhi, D. Kollias, P. Prassopoulos, and S. Kollias, “Data-": "pp. 2881–2889, 2020."
        },
        {
          "[25] A. Arsenos, A. Davidhi, D. Kollias, P. Prassopoulos, and S. Kollias, “Data-": "[28] N. Salpea, P. Tzouveli, and D. Kollias, “Medical\nimage segmentation: A"
        },
        {
          "[25] A. Arsenos, A. Davidhi, D. Kollias, P. Prassopoulos, and S. Kollias, “Data-": "review of modern architectures,” in European Conference on Computer Vi-"
        },
        {
          "[25] A. Arsenos, A. Davidhi, D. Kollias, P. Prassopoulos, and S. Kollias, “Data-": "sion, pp. 691–708, Springer, 2022."
        },
        {
          "[25] A. Arsenos, A. Davidhi, D. Kollias, P. Prassopoulos, and S. Kollias, “Data-": "[29] A. Arsenos, D. Kollias,\nE. Petrongonas,\nC.\nSkliros,\nand\nS. Kollias,"
        },
        {
          "[25] A. Arsenos, A. Davidhi, D. Kollias, P. Prassopoulos, and S. Kollias, “Data-": "“Uncertainty-guided contrastive learning for single source domain general-"
        },
        {
          "[25] A. Arsenos, A. Davidhi, D. Kollias, P. Prassopoulos, and S. Kollias, “Data-": "isation,” in ICASSP 2024-2024 IEEE International Conference on Acous-"
        },
        {
          "[25] A. Arsenos, A. Davidhi, D. Kollias, P. Prassopoulos, and S. Kollias, “Data-": "tics, Speech and Signal Processing (ICASSP), pp. 6935–6939, IEEE, 2024."
        },
        {
          "[25] A. Arsenos, A. Davidhi, D. Kollias, P. Prassopoulos, and S. Kollias, “Data-": "[30] V. Karampinis, A. Arsenos, O. Filippopoulos, E. Petrongonas, C. Skliros,"
        },
        {
          "[25] A. Arsenos, A. Davidhi, D. Kollias, P. Prassopoulos, and S. Kollias, “Data-": "D. Kollias, S. Kollias, and A. Voulodimos, “Ensuring uav safety: A vision-"
        },
        {
          "[25] A. Arsenos, A. Davidhi, D. Kollias, P. Prassopoulos, and S. Kollias, “Data-": "only and real-time framework for collision avoidance through object detec-"
        },
        {
          "[25] A. Arsenos, A. Davidhi, D. Kollias, P. Prassopoulos, and S. Kollias, “Data-": "tion, tracking, and distance estimation,” arXiv preprint arXiv:2405.06749,"
        },
        {
          "[25] A. Arsenos, A. Davidhi, D. Kollias, P. Prassopoulos, and S. Kollias, “Data-": "2024."
        },
        {
          "[25] A. Arsenos, A. Davidhi, D. Kollias, P. Prassopoulos, and S. Kollias, “Data-": "[31] A. Arsenos, E. Petrongonas, O. Filippopoulos, C. Skliros, D. Kollias, and"
        },
        {
          "[25] A. Arsenos, A. Davidhi, D. Kollias, P. Prassopoulos, and S. Kollias, “Data-": "S. Kollias,\n“Nefeli:\nA deep-learning detection and tracking pipeline\nfor"
        },
        {
          "[25] A. Arsenos, A. Davidhi, D. Kollias, P. Prassopoulos, and S. Kollias, “Data-": "enhancing autonomy in advanced air mobility,” Available at SSRN 4674579,"
        },
        {
          "[25] A. Arsenos, A. Davidhi, D. Kollias, P. Prassopoulos, and S. Kollias, “Data-": "2024."
        },
        {
          "[25] A. Arsenos, A. Davidhi, D. Kollias, P. Prassopoulos, and S. Kollias, “Data-": "[32] H. Miah, D. Kollias, G. L. Pedone, D. Provan, and F. Chen, “Can ma-"
        },
        {
          "[25] A. Arsenos, A. Davidhi, D. Kollias, P. Prassopoulos, and S. Kollias, “Data-": "chine learning assist in diagnosis of primary immune thrombocytopenia? a"
        },
        {
          "[25] A. Arsenos, A. Davidhi, D. Kollias, P. Prassopoulos, and S. Kollias, “Data-": "feasibility study,” arXiv preprint arXiv:2405.20562, 2024."
        },
        {
          "[25] A. Arsenos, A. Davidhi, D. Kollias, P. Prassopoulos, and S. Kollias, “Data-": "[33] A. Arsenos, V. Karampinis, E. Petrongonas, C. Skliros, D. Kollias, S. Kol-"
        },
        {
          "[25] A. Arsenos, A. Davidhi, D. Kollias, P. Prassopoulos, and S. Kollias, “Data-": "lias, and A. Voulodimos, “Common corruptions for evaluating and enhanc-"
        },
        {
          "[25] A. Arsenos, A. Davidhi, D. Kollias, P. Prassopoulos, and S. Kollias, “Data-": "ing robustness\nin air-to-air visual object detection,” IEEE Robotics and"
        },
        {
          "[25] A. Arsenos, A. Davidhi, D. Kollias, P. Prassopoulos, and S. Kollias, “Data-": "Automation Letters, 2024."
        },
        {
          "[25] A. Arsenos, A. Davidhi, D. Kollias, P. Prassopoulos, and S. Kollias, “Data-": "[34] F. Noroozi, C. A. Corneanu, D. Kami´nska, T. Sapi´nski, S. Escalera, and"
        },
        {
          "[25] A. Arsenos, A. Davidhi, D. Kollias, P. Prassopoulos, and S. Kollias, “Data-": "G. Anbarjafari,\n“Survey on emotional body gesture\nrecognition,”\nIEEE"
        },
        {
          "[25] A. Arsenos, A. Davidhi, D. Kollias, P. Prassopoulos, and S. Kollias, “Data-": "transactions on affective computing, vol. 12, no. 2, pp. 505–523, 2018."
        }
      ],
      "page": 24
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[35] L. Shu, J. Xie, M. Yang, Z. Li, Z. Li, D. Liao, X. Xu, and X. Yang, “A": "review of emotion recognition using physiological signals,” Sensors, vol. 18,"
        },
        {
          "[35] L. Shu, J. Xie, M. Yang, Z. Li, Z. Li, D. Liao, X. Xu, and X. Yang, “A": "no. 7, p. 2074, 2018."
        },
        {
          "[35] L. Shu, J. Xie, M. Yang, Z. Li, Z. Li, D. Liao, X. Xu, and X. Yang, “A": "[36] S. K. Khare, V. Blanes-Vidal, E. S. Nadimi, and U. R. Acharya, “Emotion"
        },
        {
          "[35] L. Shu, J. Xie, M. Yang, Z. Li, Z. Li, D. Liao, X. Xu, and X. Yang, “A": "recognition and artificial intelligence: A systematic review (2014–2023) and"
        },
        {
          "[35] L. Shu, J. Xie, M. Yang, Z. Li, Z. Li, D. Liao, X. Xu, and X. Yang, “A": "research recommendations,” Information Fusion, p. 102019, 2023."
        },
        {
          "[35] L. Shu, J. Xie, M. Yang, Z. Li, Z. Li, D. Liao, X. Xu, and X. Yang, “A": "[37] M. Zhang, Y. Zhou, X. Xu, Z. Ren, Y. Zhang, S. Liu, and W. Luo, “Multi-"
        },
        {
          "[35] L. Shu, J. Xie, M. Yang, Z. Li, Z. Li, D. Liao, X. Xu, and X. Yang, “A": "view emotional expressions dataset using 2d pose estimation,” Scientific"
        },
        {
          "[35] L. Shu, J. Xie, M. Yang, Z. Li, Z. Li, D. Liao, X. Xu, and X. Yang, “A": "Data, vol. 10, no. 1, p. 649, 2023."
        },
        {
          "[35] L. Shu, J. Xie, M. Yang, Z. Li, Z. Li, D. Liao, X. Xu, and X. Yang, “A": "[38] Z. Shen, J. Cheng, X. Hu, and Q. Dong, “Emotion recognition based on"
        },
        {
          "[35] L. Shu, J. Xie, M. Yang, Z. Li, Z. Li, D. Liao, X. Xu, and X. Yang, “A": "multi-view body gestures,” in 2019 ieee international conference on image"
        },
        {
          "[35] L. Shu, J. Xie, M. Yang, Z. Li, Z. Li, D. Liao, X. Xu, and X. Yang, “A": "processing (icip), pp. 3317–3321, IEEE, 2019."
        },
        {
          "[35] L. Shu, J. Xie, M. Yang, Z. Li, Z. Li, D. Liao, X. Xu, and X. Yang, “A": "[39] Z. Cao, T. Simon, S.-E. Wei, and Y. Sheikh, “Realtime multi-person 2d pose"
        },
        {
          "[35] L. Shu, J. Xie, M. Yang, Z. Li, Z. Li, D. Liao, X. Xu, and X. Yang, “A": "estimation using part affinity fields,” in Proceedings of the IEEE conference"
        },
        {
          "[35] L. Shu, J. Xie, M. Yang, Z. Li, Z. Li, D. Liao, X. Xu, and X. Yang, “A": "on computer vision and pattern recognition, pp. 7291–7299, 2017."
        },
        {
          "[35] L. Shu, J. Xie, M. Yang, Z. Li, Z. Li, D. Liao, X. Xu, and X. Yang, “A": "[40] D. Kollias, S. Zafeiriou, I. Kotsia, A. Dhall, S. Ghosh, C. Shao, and G. Hu,"
        },
        {
          "[35] L. Shu, J. Xie, M. Yang, Z. Li, Z. Li, D. Liao, X. Xu, and X. Yang, “A": "“7th abaw competition: Multi-task\nlearning\nand compound expression"
        },
        {
          "[35] L. Shu, J. Xie, M. Yang, Z. Li, Z. Li, D. Liao, X. Xu, and X. Yang, “A": "recognition,” arXiv preprint arXiv:2407.03835, 2024."
        },
        {
          "[35] L. Shu, J. Xie, M. Yang, Z. Li, Z. Li, D. Liao, X. Xu, and X. Yang, “A": "[41] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual\nlearning for\nimage"
        },
        {
          "[35] L. Shu, J. Xie, M. Yang, Z. Li, Z. Li, D. Liao, X. Xu, and X. Yang, “A": "recognition,” in Proceedings of the IEEE conference on computer vision and"
        },
        {
          "[35] L. Shu, J. Xie, M. Yang, Z. Li, Z. Li, D. Liao, X. Xu, and X. Yang, “A": "pattern recognition, pp. 770–778, 2016."
        },
        {
          "[35] L. Shu, J. Xie, M. Yang, Z. Li, Z. Li, D. Liao, X. Xu, and X. Yang, “A": "[42] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, “Densely"
        },
        {
          "[35] L. Shu, J. Xie, M. Yang, Z. Li, Z. Li, D. Liao, X. Xu, and X. Yang, “A": "connected convolutional networks,” in Proceedings of\nthe IEEE conference"
        },
        {
          "[35] L. Shu, J. Xie, M. Yang, Z. Li, Z. Li, D. Liao, X. Xu, and X. Yang, “A": "on computer vision and pattern recognition, pp. 4700–4708, 2017."
        },
        {
          "[35] L. Shu, J. Xie, M. Yang, Z. Li, Z. Li, D. Liao, X. Xu, and X. Yang, “A": "¨"
        },
        {
          "[35] L. Shu, J. Xie, M. Yang, Z. Li, Z. Li, D. Liao, X. Xu, and X. Yang, “A": "[43] B. Karatay, D. Be¸stepe, K. Sailunaz, T.\nOzyer,\nand R. Alhajj,\n“Cnn-"
        },
        {
          "[35] L. Shu, J. Xie, M. Yang, Z. Li, Z. Li, D. Liao, X. Xu, and X. Yang, “A": "transformer based emotion classification from facial expressions and body"
        },
        {
          "[35] L. Shu, J. Xie, M. Yang, Z. Li, Z. Li, D. Liao, X. Xu, and X. Yang, “A": "gestures,” Multimedia Tools and Applications, vol. 83, no. 8, pp. 23129–"
        },
        {
          "[35] L. Shu, J. Xie, M. Yang, Z. Li, Z. Li, D. Liao, X. Xu, and X. Yang, “A": "23171, 2024."
        },
        {
          "[35] L. Shu, J. Xie, M. Yang, Z. Li, Z. Li, D. Liao, X. Xu, and X. Yang, “A": "[44] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”"
        },
        {
          "[35] L. Shu, J. Xie, M. Yang, Z. Li, Z. Li, D. Liao, X. Xu, and X. Yang, “A": "arXiv preprint arXiv:1412.6980, 2014."
        }
      ],
      "page": 25
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A comprehensive survey on emotion recognition based on electroencephalograph (eeg) signals",
      "authors": [
        "K Kamble",
        "J Sengupta"
      ],
      "year": "2023",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "2",
      "title": "Sleep in children and adolescents with behavioral and emotional disorders",
      "authors": [
        "R Dahl",
        "A Harvey"
      ],
      "year": "2007",
      "venue": "Sleep medicine clinics"
    },
    {
      "citation_id": "3",
      "title": "Poorer sleep quality is associated with lower emotion-regulation ability in a laboratory paradigm",
      "authors": [
        "I Mauss",
        "A Troy",
        "M Lebourgeois"
      ],
      "year": "2013",
      "venue": "Cognition & emotion"
    },
    {
      "citation_id": "4",
      "title": "Deep affect prediction in-thewild: Aff-wild database and challenge, deep architectures, and beyond",
      "authors": [
        "D Kollias",
        "P Tzirakis",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "B Schuller",
        "I Kotsia",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "5",
      "title": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "arxiv": "arXiv:1910.04855"
    },
    {
      "citation_id": "6",
      "title": "Abaw: Learning from synthetic data & multi-task learning challenges",
      "authors": [
        "D Kollias"
      ],
      "year": "2022",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "7",
      "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & emotional reaction intensity estimation challenges",
      "authors": [
        "D Kollias",
        "P Tzirakis",
        "A Baird",
        "A Cowen",
        "S Zafeiriou"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "8",
      "title": "Btdnet: A multi-modal approach for brain tumor radiogenomic classification",
      "authors": [
        "D Kollias",
        "K Vendal",
        "P Gadhavi",
        "S Russom"
      ],
      "year": "2023",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "9",
      "title": "Facernet: a facial expression intensity estimation network",
      "authors": [
        "D Kollias",
        "A Psaroudakis",
        "A Arsenos",
        "P Theofilou"
      ],
      "year": "2023",
      "venue": "Facernet: a facial expression intensity estimation network",
      "arxiv": "arXiv:2303.00180"
    },
    {
      "citation_id": "10",
      "title": "Multi-label compound expression recognition: C-expr database & network",
      "authors": [
        "D Kollias"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "11",
      "title": "The 6th affective behavior analysis in-the-wild (abaw) competition",
      "authors": [
        "D Kollias",
        "P Tzirakis",
        "A Cowen",
        "S Zafeiriou",
        "C Shao",
        "G Hu"
      ],
      "year": "2024",
      "venue": "The 6th affective behavior analysis in-the-wild (abaw) competition",
      "arxiv": "arXiv:2402.19344"
    },
    {
      "citation_id": "12",
      "title": "Distribution matching for multi-task learning of classification tasks: a large-scale study on faces & beyond",
      "authors": [
        "D Kollias",
        "V Sharmanska",
        "S Zafeiriou"
      ],
      "year": "2024",
      "venue": "Distribution matching for multi-task learning of classification tasks: a large-scale study on faces & beyond",
      "arxiv": "arXiv:2401.01219"
    },
    {
      "citation_id": "13",
      "title": "Domain adaptation, explainability & fairness in ai for medical image analysis: Diagnosis of covid-19 based on 3-d chest ct-scans",
      "authors": [
        "D Kollias",
        "A Arsenos",
        "S Kollias"
      ],
      "year": "2024",
      "venue": "Domain adaptation, explainability & fairness in ai for medical image analysis: Diagnosis of covid-19 based on 3-d chest ct-scans",
      "arxiv": "arXiv:2403.02192"
    },
    {
      "citation_id": "14",
      "title": "Deep neural network augmentation: Generating faces for affect analysis",
      "authors": [
        "D Kollias",
        "S Cheng",
        "E Ververas",
        "I Kotsia",
        "S Zafeiriou"
      ],
      "year": "2020",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "15",
      "title": "Aff-wild: valence and arousal'in-the-wild'challenge",
      "authors": [
        "S Zafeiriou",
        "D Kollias",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "I Kotsia"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition workshops"
    },
    {
      "citation_id": "16",
      "title": "Bridging the gap: Protocol towards fair and consistent affect analysis",
      "authors": [
        "G Hu",
        "E Papadopoulou",
        "D Kollias",
        "P Tzouveli",
        "J Wei",
        "X Yang"
      ],
      "year": "2024",
      "venue": "Bridging the gap: Protocol towards fair and consistent affect analysis",
      "arxiv": "arXiv:2405.06841"
    },
    {
      "citation_id": "17",
      "title": "Mixaugment & mixup: Augmentation methods for facial expression recognition",
      "authors": [
        "A Psaroudakis",
        "D Kollias"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "18",
      "title": "Analysing affective behavior in the first abaw 2020 competition",
      "authors": [
        "D Kollias",
        "A Schulc",
        "E Hajiyev",
        "S Zafeiriou"
      ],
      "year": "2020",
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)(FG)"
    },
    {
      "citation_id": "19",
      "title": "Distribution matching for heterogeneous multi-task learning: a large-scale face study",
      "authors": [
        "D Kollias",
        "V Sharmanska",
        "S Zafeiriou"
      ],
      "year": "2021",
      "venue": "Distribution matching for heterogeneous multi-task learning: a large-scale face study",
      "arxiv": "arXiv:2105.03790"
    },
    {
      "citation_id": "20",
      "title": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2021",
      "venue": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "arxiv": "arXiv:2103.15792"
    },
    {
      "citation_id": "21",
      "title": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "authors": [
        "D Kollias",
        "V Sharmanska",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "arxiv": "arXiv:1910.11111"
    },
    {
      "citation_id": "22",
      "title": "Analysing affective behavior in the second abaw2 competition",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "23",
      "title": "Ai-enabled analysis of 3-d ct scans for diagnosis of covid-19 & its severity",
      "authors": [
        "D Kollias",
        "A Arsenos",
        "S Kollias"
      ],
      "year": "2023",
      "venue": "2023 IEEE International Conference on Acoustics, Speech, and Signal Processing Workshops (ICASSPW)"
    },
    {
      "citation_id": "24",
      "title": "A deep neural architecture for harmonizing 3-d input data analysis and decision making in medical imaging",
      "authors": [
        "D Kollias",
        "A Arsenos",
        "S Kollias"
      ],
      "year": "2023",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "25",
      "title": "Datadriven covid-19 detection through medical imaging",
      "authors": [
        "A Arsenos",
        "A Davidhi",
        "D Kollias",
        "P Prassopoulos",
        "S Kollias"
      ],
      "year": "2023",
      "venue": "2023 IEEE International Conference on Acoustics, Speech, and Signal Processing Workshops (ICASSPW)"
    },
    {
      "citation_id": "26",
      "title": "Covid-19 computer-aided diagnosis through ai-assisted ct imaging analysis: Deploying a medical ai system",
      "authors": [
        "D Gerogiannis",
        "A Arsenos",
        "D Kollias",
        "D Nikitopoulos",
        "S Kollias"
      ],
      "year": "2024",
      "venue": "Covid-19 computer-aided diagnosis through ai-assisted ct imaging analysis: Deploying a medical ai system",
      "arxiv": "arXiv:2403.06242"
    },
    {
      "citation_id": "27",
      "title": "Dfew: A large-scale database for recognizing dynamic facial expressions in the wild",
      "authors": [
        "X Jiang",
        "Y Zong",
        "W Zheng",
        "C Tang",
        "W Xia",
        "C Lu",
        "J Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM international conference on multimedia"
    },
    {
      "citation_id": "28",
      "title": "Medical image segmentation: A review of modern architectures",
      "authors": [
        "N Salpea",
        "P Tzouveli",
        "D Kollias"
      ],
      "year": "2022",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "29",
      "title": "Uncertainty-guided contrastive learning for single source domain generalisation",
      "authors": [
        "A Arsenos",
        "D Kollias",
        "E Petrongonas",
        "C Skliros",
        "S Kollias"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "30",
      "title": "Ensuring uav safety: A visiononly and real-time framework for collision avoidance through object detection, tracking, and distance estimation",
      "authors": [
        "V Karampinis",
        "A Arsenos",
        "O Filippopoulos",
        "E Petrongonas",
        "C Skliros",
        "D Kollias",
        "S Kollias",
        "A Voulodimos"
      ],
      "year": "2024",
      "venue": "Ensuring uav safety: A visiononly and real-time framework for collision avoidance through object detection, tracking, and distance estimation",
      "arxiv": "arXiv:2405.06749"
    },
    {
      "citation_id": "31",
      "title": "Nefeli: A deep-learning detection and tracking pipeline for enhancing autonomy in advanced air mobility",
      "authors": [
        "A Arsenos",
        "E Petrongonas",
        "O Filippopoulos",
        "C Skliros",
        "D Kollias",
        "S Kollias"
      ],
      "year": "2024",
      "venue": "Nefeli: A deep-learning detection and tracking pipeline for enhancing autonomy in advanced air mobility"
    },
    {
      "citation_id": "32",
      "title": "Can machine learning assist in diagnosis of primary immune thrombocytopenia? a feasibility study",
      "authors": [
        "H Miah",
        "D Kollias",
        "G Pedone",
        "D Provan",
        "F Chen"
      ],
      "year": "2024",
      "venue": "Can machine learning assist in diagnosis of primary immune thrombocytopenia? a feasibility study",
      "arxiv": "arXiv:2405.20562"
    },
    {
      "citation_id": "33",
      "title": "Common corruptions for evaluating and enhancing robustness in air-to-air visual object detection",
      "authors": [
        "A Arsenos",
        "V Karampinis",
        "E Petrongonas",
        "C Skliros",
        "D Kollias",
        "S Kollias",
        "A Voulodimos"
      ],
      "year": "2024",
      "venue": "IEEE Robotics and Automation Letters"
    },
    {
      "citation_id": "34",
      "title": "Survey on emotional body gesture recognition",
      "authors": [
        "F Noroozi",
        "C Corneanu",
        "D Kamińska",
        "T Sapiński",
        "S Escalera",
        "G Anbarjafari"
      ],
      "year": "2018",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "35",
      "title": "A review of emotion recognition using physiological signals",
      "authors": [
        "L Shu",
        "J Xie",
        "M Yang",
        "Z Li",
        "Z Li",
        "D Liao",
        "X Xu",
        "X Yang"
      ],
      "year": "2018",
      "venue": "Sensors"
    },
    {
      "citation_id": "36",
      "title": "Emotion recognition and artificial intelligence: A systematic review (2014-2023) and research recommendations",
      "authors": [
        "S Khare",
        "V Blanes-Vidal",
        "E Nadimi",
        "U Acharya"
      ],
      "year": "2023",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "37",
      "title": "Multiview emotional expressions dataset using 2d pose estimation",
      "authors": [
        "M Zhang",
        "Y Zhou",
        "X Xu",
        "Z Ren",
        "Y Zhang",
        "S Liu",
        "W Luo"
      ],
      "year": "2023",
      "venue": "Scientific Data"
    },
    {
      "citation_id": "38",
      "title": "Emotion recognition based on multi-view body gestures",
      "authors": [
        "Z Shen",
        "J Cheng",
        "X Hu",
        "Q Dong"
      ],
      "year": "2019",
      "venue": "2019 ieee international conference on image processing"
    },
    {
      "citation_id": "39",
      "title": "Realtime multi-person 2d pose estimation using part affinity fields",
      "authors": [
        "Z Cao",
        "T Simon",
        "S.-E Wei",
        "Y Sheikh"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "40",
      "title": "7th abaw competition: Multi-task learning and compound expression recognition",
      "authors": [
        "D Kollias",
        "S Zafeiriou",
        "I Kotsia",
        "A Dhall",
        "S Ghosh",
        "C Shao",
        "G Hu"
      ],
      "year": "2024",
      "venue": "7th abaw competition: Multi-task learning and compound expression recognition",
      "arxiv": "arXiv:2407.03835"
    },
    {
      "citation_id": "41",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "42",
      "title": "Densely connected convolutional networks",
      "authors": [
        "G Huang",
        "Z Liu",
        "L Van Der Maaten",
        "K Weinberger"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "43",
      "title": "Cnntransformer based emotion classification from facial expressions and body gestures",
      "authors": [
        "B Karatay",
        "D Beştepe",
        "K Sailunaz",
        "T Özyer",
        "R Alhajj"
      ],
      "year": "2024",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "44",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    }
  ]
}