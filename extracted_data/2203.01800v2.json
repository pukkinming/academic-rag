{
  "paper_id": "2203.01800v2",
  "title": "Automatic Facial Paralysis Estimation With Facial Action Units",
  "published": "2022-03-03T16:14:49Z",
  "authors": [
    "Xuri Ge",
    "Joemon M. Jose",
    "Pengcheng Wang",
    "Arunachalam Iyer",
    "Xiao Liu",
    "Hu Han"
  ],
  "keywords": [
    "Facial action units",
    "Facial paralysis estimation",
    "Facial palsy",
    "Skip-BiLSTM",
    "Fusion&Refining"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Facial palsy is unilateral facial nerve weakness or paralysis of rapid onset with unknown cause. Automatically estimating facial palsy severeness can be helpful for the diagnosis and treatment of people suffering from it across the world. In this work, we develop and experiment with a novel model for estimating facial palsy severity. For this, an effective Facial Action Units (AU) detection technique is incorporated into our model, where AUs refer to a unique set of facial muscle movements used to describe almost every anatomically possible facial expression. In this paper, we propose a novel Adaptive Local-Global Relational Network (ALGRNet) for facial AU detection and use it to classify facial paralysis severity. ALGRNet mainly consists of three main novel structures: (i) an adaptive region learning module that learns the adaptive muscle regions based on the detected landmarks; (ii) a skip-BiLSTM that models the latent relationships among local AUs; and (iii) a feature fusion&refining module that investigates the complementary between the local and global face. Quantitative results on two AU benchmarks, i.e., BP4D and DISFA, demonstrate our ALGRNet can achieve promising AU detection accuracy. We further demonstrate the effectiveness of its application to facial paralysis estimation by migrating ALGRNet to a facial paralysis dataset collected and annotated by medical professionals.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "R ECENTLY, facial action units detection has attracted increasing research attention in computer vision due to its wide range of potential applications in facial state analysis, i.e., diagnosing mental disease  [1] , face recognition  [2] , improving e-learning experiences  [3] , deception detection  [4] , etc. On a similar line to these applications, facial palsy is affecting a large number of population and automatic estimation of facial palsy severeness can be useful for both diagnosis and treatment. Facial palsy is the temporary or permanent weakness or lack of movement affecting one side of the face and is an acute, unilateral facial nerve weakness or paralysis of rapid onset (less than 72 hours) and unknown cause. It affects around 23 per 100,000 people per year, and current methods for facial palsy severity estimation is a relatively subjective process. However, in the literature it is rare to see such studies that extending the AU detection model to the assessment of facial paralysis grades. In fact, Manuscript received  April 19, 2005 ; revised August 26, 2015.\n\nthe severity of facial paralysis can be estimated by the manifestations of muscle areas of the face together, which is really similar to the representation of individual expressions using the Facial Action Coding System (FACS)  [5] . In this study, we propose a novel AU detection model and explore its ability to estimate facial paralysis severity.\n\nFrom a biological perspective, the activation of AU corresponds to the movement of facial muscles; however, AU detection is challenging because of the subtle facial changes caused by AU. Hand-crafted features are used to represent the appearance of different local facial regions in early works  [6] ,  [7] . In fact, this also applies to some works on facial state analyses, such as facial paralysis estimation  [8] ,  [9]  and patient pain detection  [10] . However, due to their shallow nature, hand-crafted features are not discriminative enough to depict the morphological variations in the face. In order to improve the feature representation of AUs, deep learning-based approaches for AU detection have been developed in recent years.\n\nTo improve the AU representation, most existing facial AU detection methods combine local features from numerous independent AU branches, each corresponding to a separate AU patch. As shown in Fig.  1  (a), some grid-based deep learning studies  [11] ,  [12]  combine regional (patchbased) Convolutional Neural Network (CNN) features from a face with equal grids. For example, LP-Net  [13]  using an LSTM model  [14]  to combine the local CNN features from equal partition grids. However, there are two significant issues with dividing the image into fixed grids; (i) It is hard to focus accurately on the muscle region related to each AU patch; and (ii) grid-based features may not adequately reflect the ROIs of irregularly shaped AU patches. Recent popular multi-branch combination-based methods  [15] ,  [16] ,  [17]  refine the AU-related features with irregular regions by fusing global or local features from independent AU branches based on the detected corresponding muscle region, as shown in Fig.  1 (b) . For example, the scheme in  [18]  joints face alignment and AU detection in an endto-end architecture, designed to extract corresponding AU features using multiple branches based on the detected and calculated AU centre coordinates. Furthermore, the latest approach  [17]  proposes a local AU recognition loss that refines the local attention map by the near-region pixel contributions for local regions in independent branches, rather than a pre-defined attention map; however, which also ignores the interrelationship between multiple AU areas of each face.\n\nWhile AU detection methods based on multi-branch combinations have shown their success in the fusion of local AU features, limitations remain in establishing interrelationships between adaptive AU regions and in modelling local and global context. Firstly, unlike the localisation of corresponding AU regions based on fixed landmarks, adaptive learning of AU regions (shape and position) can improve the robustness of the model based on the diversity of expressions and individual characteristics, which however is usually neglected by the exiting literature. Secondly, according to the statistics of FACS  [5] , some patches corresponding to AUs are strongly correlated in some specific expressions (here we define positive correlation (mutual assistance) if multiple patches jointly influence the activation of the target AU, otherwise negative correlation (mutual exclusion)). For instance, the cheek area and the mouth corner of the face usually active simultaneously in a common facial behaviour called Duchenne smile, resulting in high correlations between AU6 (cheek raiser) and AU12 (lip corner puller). Furthermore, due to muscle linkage, adjacent AU2 (Outer Brow Raiser) and AU7 (Lid Tightener) will usually be activated simultaneously as the startle. Inspired by these biological phenomena, we believe that it is important to capture the interactive relationship between patch-based branches, such as sequential/skipping information transfer of adjacent/non-adjacent related muscle regions, to enhance the AU features. Furthermore, the muscle activation areas of AU are often irregular due to individual and expression differences, and some non-AU areas are also commonly activated due to muscle linkage. We therefore argue that it is vital to use the information of global faces to complement the personalisation of each AU in terms of different individuals and expressions.\n\nTo this end, we propose a novel ALGRNet for AU detection and apply it to the facial paralysis estimation task to validate its robustness and transferability. Specifically, gridbased global features are extracted from a stem network consisting of multiple convolutional layers and local AU features are extracted from the calculated regions based on the detected AU centres. Different from previous methods, we calculate AU centres to suit different individuals and expressions by learning the landmarks and corresponding offsets. To catch the potential positive and negative relations among the branches, a skip-BiLSTM module is designed. To model the mutual support and exclusion information, the adjacent patches are transferred in BiLSTM  [19]  while the distant patches are connected via skipping-type gates. We model each branch as independent and equal and hence our kip connection manner can minimize the loss of information compared with traditional BiLSTM. Subsequently, in order to fuse global features to each local AU feature and also with even non-AU regional features, and in contrast to the previous approaches  [17] , a novel gated fusion architecture in the new feature fusion&refining module is proposed. This is important considering that the different AUs extracted from the face may focus on different information across the face regions. Finally, AU features combining other beneficial AU regions as well as non-AU regions are fed into a multibranch classification network for AU detection or facial palsy class estimation.\n\nOur contributions can be summarized as follows:\n\n•\n\nWe propose a skip-BiLSTM module to improve the robustness of local AU representations by modeling the mutual assistance and exclusion relationships of individual AUs based on the learned adaptive landmarks;\n\n• We propose a feature fusion&refining module, filtering information that contributes to the target AU, even non-AU areas, to facilitate more discriminative local AU feature generation;",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "•",
      "text": "The proposed ALGRNet has established new stateof-the-art performance for AU detection on two benchmark datasets, i.e., BP4D and DISFA, without any external data or pre-trained models in additional data. Notably, we exploit a facial paralysis dataset, named FPara, to verify that the proposed AU detection model can be applied to facial paralysis estimation and achieves superior performance than the baseline methods.\n\nIn comparison to the earlier conference version  [20]  of this work, we propose a new adaptive region learning module in Section 3.2 in order to further improve the accuracy of muscle regions and to better adapt to irregular muscle shapes. In particular, the adaptive region learning module contains learning of scaling factors to change the size of the corresponding muscle regions, as well as offset learning to slightly adjust landmark differences, with respect to different individuals. This suggests that adaptive region learning could better help the model to focus more accurately on the muscle region changes corresponding to each AU, and to obtain stronger robustness and generalisation ability. In addition, we did not evaluate the generalizability and transferability of the AU detection presented in the previous version, whereas we attempt it in the present study. Facial paralysis estimation is a time-consuming and subjective task for a traditional physician's diagnosis. Facial palsy is a condition characterised by motor dysfunction of the muscles of facial expression. It is usually qualified by observing the activation status of certain muscle areas and facial symmetry when the patient makes certain expressions, such as basic eyebrow raising, eye closing and mouth puckering, etc. In this study, we apply the proposed ALGRNet on facial paralysis estimation, which can improve the effectiveness of facial paralysis recognition and estimation by focusing on the activation of multiple muscle regions as well as global facial information. Specifically, we exploit a facial paralysis dataset which is annotated by medical professions to four grades of facial palsy degrees, i.e. normal, low, medium and high grade. For facial paralysis estimation we focus on the muscle areas that are more preferred in the facial paralysis ratings rather than the AU predefined muscle regions. Finally, we combine the multiple muscle region features enhanced by the interaction, as well as the useful global information, to obtain the final facial features for the facial palsy grade classification. To the best of our knowledge, there is no existing work in the literature on the estimation of facial paralysis using AU recognition methods. And we show the effectiveness and transferability of the proposed ALGRNet quantitatively through the Facial Palsy Assessment application, which was not present in our earlier conference version  [20] .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Facial Action Units Detection",
      "text": "Automatic AU detection is a task that detects the movement of a set of facial muscles. Recently, patch-learning based methods are the most popular paradigms for AU detection  [21] ,  [22] ,  [23] ,  [24] ,  [25] ,  [26] ,  [27] ,  [28] . For instance,  [29]  used the composition rules of different fixed patches for different AUs to recover facial expressions by sparse coding.  [30]  used a CNNs and BiLSTM to extract and model the image regions for AUs, which are pre-select by domain knowledge and facial geometry. However, all above methods need to pre-defined the patch location first. To address these issues,  [18]  proposed to jointly estimate the location of landmarks and the presence of action units in an end-toend framework, where landmarks can be used to compute the attention map for each AU separately. Recent works  [31] ,  [32] ,  [33] ,  [34] ,  [35] ,  [36]  explicitly take into consideration the linkage relationship between different AUs for AU detection, which rely on action unit relationship modeling to help improve recognition accuracy. Typically,  [31] ,  [37]  exploited the relationships between AU labels via a dynamic Bayesian network.  [38]  embedded the relations among AUs through a predefined graph convolutional network (GCN).  [39]  integrated the prior knowledge from FACS into an offline graph, which can construct a knowledge graph coding the AU correlations. However, these methods require prior connections by counting the co-occurrence probabilities in different datasets in advance.  [36] ,  [40] ,  [41]  applied a adaptive graph to model the relationships between AUs based on global feature, ignoring local-global interactions.\n\nThe most relevant previous studies to ours are  [17] ,  [18] , which combine AU detection and face alignment into a multi-branch network. Different from these methods, our ALGRNet can adaptively adjust the target muscle region corresponding to each AU and utilizes the learned mutual assistance and exclusion relationships between the target muscle and other muscle regions to enhance the feature representation of the target AU. Doing so allows us to provide more robustness and interpretability than  [17] .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Facial Paralysis Estimation",
      "text": "Facial paralysis estimation has recently attracted extensive research attention  [42] ,  [43] , due to the significant psychological and functional impairment to the patients. Nottingham system  [44]  is a widely accepted system for the clinical assessment of facial nerve function, which is similar with House-Brackmann (H-B)  [45] . In addition to these, there are over twenty other methods of recognising and assessing facial palsy that are available in the literature. However, all of the above traditional methods are estimated by medical professionals and are both time consuming and subjective. More Recently, deep learning has been widely applied for facial representation learning, including using the deep representation for face recognition, face alignment, etc.  [46]  and  [47]  proposed two efficient quantitative assessment of facial paralysis based on the detected key points.  [8]  proposed to obtain the facial paralysis degree by calculating the changes of the surface areas of specific facial region.  [48]  considered both static facial asymmetry and dynamic transformation factors to evaluate the degree of facial paralysis. However, most of the exiting methods were only used the deep learning methods to pave the way for physical computation and do not directly model and predict the depth features of a face image. In addition, they apply some post-processing to obtain the final result.\n\nIn contrast to these existing methods, we employ an endto-end deep framework ALGRNet to predict the grade of facial paralysis. We perform an automated estimation by combining depth features from high interest muscle regions with feature information from the global face, without any post-processing.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Approach",
      "text": "The framework of the proposed ALGRNet for AU detection is presented in Fig.  2 . It is composed of four main modules, i.e., adaptive region learning module (Subsection 3.2) for adaptive muscle region localisation, a skip-BiLSTM module (Subsection 3.3) for mutual facilitation and inhibition modeling, a feature fusion&refining module (Subsection 3.4) for refining features of irregular muscle regions, and a multi-classifier module (Subsection 3.1) for predicting the AU activation probability. Note that, our ALGRNet, when applied to facial palsy detection, only modifies the definition of muscle regions according to the recommendations of the physician.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Lmvrrnet: Learning Multi-Vision Relational Representation Network For Facial Action Units Recognition",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Overview Of Algrnet",
      "text": "Our method employs a multi-branch network  [17] ,  [18] ,  [49]  for both facial paralysis estimation and facial AU detection tasks. However, in contrast to previous methods, we believe that exploiting the relationship between multiple patches plays a crucial role in building a robust model for AU detection. In addition, due to the diversity of expressions and individual characteristics, we also attempted to learn adaptive muscle region offsets and scaling factors for each AU region. To this end, we design three modules (adaptive region learning module, skip-BiLSTM module and feature Fusion&Refining module) based on established multibranch network that can fully exploit inter-regional and local-global interactions on the basis of adaptively adjusted muscle localization.\n\nWe first adapt a hierarchical and multi-scale region learning network from  [18]  as our stem network, which is used to extract the grid-based global feature and the local muscle region features. However, unlike the predefined muscle regions based on the detected landmark in  [18] , we add two simple networks combined with the previous face alignment network, named adaptive region learning module (detailed in Section 3.2), to adaptively learn the offsets and scaling factors for each region. After that, local patches A = {A 1 , A 2 , ..., A n } are computed from the learned locations and their features V = {v 1 , v 2 , ..., v n } can be extracted through the stem network, where n is the numbers of selected patches. For the sake of simplicity, we do not repeat here the detailed structure of the stem network. The detailed structure of the stem network is not repeated here for the sake of simplicity.\n\nIn our ALGRNet, we design a novel skip-BiLSTM module (detailed in Section 3.3) to address the lack of sufficient delivery of local patch information between individual branches, which can transmit information in two ways (sequential delivery and skipping delivery) on both two directions (forward and backward), in contrast to the traditional sequence spreading of LSTM. The sequential delivery of information enables full exploration of the contextual relationships between adjacent patches. The skipping delivery highlight the interaction of information from nonadjacent related patches. After skip-BiLSTM, we get a set of local patch features S = {s 1 , s 2 , ..., s n }, which are expected to have all the useful information from adjacent and nonadjacent AU patches. Furthermore, a novel feature Fusion&Refining module (detailed in Section 3.4) is developed to deal with irregular muscle areas, which can refine the local patches to obtain salient micro-level features for the global facial feature G. Finally, the new patch-based representations R = {r 1 , r 2 , ..., r n } for AUs are obtained by integrating local muscle features and global facial features.\n\nIn this work, we integrate face alignment and face AU detection (or facial paralysis estimation) into an end-toend learning model. Our goal is to jointly learn all the parameters by minimizing both face alignment loss and facial AU detection loss (or facial paralysis estimation loss) over the training set. The face alignment loss is defined as:\n\nwhere (x i , y i ) and (x i , ŷi ) denote the ground-truth (GT) coordinate and corresponding predicted coordinate of the i-th facial landmark, and d o is the ground-truth inter-ocular distance for normalization  [17] .\n\nIn this paper, we also regard facial AU detection as a multi-label binary classification task following  [17] . It can be formulated as a supervised classification training objective as follows,\n\nwhere p i denotes the GT probability of occurrence for the i-th AU, which is 1 if occurrence and 0 otherwise, and pi denotes the predicted probability of occurrence. w i is the data balance weights, which are same as in  [18] . Moreover, we also employ a weighted multi-label Dice coefficient loss  [50]  to overcome the sample imbalance problem, which is formulated as:\n\nwhere τ is the smoothness parameter . Finally, the facial AU detection loss is defined as:\n\nFurthermore, we also minimize the loss of AU category classification L int by integrating all AUs information, including the refined AU features and the face alignment features, which is similar to the processing of L au . Finally, the joint loss of our ALGRNet for facial AU detection is defined as:\n\nwhere λ is a balancing parameter. Similar with AU detection, we also joint the loss of facial paralysis estimation and face alignment, where the loss of facial paralysis estimation is formulated as:\n\nwhere q and q are the label and predicted probability for the facial palsy grades, respectively. w i is also the data balance weights, obtained by counting the different classes in the training set. Finally, we optimize the whole end-to-end network by minimizing the jointly loss function L par + λL align over the training set.  In particular, we first use the same rules in  [17]  to get the locations of AU centers based on the detected landmarks and then update the by adding the learned offsets. Please note that, we change the predefined muscle region centers, as shown in Fig.  3  1 , based on the detected landmarks when we apply ALGRNet on facial paralysis estimation. Different from  [17] , we make the scaling factor E learnable rather than a fixed value, where e i is the width ratio between the region of AU i and whole feature map. After that, we generate an approximate Gaussian attention distribution for each AU region following  [18] . Finally, based on the learned regions, local patch features V are extracted via the stem network.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Skip-Bilstm",
      "text": "Fig.  2  (b) shows the detailed structure of our skip-BiLSTM module for contextual and skipping relationship learning. Specifically, we extract a set of local patch features V = {v 1 , v 2 , ..., v n } from the stem network, and feed them to skip-BiLSTM. Distinct from the prior works  [13] , we regard the multiple patches as a sequence structure from top to bottom, which can transfer information by a Bi-directional LSTM based model  [19]  with our skipping-type gate. Different from the traditional BiLSTM or tree-LSTM  [51] ,  [52] , our skip-BiLSTM can directly calculate the correlation between 1. Due to patient confidentiality agreements, we cannot show real patients with facial palsy. This example image is from BP4D. a target AU and all other AUs. For the t-th patch (t > 1), the extracted feature v t is used to learn the weights with forward hidden states H = {h 1 , ..., h t-1 } by the skippingtype gates, which can determine the correlation coefficient between past AUs and current AU. And then the new states Ĥ = { ĥ1 , ..., ĥt-1 } and v t are fed into the t-th forward cell in the skip-BiLSTM to learn the association weights, which can promote the transfer of relevant AUs information. The above process can be formulated as:\n\nwhere Cell(•) indicates the basic ConvLstm cell  [53] , and GAP denotes the global average pooling operation. W j is the parameters of mapping function, in which we used Conv2D. σ denotes sigmoid function. We obtain the tth patch feature for backward delivery, which follows the identical forward method as:\n\nIn order to fully promote the information interactive among individual AUs, the final representation for each patch is computed as the average of the hidden vectors in both directions, as well as the original patch feature:",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Feature Fusion&Refining Module",
      "text": "To exploit the useful global face feature, we design a gated fusion architecture and a refining architecture (F&R) that can selectively balance the relative importance of local patches and global face grids. We add these two architectures on each local AU branch because different AUs may focus on different global information. The grid-based global face feature G is extracted using a simple CNN with the same structure as the face alignment network  [17] . As shown in Fig.  4 , after obtaining the learned t-th local patch feature, it is fused with the grid-based global feature G by the fusion architecture, which can be formulated as:\n\nwhere σ is the sigmoid function, and || • || denotes the l 2normalization. C\n\n′ * and C * denote the Conv2D operation. ⊕ denotes the element-wise weighted sum of ||C g G|| 2 and ||C l s t || 2 according to the learned gate vector α.\n\nThe final local fusion feature s t for t-th patch refined by our F&R module is shown in Fig.  4 . F&R module contains three blocks. Each block consists of two convolutional layers and a maxpooling layer. Then multi-patch features R are sent to the multi-label binary classifier to calculate the occurrence probabilities of individual AUs.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Dataset",
      "text": "We evaluate the effectiveness of the proposed approach for facial AU detection on popular BP4D  [56]  and DISFA  [57]  datasets. BP4D consists of 328 facial videos from 41 participants (23 females and 18 males) who were involved in 8 sessions. Similar to  [16] ,  [17] ,  [25] , we consider 12 AUs and 140K valid frames with labels. DISFA consists of 27 participants (12 females and 15 males). Each participant has a video of 4, 845 frames. We also limited the number of AUs to 8 similar to  [17] ,  [25] . In comparison to BP4D, the experimental protocol and lighting conditions deliver DISFA to be a more challenging dataset. Following the experiment setting of  [17] , we evaluated the model using the 3-fold subject-exclusive cross-validation protocol.\n\nTo evaluate the effectiveness of our ALGRNet for facial palsy severity estimation, we exploited a facial paralysis dataset from NHS, named FPara (the details in Table  1 ), which consists of 89 videos of facial palsy patients performing various types of facial palsy exercises inline with the House-Brackmann (H-B) scale  [45] . Each of the videos consisted of facial palsy patients performing a set of exercises, such as raise eyebrows, close eyes gently, close eyes tightly, scrunch up face and smile, etc. They were part of a previous study on facial palsy with patient consent for research  [58] . These videos are assigned a H-B scale from 1 to 6, and 1 being normal and 6 being severest with no body movements. We then further split into four grades, such as normal (H-B score=1), low (H-B score=2), medium (3≤H-B score≤4) and high (5≤H-B score≤6) grades. FPara data is summarised in Table  1 . Similar to the settings of facial AU dataset, all facial paralysis grades are evaluated using subject exclusive 3-fold cross-validation, where two folds (about 80%) are used for training and the remaining one is used for testing (about 20%).",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Implementation Detail",
      "text": "Our model is trained on a single NVIDIA Tesla V100 GPU with 32 GB memory. The whole network is trained using PyTorch  [59]  with the stochastic gradient descent (SGD) solver, a Nesterov momentum  [60]  of 0.9 and a weight decay of 0.0005. The learning rate is set to 0.01 initially with a decay rate of 0.5 every 2 epochs. Maximum epoch number is set to 20. To enhance the diversity of training data, aligned faces are further randomly cropped into 176 × 176 and horizontally flipped. Regarding face alignment network and stem network, we set the value of the general parameters to be the same with  [17] . The filters for the convolutional layers in refining architecture are used 3 × 3 convolutional filters with a stride 1 and a padding 1. In our paper, all of the mapping Conv2D operations are used 1 × 1 convolutional filters with a stride 1 and a padding 1. The dimensionality of hidden state in ConvLstm cell is set to 64. The filters for the convolutional layers in ConvLstm cell are the same as refining architecture. λ is set to 0.5 for the jointly optimizing of AU detection and face alignment. The ground-truth annotations of 49 landmarks of training data is detected by SDM  [61] . Different from J ÂA-Net  [17] , we averaged the predicted probability of the local information and the integrated information as the final predicted activation probability for each AU, rather than simply using the integrated information of all the AUs.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Performance Metric.",
      "text": "We evaluate the performance of all methods in terms of the F1 score (%) which has been widely used for classification. F1-frame score is the harmonic mean of the Precision P and Recall R and calculated by F1 = 2PR/(P + R). For comparison, we calculate F1 score for all facial paralysis grades on FPara and for all the AUs on DISFA and BP4D and then average them (denoted as Avg.) separately with \"%\" omitted.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Overall Performance Of Au Detection",
      "text": "We compare the proposed ALGRNet for AU detection with several single-image based baselines in Table  2  and Table  3 , including Deep Structure Inference Network (DSIN)  [49] , Joint AU Detection and Face Alignment (JAA-Net)  [18] , Multi-Label Co-Regularization (MLCR)  [38] , Cross Modality Supervision (CMS)  [54] , Local relationship learning with Person-specific shape regularization (LP-Net)  [13] , Attention and Relation Learning (ARL)  [16] , and Joint AU detection and face alignment via Adaptive Attention Network (J ÂANet)  [17] . The performances of the baselines in Table  3  and 2 are their reported results. The first and second places are marked with the bold font and \" \", respectively. For a more comprehensive display, we also show methods (marked with * )  [36] ,  [41] ,  [55]  that use additional data, such as ImageNet  [62]  and VGGFace2  [63] , etc, for pretraining. Due to the fact that our stem network only consists of a few simple convolutional layers, even if we pre-trained on additional datasets, it is unfair compared to pre-training on deeper feature extraction networks, such as ResNet50  [64] . In fact, our results are still excellent compared with them, which demonstrates the superiority and effectiveness of our proposed learning scheme. We omit the need for additional modal inputs and non-frame-based models  [65] ,  [66] .\n\nQuantitative comparison on BP4D: We report the performance comparisons between our ALGRNet and baselines on BP4D in Table  2 . As it can be observed, our ALGRNet significantly outperforms all the other methods in terms of F1-frame score and achieves the fist and second places for most of the 12 AUs annotated in BP4D. J ÂANet is the latest state-of-the-art method which also joint AU detection and face alignment into an end-to-end multi-label multi-branch network. Our ALGRNet achieves 1.1% higher average F1frame score compared with J ÂA-Net. The main reason lies in our ALGRNet overcomes the problem of non-transferable information between branches in the J ÂA-Net and adaptively adjusts the muscle regions corresponding to the AU.\n\nQuantitative comparison on DISFA: We also report the performance of our proposed ALGRNet on DISFA. Table  3  shows the performance of our ALGRNet is the best in terms of average F1 score compared with all baselines. And our approach significantly outperforms all other methods for most of the 8 AUs annotated in DISFA. Compared with the existing end-to-end feature learning and multilabel classification methods DSIN  [49]  and ARL  [16] , the average F1-frame score of our proposed ALGRNet get 13.9% and 8.8% higher, respectively. Moreover, compared with the multi-branch combination-based state-of-the-art method J ÂANet  [17] , our ALGRNet achieves 4.0% improvements in terms of average F1-frame score.\n\nThe eventual experimental results of ALGRNet demonstrate its effectiveness in improving AU detection accuracy on DISFA and BP4D, as well as good robustness.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Ablative Analysis",
      "text": "To fully examine the impact of our proposed adaptive region learning module, skip-BiLSTM module and feature fusion&refining module, we conduct detailed ablative studies to compare different variants of ALGRNet for facial AU detection on DISFA and BP4D.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Effects Of Adaptive Region Learning Module",
      "text": "To cancel out the adaptive region learning (indicated w/o Ada), we fellow the same experiment setting as  [17]  (It means each scaling factor e is set to 0.14.) to predefined muscle region based on the detected landmarks for each AU. In Table  4  and 5, ALGRNet decreases its F1 score to 67.3% and 63.4% on DISFA and BP4D respectively. It has been shown that the adaptive region learning module can contribute the AU detection greatly.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Effects Of Skip-Bilstm",
      "text": "In Table  4  and 5, when the skip-BiLSTM module is removed (indicated by w/o S-B), ALGRNet (without adaptive region learning module) shows an absolute decrease of 1.6% and 0.7% in the average F1-frame score for DISFA and BP4D, respectively. In addition, in order to explicitly validate the effectiveness of our skipping operation, we use the basic BiLSTM  [19]  (indicated by w/ Bi) instead of skip-BiLSTM for information sequential transfer across different branches in the ALGRNet (also with Fusion&Refining module), AL-GRNet obtains lower average F1 scores of 65.2% and 62.9% on DISFA and BP4D, respectively. The performance reduction clearly verifies that roughly defining the relationships between AU-related branches from top to bottom may not be the best way to model the real relationships between AUs. Notably, skipping operation can significantly improve performance, suggesting that our skip-type gates play an important role in our model. Furthermore, the eventual experimental results demonstrate the effectiveness of our ski-BiLSTM in modelling mutual assistance and exclusion relationship between different patch-based branches the for AU detection",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Effects Of Feature Fusion&Refining Module",
      "text": "In order to illustrate the effectiveness of feature fu-sion&refining module, we directly conduct the classification over the output of skip-BiLSTM without fusion&refining module (indicated by w/o F&R in Table  4  and 5 ). When the fusion&refining module is not used, the average F1frame score drops significantly from 67.3% to 66.7% on DISFA and from 63.4% to 62.8% on BP4D, due to the lack of supplementary information from the global face for each patch. This suggests that the refined local AU features from the proposed fusion&refining module, guided by the gridbased global features, make a significant contribution in our model. Finally, after simultaneously removing all the proposed adaptive region learning module, skip-BiLSTM and fu-sion&refining module (marked by w/o full in Table  4  and 5 ), a significant performance degradation in AU detection can be observed, i.e., a 4.4% drop on DISFA and a 1.9% drop on BP4D in terms of average F1-frame score. This sufficiently demonstrates that the potential mutual assistance and exclusion relationships between the adaptive AU patches, complemented by the global facial features, can significantly improve the performance of facial AU detection.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Results For Face Alignment",
      "text": "We integrate face alignment and face AU detection into our end-to-end ALGRNet, which can be beneficial for each other as they are coherently related to each other. For example, the detected landmarks can help the model focus on the exact muscle areas of the AU patches. As shown in Table  6 , compared with baseline method J ÂA-Net  [17] , our ALGR-Net performs comparably to baseline on FPara and better on BP4D and DISFA. The robustness of the adaptive region learning module allows our ALGRNet to outperform J ÂA-Net in AU detection and facial paralysis estimation, even if sometimes with slightly lower landmark detection accuracy.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Visualization Of Results",
      "text": "Fig.  5  shows some examples of the learned class activation maps of ALGRNet (the outputs of F&R module) corresponding to different AUs. For an adequate display, we show two individuals from BP4D and DISFA respectively, containing visualisations of different genders with different AU categories. Through the learning of ALGRNet, not only",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Facial Palsy Severity Estimation",
      "text": "In this section we evaluate the effectiveness of facial palsy severity estimation.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Facial Paralysis Estimation",
      "text": "Different from facial AU detection, the exiting deeplearning-based facial paralysis estimation methods are rare, so we apply currently popular deep learning classification methods, such as the ResNet  [64]  and Transformer  [68] , on our collected facial palsy dataset (FPara). Besides, we also compare it with the state-of-the-art AU detection approach, J ÂA-Net  [17] . Specially, we evaluate the following methods:\n\n• ResNet18 and ResNet50  [64] : These methods use different depth layers based on ResNet to model the input face images, which are similar to  [69] .\n\n• Transformer-based method  [67] : This baseline is motivated from self-attention and uses the Transformer  [68]  architecture. The output of the Transformerbased encoder  [67]  is treated as the latent representation for the input of the multi-label AU classifier.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we present ALGRNet, a novel adaptive localglobal relational network for both facial action units detection and facial paralysis severity estimation, which can exploit the mutual assistance and exclusion relationships of adaptive muscle regions as well as interactions with global information. Specifically, ALGRNet adaptively represents the corresponding muscle areas in terms of different expressions and individual characteristics. It then models the potential mutual assistance and exclusion relationships between AU branches and enables efficient information transfer via a novel skip-BiLSTM to get local features. Finally, a novel feature fusion&refining module is proposed and equipped in each branch, facilitating complementarity between local feature and grid-based global feature as well as adaptation to irregular muscle regions. Experimental results on two widely used AU detection benchmarks show the effectiveness of the AU detection and the superiority of the Facial Palsy severity estimation on a facial paralysis estimation benchmark. This not only witnesses the distinct performance gains over the state-of-the-arts, but also demonstrates effective migration capability from AU detection to the facial paralysis estimation task.",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: (a), some grid-based",
      "page": 1
    },
    {
      "caption": "Figure 1: Illustration of the different schemes: (a) the traditional grid-",
      "page": 2
    },
    {
      "caption": "Figure 1: (b). For example, the scheme",
      "page": 2
    },
    {
      "caption": "Figure 2: It is composed of four main modules,",
      "page": 3
    },
    {
      "caption": "Figure 2: The overall architecture of the proposed ALGRNet for facial AU detection. We utilize a simple landmark localization network to detect the",
      "page": 4
    },
    {
      "caption": "Figure 3: New definitions for the 12 locations of muscle centers of facial",
      "page": 5
    },
    {
      "caption": "Figure 31: , based on the",
      "page": 5
    },
    {
      "caption": "Figure 2: (b) shows the detailed structure of our skip-BiLSTM",
      "page": 5
    },
    {
      "caption": "Figure 4: The architecture of our feature fusion&refining module guided by",
      "page": 6
    },
    {
      "caption": "Figure 4: , after obtaining the learned t-th local patch feature, it",
      "page": 6
    },
    {
      "caption": "Figure 4: F&R module contains",
      "page": 6
    },
    {
      "caption": "Figure 5: Class activation maps that show the discriminative regions for",
      "page": 9
    },
    {
      "caption": "Figure 5: shows some examples of the learned class activation",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table 2: As it can be observed, our ALGRNet region learning module, skip-BiLSTM module and feature",
      "data": [
        {
          "Setting\nMethods\nS-B\nF&R\nAda": "w/o full\n√\nw/o F&R\n√\nw/o S-B\n√\nw/ Bi\n√\n√\nw/o Ada",
          "AU Index\nAvg.\n1\n2\n4\n6\n9\n12\n25\n26": "47.1\n61.1\n66.3\n44.7\n52.2\n74.9\n92.2\n66.2\n63.1\n76.1\n62.6\n64.2\n72.4\n42.3\n49.9\n93.5\n72.6\n66.7\n58.7\n65.2\n73.5\n43.9\n53.5\n72.2\n94.1\n64.7\n65.7\n61.1\n58.4\n70.9\n45.5\n47.9\n74.9\n92.5\n70.8\n65.2\n46.6\n73.0\n62.6\n64.4\n72.5\n48.8\n75.7\n94.4\n67.3"
        },
        {
          "Setting\nMethods\nS-B\nF&R\nAda": "√\n√\n√\nALGRNet",
          "AU Index\nAvg.\n1\n2\n4\n6\n9\n12\n25\n26": "63.8\n65.4\n73.6\n54.1\n94.7\n67.5\n44.5\n74.0\n69.9"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 2: As it can be observed, our ALGRNet region learning module, skip-BiLSTM module and feature",
      "data": [
        {
          "Setting\nMethods\nS-B\nF&R\nAda": "w/o full\n√\nw/o F&R\n√\nw/o S-B\n√\nw/ Bi\n√\n√\nw/o Ada",
          "AU Index\nAvg.\n1\n2\n4\n6\n7\n10\n12\n14\n15\n17\n23\n24": "50.1\n47.1\n54.3\n77.3\n75.1\n82.5\n88.1\n61.7\n44.9\n62.7\n45.2\n49.9\n61.6\n79.0\n63.3\n55.7\n50.4\n46.9\n53.4\n77.4\n84.7\n87.4\n63.0\n45.3\n47.0\n62.8\n51.3\n47.6\n56.3\n78.2\n76.2\n83.7\n88.1\n64.4\n49.1\n61.9\n46.1\n49.8\n62.7\n50.0\n50.7\n55.2\n77.0\n75.7\n84.1\n88.2\n63.4\n49.1\n62.3\n47.3\n52.0\n62.9\n57.8\n77.4\n88.2\n66.4\n50.8\n47.1\n77.6\n84.7\n49.8\n61.5\n46.8\n52.3\n63.4"
        },
        {
          "Setting\nMethods\nS-B\nF&R\nAda": "√\n√\n√\nALGRNet",
          "AU Index\nAvg.\n1\n2\n4\n6\n7\n10\n12\n14\n15\n17\n23\n24": "84.9\n88.2\n50.8\n47.6\n63.5\n51.2\n48.2\n57.3\n77.9\n76.4\n64.8\n62.8\n51.9"
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Impaired recognition of affect in facial expression in depressed patients",
      "authors": [
        "D Rubinow",
        "R Post"
      ],
      "year": "1992",
      "venue": "Biological Psychiatry"
    },
    {
      "citation_id": "2",
      "title": "Heterogeneous face recognition: Matching nir to visible light images",
      "authors": [
        "B Klare",
        "A Jain"
      ],
      "year": "2010",
      "venue": "International Conference on Pattern Recognition"
    },
    {
      "citation_id": "3",
      "title": "Automatic engagement prediction with gap feature",
      "authors": [
        "X Niu",
        "H Han",
        "J Zeng",
        "X Sun",
        "S Shan",
        "Y Huang",
        "S Yang",
        "X Chen"
      ],
      "year": "2018",
      "venue": "ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "4",
      "title": "Detection of deception in adults and children via facial expressions",
      "authors": [
        "R Feldman",
        "L Jenkins",
        "O Popoola"
      ],
      "year": "1979",
      "venue": "Child Development"
    },
    {
      "citation_id": "5",
      "title": "What the face reveals: Basic and applied studies of spontaneous expression using the Facial Action Coding System (FACS)",
      "authors": [
        "P Ekman",
        "E Rosenberg"
      ],
      "year": "1997",
      "venue": "What the face reveals: Basic and applied studies of spontaneous expression using the Facial Action Coding System (FACS)"
    },
    {
      "citation_id": "6",
      "title": "Learning bayesian networks with qualitative constraints",
      "authors": [
        "Y Tong",
        "Q Ji"
      ],
      "year": "2008",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "7",
      "title": "Simultaneous facial feature tracking and facial expression recognition",
      "authors": [
        "Y Li",
        "S Wang",
        "Y Zhao",
        "Q Ji"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "8",
      "title": "Evaluation of facial paralysis degree based on regions",
      "authors": [
        "G Cheng",
        "J Dong",
        "S Wang",
        "H Qu"
      ],
      "year": "2010",
      "venue": "Third International Conference on Knowledge Discovery and Data Mining"
    },
    {
      "citation_id": "9",
      "title": "Farapy: An augmented reality feedback system for facial paralysis using action unit intensity estimation",
      "authors": [
        "G Dell'olio",
        "M Sra"
      ],
      "year": "2021",
      "venue": "The 34th Annual ACM Symposium on User Interface Software and Technology"
    },
    {
      "citation_id": "10",
      "title": "Learning pain from action unit combinations: a weakly supervised approach via multiple instance learning",
      "authors": [
        "Z Chen",
        "R Ansari",
        "D Wilkie"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "11",
      "title": "Facial expression recognition via a boosted deep belief network",
      "authors": [
        "P Liu",
        "S Han",
        "Z Meng",
        "Y Tong"
      ],
      "year": "2014",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "12",
      "title": "Feature disentangling machine-a novel approach of feature selection and disentangling in facial expression analysis",
      "authors": [
        "P Liu",
        "J Zhou",
        "-H Tsang",
        "Z Meng",
        "S Han",
        "Y Tong"
      ],
      "year": "2014",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "13",
      "title": "Local relationship learning with person-specific shape regularization for facial action unit detection",
      "authors": [
        "X Niu",
        "H Han",
        "S Yang",
        "Y Huang",
        "S Shan"
      ],
      "year": "2019",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "14",
      "title": "Long short-term memory",
      "authors": [
        "S Hochreiter",
        "J Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural Computation"
    },
    {
      "citation_id": "15",
      "title": "Joint patch and multi-label learning for facial action unit and holistic expression recognition",
      "authors": [
        "K Zhao",
        "W.-S Chu",
        "F De La Torre",
        "J Cohn",
        "H Zhang"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "16",
      "title": "Facial action unit detection using attention and relation learning",
      "authors": [
        "Z Shao",
        "Z Liu",
        "J Cai",
        "Y Wu",
        "L Ma"
      ],
      "year": "2019",
      "venue": "IEEE transactions on Affective Computing"
    },
    {
      "citation_id": "17",
      "title": "Jaa-net: joint facial action unit detection and face alignment via adaptive attention",
      "authors": [
        "Z Shao",
        "Z Liu",
        "J Cai",
        "L Ma"
      ],
      "year": "2021",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "18",
      "title": "Deep adaptive attention for joint facial action unit detection and face alignment",
      "year": "2018",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "19",
      "title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures",
      "authors": [
        "A Graves",
        "J Schmidhuber"
      ],
      "year": "2005",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "20",
      "title": "Local global relational network for facial action units recognition",
      "authors": [
        "X Ge",
        "P Wan",
        "H Han",
        "J Jose",
        "Z Ji",
        "Z Wu",
        "X Liu"
      ],
      "year": "2021",
      "venue": "IEEE International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "21",
      "title": "Dynamic cascades with bidirectional bootstrapping for action unit detection in spontaneous facial behavior",
      "authors": [
        "Y Zhu",
        "F De La Torre",
        "J Cohn",
        "Y.-J Zhang"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "22",
      "title": "Capturing global semantic relationships for facial action unit recognition",
      "authors": [
        "Z Wang",
        "Y Li",
        "S Wang",
        "Q Ji"
      ],
      "year": "2013",
      "venue": "IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "23",
      "title": "Data-free prior model for facial action unit recognition",
      "authors": [
        "Y Li",
        "J Chen",
        "Y Zhao",
        "Q Ji"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "24",
      "title": "Joint patch and multi-label learning for facial action unit detection",
      "authors": [
        "K Zhao",
        "W.-S Chu",
        "F De La Torre",
        "J Cohn",
        "H Zhang"
      ],
      "year": "2015",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "25",
      "title": "Action unit detection with region adaptation, multi-labeling learning and optimal temporal fusing",
      "authors": [
        "W Li",
        "F Abtahi",
        "Z Zhu"
      ],
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "26",
      "title": "Viewindependent facial action unit detection",
      "authors": [
        "C Tang",
        "W Zheng",
        "J Yan",
        "Q Li",
        "Y Li",
        "T Zhang",
        "Z Cui"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "27",
      "title": "Au r-cnn: Encoding expert prior knowledge into r-cnn for action unit detection",
      "authors": [
        "C Ma",
        "L Chen",
        "J Yong"
      ],
      "year": "2019",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "28",
      "title": "A transfer learning approach to heatmap regression for action unit intensity estimation",
      "authors": [
        "I Ntinou",
        "E Sanchez",
        "A Bulat",
        "M Valstar",
        "Y Tzimiropoulos"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "29",
      "title": "Structure-preserving sparse decomposition for facial expression analysis",
      "authors": [
        "S Taheri",
        "Q Qiu",
        "R Chellappa"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "30",
      "title": "Deep learning the dynamic appearance and shape of facial action units",
      "authors": [
        "S Jaiswal",
        "M Valstar"
      ],
      "year": "2016",
      "venue": "IEEE Winter Conference on Applications of Computer Vision"
    },
    {
      "citation_id": "31",
      "title": "Facial action unit recognition by exploiting their dynamic and semantic relationships",
      "authors": [
        "Y Tong",
        "W Liao",
        "Q Ji"
      ],
      "year": "2007",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "32",
      "title": "Deep facial action unit recognition from partially labeled data",
      "authors": [
        "S Wu",
        "S Wang",
        "B Pan",
        "Q Ji"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "33",
      "title": "Capturing feature and label relations simultaneously for multiple facial action unit recogni-tion",
      "authors": [
        "S Wang",
        "S Wu",
        "G Peng",
        "Q Ji"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "34",
      "title": "Eeg emotion recognition using dynamical graph convolutional neural networks",
      "authors": [
        "T Song",
        "W Zheng",
        "P Song",
        "Z Cui"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "35",
      "title": "Exploring domain knowledge for facial expression-assisted action unit activation recognition",
      "authors": [
        "S Wang",
        "G Peng",
        "Q Ji"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "36",
      "title": "Hybrid message passing with performance-driven structures for facial action unit detection",
      "authors": [
        "T Song",
        "Z Cui",
        "W Zheng",
        "Q Ji"
      ],
      "year": "2021",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "37",
      "title": "Deep region and multi-label learning for facial action unit detection",
      "authors": [
        "K Zhao",
        "W.-S Chu",
        "H Zhang"
      ],
      "year": "2016",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "38",
      "title": "Multi-label coregularization for semi-supervised facial action unit recognition",
      "authors": [
        "X Niu",
        "H Han",
        "S Shan",
        "X Chen"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "39",
      "title": "Semantic relationships guided representation learning for facial action unit recognition",
      "authors": [
        "G Li",
        "X Zhu",
        "Y Zeng",
        "Q Wang",
        "L Lin"
      ],
      "year": "2019",
      "venue": "AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "40",
      "title": "Dynamic probabilistic graph convolution for facial action unit intensity estimation",
      "authors": [
        "T Song",
        "Z Cui",
        "Y Wang",
        "W Zheng",
        "Q Ji"
      ],
      "year": "2021",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "41",
      "title": "Uncertain graph neural networks for facial action unit detection",
      "authors": [
        "T Song",
        "L Chen",
        "W Zheng",
        "Q Ji"
      ],
      "year": "2021",
      "venue": "AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "42",
      "title": "Facial nerve function index: a clinical measurement of facial nerve activity in patients with facial nerve palsies",
      "authors": [
        "M Fields",
        "N Peckitt"
      ],
      "year": "1990",
      "venue": "Facial nerve function index: a clinical measurement of facial nerve activity in patients with facial nerve palsies"
    },
    {
      "citation_id": "43",
      "title": "Threshold of visual perception of facial asymmetry in a facial paralysis model",
      "authors": [
        "E Chu",
        "T Farrag",
        "L Ishii",
        "P Byrne"
      ],
      "year": "2011",
      "venue": "Archives of Facial Plastic Surgery"
    },
    {
      "citation_id": "44",
      "title": "The nottingham system: objective assessment of facial nerve function in the clinic",
      "authors": [
        "G Murty",
        "J Diver",
        "P Kelly",
        "G O'donoghue",
        "P Bradley"
      ],
      "year": "1994",
      "venue": "Otolaryngology-Head and Neck Surgery"
    },
    {
      "citation_id": "45",
      "title": "Facial nerve grading system",
      "authors": [
        "W House"
      ],
      "year": "1985",
      "venue": "Otolaryngol Head Neck Surg"
    },
    {
      "citation_id": "46",
      "title": "An approach for quantitative evaluation of the degree of facial paralysis based on salient point detection",
      "authors": [
        "J Dong",
        "L Ma",
        "Q Li",
        "S Wang",
        "L Liu",
        "Y Lin",
        "M Jian"
      ],
      "year": "2008",
      "venue": "International Symposium on Intelligent Information Technology Application Workshops"
    },
    {
      "citation_id": "47",
      "title": "Efficient quantitative assessment of facial paralysis using iris segmentation and active contour-based key points detection with hybrid classifier",
      "authors": [
        "J Barbosa",
        "K Lee",
        "S Lee",
        "B Lodhi",
        "J.-G Cho",
        "W.-K Seo",
        "J Kang"
      ],
      "year": "2016",
      "venue": "BMC medical imaging"
    },
    {
      "citation_id": "48",
      "title": "Automatic evaluation of the degree of facial nerve paralysis",
      "authors": [
        "T Wang",
        "S Zhang",
        "J Dong",
        "L Liu",
        "H Yu"
      ],
      "year": "2016",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "49",
      "title": "Deep structure inference network for facial action unit recognition",
      "authors": [
        "C Corneanu",
        "M Madadi",
        "S Escalera"
      ],
      "year": "2018",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "50",
      "title": "V-net: Fully convolutional neural networks for volumetric medical image segmentation",
      "authors": [
        "F Milletari",
        "N Navab",
        "S.-A Ahmadi"
      ],
      "year": "2016",
      "venue": "The Fourth International Conference on 3D Vision"
    },
    {
      "citation_id": "51",
      "title": "Improved semantic representations from tree-structured long short-term memory networks",
      "authors": [
        "K Tai",
        "R Socher",
        "C Manning"
      ],
      "year": "2015",
      "venue": "The Association for Computer Linguistics"
    },
    {
      "citation_id": "52",
      "title": "Structured multi-modal feature embedding and alignment for imagesentence retrieval",
      "authors": [
        "X Ge",
        "F Chen",
        "J Jose",
        "Z Ji",
        "Z Wu",
        "X Liu"
      ],
      "year": "2021",
      "venue": "ACM International Conference on Multimedia"
    },
    {
      "citation_id": "53",
      "title": "Convolutional lstm network: A machine learning approach for precipitation nowcasting",
      "authors": [
        "X Shi",
        "Z Chen",
        "H Wang",
        "D.-Y Yeung",
        "W.-K Wong",
        "W.-C Woo"
      ],
      "year": "2015",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "54",
      "title": "Representation learning through cross-modality super-vision",
      "authors": [
        "N Sankaran",
        "D Mohan",
        "S Setlur",
        "V Govindaraju",
        "D Fedorishin"
      ],
      "year": "2019",
      "venue": "IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "55",
      "title": "Dual learning for joint facial landmark detection and action unit recognition",
      "authors": [
        "S Wang",
        "Y Chang",
        "C Wang"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "56",
      "title": "Bp4d-spontaneous: a high-resolution spontaneous 3d dynamic facial expression database",
      "authors": [
        "X Zhang",
        "L Yin",
        "J Cohn",
        "S Canavan",
        "M Reale",
        "A Horowitz",
        "P Liu",
        "J Girard"
      ],
      "year": "2014",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "57",
      "title": "Disfa: A spontaneous facial action intensity database",
      "authors": [
        "S Mavadati",
        "M Mahoor",
        "K Bartlett",
        "P Trinh",
        "J Cohn"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "58",
      "title": "Objective method of assessing and presenting the house-brackmann and regional grades of facial palsy by production of a facogram",
      "authors": [
        "B O'reilly",
        "J Soraghan",
        "S Mcgrenary",
        "S He"
      ],
      "year": "2010",
      "venue": "Otology & Neurotology"
    },
    {
      "citation_id": "59",
      "title": "Pytorch: An imperative style, high-performance deep learning library",
      "authors": [
        "A Paszke",
        "S Gross",
        "F Massa",
        "A Lerer",
        "J Bradbury",
        "G Chanan",
        "T Killeen",
        "Z Lin",
        "N Gimelshein",
        "L Antiga"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "60",
      "title": "On the importance of initialization and momentum in deep learning",
      "authors": [
        "I Sutskever",
        "J Martens",
        "G Dahl",
        "G Hinton"
      ],
      "year": "2013",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "61",
      "title": "Supervised descent method and its applications to face alignment",
      "authors": [
        "X Xiong",
        "F De"
      ],
      "year": "2013",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "62",
      "title": "Imagenet: A large-scale hierarchical image database",
      "authors": [
        "J Deng",
        "W Dong",
        "R Socher",
        "L.-J Li",
        "K Li",
        "L Fei-Fei"
      ],
      "year": "2009",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "63",
      "title": "Vggface2: A dataset for recognising faces across pose and age",
      "authors": [
        "Q Cao",
        "L Shen",
        "W Xie",
        "O Parkhi",
        "A Zisserman"
      ],
      "year": "2018",
      "venue": "IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "64",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "65",
      "title": "Adaptive multimodal fusion for facial action units recognition",
      "authors": [
        "H Yang",
        "T Wang",
        "L Yin"
      ],
      "year": "2020",
      "venue": "ACM International Conference on Multimedia"
    },
    {
      "citation_id": "66",
      "title": "Multi-modality empowered network for facial action unit detection",
      "authors": [
        "P Liu",
        "Z Zhang",
        "H Yang",
        "L Yin"
      ],
      "year": "2019",
      "venue": "IEEE Winter Conference on Applications of Computer Vision"
    },
    {
      "citation_id": "67",
      "title": "Medical transformer: Gated axial-attention for medical image segmentation",
      "authors": [
        "J Valanarasu",
        "P Oza",
        "I Hacihaliloglu",
        "V Patel"
      ],
      "year": "2021",
      "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention"
    },
    {
      "citation_id": "68",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "69",
      "title": "Neurologist standard classification of facial nerve paralysis with deep neural networks",
      "authors": [
        "A Song",
        "Z Wu",
        "X Ding",
        "Q Hu",
        "X Di"
      ],
      "year": "2018",
      "venue": "Future Internet"
    }
  ]
}