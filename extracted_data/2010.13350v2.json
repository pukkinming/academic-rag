{
  "paper_id": "2010.13350v2",
  "title": "Emotion Controllable Speech Synthesis Using Emotion-Unlabeled Dataset With The Assistance Of Cross-Domain Speech Emotion Recognition",
  "published": "2020-10-26T05:31:55Z",
  "authors": [
    "Xiong Cai",
    "Dongyang Dai",
    "Zhiyong Wu",
    "Xiang Li",
    "Jingbei Li",
    "Helen Meng"
  ],
  "keywords": [
    "Emotion",
    "expressive",
    "global style token",
    "speech emotion recognition",
    "speech synthesis"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Neural text-to-speech (TTS) approaches generally require a huge number of high quality speech data, which makes it difficult to obtain such a dataset with extra emotion labels. In this paper, we propose a novel approach for emotional TTS synthesis on a TTS dataset without emotion labels. Specifically, our proposed method consists of a cross-domain speech emotion recognition (SER) model and an emotional TTS model. Firstly, we train the cross-domain SER model on both SER and TTS datasets. Then, we use emotion labels on the TTS dataset predicted by the trained SER model to build an auxiliary SER task and jointly train it with the TTS model. Experimental results show that our proposed method can generate speech with the specified emotional expressiveness and nearly no hindering on the speech quality.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "In the past few years, neural speech synthesis techniques have experienced great development. End-to-end TTS systems, such as  [1] [2] [3]  have achieved remarkable results in terms of naturalness and intelligibility of speech. Benefiting from these techniques, the field of controllable TTS has attracted extensive attention from researchers because it is closer to the practical applications.\n\nThe emotion contained in speech is an important paralinguistic information that can effectively expresse the intention of the speaker. Therefore, it is necessary to add emotional control to the TTS system for building a more intelligent interactive interface.\n\nMany pioneering methods have been proposed for emotional TTS.  [4]  proposes a LSTM-based acoustic model for emotional TTS, where several kinds of emotional category labels such as one-hot vector or perception vector are used as an extra input to the acoustic model.  [5]  uses a improved tacotron  [1]  model for end-to-end emotional TTS, in which the emotion labels are concatenated to the output of both the decoder pre-net and the first decoder RNN layer. Some other studies use the global style tokens (GST)  [6]  framework to model the emotional features.  [7]  proposes an effective style token weights control scheme that uses the centroid of weight vectors of each emotion cluster to generate speech of the emotion.  [8]  is also a GST-based method for emotional TTS, where the authors propose an inter-to-intra distance ratio algorithm that well considers the distribution of emotions to determine the emotion weights. † Equal contribution * Corresponding author\n\nThe methods mentioned above report some promising results in the aspect of emotion expressiveness, but these methods rely on an emotion-annotated dataset which is most likely not available. In fact, the lack of emotion-annotated dataset is one of the main obstacles that limit the research of emotional speech synthesis. This problem mainly stems from two reasons: on the one hand, TTS requires a large amount of data, which makes it costly to label emotions; on the other hand, TTS requires high quality of speech data, so dataset from the speech emotion recognition field can not be directly used for emotional speech synthesis.\n\nTherefore, some semi-supervised approaches have been proposed to alleviate the burden of data requirements.  [9]  proposes to fine-tune a pre-trained TTS model on a small emotional dataset for low resource emotional TTS.  [10]  uses a variant of the GST model and shows that training with 5% labeled data can achieve satisfactory results.  [11]  proposes to merge an external SER dataset and a labeled subset of TTS dataset to train a SER model and label the whole TTS dataset by the trained SER model. These semisupervised methods can greatly reduce the amount of labeled data required for model training. However, these methods are still not universal enough because a subset of the new dataset still needs to be manually annotated when it is is used for emotional TTS.\n\nSpeech emotion recognition (SER) is another important topic in the field of speech processing. A variety of datasets  [12] [13] [14]  are publicly released and a large number of approaches  [15] [16] [17] [18]  are proposed in this topic. Therefore, a natural idea is whether we can utilize the achievements in SER to solve the problem of lack of emotion-annotated dataset for emotional TTS. In this paper, we propose a novel GST-based model that is trained on a fully emotionunlabeled dataset and can generate speech with expected emotions. We perform mean opinion score (MOS) evaluations and emotion recognition perception evaluations in 4 emotion categories (neutral, happy, sad and angry) and 2 polarities of emotion dimensions (high or low for arousal, positive or negative for valence). The evaluation results show that our proposed method can generate speech with the specified emotion expressiveness and nearly no hindering on the speech quality.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "The idea most similar to ours is  [19] . The authors propose a general method that enables control of arbitrary variations of speech on a dataset without labels of this variation and validate their idea with an example of emotion control. The authors train a SER model using an external SER dataset and label the TTS dataset by this trained SER model. Then the predicted labels is fed as an extra input to the statistical parametric speech synthesis (SPSS) model. Different from  [19] , we inject emotion information to the TTS model using an emotion embedding, which can retain more prosody related features rather than a simple emotion label. Meanwhile, as for SER model, we use a domain adaptation technique to reduce distribution shift between SER and TTS datasets, which is also not considered in  [19] .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methodology",
      "text": "As shown in Figure  1 , our proposed method includes a cross-domain SER model and a GST-based TTS model. The training and inference procedures of our method are as follows. Firstly, the cross-domain SER model is pre-trained on the emotion labeled SER dataset (as the source domain) and the emotion-unlabeled TTS dataset (as the target domain). Then, the soft emotion labels of the TTS dataset are obtained from the softmax output of the trained SER model (the green dashed arrow). Finally, the TTS model and an emotion predictor are jointly trained on the TTS dataset with the emotion labels. For the inference, we firstly select a reference audio set for each emotion category from the TTS dataset. Then, we average the style tokens' weights of all audios in the reference audio set to synthesize speech for this kind of emotion.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Cross-Domain Ser Model",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Model Structure",
      "text": "As shown in the upper part of Figure  1 , the SER model consists of a feature extraction encoder and an emotion classifier. The encoder is a CNN-RNN network, which is a popular structure in SER. Firstly, a 4-layer of 2D convolution network takes the 80-dimension log mel spectrum as input feature and outputs a 2D feature map which is flattened into a sequence of feature vectors. Then, a bidirectional GRU layer is followed and outputs the hidden state vector of the last time step as the final feature vector. The emotion classifier is a 2layer dense network with a softmax activation for the output. The number of output units is 4 for the emotion category classification task and 2 for the arousal and valence polarity classification task.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Cross-Domain Training",
      "text": "Since the TTS and SER datasets are quite different in speakers, recording devices and recording environment, some domain adaptation technique is necessary for our SER task to reduce the distribution shift of these two datasets. The Maximum Mean Discrepancy (MMD)  [20]  is a kernel-based test statistic to judge whether two distributions are equal, which is widely used in domain adaptation  [21, 22]  for measuring the similarity of two distributions. It has also been proved to be effective for cross-domain SER in  [16, 17] . Considering that the training of MMD is stable and our TTS dataset has no available emotion labels for parameter tuning,we choose the MMD method for our cross-domain SER task.\n\nWe refer to the SER dataset as the source domain Ds and the TTS dataset as the target domain Dt. As described in  [23] , we minimize the following MMD loss to reduce the distribution discrepancy between the two domains.\n\nWhere si and ti are the output features of the encoder from Ds and Dt respectively; m and n are the number of samples of Ds and Dt respectively; k(., .) is the kernel function that is a linear combination of multiple RBF kernels:k(xi, xj) = Σnηn exp{-1 2σn xi -\n\nWhere ω k is the weight of the k-th emotion category which is inversely proportional to the number of samples in this emotion category. Therefore, the final loss function for training SER model is:\n\nWhere λ is the weight of MMD loss.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Gst-Based Emotional Tts Model",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Model Structure",
      "text": "As shown in the lower part of Figure  1 , our emotional TTS model consists of a TTS module, reference encoder and GST module. The TTS module is a standard Tacotron2 model except that we add a post-net as in  [1]  to convert the mel spectrum to linear spectrum. The reference encoder and GST module are also the same as in  [6]  except that we add an auxiliary emotion prediction task to explicitly guide the style tokens to learn emotion-related features. For the generation of waveform, we simply use the Griffin-Lim algorithm  [23]  to convert the predicted linear spectrum to waveform samples, because our goal is mainly to verify the effectiveness of the proposed method rather than to generate high fidelity speech.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emotion Prediction Task",
      "text": "The original GST  [6]  is designed to unsupervisedly learn the styles from a reference audio, where the style learned by each token is random and uncertain, and therefore uncontrollable. In our emotional TTS task, in order to force the GST module pay more attention to learn the emotion-related styles, we explicitly add an emotion prediction task based on the style token weights, which will also been verified in our experiments to be critical for the emotional expressiveness of speech.\n\nIn this study, we conduct two emotion control methods corresponding to the two most commonly used emotion descriptions: basic emotion categories and emotion dimensions. For the emotion category, the emotion prediction task is a classifier of a single dense layer that takes the style token weight vector as input and outputs a vector of length 4 for the 4 emotion categories (neutral, happy, sad and angry). For the emotion dimensions, since the used SER dataset is originally annotated as discrete scores for emotion dimensions, we build two binary classifiers for predicting arousal and valence polarity respectively as the emotion prediction task. Meanwhile, in order to independently control these two orthogonal emotion dimensions, we split the style token weight vector into two valves, which are then fed into the arousal and valence classifiers respectively.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Choice For Reference Audio Set Of Each Emotion Class",
      "text": "There are two common methods for the synthesis of GST model: selecting a reference audio with the desired style or manually specifying the weights of the style tokens. In our experiments, the quality and stability of speech synthesized from a single reference audio heavily depend on the choice of reference audio and the content of the text to be synthesized, which may be due to the fact that the GST module still does not completely disentangle the audio style and text content. Therefore, we manually specify the weight of style tokens by averaging the style token weights of an audio set that belongs to a certain kind of emotion. The audio set can be directly specified as all the utterances with the same emotion label, as used in  [7] , when the TTS dataset has emotion labels. However, our TTS dataset has no ground-truth emotion labels, and only the soft labels predicted from the cross-domain SER model are available. Because the crossdomain SER model is far less reliable than humans, these predicted emotion labels may contain a great number of mispredictions and we can not use all the utterances with the same soft label as the reference audio set. In order to choose a more reliable reference audio set, we propose that only the K utterances with highest posterior are selected as the reference audio set, rather than the full set of a certain emotion. Choosing these K high-confidence audios can greatly reduce the impact of the prediction error of the SER model and it is a very important choice for generating an emotional speech in our experiments. We set K=50 for all of our experiments.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Datasets",
      "text": "We use the IEMOCAP as the SER dataset and the English dataset of Blizzard Challenge 2013 (BC2013-English) as the TTS dataset for our emotional TTS experiments. In addition, because there are no emotion labels on BC2013-English, we use the an additional SER dataset: RECOLA to verify the effectiveness of MMD on crossdomain SER tasks. IEMOCAP  [12]  is an audiovisual database of English dyadic conversations performed by ten professional actors, which contains about 12.5 hours and a total of 10,039 utterances. For emotion category schedule, we use the subset of the data that contains only the neutral, angry, sad, happy, excited utterances and merge the excited to the happy category. For arousal and valence dimension schedule, we use all utterances and map the original 5-point label into binary one-hot label with 2.5 as the dividing point. RECOLA  [13]  is a multimodal database of French dyadic conversations, where continuous arousal and valence label in the range [-1, 1] are annotated at frame level. In our study, we use all the 1,308 freely available utterances from 23 speakers and map the utterancelevel average of the original labels to the binary one-hot label with 0.0 as the dividing point. BC2013-English  [24]  is a audiobook dataset provided by The Voice Factory, where approximately 300 hours of chapter-sized mp3 files and 19 hours of wav files are available. We use a segmented subset of approximately 198 hours data and filter out utterances that are longer than 14 seconds or have more more than 100 characters from this subset. The final dataset used in our experiments is about 73 hours of total 95k utterances. In addition, the stories are read expressively by a single female speaker and this expressive styles is not annotated, which makes it appropriate for our task.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Training Setup",
      "text": "Our training process involves first training a cross-domain SER model, followed by training an end-to-end GST-based TTS model.\n\nTo train the SER model, we use the cross-entropy loss and an additional MMD loss with a batch size of 96. The Adam  [25]  optimizer and learning rate schedule: initialized to 3e-5 and fixed to 3e-4 after 100 steps are used to optimize model parameters. The 80band mel-scale spectrum is extracted frame-wise as input feature to be consistent with the input of TTS model. We randomly select N utterances from the source domain dataset as validation set for early stopping (N=500 for IEMOCAP and 200 for RECOLA).\n\nTo train the emotional TTS model, we use the mean absolute error for the reconstruction of both mel and linear spectrum and a cross-entropy loss for the auxiliary emotion prediction task with a batch size of 32. We train all the TTS models with 150k steps using the Adam optimizer with a warm-up learning rate schedule: initial rate 2e-3, warm-up steps 4,000 and decay power -0.5. We also set the softmax temperature of attention mechanism to 2.0 in the synthesis phase, which can effectively improve the stability of alignments especially for synthesizing highly expressive emotion speech. We publicly release the audio demos 1  and all the source codes 2  online, where more training and model details can be found. Table  1 . WA and UA results for baseline and mmd-based models.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Cross-Domain Ser Results",
      "text": "In order to evaluate the effectiveness of MMD for cross-domain SER tasks, we conduct experiments for the arousal and valence emotion dimensions on the IEMOCAP and RECOLA datasets. It is worth noting that the RECOLA dataset is not annotated with emotion category labels, and thus we do not preform experiments for the emotion category task. We train two models: ser-base and ser-mmd, and the structure of the two models is the same as described in section 3.1.1, except that the ser-mmd model has an additional MMD loss with weight λ=0.5. We use Rec and Iem to represent the RECOLA and IEMOCAP dataset, and Rec2Iem means training on Rec and testing on Iem and vice versa. Both the Weighted Accuracy (WA) and Unweighted Accuracy (UA) are selected as our evaluation criteria since an imbalance of emotion categories exists on these two datasets. Table 1 reports the WA and UA results for the models.\n\nAs shown in Table  1 , except for the experiment of Iem2Rec of arousal, the ser-mmd model shows an improvement of at least 1.2% compared to the ser-base model on both WA and UA. And the total average improvement of WA and UA over all four experiments is 3.2% and 2.4% respectively. This shows that the performance of cross-domain SER can be improved after the discrepancy of feature distribution between two domains is reduced.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emotional Tts Results",
      "text": "The overall quality and emotional expressiveness of synthesized speech are the two most significant evaluation criteria for an emo-tional TTS system. We perform two subjective experiments for these two criteria using three systems: our proposed system for 4 emotion categories (our-4cls), our proposed system for 2 emotion dimensions (our-2d) and a baseline system (base-4cls) which is the same as our-4cls except that the auxiliary emotion prediction task is not used in training phase. We randomly choose 10 sentences outside the TTS dataset as inference texts, and 20 university students are invited to participate in our subjective experiments.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Evaluation For The Overall Quality Of Speech",
      "text": "In this section, we perform mean opinion score (MOS) evaluation experiment in terms of the overall quality of speech (naturalness, intelligibility and speech quality). Table  2  and Table  3  report the MOS results of the above three systems. The average score of base-4cls is higher than that of the our-4cls and our-2d models, but the p-values are much greater than the singificance leval of α = 0.05 indicating that there is no significant difference between these models. This result suggests that our proposed our-4cls and our-2d models can almost achieve as good speech quality as the baseline system. In addition, it can also be found that the MOS score of happy is greatly lower than the others for both base-4cls and our-4cls. One possible reason is that the SER model itself has lower accuracy for happy in our experiments, which will lead to great differences in the prosody consistency of the top K reference audios selected by the SER model. This also indicates that the performance of the cross-domain SER model is critical for our proposed approach. In this section, we carry out emotion perception experiments in terms of emotional expressiveness. Specifically, for a given text, we generate a set of audios for all emotion categories and then randomly shuffle these audios. Then, the subjects are asked to choose a most likely emotion category for each audio, according to two reference audios given for each emotion. In order to verify the role of our proposed top-K scheme, we also add a scheme full-4cls that use the same checkpoint with our-4cls but chooses the full set of audios predicted by the SER model as the same emotion category as the reference audio set. Figure  2  and Figure  3  show the confusion matrices of the subjective emotion prediction results. We can find that the average accuracies of both our-4cls (78.75%) and full-4cls (49.25%) are much higher than the base-4cls's 36.75%. This result shows that explicitly adding an emotion prediction task based on style token weights can help the GST module to more effectively the extract emotion-related feature. Moreover, a large gap in terms of average accuracy can also be found between our-4cls and full-4cls, which proves that the proposed top-K scheme is fairly effective for the emotional expressiveness. Similarly, as shown in Figure  3 , the average accuracy of arousal and valence is 91.0% and 55.5% respectively, which are greater than the random level of 50.0%. This also suggests that our proposed systems can effectively model the emotional expressiveness in speech.\n\nFinally, we also use the t-SNE  [26]  algorithm to project the style token weights on the reference audio sets into the 2D space for our-4cls and base-4cls. As shown in Figure  4 , the style token weights of the 4 emotion categories are clearly clustered into 4 clusters for the our-4cls model, while for the base-4cls model, except for the angry catetory, there is no obvious clustering boundary for the other three categories. This can also be considered as another possible evidience to explain the role of the auxiliary emotion prediction task.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper, we propose a novel GST-based approach for emotional speech synthesis. Our proposed approach has the following three characteristics: 1) only requires an expressive but emotion-unlabeled TTS dataset; 2) can generate speech with a desired emotional expressiveness; 3) nearly do not hurt the quality of synthesized speech, except for some highly expressive utterances. And three key technologies ensure that our proposed approach works well: 1) an MMDbased cross-domain SER model provides effective emotion labels for the TTS dataset; 2) an auxiliary supervised emotion prediction task based on the weight of style tokens guides the GST module to model the emotion-related feature more thoroughly; 3) the top-K scheme is used to choose the reference audio set for each emotion category. Our proposed approach greatly reduces the threshold of emotional speech synthesis in terms of emotion-annotated data, and its main idea can also be easily applied to other neural TTS systems. For future works, we will explore better decoupling in the control of arousal and valence to achieve more flexible emotional control.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , our proposed method includes a cross-domain",
      "page": 2
    },
    {
      "caption": "Figure 1: , the SER model consists of a",
      "page": 2
    },
    {
      "caption": "Figure 1: The overall structure of the cross-domain SER and GST-",
      "page": 2
    },
    {
      "caption": "Figure 1: , our emotional TTS model",
      "page": 2
    },
    {
      "caption": "Figure 2: and Figure 3 show the confusion matrices",
      "page": 4
    },
    {
      "caption": "Figure 3: , the average accuracy of arousal and",
      "page": 4
    },
    {
      "caption": "Figure 4: , the style token weights of",
      "page": 4
    },
    {
      "caption": "Figure 2: Confusion matrices of 4 emotion categories for the three",
      "page": 4
    },
    {
      "caption": "Figure 3: Confusion matrices of polarities of arousal and valence emo-",
      "page": 4
    },
    {
      "caption": "Figure 4: The t-SNE 2D visualization for style token weights on the",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": "The methods mentioned above report some promising results in"
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": "the aspect of emotion expressiveness, but\nthese methods rely on an"
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": ""
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": "emotion-annotated dataset which is most likely not available. In fact,"
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": ""
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": "the lack of emotion-annotated dataset\nis one of\nthe main obstacles"
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": ""
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": "that\nlimit\nthe research of emotional speech synthesis. This problem"
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": ""
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": "mainly stems from two reasons:\non the one hand, TTS requires a"
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": ""
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": "large amount of data, which makes it costly to label emotions; on"
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": ""
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": "the other hand, TTS requires high quality of speech data, so dataset"
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": ""
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": "from the speech emotion recognition ﬁeld can not be directly used"
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": ""
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": "for emotional speech synthesis."
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": ""
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": "Therefore,\nsome\nsemi-supervised approaches have been pro-"
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": ""
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": "posed to alleviate\nthe burden of data\nrequirements.\n[9] proposes"
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": ""
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": "to ﬁne-tune a pre-trained TTS model on a small emotional dataset"
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": ""
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": "for\nlow resource emotional TTS.\n[10] uses a variant of\nthe GST"
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": ""
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": "model and shows\nthat\ntraining with 5% labeled data can achieve"
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": "satisfactory results. [11] proposes to merge an external SER dataset"
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": "and a labeled subset of TTS dataset\nto train a SER model and label"
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": "the whole TTS dataset by the trained SER model.\nThese semi-"
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": "supervised methods can greatly reduce the amount of\nlabeled data"
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": "required for model\ntraining. However,\nthese methods are still not"
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": ""
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": "universal enough because a subset of the new dataset still needs to"
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": ""
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": "be manually annotated when it is is used for emotional TTS."
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": ""
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": "Speech emotion recognition (SER) is another important topic in"
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": ""
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": "the ﬁeld of\nspeech processing.\nA variety of datasets\n[12–14] are"
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": ""
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": "publicly released and a large number of approaches [15–18] are pro-"
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": ""
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": "posed in this\ntopic.\nTherefore, a natural\nidea is whether we can"
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": ""
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": "utilize the achievements\nin SER to solve the problem of\nlack of"
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": ""
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": "emotion-annotated dataset for emotional TTS. In this paper, we pro-"
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": ""
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": "pose a novel GST-based model\nthat\nis trained on a fully emotion-"
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": ""
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": "unlabeled dataset and can generate speech with expected emotions."
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": ""
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": "We perform mean opinion score (MOS) evaluations and emotion"
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": ""
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": "recognition perception evaluations in 4 emotion categories (neutral,"
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": ""
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": "happy, sad and angry) and 2 polarities of emotion dimensions (high"
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": ""
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": "or\nlow for arousal, positive or negative for valence).\nThe evalua-"
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": ""
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": "tion results show that our proposed method can generate speech with"
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": ""
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": "the speciﬁed emotion expressiveness and nearly no hindering on the"
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": ""
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": "speech quality."
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": ""
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": "2. RELATED WORK"
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": ""
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": "The idea most similar to ours is [19]. The authors propose a general"
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": ""
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": "method that enables control of arbitrary variations of speech on a"
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": ""
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": "dataset without labels of this variation and validate their idea with an"
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": ""
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": "example of emotion control. The authors train a SER model using"
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": ""
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": "an external SER dataset and label\nthe TTS dataset by this trained"
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": "SER model.\nThen the predicted labels is fed as an extra input\nto"
        },
        {
          "{cai-x18, ddy17, xiang-li20, lijb19}@mails.tsinghua.edu.cn, {zywu, hmmeng}@se.cuhk.edu.hk": "the statistical parametric speech synthesis (SPSS) model. Different"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "from [19], we inject emotion information to the TTS model using an": "emotion embedding, which can retain more prosody related features"
        },
        {
          "from [19], we inject emotion information to the TTS model using an": "rather than a simple emotion label. Meanwhile, as for SER model,"
        },
        {
          "from [19], we inject emotion information to the TTS model using an": "we use a domain adaptation technique to reduce distribution shift"
        },
        {
          "from [19], we inject emotion information to the TTS model using an": "between SER and TTS datasets, which is also not considered in [19]."
        },
        {
          "from [19], we inject emotion information to the TTS model using an": "3. METHODOLOGY"
        },
        {
          "from [19], we inject emotion information to the TTS model using an": "As shown in Figure 1, our proposed method includes a cross-domain"
        },
        {
          "from [19], we inject emotion information to the TTS model using an": "SER model and a GST–based TTS model. The training and inference"
        },
        {
          "from [19], we inject emotion information to the TTS model using an": "procedures of our method are as follows. Firstly,\nthe cross-domain"
        },
        {
          "from [19], we inject emotion information to the TTS model using an": "SER model is pre-trained on the emotion labeled SER dataset (as the"
        },
        {
          "from [19], we inject emotion information to the TTS model using an": "source domain) and the emotion-unlabeled TTS dataset (as the target"
        },
        {
          "from [19], we inject emotion information to the TTS model using an": "domain). Then,\nthe soft emotion labels of the TTS dataset are ob-"
        },
        {
          "from [19], we inject emotion information to the TTS model using an": "tained from the softmax output of the trained SER model (the green"
        },
        {
          "from [19], we inject emotion information to the TTS model using an": "dashed arrow). Finally, the TTS model and an emotion predictor are"
        },
        {
          "from [19], we inject emotion information to the TTS model using an": ""
        },
        {
          "from [19], we inject emotion information to the TTS model using an": "jointly trained on the TTS dataset with the emotion labels. For the"
        },
        {
          "from [19], we inject emotion information to the TTS model using an": ""
        },
        {
          "from [19], we inject emotion information to the TTS model using an": "inference, we ﬁrstly select a reference audio set\nfor each emotion"
        },
        {
          "from [19], we inject emotion information to the TTS model using an": ""
        },
        {
          "from [19], we inject emotion information to the TTS model using an": "category from the TTS dataset. Then, we average the style tokens’"
        },
        {
          "from [19], we inject emotion information to the TTS model using an": ""
        },
        {
          "from [19], we inject emotion information to the TTS model using an": "weights of all audios in the reference audio set to synthesize speech"
        },
        {
          "from [19], we inject emotion information to the TTS model using an": ""
        },
        {
          "from [19], we inject emotion information to the TTS model using an": "for this kind of emotion."
        },
        {
          "from [19], we inject emotion information to the TTS model using an": ""
        },
        {
          "from [19], we inject emotion information to the TTS model using an": "3.1. Cross-domain SER model"
        },
        {
          "from [19], we inject emotion information to the TTS model using an": "3.1.1. Model structure"
        },
        {
          "from [19], we inject emotion information to the TTS model using an": ""
        },
        {
          "from [19], we inject emotion information to the TTS model using an": "As shown in the upper part of Figure 1, the SER model consists of a"
        },
        {
          "from [19], we inject emotion information to the TTS model using an": ""
        },
        {
          "from [19], we inject emotion information to the TTS model using an": "feature extraction encoder and an emotion classiﬁer. The encoder is"
        },
        {
          "from [19], we inject emotion information to the TTS model using an": "a CNN-RNN network, which is a popular structure in SER. Firstly,"
        },
        {
          "from [19], we inject emotion information to the TTS model using an": ""
        },
        {
          "from [19], we inject emotion information to the TTS model using an": "a 4-layer of 2D convolution network takes the 80-dimension log mel"
        },
        {
          "from [19], we inject emotion information to the TTS model using an": "spectrum as input\nfeature and outputs a 2D feature map which is"
        },
        {
          "from [19], we inject emotion information to the TTS model using an": ""
        },
        {
          "from [19], we inject emotion information to the TTS model using an": "ﬂattened into a sequence of\nfeature vectors.\nThen, a bidirectional"
        },
        {
          "from [19], we inject emotion information to the TTS model using an": ""
        },
        {
          "from [19], we inject emotion information to the TTS model using an": "GRU layer is followed and outputs the hidden state vector of the last"
        },
        {
          "from [19], we inject emotion information to the TTS model using an": "time step as the ﬁnal feature vector. The emotion classiﬁer is a 2-"
        },
        {
          "from [19], we inject emotion information to the TTS model using an": "layer dense network with a softmax activation for\nthe output. The"
        },
        {
          "from [19], we inject emotion information to the TTS model using an": ""
        },
        {
          "from [19], we inject emotion information to the TTS model using an": "number of output units is 4 for the emotion category classiﬁcation"
        },
        {
          "from [19], we inject emotion information to the TTS model using an": ""
        },
        {
          "from [19], we inject emotion information to the TTS model using an": "task and 2 for the arousal and valence polarity classiﬁcation task."
        },
        {
          "from [19], we inject emotion information to the TTS model using an": ""
        },
        {
          "from [19], we inject emotion information to the TTS model using an": ""
        },
        {
          "from [19], we inject emotion information to the TTS model using an": "3.1.2. Cross-domain training"
        },
        {
          "from [19], we inject emotion information to the TTS model using an": ""
        },
        {
          "from [19], we inject emotion information to the TTS model using an": "Since the TTS and SER datasets are quite different\nin speakers,"
        },
        {
          "from [19], we inject emotion information to the TTS model using an": ""
        },
        {
          "from [19], we inject emotion information to the TTS model using an": "recording devices and recording environment, some domain adapta-"
        },
        {
          "from [19], we inject emotion information to the TTS model using an": ""
        },
        {
          "from [19], we inject emotion information to the TTS model using an": "tion technique is necessary for our SER task to reduce the distribu-"
        },
        {
          "from [19], we inject emotion information to the TTS model using an": ""
        },
        {
          "from [19], we inject emotion information to the TTS model using an": "tion shift of these two datasets. The Maximum Mean Discrepancy"
        },
        {
          "from [19], we inject emotion information to the TTS model using an": ""
        },
        {
          "from [19], we inject emotion information to the TTS model using an": "(MMD)\n[20]\nis a kernel-based test\nstatistic to judge whether\ntwo"
        },
        {
          "from [19], we inject emotion information to the TTS model using an": ""
        },
        {
          "from [19], we inject emotion information to the TTS model using an": "distributions\nare\nequal, which is widely used in domain adapta-"
        },
        {
          "from [19], we inject emotion information to the TTS model using an": ""
        },
        {
          "from [19], we inject emotion information to the TTS model using an": "tion [21, 22] for measuring the similarity of two distributions.\nIt has"
        },
        {
          "from [19], we inject emotion information to the TTS model using an": "also been proved to be effective for cross-domain SER in [16, 17]."
        },
        {
          "from [19], we inject emotion information to the TTS model using an": "Considering that the training of MMD is stable and our TTS dataset"
        },
        {
          "from [19], we inject emotion information to the TTS model using an": ""
        },
        {
          "from [19], we inject emotion information to the TTS model using an": "has no available emotion labels for parameter tuning,we choose the"
        },
        {
          "from [19], we inject emotion information to the TTS model using an": ""
        },
        {
          "from [19], we inject emotion information to the TTS model using an": "MMD method for our cross-domain SER task."
        },
        {
          "from [19], we inject emotion information to the TTS model using an": ""
        },
        {
          "from [19], we inject emotion information to the TTS model using an": "We refer\nto the SER dataset as the source domain Ds and the"
        },
        {
          "from [19], we inject emotion information to the TTS model using an": ""
        },
        {
          "from [19], we inject emotion information to the TTS model using an": "TTS dataset as the target domain Dt. As described in [23], we mini-"
        },
        {
          "from [19], we inject emotion information to the TTS model using an": ""
        },
        {
          "from [19], we inject emotion information to the TTS model using an": "mize the following MMD loss to reduce the distribution discrepancy"
        },
        {
          "from [19], we inject emotion information to the TTS model using an": ""
        },
        {
          "from [19], we inject emotion information to the TTS model using an": "between the two domains."
        },
        {
          "from [19], we inject emotion information to the TTS model using an": ""
        },
        {
          "from [19], we inject emotion information to the TTS model using an": ""
        },
        {
          "from [19], we inject emotion information to the TTS model using an": "1 m\n1 n\ni=1Σm\ni=1Σn"
        },
        {
          "from [19], we inject emotion information to the TTS model using an": "LMM D =\nj=1k(si, sj) +\nj=1k(ti, tj)\n2 Σm\n2 Σn"
        },
        {
          "from [19], we inject emotion information to the TTS model using an": ""
        },
        {
          "from [19], we inject emotion information to the TTS model using an": "1 m\n−\nΣm\n(1)\ni=1Σn\nj=1k(si, tj)"
        },
        {
          "from [19], we inject emotion information to the TTS model using an": "n"
        },
        {
          "from [19], we inject emotion information to the TTS model using an": ""
        },
        {
          "from [19], we inject emotion information to the TTS model using an": "Where si and ti are the output features of the encoder from Ds and"
        },
        {
          "from [19], we inject emotion information to the TTS model using an": "Dt respectively; m and n are the number of samples of Ds and Dt"
        },
        {
          "from [19], we inject emotion information to the TTS model using an": "respectively; k(., .) is the kernel function that\nis a linear combina-"
        },
        {
          "from [19], we inject emotion information to the TTS model using an": "(cid:107)xi −\ntion of multiple RBF kernels:k(xi, xj) = Σnηn exp{− 1"
        },
        {
          "from [19], we inject emotion information to the TTS model using an": "2σn"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "ity respectively as the emotion prediction task. Meanwhile, in order",
          "by a single female speaker and this expressive styles is not annotated,": "which makes it appropriate for our task."
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "to independently control\nthese two orthogonal emotion dimensions,",
          "by a single female speaker and this expressive styles is not annotated,": ""
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "",
          "by a single female speaker and this expressive styles is not annotated,": "4.2. Training setup"
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "we split the style token weight vector into two valves, which are then",
          "by a single female speaker and this expressive styles is not annotated,": ""
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "",
          "by a single female speaker and this expressive styles is not annotated,": "Our\ntraining process\ninvolves ﬁrst\ntraining a\ncross-domain SER"
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "fed into the arousal and valence classiﬁers respectively.",
          "by a single female speaker and this expressive styles is not annotated,": ""
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "",
          "by a single female speaker and this expressive styles is not annotated,": "model, followed by training an end-to-end GST-based TTS model."
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "3.2.3. Choice for reference audio set of each emotion class",
          "by a single female speaker and this expressive styles is not annotated,": "To train the SER model, we use the cross-entropy loss and an"
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "",
          "by a single female speaker and this expressive styles is not annotated,": "additional MMD loss with a batch size of 96. The Adam [25] op-"
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "There are two common methods for\nthe synthesis of GST model:",
          "by a single female speaker and this expressive styles is not annotated,": ""
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "",
          "by a single female speaker and this expressive styles is not annotated,": "timizer and learning rate schedule:\ninitialized to 3e-5 and ﬁxed to"
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "selecting a reference audio with the desired style or manually speci-",
          "by a single female speaker and this expressive styles is not annotated,": ""
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "",
          "by a single female speaker and this expressive styles is not annotated,": "3e-4 after 100 steps are used to optimize model parameters. The 80-"
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "fying the weights of the style tokens. In our experiments, the quality",
          "by a single female speaker and this expressive styles is not annotated,": ""
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "",
          "by a single female speaker and this expressive styles is not annotated,": "band mel-scale spectrum is extracted frame-wise as input feature to"
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "and stability of\nspeech synthesized from a single reference audio",
          "by a single female speaker and this expressive styles is not annotated,": ""
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "",
          "by a single female speaker and this expressive styles is not annotated,": "be consistent with the input of TTS model. We randomly select N"
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "heavily depend on the choice of reference audio and the content of",
          "by a single female speaker and this expressive styles is not annotated,": ""
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "",
          "by a single female speaker and this expressive styles is not annotated,": "utterances from the source domain dataset as validation set for early"
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "the text to be synthesized, which may be due to the fact that the GST",
          "by a single female speaker and this expressive styles is not annotated,": ""
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "",
          "by a single female speaker and this expressive styles is not annotated,": "stopping (N=500 for IEMOCAP and 200 for RECOLA)."
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "module still does not completely disentangle the audio style and text",
          "by a single female speaker and this expressive styles is not annotated,": ""
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "",
          "by a single female speaker and this expressive styles is not annotated,": "To train the emotional TTS model, we use the mean absolute"
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "content. Therefore, we manually specify the weight of style tokens",
          "by a single female speaker and this expressive styles is not annotated,": ""
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "",
          "by a single female speaker and this expressive styles is not annotated,": "error\nfor\nthe reconstruction of both mel and linear spectrum and a"
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "by averaging the style token weights of an audio set\nthat belongs to",
          "by a single female speaker and this expressive styles is not annotated,": ""
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "",
          "by a single female speaker and this expressive styles is not annotated,": "cross-entropy loss for\nthe auxiliary emotion prediction task with a"
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "a certain kind of emotion. The audio set can be directly speciﬁed as",
          "by a single female speaker and this expressive styles is not annotated,": ""
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "",
          "by a single female speaker and this expressive styles is not annotated,": "batch size of 32. We train all the TTS models with 150k steps using"
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "all\nthe utterances with the same emotion label, as used in [7], when",
          "by a single female speaker and this expressive styles is not annotated,": ""
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "",
          "by a single female speaker and this expressive styles is not annotated,": "the Adam optimizer with a warm-up learning rate schedule:\ninitial"
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "the TTS dataset has emotion labels. However, our TTS dataset has",
          "by a single female speaker and this expressive styles is not annotated,": ""
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "",
          "by a single female speaker and this expressive styles is not annotated,": "rate 2e-3, warm-up steps 4,000 and decay power -0.5. We also set"
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "no ground-truth emotion labels, and only the soft\nlabels predicted",
          "by a single female speaker and this expressive styles is not annotated,": ""
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "",
          "by a single female speaker and this expressive styles is not annotated,": "the softmax temperature of attention mechanism to 2.0 in the synthe-"
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "from the cross-domain SER model are available. Because the cross-",
          "by a single female speaker and this expressive styles is not annotated,": ""
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "",
          "by a single female speaker and this expressive styles is not annotated,": "sis phase, which can effectively improve the stability of alignments"
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "domain SER model is far less reliable than humans,\nthese predicted",
          "by a single female speaker and this expressive styles is not annotated,": ""
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "",
          "by a single female speaker and this expressive styles is not annotated,": "especially for synthesizing highly expressive emotion speech. We"
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "emotion labels may contain a great number of mispredictions and",
          "by a single female speaker and this expressive styles is not annotated,": ""
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "",
          "by a single female speaker and this expressive styles is not annotated,": "publicly release the audio demos1 and all\nthe source codes2 online,"
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "we can not use all the utterances with the same soft label as the ref-",
          "by a single female speaker and this expressive styles is not annotated,": ""
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "",
          "by a single female speaker and this expressive styles is not annotated,": "where more training and model details can be found."
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "erence audio set.\nIn order to choose a more reliable reference audio",
          "by a single female speaker and this expressive styles is not annotated,": ""
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "set, we propose that only the K utterances with highest posterior are",
          "by a single female speaker and this expressive styles is not annotated,": "Table 1. WA and UA results for baseline and mmd-based models."
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "selected as the reference audio set, rather than the full set of a cer-",
          "by a single female speaker and this expressive styles is not annotated,": "Iem2Rec\nRec2Iem"
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "tain emotion. Choosing these K high-conﬁdence audios can greatly",
          "by a single female speaker and this expressive styles is not annotated,": "model\narousal\nvalence\narousal\nvalence\naverage"
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "reduce the impact of the prediction error of the SER model and it is",
          "by a single female speaker and this expressive styles is not annotated,": ""
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "",
          "by a single female speaker and this expressive styles is not annotated,": "0.534\n0.632\n0.617\n0.488\n0.568\nser-base"
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "",
          "by a single female speaker and this expressive styles is not annotated,": "WA"
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "a very important choice for generating an emotional speech in our",
          "by a single female speaker and this expressive styles is not annotated,": ""
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "",
          "by a single female speaker and this expressive styles is not annotated,": "0.538\nser-mmd\n0.720\n0.642\n0.500\n0.600"
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "experiments. We set K=50 for all of our experiments.",
          "by a single female speaker and this expressive styles is not annotated,": ""
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "",
          "by a single female speaker and this expressive styles is not annotated,": "0.529\n0.470\n0.500\n0.505\n0.501\nser-base"
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "",
          "by a single female speaker and this expressive styles is not annotated,": "UA"
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "",
          "by a single female speaker and this expressive styles is not annotated,": "ser-mmd\n0.550\n0.489\n0.543\n0.518\n0.525"
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "4. EXPERIMENTS",
          "by a single female speaker and this expressive styles is not annotated,": ""
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "",
          "by a single female speaker and this expressive styles is not annotated,": "4.3. Cross-domain SER results"
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "4.1. Datasets",
          "by a single female speaker and this expressive styles is not annotated,": ""
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "",
          "by a single female speaker and this expressive styles is not annotated,": "In order to evaluate the effectiveness of MMD for cross-domain SER"
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "We use the IEMOCAP as the SER dataset and the English dataset of",
          "by a single female speaker and this expressive styles is not annotated,": ""
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "",
          "by a single female speaker and this expressive styles is not annotated,": "tasks, we conduct experiments for the arousal and valence emotion"
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "Blizzard Challenge 2013 (BC2013-English) as the TTS dataset for",
          "by a single female speaker and this expressive styles is not annotated,": ""
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "",
          "by a single female speaker and this expressive styles is not annotated,": "dimensions on the IEMOCAP and RECOLA datasets.\nIt\nis worth"
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "our emotional TTS experiments.\nIn addition, because there are no",
          "by a single female speaker and this expressive styles is not annotated,": ""
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "",
          "by a single female speaker and this expressive styles is not annotated,": "noting that the RECOLA dataset is not annotated with emotion cate-"
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "emotion labels on BC2013-English, we use the an additional SER",
          "by a single female speaker and this expressive styles is not annotated,": ""
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "",
          "by a single female speaker and this expressive styles is not annotated,": "gory labels, and thus we do not preform experiments for the emotion"
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "dataset: RECOLA to verify the effectiveness of MMD on cross-",
          "by a single female speaker and this expressive styles is not annotated,": ""
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "",
          "by a single female speaker and this expressive styles is not annotated,": "category task. We train two models: ser-base and ser-mmd, and the"
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "domain SER tasks.",
          "by a single female speaker and this expressive styles is not annotated,": ""
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "",
          "by a single female speaker and this expressive styles is not annotated,": "structure of the two models is the same as described in section 3.1.1,"
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "IEMOCAP [12] is an audiovisual database of English dyadic con-",
          "by a single female speaker and this expressive styles is not annotated,": ""
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "",
          "by a single female speaker and this expressive styles is not annotated,": "except\nthat\nthe ser-mmd model has an additional MMD loss with"
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "versations performed by ten professional\nactors, which contains",
          "by a single female speaker and this expressive styles is not annotated,": ""
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "",
          "by a single female speaker and this expressive styles is not annotated,": "weight λ=0.5. We use Rec and Iem to represent\nthe RECOLA and"
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "about 12.5 hours and a total of 10,039 utterances. For emotion cat-",
          "by a single female speaker and this expressive styles is not annotated,": ""
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "",
          "by a single female speaker and this expressive styles is not annotated,": "IEMOCAP dataset, and Rec2Iem means training on Rec and testing"
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "egory schedule, we use the subset of the data that contains only the",
          "by a single female speaker and this expressive styles is not annotated,": ""
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "",
          "by a single female speaker and this expressive styles is not annotated,": "on Iem and vice versa. Both the Weighted Accuracy (WA) and Un-"
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "neutral, angry, sad, happy, excited utterances and merge the excited",
          "by a single female speaker and this expressive styles is not annotated,": ""
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "",
          "by a single female speaker and this expressive styles is not annotated,": "weighted Accuracy (UA) are selected as our evaluation criteria since"
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "to the happy category. For arousal and valence dimension schedule,",
          "by a single female speaker and this expressive styles is not annotated,": ""
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "",
          "by a single female speaker and this expressive styles is not annotated,": "an imbalance of emotion categories exists on these two datasets. Ta-"
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "we use all utterances and map the original 5-point\nlabel\ninto binary",
          "by a single female speaker and this expressive styles is not annotated,": ""
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "",
          "by a single female speaker and this expressive styles is not annotated,": "ble 1 reports the WA and UA results for the models."
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "one-hot label with 2.5 as the dividing point.",
          "by a single female speaker and this expressive styles is not annotated,": ""
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "",
          "by a single female speaker and this expressive styles is not annotated,": "As shown in Table 1, except for the experiment of Iem2Rec of"
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "RECOLA [13] is a multimodal database of French dyadic conver-",
          "by a single female speaker and this expressive styles is not annotated,": ""
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "",
          "by a single female speaker and this expressive styles is not annotated,": "arousal, the ser-mmd model shows an improvement of at least 1.2%"
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "sations, where continuous arousal and valence label in the range [-1,",
          "by a single female speaker and this expressive styles is not annotated,": ""
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "",
          "by a single female speaker and this expressive styles is not annotated,": "compared to the ser-base model on both WA and UA. And the total"
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "1] are annotated at frame level.\nIn our study, we use all\nthe 1,308",
          "by a single female speaker and this expressive styles is not annotated,": ""
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "",
          "by a single female speaker and this expressive styles is not annotated,": "average improvement of WA and UA over all\nfour experiments is"
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "freely available utterances from 23 speakers and map the utterance-",
          "by a single female speaker and this expressive styles is not annotated,": ""
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "",
          "by a single female speaker and this expressive styles is not annotated,": "3.2% and 2.4% respectively.\nThis shows that\nthe performance of"
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "level average of the original\nlabels to the binary one-hot\nlabel with",
          "by a single female speaker and this expressive styles is not annotated,": ""
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "",
          "by a single female speaker and this expressive styles is not annotated,": "cross-domain SER can be improved after the discrepancy of feature"
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "0.0 as the dividing point.",
          "by a single female speaker and this expressive styles is not annotated,": ""
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "",
          "by a single female speaker and this expressive styles is not annotated,": "distribution between two domains is reduced."
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "BC2013-English [24] is a audiobook dataset provided by The Voice",
          "by a single female speaker and this expressive styles is not annotated,": ""
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "Factory, where approximately 300 hours of chapter-sized mp3 ﬁles",
          "by a single female speaker and this expressive styles is not annotated,": "4.4. Emotional TTS results"
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "and 19 hours of wav ﬁles are available. We use a segmented subset of",
          "by a single female speaker and this expressive styles is not annotated,": ""
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "",
          "by a single female speaker and this expressive styles is not annotated,": "The overall quality and emotional\nexpressiveness of\nsynthesized"
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "approximately 198 hours data and ﬁlter out utterances that are longer",
          "by a single female speaker and this expressive styles is not annotated,": ""
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "",
          "by a single female speaker and this expressive styles is not annotated,": "speech are the two most signiﬁcant evaluation criteria for an emo-"
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "than 14 seconds or have more more than 100 characters from this",
          "by a single female speaker and this expressive styles is not annotated,": ""
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "subset. The ﬁnal dataset used in our experiments is about 73 hours",
          "by a single female speaker and this expressive styles is not annotated,": "1https://thuhcsi.github.io/icassp2021-emotion-tts"
        },
        {
          "build two binary classiﬁers for predicting arousal and valence polar-": "of total 95k utterances.\nIn addition, the stories are read expressively",
          "by a single female speaker and this expressive styles is not annotated,": "2https://github.com/thuhcsi/icassp2021-emotion-tts"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "tional TTS system. We perform two subjective\nexperiments\nfor": "these two criteria using three systems:\nour proposed system for 4",
          "Finally, we also use the t-SNE [26] algorithm to project the style": "token weights on the reference audio sets into the 2D space for our-"
        },
        {
          "tional TTS system. We perform two subjective\nexperiments\nfor": "emotion categories (our-4cls), our proposed system for 2 emotion",
          "Finally, we also use the t-SNE [26] algorithm to project the style": "4cls and base-4cls. As shown in Figure 4, the style token weights of"
        },
        {
          "tional TTS system. We perform two subjective\nexperiments\nfor": "dimensions\n(our-2d)\nand a baseline\nsystem (base-4cls) which is",
          "Finally, we also use the t-SNE [26] algorithm to project the style": "the 4 emotion categories are clearly clustered into 4 clusters for the"
        },
        {
          "tional TTS system. We perform two subjective\nexperiments\nfor": "the same as our-4cls except\nthat\nthe auxiliary emotion prediction",
          "Finally, we also use the t-SNE [26] algorithm to project the style": "our-4cls model, while for the base-4cls model, except for the angry"
        },
        {
          "tional TTS system. We perform two subjective\nexperiments\nfor": "task is not used in training phase. We randomly choose 10 sentences",
          "Finally, we also use the t-SNE [26] algorithm to project the style": "catetory,\nthere is no obvious clustering boundary for the other three"
        },
        {
          "tional TTS system. We perform two subjective\nexperiments\nfor": "outside the TTS dataset as inference texts, and 20 university students",
          "Finally, we also use the t-SNE [26] algorithm to project the style": "categories. This can also be considered as another possible evidience"
        },
        {
          "tional TTS system. We perform two subjective\nexperiments\nfor": "are invited to participate in our subjective experiments.",
          "Finally, we also use the t-SNE [26] algorithm to project the style": "to explain the role of the auxiliary emotion prediction task."
        },
        {
          "tional TTS system. We perform two subjective\nexperiments\nfor": "4.4.1. Evaluation for the overall quality of speech",
          "Finally, we also use the t-SNE [26] algorithm to project the style": ""
        },
        {
          "tional TTS system. We perform two subjective\nexperiments\nfor": "In this section, we perform mean opinion score (MOS) evaluation",
          "Finally, we also use the t-SNE [26] algorithm to project the style": ""
        },
        {
          "tional TTS system. We perform two subjective\nexperiments\nfor": "experiment in terms of the overall quality of speech (naturalness, in-",
          "Finally, we also use the t-SNE [26] algorithm to project the style": ""
        },
        {
          "tional TTS system. We perform two subjective\nexperiments\nfor": "telligibility and speech quality). Table 2 and Table 3 report the MOS",
          "Finally, we also use the t-SNE [26] algorithm to project the style": ""
        },
        {
          "tional TTS system. We perform two subjective\nexperiments\nfor": "results of the above three systems. The average score of base-4cls is",
          "Finally, we also use the t-SNE [26] algorithm to project the style": ""
        },
        {
          "tional TTS system. We perform two subjective\nexperiments\nfor": "higher than that of the our-4cls and our-2d models, but the p-values",
          "Finally, we also use the t-SNE [26] algorithm to project the style": ""
        },
        {
          "tional TTS system. We perform two subjective\nexperiments\nfor": "are much greater than the singiﬁcance leval of α = 0.05 indicating",
          "Finally, we also use the t-SNE [26] algorithm to project the style": ""
        },
        {
          "tional TTS system. We perform two subjective\nexperiments\nfor": "",
          "Finally, we also use the t-SNE [26] algorithm to project the style": "(a) base-4cls\n(b) our-4cls\n(c) full-4cls"
        },
        {
          "tional TTS system. We perform two subjective\nexperiments\nfor": "that\nthere is no signiﬁcant difference between these models.\nThis",
          "Finally, we also use the t-SNE [26] algorithm to project the style": ""
        },
        {
          "tional TTS system. We perform two subjective\nexperiments\nfor": "result suggests that our proposed our-4cls and our-2d models can",
          "Finally, we also use the t-SNE [26] algorithm to project the style": "Fig. 2.\nConfusion matrices of 4 emotion categories\nfor\nthe three"
        },
        {
          "tional TTS system. We perform two subjective\nexperiments\nfor": "almost achieve as good speech quality as the baseline system. In ad-",
          "Finally, we also use the t-SNE [26] algorithm to project the style": "methods: base-4cls, our-4cls and full-4cls."
        },
        {
          "tional TTS system. We perform two subjective\nexperiments\nfor": "dition,\nit can also be found that\nthe MOS score of happy is greatly",
          "Finally, we also use the t-SNE [26] algorithm to project the style": ""
        },
        {
          "tional TTS system. We perform two subjective\nexperiments\nfor": "lower than the others for both base-4cls and our-4cls. One possible",
          "Finally, we also use the t-SNE [26] algorithm to project the style": ""
        },
        {
          "tional TTS system. We perform two subjective\nexperiments\nfor": "reason is that\nthe SER model\nitself has lower accuracy for happy in",
          "Finally, we also use the t-SNE [26] algorithm to project the style": ""
        },
        {
          "tional TTS system. We perform two subjective\nexperiments\nfor": "our experiments, which will lead to great differences in the prosody",
          "Finally, we also use the t-SNE [26] algorithm to project the style": ""
        },
        {
          "tional TTS system. We perform two subjective\nexperiments\nfor": "consistency of the top K reference audios selected by the SER model.",
          "Finally, we also use the t-SNE [26] algorithm to project the style": ""
        },
        {
          "tional TTS system. We perform two subjective\nexperiments\nfor": "This also indicates that\nthe performance of\nthe cross-domain SER",
          "Finally, we also use the t-SNE [26] algorithm to project the style": ""
        },
        {
          "tional TTS system. We perform two subjective\nexperiments\nfor": "model is critical for our proposed approach.",
          "Finally, we also use the t-SNE [26] algorithm to project the style": ""
        },
        {
          "tional TTS system. We perform two subjective\nexperiments\nfor": "Table 2. MOS of base-4cls and our-4cls for 4 emotion categories.",
          "Finally, we also use the t-SNE [26] algorithm to project the style": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6. REFERENCES": "",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": "Denis Lalanne, “Introducing the RECOLA multimodal corpus"
        },
        {
          "6. REFERENCES": "[1] Yuxuan Wang, RJ Skerry-Ryan, Daisy Stanton, Yonghui Wu,",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": "of remote collaborative and affective interactions,” in IEEE In-"
        },
        {
          "6. REFERENCES": "Ron J Weiss, Navdeep Jaitly, Zongheng Yang, Ying Xiao,",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": "ternational Conference and Workshops on Automatic Face and"
        },
        {
          "6. REFERENCES": "Zhifeng Chen, Samy Bengio, et al.,\n“Tacotron: Towards end-",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": "Gesture Recognition (FG), 2013, pp. 1–8."
        },
        {
          "6. REFERENCES": "the International\nto-end speech synthesis,”\nin Conference of",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": ""
        },
        {
          "6. REFERENCES": "",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": "[14] Carlos Busso, Srinivas Parthasarathy, Alec Burmania, Mo-"
        },
        {
          "6. REFERENCES": "Speech Communication Association (INTERSPEECH), 2017.",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": ""
        },
        {
          "6. REFERENCES": "",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": "hammed AbdelWahab, Najmeh Sadoughi, and Emily Mower"
        },
        {
          "6. REFERENCES": "[2]\nJonathan Shen, Ruoming Pang, Ron J Weiss, Mike Schuster,",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": "Provost, “Msp-improv: An acted corpus of dyadic interactions"
        },
        {
          "6. REFERENCES": "Navdeep Jaitly, Zongheng Yang, Zhifeng Chen, Yu Zhang,",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": "IEEE Transactions on Affective\nto study emotion perception,”"
        },
        {
          "6. REFERENCES": "Yuxuan Wang, Rj Skerrv-Ryan, et al.,\n“Natural\ntts synthesis",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": "Computing, vol. 8, no. 1, pp. 67–80, 2016."
        },
        {
          "6. REFERENCES": "by conditioning wavenet on mel spectrogram predictions,”\nin",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": ""
        },
        {
          "6. REFERENCES": "",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": "[15]\nJianyou Wang, Michael Xue, Ryan Culhane, Enmao Diao, Jie"
        },
        {
          "6. REFERENCES": "IEEE International Conference on Acoustics, Speech and Sig-",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": ""
        },
        {
          "6. REFERENCES": "",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": "Ding, and Vahid Tarokh,\n“Speech emotion recognition with"
        },
        {
          "6. REFERENCES": "nal Processing (ICASSP), 2018, pp. 4779–4783.",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": ""
        },
        {
          "6. REFERENCES": "",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": "dual-sequence lstm architecture,”\nin IEEE International Con-"
        },
        {
          "6. REFERENCES": "[3] Naihan Li, Shujie Liu, Yanqing Liu, Sheng Zhao, and Ming",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": ""
        },
        {
          "6. REFERENCES": "",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": "ference on Acoustics, Speech and Signal Processing (ICASSP),"
        },
        {
          "6. REFERENCES": "Liu,\n“Neural speech synthesis with transformer network,”\nin",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": ""
        },
        {
          "6. REFERENCES": "",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": "2020, pp. 6474–6478."
        },
        {
          "6. REFERENCES": "Proceedings of\nthe AAAI Conference on Artiﬁcial Intelligence",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": ""
        },
        {
          "6. REFERENCES": "",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": "[16]\nPeng Song, Wenming Zheng, Shifeng Ou, Xinran Zhang, Yun"
        },
        {
          "6. REFERENCES": "(AAAI), 2019, vol. 33, pp. 6706–6713.",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": ""
        },
        {
          "6. REFERENCES": "",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": "Jin, Jinglei Liu, and Yanwei Yu,\n“Cross-corpus speech emo-"
        },
        {
          "6. REFERENCES": "[4]\nJaime Lorenzo-Trueba, Gustav Eje Henter, Shinji Takaki, Ju-",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": ""
        },
        {
          "6. REFERENCES": "",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": "tion recognition based on transfer non-negative matrix factor-"
        },
        {
          "6. REFERENCES": "nichi Yamagishi, Yosuke Morino, and Yuta Ochiai, “Investigat-",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": ""
        },
        {
          "6. REFERENCES": "",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": "ization,” Speech Communication, vol. 83, pp. 34–41, 2016."
        },
        {
          "6. REFERENCES": "ing different representations for modeling and controlling mul-",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": ""
        },
        {
          "6. REFERENCES": "",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": "[17] Yuan Zong, Wenming Zheng,\nTong Zhang,\nand Xiaohua"
        },
        {
          "6. REFERENCES": "Speech Com-\ntiple emotions in dnn-based speech synthesis,”",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": ""
        },
        {
          "6. REFERENCES": "",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": "Huang,\n“Cross-corpus speech emotion recognition based on"
        },
        {
          "6. REFERENCES": "munication, vol. 99, pp. 135–143, 2018.",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": ""
        },
        {
          "6. REFERENCES": "",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": "IEEE signal pro-\ndomain-adaptive least-squares regression,”"
        },
        {
          "6. REFERENCES": "[5] Younggun Lee, Azam Rabiee, and Soo-Young Lee,\n“Emo-",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": ""
        },
        {
          "6. REFERENCES": "",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": "cessing letters, vol. 23, no. 5, pp. 585–589, 2016."
        },
        {
          "6. REFERENCES": "arXiv preprint\ntional end-to-end neural speech synthesizer,”",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": ""
        },
        {
          "6. REFERENCES": "",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": "[18]\nSiddique Latif, Junaid Qadir, and Muhammad Bilal, “Unsuper-"
        },
        {
          "6. REFERENCES": "arXiv:1711.05447, 2017.",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": ""
        },
        {
          "6. REFERENCES": "",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": "vised adversarial domain adaptation for cross-lingual speech"
        },
        {
          "6. REFERENCES": "[6] Yuxuan Wang, Daisy Stanton, Yu Zhang, RJ-Skerry Ryan, Eric",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": ""
        },
        {
          "6. REFERENCES": "",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": "emotion recognition,”\nin International Conference on Affec-"
        },
        {
          "6. REFERENCES": "Battenberg, Joel Shor, Ying Xiao, Ye Jia, Fei Ren, and Rif A",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": ""
        },
        {
          "6. REFERENCES": "",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": "tive Computing and Intelligent\nInteraction (ACII), 2019, pp."
        },
        {
          "6. REFERENCES": "Saurous, “Style tokens: Unsupervised style modeling, control",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": ""
        },
        {
          "6. REFERENCES": "",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": "732–737."
        },
        {
          "6. REFERENCES": "and transfer in end-to-end speech synthesis,”\nin International",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": ""
        },
        {
          "6. REFERENCES": "",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": "[19] Zack Hodari, Oliver Watts, Srikanth Ronanki, and Simon King,"
        },
        {
          "6. REFERENCES": "Conference on Machine Learning (ICML), 2018, pp. 5180–",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": ""
        },
        {
          "6. REFERENCES": "",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": "“Learning interpretable control dimensions for speech synthe-"
        },
        {
          "6. REFERENCES": "5189.",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": ""
        },
        {
          "6. REFERENCES": "",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": "sis by using external data.,” in Conference of the International"
        },
        {
          "6. REFERENCES": "[7] Ohsung Kwon, Inseon Jang, ChungHyun Ahn, and Hong-Goo",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": ""
        },
        {
          "6. REFERENCES": "",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": "Speech Communication Association (INTERSPEECH), 2018,"
        },
        {
          "6. REFERENCES": "Kang,\n“An effective style token weight control\ntechnique for",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": ""
        },
        {
          "6. REFERENCES": "",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": "pp. 32–36."
        },
        {
          "6. REFERENCES": "end-to-end emotional speech synthesis,” IEEE Signal Process-",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": ""
        },
        {
          "6. REFERENCES": "",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": "[20] Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bern-"
        },
        {
          "6. REFERENCES": "ing Letters (SPL), vol. 26, no. 9, pp. 1383–1387, 2019.",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": ""
        },
        {
          "6. REFERENCES": "",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": "hard Sch¨olkopf, and Alexander Smola,\n“A kernel\ntwo-sample"
        },
        {
          "6. REFERENCES": "[8]\nSe-Yun Um, Sangshin Oh, Kyungguen Byun,\nInseon Jang,",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": ""
        },
        {
          "6. REFERENCES": "",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": "test,” The Journal of Machine Learning Research (JMLR), vol."
        },
        {
          "6. REFERENCES": "ChungHyun Ahn, and Hong-Goo Kang,\n“Emotional speech",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": ""
        },
        {
          "6. REFERENCES": "",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": "13, no. 1, pp. 723–773, 2012."
        },
        {
          "6. REFERENCES": "synthesis with rich and granularized control,” in IEEE Interna-",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": ""
        },
        {
          "6. REFERENCES": "tional Conference on Acoustics, Speech and Signal Processing",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": "[21] Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jor-"
        },
        {
          "6. REFERENCES": "(ICASSP), 2020, pp. 7254–7258.",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": "dan,\n“Learning transferable\nfeatures with deep adaptation"
        },
        {
          "6. REFERENCES": "",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": "networks,”\nin International conference on machine learning"
        },
        {
          "6. REFERENCES": "[9] No´e Tits, Kevin El Haddad, and Thierry Dutoit,\n“Exploring",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": ""
        },
        {
          "6. REFERENCES": "",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": "(ICML), 2015, pp. 97–105."
        },
        {
          "6. REFERENCES": "transfer learning for low resource emotional\ntts,”\nin Proceed-",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": ""
        },
        {
          "6. REFERENCES": "ings of SAI Intelligent Systems Conference, 2019, pp. 52–60.",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": "[22] Konstantinos Bousmalis, George Trigeorgis, Nathan Silber-"
        },
        {
          "6. REFERENCES": "",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": "man, Dilip Krishnan, and Dumitru Erhan,\n“Domain separa-"
        },
        {
          "6. REFERENCES": "[10]\nPengfei Wu,\nZhenhua\nLing,\nLijuan\nLiu,\nYuan\nJiang,",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": ""
        },
        {
          "6. REFERENCES": "",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": "information processing\ntion networks,”\nin Advances in neural"
        },
        {
          "6. REFERENCES": "Hongchuan Wu,\nand Lirong Dai,\n“End-to-end\nemotional",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": ""
        },
        {
          "6. REFERENCES": "",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": "systems, 2016, pp. 343–351."
        },
        {
          "6. REFERENCES": "speech synthesis using style tokens and semi-supervised train-",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": ""
        },
        {
          "6. REFERENCES": "ing,” in Asia-Paciﬁc Signal and Information Processing Asso-",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": "[23] Daniel Grifﬁn and Jae Lim, “Signal estimation from modiﬁed"
        },
        {
          "6. REFERENCES": "ciation Annual Summit and Conference (APSIPA ASC), 2019,",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": "short-time fourier transform,” IEEE Transactions on Acoustics,"
        },
        {
          "6. REFERENCES": "pp. 623–627.",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": "Speech, and Signal Processing, vol. 32, no. 2, pp. 236–243,"
        },
        {
          "6. REFERENCES": "",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": "1984."
        },
        {
          "6. REFERENCES": "[11] Yang Gao, Weiyi Zheng, Zhaojun Yang, Thilo Kohler, Chris-",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": ""
        },
        {
          "6. REFERENCES": "tian Fuegen,\nand Qing He,\n“Interactive\ntext-to-speech via",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": "[24]\nS. King and Vasilis Karaiskos, “The blizzard challenge 2013,”"
        },
        {
          "6. REFERENCES": "arXiv\npreprint\nsemi-supervised\nstyle\ntransfer\nlearning,”",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": "2013."
        },
        {
          "6. REFERENCES": "arXiv:2002.06758, 2020.",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": ""
        },
        {
          "6. REFERENCES": "",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": "[25] Diederik P. Kingma\nand\nJimmy Ba,\n“Adam:\nA method"
        },
        {
          "6. REFERENCES": "[12] Carlos\nBusso,\nMurtaza\nBulut,\nChi-Chun\nLee,\nAbe",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": "for stochastic optimization,”\nin International Conference on"
        },
        {
          "6. REFERENCES": "Kazemzadeh,\nEmily Mower,\nSamuel Kim,\nJeannette N",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": "Learning Representations (ICLR), 2015."
        },
        {
          "6. REFERENCES": "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“IEMO-",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": ""
        },
        {
          "6. REFERENCES": "",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": "[26] Laurens van der Maaten and Geoffrey Hinton,\n“Visualiz-"
        },
        {
          "6. REFERENCES": "CAP:\nInteractive emotional dyadic motion capture database,”",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": ""
        },
        {
          "6. REFERENCES": "",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": "Journal of machine learning research\ning data using t-sne,”"
        },
        {
          "6. REFERENCES": "Language Resources and Evaluation, vol. 42, no. 4, pp. 335,",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": ""
        },
        {
          "6. REFERENCES": "",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": "(JMLR), vol. 9, no. Nov, pp. 2579–2605, 2008."
        },
        {
          "6. REFERENCES": "2008.",
          "[13]\nFabien Ringeval, Andreas Sonderegger,\nJuergen Sauer,\nand": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Tacotron: Towards endto-end speech synthesis",
      "authors": [
        "Yuxuan Wang",
        "R Skerry-Ryan",
        "Daisy Stanton",
        "Yonghui Wu",
        "Ron Weiss",
        "Navdeep Jaitly",
        "Zongheng Yang",
        "Ying Xiao",
        "Zhifeng Chen",
        "Samy Bengio"
      ],
      "year": "2017",
      "venue": "Conference of the International Speech Communication Association (INTERSPEECH)"
    },
    {
      "citation_id": "3",
      "title": "Natural tts synthesis by conditioning wavenet on mel spectrogram predictions",
      "authors": [
        "Jonathan Shen",
        "Ruoming Pang",
        "Ron Weiss",
        "Mike Schuster",
        "Navdeep Jaitly",
        "Zongheng Yang",
        "Zhifeng Chen",
        "Yu Zhang",
        "Yuxuan Wang",
        "Rj Skerrv-Ryan"
      ],
      "year": "2018",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "4",
      "title": "Neural speech synthesis with transformer network",
      "authors": [
        "Naihan Li",
        "Shujie Liu",
        "Yanqing Liu",
        "Sheng Zhao",
        "Ming Liu"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)"
    },
    {
      "citation_id": "5",
      "title": "Investigating different representations for modeling and controlling multiple emotions in dnn-based speech synthesis",
      "authors": [
        "Jaime Lorenzo-Trueba",
        "Gustav Henter",
        "Shinji Takaki",
        "Junichi Yamagishi",
        "Yosuke Morino",
        "Yuta Ochiai"
      ],
      "year": "2018",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "6",
      "title": "Emotional end-to-end neural speech synthesizer",
      "authors": [
        "Younggun Lee",
        "Azam Rabiee",
        "Soo-Young Lee"
      ],
      "year": "2017",
      "venue": "Emotional end-to-end neural speech synthesizer",
      "arxiv": "arXiv:1711.05447"
    },
    {
      "citation_id": "7",
      "title": "Style tokens: Unsupervised style modeling, control and transfer in end-to-end speech synthesis",
      "authors": [
        "Yuxuan Wang",
        "Daisy Stanton",
        "Rj-Skerry Zhang",
        "Eric Ryan",
        "Joel Battenberg",
        "Ying Shor",
        "Ye Xiao",
        "Fei Jia",
        "Rif Ren",
        "Saurous"
      ],
      "year": "2018",
      "venue": "International Conference on Machine Learning (ICML)"
    },
    {
      "citation_id": "8",
      "title": "An effective style token weight control technique for end-to-end emotional speech synthesis",
      "authors": [
        "Ohsung Kwon",
        "Inseon Jang",
        "Chunghyun Ahn",
        "Hong-Goo Kang"
      ],
      "year": "2019",
      "venue": "IEEE Signal Processing Letters (SPL)"
    },
    {
      "citation_id": "9",
      "title": "Emotional speech synthesis with rich and granularized control",
      "authors": [
        "Se-Yun Um",
        "Sangshin Oh",
        "Kyungguen Byun",
        "Inseon Jang",
        "Chunghyun Ahn",
        "Hong-Goo Kang"
      ],
      "year": "2020",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "10",
      "title": "Exploring transfer learning for low resource emotional tts",
      "authors": [
        "Noé Tits",
        "Kevin Haddad",
        "Thierry Dutoit"
      ],
      "year": "2019",
      "venue": "Proceedings of SAI Intelligent Systems Conference"
    },
    {
      "citation_id": "11",
      "title": "End-to-end emotional speech synthesis using style tokens and semi-supervised training",
      "authors": [
        "Pengfei Wu",
        "Zhenhua Ling",
        "Lijuan Liu",
        "Yuan Jiang",
        "Hongchuan Wu",
        "Lirong Dai"
      ],
      "year": "2019",
      "venue": "Asia-Pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "12",
      "title": "Interactive text-to-speech via semi-supervised style transfer learning",
      "authors": [
        "Yang Gao",
        "Weiyi Zheng",
        "Zhaojun Yang",
        "Thilo Kohler"
      ],
      "year": "2020",
      "venue": "Interactive text-to-speech via semi-supervised style transfer learning",
      "arxiv": "arXiv:2002.06758"
    },
    {
      "citation_id": "13",
      "title": "IEMO-CAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "14",
      "title": "Introducing the RECOLA multimodal corpus of remote collaborative and affective interactions",
      "authors": [
        "Fabien Ringeval",
        "Andreas Sonderegger",
        "Juergen Sauer",
        "Denis Lalanne"
      ],
      "year": "2013",
      "venue": "IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "15",
      "title": "Msp-improv: An acted corpus of dyadic interactions to study emotion perception",
      "authors": [
        "Carlos Busso",
        "Srinivas Parthasarathy",
        "Alec Burmania",
        "Mohammed Abdelwahab",
        "Najmeh Sadoughi",
        "Emily Provost"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "16",
      "title": "Speech emotion recognition with dual-sequence lstm architecture",
      "authors": [
        "Jianyou Wang",
        "Michael Xue",
        "Ryan Culhane",
        "Enmao Diao",
        "Jie Ding",
        "Vahid Tarokh"
      ],
      "year": "2020",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "17",
      "title": "Cross-corpus speech emotion recognition based on transfer non-negative matrix factorization",
      "authors": [
        "Peng Song",
        "Wenming Zheng",
        "Shifeng Ou",
        "Xinran Zhang",
        "Yun Jin",
        "Jinglei Liu",
        "Yanwei Yu"
      ],
      "year": "2016",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "18",
      "title": "Cross-corpus speech emotion recognition based on domain-adaptive least-squares regression",
      "authors": [
        "Yuan Zong",
        "Wenming Zheng",
        "Tong Zhang",
        "Xiaohua Huang"
      ],
      "year": "2016",
      "venue": "IEEE signal processing letters"
    },
    {
      "citation_id": "19",
      "title": "Unsupervised adversarial domain adaptation for cross-lingual speech emotion recognition",
      "authors": [
        "Siddique Latif",
        "Junaid Qadir",
        "Muhammad Bilal"
      ],
      "year": "2019",
      "venue": "International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "20",
      "title": "Learning interpretable control dimensions for speech synthesis by using external data",
      "authors": [
        "Zack Hodari",
        "Oliver Watts",
        "Srikanth Ronanki",
        "Simon King"
      ],
      "year": "2018",
      "venue": "Conference of the International Speech Communication Association (INTERSPEECH)"
    },
    {
      "citation_id": "21",
      "title": "A kernel two-sample test",
      "authors": [
        "Arthur Gretton",
        "Karsten Borgwardt",
        "J Malte",
        "Bernhard Rasch",
        "Alexander Schölkopf",
        "Smola"
      ],
      "year": "2012",
      "venue": "The Journal of Machine Learning Research (JMLR)"
    },
    {
      "citation_id": "22",
      "title": "Learning transferable features with deep adaptation networks",
      "authors": [
        "Mingsheng Long",
        "Yue Cao",
        "Jianmin Wang",
        "Michael Jordan"
      ],
      "year": "2015",
      "venue": "International conference on machine learning (ICML)"
    },
    {
      "citation_id": "23",
      "title": "Domain separation networks",
      "authors": [
        "Konstantinos Bousmalis",
        "George Trigeorgis",
        "Nathan Silberman",
        "Dilip Krishnan",
        "Dumitru Erhan"
      ],
      "year": "2016",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "24",
      "title": "Signal estimation from modified short-time fourier transform",
      "authors": [
        "Daniel Griffin",
        "Jae Lim"
      ],
      "year": "1984",
      "venue": "IEEE Transactions on Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "25",
      "title": "The blizzard challenge 2013",
      "authors": [
        "S King",
        "Vasilis Karaiskos"
      ],
      "year": "2013",
      "venue": "The blizzard challenge 2013"
    },
    {
      "citation_id": "26",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2015",
      "venue": "International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "27",
      "title": "Visualizing data using t-sne",
      "authors": [
        "Laurens Van Der Maaten",
        "Geoffrey Hinton"
      ],
      "year": "2008",
      "venue": "Journal of machine learning research (JMLR)"
    }
  ]
}