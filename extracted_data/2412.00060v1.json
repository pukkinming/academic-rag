{
  "paper_id": "2412.00060v1",
  "title": "Mosabench: Multi-Object Sentiment Analysis Benchmark For Evaluating Multimodal Large Language Models Understanding Of Complex Image",
  "published": "2024-11-25T09:00:36Z",
  "authors": [
    "Shezheng Song",
    "Chengxiang He",
    "Shasha Li",
    "Shan Zhao",
    "Chengyu Wang",
    "Tianwei Yan",
    "Xiaopeng Li",
    "Qian Wan",
    "Jun Ma",
    "Jie Yu",
    "Xiaoguang Mao"
  ],
  "keywords": [
    "Multimodal Large Language Model",
    "Multimodal Sentiment Analysis",
    "Benchmark"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal large language models (MLLMs) have shown remarkable progress in high-level semantic tasks such as visual question answering, image captioning, and emotion recognition. However, despite advancements, there remains a lack of standardized benchmarks for evaluating MLLMs performance in multi-object sentiment analysis, a key task in semantic understanding. To address this gap, we introduce MOSABench, a novel evaluation dataset designed specifically for multi-object sentiment analysis. MOSABench includes approximately 1,000 images with multiple objects, requiring MLLMs to independently assess the sentiment of each object, thereby reflecting realworld complexities. Key innovations in MOSABench include distance-based target annotation, post-processing for evaluation to standardize outputs, and an improved scoring mechanism. Our experiments reveal notable limitations in current MLLMs: while some models, like mPLUG-owl and Qwen-VL2, demonstrate effective attention to sentiment-relevant features, others exhibit scattered focus and performance declines, especially as the spatial distance between objects increases. This research underscores the need for MLLMs to enhance accuracy in complex, multiobject sentiment analysis tasks and establishes MOSABench as a foundational tool for advancing sentiment analysis capabilities in MLLMs.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "In recent years, multimodal large language models  [1]  (MLLMs) have made significant progress in image understanding tasks, demonstrating immense potential, particularly in high-level semantic tasks such as visual question answering  [2, 3, 4] , image captioning  [5, 6] , and emotion Open-sourced Close-sourced Fig.  1 : F1 comparison on MOSABench across multimodal large language models recognition  [7, 8, 9] . Commercial platforms like GPT4o  [10]  and Gemini  [11] , as well as open-source models such as Flamingo  [12]  and LLaVA  [13] , have achieved outstanding performance on a wide range of traditional tasks, even surpassing human-level performance in tasks like multimodal named entity recognition  [14, 15, 16] . This impressive potential has driven the development of multimodal evaluation tasks. However, sentiment analysis  [17, 18] , as one of the key tasks in semantic understanding, still lacks a standardized benchmark specifically designed for evaluating MLLMs. Figure  1  shows that the performance of MLLMs on our MOSABench is unsatisfactory.\n\nAs shown in Figure  2  and 3, current sentiment analysis datasets have limitations in accurately evaluating the capabilities of MLLMs. Lack of focus on multi-object understanding: In real-world social media, images typically contain multiple objects (such as people), each of which may convey different emotional information. Therefore, MLLMs are facing the challenge of multiple object sentiment analysis. However, most existing sentiment analysis benchmarks, such as Twit-ter15, Twitter17, and MSED  [19] , are dominated by singleobject samples, which leads to models focusing primarily on single-object sentiment classification ability  [20, 21] . When single-object data dominate the dataset, the model's good performance likely reflects its ability to classify emotions for individual objects, rather than its capacity to analyze emotions across multiple objects in an image. As a result, evaluating models on such imbalanced datasets fails to comprehensively assess their ability to understand multiple-object emotions, which limits the potential of multimodal sentiment analysis models. Lack of adaptability to MLLMs: Moreover, existing datasets  [19, 22]  have substantial limitations in evaluating the capabilities of MLLMs. Most of these datasets are designed for smaller models, lack instruction, and require extensive adaptation, making it challenging to comprehensively assess the performance of MLLMs. Additionally, since MLLMs generate outputs in varying formats rather than strictly following predefined formats, accurately evaluating their performance is more difficult. Therefore, developing a standardized, multidimensional benchmark specifically designed for evaluating multi-object sentiment analysis tasks has become an urgent challenge. To bridge this research gap, we introduce a new evaluation benchmark, MOSABench, comprising a dataset for testing and a scoring system adapted for MLLMs, offering a standardized and thorough assessment in multi-object sentiment analysis tasks. MOSABench contains approximately 1,000 text-image pairs, each with multiple objects, requiring the MLLM to independently assess the sentiment for each object to handle complex multi-object scenarios. Besides, MOSABench presents a standardized approach for assessing large model outputs in multi-object sentiment analysis. By integrating an enhanced scoring mechanism with F1 scores, we provide a comprehensive evaluation of model performance on multiple objects, advancing research and applications in this domain. Specifically, MOSABench introduces three key innovations to advance the evaluation of multi-object sentiment analysis for MLLMs: distance-based object annotation, post-processing for evaluation, and improved scoring mechanism. In (1) distancebased target annotation, we label the spatial distance between objects within images, revealing a significant relationship between object proximity and sentiment prediction accuracy-the further apart the objects are, the lower the accuracy tends to be. This provides a new perspective on MLLM limitations, highlighting that sentiment prediction accuracy can be influenced by the spatial arrangement of targets in images. To address issues with inconsistent output formats in MLLM responses, we propose (2) post-processing for evaluation. This step is designed to standardize model outputs, reducing the impact of the format of MLLM responses on evaluation accuracy. By minimizing the influence of nonstandard formats, our post-processing enables a more accurate assessment of MLLM capabilities in understanding image content. Our (3) improved scoring mechanism introduces a unique multi-object evaluation approach. For each sample, separate sentiment judgments are required for two objects, assigning 3 points if both judgments are correct, 1 point if only one is correct, and 0 if both are incorrect. Together with traditional metrics like F1, Precision, and Recall, this scoring framework provides a comprehensive evaluation of model performance in handling multi-object sentiment tasks, allowing for a nuanced analysis of MLLM strengths and limitations.\n\nWe conduct a comprehensive evaluation of mainstream MLLMs on our MOSABench. Our analysis leads to the following key observations: (1) Challenges in MLLMs for multiple objects analysis: Most current models perform suboptimally on the MOSABench multi-object sentiment analysis task, revealing significant challenges in handling multiple target emotions simultaneously. Although these models perform well on other tasks, such as VQA  [2, 23]  and MNER  [14, 15] , they have limitations when it comes to multi-object sentiment analysis, which demands higher comprehension and reasoning abilities for multiple objects. (2) Impact of object distance on accuracy: Our further analysis reveals a significant correlation between task accuracy and the spatial relationship between targets in the image. We categorize the distance between objects in the data into three types: Interlap, Close, and Far. The experimental results show that as the distance between targets increases, the performance of most MLLMs decreases. This finding provides a new perspective for improving model performance in multi-object sentiment analysis and emphasizes the importance of spatial relationships in multi-object contexts. (3) Performance variations across models: We also observe significant performance differences across models. For example, the mPLUG-owl  [24]  and Qwen-VL2  [25]  models demonstrate relatively high stability and accuracy across various evaluation metrics. This may be due to their optimized architectures and larger parameter scales, which enable them to better adapt to complex sentiment analysis tasks. In contrast, models such as Qwen-VL and BLIVA-Flant5 show poorer performance, suggesting a lack of sufficient generalization capability in multi-object sentiment analysis, making it difficult for these models to accurately capture emotional features of multiple objects.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work A. Advances In Mllms For Sentiment Analysis And The Demand For Scientific Benchmarking",
      "text": "The recent advancements in large language models (LLMs)  [26, 27, 28, 29]  have significantly improved multimodal sentiment analysis, effectively handling complex sentiment tasks across various modalities  [30, 31] . Studies have demonstrated that LLMs achieve high accuracy on standard datasets, such as Twitter15 and Twitter17  [22] , which are widely used to assess sentiment analysis capabilities by integrating text and image data to analyze social media posts. For example, the WisdoM  [32]  framework leverages large visionlanguage models (LVLMs) to enhance sentiment analysis by incorporating contextual world knowledge from images and text, thus improving LLMs interpretability and performance  [33] . Similarly, the PSL  [34]  framework is a pipelinebased approach for aspect-based sentiment analysis, using small language models (SLMs)  [35]  for aspect extraction and MLLMs for sentiment analysis. This structured guidance enables MLLMs to focus on relevant image regions, effectively addressing the complexities of multimodal sentiment tasks  [34] . These frameworks highlight the progress LLMs have made in multimodal alignment and sentiment understanding  [20, 33] . However, current benchmarks, like Twitter15 and Twitter17  [22] , reveal limitations in assessing MLLMs true multimodal comprehension capabilities. Primarily, these datasets often lack image-text consistency, where the targets mentioned in the text may not appear in the associated image, hindering accurate evaluations of MLLMs capacity to integrate visual context  [33] . Additionally, these benchmarks are not equipped with LLM-specific instructions  [36] , making it challenging to assess the impact of different prompting methods on sentiment prediction  [34] . Lastly, they lack multi-object sentiment assessment, which is crucial for evaluating the ability of MLLMs to independently analyze sentiments towards multiple entities within a single post  [32, 34] . These gaps underscore the need for a scientifically designed benchmark that captures multimodal nuances, includes structured prompts tailored for LLM tasks, and offers multi-object sentiment assessment to comprehensively evaluate LLM performance in real-world multimodal sentiment applications.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "B. Limitations Of Current Benchmarks In Addressing Sentiment Analysis Needs",
      "text": "In the evaluation of LLMs, tasks such as Named Entity Recognition (MNER)  [14, 15]  have developed dedicated benchmarks to better measure model performance. However, sentiment analysis lacks a specialized benchmark tailored specifically for LLMs. While some comprehensive benchmarks, such as MM-SOC  [37]  and MM-BigBench  [38] , include multimodal tasks related to sentiment analysis, they still exhibit notable limitations in design and evaluation  [39, 40] , particularly in areas of image-text consistency, instruction design, and multi-object sentiment judgment  [32, 34] . For instance, MM-SOC primarily targets multimodal tasks within social media environments, covering sentiment analysis, hate speech detection  [41] , and more, using the Memotion dataset  [42]  for joint image-text emotion detection. However, MM-SOC does not specifically address image-text consistency, which limits the evaluation of whether all entities mentioned in the text are also represented in the image. Additionally, MM-SOC lacks LLM-specific instructions, restricting its utility in large-scale multimodal tasks that require instruction-following capabilities  [34] . MM-BigBench, on the other hand, covers a broad range of multimodal comprehension tasks, including visual question answering and multimodal sentiment analysis (MSA), focusing on multimodal information fusion and deep understanding. However, MM-BigBench does not provide detailed evaluations for multi-object sentiment analysis and does not adequately emphasize image-text consistency, which is essential for accurately identifying the sentiments toward multiple entities mentioned in the text. Furthermore, this benchmark lacks instructions specifically designed for LLMs, making it difficult to assess how different prompt structures might affect performance.\n\nIn summary, while current benchmarks have made progress in multimodal sentiment analysis, there remains significant room for improvement in image-text consistency, multi-object sentiment assessment, and instruction design specifically for LLMs.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Iii. Mosa Benchmark Construction",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Instructions:",
      "text": "Based on the text and image provided, identify the sentiment for both 'Cristiano Ronaldo' and 'the host' directly from the options below. Answer only in the format 'X, Y' where: X is the sentiment for 'Cristiano Ronaldo', Y is the sentiment for 'the host' Select one of the following sentiments: Fig.  4 : Data example of our MOSABench. \"Instruction\" specifies the task that the LLM needs to perform, while \"Answer\" represents the expected result of the task execution. \"Objects\" indicates the targets present in the image, requiring the LLM to complete the task in the \"Instruction\" by integrating the \"Text\" and \"Image\".",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Dataset Construction",
      "text": "We construct the MOSABench dataset to address limitations in current MLLMs for multi-object sentiment analysis tasks. First, samples are selected from Twitter15, Twitter17, and TwiBot-20 that meet the requirements for multi-object",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Interlap",
      "text": "Close Far sentiment analysis. To ensure data accuracy and diversity, strict filtering criteria are applied: each text must contain multiple distinct targets, and each target must also appear in the corresponding image, enabling the model to capture emotional cues from both visual and textual information. Abstract terms such as \"United Nations\" are excluded to ensure that each target in the sample has a clear emotional expression, thereby enhancing the model in distinguishing sentiments across multiple targets. During annotation, these samples are adapted from the original Multi-Aspect Based Sentiment Analysis (MABSA) format to a sentiment analysis format. Given a specified target, diverse question forms are designed, such as \"Please confirm the sentiment of X\" or \"Please judge the status of X,\" to simulate the model adaptability to different question types. This diverse design not only enhances the generalizability of the task but also prevents potential bias that may arise from a single question style, thereby enabling a more comprehensive evaluation of model capability.\n\nTo ensure objectivity and consistency in evaluation, the labeling system in our dataset is simplified by adopting a binary-choice structure in which \"A\" and \"B\" represent different sentiment categories, such as negative, neutral, and positive, using a fixed sentiment mapping. This structure reduces errors caused by inconsistent labels and streamlines model output processing, making sentiment analysis evaluation more straightforward. Additionally, the evaluation method is improved by implementing a novel scoring mechanism: for binary-choice questions, fully correct answers receive 3 points, partially correct answers receive 1 point, and fully incorrect answers receive no points. This scoring method enables a more comprehensive assessment of MLLM capabilities, enhancing the reliability and precision of sentiment analysis evaluation and providing a practical benchmark for multi-object sentiment analysis tasks.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Distance Annotation",
      "text": "As shown in Figure  5 , we also annotate and analyze the spatial distances between targets within each image. This annotation strategy aims to verify the relationship between target distance and task accuracy, revealing the potential limitations of MLLMs in multi-object sentiment analysis. Experimental results show that target distance significantly impacts sentiment judgment accuracy; in particular, the accuracy decreases Step 1: Compute center points C 1 and C 2 :\n\n▷ Calculate the midpoint of each bounding box\n\nStep 2: Check for overlap (Interlap) As shown in this algorithm 1, we calculate the spatial relationship between two detected objects based on their bounding boxes. The bounding boxes, B 1 and B 2 , are obtained using an object detection model, where only the two highest confidence detections of category \"person\" are selected as input. This ensures that the algorithm focuses on the spatial relation between two human figures within the image. The input image length L and threshold parameter k allow us to classify the relationship as Interlap, Close, or Far. Specifically, the parameter k serves as a tunable threshold to adjust the proximity level for determining Close and Far classifications.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Datasets Statistics",
      "text": "We conduct a statistical analysis of our MOSABench dataset, with results presented in Table  I . The dataset contains a total of 1,047 samples, categorized into three distance groups: Close, Interlap, and Far, to capture varying spatial configurations. Among these samples, 57.11% are classified as Close, 31.18% as Interlap, and 11.71% as Far, providing a balanced representation across different target proximities. This distribution facilitates a comprehensive evaluation of MLLM performance under diverse spatial contexts.\n\nThe sentiment distribution in MOSABench reflects a scientifically consistent approach, aligning with the proportions observed in prior datasets. In MOSABench, Negative, Neutral, and Positive sentiments are represented by 15.44%, 49.43%, and 35.13%, respectively. This design aligns closely with the sentiment distributions in the Twitter15 and Twitter17 datasets  [22] , which feature similar proportions: Twitter15 includes 12.06% Negative, 59.29% Neutral, and 28.65% Positive samples, while Twitter17 comprises 12.19% Negative, 45.68% Neutral, and 42.13% Positive samples. By mirroring these established distributions, MOSABench provides a scientifically robust benchmark for evaluating MLLMs sentiment analysis capabilities across multiple targets.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "D. Mllm Baselines",
      "text": "Based on the MOSABench dataset we constructed, we conduct a comprehensive evaluation of existing MLLMs to assess their performance in multi-object sentiment analysis tasks. To this end, we propose two baseline methods and systematically test various mainstream model architectures, covering three distinct types. In terms of model selection, we choose representative MLLMs, categorized into three groups according to their architectures:\n\n• Open-sourced models: LLaVA1.6  [13] , mPLUG-Owl  [24] , Qwen-VL  [43] , Qwen-VL2  [25] , VisualGLM  [44] , BLIVA-FlanT5  [45] , Monkey  [46] , GLM4V  [47] , InternLM2.5  [48] . • Close-sourced models: ERNIE-Bot  [49] , GPT-4o  [10]  and Gemini  [11] . This diverse selection of models ensures the broad applicability and representativeness of the evaluation results.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "E. Post-Processing For Llm Scoring Assistance",
      "text": "In our experiments, we observe that not all large language models generate outputs with consistent formatting. Some models show variability in response length or randomness due to pre-training biases, which makes it difficult to score responses effectively using simple regular expression matching. To address this issue, we design a post-processing program that leverages a dedicated language model to simplify and standardize outputs before scoring them with a regex-based approach.\n\nThe post-processing program simplifies complex generated text, enabling it to conform to standardized scoring requirements. Some generated responses contain lengthy explanations, which complicates direct regex matching. The dedicated language model simplifies and reformats these outputs to enable subsequent scoring automation.\n\nExample: \"Ground Truth Answer\": \"A, B\". \"LLM Response\": \"Based on the text provided, the sentiment for 'Midwest' can be inferred as A. Negative, due to the implication of disruption caused by the travel advisory. For 'FAA', since it is an authoritative body issuing the advisory, the sentiment could be interpreted as B. Neutral, as it is not expressing a positive or negative opinion but rather providing factual information.\"\n\nIn this example, the \"LLM Response\" output contains complex explanatory text, which makes it difficult to match directly with \"Ground Truth Answer\" using regular expressions. With post-processing, however, the \"LLM Response\" output is reformatted to (A, B), allowing the regex-based scoring program to interpret and score the response accurately.\n\nThis post-processing strategy improves scoring accuracy and reduces the need for extensive formatting instructions in multimodal sentiment analysis, minimizing potential interference with model performance. We select the Qwen2.5-7B-Instruct model for this post-processing, as it demonstrates strong capabilities in instruction-following and structured output generation. Other language models with structured output capabilities are also suitable for this purpose.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "F. Metrics",
      "text": "In the Benchmark evaluation study, a novel scoring method is introduced to enhance the assessment of MOSABench performance. Specifically, each question is designed as a multiplechoice task where each sample requires separate emotion assessments for object1 and object2. Following an examinationstyle grading approach, if both assessments are correct, the sample receives a score of 3; if only one assessment is correct, it receives a score of 1; and if both are incorrect, no points are awarded, with a maximum score of 3 for each sample. To facilitate comparison with traditional benchmarks, each MLLM performance is also evaluated using standard metrics, including F1, Precision, and Recall. This combined assessment provides a comprehensive view of model effectiveness in multi-object sentiment analysis tasks.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Iv. Experiments And Analysis",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Main Result Analysis",
      "text": "As shown in Table  II , we evaluate the performance of various MLLMs on MOSABench, focusing on multi-object sentiment analysis across different target distances. This table  includes both open-sourced models (e.g., Qwen-VL, Visual-GLM, Monkey) and close-sourced models (e.g., ERNIE Bot, GPT4o, Gemini), with performance F1, Precision, and Recall for three distance categories: Interlap, Close, and Far. Opensourced models exhibit a broad performance range. Among them, Qwen-VL2-7B achieves the highest overall F1 Score of 58.39, showing balanced performance across all categories. However, models like Qwen-VL-7B and VisualGLM-6B score significantly lower, especially with distant targets in the Far category. Close-sourced models generally outperform opensourced ones, with ERNIE Bot achieving the highest overall F1 Score of 68.62, followed by Gemini at 57.26, indicating an advantage for close-sourced models in complex, multi-object sentiment analysis.\n\nIn the open-source category, Qwen-VL2-7B and GLM4V-9B demonstrate strong performance, particularly in the Close and Interlap categories, suggesting effective sentiment detection for nearby or overlapping targets. Monkey, with an overall F1 of 39.87, shows inconsistent performance, especially in the Far category, reflecting limitations with distant targets. Among closed-source models, ERNIE Bot consistently outperforms GPT4o and Gemini, maintaining high F1, Precision, and Re-  The results show that most models perform best in the Close category, followed by Interlap, with the lowest performance in the Far category. For example, ERNIE Bot scores an F1 of 69.79 in Interlap and 69.05 in Close, but drops to 63.32 in Far. This trend highlights the difficulty of interpreting emotional cues from distant objects, a challenge shared by both opensourced and close-sourced models. The decline in metrics from Close to Far underscores the limitations of current MLLMs in identifying emotions across spatial resolutions. The performance decline across distance categories and differences between models highlight the need for improvements in MLLM capabilities for multi-object sentiment analysis. While models like ERNIE Bot and mPLUG set high performance standards, especially for spatially proximate targets, low scores in the Far category reveal a gap in current architectures for handling distant emotional targets. Future research should focus on enhancing MLLM accuracy across diverse spatial contexts, particularly in challenging multi-object scenarios.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "B. Score Distribution Across Distance Labels",
      "text": "Table  III  shows model performance across Interlap, Close, and Far categories, highlighting a decline in correct predictions (S3) and average scores ( S) as object distance increases. This trend indicates that MLLMs struggle more with emotion recognition as targets become more distant, underscoring their limitations in complex, multi-object scenarios. Among opensourced models, Qwen-VL2-7B achieves the highest average score ( S) of 1.54, with strong S3 proportions across distances, showing effective emotion detection, especially for nearby objects. In contrast, Qwen-VL-7B performs the worst, with an average score of 0.31 and high S0 proportions, particularly in",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "C. Attention Visualization Analysis",
      "text": "Figure  8  visualizes the attention regions of various MLLMs during multi-object sentiment analysis, highlighting how each model interprets visual information, particularly in recognizing facial expressions, which are crucial for accurate sentiment detection. mPLUG-Owl, with the highest F1 (73.16), demonstrates the best focus, accurately targeting the facial expressions of both subjects. This focused attention suggests a strong understanding of task requirements, enabling it to avoid distractions and focus on sentiment-relevant features. Qwen-VL2, with an F1 of 58.39, also attends to the facial areas but with less intensity and consistency, indicating some missed or under-emphasized sentiment details, which may explain its lower accuracy compared to mPLUG-Owl. LLaVA, with an F1 of 51.31, disperses its attention between faces and irrelevant areas, suggesting an inability to isolate sentiment-relevant regions effectively. This scattered focus likely introduces noise, reducing its accuracy. VisualGLM, with the lowest F1 (16.71), fails to target facial expressions and instead focuses on unrelated areas, significantly impairing its sentiment detection performance. These visualizations underscore the importance of targeted attention in multi-object sentiment analysis: models that concentrate on sentiment-relevant areas, like facial expressions, achieve higher accuracy, while those with dispersed or misaligned focus, such as VisualGLM, perform poorly.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "D. Confusion Matrix Analysis",
      "text": "Figure  9  presents the confusion matrices for four MLLMs, illustrating their performance in multi-object sentiment analysis, with redder cells indicating higher counts for each sentiment classification. mPLUG-Owl, the model with the highest performance, shows strong results along the diagonal, with the reddest cells indicating it accurately classifies sentiments across categories, including neutral (neu) and negative (neg) sentiments. Qwen-VL2 also performs well but has a tendency to misclassify neutral sentiments as negative, as indicated by the red cells off the diagonal in the neutral-negative area. LLaVA, in contrast, often misclassifies negative sentiments as neutral, as shown by the reddish cells in the negativeneutral category. VisualGLM, the model with the weakest performance, struggles significantly, frequently misclassifying negative sentiments as positive, with noticeable red shading in the off-diagonal cells of the negative-positive category. This analysis highlights the distinct ways each model handles sentiment classification errors, with mPLUG-Owl achieving the most accurate classifications, while VisualGLM exhibits considerable challenges in sentiment analysis.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "V. Conclusion",
      "text": "We systematically investigate the limitations of MLLMs in multi-object sentiment analysis, particularly in scenarios with visually distinct and spatially varied targets. We introduce MOSABench, a benchmark dataset specifically designed to evaluate MLLM capabilities in independently and accurately assessing sentiments across multiple objects within a single image. MOSABench includes a wide range of spatial relationships, enabling a detailed analysis of how target proximity and feature diversity impact model performance. Our findings reveal the significant challenges MLLMs face in complex multiobject environments, highlighting the need for architectural enhancements to improve their adaptability to these tasks. By providing a dedicated dataset and comprehensive evaluation framework, this work lays a foundation for future research aimed at advancing MLLM performance in nuanced, multitarget sentiment analysis. MOSABench serves as an initial exploration for future MLLM research, offering directions and insights for evaluating multi-object sentiment analysis. This benchmark aims to contribute to the development and assessment of models better suited for complex multimodal tasks, supporting progress in multi-object sentiment understanding.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: F1 comparison on MOSABench across multimodal large",
      "page": 1
    },
    {
      "caption": "Figure 2: and 3, current sentiment analysis",
      "page": 1
    },
    {
      "caption": "Figure 2: Example from a previous dataset, illustrating single-object",
      "page": 2
    },
    {
      "caption": "Figure 3: Example from our MOSABench.",
      "page": 2
    },
    {
      "caption": "Figure 4: Data example of our MOSABench. “Instruction” specifies the",
      "page": 3
    },
    {
      "caption": "Figure 5: Distance label examples in MOSABench. “Interlap” indicates that the bounding boxes of the individuals overlap. “Close” denotes",
      "page": 4
    },
    {
      "caption": "Figure 5: , we also annotate and analyze the",
      "page": 4
    },
    {
      "caption": "Figure 6: Analysis of MLLM Score Variations with Target Distance on MOSABench: average score ¯S and distribution of P3 (Correct), P1",
      "page": 7
    },
    {
      "caption": "Figure 7: F1 across distance categories for different MLLMs",
      "page": 7
    },
    {
      "caption": "Figure 8: Attention heatmaps of MLLMs. F1 scores: 73.16 (mPLUG-Owl), 58.39 (Qwen-VL2), 51.31 (LLaVA), and 16.71 (VisualGLM).",
      "page": 8
    },
    {
      "caption": "Figure 9: The confusion matrix between actual predictions and ground truth of MLLMs",
      "page": 8
    },
    {
      "caption": "Figure 6: illustrates the decline in",
      "page": 8
    },
    {
      "caption": "Figure 8: visualizes the attention regions of various MLLMs",
      "page": 8
    },
    {
      "caption": "Figure 9: presents the confusion matrices for four MLLMs,",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Total Samples": "Shortest Text Length",
          "1047": "20"
        },
        {
          "Total Samples": "Longest Text Length",
          "1047": "146"
        },
        {
          "Total Samples": "Average Text Length",
          "1047": "86.27"
        },
        {
          "Total Samples": "Answer Proportions (Neg, Neu, Pos)",
          "1047": "15.4%, 49.4%, 35.1%"
        },
        {
          "Total Samples": "Distance Proportions (I, C, F)",
          "1047": "31.2%, 57.1%, 11.7%"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A survey on multimodal large language models",
      "authors": [
        "S Yin",
        "C Fu",
        "S Zhao",
        "K Li",
        "X Sun",
        "T Xu",
        "E Chen"
      ],
      "year": "2024",
      "venue": "National Science Review"
    },
    {
      "citation_id": "2",
      "title": "Vqa: Visual question answering",
      "authors": [
        "S Antol",
        "A Agrawal",
        "J Lu",
        "M Mitchell",
        "D Batra",
        "C Zitnick",
        "D Parikh"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE international conference on computer vision"
    },
    {
      "citation_id": "3",
      "title": "Visual question answering: A survey of methods and datasets",
      "authors": [
        "Q Wu",
        "D Teney",
        "P Wang",
        "C Shen",
        "A Dick",
        "A Van Den",
        "Hengel"
      ],
      "year": "2017",
      "venue": "Computer Vision and Image Understanding"
    },
    {
      "citation_id": "4",
      "title": "Spatial-semantic collaborative graph network for textbook question answering",
      "authors": [
        "Y Wang",
        "B Wei",
        "J Liu",
        "Q Lin",
        "L Zhang",
        "Y Wu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "5",
      "title": "A comprehensive survey of deep learning for image captioning",
      "authors": [
        "M Hossain",
        "F Sohel",
        "M Shiratuddin",
        "H Laga"
      ],
      "year": "2019",
      "venue": "ACM Computing Surveys (CsUR)"
    },
    {
      "citation_id": "6",
      "title": "Semantic alignment network for multi-modal emotion recognition",
      "authors": [
        "M Hou",
        "Z Zhang",
        "C Liu",
        "G Lu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "7",
      "title": "Human emotion recognition: Review of sensors and methods",
      "authors": [
        "A Dzedzickis",
        "A Kaklauskas",
        "V Bucinskas"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "8",
      "title": "Emotion recognition in human-computer interaction",
      "authors": [
        "R Cowie",
        "E Douglas-Cowie",
        "N Tsapatsoulis",
        "G Votsis",
        "S Kollias",
        "W Fellenz",
        "J Taylor"
      ],
      "year": "2001",
      "venue": "IEEE Signal processing magazine"
    },
    {
      "citation_id": "9",
      "title": "Stay in grid: Improving video captioning via fully gridlevel representation",
      "authors": [
        "M Tang",
        "Z Wang",
        "Z Zeng",
        "X Li",
        "L Zhou"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "10",
      "title": "Gpt-4 technical report",
      "authors": [
        "Openai"
      ],
      "year": "2023",
      "venue": "Gpt-4 technical report",
      "arxiv": "arXiv:2303.08774"
    },
    {
      "citation_id": "11",
      "title": "Gemini: A family of highly capable multimodal models",
      "authors": [
        "G Team",
        "R Anil",
        "S Borgeaud",
        "J.-B Alayrac",
        "J Yu",
        "R Soricut",
        "J Schalkwyk"
      ],
      "year": "2024",
      "venue": "Gemini: A family of highly capable multimodal models"
    },
    {
      "citation_id": "12",
      "title": "Flamingo: a visual language model for few-shot learning",
      "authors": [
        "J.-B Alayrac"
      ],
      "year": "2022",
      "venue": "Flamingo: a visual language model for few-shot learning"
    },
    {
      "citation_id": "13",
      "title": "Visual instruction tuning",
      "authors": [
        "H Liu"
      ],
      "year": "2023",
      "venue": "Visual instruction tuning",
      "arxiv": "arXiv:2304.08485"
    },
    {
      "citation_id": "14",
      "title": "Mner-qg: An end-to-end mrc framework for multimodal named entity recognition with query grounding",
      "authors": [
        "M Jia",
        "L Shen",
        "X Shen",
        "L Liao",
        "M Chen",
        "X He",
        "Z Chen",
        "J Li"
      ],
      "year": "2023",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "15",
      "title": "Cmner: A chinese multimodal ner dataset based on social media",
      "authors": [
        "Y Ji",
        "B Li",
        "J Zhou",
        "F Li",
        "C Teng",
        "D Ji"
      ],
      "year": "2024",
      "venue": "Cmner: A chinese multimodal ner dataset based on social media",
      "arxiv": "arXiv:2402.13693"
    },
    {
      "citation_id": "16",
      "title": "Maln: Multimodal adversarial learning network for conversational emotion recognition",
      "authors": [
        "M Ren",
        "X Huang",
        "J Liu",
        "M Liu",
        "X Li",
        "A.-A Liu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "17",
      "title": "Multimodal sentiment analysis: a survey of methods, trends, and challenges",
      "authors": [
        "R Das",
        "T Singh"
      ],
      "year": "2023",
      "venue": "ACM Computing Surveys"
    },
    {
      "citation_id": "18",
      "title": "Research anthology on implementing sentiment analysis across multiple disciplines",
      "authors": [
        "R Kaur",
        "S Kautish"
      ],
      "year": "2022",
      "venue": "Research anthology on implementing sentiment analysis across multiple disciplines"
    },
    {
      "citation_id": "19",
      "title": "Beyond emotion: A multi-modal dataset for human desire understanding",
      "authors": [
        "A Jia",
        "Y He",
        "Y Zhang",
        "S Uprety",
        "D Song",
        "C Lioma"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "20",
      "title": "A sentiment analysis method of objects by integrating sentiments from tweets",
      "authors": [
        "H Phan",
        "N Nguyen",
        "V Tran",
        "D Hwang"
      ],
      "year": "2019",
      "venue": "Journal of Intelligent & Fuzzy Systems"
    },
    {
      "citation_id": "21",
      "title": "Mcl: multimodal contrastive learning for deepfake detection",
      "authors": [
        "X Liu",
        "Y Yu",
        "X Li",
        "Y Zhao"
      ],
      "year": "2023",
      "venue": "IEEE Trans-actions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "22",
      "title": "Adaptive co-attention network for named entity recognition in tweets",
      "authors": [
        "Q Zhang",
        "J Fu",
        "X Liu",
        "X Huang"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": "10.1609/aaai.v32i1.11962"
    },
    {
      "citation_id": "23",
      "title": "Making the v in vqa matter: Elevating the role of image understanding in visual question answering",
      "year": "2017",
      "venue": "CVPR"
    },
    {
      "citation_id": "24",
      "title": "mplug-owl: Modularization empowers large language models with multimodality",
      "authors": [
        "Q Ye"
      ],
      "year": "2023",
      "venue": "mplug-owl: Modularization empowers large language models with multimodality",
      "arxiv": "arXiv:2304.14178"
    },
    {
      "citation_id": "25",
      "title": "Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution",
      "authors": [
        "P Wang",
        "S Bai",
        "S Tan",
        "S Wang",
        "Z Fan",
        "J Bai",
        "K Chen",
        "X Liu",
        "J Wang",
        "W Ge",
        "Y Fan",
        "K Dang",
        "M Du",
        "X Ren",
        "R Men",
        "D Liu",
        "C Zhou",
        "J Zhou",
        "J Lin"
      ],
      "year": "2024",
      "venue": "Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution",
      "arxiv": "arXiv:2409.12191"
    },
    {
      "citation_id": "26",
      "title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
      "authors": [
        "D Zhu"
      ],
      "year": "2023",
      "venue": "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
      "arxiv": "arXiv:2304.10592"
    },
    {
      "citation_id": "27",
      "title": "Shikra: Unleashing multimodal llm's referential dialogue magic",
      "authors": [
        "K Chen"
      ],
      "year": "2023",
      "venue": "Shikra: Unleashing multimodal llm's referential dialogue magic",
      "arxiv": "arXiv:2306.15195"
    },
    {
      "citation_id": "28",
      "title": "Lenna: Language enhanced reasoning detection assistant",
      "authors": [
        "F Wei"
      ],
      "year": "2023",
      "venue": "Lenna: Language enhanced reasoning detection assistant",
      "arxiv": "arXiv:2312.02433"
    },
    {
      "citation_id": "29",
      "title": "Idealgpt: Iteratively decomposing vision and language reasoning via large language models",
      "authors": [
        "H You"
      ],
      "year": "2023",
      "venue": "Idealgpt: Iteratively decomposing vision and language reasoning via large language models",
      "arxiv": "arXiv:2305.14985"
    },
    {
      "citation_id": "30",
      "title": "Multimodal foundation models: From specialists to general-purpose assistants",
      "authors": [
        "C Li"
      ],
      "year": "2023",
      "venue": "Multimodal foundation models: From specialists to general-purpose assistants",
      "arxiv": "arXiv:2309.10020"
    },
    {
      "citation_id": "31",
      "title": "Multimodal-gpt: A vision and language model for dialogue with humans",
      "authors": [
        "T Gong"
      ],
      "year": "2023",
      "venue": "Multimodal-gpt: A vision and language model for dialogue with humans",
      "arxiv": "arXiv:2305.04790"
    },
    {
      "citation_id": "32",
      "title": "Wisdom: Improving multimodal sentiment analysis by fusing contextual world knowledge",
      "authors": [
        "W Wang",
        "L Ding",
        "L Shen",
        "Y Luo",
        "H Hu",
        "D Tao"
      ],
      "year": "2024",
      "venue": "Wisdom: Improving multimodal sentiment analysis by fusing contextual world knowledge",
      "arxiv": "arXiv:2401.06659"
    },
    {
      "citation_id": "33",
      "title": "Sentiment analysis in the age of generative ai",
      "authors": [
        "J Krugmann",
        "J Hartmann"
      ],
      "year": "2024",
      "venue": "Customer Needs and Solutions"
    },
    {
      "citation_id": "34",
      "title": "Psl: A pipeline framework leveraging slm and llm for few-shot multimodal aspect-based sentiment analysis",
      "authors": [
        "S Song"
      ],
      "year": "2023",
      "venue": "arXiv preprint arXiv"
    },
    {
      "citation_id": "35",
      "title": "Dual-encoder transformers with crossmodal alignment for multimodal aspect-based sentiment analysis",
      "authors": [
        "Z Yu"
      ],
      "year": "2022",
      "venue": "AACL-IJCNLP. Online only"
    },
    {
      "citation_id": "36",
      "title": "Dreamllm: Synergistic multimodal comprehension and creation",
      "authors": [
        "R Dong"
      ],
      "year": "2023",
      "venue": "Dreamllm: Synergistic multimodal comprehension and creation",
      "arxiv": "arXiv:2309.11499"
    },
    {
      "citation_id": "37",
      "title": "Mm-soc: Benchmarking multimodal large language models in social media platforms",
      "authors": [
        "Y Jin",
        "M Choi",
        "G Verma",
        "J Wang",
        "S Kumar"
      ],
      "year": "2024",
      "venue": "Mm-soc: Benchmarking multimodal large language models in social media platforms",
      "arxiv": "arXiv:2402.14154"
    },
    {
      "citation_id": "38",
      "title": "Mm-bigbench: Evaluating multimodal models on multimodal content comprehension tasks",
      "authors": [
        "X Yang",
        "W Wu",
        "S Feng",
        "M Wang",
        "D Wang",
        "Y Li",
        "Q Sun",
        "Y Zhang",
        "X Fu",
        "S Poria"
      ],
      "year": "2023",
      "venue": "Mm-bigbench: Evaluating multimodal models on multimodal content comprehension tasks",
      "arxiv": "arXiv:2310.09036"
    },
    {
      "citation_id": "39",
      "title": "Evaluating large language models: A comprehensive survey",
      "authors": [
        "Z Guo"
      ],
      "year": "2023",
      "venue": "Evaluating large language models: A comprehensive survey",
      "arxiv": "arXiv:2310.19736"
    },
    {
      "citation_id": "40",
      "title": "Evaluation and analysis of hallucination in large vision-language models",
      "authors": [
        "J Wang"
      ],
      "year": "2023",
      "venue": "Evaluation and analysis of hallucination in large vision-language models",
      "arxiv": "arXiv:2308.15126"
    },
    {
      "citation_id": "41",
      "title": "Detgpt: Detect what you need via reasoning",
      "authors": [
        "R Pi"
      ],
      "year": "2023",
      "venue": "Detgpt: Detect what you need via reasoning",
      "arxiv": "arXiv:2305.14167"
    },
    {
      "citation_id": "42",
      "title": "Memotion 2: Dataset on sentiment and emotion analysis of memes",
      "authors": [
        "S Ramamoorthy",
        "N Gunti",
        "S Mishra",
        "S Suryavardan",
        "A Reganti",
        "P Patwa",
        "A Das",
        "T Chakraborty",
        "A Sheth"
      ],
      "year": "2022",
      "venue": "Proceedings of De-Factify: Workshop on Multimodal Fact Checking and Hate Speech Detection"
    },
    {
      "citation_id": "43",
      "title": "Qwen-vl: A frontier large vision-language model with versatile abilities",
      "authors": [
        "J Bai"
      ],
      "year": "2023",
      "venue": "Qwen-vl: A frontier large vision-language model with versatile abilities",
      "arxiv": "arXiv:2308.12966"
    },
    {
      "citation_id": "44",
      "title": "Glm: General language model pretraining with autoregressive blank infilling",
      "authors": [
        "Z Du",
        "Y Qian",
        "X Liu",
        "M Ding",
        "J Qiu",
        "Z Yang",
        "J Tang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "45",
      "title": "Bliva: A simple multimodal llm for better handling of text-rich visual questions",
      "authors": [
        "W Hu",
        "Y Xu",
        "Y Li",
        "W Li",
        "Z Chen",
        "Z Tu"
      ],
      "year": "2023",
      "venue": "Bliva: A simple multimodal llm for better handling of text-rich visual questions"
    },
    {
      "citation_id": "46",
      "title": "Monkey: Image resolution and text label are important things for large multi-modal models",
      "authors": [
        "Z Li"
      ],
      "year": "2024",
      "venue": "CVPR"
    },
    {
      "citation_id": "47",
      "title": "Cogvlm: Visual expert for pretrained language models",
      "authors": [
        "W Wang"
      ],
      "year": "2023",
      "venue": "Cogvlm: Visual expert for pretrained language models",
      "arxiv": "arXiv:2311.03079"
    },
    {
      "citation_id": "48",
      "title": "",
      "authors": [
        "Z Cai",
        "M Cao",
        "H Chen"
      ],
      "year": "2024",
      "venue": "",
      "arxiv": "arXiv:2403.17297"
    },
    {
      "citation_id": "49",
      "title": "Ernie 2.0: A continual pre-training framework for language understanding",
      "authors": [
        "Y Sun",
        "S Wang",
        "Y Li",
        "S Feng",
        "H Tian",
        "H Wu",
        "H Wang"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "50",
      "title": "Bliva: A simple multimodal llm for better handling of text-rich visual questions",
      "authors": [
        "W Hu"
      ],
      "year": "2023",
      "venue": "Bliva: A simple multimodal llm for better handling of text-rich visual questions",
      "arxiv": "arXiv:2308.09936"
    },
    {
      "citation_id": "51",
      "title": "Cogvlm: Visual expert for pretrained language models",
      "authors": [
        "W Wang"
      ],
      "year": "2023",
      "venue": "Cogvlm: Visual expert for pretrained language models",
      "arxiv": "arXiv:2311.03079"
    }
  ]
}