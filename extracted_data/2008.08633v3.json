{
  "paper_id": "2008.08633v3",
  "title": "Spatio-Temporal Eeg Representation Learning On Riemannian Manifold And Euclidean Space",
  "published": "2020-08-19T18:56:49Z",
  "authors": [
    "Guangyi Zhang",
    "Ali Etemad"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "We present a novel deep neural architecture for learning electroencephalogram (EEG). To learn the spatial information, our model first obtains the Riemannian mean and distance from spatial covariance matrices (SCMs) on a Riemannian manifold. We then project the spatial information onto a Euclidean space via tangent space learning. Following, two fully connected layers are used to learn the spatial information embeddings. Moreover, our proposed method learns the temporal information via differential entropy and logarithm power spectrum density features extracted from EEG signals in a Euclidean space using a deep long short-term memory network with a soft attention mechanism. To combine the spatial and temporal information, we use an effective fusion strategy, which learns attention weights applied to embeddingspecific features for decision making. We evaluate our proposed framework on four public datasets across three popular EEGrelated tasks, notably emotion recognition, vigilance estimation, and motor imagery classification, containing various types of tasks such as binary classification, multi-class classification, and regression. Our proposed architecture outperforms other methods on SEED-VIG, and approaches the state-of-the-art on the other three datasets (SEED, BCI-IV 2A, and BCI-IV 2B), showing the robustness of our framework in EEG representation learning. The source code of our paper is publicly available at https://github.com/guangyizhangbci/EEG Riemannian.",
      "page_start": 1,
      "page_end": 8
    },
    {
      "section_name": "I. Introduction",
      "text": "Brain computer interfaces (BCI) enable communication between users and computers through learning and interpreting brain activity, for example, brain signals and neuroimaging  [1] . Non-invasive technologies such as electroencephalogram (EEG), functional magnetic resonance imaging  [2] , functional near-infrared spectroscopy  [3] , and magnetoencephalography  [4] , have been widely used for BCI. Among the abovementioned technologies, EEG is one of the most popular due to portability and low cost, high temporal resolution, and the ability to provide real-time monitoring.\n\nEEG-based BCI systems have been widely used in many application areas. For example, BCI can enable users to move virtual/digital objects on screens via imagining specific movements (e.g., left hand and right hand)  [5] . BCI can also help identify users' affective states (e.g., happy, sad, or neutral) through learning neural patterns when consuming emotionally charged content  [1] . Furthermore, BCI can provide early detection of fatigue or other impairments through real-time monitoring of the brain activity  [6] .\n\nG. Zhang and A. Etemad are with the Department of Electrical and Computer Engineering & Ingenuity Labs Research Institute, Queen's University, Kingston, Canada, K7L 3N9 e-mail: {guangyi.zhang, ali.etemad}@queensu.ca Various approaches have been proposed and implemented for brain-computer interaction through learning discriminative task-relevant features from EEG signals. For example, spatial filtering has been one of the most common techniques used to explore the features that contain the optimal amount of variance with respect to different tasks  [7] . Statistical models have also been used to investigate linear relationships between EEG features and output labels  [8] ,  [9] . Numerous machine learning techniques have been implemented to model the nonlinear relationships encountered in EEG-based classification tasks  [10] . Lastly, deep learning techniques have considerably improved the performance of EEG-based BCI systems in recent years  [11] .\n\nGiven the complexity and high dimensionality of EEG, most existing solutions are often unable to learn the nonlinearities observed in high-dimensional multi-channel EEG manifolds and extracted representations. As a result, most existing EEG-related works propose pipelines customized for particular classification or regression tasks. Thus, developing frameworks for EEG representation learning that generalize well across different BCI tasks (e.g., motor imagery (MI) classification, emotion recognition, and vigilance estimation) remains challenging  [12] ,  [13] .\n\nThus far, many BCI solutions rely on spatial covariance matrices (SCMs) computed from raw multi-channel EEG to learn spatial information. Euclidean metric learning such as the distance between two SCMs and averaging of SCMs have been widely used for EEG signal classification  [14] -  [16] . Euclidean metric learning, on the other hand, suffers from poor signal classification performance due to two problems. First, EEG often suffers from the linear mixing effect due to volume conduction  [17] -  [20] , resulting in the poor estimation of the distance between two SCMs. Second, SCMs of raw EEG are symmetric positive definite (SPD). The calculation of Euclidean mean of SPD matrices may be inaccurate since the determinant of the mean value of SPD matrices can be strictly larger than the determinant of any of the SPD matrices (also known as the swelling effect)  [21] . This in turn will often result in poor classification performance  [16] .\n\nTo overcome the above-mentioned issues in BCI applications, a Riemannian metric learning method was recently applied on SCMs and enhanced the EEG classification performance  [11] ,  [22] . Riemannian distance between any two full rank SCMs, unlike the Euclidean distance, is affine-invariant  [22] . If any linear transformation is applied on EEG signals, this affine-invariance property will allow the distance between the two SCMs of EEG signals to remain unchanged. As a result, the linear mixing effect of EEG will be minimized when Riemannian distance is used for EEG classification  [23] . Furthermore, according to  [16] , no swelling effect exists during the estimation of Riemannian mean.\n\nIn this paper, we aim to to provide a generalized solution for BCI applications by effectively learning EEG representations. Our method first learns spatial information on a Riemannian manifold and temporal information in a Euclidean space individually, then fuses the learned representations effectively. Our spatial information includes the distance between two SCMs as well as the mean of SCMs, while our temporal information contains EEG features extracted and learned from signals in consecutive time periods. We overcame the following challenges during this process:\n\n(1) Spatial information learning on a Riemannian manifold. Because SPD matrices belong to a Riemannian manifold rather than a Euclidean space  [16] ,  [21] , it is important to project spatial information learned from a Riemannian manifold  [21]  onto a Euclidean space. For instance, direct spatial feature learning on a Riemannian Manifold is not appropriate because important information may be lost during the learning process with Euclidean metrics. To address this challenge, we project the spatial information from a Riemannian manifold onto a Euclidean space via tangent space learning before feeding the learnt embeddings to a deep learning architecture, ensuring that the important information is preserved  [21] . More importantly, the information transformed to Euclidean space may end up having a high dimensionality, which in turn can result in overfitting. We overcome this issue by resorting to dropout layers within our model.\n\n(2) Riemannian metric learning on SCMs. The SCMs of raw multi-channel EEG are often SPD since the signals in each channel cannot be strictly expressed as the linear combination of the others (also known as full rank)  [22] . However, the artefact suppression steps performed during EEG pre-processing may discard portions of information from multi-channel EEG  [24] . Insufficient data may also result in poor estimation of SCMs  [25] . This, in turn, may result in rank deficiency of EEG, leading the SCMs to become symmetric positive semi-definite (SPSD) which is no longer suitable for applying Riemannian metrics  [22] . To tackle this problem, we employ principle component analysis (PCA) to project the SCMs from SPSD to SPD via dimensionality reduction, enabling the Riemannian metric learning on SCMs  [22] , while capturing most of the variance.\n\n(3) Feature fusion of the Riemannian spatial and Euclidean temporal information. Combining two different representations of the input, spatial information on a Riemannian manifold and temporal information in a Euclidean space, is challenging given their different geometric structures  [16] . Moreover, each representation contributes differently to the final task at hand, requiring a learned fusion strategy for optimal performance. To solve these challenges, we apply an encoder for each information stream and effectively learn both the original and encoded information with a soft attention mechanism.\n\nWe build the solutions above in a single end-to-end deep architecture. To show the robustness of our proposed network, we evaluate the proposed architecture on four public datasets based on the following considerations: i): covering different application areas of EEG-based BCI such as emotion classification, vigilance estimation, and MI classification; ii): including problems with both binary and multi-class classification (2, 3, or 4 classes); iii): the tasks containing both continuous label prediction (regression) as well as classification; and iv): the tasks consisting of different distributions and number of EEG channels.\n\nIn summary, we make the following contributions:\n\n• We propose a novel framework for EEG representation learning based on learning spatial information on a Riemannian manifold and temporal information in a Euclidean space followed by an effective fusion strategy. • We test the proposed framework on three different EEGrelated problem domains namely emotion recognition, motor-imagery classification, and vigilance estimation, using four widely used public datasets. • Our method performs well in all the experiments, approaching the state-of-the-art in three datasets and outperforming the best results of existing works in one dataset, setting new state-of-the-art. The rest of this paper is organized as follows. In Section II, we provide an overview of related work on EEG-based BCI applications in the three different application areas namely emotion recognition, MI classification, and vigilance estimation. Section III gives a systematic description of the proposed architecture including feature extraction and learning for both spatial correlations and temporal dependencies, as well as the fusion strategy used. In Section IV, we give a description of all the datasets, implementation details, and evaluation protocols. We further discuss the results and perform ablation studies. Section V presents the summary and conclusions of this paper.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work",
      "text": "In this section, we summarize the related work on the problem domains studies in this paper. First, we group and study on emotion recognition and vigilance estimation papers together as many common techniques have been used for these two areas. Next, we provide an overview of the related work on motor-imagery classification.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Emotion Recognition And Vigilance Estimation",
      "text": "Recently, numerous EEG-based solutions have been proposed for emotion recognition. Pipelines usually consist of feature extraction followed by a classification or regression network  [10] . In these approaches, a critical step is often the selection of powerful features from noisy raw EEG signals due to the non-linear and non-stationary nature of EEG  [26] . As an example, differential entropy (DE) has been recently reported as an effective and robust feature for emotion classification and vigilance regression models  [1] ,  [6] . Successive to feature extraction, various types of algorithms have been successfully exploited for the classification/regression tasks. For instance, group sparse canonical correlation analysis (GSCCA) was proposed to model the linear relationship between extracted features (including DE) and output labels  [9] . To investigate the non-linearities in the aforementioned extracted features, several classical machine learning methods such as k-nearest neighbor (kNN)  [1] , linear regression (LR)  [1] , graph regularized sparse linear regression (GRSLR)  [27] , support vector machine (SVM)  [1] , and random forest (RF)  [28]  have been used for EEG-based emotion classification. Support vector regression (SVR) was employed in  [6]  to predict continuous values for a regression formulation of the problem. To better learn the extracted features, feed-forward artificial neural networks (ANNs) such as graph regularized extreme learning machine (GELM)  [29]  have been used to improve the performance.\n\nIn order to explore the most discriminative and task-relevant features, deep learning frameworks were applied. For instance, deep belief network (DBN) was employed to extract highlevel representations through deep hidden layers  [1] . Doublelayered neural network with subnetwork nodes (DNNSN) was adopted for predicting vigilance labels  [30] . Convolutional neural network (CNN) along with capsule attention  [31]  was adopted to learn spatiotemporal EEG information. Spatialtemporal recurrent neural network (STRNN)  [32]  and long short-term memory (LSTM) network  [33]  have been employed to learn the temporal information embedded in the EEG timeseries. A three-dimensional convolutional attention neural network was introduced to capture the dynamic interactions and internal spatial relationships among EEG signals over continuous time periods  [34] . Similarly, an end-to-end spatio-temporal demographic network was proposed to integrate both spatial and temporal EEG information using single-link hierarchical clustering  [35] . Domain adaptation network (DAN)  [36]  was recently exploited to achieve better performance with utilizing prior knowledge of data distribution in the target domain. DAN aims to reduce the effects of domain shift by leveraging the information from the source domain (train set) and adapting it to the target domain (test set). This could be achieved by aligning the distributions of the source and target domains in a shared latent space. Bi-hemispheres domain adversarial neural network (BiDANN) was proposed to minimize the domain shift between training and testing data through a discriminator  [37] . Bi-hemispheric discrepancy model (BiHDM) was proposed to improve the performance based on the architecture of an RNN and DAN through learning domain-invariant features from two brain hemispheres  [28] . A similar approach, regional to global brain-spatial-temporal neural network (R2G-STNN), explored spatial-temporal features through bidirectional LSTM (BiLSTM) and decreased the domain-shift through training a discriminator  [38] . Recently, LSTM has also been used to explore variational pathway reasoning (VPR)  [39] , and has approached state-of-the-art performance by firstly employing the RNN network to explore the between-electrode dependencies, thus encoding pathways generated from random walk. Then, it chose salient pathways with the most important pair-wise connections via scaling factors as well as pseudo-pathways. Graph neural networks (GNN) such as dynamical graph convolutional neural networks (DGCNN)  [40] , and regularized graph neural networks (RGNN)  [41]  have recently been utilized to explore the topological structure of EEG electrodes as well as inter-channel relationships by learning graph connections, approaching state-of-the-art results.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Eeg Motor Imagery Classification",
      "text": "Many works on EEG-based MI research rely on CSP as the spatial filtering method for classification  [14] . Filter bank common spatial pattern (FBCSP) which decomposes EEG to few sub-frequency bands before the use of CSP has also been frequently used for feature extraction  [14] . For example, in binary classification problems (e.g., left hand Vs. right hand), CSP filters maximize the variance of EEG trials from the lefthand class while minimizing the variance of the EEG trial from the right-hand class, through applying simultaneous diagonalization on two covariance matrices from both classes whose eigenvalues are summed to one  [14] . CSP filters have also been used in multi-class MI tasks, mainly through one-versusrest and one-versus-one strategies  [42] -  [44] . Several classifiers such as SVM, naïve Bayes (NB), RF, and linear discriminant analysis (LDA) have been reported with considerable results while using CSP or FBCSP as the feature extraction method  [42] -  [45] . For multi-class classification, extended sequential adaptive fuzzy inference system (ESAFIS) proposed in  [46]  shows better results on learning CSP features in comparison to other classifiers such as SVM. To better explore the energy features through CSP, deep learning techniques such as CNN with average pooling and the parallel combination of multilayer perceptron (MLP) and CNN has been used and achieved better results compared to SVM  [47] .\n\nLastly, end-to-end deep learning approaches have recently been adopted in MI classification. CNN-based architectures such as EEGNet have been directly implemented on raw EEG data  [48] . In another direct approach, capsule networks (CapsNet) have been applied on spectrogram of EEG data, achieving considerable results  [49] . Similar to emotion recognition, the integration of spatial and temporal EEG information plays a crucial role in MI pattern recognition. For instance, a CNN-based end-to-end framework utilized two-dimensional convolution blocks to extract both spatial and temporal features from EEG signals  [50] . Furthermore, a framework based on graph convolution was designed to leverage not only spatial and temporal features, but also their interactions for MI-EEG decoding  [51] .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iii. Proposed Architecture",
      "text": "In the following context, the notations used are described as follow: 'a' represents a scalar, 'a' represents a vector, 'A' represents a matrix, 'A' represents a differentiable manifold.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Solution Overview",
      "text": "We design a novel architecture for learning spatio-temporal EEG representations. Initially, we apply a filter bank on EEG. In order to learn the spatial information, we first compute SCMs of multiple frequency sub-bands. Then, to tackle the possible rank deficiency caused by artefact suppression, we employ PCA to ensure SCMs are in the space of SPD, thus enabling the use of affine-invariant Riemannian distance. Next, we apply the Riemannian distance on the SCMs and estimate the Riemannian mean. Following, we map the spatial information of SCMs on Riemannian manifold to the feature vectors in Euclidean space via tangent space learning, with the Riemannian mean as the reference. Lastly, we use two fully connected (FC) layers to learn the spatial information embedded in the feature vectors.\n\nIn order to obtain temporal information in Euclidean space, we employ a three-layer LSTM network with attention to learn the temporal dependencies of entropy and frequency features extracted from the same EEG frequency sub-bands as those used in the Riemannian pipeline. Next, we feed forward the temporal information learned from the attention mechanism to an FC layer to obtain latent representations.\n\nFollowing that, to learn the mutual and selective information embedded in the latent spatial and temporal representations in Euclidean space, we exploit a fusion strategy to obtain a final embedding suitable for various classification or regression tasks.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Data Pre-Processing",
      "text": "To keep consistent with the related work using the same datasets, EEG sampling rates were downsampled from 1000Hz to 200Hz for emotion and vigilance datasets while being kept unchanged at 250Hz for both MI datasets. For each of the four datasets, the EEG signals were band-pass filtered between 0.5 -70 Hz using a 5 th order Butterworth filter to lower artifacts. Then, a notch filter at 50 Hz was applied to reduce power line noise. Signal amplitudes were re-scaled to the range of [-1, 1] through min-max normalization so that the data discrepancy across different recording sessions was decreased for each subject.\n\nC. Temporal Feature Processing 1) Feature Extraction: Two types of features namely logarithm power spectrum density (PSD) and DE are extracted. PSD is defined in Eq. 1 and DE of EEG time-series X with a Gaussian distribution is shown in Eq. 2 respectively. To avoid spectral leakage, frequency domain features are extracted through short-time Fourier transform (STFT) using the periodogram method. 1-second Hanning windows are used with an overlap of 50% offering L windows (L = ⌊2 × T -1⌋, where T is the length of each EEG segment) for feature extraction.\n\n(\n\n2) LSTM Network with Attention: LSTM is a type of RNN that enables the learning of both long and short-term dependencies from sequential data (e.g., text, audio, and bio-signals) while addressing gradient exploding and vanishing problems  [52] . LSTM networks have been recently successfully implemented on EEG signals in BCI tasks and achieved notable results  [26] ,  [33] . Similarly, in our experiments, following feature extraction, concatenated task-relevant EEG features (DE and logarithm PSD) from different frequency sub-bands (total number of H) in different windows are fed into L corresponding LSTM cells (also called time-steps). Then, information in different LSTM cells (s i ) are learned through deciding which part to remember or forget through weights updated during network training  [26] . As shown in Eq. 3, h i is the generated output of the hidden states at each time-step i which are passed forward to the the LSTM cell of the next LSTM layer for higher level feature representation learning.\n\nTo improve the capability of handling temporal information, deep LSTM architectures followed by soft attention or capsule attention mechanisms have been lately implemented showing great performance on different EEG-related classification or regression tasks  [26] ,  [31] . Compared to a conventional LSTM network that only considers the last hidden state h L as the network output, a soft attention mechanism evaluates the importance of all output information ({h i } L i=1 ) from the last LSTM layer by assigning trainable attention weights α i applied on each h i as shown in Eq. 4 and 5. Thus, more taskrelevant information can be obtained by focusing on certain time-steps through optimizing attention weights. The equations are presented as follows:\n\nwhere vector v is the output of the LSTM network with attention, and W and b are the trainable parameters. Accordingly, in our architecture, we employ a soft attention mechanism following a three-layer LSTM network to help focus on the most discrepant higher level features in different time-steps.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "D. Spatial Feature Processing",
      "text": "Background: As mentioned earlier in the Introduction, SCMs of raw EEG are SPD matrices on a Riemannian manifold  [22] . Riemannian geometry is employed to better learn and manipulate the SPD matrices, in order to capture spatial information. Recent studies show that Riemannian approach achieves better performance than CSP approaches using the same classifier in BCI applications  [53] . Other Riemannian approaches using minimum distance to Riemannian mean (MDRM) and tangent space LDA (TSLDA) as classifiers consistently outperformed CSP methods in different EEG classification tasks  [54] . Local isometric embedding (LIE) was proposed based on using tangent space to better learn the features through dimensionality reduction, compared with MDRM classifier and tangent space followed by an SVM classifier  [55] . A very recent study claimed that artefactsuppression reduced the robustness of Riemannian-based approaches  [22] . Next, we briefly introduce Riemannian geometry.\n\nRiemannian Geometry: Let M be a differentiable manifold with G dimensions. As shown in Figure  1 , T C M denotes the tangent space (also called derivative) of M at C ∈ M.\n\nThe inner product of two tangent vectors (T 1 , T 2 ∈ T C M) is defined as  [56] :\n\nwhere Tr (.) is a trace operator. Also, the inner product introduces the norm of a tangent vector T as  [22] :\n\nLogarithm mapping (Log) in Eq. 9 helps project C ′ from M to T ′ in T C M. Meanwhile, Exponential mapping (Exp) in Eq. 10 is introduced to project T ′ back to C ′ as shown in following  [53] :\n\nwhere C, C ′ ∈ M, T ′ ∈ T C M, log (.), exp (.) are logarithm and exponential operations applied on a matrix. Riemannian distance (also called geodesic distance) is a very important metric representing the distance of the shortest path between C and C ′ (shown as the curve in Figure  1 ) on manifold M. The geodesic distance (δ R ) is equivalent to the length of its tangent vector  [56] ,  [57] , expressed as follows:\n\nIn the context of this work, we denote\n\nis the space of SPSD matrices, where rank(M) is the rank of a matrix, and\n\nOur Method: To learn spatial information embedded in multi-channel EEG, we compute SCMs on filtered signals in each frequency sub-band. Suppose {X i } P i=1 ∈ R N ×T , where i denotes i th segment, P is the EEG segment number, N is the EEG channel number, and T represents the number of data samples per segment. The SCMs can be estimated as\n\ni , which may be in S R after artefact suppression. PCA is then employed to project SCMs from S R to S + R in order to enable the affine-invariant property during geodesic distance calculation. At last, we obtain the spatial information (geodesic distance) of SCMs in the Riemannian manifold as feature vectors in Euclidean space via tangent space learning based on the chosen reference matrix as described in the following text.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "1) Dimensionality Reduction On {C",
      "text": "N , artifact suppression may destroy some important information  [24]  that could result in the rank deficiency of EEG data  [22] , leading for C i to belong to S R . Wasserstein distance has been applied on C i in S R  [22] . However, it lacks affine-invariance in dealing with the linear mixing effect in multi-channel EEG recordings  [22] . Therefore, we perform dimensionality reduction to project C i to S + R , enabling the use of affine-invariant Riemannian distance. First, we use PCA to estimate spatial filter W  [22] . Specifically, we sort the eigenvector matrix V based on descending eigenvalues, and choose V containing only top R eigenvalues of the averaged {C i } P i=1 (Eq. 14) as the spatial filter W, thus maximizing the variance  [22] . Then, we apply the spatial filter\n\nR after dimensionality reduction, we apply Riemannian distance on C i where the affine-invariant property of distance is illustrated as following:\n\nwhere C, C i ∈ R R×R , WW -1 = I, and W ⊤ X ′ i is the linear transform of the EEG  [22] , which was proven in  [57] . The affine-invariance property (Eq. 12) enables the geodesic distance between C and C i in M to be invariant when linear transforms are applied to EEG (e.g., linear mixing effect). Unlike many approaches, we do not apply Log-Euclidean distance on C i in S + R due to the lack of the affine-invariance property  [16] ,  [58] . Next, instead of conventional Riemannian approach that apply geodesic distances directly for classification (e.g., MDRM)  [59] , we preserve the geodesic information (||T i || C ) of {C i } P i=1 as feature vectors in Euclidean space (Eq. 11) for further higher level feature learning. To achieve this, we first carefully select the reference matrix so that the spatial information on Riemannian manifold represented by geodesic distance between {C i } P i=1 and the reference matrix in S + R can be projected onto the same tangent space, in order to best capture geodesic information of\n\nWe denote C ref ∈ M as the reference matrix for {C i } P i=1 during tangent space (T C ref M) learning. In the recent studies, the approaches using Riemannian mean (C R ) as the reference (C ref ) during the tangent space learning have outperformed approaches that used other references such as Identity matrix (I) and Euclidean mean (C E )  [53] ,  [59] . The Euclidean distance, Euclidean mean, and Riemannian mean equations are presented as following:\n\nwhere ||.|| F is the Frobenius norm of a matrix.\n\nC R = arg min  Subsequently, spatial and temporal information is learned using a feature fusion method, which enables final decision-making in various tasks. Since there are no closed-form solutions for computing Riemannian mean, we implement the gradient descent algorithm (Algorithm 1) presented in  [60]  for its efficient computation  [59] . The algorithm implements an iterative procedure to approximate Riemannian mean through minimizing the arithmetic mean of the tangent vectors J. In the first step, we initialize C ref using arithmetic mean. Then we use Logarithm mapping to project C i to the tangent space T C M and compute J. Next, we project J back to M to update C ref .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Algorithm 1 Riemannian Mean Algorithm",
      "text": "The algorithm terminates if either Frobenius norm of J is less than the tolerance value (ϵ = 10 -9 ) or the algorithm reaches maximum iteration of 50 times. Lastly, we use Riemannian mean as C ref to project geodesic information of {C i } P i=1 onto the same tangent space  [56] .\n\n4) Tangent Space Learning for {C i } P i=1 in S + R : As mentioned in the previous section, we obtain the geodesic information\n\nR based on Logarithm mapping (Eq. 9) using the estimated Riemannian mean (Algorithm 1) as C ref . Then, in order to obtain the feature vectors containing geodesic information in M for classification or regression purposes, we require a mapping  [22] . From Eq. 8 and 9, we have:\n\nwhere Vect (.) is the vectorization operator and\n\n. Therefore, from Eq. 16, we have\n\n5) Vectorization: To further present the geodesic information of C i in M as spatial feature vectors, we denote the spatial information mapping as\n\ni=1 are in a small region on M as stated in  [22] ,  [59] , we have:\n\nwhere C ref is the Riemannian mean of {C i } P i=1 . Eq. 17 demonstrates that the geodesic distance between C i and C j can be approximated by the geodesic distance between {C i } P i=1 and Riemannian mean. Therefore, the geodesic information obtained through the tangent space learning using Riemannian mean as reference are able to represent geodesic information for {C i } P i=1 . Important information of Φ C ref (C i ) are completely determined by upper triangular components of S i since it is symmetric. Thus, we use the half-vectorization Upper(S i ) instead of full-vectorization",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "E. Fusion Strategy",
      "text": "The strategy for the fusion of spatial and temporal information plays an essential role in dealing with multimodal or multi-learning approaches of one modality in order to perform classification/regression. Attention mechanisms have been successfully implemented for refining fusion weights applied to different modalities. For instance, for EEG and electrooculogram (EOG) representation learning, a CapsNet was used in  [31]  as an attention mechanism for fusion.\n\nIn the context of this problem, various tasks rely differently on spatial and temporal information. Therefore, a fusion strategy presented as Figure  2  is adopted. This strategy is inspired by  [61]  where a hybrid attention-based multimodal architecture was proposed to learn acoustic and textual features and achieved the state-of-the-art performance on several spoken language classification tasks  [61] . In our architecture, we first use encoders to learn embedding-specific features. Then we employ soft attention to learn the weight (α) applied on each embedding-specific feature. Next, we compute the new weighted embedding by multiplying the weight score with the original individual learning embedding. The weighted score of (1 + α) is adopted to apply the learned weight in order to maintain the original characteristic  [61] . Finally, we perform decision-level fusion on the concatenation of the two new embeddings using an FC layer equipped with different activation functions with respect to discrepant tasks, as illustrated in Figures 2.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Iv. Experiments A. Datasets",
      "text": "In the following sections we describe the four datasets used in this study. The EEG data in these datasets have all been recorded with the international 10 -20 system.\n\n1) SEED: The SEED dataset has been collected as described in  [1]  to perform three emotion classification tasks (positive, neutral, and negative). 15 film clips were chosen as stimuli in the experiments. 15 subjects (8 females and 7 males, with an average age of 23.3 ± 2.4) participated in the experiments. Each subject performed experiments in two runs of experiment with 15 sessions in each run, yielding a total of 30 sessions. Each session includes four stages: 5 seconds notice before the movie starts, around 4 minutes of movie watching, 45 seconds of self-assessment, and 15 seconds of rest. 62 EEG channels were recorded at a sampling frequency of 1000Hz. EEG signals are split into EEG segments of T = 8 seconds with no overlap, as presented in Table  I .\n\n2) SEED-VIG: The SEED-VIG dataset has been collected by  [6]  to estimate driver vigilance. A total of 23 subjects (12 female and 11 male, with an average age of 23.3 ± 1.4) participated in the experiment. 17 channels EEG were collected at sampling frequency of 1000Hz. The duration of each experiment was around 2 hours, yielding 885 EEG trials in total. subjects were asked to drive the simulated car in a virtual environment. Most experiments were performed after lunch so that fatigue during simulated driving could be easily induced  [6] . The vigilance estimation annotation used a metric called PERCLOS  [6] , which was measured using eye-tracking glasses. Similar to the SEED dataset, EEG signals are split into EEG segments of 8 seconds, with no overlap, as shown in Table  I .\n\n3) BCI-IV 2a: The BCI-IV 2a dataset has been collected by  [62]  to classify four MI tasks (left hand, right hand, tongue and both feet). Each of 9 subjects (4 female and 5 male, with an average age of 23.1 ± 2.6) participated experiments in two session on two different days. Each session contain 72 trials for each of the four classes, yielding 288 trials in total. All sessions contain data without feedback. In these sessions, each trial length is 7.5 seconds including fixation, visual cue and MI period, and rest. 22 EEG channels were recorded at the sampling frequency of 250Hz during the experiment. We use the interval of [2.0 -6.0s] in each trial (T = 4s), as presented in Table  I .",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "4) Bci-Iv 2B:",
      "text": "The BCI-IV 2b dataset has been collected by  [5]  to perform binary MI classification (left hand versus right hand). Each of the 9 subjects (4 female and 5 male, with an average age of 24.2 ± 3.7) participated in five sessions of the experiment. The first two sessions were conducted without feedback while rest three sessions were conducted with feedback. Each of the first two sessions contain 120 trials and each of last three sessions contain 160 trials. In the sessions containing data without feedback, each trial length is 8 seconds including fixation, visual cue, MI period, and rest. In the sessions containing data with feedback, each trial length is 8 seconds including visual cue, feedback period, and rest. 3 EEG channels were recorded at a sampling frequency of 250Hz. We use the interval of  [3.4 -7.4s ] in each trial (T = 4s), as presented in Table  I .",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "B. Implementation Details",
      "text": "1) Filter Bank: A filter bank, consisting of 5 th order Butterworth bandpass filters, was used to decompose EEG signals into different frequency sub-bands. Following that, EEG features were extracted from each of the frequency subbands to capture information containing different functional characteristics. For the SEED dataset, DE and logarithm PSD features were calculated on the STFT outputs from five important EEG rhythms, notably delta (1 -3Hz), theta (4 -7Hz), alpha (8 -13Hz), beta (14 -30Hz), and gamma (31 -50Hz) bands  [1] . For SEED-VIG, DE and logarithm PSD features were determined on the STFT outputs in the range of (0.5 -50.5 Hz) with a 2 Hz resolution  [6] , yielding a total of 50 frequency sub-bands. For BCI-IV 2a and BCI-IV 2b datasets, no such standard feature extraction approaches were suggested. We therefore use H=25 since our experiments showed that higher resolution bands were beneficial, as shown in Table  I .\n\n2) Temporal Information Stream: The total number of DE and logarithm PSD features extracted from each of the Hanning windows is 2×H ×N , as presented in Table  I . These extracted features are then fed to our attention based LSTM network. Dropout rates of 0.2, 0.1, and 0.1 are applied after each of three LSTM layers with 256 hidden units respectively to reduce overfitting in BCI-IV 2a dataset, as shown in Figure  2 . For the rest of datasets, batch normalization (BatchNorm) is applied after each LSTM layer to accelerate the training phase. BatchNorm layers reduce the covariance shift of LSTM output values in each batch, thus increasing model stability  [63] . Then, LeakyReLu (slope of 0.3) is adopted to enable the activation of hidden neurons for the BatchNorm layer's negative output values  [64] , as shown in Figure  2 . An FC layer of 64 units is used for learning temporal information embedding before fusion.\n\n3) Spatial Information Stream: Total number of spatial information features in concatenated feature vectors Φ C ref (C i ) from all the frequency sub-bands is H × R × (R + 1)/2, as presented in Table  I . Following the spatial information embedding, a dropout rate of 0.5 was applied on each of two FC layers consisting of 512 and 64 hidden units to prevent overfitting, as shown in Figure  2 .\n\n4) Fusion Strategy: As shown in Figure  2 , each of the two encoders used in the feature fusion block contains an FC layer of 32 units followed by a single-unit FC layer. The two encoders learn the temporal-and spatial-specific features respectively, as mentioned earlier in Section III-E. Lastly, successive to the employed soft-attention mechanism, an FC layer with 128 units is used to learn the fused and weighted embeddings for decision making.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "5) Loss Function And Training:",
      "text": "The loss function of the model and the activation function of the output layer (the final FC in the model as shown in Figure  2  have been chosen with respect to different task. Since the different datasets involve different classification or regression tasks, different activation function were selected accordingly. Particularly, softmax, sigmoid, softmax and sigmoid were used for SEED, SEED-VIG, BCI-IV 2a, and BCI-IV 2b dataset respectively. Moreover, loss functions were selected with consideration of the different tasks and activation functions. Specifically, categorical crossentropy, mean squared error, categorical cross-entropy and binary cross-entropy were used for the 4 datasets respectively. Adam optimizer  [68]  with default learning rate is used to help minimize the loss. We use 200 epochs and batch size of 32 to train our network. The pipeline is implemented using TensorFlow on a pair of NVIDIA RTX 2080Ti GPUs.\n\nFor hyper-parameter selection, we use the training set of the cross-validation scheme (which we use for training and validation). Specifically, we used 20% of training data as validation set for tuning hyperparameter R. Regarding other hyperparameters such as learning rate, batch size and number of LSTM layers, we chose to rely on standards used in existing methods of related work, instead of further tuning.. Moreover, for the spatial information stream, the Riemannian mean of the test set is chosen only with the trials in each batch rather than with the entire set.\n\nWe make our source code publicly available at https://github.com/guangyizhangbci/EEG Riemannian.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "C. Evaluation Protocol",
      "text": "To evaluate our architecture, we adopt the same subjectdependent protocols that have been used in the original papers accompanying the datasets. In the following sections we describe the evaluation protocol details and metrics in detail for each dataset.\n\n1) SEED: As in  [1] , we use the pre-defined 9 sessions as training data and the remaining 6 sessions as testing data in each experiment run, yielding 248 and 170 EEG trials for training and testing, respectively.\n\n2) SEED-VIG: For this dataset, we use 5-fold crossvalidation to split the data into training and testing sets, as in  [6] . Two frequently used evaluation metrics for regression, notably root mean squared error (RMSE) and Pearson correlation coefficient (PCC) have been used  [6] .\n\n3) BCI-IV 2a: As in  [62] , we use the pre-defined training and testing data to evaluate our model, where each contains 288 EEG trials. Both accuracy (Acc.) and kappa values (K = P0-Pe 1-Pe ) are used where P 0 is the observed agreement ratio (identical to accuracy), and P e is the expected agreement ratio while labels are assigned randomly. This metric aims to evaluate the agreement between two label vectors.\n\n4) BCI-IV 2b: As per  [5] , we use the pre-defined training data (first three sessions) with a total of 400 trials and testing data (last two sessions) with a total of 320 trials to evaluate our model. We also use the same evaluation metrics as in the BCI-IV 2a dataset.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "D. Results And Comparison",
      "text": "1) SEED: Table  II  shows the performance comparison between our model and other related works on the SEED dataset. We compare our results to the existing methods that include statistical models, machine learning method, and    ). In  [40] ,  [41] , DGCNN and RGNN learned the spatial information through discovering the topological structure of EEG channels using graphs, achieving accuracies of 90.40% and 94.24%, respectively. In  [32] ,  [38] , STRNN and R2G-STNN utilized both spatial and temporal information with RNN or LSTM to provide performances of 89.50% and 93.38%, respectively. In  [28] ,  [37] , BiDANN and BiHDM employed DAN to utilize the prior distribution information of the target domain, achieving very high performances of 92.38% and 93.12% respectively. Recently, in  [65] , a graph-based multi-task self-supervised (GMSS) learning method learned more general EEG graph representation by integrating selfsupervised tasks and contrastive learning tasks, achieving best accuracy of 96.48%. Our model fully explores the spatial and temporal information, approaching the state-of-the-art result.\n\n2) SEED-VIG: The comparison of our model and other existing work on the SEED-VIG dataset is shown in Table  III . In  [6]  a baseline SVR model obtained an RMSE of 0.1327 and a PCC of 0.7001. In  [30] , DNNSN used subnetwork nodes to process the DE features, achieving an RMSE of 0.1175 and a PCC of 0.7201. In  [66] , GELM outperformed SVR with an RMSE of 0.1037 and a PCC of 0.7013. In  [33] , temporal dependency information learned by LSTM provided a considerable results with an RMSE of 0.0927 and a PCC of 0.8237. In  [67] , a transformer encoder was used to learn the temporal information, obtaining an RMSE of 0.0870 and a PCC of 0.8970. Our model achieves considerably superior results with an RMSE of 0.0348 and a PCC of 0.9890, setting a new state-of-the-art for this dataset.\n\n3) BCI-IV 2a: We compare the performance of our architecture on this dataset to other methods as presented in Table  IV . In  [14] ,  [42] ,  [43] , pipelines consisting of CSP or FBCSP as feature extractors, followed by machine learning technique (e.g., NB, LDA) have been implemented. In  [69] , a filter method based on multivariate empirical mode decomposition (MEMD) was employed. In  [43] , CSP followed by LDA achieves a kappa value of 0.6156. In  [70] , the use of a CNN applied to SCMs outperformed the aforementioned pipelines with a kappa of 0.6594 and accuracy of 0.7446. In  [55] , pipelines used the LIE approach to extract spatial features, followed by an SVM classifier. Pipelines using CSP as feature extractor and CNN and MLP as classifier achieved an accuracy of 70.60%. In  [71] , a method using CSP for feature extraction followed by an ensemble of classifiers achieved an accuracy of 85.40%. In  [72] , an attention-based CNN framework has been applied on raw EEG signals, obtaining a kappa of 0.8100 and accuracy of 85.40%, achieving state-of-the-art results. Our model achieves a kappa of 0.6734 and an accuracy of 75.51%. For fair comparison, we do not consider the references that have employed different evaluation protocols on this dataset (e.g.,  [48] ,  [54] ). Moreover, we do not compare our results to references that have performed binary classification (e.g.  [53] ).\n\n4) BCI-IV 2b: Table  V  presents the results of our method and related works using this dataset. In  [42] , FBCSP followed by NB as the classifier shows very good performance with a kappa of 0.6000, obtaining the first rank in the BCI competition. A very similar method, using an RF instead of the NB achieves very similar results in  [45] . In  [49] , deep learning techniques such as CNN and CapsNet have been employed to learn the discriminative information from spectrograms instead of SCMs, achieving accuracies of 74.99% and 77.00%, respectively. In  [46] , a method using CSP for feature extraction and ESAFIS for classification obtained a kappa of 0.6174 and an accuracy of 80.90%. In  [73] , a CNN-based transfer learning architecture achieved the best result with an accuracy of 86.99%.\n\nOur framework achieved considerably better results with a kappa of 0.6720 and an accuracy of 83.60%, approaching state-of-the-art. Similar to other BCI-IV 2a, references that use different evaluation protocols (e.g.,  [74] -  [76] ) are not listed in this table.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "E. Discussion",
      "text": "Table VI presents the summary of the performance of our proposed model compared to the state-of-the-art in the four datasets. We also show the performance of our individual learning streams with the same parameter settings as used in our network. We observe that the spatial information stream performs better in both MI datasets while the temporal information stream performs superior for emotion recognition and vigilance estimation datasets. This demonstrates the necessity to exploit both spatial and temporal information from EEG, in order to develop a generalized model suitable for different BCI applications (e.g., emotion recognition, vigilance estimation, and MI classification). Moreover, we observe that our model achieves much better results than both individual learning streams even when the difference among the performance of the two streams is very small as with the BCI-IV 2b dataset. Interestingly, the performance of our model is only slightly better than each individual stream when the difference between them is large, as seen with the SEED-VIG and BCI-IV 2a datasets. This indicates that the two streams are likely to contain more contradictory information, resulting in difficulty for the model to learn a strong relationship between learned representations and outputs. Overall, the results in Table  VI  show the performance of each stream is dataset-dependent and demonstrate the importance of combining the two streams.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "F. Ablation Experiments And Analysis",
      "text": "We conduct numerous ablation and analysis studies to evaluate the impact of different components of our framework on the performance.\n\n1) Impact of LSTM layers on Temporal Information Learning: We evaluate the depth of the LSTM network and the performance of the LSTM compared with BiLSTM on learning temporal information. As shown in Figure  3 , LSTM with three layers consistently has the best performance among LSTMs with different numbers of layers. Also, the LSTM performs better than Bi-LSTM with the same number of layers for most datasets.\n\n2) Importance of Riemannian Approach on Spatial Information Learning: To show the importance of the Riemannian approach on spatial information learning, we compare our solution with a Euclidean approach that directly employs vectorization followed by FC layers on spatial covariance matrices ({C i } P i=1 ).  [53] . We also implement other deep learning techniques such as CNN and CapsNet  [77]  directly on {C i } P i=1 without vectorization for comparison. Table  VII  shows the comparison of these different approaches applied on SCMs for spatial information learning. Our Riemannian approach consistently outperforms other approaches for all 4 datasets, addressing the first challenge of spatial information learning on the Riemannian manifold which was mentioned earlier in the Introduction section.\n\nNext, we explore the learned representation space using uniform manifold approximation and projection (UMAP)  [78]  to better understand the impact of our Riemannian approach. Figure  5  shows the comparison between the feature spaces using our Riemannian approach Φ C ref ({C i } P i=1 ) versus a direct vectorization of spatial covariance matrices without Riemannian Vect({C i } P i=1 ) for a sample subject. In SEED, SEED-VIG, and BCI-IV 2a datasets, the information in Φ C ref with the Riemannian approach are clearly more separable than the information in Vect({C i } P i=1 ) without Riemannian. In BCI-IV 2b, the difference in separability is very small. This is likely due to the limited number of channels (N = 3). Our observations are consistent with the comparison results     shown in Table  VII . Overall, our proposed architecture results in superior separability in the feature space.\n\n3) Impact of Dimensionality Reduction on Spatial Information Learning: We evaluate the effect of dimensionality reduction by observing the performance of spatial information learning with different R values representing the full rank of the covariance matrix. To this end, we perform a grid search on R in the range of [1, N -1]. Figure  4  shows the effect of dimensionality reduction with different R values on spatial information learning, based on different evaluation metrics for the 4 datasets. We observe that the best performances are achieved at the rank R of 48, 11, 18 for SEED, SEED-VIG, and BCI-IV 2a datasets, respectively. For the BCI-IV 2b dataset, only 3 EEG channels are available, therefore dimensionality reduction is not necessary, hence, the best performance has been expectedly achieved at N = 3. Overall, the results in Figure  4  demonstrate the importance of projecting the SCMs from SPSD to SPD via dimensionality reduction. It also shows that our spatial information approach addressed the second challenge of Riemannian metric learning on SCMs which was mentioned earlier in the Introduction section.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "4) Impact Of Fusion Strategy On Both Learning Embeddings:",
      "text": "We employ different feature fusion techniques such as naive concatenation, soft attention mechanisms, and our fusion strategy. The results in Table VIII show that our method marginally but consistently outperforms other fusion strategies for all the datasets, addressing the third challenge of feature fusion of the Riemannian spatial and Euclidean temporal information which was mentioned in the Introduction section.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "5) Effect Of Our Model On Low-Performing Subjects:",
      "text": "We also investigate the impact of our model on subjects with lower-than-average performance (also called low-performing subjects). Figure  6  shows the performance of our model compared to individual spatial and temporal information streams 2) by 10.04% and 7.53%, respectively. Overall, our model demonstrates its effectiveness by enhancing the performance of subjects with low performance based on individual spatial and temporal information streams.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "V. Conclusions",
      "text": "In this paper, we propose a novel deep architecture to learn EEG using spatio-temporal information on a Riemannian manifold as well as a Euclidean space. Spatial information is efficiently learned from spatial covariance matrices of EEG signals through our Riemannian approach. Moreover, temporal information is obtained by extracting features from EEG signals in consecutive time periods and learning them using our deep LSTM network followed by an attention mechanism. Our fusion strategy exploits the complementary information from both information streams. We test our framework with four public datasets with various types of tasks in the three popular EEG fields of emotion recognition, vigilance estimation, and MI classification. Our results demonstrate the robustness of our model in both fields on binary classification, multi-class classification, and even regression. We set new state-of-the-art result on SEED-VIG, while approaching the existing state-ofthe-art for emotion recognition on the SEED dataset and for MI classification on BCI-IV 2a and BCI-IV 2b datasets.",
      "page_start": 12,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The Concept of Riemannian Manifold. It demonstrates",
      "page": 4
    },
    {
      "caption": "Figure 1: , TCM denotes",
      "page": 5
    },
    {
      "caption": "Figure 2: The overview of the experiment work-flow is presented. The raw, multi-channel EEG signals undergo preprocessing",
      "page": 6
    },
    {
      "caption": "Figure 2: To this end,",
      "page": 7
    },
    {
      "caption": "Figure 2: is adopted. This strategy is",
      "page": 7
    },
    {
      "caption": "Figure 2: For the rest of datasets, batch normalization (BatchNorm)",
      "page": 8
    },
    {
      "caption": "Figure 2: 4) Fusion Strategy: As shown in Figure 2, each of the",
      "page": 8
    },
    {
      "caption": "Figure 2: have been chosen with",
      "page": 8
    },
    {
      "caption": "Figure 3: , LSTM with three",
      "page": 10
    },
    {
      "caption": "Figure 5: shows the comparison between the feature spaces",
      "page": 10
    },
    {
      "caption": "Figure 3: Impact of LSTM layers on the temporal stream.",
      "page": 11
    },
    {
      "caption": "Figure 4: Effect of dimensionality reduction on the spatial stream.",
      "page": 11
    },
    {
      "caption": "Figure 4: shows the effect of",
      "page": 11
    },
    {
      "caption": "Figure 4: demonstrate the importance of projecting",
      "page": 11
    },
    {
      "caption": "Figure 6: shows the performance of our model com-",
      "page": 11
    },
    {
      "caption": "Figure 5: Comparison between spatial information vectors without Riemannian (1st row) and with Riemannian (2nd row) using",
      "page": 12
    },
    {
      "caption": "Figure 6: The performance of our proposed network in compar-",
      "page": 13
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Ours\nSpatial": "Temporal\nAverage (spatial)\nAverage (temporal)"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Ours\nSpatial\nTemporal\nAverage (spatial)": "Average (temporal)"
        }
      ],
      "page": 13
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Investigating critical frequency bands and channels for eeg-based emotion recognition with deep neural networks",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2009",
      "venue": "IEEE Transactions on Autonomous Mental Development"
    },
    {
      "citation_id": "2",
      "title": "Functional neuroanatomy of emotion: a meta-analysis of emotion activation studies in pet and fmri",
      "authors": [
        "K Phan",
        "T Wager",
        "S Taylor",
        "I Liberzon"
      ],
      "year": "2002",
      "venue": "NeuroImage"
    },
    {
      "citation_id": "3",
      "title": "Auditory cortex activation is modulated by emotion: a functional near-infrared spectroscopy (fnirs) study",
      "authors": [
        "M Plichta",
        "A Gerdes",
        "G Alpers",
        "W Harnisch",
        "S Brill",
        "M Wieser",
        "A Fallgatter"
      ],
      "year": "2011",
      "venue": "NeuroImage"
    },
    {
      "citation_id": "4",
      "title": "A brain computer interface with online feedback based on magnetoencephalography",
      "authors": [
        "T Lal",
        "M Schröder",
        "N Hill",
        "H Preissl",
        "T Hinterberger",
        "J Mellinger",
        "M Bogdan",
        "W Rosenstiel",
        "T Hofmann",
        "N Birbaumer"
      ],
      "year": "2005",
      "venue": "Proceedings of the 22nd International Conference on Machine Learning"
    },
    {
      "citation_id": "5",
      "title": "Bci competition 2008-graz data set b",
      "authors": [
        "R Leeb",
        "C Brunner",
        "G Müller-Putz",
        "A Schlögl",
        "G Pfurtscheller"
      ],
      "year": "2008",
      "venue": "Bci competition 2008-graz data set b"
    },
    {
      "citation_id": "6",
      "title": "A multimodal approach to estimating vigilance using eeg and forehead eog",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2009",
      "venue": "Journal of neural engineering"
    },
    {
      "citation_id": "7",
      "title": "Optimal spatial filtering of single trial eeg during imagined hand movement",
      "authors": [
        "H Ramoser",
        "J Muller-Gerking",
        "G Pfurtscheller"
      ],
      "year": "2000",
      "venue": "IEEE Transactions on Rehabilitation Engineering"
    },
    {
      "citation_id": "8",
      "title": "Canonical correlation analysis",
      "authors": [
        "B Thompson"
      ],
      "year": "2005",
      "venue": "Encyclopedia of Statistics in Behavioral Science"
    },
    {
      "citation_id": "9",
      "title": "Multichannel eeg-based emotion recognition via group sparse canonical correlation analysis",
      "authors": [
        "W Zheng"
      ],
      "year": "2009",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "10",
      "title": "A review of classification algorithms for eeg-based brain-computer interfaces",
      "authors": [
        "F Lotte",
        "M Congedo",
        "A Lécuyer",
        "F Lamarche",
        "B Arnaldi"
      ],
      "year": "2007",
      "venue": "Journal of Neural Engineering"
    },
    {
      "citation_id": "11",
      "title": "A review of classification algorithms for eegbased brain-computer interfaces: a 10 year update",
      "authors": [
        "F Lotte",
        "L Bougrain",
        "A Cichocki",
        "M Clerc",
        "M Congedo",
        "A Rakotomamonjy",
        "F Yger"
      ],
      "year": "2018",
      "venue": "Journal of Neural Engineering"
    },
    {
      "citation_id": "12",
      "title": "Multi-scale neural network for eeg representation learning in bci",
      "authors": [
        "W Ko",
        "E Jeon",
        "S Jeong",
        "H.-I Suk"
      ],
      "year": "2021",
      "venue": "IEEE Computational Intelligence Magazine"
    },
    {
      "citation_id": "13",
      "title": "Eegnet: a compact convolutional neural network for eeg-based brain-computer interfaces",
      "authors": [
        "V Lawhern",
        "A Solon",
        "N Waytowich",
        "S Gordon",
        "C Hung",
        "B Lance"
      ],
      "year": "2018",
      "venue": "Journal of neural engineering"
    },
    {
      "citation_id": "14",
      "title": "Filter bank common spatial pattern algorithm on bci competition iv datasets 2a and 2b",
      "authors": [
        "K Ang",
        "Z Chin",
        "C Wang",
        "C Guan",
        "H Zhang"
      ],
      "year": "2009",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "15",
      "title": "Averaging covariance matrices for eeg signal classification based on the csp: an empirical study",
      "authors": [
        "F Yger",
        "F Lotte",
        "M Sugiyama"
      ],
      "year": "2015",
      "venue": "2015 23rd European Signal Processing Conference (EUSIPCO)"
    },
    {
      "citation_id": "16",
      "title": "From euclidean to riemannian means: Information geometry for ssvep classification",
      "authors": [
        "E Kalunga",
        "S Chevallier",
        "Q Barthélemy",
        "K Djouani",
        "Y Hamam",
        "E Monacelli"
      ],
      "year": "2015",
      "venue": "International Conference on Geometric Science of Information"
    },
    {
      "citation_id": "17",
      "title": "Volume conduction effects in eeg and meg",
      "authors": [
        "S Van Den Broek",
        "F Reinders",
        "M Donderwinkel",
        "M Peters"
      ],
      "year": "1998",
      "venue": "Electroencephalography and Clinical Neurophysiology"
    },
    {
      "citation_id": "18",
      "title": "Quantification of the effects of volume conduction on the eeg/meg connectivity estimates: an index of sensitivity to brain interactions",
      "authors": [
        "A Khadem",
        "G.-A Hossein-Zadeh"
      ],
      "year": "2014",
      "venue": "Physiological measurement"
    },
    {
      "citation_id": "19",
      "title": "Ghost interactions in meg/eeg source space: A note of caution on inter-areal coupling measures",
      "authors": [
        "J Palva",
        "S Wang",
        "S Palva",
        "A Zhigalov",
        "S Monto",
        "M Brookes",
        "J.-M Schoffelen",
        "K Jerbi"
      ],
      "year": "2018",
      "venue": "Neuroimage"
    },
    {
      "citation_id": "20",
      "title": "Eeg functional connectivity metrics wpli and wsmi account for distinct types of brain functional interactions",
      "authors": [
        "L Imperatori",
        "M Betta",
        "L Cecchetti",
        "A Canales-Johnson",
        "E Ricciardi",
        "F Siclari",
        "P Pietrini",
        "S Chennu",
        "G Bernardi"
      ],
      "year": "2019",
      "venue": "Scientific reports"
    },
    {
      "citation_id": "21",
      "title": "Geometric means in a novel vector space structure on symmetric positive-definite matrices",
      "authors": [
        "V Arsigny",
        "P Fillard",
        "X Pennec",
        "N Ayache"
      ],
      "year": "2007",
      "venue": "SIAM Journal on Matrix Analysis and Applications"
    },
    {
      "citation_id": "22",
      "title": "Manifold-regression to predict from meg/eeg brain signals without source modeling",
      "authors": [
        "D Sabbagh",
        "P Ablin",
        "G Varoquaux",
        "A Gramfort",
        "D Engemann"
      ],
      "year": "2006",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "23",
      "title": "A riemannian framework for tensor computing",
      "authors": [
        "X Pennec",
        "P Fillard",
        "N Ayache"
      ],
      "year": "2006",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "24",
      "title": "Signal-space projection method for separating meg or eeg into components",
      "authors": [
        "M Uusitalo",
        "R Ilmoniemi"
      ],
      "year": "1997",
      "venue": "Medical and Biological Engineering and Computing"
    },
    {
      "citation_id": "25",
      "title": "Automated model selection in covariance estimation and spatial whitening of meg and eeg signals",
      "authors": [
        "D Engemann",
        "A Gramfort"
      ],
      "year": "2015",
      "venue": "NeuroImage"
    },
    {
      "citation_id": "26",
      "title": "Classification of hand movements from eeg using a deep attention-based lstm network",
      "authors": [
        "G Zhang",
        "V Davoodnia",
        "A Sepas-Moghaddam",
        "Y Zhang",
        "A Etemad"
      ],
      "year": "2019",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "27",
      "title": "Eeg emotion recognition based on graph regularized sparse linear regression",
      "authors": [
        "Y Li",
        "W Zheng",
        "Z Cui",
        "Y Zong",
        "S Ge"
      ],
      "year": "2019",
      "venue": "Neural Processing Letters"
    },
    {
      "citation_id": "28",
      "title": "A novel bi-hemispheric discrepancy model for eeg emotion recognition",
      "authors": [
        "Y Li",
        "W Zheng",
        "L Wang",
        "Y Zong",
        "L Qi",
        "Z Cui",
        "T Zhang",
        "T Song"
      ],
      "year": "2019",
      "venue": "A novel bi-hemispheric discrepancy model for eeg emotion recognition",
      "arxiv": "arXiv:1906.01704"
    },
    {
      "citation_id": "29",
      "title": "Identifying stable patterns over time for emotion recognition from eeg",
      "authors": [
        "W.-L Zheng",
        "J.-Y Zhu",
        "B.-L Lu"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "30",
      "title": "A regression method with subnetwork neurons for vigilance estimation using eog and eeg",
      "authors": [
        "W Wu",
        "Q Wu",
        "W Sun",
        "Y Yang",
        "X Yuan",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "31",
      "title": "Capsule attention for multimodal eeg-eog representation learning with application to driver vigilance estimation",
      "authors": [
        "G Zhang",
        "A Etemad"
      ],
      "year": "2007",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "32",
      "title": "Spatial-temporal recurrent neural network for emotion recognition",
      "authors": [
        "T Zhang",
        "W Zheng",
        "Z Cui",
        "Y Zong",
        "Y Li"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Cybernetics"
    },
    {
      "citation_id": "33",
      "title": "Continuous vigilance estimation using lstm neural networks",
      "authors": [
        "N Zhang",
        "W.-L Zheng",
        "W Liu",
        "B.-L Lu"
      ],
      "year": "2016",
      "venue": "International Conference on Neural Information Processing"
    },
    {
      "citation_id": "34",
      "title": "3dcann: A spatio-temporal convolution attention neural network for eeg emotion recognition",
      "authors": [
        "S Liu",
        "X Wang",
        "L Zhao",
        "B Li",
        "W Hu",
        "J Yu",
        "Y.-D Zhang"
      ],
      "year": "2021",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "35",
      "title": "Sstd: a novel spatiotemporal demographic network for eeg-based emotion recognition",
      "authors": [
        "R Li",
        "C Ren",
        "C Li",
        "N Zhao",
        "D Lu",
        "X Zhang"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Computational Social Systems"
    },
    {
      "citation_id": "36",
      "title": "Cross-subject emotion recognition using deep adaptation networks",
      "authors": [
        "H Li",
        "Y.-M Jin",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2018",
      "venue": "International Conference on Neural Information Processing"
    },
    {
      "citation_id": "37",
      "title": "A novel neural network model based on cerebral hemispheric asymmetry for eeg emotion recognition",
      "authors": [
        "Y Li",
        "W Zheng",
        "Z Cui",
        "T Zhang",
        "Y Zong"
      ],
      "year": "2018",
      "venue": "IJCAI"
    },
    {
      "citation_id": "38",
      "title": "From regional to global brain: A novel hierarchical spatial-temporal neural network model for eeg emotion recognition",
      "authors": [
        "Y Li",
        "W Zheng",
        "L Wang",
        "Y Zong",
        "Z Cui"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "39",
      "title": "Variational pathway reasoning for eeg emotion recognition",
      "authors": [
        "T Zhang",
        "Z Cui",
        "C Xu",
        "W Zheng",
        "J Yang"
      ],
      "year": "2020",
      "venue": "AAAI"
    },
    {
      "citation_id": "40",
      "title": "Eeg emotion recognition using dynamical graph convolutional neural networks",
      "authors": [
        "T Song",
        "W Zheng",
        "P Song",
        "Z Cui"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "41",
      "title": "Eeg-based emotion recognition using regularized graph neural networks",
      "authors": [
        "P Zhong",
        "D Wang",
        "C Miao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "42",
      "title": "Review of the bci competition iv",
      "authors": [
        "M Tangermann",
        "K.-R Müller",
        "A Aertsen",
        "N Birbaumer",
        "C Braun",
        "C Brunner",
        "R Leeb",
        "C Mehring",
        "K Miller",
        "G Mueller-Putz"
      ],
      "year": "2012",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "43",
      "title": "Extracting common spatial patterns from eeg time segments for classifying motor imagery classes in a brain computer interface (bci)",
      "authors": [
        "H Ghaheri",
        "A Ahmadyfard"
      ],
      "year": "2013",
      "venue": "scientiairanica"
    },
    {
      "citation_id": "44",
      "title": "Fast and accurate multiclass inference for mi-bcis using large multiscale temporal and spectral features",
      "authors": [
        "M Hersche",
        "T Rellstab",
        "P Schiavone",
        "L Cavigelli",
        "L Benini",
        "A Rahimi"
      ],
      "year": "2018",
      "venue": "2018 26th European Signal Processing Conference"
    },
    {
      "citation_id": "45",
      "title": "Random forest and filter bank common spatial patterns for eeg-based motor imagery classification",
      "authors": [
        "M Bentlemsan",
        "E.-T Zemouri",
        "D Bouchaffra",
        "B Yahya-Zoubir",
        "K Ferroudji"
      ],
      "year": "2014",
      "venue": "2014 5th International Conference on Intelligent Systems, Modelling and Simulation"
    },
    {
      "citation_id": "46",
      "title": "Incremental adaptive eeg classification of motor imagery-based bci",
      "authors": [
        "H.-J Rong",
        "C Li",
        "R.-J Bao",
        "B Chen"
      ],
      "year": "2018",
      "venue": "2018 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "47",
      "title": "Parallel convolutional-linear neural network for motor imagery classification",
      "authors": [
        "S Sakhavi",
        "C Guan",
        "S Yan"
      ],
      "year": "2015",
      "venue": "2015 23rd European Signal Processing Conference (EUSIPCO)"
    },
    {
      "citation_id": "48",
      "title": "Compressing subjectspecific brain-computer interface models into one model by superposition in hyperdimensional space",
      "authors": [
        "M Hersche",
        "P Rupp",
        "L Benini",
        "A Rahimi"
      ],
      "year": "2020",
      "venue": "Design, Automation and Test in Europe"
    },
    {
      "citation_id": "49",
      "title": "Decoding two-class motor imagery eeg with capsule networks",
      "authors": [
        "K.-W Ha",
        "J.-W Jeong"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Big Data and Smart Computing (BigComp)"
    },
    {
      "citation_id": "50",
      "title": "Motor imagery eeg decoding based on multi-scale hybrid networks and feature enhancement",
      "authors": [
        "X Tang",
        "C Yang",
        "X Sun",
        "M Zou",
        "H Wang"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "51",
      "title": "A spatio-temporal interactive attention network for motor imagery eeg decoding",
      "authors": [
        "Y Ma",
        "D Bian",
        "D Xu",
        "W Zou",
        "J Wang",
        "N Hu"
      ],
      "year": "2022",
      "venue": "2022 IEEE International Conference on Signal Processing, Communications and Computing (ICSPCC)"
    },
    {
      "citation_id": "52",
      "title": "Long short-term memory",
      "authors": [
        "S Hochreiter",
        "J Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural Computation"
    },
    {
      "citation_id": "53",
      "title": "Classification of covariance matrices using a riemannian-based kernel for bci applications",
      "authors": [
        "A Barachant",
        "S Bonnet",
        "M Congedo",
        "C Jutten"
      ],
      "year": "2013",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "54",
      "title": "Motor imagery classification based on bilinear sub-manifold learning of symmetric positive-definite matrices",
      "authors": [
        "X Xie",
        "Z Yu",
        "H Lu",
        "Z Gu",
        "Y Li"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "55",
      "title": "Motor imagery classification based on local isometric embedding of riemannian manifold",
      "authors": [
        "S Li",
        "X Xie",
        "Z Gu",
        "Z Yu",
        "Y Li"
      ],
      "year": "2019",
      "venue": "2019 14th IEEE Conference on Industrial Electronics and Applications (ICIEA)"
    },
    {
      "citation_id": "56",
      "title": "Pedestrian detection via classification on riemannian manifolds",
      "authors": [
        "O Tuzel",
        "F Porikli",
        "P Meer"
      ],
      "year": "2008",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "57",
      "title": "A metric for covariance matrices",
      "authors": [
        "W Förstner",
        "B Moonen"
      ],
      "year": "2003",
      "venue": "A metric for covariance matrices"
    },
    {
      "citation_id": "58",
      "title": "Riemannian geometry for eeg-based brain-computer interfaces; a primer and a review",
      "authors": [
        "M Congedo",
        "A Barachant",
        "R Bhatia"
      ],
      "year": "2017",
      "venue": "Brain-Computer Interfaces"
    },
    {
      "citation_id": "59",
      "title": "Multiclass brain-computer interface classification by riemannian geometry",
      "authors": [
        "A Barachant",
        "S Bonnet",
        "M Congedo",
        "C Jutten"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "60",
      "title": "Principal geodesic analysis on symmetric spaces: Statistics of diffusion tensors",
      "authors": [
        "P Fletcher",
        "S Joshi"
      ],
      "year": "2004",
      "venue": "Computer Vision and Mathematical Methods in Medical and Biomedical Image Analysis"
    },
    {
      "citation_id": "61",
      "title": "Hybrid attention based multimodal network for spoken language classification",
      "authors": [
        "Y Gu",
        "K Yang",
        "S Fu",
        "S Chen",
        "X Li",
        "I Marsic"
      ],
      "year": "2018",
      "venue": "Proceedings of the conference. Association for Computational Linguistics. Meeting"
    },
    {
      "citation_id": "62",
      "title": "Bci competition 2008-graz data set a",
      "authors": [
        "C Brunner",
        "R Leeb",
        "G Müller-Putz",
        "A Schlögl",
        "G Pfurtscheller"
      ],
      "year": "2008",
      "venue": "Laboratory of Brain-Computer Interfaces"
    },
    {
      "citation_id": "63",
      "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "authors": [
        "S Ioffe",
        "C Szegedy"
      ],
      "year": "2015",
      "venue": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "arxiv": "arXiv:1502.03167"
    },
    {
      "citation_id": "64",
      "title": "Rectifier nonlinearities improve neural network acoustic models",
      "authors": [
        "A Maas",
        "A Hannun",
        "A Ng"
      ],
      "year": "2013",
      "venue": "Proc. icml"
    },
    {
      "citation_id": "65",
      "title": "Gmss: Graph-based multi-task self-supervised learning for eeg emotion recognition",
      "authors": [
        "Y Li",
        "J Chen",
        "F Li",
        "B Fu",
        "H Wu",
        "Y Ji",
        "Y Zhou",
        "Y Niu",
        "G Shi",
        "W Zheng"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "66",
      "title": "Driving fatigue detection with fusion of eeg and forehead eog",
      "authors": [
        "X.-Q Huo",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2016",
      "venue": "IEEE International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "67",
      "title": "Vigilancenet: Decouple intra-and inter-modality learning for multimodal vigilance estimation in rsvp-based bci",
      "authors": [
        "X Cheng",
        "W Wei",
        "C Du",
        "S Qiu",
        "S Tian",
        "X Ma",
        "H He"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "68",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "69",
      "title": "A multi-class eeg-based bci classification using multivariate empirical mode decomposition based filtering and riemannian geometry",
      "authors": [
        "P Gaur",
        "R Pachori",
        "H Wang",
        "G Prasad"
      ],
      "year": "2018",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "70",
      "title": "Learning temporal information for brain-computer interface using convolutional neural networks",
      "authors": [
        "S Sakhavi",
        "C Guan",
        "S Yan"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "71",
      "title": "Motor-imagery-based brain-computer interface using signal derivation and aggregation functions",
      "authors": [
        "J Fumanal-Idocin",
        "Y.-K Wang",
        "C.-T Lin",
        "J Fernández",
        "J Sanz",
        "H Bustince"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Cybernetics"
    },
    {
      "citation_id": "72",
      "title": "Physics-informed attention temporal convolutional network for eeg-based motor imagery classification",
      "authors": [
        "H Altaheri",
        "G Muhammad",
        "M Alsulaiman"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Industrial Informatics"
    },
    {
      "citation_id": "73",
      "title": "Bridging the bci illiteracy gap: a subject-to-subject semantic style transfer for eeg-based motor imagery classification",
      "authors": [
        "D.-H Kim",
        "D.-H Shin",
        "T.-E Kam"
      ],
      "year": "2023",
      "venue": "Frontiers in Human Neuroscience"
    },
    {
      "citation_id": "74",
      "title": "Dynamic frequency feature selection based approach for classification of motor imageries",
      "authors": [
        "J Luo",
        "Z Feng",
        "J Zhang",
        "N Lu"
      ],
      "year": "2016",
      "venue": "Computers in Biology and Medicine"
    },
    {
      "citation_id": "75",
      "title": "A contralateral channel guided model for eeg based motor imagery classification",
      "authors": [
        "L Sun",
        "Z Feng",
        "B Chen",
        "N Lu"
      ],
      "year": "2018",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "76",
      "title": "Densely feature fusion based on convolutional neural networks for motor imagery eeg classification",
      "authors": [
        "D Li",
        "J Wang",
        "J Xu",
        "X Fang"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "77",
      "title": "Dynamic routing between capsules",
      "authors": [
        "S Sabour",
        "N Frosst",
        "G Hinton"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "78",
      "title": "Umap: Uniform manifold approximation and projection for dimension reduction",
      "authors": [
        "L Mcinnes",
        "J Healy",
        "J Melville"
      ],
      "year": "2018",
      "venue": "Umap: Uniform manifold approximation and projection for dimension reduction",
      "arxiv": "arXiv:1802.03426"
    },
    {
      "citation_id": "79",
      "title": "He leads the Ambient Intelligence and Interactive Machines (Aiim) lab, where his main area of research is machine learning and deep learning focused on human-centered applications with wearables, smart devices, and smart environments",
      "authors": [
        "( Guangyi",
        "; T-Affc Patrick",
        "Ieee Sens"
      ],
      "year": "2019",
      "venue": "Etemad is an Associate Editor for IEEE Transactions on Artificial Intelligence, and has been a PC member/reviewer for many notable conferences and journals in the field. He has been the General Chair for the AAAI Workshop on Representation Learning for Responsible Human-Centric AI"
    }
  ]
}