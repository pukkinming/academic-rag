{
  "paper_id": "2412.07116v1",
  "title": "A Review Of Human Emotion Synthesis Based On Generative Technology",
  "published": "2024-12-10T02:06:10Z",
  "authors": [
    "Fei Ma",
    "Yukan Li",
    "Yifan Xie",
    "Ying He",
    "Yi Zhang",
    "Hongwei Ren",
    "Zhou Liu",
    "Wei Yao",
    "Fuji Ren",
    "Fei Richard Yu",
    "Shiguang Ni"
  ],
  "keywords": [
    "Emotion Synthesis",
    "Generative Technology",
    "Autoencoder",
    "Generative Adversarial Network",
    "Diffusion Model",
    "Large Language Model",
    "Sequence-to-Sequence Model"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Human emotion synthesis is a crucial aspect of affective computing. It involves using computational methods to mimic and convey human emotions through various modalities, with the goal of enabling more natural and effective human-computer interactions. Recent advancements in generative models, such as Autoencoders, Generative Adversarial Networks, Diffusion Models, Large Language Models, and Sequence-to-Sequence Models, have significantly contributed to the development of this field. However, there is a notable lack of comprehensive reviews in this field. To address this problem, this paper aims to address this gap by providing a thorough and systematic overview of recent advancements in human emotion synthesis based on generative models. Specifically, this review will first present the review methodology, the emotion models involved, the mathematical principles of generative models, and the datasets used. Then, the review covers the application of different generative models to emotion synthesis based on a variety of modalities, including facial images, speech, and text. It also examines mainstream evaluation metrics. Additionally, the review presents some major findings and suggests future research directions, providing a comprehensive understanding of the role of generative technology in the nuanced domain of emotion synthesis.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "A FFECTIVE computing is an interdisciplinary research field that aims to endow computers with the ability to recognize, understand, express, and respond to human emotions  [1] ,  [2] . It integrates theories and methods from multiple disciplines such as computer science, psychology, and cognitive science, attempting to reveal the essence of human emotions and apply it to human-computer interaction and intelligent systems. The core goal of affective computing is to enable computers to perceive, understand, and express emotions like humans, thereby achieving more natural and friendly human-computer interaction  [3] ,  [4] ,  [5] ,  [6] .\n\nEmotion synthesis  [7]  is an important branch of affective computing, which aims to enable computers to generate emotional expressions similar to human emotions. This ability can be realized through various common modalities, such as facial images, speech, and text. To achieve emotion synthesis, researchers have proposed a series of traditional methods by analyzing the characteristics of human emotional expressions and establishing mathematical models. These models are then used to generate speech and facial expressions with specific emotions using computers  [8] ,  [9] .\n\nArtificial intelligence has made remarkable advances in synthesizing human emotions, marking a significant breakthrough in the field. In particular, generative technology has greatly improved the effect and application scope of emotion synthesis  [10] ,  [11] ,  [12] . Compared with traditional methods, these new models can automatically learn the characteristics of emotional expressions from massive data without relying on manually designed rules and models  [13] ,  [14] ,  [15] ,  [16] . With their powerful generation capabilities, generative models can generate emotional samples that are highly similar to real data and more flexible, greatly expanding the research boundaries in the field of emotion synthesis. For example, some researchers use Autoencoders (AEs)  [17]  to generate speech with emotions. By modifying this structure, they can extract speaker embeddings, isolate timbre information, and control the flow of emotional attributes  [18] . Other researchers use Generative Adversarial Networks (GANs)  [19]  to generate facial images with specific emotions. By controlling the input of the generative model, they can generate faces expressing different emotions such as happiness, sadness, and anger  [20] . In recent years, Diffusion Models (DMs)  [21]  and Large Language Models (LLMs)  [22]  have also been widely used in emo-tion synthesis tasks. Some researchers use DMs to enhance image and audio processing by employing a reconstruction Module, which leverages noising and denoising in latent spaces  [23] . Other researchers use LLMs to generate empathic conversational texts by training language models on empathic conversations and injecting emotional information into response generation  [24] . However, to the best of our knowledge, there is a conspicuous absence of a systematic review that specifically focuses on generative technology for human emotion synthesis within this burgeoning field.\n\nThis study examines how generative AI models synthesize human emotions, addressing current gaps in research through a systematic analysis. The overall schematic diagram is illustrated in Fig.  1 . Specifically, this paper will firstly introduce several generative models and their underlying mathematical principles to help readers better understand the technical background of this field, including AEs, GANs, DMs, LLMs, and Sequence-to-Sequence (Seq2Seq) models  [25] . AEs learn compressed data representations through encoder-decoder reconstruction, enabling dimensionality reduction and feature extraction. GANs use adversarial training between a generator and discriminator to produce realistic data samples. DMs generate data by learning to reverse a noise diffusion process, often avoiding vanishing gradients. LLMs leverage transformer architectures and massive text datasets for human language processing and generation. Seq2Seq models use encoderdecoder architectures to map input sequences to output sequences, facilitating tasks like translation and synthesis. Then, this review summarizes the commonly used human emotion synthesis datasets from unimodal to cross-modal. Each dataset has annotated emotion labels according to its purpose, covering the simplest positive and negative emotion labels  [26]  to complex 32 categories of compound emotion labels  [27] .\n\nSubsequently, we categorize human emotion synthesis into three subfields: facial emotion synthesis, speech emotion synthesis, and textual emotion synthesis. The taxonomy of this survey is shown in Fig.  2 . Facial emotion synthesis involves modifying facial information in computer-generated faces to create more realistic and diverse emotions. This area of research intersects with face reenactment  [28] ,  [29] , face manipulation  [30] , and talking head generation  [31] ,  [32] . Speech emotion synthesis involves altering the emotional attributes of speech segments or generating new speech that conveys specific emotions by manipulating acoustic features. This section will cover various studies on Voice Conversion  [33] ,  [34] , Text-to-Speech (TTS)  [35] , and speech manipulation tasks  [36] . Textual emotion synthesis refers to using computational techniques to infuse textual content with different emotions or sentiments, thereby enhancing its expressiveness. This section will focus on text emotion transfer and empathetic dialogue generation.\n\nFinally, this review summarize the prevalent evaluation metrics employed in the task, present key findings from multiple perspectives, and offer insights into future research directions based on the aforementioned systematic analysis. The main findings include: (i) Generative models have made significant progress in emotion synthesis across multiple modalities, including facial images, speech, and text. (ii) In the past, GAN-based methods have demonstrated strong capability in facial emotion synthesis, excelling at capturing subtle nuances in expressions. However, DMs have now emerged as a more promising alternative, offering superior control, stronger adaptability across different modalities. (iii) Speech emotion synthesis has benefited from the adaptation of GANs and Seq2Seq models, with further improvements through AEs and DMs to enhance emotional depth and prosodic control. (iv) Textual emotion synthesis has increasingly leveraged LLMs and Seq2Seq architectures, using sentiment control and emotional valence modulation to produce emotionally resonant content, although challenges remain in balancing emotional expressiveness with conversational coherence. (v) Both subjective and objective metrics are essential for evaluating emotion synthesis models, with future research focusing on refining both to better capture emotional subtleties and align with human judgments. Looking ahead, there are still many directions worth exploring, which include: (i) Combining different generative models like GANs, Seq2Seqs, AEs, DMs, and LLMs can enhance emotion synthesis by leveraging the strengths of each model for more accurate and realistic outputs. (ii) Exploring new modalities like gestures, electroencephalogram (EEG)  [37] , and electrocardiogram (ECG)  [38] , as well as crossmodal models, expanding the potential for immersive and interactive emotional experiences. (iii) Real-time emotion generation on edge devices like smartphones and wearables can enable personalized, adaptive emotional interactions, with applications in healthcare, retail, and more. (iv) Emotion synthesis can transform digital entertainment and filmmaking by enabling more authentic emotional expressions in virtual characters, enhancing storytelling, and allowing real-time emotional adjustments in films based on audience feedback. This research provides a framework for understanding how generative models replicate human emotions, offering insights to guide future developments in the field. Overall, the main contributions of this paper include:\n\n• To the best of our knowledge, this review provides the first systematic overview of human emotion synthesis based on generative technology.\n\n• By analyzing more than 230 related papers, this review gives a taxonomy of generative technologybased human emotion synthesis in different modalities.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "•",
      "text": "We summarize commonly used datasets and evaluation metrics for human emotion synthesis across different modalities.\n\n• Finally, we discuss the current research status of human emotion synthesis based on generation technology and present a future outlook.\n\nThe remainder of this paper is structured as follows: Sections 3 -6 describe the gaps in existing review research, the mainstream emotion models, mathematical principles for generating models, and the commonly used datasets, Sections 7 -9 introduce the latest human emotion synthesis works in the three modalities of face images, speech, and text, Section 10 and 11 summarize the common evaluation metrics in the field and discuss the current state of research and development trends, and Section 12 provides the conclusion. A list of abbreviations is given in Table  1 . Genrative Models",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Facial Emotion Synthesis",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Speech Emotion Synthesis Textual Emotion Synthesis",
      "text": "Fig. approach was followed to ensure the comprehensiveness and relevance of the literature. The overall screening process is shown in Fig.  3 . Initially, we conducted comprehensive searches across key academic databases, including IEEE Xplore, ScienceDirect, and Google Scholar. The search strategy combined general and modality-specific keywords related to generative models and emotion synthesis. Examples of search terms included \"facial emotion synthesis\" + \"generative models,\" \"speech emotion synthesis\" + \"generative models,\" \"emotional face reenactment,\" and \"emotional voice conversion,\" etc., aiming to cover a wide array of studies in these domains. To ensure the inclusion of the most recent advancements, the search focused on papers published between 2017 and 2024, providing a comprehensive overview of developments in this timeframe. To further broaden the scope, we included terms referencing specific generative model architectures, such as AE, GAN, DM, LLM, and Seq2Seq. The initial search yielded more than 270 papers. Furthermore, we established strict inclusion criteria for the selection process: (1) Peer-reviewed papers published After the initial retrieval, a two-step filtering process was applied to ensure the focus remained on emotion synthesis. Studies whose primary aim was not related to emotional generation or did not involve the use of generative models were excluded. Furthermore, we eliminated papers with incremental contributions to maintain the diversity of the sources reviewed. The final set of selected works provides a detailed overview of the current and impactful advancements in the field.\n\nBy following this structured methodology, we ensured the thoroughness and relevance of the selected studies, offering a timely and well-rounded perspective on the role of generative technology in human emotion synthesis across multiple modalities. This process ultimately enabled a focused analysis of both seminal works and the latest advancements, contributing to a deepened understanding of generative models in affective computing.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Emotion Model",
      "text": "Emotion is commonly understood as a complex and everchanging state of mind and body, which can be triggered by various interactions  [39] , perceptions, or thoughts  [40] . It encompasses a wide range of experiences, cognitive evaluations, behavioral responses, physiological reactions  [41] , and communicative expressions. In the realm of human cognition, emotions play a crucial role in decision-making  [42] , shaping our perceptions, and guiding our interactions with others  [43] .\n\nAs illustrated in Fig.  4 , the study of emotions has resulted in the development of different theoretical models, which can be primarily categorized into discrete emotions theory and multidimensional emotion theory  [44] . In the  most basic discrete emotion framework, emotions are simply categorized as positive or negative, also known as polarity  [45] ,  [46] . Within this framework, the term \"emotion\" is often replaced with \"sentiment,\" which sometimes includes a neutral category as well. However, this sentiment categorization is considered too simplistic for certain contexts. Therefore, the more detailed discrete emotion theory categorizes basic emotions that are universally recognized across cultures into six or eight types  [47] ,  [48] ,  [49] . On the other hand, the multidimensional emotions theory suggests that emotions can be viewed along a continuous spectrum, often defined by dimensions such as 2D (valence and arousal)  [50]  or 3D (valence, arousal, and dominance)  [51] . These theoretical perspectives provide valuable insights into the complex nature of human emotions and serve as foundational principles for emotion synthesis. By considering emotions in generative tasks, machines not only understand and process information, but also become attuned to the emotional dimensions of human experience. This enriches the human-machine interaction landscape and opens up new avenues for the development of empathetic and intuitive AI developments.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Difference",
      "text": "The current state of research in affective computing primarily focuses on areas such as human sentiment analysis  [10] , emotion detection, and emotion recognition  [12] . These tasks have a long-standing tradition and are considered significant when viewed in a broader context. For instance, Saxena et al.  [58]  conducted a survey on emotion recognition methods, examining facial, physiological, speech, and text approaches. They highlighted key techniques like Stationary Wavelet Transform and Particle Swarm Optimization. In another study, Nandwani et al.  [59]  analyzed sentiment analysis and emotion detection methods, discussing the transition from lexicon-based to deep learning techniques. They addressed challenges and emphasized the need for advanced and versatile models to improve accuracy and adaptability across different domains and languages. Canal et al.  [60]  presented a systematic literature review on facial emotion recognition from images. They categorized the techniques into classical and neural network-based approaches, highlighting the slightly higher accuracy of classical methods compared to neural networks, despite the latter's generalization capabilities.\n\nGenerative models currently constitute one of the mainstream directions in artificial intelligence research. However, most of the existing reviews only focus on the synthesis of a single modality, such as facial images  [52] ,  [53] , speech  [54] ,  [55] , and text  [56] ,  [57] , and many of them ignore the emotional aspects of the synthesis process. The most relevant work to ours is  [9] . This paper provides a thorough review of GANs in synthesizing human emotions, with a focus on facial expressions, speech, and cross-modal synthesis. It details various GAN architectures, their applications in emotion synthesis, challenges faced, and future directions. By evaluating numerous studies, it highlights how GANs enhance emotion recognition accuracy, offer data augmentation, and create realistic, diverse emotional samples across modalities. However, the key distinction between our survey and the aforementioned one lies in the fact that ours exclusively focuses on emotion synthesis. In addition, this review introduces a series of generation models other than GANs, such as the Seq2Seq model in the field of emotional speech synthesis, and LLMs in text emotion synthesis.\n\nIn summary, the differences are shown in Table  2 . Our survey is based on a clear definition of human emotion synthesis, focusing on the application of generative models in this emerging field, as well as the latest research developments in various sub-fields.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Generative Model",
      "text": "The generative model  [61]  refers to a model that can be described as generating data, belonging to a type of probability model. In machine learning, it can model data directly or establish conditional probability distributions between variables via Bayes' theorem, allowing the generation of new data not present in the training set. In this section, we explain the mathematics of different generative models, including AE, GAN, DM, LLM, and Seq2Seq.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Auto-Encoder (Ae)",
      "text": "AEs  [17]  are neural network models used in generative tasks to efficiently learn data representations. These models compress input data into simplified patterns, then reconstruct it by preserving key features while minimizing errors in the reproduction process. As a variant of AE, Variational Auto-Encoders (VAEs) introduced by Kingma and Welling in 2013  [62] , offer a principled approach to learning latent data representations to account for data uncertainty and variability, making them well-suited for generative tasks. The training of VAEs is guided by the maximization of the Evidence Lower BOund (ELBO), which can be expressed as follows:\n\nwhere the encoder, q ϕ (z|x), maps the input data x to a distribution over the latent space characterized by parameters ϕ. Typically, this distribution is assumed to be Gaussian, with the encoder outputting the mean and variance of the distribution. The latent variable z, sampled from this distribution, is then fed into the decoder, p θ (x|z), which attempts to reconstruct the input data, where θ denotes the parameters of the decoder. In the Equation (1), the first term is the expected log-likelihood of the data given the latent variables, encouraging accurate reconstruction of the data. The second component measures how closely the encoded data patterns match an expected statistical distribution p(z), using the Kullback-Leibler divergence formula. By optimizing the ELBO, VAEs learn to balance the trade-off between fidelity in data reconstruction and adherence to a structured latent space.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Generative Adversarial Network (Gan)",
      "text": "GANs  [19]  are distinguished by their unique training methodology, which leverages the concept of adversarial learning, setting up a dynamic competition between two distinct neural networks: the generator and the discriminator. The generator network, G, aims to map latent space vectors, drawn from a prior distribution p z (z), to data space, effectively generating new data samples that mimic the distribution of real data, p data (x). In contrast, the discriminator network, D, is trained to distinguish between samples drawn from the real data distribution and those produced by the generator. The competition between networks steadily improves their performance, ultimately enabling the generator to create lifelike outputs. The training of GANs is formulated as a min-max game, which can be formally represented by the following value function V (G, D):\n\nwhere the first term represents the expected log-probability that the discriminator correctly identifies real data samples as real. The second term represents the expected logprobability that the discriminator correctly identifies fake samples (generated by G) as fake. Training GANs involves alternating between optimizing D to maximize V (D, G) for fixed G (improving D's accuracy in distinguishing real from fake samples) and optimizing G to minimize V (D, G) for fixed D (improving G's ability to generate realistic samples). This adversarial training process continues until a state of equilibrium is reached where G generates samples indistinguishable from real data by D. The adversarial training mechanism of GANs has proven to be highly effective for generating complex, high-dimensional data.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Diffusion Model (Dm)",
      "text": "DMs  [21]  have emerged as a class of powerful generative models that synthesize data by gradually refining random noise into structured patterns. The fundamental principle behind DMs involves two phases: a forward (noising) phase and a reverse (denoising) phase. In the forward phase, step by step, the model gradually adds random noise to the data until the original information becomes completely obscured. This process is described by a Markov chain that gradually corrupts the original data distribution, x 0 , into a tractable noise distribution, x T , over T steps. Mathematically, this can be expressed through a sequence of conditional probabilities:\n\nwhere β t represents the variance of the noise added at each step, and I is the identity matrix. The sequence of β t values is predefined to control the noise level at each step, ensuring a smooth transition from data to noise. The reverse phase aims to learn the reverse process, modeling the conditional distribution of x t-1 given x t , effectively denoising the data.\n\nThe model, typically parameterized by a neural network, is trained to approximate the reverse conditional probabilities:\n\nwhere the parameters µ θ and Σ θ are functions learned during training, with θ representing the model's parameters.\n\nThe model learns by minimizing the gap between real and generated data distributions, using either statistical bounds or direct probability optimization. Despite their computational intensity, DMs have emerged as a cornerstone in generative modeling due to their impressive performance in generating high-quality samples.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Large Language Model (Llm)",
      "text": "Large Language Models are AI systems trained on massive text datasets, using billions of parameters to learn language patterns. Their deep understanding of language enables human-like text comprehension and generation, transforming how computers process natural language. The advent of Transformer architecture, developed by Vaswani et al. in 2017  [22] , marked a significant breakthrough in language modeling.\n\nThe training objective of LLMs can generally be described as learning a conditional probability distribution over sequences. Given an input sequence X = (x 1 , x 2 , . . . , x T ), the model maximizes the likelihood of generating each token based on prior tokens, represented as:\n\nwhere p(x t |x <t ) is the probability of token x t given its context x <t . A key feature of this process is the attention mechanism, which allows the model to dynamically focus on different parts of the context at each step. In self-attention, the probability of generating each token is influenced by a weighted sum of the context, with attention weights α tj computed as:\n\nand the new token representation z t is given by:\n\nBy analyzing relationships between tokens, the attention system helps the model understand context and produce coherent, relevant text. So far, several LLMs have gained prominence, including Generative Pre-trained Transformer (GPT)  [63]  series, Bidirectional Encoder Representations from Transformers (BERT)  [64] , eXtreme Language Understanding Network (XLNet)  [65] , and Text-to-Text Transfer Transformer (T5)  [66] , etc. These models have been pre-trained on vast amounts of text data and can effectively fine-tune for specific tasks, such as language translation, sentiment analysis, and text generation  [67] ,  [68] .",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Sequence-To-Sequence (Seq2Seq) Model",
      "text": "Seq2Seq model  [25]  is a neural network architecture designed for tasks involving sequential input-output pairs. It follows an encoder-decoder structure, where each component is typically implemented with recurrent neural networks (RNNs) or Transformers in later models.\n\nOne defining feature of Seq2Seq models is their focus on sequential data, which differentiates them from models like GANs or VAEs that do not inherently account for sequential dependencies. Unlike general language models, Seq2Seq models specialize in transforming one sequence into another, effectively capturing both immediate and distant patterns in the data.\n\nIn the Seq2Seq model, the encoder processes the input sequence (x 1 , x 2 , . . . , x T ) to produce a context vector c, often represented by the encoder's final hidden state:\n\nwhere h T encapsulates the input sequence's essential information. The decoder then uses this context vector to generate the output sequence (y 1 , y 2 , . . . , y T ′ ) one element at a time, conditioned on c and previously generated outputs:\n\nwhere s t is the hidden state at time t, and W is a weight matrix for calculating the output distribution. Seq2Seq models are effective for handling variablelength input and output sequences, making them wellsuited for applications like translation, summarization, and question answering, where coherent sequence transformation is required. The differences between Seq2Seq models and other generative models are shown in Table  3 .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Databases",
      "text": "The performance of human emotion synthesis tasks based on generative models is closely tied to the quality and richness of the utilized datasets. To be specific, the diversity and scope of the datasets play a crucial role in the model's ability to generalize across various emotional states, cultural contexts, and individual differences. The structure and content of emotion databases directly shape how researchers design and build emotion synthesis models. The structure, annotation scheme, and inherent biases of the datasets influence the choice of model architecture, loss functions, and training strategies. Furthermore, the size and",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Model Type Belongs To Seq2Seq? Description",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Llms",
      "text": "Partially belongs Some LLMs (e.g., T5, BART) are implementations of Seq2Seq, but LLMs cover a broader range.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Aes",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Does Not Belong",
      "text": "Similar to Seq2Seq in architecture, but with different goals and tasks.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Gans",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Does Not Belong",
      "text": "Completely different model type with unrelated goals and structures.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Dms",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Does Not Belong",
      "text": "Entirely different generative models, unrelated to Seq2Seq.\n\nquality of the datasets influence the choice between end-toend learning approaches and modular architectures, with large, high-quality datasets enabling end-to-end learning of emotion synthesis, while smaller or noisier datasets might necessitate the use of pre-trained components or transfer learning techniques. Based on these emotional datasets of different modalities, such as facial images, speech, and text, the designed models can imitate human emotional expressions with high precision from different aspects. Table  4  summarizes the common datasets used in the field of human emotion synthesis, providing a comprehensive overview of the available resources for researchers and practitioners in this domain.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Facial Emotion Synthesis",
      "text": "Facial emotion synthesis is a crucial research field within human emotion synthesis, aiming to generate faces that express specified emotions. This technology holds significant academic value in computer graphics and computer vision, while also demonstrating great promise for applications in virtual reality (VR), gaming, and interactive computer systems. Based on existing works, we can broadly categorize facial emotion synthesis into three main approaches: face reenactment (Section 7.1), talking head generation (Section 7.3), and facial manipulation (Section 7.2). The related works are illustrated in Table  5 .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Face Reenactment",
      "text": "Face reenactment focuses on transferring facial expressions from a source actor to a target face, preserving the identity of the target while adopting the emotional expressions of the source. This technique is particularly useful in applications like film dubbing, virtual avatars, and privacy-preserving video conferencing. In facial reenactment, there are some tasks that emphasize emotional attributes in the face. For example, in  [90] , Tripathy et al. introduced ICface, a GAN-based face animator that manipulated facial expressions in a given image. The animation process was guided by interpretable control signals, such as head pose angles and Action Units (AU) values, which were derived from various sources, allowing for selective emotion transfer. Zeng et al.  [91]  proposed DAE-GAN, which employed two deforming autoencoders to separate identity and pose in unlabeled videos, reducing the need for manual annotation. It realized emotional transfer between different identities with varied poses using conditional generation and disentangled features. Strizhkova et al.  [92]  proposed a novel method for emotion editing in head reenactment videos by manipulating the latent space of a pre-trained GAN. This technique disentangled emotion, identity, and pose within the latent space, allowing for the direct modification of emotions in the reenactment videos without affecting the person's identity or the speech-related facial expressions. Groth et al.  [93]  designed a new method to achieve emotion mapping by generating correctly recognized expressions, using video reenactments to influence the intensity of the emotion. In  [94] , Ali et al. utilized two encoders to separately capture expression from a source and identity from a target image, merging these features to create expressive images, enhanced by innovative consistency losses for both expression and identity features. In Fig.  5 , Xue et al. presented LSGAN  [95] , employing a transformative generator that combined target expression labels with specific facial region features to produce clear and distinct facial expressions in images. Shao et al.  [96]  utilized dual parallel generators and wavelet-based discriminators for facial expression translation, enhancing realism by focusing on key areas with an attention mechanism and capturing expression details across scales without the bidirectional translation interference seen in single-generator models. Fig.  5 . A mask-based GAN  [95]  for face reenactment. The system included four main components. A Semantic Mask Generator (SMG) produced masks for specific facial regions (eyes, mouth, cheeks). Then these masks were encoded into latent codes through an Adversarial Autoencoder (AAE). A Transformative Generator (TG) used these codes along with target expression labels to generate new facial expressions, with an AU-intensity Discriminator (AUD) that assessed their quality and intensity.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Face Manipulation",
      "text": "Face manipulation involves editing specific attributes of a face, such as changing expressions, age, hairstyles, etc., to generate different versions of the same person. It can be seen that this has a completely different goal compared to face reenactment which transfers the expressions and movements of a source face to a target face. Researchers in this field focus on the alteration of specific facial attributes while preserving the remaining attributes unchanged, under the condition of explicit predefined facial information, thereby effecting changes in emotional expression. Works in this field are mainly based on GAN and its variants  [20] ,  [30] ,  [97] ,  [98] ,  [99] ,  [100] ,  [101] ,  [102] ,  [103] ,  [104] ,  [105] ,  [106] . For",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Talking Head Generation",
      "text": "Talking head generation aims to create realistic, animated facial models that can speak and emote based on input audio or text. This approach is particularly valuable in creating virtual assistants, digital newscasters, and personalized content for educational or entertainment purposes. A series of works that incorporate emotion synthesis into talking head generation have been explored. For example, Eskimez et al.  [116]  presented a novel system for generating talking faces, achieving independent control of emotional expressions by disregarding the emotions expressed in the input speech audio and instead conditioning the face generation on an independent categorical emotion variable. In  [117] , Vougioukas et al. developed a specialized GAN that uses three discriminators to create detailed facial expressions that match a speaker's emotional state. As illustrated in Fig.  6 , Zhang et al. presented EmoTalker  [23] , a framework for emotionally editable talking face generation, utilizing a diffusion model and a novel Emotion Intensity Block, and integrating a custom dataset to enhance emotion interpretation. In  [118] , Zeng et al. presented ET-GAN, an end-to-end system for generating talking faces with tailored expressions from guiding videos, identity images, and arbitrary audio, utilizing multiple encoders for identity, expression, and audio-lip synchronization, alongside advanced frame and spatial-temporal discriminators. Gan et al.  [119]  generated synchronized emotion coefficients and emotion-driven facial images through flow-based and vector-quantized models, while using lightweight emotion prompts or CLIP supervision for model control and adaptation. In  [120] , Tan et al. used the regularized flow model to generate emotion-related expression coefficients, mapped the input emotion coefficients to the latent space, and generated diverse expression dynamics based on audio and emotion labels, while achieving high-fidelity restoration of emotional details through a vector quantization image generator. In another work  [121] , they used orthogonal basis decomposition to decompose facial dynamics into independent latent spaces of lip shape, head posture, and emotional expression, allowing the model to adjust the relevant expression coefficients in each independent latent space.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Speech Emotion Synthesis",
      "text": "Speech emotion synthesis is a critical research field within human emotion synthesis, focusing on the manipulation and generation of acoustic features in speech signals to alter or create specific emotional states. This area of study aims to modify key vocal parameters such as volume, intonation, pitch, speaking rate, and timbre to effectively convey a desired emotional state in synthesized speech. Based on existing works, we divide it into voice conversion (Section 8.1) ,text-to-speech (Section 8.2) and speech manipulation (Section 8.3). Table  6  illustrates the overall papers about speech emotion synthesis.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Voice Conversion",
      "text": "Voice conversion technology modifies a speaker's voice to express different emotions while keeping the original words intact. This enables emotional dubbing in films and games, and helps create more natural-sounding AI assistants.\n\nFor instance, in  [76] , Zhou et al. leveraged VAW-GAN and deep emotional features from speech emotion recognition to describe emotional prosody. By using adjustable emotional features to guide the decoder, this approach could transform speech to express both familiar and new emotions. In  [137] , they combined VAW-GAN and Continuous Wavelet Transform (CWT) for advanced spectrum and prosody conversion. By incorporating CWT-based F0 modeling, the system uniquely enhanced the granularity of prosody representation. In  [138] , they focused on disentangling and re-composing emotional elements in speech, innovatively employing CWT for detailed prosody analysis and integrating F0 conditioning in the decoder to enhance emotion conversion performance. Similarly, in  [139] , they proposed a CycleGAN-based model  [140]  that did not require parallel data. It utilized CWT to analyze F0 on multiple scales, enabling detailed prosody modification. In  [141] , Fu et al. also presented an improved CycleGANbased model that incorporated a transformer to augment temporal dependencies and integrated curriculum learning and a fine-grained level discriminator, enhancing the model's ability to capture and convert emotional nuances in speech more effectively. Rizos et al.  [142]  utilized classconditional GAN and an auxiliary domain classifier to generate emotional speech samples. In  [143] , Elgaar et al. introduced a Factorized Hierarchical Variational Autoencoder (FHVAE) for multi-speaker and multi-domain emotional voice conversion, enhancing disentangled representation and emotion conversion quality through novel algorithms and loss functions. Gao et al.  [144]  used style transfer autoencoders for emotional voice conversion without parallel data, leveraging disentangled representation learning to modify emotion-related characteristics. In Fig.  7 , Chen et al. introduced a Tacotron2-based framework using emotion disentangling modules  [145]  to achieve cross-speaker emotion transfer by separating speaker identity from emotion. In  [146] , Oh et al. proposed the DurFlex-EVC model, integrating a style autoencoder and unit aligner for advanced control and flexibility. It leveraged HuBERT features, denoising diffusion models, and self-supervised learning to modify emotional tones while maintaining linguistic content and unique vocal traits. Moreover, there were also some works related to emotional voice conversion based on StarGAN  [147] ,  [148] ,  [149] , AE  [150] , and Seq2Seq  [151] ,  [152] ,  [153] . Fig.  7 . A two-stage model  [145]  for voice conversion. In the first stage, the method separated emotional and content features from speech using an attention-based mechanism, incorporating inter-speech relation to model emotional strength. The second stage employed a conversion adaptation strategy, leveraging a multi-view consistency mechanism to ensure that the emotional nuances were accurately transformed while the core speech content remained intact. This framework facilitated precise control over the emotional output of synthesized speech.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Text-To-Speech (Tts)",
      "text": "TTS aims to generate natural-sounding speech based on the semantic content and context of the input text. This technology is particularly crucial for enabling more engaging and human-like interactions with virtual assistants, audiobook narrations, and personalized content delivery. Some researchers have attempted to incorporate emotional factors into TTS systems to synthesize speech with emotional expressiveness.\n\nFor example, in  [154] , Lei et al. introduced a unified model for fine-grained emotional speech synthesis, obtaining fine-grained emotion expressions with emotion descriptors or phoneme-level manual labels. Li et al.  [155]  developed DiCLET-TTS, which improves emotional speech synthesis by combining emotion separation techniques with advanced probability-based decoding. In  [156] , Tits et al. explored adapting a Deep Convolutional TTS (DCTTS) model to various emotions using minimal emotional data. In  [157] , Schnell et al. leveraged WaveNet and emotion intensity extraction using attention LSTM and transformer models, demonstrating increased perceived emotion accuracy. Wu et al  [158]  synthesized emotional speech with limited labeled data, achieving comparable performance to fully supervised models. In  [159] , Um et al. employed an inter-to-intra distance ratio algorithm and an effective interpolation technique to achieve nuanced emotion intensity control. In  [81] , Im et al. proposed EmoQ-TTS, a system that synthesized expressive emotional speech by conditioning phoneme-wise emotion information with fine-grained emotion intensity, using intensity pseudo-labels generated via distance-based intensity quantization. Hortal et al.  [160]  combined Tacotron 2 with GANs to modulate prosody, allowing customization of inferred speech with specified emotions. Guo et al.  [161]  presented \"EmoDiff,\" a DM-based model that enabled intensity-controllable emotional speech synthesis using a soft-label guidance technique. Li et al.  [162]  utilized the Tacotron framework, enhanced with emotion classifiers and style loss, to generate expressive, controllable emotional speech efficiently. In  [163] , Lei et al. introduced a novel method integrating global-level, utterance-level, and locallevel modules to achieve precise emotion modeling and transfer, allowing for versatile emotional expression in synthesized speech. In Fig.  8 , Li et al. created a system  [164]  that extracts emotion patterns independent of the speaker's voice, allowing emotions to be transferred between speakers and adjusted in intensity. In  [165] , Li et al. developed a technique that analyzes speech style at different scales, capturing both broad and fine details to produce more controllable and expressive synthetic voices. In  [166] , Tang et al. introduced mix methods, enabling the manual combination of noise at runtime to produce diverse emotional mixtures, which was validated through evaluations demonstrating its capability to generate speech with various mixed emotions. Fig.  8 . A Tacotron2-based TTS model from  [164] . This system integrated an Emotion Disentangling Module (EDM) that utilized dual encoders to separate emotion-related features from speaker identity, refining the emotion embedding for clearer expression without speaker leakage. An identity controller maintained the target speaker's identity by incorporating speaker-specific information, enabling the synthesis of emotionally expressive speech consistent with the target speaker's vocal characteristics.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Speech Manipulation",
      "text": "Speech manipulation focuses on modifying various aspects of speech signals, such as the speaker's identity or linguistic content. It shares similarities with face manipulation in Section 7.2, as both aim to alter or control specific attributes of human-generated data, be it facial features or speech characteristics. Recently, some researchers have begun exploring how to manipulate the emotional attributes of speech.\n\nFor instance, in  [79] , Jia et al. presented ET-GAN, an innovative cross-language emotion transfer system for speech synthesis, which was unique for not necessitating parallel training data and utilized CycleGAN, achieving significant improvements in emotional accuracy and naturalness of synthetic speech. In  [167] , Matsumoto et al. utilized WaveNet and auxiliary features like voiced, unvoiced, and silence flags to generate speech-like emotional sounds without linguistic content, enhancing emotional expressiveness control. In  [168] , Wang et al. presented Emo-CampNet, a text-based speech editing model. It integrated emotion attributes and a context-aware mask prediction network, employing generative adversarial training and data augmentation to enhance emotional expressiveness and speaker variability in edited speech. In  [36] , Inoue et al. introduced a novel emotion editing technique in speech synthesis utilizing a hierarchical emotion distribution extractor within the FastSpeech2 framework  [169] , enabling fine-grained, quantitative emotion control at phoneme, word, and utterance levels for dynamic and nuanced speech generation.    9 . A text emotion transfer model from  [183] . During training, the model restored a corrupted input by using a stable \"style vector\" derived from the preceding sentence to condition the reconstruction process. A new style vector allowed precise, targeted style modifications, or \"targeted restyling,\" during inference by adjusting the direction and magnitude of the style shift relative to a baseline, using a few exemplar sentences to define the desired output style.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Textual Emotion Synthesis",
      "text": "Textual emotion synthesis is a vital research field within human emotion synthesis, focusing on the generation of texts that possess specific emotional, sentiment, or empathetic attributes. This area of study aims to create or modify written content to convey desired emotional states, sentiments, or empathetic responses, thereby enhancing the expressiveness and impact of textual communication. Based on existing works, we consider the following two types of tasks: text emotion transfer (Section 9.1) and empathetic dialogue generation (Section 9.2). Table  7  shows the literature about textual emotion synthesis.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Text Emotion Transfer",
      "text": "Text emotion transfer focuses on transforming the emotional tone or sentiment of an existing text while preserving its core semantic content. It allows for the modification of neutral text into emotionally charged content, or the alteration of one emotional state to another.\n\nFor example, in  [184] , Mohammadibaghmolaei et al. proposed a text emotion transfer technique based on masked language modeling and transfer learning, and a GPT-2 model underwent training to construct an initial sentence based on its altered sequences, allowing the model to perform efficiently even with limited emotion-annotated A novel text sentiment transfer methodology was proposed by Li et al.  [26]  in which they employed a three-step process-Delete, Retrieve, and Generate. This approach, powered by unsupervised learning and neural sequence-tosequence models, effectively altered sentiment while retaining content. In  [185] , Jin et al. introduced the IMaT model that constructed a pseudo-parallel corpus through semantic alignment, then applied a sequence-to-sequence model for attribute translation, refining this alignment across iterations. Wu et al.  [186]  proposed a two-stage \"Mask and Infill\" methodology that significantly enhanced the performance of non-parallel text sentiment transfer. Following the \"mask and infill\" method, in  [187] , Malmi et al. introduced an innovative unsupervised method using padded masked language models (MLMs) for sentiment transfer, using a padded MLM variant to avoid having to predetermine the number of inserted tokens. In  [188] , Yang et al. presented a technique leveraging language models as discriminators for unsupervised sentiment manipulation, enhancing stability and content fidelity in generated text. In  [189] , Shen et al. developed a technique for text sentiment transfer without parallel data, utilizing refined alignment of latent representations, which effectively separated content from style, allowing for sentiment modification by mapping sentences to a style-independent content vector, then decoding this vector into another style. Zhang et al.  [190]  employed GAN for sentiment transfer across different text domains, innovatively combining adversarial and reinforcement learning with a cross-domain sentiment transfer model, enhancing the ability to generate emotionally nuanced text while maintaining domain-specific content. In Fig.  9 , Riley et al. leveraged T5 to extract a style vector from preceding sentences and used it to provide extra conditioning for the decoder  [183] . Huang et al. introduced a method  [191]  based on Cycleconsistent Adversarial AutoEncoders, comprising LSTM autoencoders, adversarial style transfer networks, and cycleconsistent constraints, innovating unsupervised text style transfer. In  [192] , Luo et al. proposed the Seq2SentiSeq model, incorporating sentiment intensity via a Gaussian kernel in the decoder, enhancing sentiment control. They trained the model using cycle reinforcement learning, which maintains the original message while changing emotional tone without requiring matched data pairs.",
      "page_start": 14,
      "page_end": 15
    },
    {
      "section_name": "Empathetic Dialogue Generation",
      "text": "Empathetic dialogue generation is a crucial aspect of creating more human-like and emotionally intelligent conversational agents. It goes beyond simply generating contextually relevant responses and focuses on incorporating emotional understanding and support into the generated dialogue.\n\nA lot of researchers focused on how to generate responses with specific emotional tendencies or more empathy by LLMs. For example, in  [193] , Li et al. employed the Emotional Chain-of-Thought (ECoT) technique, enhancing Large Language Models' capability for nuanced emotional text generation, focusing on human emotional intelligence alignment. Yang et al.  [194]  provided a Hybrid Empathetic Framework (HEF) that used SEMs as flexible enhancements to LLMs, implementing a two-stage emotion prediction strategy and an emotion cause perception strategy. In  [195] , Sun et al. utilized the Chain of Emotion-aware Empathetic prompting (CoNECT) for better context understanding and emotional engagement. In  [196] , Lee et al. investigated GPT-3's capacity for empathetic dialogue generation, employing in-context learning in zero-shot and few-shot settings. Casas et al.  [24]  introduced an empathic chatbot framework utilizing transformer-based language models for generating responses that recognized and adapted to the user's emotional state. In  [197] , Chen et al. enhanced the empathetic responses of ChatGLM-6B by fine-tuning it with a specialized dataset comprising over 2 million multi-turn empathy conversation samples.\n\nApart from LLMs, there are some studies on emotional or empathetic dialogue generation based on models like Seq2Seq model and conditional variational autoencoder (CVAE). For example, as shown in Fig.  10 , Song et al. proposed an emotional dialogue system, EmoDS  [198] , that enhanced a Seq2Seq framework with a lexicon-based attention Fig.  10 . A empathetic dialogue generation system from  [198] . The system employed a bidirectional LSTM encoder to process the input text into a vector representation, which initialized a decoder. This decoder, guided by an emotion classifier and enriched with a lexicon-based attention mechanism, integrated emotional lexicon words seamlessly into the responses.\n\nmechanism and an emotion classifier, generating dialogue responses that expressed specified emotions, either explicitly or implicitly. In  [199] , Zhou et al. introduced an Emotional Chatting Machine (ECM) that integrated internal and external memory mechanisms along with emotion category embeddings. In  [200] , Kong et al. proposed a model that effectively combined CGANs with either standard Seq2Seq or CVAE models to produce dialogue responses with the specified sentiment. Li et al.  [201]  introduced a novel Dual-View CVAE model that synthesized emotional dialogue, changing the emotional expression of responses with higher content relevance. In  [202] , Xu et al. proposed a framework that incorporated multi-task learning and dual attention mechanisms, effectively decoupling and processing content and emotional information from the input. Asghar et al.  [203]  suggested an enhanced Seq2Seq model that incorporated three emotional strategies for the input, training, and inference processes, which was based on a designed dictionary with Valence, Arousal, and Dominance (VAD) scores. In  [204] , Colombo et al. employed a Seq2Seq architecture augmented by emotion embeddings and a VAD lexicon for word and sequence-level emotion modeling. It utilized an affect regularizer to favor emotionally charged words and an affect sampling method for generating emotionally relevant diverse responses. Huang et al.  [205]  introduced a novel approach to integrate emotions into dialogue generation by appending an emotion token to the dialogue input or injecting the emotion directly into the decoder. In  [206] , Lin et al. proposed a model integrating Transformer and CVAE, with an emotion perception encoder and a BERTbased emotion classification model to embed emotional intelligence, enabling the generation of nuanced and contextually relevant empathetic responses.\n\nIn addition to the aforementioned methods, there have been several research efforts exploring the use of GANs to achieve empathetic dialogue generation. For example, in  [207] ,  [208] , Wang et al. developed SentiGAN, an innovative structure featuring numerous generators alongside a singular multi-class discriminator. This setup encouraged each generator to concentrate on crafting text samples that distinctly exhibited a designated sentiment label. Chen et al.  [209]  utilized GAN with multiple classifiers to enhance emotional dialogue production, with an emotion discriminative model to align the generated dialogue's emotion with the intended one. Bi et al.  [210]  utilized a diffusion model-based approach to generate empathetic responses, distinctive for its use of multi-grained control signals, incorporating communication mechanism, intent, and semantic frame as control levels, enabling nuanced guidance over the generated responses. In  [211] , Chen et al. proposed CTGAN for emotional text generation by incorporating emotion labels, allowing for the production of text that aligned with specific emotional tones within variable-length outputs.",
      "page_start": 14,
      "page_end": 15
    },
    {
      "section_name": "Evaluation Metric",
      "text": "In the field of human emotion synthesis, several commonly used evaluation metrics are employed to assess the quality of generated content across the three modalities of facial images, speech, and text. These metrics provide a comprehensive assessment of the synthesized emotions from various perspectives. While some evaluation indicators are universal and applicable to all modalities, others are exclusive to specific modalities, as illustrated in Table  8 .\n\nFor common evaluation indicators, there are usually two forms: user study and accuracy. User studies are a qualitative evaluation method that assesses the quality of generated content from aspects such as clarity, naturalness, and emotional authenticity. These studies involve gathering feedback and opinions from human participants to gauge their perception and experience of the synthesized emotions. For example, in tasks like face reenactment, where emotional authenticity is critical, researchers conduct a \"Real vs. Fake\" perceptual study on platforms like Amazon Mechanical Turk (AMT) to evaluate the outputs.\" In the field of TTS and voice conversion, researchers  [146] ,  [159] ,  [166]  use subjective evaluation metrics like Mean Opinion Score (MOS) and Similarity Mean Opinion Score (SMOS) to assess naturalness and emotional similarity. Accuracy is another important evaluation metric for human emotion synthesis. For example, in face reenactment, researchers use classifiers to assess generated facial images. Higher accuracy in expression higher accuracy in expression translation by the models  [96] . Similarly,  [23]  utilizes an emotion classifier network from EVP  [226]  to measure the emotion accuracy of face manipulation.\n\nIn addition to the aforementioned universal evaluation metrics, different modalities in the field of emotion synthesis have their own specific and widely used evaluation indicators. These indicators target the unique attributes and synthesis goals of each modality, providing a more fine-grained and specialized assessment perspective. For example, in facial emotion synthesis, PSNR (Peak Signalto-Noise Ratio)  [107] ,  [227]  quantifies image quality by comparing compressed images to their originals. It is calculated using the logarithm of the ratio between maximum pixel value and mean squared error, with higher values indicating better quality. SSIM (Structural Similarity Index)  [96]  improves upon PSNR by evaluating image similarity based on luminance, contrast, and structure, focusing on perceived quality and structural integrity. FID (Fréchet Inception Distance)  [91]  measures the similarity between sets of images by comparing feature vectors, with lower scores indicating higher quality and diversity in generated images. In speech emotion synthesis, MCD (Mel Cepstral Distortion)  [166] ,  [228]  objectively measures spectral similarity between reference and generated mel-spectrograms, providing quantitative feedback on emotional voice synthesis accuracy. F0 RMSE (F0 Root Mean Square Error)  [81] ,  [158]  evaluates the accuracy of the fundamental frequency contour in synthesized speech compared to the reference. Lower values indicate higher pitch accuracy, contributing to perceived naturalness and expressiveness. In textual emotion synthesis, the BLEU (Bilingual Evaluation Understudy) score  [26] ,  [186] , borrowed from machine translation evaluation, can measure lexical similarity between generated and reference texts, indicating how well the generated text maintains desired linguistic properties while altering emotional tone. PPL (Perplexity)  [185]  measures a language model's prediction accuracy, reflecting its ability to produce coherent and fluent text. Lower PPL suggests the generated text more closely mirrors human language patterns.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Discussion",
      "text": "",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Major Findings",
      "text": "Existing methods in generative technology for human emotion synthesis have made substantial progress across multiple modalities, including facial images, speech, and text. Each modality benefits from distinct approaches and these developments not only enhance the perceived emotional intelligence of systems but also push the boundaries of how machines can generate and interpret nuanced emotional states.\n\nIn facial emotion synthesis, GAN-based methods used to be the mainstream methods. However, in recent years, DMbased methods have also gained considerable attention and made significant progress. Specifically, some classical GANbased models, such as StarGAN and StyleGAN, excel at altering the emotional expressions of faces but face challenges when attempting to generate more subtle or complex emotional states. Moreover, generating mixed emotions-that is, facial expressions that convey a strong contrast of emotions like happiness and sadness-still lacks finer control. Comparatively, DM generates images through progressive denoising, which avoids the problem of model collapse that GANs may have when generating facial expressions. It also shows strong adaptability in generating mixed emotions, better capturing subtle expression changes and emotional nuances, and can handle multiple combinations of emotions in facial expressions.\n\nSpeech emotion synthesis achieved similar progress through the adaptation of GANs and Seq2Seqs like Tacotron  [229] , where emotional intonation is introduced into synthesized speech. These models produce more natural and expressive speech by adjusting vocal elements like pitch, rhythm, and tone. However, recent research has also incorporated AEs, and DMs to further improve the emotional depth and expressiveness of synthesized speech. Specifically, AEs are used to disentangle emotional features from speaker identity, enabling more flexible emotion transfer while preserving the naturalness of speech. DMs, with their capacity for modeling complex data distributions, offer promising results in generating emotional speech with high fidelity and more controlled variations in prosody.\n\nTextual emotion synthesis has increasingly leveraged LLMs and Seq2Seq architectures, utilizing mechanisms like sentiment control and emotional valence modulation to produce emotionally resonant content  [230] . These systems are often employed in applications such as empathetic chatbots and emotionally responsive dialogue systems. Despite their effectiveness, generating responses with emotional depth that appropriately reflect varying levels of empathy, sympathy, or other complex emotional tones based on user inputs remains a challenge. Current models still struggle with maintaining a balance between emotional expressiveness and conversational coherence, especially in response to ambiguous or contextually nuanced inputs. Moreover, understanding the contextual triggers for emotional responses and integrating them effectively into generative models will be an ongoing research area.\n\nIn terms of evaluation metrics, quality assessment remains a complex, multidimensional task that integrates both subjective and objective indicators. Subjective metrics typically involve human evaluation, which focus on capturing the emotional authenticity, resonance, and overall impression of the generated content. However, subjective evaluations can be time-consuming and costly, and they are influenced by individual biases and cultural differences. On the other hand, objective metrics aim to quantify various aspects of emotion synthesis using automated computational methods, such as accuracy, PSNR, etc. They provide a scalable and reproducible way of assessment but may not fully capture the emotional nuances and human-perceived quality of the generated content. Future research directions include developing more fine-grained subjective evaluation methods to better capture the subtle differences in emotional content and the complexity of human responses. At the same time, improving objective metrics to more accurately quantify various aspects of emotional expression and correlating them with human judgments is also an important area of research.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Future Perspectives",
      "text": "Advances in generative AI have revolutionized how we simulate human emotions, creating more authentic and nuanced emotional expressions. Looking ahead, several promising avenues for further research can be explored: Firstly, combining the capabilities of different generative models such as GANs, Seq2Seqs, AEs, DMs, and LLMs holds promise for further enhancing the quality of generated outputs. Each model has its unique strengths and limitations, and intelligently integrating them can compensate for the shortcomings of individual models, enabling more accurate and realistic emotion synthesis. By designing innovative hybrid architectures that leverage the strengths of each model, more powerful and comprehensive emotion synthesis systems can be developed. These hybrid models can seamlessly transition between different modalities, generating emotionally consistent and complementary outputs. For instance, a model combining DMs with the insights of Seq2Seq can generate high-quality emotional speech with appropriate facial expressions and lip synchronization.\n\nSecondly, the horizon of emotion synthesis is not limited to common modalities like facial images, speech, and text. As generative models continue to evolve, we may see multimodal human emotion synthesis results  [231] ,  [232]  in the future, including modalities beyond those mentioned in this paper, such as gesture  [233]  and physiological signals like EEG and ECG. In addition, emerging cross-modal generative models, such as text-to-image (T2I), text-to-video (T2V), and even text-to-3D, are poised to expand the creative and interactive potential of emotion synthesis. T2I models can generate imagery that resonates emotionally with written narratives, producing visuals that reflect subtle emotional undertones, while T2V models can bring stories to life by translating emotional content into animated, visually expressive scenes that engage audiences on a deeper emotional level. Moreover, as the technology matures, the potential for converting image-based emotions into sound (e.g., generating soundscapes that mirror the mood of a visual scene) opens up new dimensions for immersive experiences in fields like VR  [234]  and interactive entertainment.\n\nThirdly, the integration of generative models with edge devices-from server-based processing to smart terminals-marks a pivotal shift in the accessibility and application of emotion synthesis  [235] . As the computational power of edge devices continues to grow, there is a growing potential for real-time emotion generation and synthesis directly on devices such as smartphones, wearables, and VR headsets. This transition from centralized server processing to edge computing opens up a wide range of applications, enabling personalized, on-the-fly emotional interactions. For instance, smart devices can analyze users' facial expressions, voice tone, or even physiological signals, generating responsive emotional content that adapts to the user's immediate emotional state, location, or context. Additionally, as AI models become more efficient, smallerscale devices like wearables or even IoT sensors  [236]  can incorporate emotion-aware interactions, enhancing user experience in a range of industries-from healthcare, where emotion synthesis can assist in mental health monitoring and intervention, to retail, where it can personalize consumer experiences in-store or online. In this new paradigm, generative models are poised to provide immediate, adaptive emotional content, offering users a deeper connection to the digital world.\n\nFourthly, human emotion synthesis holds the potential to drive profound transformations across a variety of industries and can revolutionize domains such as digital entertainment and filmmaking  [237] ,  [238] . In the realm of digital entertainment, emotion synthesis lays the foundation for highly immersive experiences, enabling virtual characters and environments to express emotions with a level of authenticity rivaling that of human actors. By precisely generating emotional nuances in facial expressions, vocal tones, and body language, these technologies can elevate interactive media to new heights. In film production, AIdriven tools are already being employed to infuse emotional depth into character performances, achieving more dynamic and expressive storytelling that transcends the limitations of traditional physical acting. Moreover, the combination of AIGC video and emotion synthesis opens up new avenues for creation, empowering filmmakers to craft content with emotionally evocative visual and auditory cues. This presents opportunities for real-time emotional adjustments within films, where characters' emotional arcs can be dynamically altered based on audience feedback or narrative shifts, further immersing viewers in the experience  [239] . Through these advancements, the industry is entering a new chapter where the integration of emotion synthesis will unleash vast creative potential.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Conclusion",
      "text": "This review presents a detailed investigation of current generative technology for human emotion synthesis across various modalities, including facial images, speech, and text. It reveals how different genrative models, ranging from well-established approaches like AEs and GANs to emerging techniques such as DMs and LLMs, are capable of generating complex emotional expressions with remarkable depth and subtle nuances.\n\nIn Section 1, we introduce the background of human emotion synthesis, a pivotal area of research within affective computing. With the growing sophistication of generative models, which possess advanced data modeling and multimodal generation capabilities, new avenues for emotion synthesis are emerging. However, there is currently a lack of comprehensive reviews on this subject. To address this gap, we present the first survey of generative technology for human emotion synthesis, building upon existing literature and filling the void in current research. In Section 2, we outline our rigorous literature screening strategy, which allowed us to systematically collect and classify key research in the field of generative models for emotion synthesis. In Sections 3 to 6, we highlight the gaps between previous reviews and our survey, demonstrating the novelty and significance of our contribution. We also provide an overview of emotion models and mathematics of generative models, as well as commonly used datasets, offering a deeper understanding of the latest advancements in this interdisciplinary field.\n\nSections 7 to 9 provide a detailed discussion of the latest research on human emotion synthesis based on facial images, speech, and text. We categorize the specific tasks under each modality, discuss the applications of different generative models, and summarize the performance of existing works in comprehensive tables. We classify specific tasks within each modality, analyze the distribution of different models across these tasks, and discuss their strengths and limitations in various contexts. For each modality, we summarize the application models and performance of existing works in comprehensive tables. Finally, in Sections 10 and 11, we summarize the commonly used evaluation metrics for emotion synthesis and discuss the current state and future development trends in this field, providing insights into the challenges and opportunities faced by emotion synthesis.\n\nIn summary, this review highlights the transformative potential of existing generative models in shaping the future of human emotion synthesis. These advancements will not only enhance the granularity and authenticity of synthesized emotions but also usher in a new era where machines can resonate with the subtleties of human emotions, fostering deeper and more empathetic connections.",
      "page_start": 19,
      "page_end": 20
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Specifically, this paper",
      "page": 2
    },
    {
      "caption": "Figure 2: Facial emotion synthesis in-",
      "page": 2
    },
    {
      "caption": "Figure 1: Schematic Diagram of Generation Technology for Human Emo-",
      "page": 3
    },
    {
      "caption": "Figure 2: Taxonomy of This Survey.",
      "page": 3
    },
    {
      "caption": "Figure 3: Initially, we conducted compre-",
      "page": 3
    },
    {
      "caption": "Figure 3: A Comprehensive Review Methodology.",
      "page": 4
    },
    {
      "caption": "Figure 4: , the study of emotions has re-",
      "page": 4
    },
    {
      "caption": "Figure 4: Plutchik Wheel (left) and 2D Emotion Model (right).",
      "page": 4
    },
    {
      "caption": "Figure 5: A mask-based GAN [95] for face reenactment. The system",
      "page": 7
    },
    {
      "caption": "Figure 6: A DM-based talking head generation model from [23]. This archi-",
      "page": 8
    },
    {
      "caption": "Figure 6: , Zhang et al. presented EmoTalker",
      "page": 8
    },
    {
      "caption": "Figure 7: A two-stage model [145] for voice conversion. In the first stage,",
      "page": 11
    },
    {
      "caption": "Figure 8: , Li et al. created a system [164]",
      "page": 12
    },
    {
      "caption": "Figure 8: A Tacotron2-based TTS model from [164]. This system integrated",
      "page": 12
    },
    {
      "caption": "Figure 9: A text emotion transfer model from [183]. During training, the",
      "page": 14
    },
    {
      "caption": "Figure 9: , Riley et al. leveraged",
      "page": 14
    },
    {
      "caption": "Figure 10: , Song et al. pro-",
      "page": 14
    },
    {
      "caption": "Figure 10: A empathetic dialogue generation system from [198]. The",
      "page": 15
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Acronym": "AAE",
          "Full Form": "Adversarial Autoencoder"
        },
        {
          "Acronym": "ACC",
          "Full Form": "Accuracy"
        },
        {
          "Acronym": "AE",
          "Full Form": "Autoencoder"
        },
        {
          "Acronym": "AIGC",
          "Full Form": "Artificial Intelligence Generated Content"
        },
        {
          "Acronym": "AU",
          "Full Form": "Action Unit"
        },
        {
          "Acronym": "AUD",
          "Full Form": "AU-intensity Discriminator"
        },
        {
          "Acronym": "BLEU",
          "Full Form": "Bilingual Evaluation Understudy"
        },
        {
          "Acronym": "CAAE",
          "Full Form": "Conditional Adversarial Autoencoder"
        },
        {
          "Acronym": "CGAN",
          "Full Form": "Conditional GAN"
        },
        {
          "Acronym": "CWT",
          "Full Form": "Continuous Wavelet Transform"
        },
        {
          "Acronym": "DM",
          "Full Form": "Diffusion Model"
        },
        {
          "Acronym": "FID",
          "Full Form": "Fréchet Inception Distance"
        },
        {
          "Acronym": "F0 RMSE",
          "Full Form": "F0 Root Mean Square Error"
        },
        {
          "Acronym": "GAN",
          "Full Form": "Generative Adversarial Network"
        },
        {
          "Acronym": "GPT",
          "Full Form": "Generative Pre-trained Transformer"
        },
        {
          "Acronym": "MCD",
          "Full Form": "Mel Cepstral Distortion"
        },
        {
          "Acronym": "MFCC",
          "Full Form": "Mel-Frequency Cepstral Coefficient"
        },
        {
          "Acronym": "MOS",
          "Full Form": "Mean Opinion Score"
        },
        {
          "Acronym": "PPL",
          "Full Form": "Perplexity"
        },
        {
          "Acronym": "PSNR",
          "Full Form": "Peak Signal-to-Noise Ratio"
        },
        {
          "Acronym": "RNN",
          "Full Form": "Recurrent Neural Network"
        },
        {
          "Acronym": "Seq2Seq",
          "Full Form": "Sequence-to-Sequence"
        },
        {
          "Acronym": "SMOS",
          "Full Form": "Similarity Mean Opinion Score"
        },
        {
          "Acronym": "SSIM",
          "Full Form": "Structural Similarity Index"
        },
        {
          "Acronym": "TTS",
          "Full Form": "Text-to-Speech"
        },
        {
          "Acronym": "T5",
          "Full Form": "Text-to-Text Transfer Transformer"
        },
        {
          "Acronym": "VAE",
          "Full Form": "Variational Autoencoder"
        },
        {
          "Acronym": "VC",
          "Full Form": "Voice Conversion"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Reference": "Our work",
          "Related to\ngenerative models": "✓",
          "Related to\nemotion synthesis": "✓",
          "Focus on\nmultiple modalities": "✓"
        },
        {
          "Reference": "Kammoun et al. [52]",
          "Related to\ngenerative models": "✓*",
          "Related to\nemotion synthesis": "",
          "Focus on\nmultiple modalities": ""
        },
        {
          "Reference": "Liu et al. [53]",
          "Related to\ngenerative models": "✓*",
          "Related to\nemotion synthesis": "",
          "Focus on\nmultiple modalities": ""
        },
        {
          "Reference": "Triantafyllopoulos et al. [54]",
          "Related to\ngenerative models": "✓",
          "Related to\nemotion synthesis": "✓",
          "Focus on\nmultiple modalities": ""
        },
        {
          "Reference": "Wali et al. [55]",
          "Related to\ngenerative models": "✓*",
          "Related to\nemotion synthesis": "✓",
          "Focus on\nmultiple modalities": ""
        },
        {
          "Reference": "Zhang et al. [56]",
          "Related to\ngenerative models": "✓",
          "Related to\nemotion synthesis": "✓",
          "Focus on\nmultiple modalities": ""
        },
        {
          "Reference": "De et al. [57]",
          "Related to\ngenerative models": "✓*",
          "Related to\nemotion synthesis": "",
          "Focus on\nmultiple modalities": ""
        },
        {
          "Reference": "Hajarolasvadi et al. [9]",
          "Related to\ngenerative models": "✓*",
          "Related to\nemotion synthesis": "✓",
          "Focus on\nmultiple modalities": "✓"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 4: expression details across scales without the bidirectional",
      "data": [
        {
          "Model Type": "LLMs",
          "Belongs to\nSeq2Seq?": "Partially\nbelongs",
          "Description": "Some LLMs (e.g., T5, BART) are\nimplementations\nof\nSeq2Seq,\nbut\nLLMs\ncover\na\nbroader\nrange."
        },
        {
          "Model Type": "AEs",
          "Belongs to\nSeq2Seq?": "Does not\nbelong",
          "Description": "Similar\nto Seq2Seq in architec-\nture,\nbut with\ndifferent\ngoals\nand tasks."
        },
        {
          "Model Type": "GANs",
          "Belongs to\nSeq2Seq?": "Does not\nbelong",
          "Description": "Completely\ndifferent\nmodel\ntype with unrelated goals and\nstructures."
        },
        {
          "Model Type": "DMs",
          "Belongs to\nSeq2Seq?": "Does not\nbelong",
          "Description": "Entirely\ndifferent\ngenerative\nmodels, unrelated to Seq2Seq."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Database": "Oulu-CASIA [69]",
          "Year": "2011",
          "Modalities": "visual",
          "Samples": "480 sequences",
          "Subjects": "80",
          "Category": "happiness, surprise, sadness, anger, fear, disgust"
        },
        {
          "Database": "RaFD [70]",
          "Year": "2010",
          "Modalities": "visual",
          "Samples": "8040 images",
          "Subjects": "49",
          "Category": "sad, neutral, angry, contemptuous, disgusted, surprised, fearful, happy"
        },
        {
          "Database": "CK+ [71]",
          "Year": "2010",
          "Modalities": "visual",
          "Samples": "593 images",
          "Subjects": "123",
          "Category": "happiness, sadness, surprise, fear, anger, disgust, neutral, contempt"
        },
        {
          "Database": "CFEE [72]",
          "Year": "2014",
          "Modalities": "visual",
          "Samples": "229 images",
          "Subjects": "230",
          "Category": "happiness, surprise, sadness, anger, fear, disgust"
        },
        {
          "Database": "AffectNet [73]",
          "Year": "2017",
          "Modalities": "visual",
          "Samples": "450,000 images",
          "Subjects": "/",
          "Category": "happiness, sadness, surprise, fear, anger, disgust, neutral, contempt"
        },
        {
          "Database": "DISFA [74]",
          "Year": "2013",
          "Modalities": "visual",
          "Samples": "130,788 images",
          "Subjects": "27",
          "Category": "continuous annotation of graded changes in spontaneous facial expression of emotion"
        },
        {
          "Database": "EmotioNet [75]",
          "Year": "2016",
          "Modalities": "visual",
          "Samples": "1,000,000 images",
          "Subjects": "/",
          "Category": "23 basic or compound emotion categories(happy, sad, fearful, angrily surprised, sadly angry, etc.)"
        },
        {
          "Database": "ESD [76]",
          "Year": "2021",
          "Modalities": "audio",
          "Samples": "350 utterances",
          "Subjects": "20",
          "Category": "happy, sad, neutral, angry, surprise"
        },
        {
          "Database": "EmoV-DB [77]",
          "Year": "2018",
          "Modalities": "audio",
          "Samples": "7590 utterances",
          "Subjects": "5",
          "Category": "neutral, amused, angry, sleepy, disgust"
        },
        {
          "Database": "Emo-DB [78]",
          "Year": "2005",
          "Modalities": "audio",
          "Samples": "800 sentences",
          "Subjects": "10",
          "Category": "neutral, anger, fear,\njoy, sadness, disgust, boredom"
        },
        {
          "Database": "MEmoSD [79]",
          "Year": "2019",
          "Modalities": "audio",
          "Samples": "/",
          "Subjects": "4",
          "Category": "angry, happy, neutral, sad"
        },
        {
          "Database": "CaFE [80]",
          "Year": "2018",
          "Modalities": "audio",
          "Samples": "936 samples",
          "Subjects": "12",
          "Category": "neutral, sadness, happiness, anger, fear, disgust, surprise"
        },
        {
          "Database": "KES [81]",
          "Year": "2019",
          "Modalities": "audio",
          "Samples": "21,000 speeches",
          "Subjects": "1",
          "Category": "neutral, happy, sad, angry, surprised, fearful, disgusted"
        },
        {
          "Database": "ETOD [81]",
          "Year": "2019",
          "Modalities": "audio",
          "Samples": "6000 speeches",
          "Subjects": "13",
          "Category": "neutral, happy, sad, angry"
        },
        {
          "Database": "YELP review [26]",
          "Year": "/",
          "Modalities": "text",
          "Samples": "6,990,280 reviews",
          "Subjects": "/",
          "Category": "positive, negative"
        },
        {
          "Database": "AMAZON review [82]",
          "Year": "/",
          "Modalities": "text",
          "Samples": "/",
          "Subjects": "/",
          "Category": "positive, negative"
        },
        {
          "Database": "EmpatheticDialogue [27]",
          "Year": "2019",
          "Modalities": "text",
          "Samples": "24,850 conversations",
          "Subjects": "810",
          "Category": "32 emotion labels (suprised, excited, angry, proud, sad, annoyed, grateful, etc.)"
        },
        {
          "Database": "MojiTalk [83]",
          "Year": "2018",
          "Modalities": "text",
          "Samples": "662,159 conversations",
          "Subjects": "/",
          "Category": "64 emoji labels"
        },
        {
          "Database": "MEAD [84]",
          "Year": "2020",
          "Modalities": "visual + audio",
          "Samples": "281,400 clips",
          "Subjects": "60",
          "Category": "angry, disgust, contempt, fear, happy, sad, surprise, neutral"
        },
        {
          "Database": "CREMA-D [85]",
          "Year": "2014",
          "Modalities": "visual + audio",
          "Samples": "7442 utterances",
          "Subjects": "91",
          "Category": "happiness, surprise, sadness, anger, fear, disgust"
        },
        {
          "Database": "EmoVoxCeleb [86]",
          "Year": "2018",
          "Modalities": "visual + audio",
          "Samples": "153,500 tracks",
          "Subjects": "1251",
          "Category": "neutral, happiness, surprise, sadness, anger, disgust, fear, contempt"
        },
        {
          "Database": "SAVEE [87]",
          "Year": "2008",
          "Modalities": "visual + audio",
          "Samples": "480 utterances",
          "Subjects": "4",
          "Category": "neutral, anger, disgust, fear, happiness, sadness, surprise"
        },
        {
          "Database": "RAVDESS [88]",
          "Year": "2018",
          "Modalities": "visual + audio",
          "Samples": "7356 videos",
          "Subjects": "24",
          "Category": "Happiness, Sadness, Surprise, Fear, Anger, Disgust, Neutral, Contempt"
        },
        {
          "Database": "IEMOCAP [89]",
          "Year": "2008",
          "Modalities": "visual + audio + text",
          "Samples": "10,039 samples",
          "Subjects": "10",
          "Category": "categorical and continuous annotations"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Reference": "Ali et al. [94]",
          "Year": "2019",
          "Model": "TER-GAN",
          "Dataset": "Oulu-CASIA",
          "Performance": "User study"
        },
        {
          "Reference": "Shao et al. [96]",
          "Year": "2021",
          "Model": "WP2-GAN",
          "Dataset": "RaFD/CFEE",
          "Performance": "Accuracy: 89.47, 87.97/FID: 41.74, 24.91/SSIM: 0.6818, 0.6659"
        },
        {
          "Reference": "Tripathy et al. [90]",
          "Year": "2020",
          "Model": "GAN",
          "Dataset": "VoxCeleb",
          "Performance": "Image Quality Assesment scores: 25.02, 33.08"
        },
        {
          "Reference": "Zeng et al. [91]",
          "Year": "2020",
          "Model": "DAE-GAN",
          "Dataset": "VoxCeleb1/RaFD",
          "Performance": "SSIM: 0.65, 0.73/FID: 60.8, 13.8/User Study: 0.61"
        },
        {
          "Reference": "Strizhkova et al. [92]",
          "Year": "2021",
          "Model": "GAN",
          "Dataset": "MUG/MEAD",
          "Performance": "ECS: 0.88, 0.58/FID: 19.7, 25.5/ACD: 0.10, 0.13"
        },
        {
          "Reference": "Groth et al. [93]",
          "Year": "2020",
          "Model": "Encoder-Decoder",
          "Dataset": "MoCap",
          "Performance": "Ratings figure for perceived intensity and sincerity"
        },
        {
          "Reference": "Xue et al. [95]",
          "Year": "2024",
          "Model": "LSGAN",
          "Dataset": "RaFD/DISFA",
          "Performance": "FID: 31.121/PSNR: 23.417/SSIM: 0.858"
        },
        {
          "Reference": "Hu et al. [122]",
          "Year": "2023",
          "Model": "2CET-GAN",
          "Dataset": "CFEE/RaFD",
          "Performance": "FID: 31.1/IS: 1.55/Expression Transfer Score: 3.37"
        },
        {
          "Reference": "Choi et al. [123]",
          "Year": "2018",
          "Model": "StarGAN",
          "Dataset": "CelebA/RaFD",
          "Performance": "Classification error: 2.12%"
        },
        {
          "Reference": "Liu et al. [111]",
          "Year": "2022",
          "Model": "EvoGAN",
          "Dataset": "EmotioNet/RaFD",
          "Performance": "User study"
        },
        {
          "Reference": "Xie et al. [30]",
          "Year": "2023",
          "Model": "GAN",
          "Dataset": "FFHQ/RaFD/LFW",
          "Performance": "FID: 8.32-16.05/Accuracy: 97.42-98.61"
        },
        {
          "Reference": "Tang et al. [109]",
          "Year": "2020",
          "Model": "EGGAN",
          "Dataset": "RaFD/MMI",
          "Performance": "MSE: 0.24/PCC: 0.66"
        },
        {
          "Reference": "Patashnik et al. [110]",
          "Year": "2021",
          "Model": "StyleGAN",
          "Dataset": "FFHQ",
          "Performance": "User study"
        },
        {
          "Reference": "Sola et al. [113]",
          "Year": "2023",
          "Model": "ECGAN",
          "Dataset": "RAFDB",
          "Performance": "Accuracy: 78.54/SSIM: 0.52-0.64/FID: 244.75-668.68/Perceptual Loss: 1.21-1.46"
        },
        {
          "Reference": "Zhu et al. [124]",
          "Year": "2019",
          "Model": "UGAN",
          "Dataset": "CFEE",
          "Performance": "FID: 44.8"
        },
        {
          "Reference": "Ding et al. [125]",
          "Year": "2018",
          "Model": "ExprGAN",
          "Dataset": "Oulu-CASIA",
          "Performance": "User study"
        },
        {
          "Reference": "Tesei et al. [126]",
          "Year": "2019",
          "Model": "CCycleGAN",
          "Dataset": "FER2013",
          "Performance": "FID figure"
        },
        {
          "Reference": "Tang et al. [112]",
          "Year": "2019",
          "Model": "ECGAN",
          "Dataset": "AR/Yale/JAFFE/FERG/3DFE",
          "Performance": "AMT Score: 35.32 VGG Score: 78.13, 80.32"
        },
        {
          "Reference": "Wang et al. [127]",
          "Year": "2019",
          "Model": "Comp-GAN",
          "Dataset": "CelebA/F 2ED",
          "Performance": "Accuracy figure"
        },
        {
          "Reference": "Lindt et al. [114]",
          "Year": "2019",
          "Model": "CAAE",
          "Dataset": "AffectNet",
          "Performance": "RMSE: 0.528, 0.607/SAGR: 0.567, 0.483/CCC: 0.312, 0.210"
        },
        {
          "Reference": "Kong et al. [108]",
          "Year": "2021",
          "Model": "DualPathGAN",
          "Dataset": "EmoVoxCeleb/VoxCeleb2",
          "Performance": "PSNR: 32.56/SSIM: 0.953/FID: 9.20"
        },
        {
          "Reference": "Doukas et al. [97]",
          "Year": "2021",
          "Model": "HeadGAN",
          "Dataset": "VoxCeleb",
          "Performance": "FID: 50.9/FCD: 334/CSIM: 0.716"
        },
        {
          "Reference": "Zhao et al. [98]",
          "Year": "2021",
          "Model": "PAttGAN",
          "Dataset": "DISFA/DISFACat",
          "Performance": "ICC: 0.89, 0.88/MAE: 0.24, 0.28/MSE: 0.22, 0.29/FID: 27.32, 21.77"
        },
        {
          "Reference": "Xia et al. [99]",
          "Year": "2021",
          "Model": "LGP-GAN",
          "Dataset": "RaFD",
          "Performance": "IS: 1.31/FID: 11.88"
        },
        {
          "Reference": "Wang et al. [100]",
          "Year": "2019",
          "Model": "GAN",
          "Dataset": "EmotioNet/RaFD",
          "Performance": "User study"
        },
        {
          "Reference": "Akram et al. [101]",
          "Year": "2023",
          "Model": "SARGAN",
          "Dataset": "KDEF/CFEE/RaFD",
          "Performance": "ACD: 0.306/FVS: 94.18±1.11/FID: 53.95"
        },
        {
          "Reference": "Zhang et al. [102]",
          "Year": "2021",
          "Model": "SwitchGAN",
          "Dataset": "RaFD",
          "Performance": "FID: 17.4/Accuracy: 98.32"
        },
        {
          "Reference": "Akram et al. [103]",
          "Year": "2024",
          "Model": "US-GAN",
          "Dataset": "KDEF/RaFD/CFEE",
          "Performance": "ACD: 0.2832/FVS: 94.19±1.11/User study: 43% improvement(expression plausibility)"
        },
        {
          "Reference": "Azari et al. [104]",
          "Year": "2024",
          "Model": "StyleGAN",
          "Dataset": "CelebA/FFHQ",
          "Performance": "LPIPS: 0.07/FID: 7.86/ID: 0.88"
        },
        {
          "Reference": "Apolito et al. [105]",
          "Year": "2021",
          "Model": "GAN",
          "Dataset": "AffectNet",
          "Performance": "Smoothness score:0.33-0.38/ERE: 0.020, 0.018/FED: 0.71"
        },
        {
          "Reference": "Peng et al. [106]",
          "Year": "2019",
          "Model": "ApprGAN",
          "Dataset": "Bosphorus/CK+/MUG",
          "Performance": "Correlation coefficients: 0.972, 0.953, 0.975/Normalised distances: 2.061, 1.879, 1.835"
        },
        {
          "Reference": "Pumarola et al. [20]",
          "Year": "2018",
          "Model": "GAN",
          "Dataset": "EmotioNet/RaFD",
          "Performance": "User study"
        },
        {
          "Reference": "Ding et al. [125]",
          "Year": "2018",
          "Model": "ExprGAN",
          "Dataset": "Oulu-CASIA",
          "Performance": "User study"
        },
        {
          "Reference": "Song et al. [107]",
          "Year": "2018",
          "Model": "G2-GAN",
          "Dataset": "CK+/Oulu-CASIA",
          "Performance": "User study"
        },
        {
          "Reference": "Xu et al. [128]",
          "Year": "2024",
          "Model": "StyleGAN",
          "Dataset": "MEAD/RAVDESS",
          "Performance": "Realism: 0.40/Emotion similarity: 0.36/Mouth shape similarity: 0.43"
        },
        {
          "Reference": "Eskimez et al. [116]",
          "Year": "2021",
          "Model": "GAN",
          "Dataset": "CREMA-D",
          "Performance": "PSNR: 30.91/SSIM: 0.85/Accuracy: 55.3"
        },
        {
          "Reference": "Zhang et al. [23]",
          "Year": "2024",
          "Model": "Diffusion model",
          "Dataset": "MEAD/CREMA-D/FED",
          "Performance": "CSIMD: 0.67, 0.51/Accuracy: 84.76, 75.13"
        },
        {
          "Reference": "Vougioukas et al. [117]",
          "Year": "2020",
          "Model": "GAN",
          "Dataset": "CREMA-D",
          "Performance": "PSNR: 23.565/SSIM: 0.70/ACD: 1.40 · 10−4"
        },
        {
          "Reference": "Tan et al. [129]",
          "Year": "2023",
          "Model": "Encoder-Decoder",
          "Dataset": "CFD/MEAD/CREMA-D",
          "Performance": "Accuracy: 65.20/PSNR: 29.38, 30.03/SSIM: 0.66, 0.68/M-LMD: 2.78, 3.03/F-LMD: 2.87, 3.16"
        },
        {
          "Reference": "Tan et al. [130]",
          "Year": "2024",
          "Model": "Encoder-Decoder",
          "Dataset": "MEAD",
          "Performance": "SSIM: 0.795/FID: 23.207/M-LMD: 3.317/F-LMD: 2.696"
        },
        {
          "Reference": "Zhai et al. [131]",
          "Year": "2023",
          "Model": "Seq2Seq",
          "Dataset": "MEAD/CREMA-D",
          "Performance": "SSIM: 0.82, 0.79/PSNR: 30.29, 30.55/M-LMD: 2.14, 1.16/LMD: 2.44, 1.46/FID: 19.59, 42.53/EP: 80.52, 85.64"
        },
        {
          "Reference": "Sheng et al. [132]",
          "Year": "2023",
          "Model": "DVAE",
          "Dataset": "MEAD/CREMA-D",
          "Performance": "SSIM: 0.79, 0.92/LPIPS: 0.07, 0.08/LMD: 1.83, 1.34/LVD: 1.56, 0.84/CSIM: 0.88, 0.87/Emoacc: 0.87, 0.84"
        },
        {
          "Reference": "Gan et al. [119]",
          "Year": "2023",
          "Model": "Encoder-Decoder",
          "Dataset": "VoxCeleb2/MEAD",
          "Performance": "PSNR: 21.75/SSIM: 0.68/FID: 19.69/M-LMD: 2.25/F-LMD: 2.47/Accuracy: 75.43"
        },
        {
          "Reference": "Tan et al. [120]",
          "Year": "2024",
          "Model": "Diffusion model",
          "Dataset": "HDTF/MEAD",
          "Performance": "SSIM: 0.708, 0.689/FID: 15.165, 16.553/M-LMD: 1.643, 1.939/F-LMD: 1.958, 2.061/Accuracy: 71.53"
        },
        {
          "Reference": "Tan et al. [121]",
          "Year": "2024",
          "Model": "Encoder-Decoder",
          "Dataset": "HDTF/MEAD",
          "Performance": "PSNR: 26.504, 22.771/SSIM: 0.845, 0.769/FID: 13.172, 15.548/M-LMD: 1.197, 1.102/F-LMD: 1.111, 1.060/Accuracy: 68.85"
        },
        {
          "Reference": "Ma et al. [133]",
          "Year": "2023",
          "Model": "Diffusion model",
          "Dataset": "MEAD/HDTF/Voxceleb2",
          "Performance": "SSIM: 0.86, 0.85, 0.69/CPBD: 0.16, 0.31, 0.30/F-LMD: 1.93, 1.80, 2.69/M-LMD: 2.91, 2.15, 2.72"
        },
        {
          "Reference": "Zhang et al. [134]",
          "Year": "2023",
          "Model": "Diffusion model",
          "Dataset": "MEAD/HDTF",
          "Performance": "LPIPS: 0.169, 0.176/CPBD: 0.299, 0.280/F-LMD: 3.845, 3.948"
        },
        {
          "Reference": "Zeng et al. [118]",
          "Year": "2020",
          "Model": "ET-GAN",
          "Dataset": "CREMA-D/GRID",
          "Performance": "PSNR: 23.981, 25.771/SSIM: 0.733, 0.810/FID: 76.92, 61.33"
        },
        {
          "Reference": "Sun et al. [135]",
          "Year": "2023",
          "Model": "StyleGAN",
          "Dataset": "MEAD/RAVDESS",
          "Performance": "LIE: 0.739/CPBD: 0.247, 0.166/FED: 9.40/FID: 30.36/ID: 0.931/LSE-C: 7.11/LSE-D: 8.03"
        },
        {
          "Reference": "Wang et al. [136]",
          "Year": "2024",
          "Model": "Diffusion model",
          "Dataset": "MEAD/CREMA-D",
          "Performance": "PSNR: 32.6131, 34.3401/CPBD: 0.3803, 0.5180/Emo-Acc: 74.57"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Reference": "Zhou et al. [76]",
          "Year": "2021",
          "Model": "VAW-GAN",
          "Dataset": "ESD/IEMOCAP",
          "Performance": "MCD: 4.127-4.916/MOS: 3.24, 2.94, 3.15/Preference test"
        },
        {
          "Reference": "Fu et al. [141]",
          "Year": "2022",
          "Model": "CycleGAN",
          "Dataset": "Japanese emotional speech dataset/ESD",
          "Performance": "MOS figures/MCD: 5.65, 16.38/F0 RMSE: 44.91, 105.99"
        },
        {
          "Reference": "Zhou et al. [138]",
          "Year": "2021",
          "Model": "VAW-GAN",
          "Dataset": "EmoV-DB",
          "Performance": "MCD: 4.085, 4.278/LSD: 5.681, 6.106/F0 RMSE: 61.712, 52.348/PCC: 0.893, 0.867/Preference test"
        },
        {
          "Reference": "Zhou et al. [139]",
          "Year": "2020",
          "Model": "CycleGAN",
          "Dataset": "Emotional speech corpus",
          "Performance": "MCD: 10.23, 8.71/F0 RMSE: 65.05, 63.03/PCC: 0.76, 0.76/Preference test"
        },
        {
          "Reference": "Rizos et al. [142]",
          "Year": "2020",
          "Model": "StarGAN",
          "Dataset": "IEMOCAP",
          "Performance": "Spectrogram representation/Human evalution score"
        },
        {
          "Reference": "Zhou et al. [137]",
          "Year": "2020",
          "Model": "VAW-GAN",
          "Dataset": "EmoV-DB/English emotional speech corpus/JL-Corpus",
          "Performance": "MCD: 4.439, 4.683/LSD: 6.161, 6.275/PCC: 0.776, 0.691/MOS: 2.808"
        },
        {
          "Reference": "Gao et al. [144]",
          "Year": "2018",
          "Model": "AE",
          "Dataset": "IEMOCAP",
          "Performance": "Emotion conversion MOS: 48%/Speaker similarity MOS: 3.55"
        },
        {
          "Reference": "Zhou et al. [151]",
          "Year": "2022",
          "Model": "Seq2Seq",
          "Dataset": "VCTK/ESD",
          "Performance": "MOS figures/MCD: 4.13, 4.15, 4.25/DDUR: 0.24, 0.17, 0.31"
        },
        {
          "Reference": "Oh et al. [146]",
          "Year": "2024",
          "Model": "AE Diffusion model",
          "Dataset": "ESD",
          "Performance": "nMOS: 3.70/sMOS: 3.63/eMOC: 72.97/UTMOS: 3.58/ECA: 91.58/SECS: 74.83"
        },
        {
          "Reference": "Elgaar et al. [143]",
          "Year": "2020",
          "Model": "VAE",
          "Dataset": "Collected dataset/IEMOCAP",
          "Performance": "Subjective evalution figures"
        },
        {
          "Reference": "Kreuk et al. [152]",
          "Year": "2022",
          "Model": "Seq2Seq",
          "Dataset": "VCTK/EmoV",
          "Performance": "MOS figures/eMOC figures"
        },
        {
          "Reference": "Du et al. [150]",
          "Year": "2021",
          "Model": "VAE",
          "Dataset": "ESD",
          "Performance": "MCD figures/F0 RMSE figures/SV accuracy figures/MOS: 3.53, 3.58, 3.74/Preference test"
        },
        {
          "Reference": "Du et al. [147]",
          "Year": "2021",
          "Model": "StarGAN",
          "Dataset": "ESD",
          "Performance": "MCD figures/MOS: 3.044/Preference test"
        },
        {
          "Reference": "Chen et al. [145]",
          "Year": "2023",
          "Model": "AE",
          "Dataset": "ESD",
          "Performance": "MCD: 4.596/ACC: 0.830/RMSE: 0.117/Emotion similarity: 76.12%"
        },
        {
          "Reference": "Meftah et al. [148]",
          "Year": "2023",
          "Model": "StarGAN",
          "Dataset": "ESD",
          "Performance": "MCD,F0 RMSE figures/Waveforms figures/Confusion matrices/Spectrograms figures"
        },
        {
          "Reference": "Shah et al. [149]",
          "Year": "2023",
          "Model": "StarGAN",
          "Dataset": "Hindi Emotional Speech Database",
          "Performance": "MOS: 4.21±0.01/Similarity: 0.76/EmoAcc: 41.50/Preference test"
        },
        {
          "Reference": "Choi et al. [153]",
          "Year": "2021",
          "Model": "Seq2Seq",
          "Dataset": "Plain-to-emotional dataset",
          "Performance": "MOS: 4.14/MCD: 5.778, 16.055/Emotion confusion matrices/Emotion similarity figures"
        },
        {
          "Reference": "Qi et al. [170]",
          "Year": "2024",
          "Model": "CVAE",
          "Dataset": "ESD",
          "Performance": "MCD: 4.06/RMSE: 38.28/DDUR: 0.21/MSD: 19.61, 20.55, 21.54, 22.24"
        },
        {
          "Reference": "Zhang et al. [18]",
          "Year": "2023",
          "Model": "AE",
          "Dataset": "Multi-S60-E3/Child-S1-E6/VA-S2/Read-S40",
          "Performance": "MOS: 4.09±0.03, 3.76±0.05, 4.10±0.03 (Emotion Similarity,Speaker Similarity,Voice Quality)"
        },
        {
          "Reference": "Lei et al. [154]",
          "Year": "2021",
          "Model": "Seq2Seq",
          "Dataset": "Internal corpus",
          "Performance": "MCD: 4.91, 5.03/F0 trajectories"
        },
        {
          "Reference": "Li et al. [155]",
          "Year": "2023",
          "Model": "Diffusion model",
          "Dataset": "5 female monolingual speakers",
          "Performance": "Emotion similarity DMOS: 3.90-4.04/Cosine Similarity: 0.25, 0.73"
        },
        {
          "Reference": "Tits et al. [156]",
          "Year": "2020",
          "Model": "Seq2Seq",
          "Dataset": "EmoV-DB",
          "Performance": "MOS: 2.00, 2.10, 2.27, 3.59, 3.29 (amused, angry, disgusted, neutral, sleepy)"
        },
        {
          "Reference": "Schnell et al. [157]",
          "Year": "2021",
          "Model": "Seq2Seq",
          "Dataset": "SAVEE/IEMOCAP/WSJCAM0",
          "Performance": "Accuracy: 35.5, 28.9(total, emo)"
        },
        {
          "Reference": "Wu et al. [158]",
          "Year": "2019",
          "Model": "Seq2Seq",
          "Dataset": "Emotional speech corpus",
          "Performance": "MCD: 2.64/F0 RMSE: 64.4/V/UV: 8.26/FFE: 21.81"
        },
        {
          "Reference": "Um et al. [159]",
          "Year": "2020",
          "Model": "Seq2Seq",
          "Dataset": "Korean male voice database",
          "Performance": "MOS: 3.90±0.54/Recognition acurracy/Preference test"
        },
        {
          "Reference": "Im et al. [81]",
          "Year": "2022",
          "Model": "Seq2Seq",
          "Dataset": "KES/ETOD",
          "Performance": "MOS: 3.72, 3.95/MCD: 4.81, 2.94/F0 RMSE: 53.15, 30.61/EmoAcc: 99.39, 86.85"
        },
        {
          "Reference": "Hortal et al. [160]",
          "Year": "2021",
          "Model": "Seq2Seq GAN",
          "Dataset": "LJ Speech/VESUS/CREMA-D/RAVDESS",
          "Performance": "Accuracy"
        },
        {
          "Reference": "Guo et al. [161]",
          "Year": "2023",
          "Model": "Diffusion model",
          "Dataset": "ESD",
          "Performance": "MOS: 4.13±0.10/MCD: 5.94/Classification accuracy/Preference test"
        },
        {
          "Reference": "Kang et al. [171]",
          "Year": "2023",
          "Model": "Diffusion model",
          "Dataset": "Multi-emotional dataset/ESD/LibriTTS Test",
          "Performance": "ECA: 51.59, 38.89, 39.86, 32.57/MOS: 3.44, 3.31/SMOS: 3.22"
        },
        {
          "Reference": "Zhu et al. [172]",
          "Year": "2019",
          "Model": "Seq2Seq",
          "Dataset": "Emotional speech corpus",
          "Performance": "Mel spectrograms/Pitch/PCA ordination diagram trajectories"
        },
        {
          "Reference": "Tang et al. [166]",
          "Year": "2023",
          "Model": "Diffusion model",
          "Dataset": "IEMOCAP/ESD",
          "Performance": "MOS: 4.10, 3.92/SMOS: 4.02, 3.82/MCD: 5.29, 5.65"
        },
        {
          "Reference": "Lei et al. [173]",
          "Year": "2022",
          "Model": "GAN",
          "Dataset": "Internal corpus",
          "Performance": "Emotion MOS: 4.04, 4.01, 3.86/Pearson Correlation: 0.776, 0.790, 0.759, 0.794/F0 curves"
        },
        {
          "Reference": "Lei et al. [163]",
          "Year": "2022",
          "Model": "Seq2Seq",
          "Dataset": "Emotional speech corpus",
          "Performance": "MCD: 3.63/MOS: 4.02±0.119/CMOS: 0.520, 0.342/Preference: 58.1, 51.2/F0 curves"
        },
        {
          "Reference": "Li et al. [162]",
          "Year": "2021",
          "Model": "Seq2Seq",
          "Dataset": "Emotional speech corpus",
          "Performance": "Peference test/Strength confusion matrices/pitch trajectories"
        },
        {
          "Reference": "Li et al. [164]",
          "Year": "2022",
          "Model": "Seq2Seq",
          "Dataset": "DB_1/AIC/DB_6",
          "Performance": "Emotion similarity DMOS: 3.71±0.066/Cosine simlarity: 0.28, 0.60"
        },
        {
          "Reference": "Li et al. [165]",
          "Year": "2021",
          "Model": "Seq2Seq",
          "Dataset": "Internal corpus",
          "Performance": "MOS: 4.155±0.552, 4.136±0.701"
        },
        {
          "Reference": "Cai et al. [174]",
          "Year": "2021",
          "Model": "Seq2Seq",
          "Dataset": "IEMOCAP/BC2013-English/RECOLA",
          "Performance": "MOS: 3.66 Accuracy: 78.75, 91.0"
        },
        {
          "Reference": "Wang et al. [175]",
          "Year": "2023",
          "Model": "Seq2Seq",
          "Dataset": "EmoV-DB",
          "Performance": "MCD: 4.66/MOS: 3.76±0.03/Accuracy: 0.67, 0.67, 0.77/preference test"
        },
        {
          "Reference": "Zhou et al. [176]",
          "Year": "2022",
          "Model": "Seq2Seq",
          "Dataset": "VCTK/ESD",
          "Performance": "MCD figures/PCC figures/MOS: 3.21-3.81/BWS test"
        },
        {
          "Reference": "Diatlova et al. [177]",
          "Year": "2023",
          "Model": "Seq2Seq",
          "Dataset": "ESD",
          "Performance": "MOS: 4.37/NISQA: 4.1"
        },
        {
          "Reference": "Guan et al. [178]",
          "Year": "2024",
          "Model": "Seq2Seq",
          "Dataset": "MEAD-TTS",
          "Performance": "MOS: 4.36, 3.95, 4.11, 3.77, 4.19, 3.93/SMOS: 4.61, 4.03 /MCD: 3.17, 6.69/EmoAcc: 0.659, 0.261, 0.636, 0.318"
        },
        {
          "Reference": "Zhou et al. [179]",
          "Year": "2024",
          "Model": "Encoder-Decoder",
          "Dataset": "ESD/LibriTTS",
          "Performance": "MOS: 3.67/CMOS: 0.64, 0.71, 0.15, 0.92/preference test"
        },
        {
          "Reference": "Tang et al. [180]",
          "Year": "2024",
          "Model": "Diffusion model",
          "Dataset": "IEMOCAP",
          "Performance": "MOS: 4.12/SMOS: 4.10/ERA: 0.749/EDER: 27.8"
        },
        {
          "Reference": "Jing et al. [181]",
          "Year": "2024",
          "Model": "Diffusion model",
          "Dataset": "ESD/MSP-Podcast",
          "Performance": "MOS-Q: 3.88/MOS-S: 3.36, 3.22, 3.05, 3.34/preference test"
        },
        {
          "Reference": "Jia et al. [79]",
          "Year": "2019",
          "Model": "ET-GAN",
          "Dataset": "IEMOCAP/Emo-DB/CaFE/MEmoSD",
          "Performance": "FAD,naturalness MOS figures/Preference test"
        },
        {
          "Reference": "Inoue et al. [36]",
          "Year": "2024",
          "Model": "Seq2Seq",
          "Dataset": "Blizzard/ESD",
          "Performance": "MOS: 3.596±0.141/MCD: 4.348/Pitch Distortion: 1.151/Energy Distortion: 4.018/FD: 6.881/BWS test"
        },
        {
          "Reference": "Wang et al. [168]",
          "Year": "2024",
          "Model": "CampNet",
          "Dataset": "VCTK/ESD",
          "Performance": "F0 curve/MCD: 3.078, 3.495, 3.528, 3.425, 3.332/Preference test/MOS figures"
        },
        {
          "Reference": "Matsumoto et al. [167]",
          "Year": "2020",
          "Model": "WaveNet",
          "Dataset": "JSUT corpus",
          "Performance": "MOS figures/F0 distribution/Confusion matrices (Subject-perceived emotions)"
        },
        {
          "Reference": "Shi et al. [182]",
          "Year": "2024",
          "Model": "Encoder-Decoder",
          "Dataset": "ESD",
          "Performance": "MOS: 3.98/SMOS: 4.15/EmoAcc: 99.31/MCD: 4.81"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Reference": "Mohammadibaghmolaei et al. [184]",
          "Year": "2023",
          "Model": "LLM",
          "Dataset": "ISEAR/TEC",
          "Performance": "Transfer strength figures/Content preservation: 0.8403-0.8967/Fluency: 164.9118-220.4884"
        },
        {
          "Reference": "Li et al. [26]",
          "Year": "2018",
          "Model": "Seq2Seq",
          "Dataset": "YELP/CAPTIONS/AMAZON",
          "Performance": "Human evalution/BLEU: 11.8, 17.1, 27.1/Classifier Accuracy: 95.4, 96.8, 70.3"
        },
        {
          "Reference": "Jin et al. [185]",
          "Year": "2019",
          "Model": "Seq2Seq",
          "Dataset": "YELP/FORMALITY",
          "Performance": "Human evalution/Accuracy: 95.90, 72.07/BLEU: 22.46, 38.16/PPL: 14.89, 32.63"
        },
        {
          "Reference": "Wu et al. [186]",
          "Year": "2019",
          "Model": "LLM",
          "Dataset": "YELP/AMAZON",
          "Performance": "Human evalution/ACC: 97.3, 84.5/BLEU: 15.9, 32.1"
        },
        {
          "Reference": "Malmi et al. [187]",
          "Year": "2020",
          "Model": "LaserTagger",
          "Dataset": "YELP",
          "Performance": "BLEU: 14.5, 15.3/ACC: 40.9, 49.6"
        },
        {
          "Reference": "Yang et al. [188]",
          "Year": "2018",
          "Model": "Encoder-Decoder LM",
          "Dataset": "YELP",
          "Performance": "ACC: 90.0, 85.4/BLEU: 22.3, 13.4/P P LX : 48.4, 32.8/P P LY : 61.6, 40.5"
        },
        {
          "Reference": "Shen et al. [189]",
          "Year": "2017",
          "Model": "AE",
          "Dataset": "YELP",
          "Performance": "Accuracy:78.4/Human evaluation (sentiment: 62.6,fluency: 2.8,overall transfer quality: 41.5)"
        },
        {
          "Reference": "Zhang et al. [190]",
          "Year": "2019",
          "Model": "Seq2Seq GAN",
          "Dataset": "YELP/AMAZON",
          "Performance": "Sentiment Transfer Strength: 81.7, 69.2/Cosine Similarity: 95.0, 92.2/Word Overlap: 83.9, 75.4"
        },
        {
          "Reference": "Luo et al. [192]",
          "Year": "2019",
          "Model": "Seq2Seq",
          "Dataset": "YELP",
          "Performance": "BLEU: 32.5, 10.3/MAE: 0.13/MRRR: 0.78/PPL: 35.1/Avg human evaluation: 3.96"
        },
        {
          "Reference": "Reif et al. [212]",
          "Year": "2022",
          "Model": "LLM",
          "Dataset": "YELP/GYAFC",
          "Performance": "Human evalution figures/ACC: 90.6/BLEU: 10.4/PPL: 79"
        },
        {
          "Reference": "Yi et al. [213]",
          "Year": "2021",
          "Model": "Encoder-Decoder",
          "Dataset": "YELP",
          "Performance": "ACC: 90.8/BLEU: 26.3/Cos: 96/PPL: 109/GM: 14.83"
        },
        {
          "Reference": "Li et al. [214]",
          "Year": "2020",
          "Model": "Dual-Generator Network",
          "Dataset": "YELP/IMDb",
          "Performance": "ACC: 88.0, 70.1/BLEU: 54.5, 70.2"
        },
        {
          "Reference": "Huang et al. [191]",
          "Year": "2020",
          "Model": "AAE",
          "Dataset": "YELP",
          "Performance": "Tranfer: 86.9%/BLEU: 22.51/PPL: 21.6/RPPL: 57.0"
        },
        {
          "Reference": "Riley et al. [183]",
          "Year": "2021",
          "Model": "LLM",
          "Dataset": "AMAZON",
          "Performance": "ACC: 73.7, 54.9/Content preservation: 34.7, 55.8/Sentiment: 2.5/Preservation: 2.6/Fluency: 4.0"
        },
        {
          "Reference": "Sancheti et al. [215]",
          "Year": "2020",
          "Model": "Seq2Seq",
          "Dataset": "YELP",
          "Performance": "BLEU: 0.153,0.088/Accuracy: 0.922, 0.744/Human evaluation"
        },
        {
          "Reference": "Chawla et al. [216]",
          "Year": "2020",
          "Model": "Encoder-Decoder LM",
          "Dataset": "GYAFC/YELP/AMAZON",
          "Performance": "ACC: 86.2, 68.9/BLEU: 14.1, 28.6"
        },
        {
          "Reference": "Li et al. [193]",
          "Year": "2024",
          "Model": "LLM",
          "Dataset": "IEMOCAP/DailyDialog/Empathetic-Dialogues/ESConv/PENS",
          "Performance": "EGS: 36.02, 36.42, 36.21, 36.70, 33.48"
        },
        {
          "Reference": "Yang et al. [194]",
          "Year": "2024",
          "Model": "LLM",
          "Dataset": "EmpatheticDialogue dataset",
          "Performance": "Acc: 45.63/Distinct-1: 42.23/Distinct-2: 80.08"
        },
        {
          "Reference": "Sun et al. [195]",
          "Year": "2023",
          "Model": "LLM",
          "Dataset": "EmpatheticDialogue dataset",
          "Performance": "Human evaluation/PPL: 18.86/ACC: 53.44"
        },
        {
          "Reference": "Lee et al. [196]",
          "Year": "2022",
          "Model": "LLM",
          "Dataset": "EmpatheticDialogue dataset",
          "Performance": "Human evaluation/EMOACC: 0.1683/Explorations: 0.4970/Interpretations: 0.2780"
        },
        {
          "Reference": "Casas et al. [24]",
          "Year": "2021",
          "Model": "LLM",
          "Dataset": "DD/ED dataset",
          "Performance": "Emotion Reflection: 0.465/Emotional: 0.487/Empathy Score: 0.443/PPL: 149.8"
        },
        {
          "Reference": "Chen et al. [197]",
          "Year": "2023",
          "Model": "LLM",
          "Dataset": "SoulChat-Corpus/SMILECHAT",
          "Performance": "BLEU: 33.78, 20.07,12.86, 8.52/ROUGE: 31.47, 8.92, 26.57/Emp: 1.84/Human evaluation"
        },
        {
          "Reference": "Liu et al. [217]",
          "Year": "2022",
          "Model": "LLM",
          "Dataset": "EmpatheticDialogues dataset",
          "Performance": "Emotion accuracy: 0.5262/Perplexity: 13.57/Dist-1: 2.04/Dist-2: 11.68"
        },
        {
          "Reference": "Qian et al. [218]",
          "Year": "2023",
          "Model": "LLM",
          "Dataset": "EmpatheticDialogues dataset",
          "Performance": "Dist-1: 2.96/Dist-2: 18.29/BERTscore: 0.8803, 0.8816, 0.8774/BLEU-2: 9.37/BLEU-4: 3.26/Human evaluation"
        },
        {
          "Reference": "Song et al. [198]",
          "Year": "2019",
          "Model": "Seq2Seq",
          "Dataset": "STC/NLPCC",
          "Performance": "Embedding score: 0.634, 0.451, 0.435/BLUE: 1.73/Dist-1: 0.0113/Dist-2: 0.0867/emotion-a: 0.810/emotion-w: 0.687"
        },
        {
          "Reference": "Zhou et al. [199]",
          "Year": "2018",
          "Model": "Seq2Seq",
          "Dataset": "STC/NLPCC",
          "Performance": "Perplexity: 61.8/Accuracy: 0.773/Human evalution"
        },
        {
          "Reference": "Kong et al. [200]",
          "Year": "2019",
          "Model": "CGAN Seq2Seq CAVE",
          "Dataset": "MojiTalk dataset",
          "Performance": "PPL: 69.54/ Sentiment Acc: 78.8, 78.9/Quality: 3.9"
        },
        {
          "Reference": "Li et al. [201]",
          "Year": "2021",
          "Model": "CAVE",
          "Dataset": "NLPCC2017/Weibo dataset/MojiTalk dataset",
          "Performance": "PPL: 25.70, 23.60, 33.7/Dist-1: 0.109, 0.017, 0.105/Dist-2: 0.400, 0.225, 0.517/Human evalution"
        },
        {
          "Reference": "Xu et al. [202]",
          "Year": "2019",
          "Model": "CVAE Seq2Seq",
          "Dataset": "NLPCC2017",
          "Performance": "PPL: 34.6/Accuracy: 0.637/Dist-1: 0.3315/Dist-2: 0.7900/Dist-3: 0.9023/Human evalution"
        },
        {
          "Reference": "Asghar et al. [203]",
          "Year": "2018",
          "Model": "Seq2Seq",
          "Dataset": "Cornell Movie Dialogs Corpus",
          "Performance": "Syntactic Coherence: 1.76/Naturalness: 1.09/Emotional Appropriateness: 1.10"
        },
        {
          "Reference": "Colombo et al. [204]",
          "Year": "2019",
          "Model": "Seq2Seq",
          "Dataset": "OpenSubtitles2018/Cornell",
          "Performance": "Dist-1: 0.0406, 0.0305/Dist-2: 0.2030, 0.1431/BLEU: 0.0140, 0.110/Hyper-parameter optimization"
        },
        {
          "Reference": "Huang et al. [205]",
          "Year": "2018",
          "Model": "Seq2Seq",
          "Dataset": "CBET/OpenSubtitles dataset",
          "Performance": "Average Acc: 76.69, 75.91, 78.46"
        },
        {
          "Reference": "Lin et al. [206]",
          "Year": "2022",
          "Model": "CVAE",
          "Dataset": "EmpatheticDialogues dataset",
          "Performance": "PPL: 19.6/Diversity: 0.0208, 0.1404, 0.3976/Human evaluation"
        },
        {
          "Reference": "Firdaus et al. [219]",
          "Year": "2020",
          "Model": "CVAE",
          "Dataset": "SEMD",
          "Performance": "PPL: 34.8/Sentiment Acc:0.85/Emotion Acc: 0.80/Dist-1: 0.0203/Dist-2: 0.0520/Human evaluation"
        },
        {
          "Reference": "Majumder et al. [220]",
          "Year": "2020",
          "Model": "Encoder-Decoder",
          "Dataset": "EmpatheticDialogues dataset",
          "Performance": "BLEU: 2.98/Human evaluation/Preference test"
        },
        {
          "Reference": "Sabour et al. [221]",
          "Year": "2022",
          "Model": "Encoder-Decoder",
          "Dataset": "EmpatheticDialogues dataset",
          "Performance": "PPL: 35.60/Dist-1: 0.66/Dist-2: 2.99/Acc: 39.11/Human evaluation"
        },
        {
          "Reference": "Li et al. [222]",
          "Year": "2019",
          "Model": "Encoder-Decoder",
          "Dataset": "NLPCC2017",
          "Performance": "PPL: 62.2, 61, 61.4/Accuracy: 0.871, 0.870, 0.869/Human evaluation"
        },
        {
          "Reference": "Wang et al. [208]",
          "Year": "2019",
          "Model": "GAN",
          "Dataset": "MR/BR/CR/Emotional tweet conversation",
          "Performance": "Sentiment Acc: 0.885, 0.841, 0.803/Novelty: 0.395, 0.427, 0.549/Diversity: 0.741, 0.713, 0.708/Human evaluation"
        },
        {
          "Reference": "Chen et al. [209]",
          "Year": "2021",
          "Model": "GAN",
          "Dataset": "NLPW/XHJ",
          "Performance": "Acc: 0.701-0.870/Fluency score: 6.300-11.33/Human evaluation"
        },
        {
          "Reference": "Bi et al. [210]",
          "Year": "2023",
          "Model": "Diffusion model",
          "Dataset": "EmpatheticDialogues dataset",
          "Performance": "BERTScore: 0.5205/MIScore: 626.92/Acc: 92.36, 84.24/F1-SF: 52.79/Dist-1, 2, 4: 2.84, 29.25, 73.45/Self-BLEU: 1.09"
        },
        {
          "Reference": "Li et al. [223]",
          "Year": "2020",
          "Model": "GAN Seq2Seq",
          "Dataset": "NLPCC2017",
          "Performance": "PPL: 62.8/Acc: 0.735/Distinct: 0.0287, 0.3062/Human evaluation"
        },
        {
          "Reference": "Chen et al. [211]",
          "Year": "2020",
          "Model": "GAN",
          "Dataset": "Yelp restaurant reviews/Amazon reviews/Film review/Obama speech",
          "Performance": "Similarity: 0.1-0.2/Sentiment figures/Average Acc: 0.56"
        },
        {
          "Reference": "Gu et al. [224]",
          "Year": "2024",
          "Model": "LLM",
          "Dataset": "EmpatheticDialogues dataset",
          "Performance": "Dist-1: 2.98/Dist-2: 18.38/B-2: 7.64/B-4: 2.57/PBERT: 87.29/RBERT: 88.04/FBERT:87.63/Human evaluation(Empaythy): 4.17"
        },
        {
          "Reference": "Chen et al. [225]",
          "Year": "2024",
          "Model": "LLM",
          "Dataset": "EmpatheticDialogues dataset",
          "Performance": "Acc: 52.73/Dist-1: 2.96/Dist-2: 19.52/BLEU-2: 10.54/BLEU-4: 5.17"
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Modalities": "Common",
          "Metrics": "User\nstudy",
          "Description": "Assesses\nthe\nquality\nand\nrealism of\nsynthesized emotion contents based on\nscoring by human participants."
        },
        {
          "Modalities": "",
          "Metrics": "Accuracy",
          "Description": "Measures the percentage of correct pre-\ndictions in emotion classification tasks."
        },
        {
          "Modalities": "Facial\nEmotion\nSynthesis",
          "Metrics": "PSNR",
          "Description": "Quantifies\nthe\nsimilarity\nbetween\nthe\nsynthesized and original\nimages, where\nhigher values indicate better quality."
        },
        {
          "Modalities": "",
          "Metrics": "SSIM",
          "Description": "Assesses\nthe perceived visual\nquality\nby comparing structural information be-\ntween the original\nand generated im-\nages."
        },
        {
          "Modalities": "",
          "Metrics": "FID",
          "Description": "Evaluates\nthe\ndistribution\ndifference\nbetween real\nand synthesized images\nby comparing deep feature representa-\ntions."
        },
        {
          "Modalities": "Speech\nEmotion\nSynthesis",
          "Metrics": "MCD",
          "Description": "Measures\nthe\ndistortion\nbetween\nthe\nsynthesized and reference speech in the\ncepstral domain."
        },
        {
          "Modalities": "",
          "Metrics": "F0 RMSE",
          "Description": "Quantifies\nthe difference\nin pitch be-\ntween synthesized and real speech."
        },
        {
          "Modalities": "Textual\nEmotion\nSynthesis",
          "Metrics": "BLEU",
          "Description": "Computes the lexical similarity between\nsynthesized and reference text."
        },
        {
          "Modalities": "",
          "Metrics": "PPL",
          "Description": "Measures the fluency of generated text,\nwith lower values indicating more pre-\ndictable and coherent sentences."
        }
      ],
      "page": 17
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Affective computing",
      "authors": [
        "R Picard"
      ],
      "year": "2000",
      "venue": "Affective computing"
    },
    {
      "citation_id": "2",
      "title": "Neuro or symbolic? fine-tuned transformer with unsupervised lda topic clustering for text sentiment analysis",
      "authors": [
        "F Ding",
        "X Kang",
        "F Ren"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "3",
      "title": "Affective computing in education: A systematic review and future research",
      "authors": [
        "E Yadegaridehkordi",
        "N Noor",
        "M Ayub",
        "H Affal",
        "N Hussin"
      ],
      "year": "2019",
      "venue": "Computers & education"
    },
    {
      "citation_id": "4",
      "title": "An efficient framework for constructing speech emotion corpus based on integrated active learning strategies",
      "authors": [
        "F Ren",
        "Z Liu",
        "X Kang"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "5",
      "title": "A survey of textual emotion recognition and its challenges",
      "authors": [
        "J Deng",
        "F Ren"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "6",
      "title": "Prompt consistency for multilabel textual emotion detection",
      "authors": [
        "Y Zhou",
        "X Kang",
        "F Ren"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "7",
      "title": "Emotional speech synthesis: A review",
      "authors": [
        "M Schröder"
      ],
      "year": "2001",
      "venue": "Seventh European Conference on Speech Communication and Technology"
    },
    {
      "citation_id": "8",
      "title": "Speech synthesis based on hidden markov models",
      "authors": [
        "K Tokuda",
        "Y Nankaku",
        "T Toda",
        "H Zen",
        "J Yamagishi",
        "K Oura"
      ],
      "year": "2013",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "9",
      "title": "Generative adversarial networks in human emotion synthesis: A review",
      "authors": [
        "N Hajarolasvadi",
        "M Ramirez",
        "W Beccaro",
        "H Demirel"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "10",
      "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
      "authors": [
        "S Poria",
        "E Cambria",
        "R Bajpai",
        "A Hussain"
      ],
      "year": "2017",
      "venue": "Information fusion"
    },
    {
      "citation_id": "11",
      "title": "A comprehensive survey of ai-generated content (aigc): A history of generative ai from gan to chatgpt",
      "authors": [
        "Y Cao",
        "S Li",
        "Y Liu",
        "Z Yan",
        "Y Dai",
        "P Yu",
        "L Sun"
      ],
      "year": "2023",
      "venue": "A comprehensive survey of ai-generated content (aigc): A history of generative ai from gan to chatgpt",
      "arxiv": "arXiv:2303.04226"
    },
    {
      "citation_id": "12",
      "title": "Generative technology for human emotion recognition: A scoping review",
      "authors": [
        "F Ma",
        "Y Yuan",
        "Y Xie",
        "H Ren",
        "I Liu",
        "Y He",
        "F Ren",
        "F Yu",
        "S Ni"
      ],
      "year": "2024",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "13",
      "title": "An introduction to deep generative modeling",
      "authors": [
        "L Ruthotto",
        "E Haber"
      ],
      "year": "2021",
      "venue": "GAMM-Mitteilungen"
    },
    {
      "citation_id": "14",
      "title": "Overview of the transformer-based models for nlp tasks",
      "authors": [
        "A Gillioz",
        "J Casas",
        "E Mugellini",
        "O Khaled"
      ],
      "year": "2020",
      "venue": "2020 15th Conference on Computer Science and Information Systems (FedCSIS)"
    },
    {
      "citation_id": "15",
      "title": "Generative ai",
      "authors": [
        "S Feuerriegel",
        "J Hartmann",
        "C Janiesch",
        "P Zschech"
      ],
      "year": "2024",
      "venue": "Business & Information Systems Engineering"
    },
    {
      "citation_id": "16",
      "title": "Ccis-diff: A generative model with stable diffusion prior for controlled colonoscopy image synthesis",
      "authors": [
        "Y Xie",
        "J Wang",
        "T Feng",
        "F Ma",
        "Y Li"
      ],
      "year": "2024",
      "venue": "Ccis-diff: A generative model with stable diffusion prior for controlled colonoscopy image synthesis",
      "arxiv": "arXiv:2411.12198"
    },
    {
      "citation_id": "17",
      "title": "Reducing the dimensionality of data with neural networks",
      "authors": [
        "G Hinton",
        "R Salakhutdinov"
      ],
      "year": "2006",
      "venue": "Science"
    },
    {
      "citation_id": "18",
      "title": "iemotts: Toward robust cross-speaker emotion transfer and control for speech synthesis based on disentanglement between prosody and timbre",
      "authors": [
        "G Zhang",
        "Y Qin",
        "W Zhang",
        "J Wu",
        "M Li",
        "Y Gai",
        "F Jiang",
        "T Lee"
      ],
      "year": "2023",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "19",
      "title": "Generative adversarial nets",
      "authors": [
        "I Goodfellow",
        "J Pouget-Abadie",
        "M Mirza",
        "B Xu",
        "D Warde-Farley",
        "S Ozair",
        "A Courville",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "20",
      "title": "Ganimation: Anatomically-aware facial animation from a single image",
      "authors": [
        "A Pumarola",
        "A Agudo",
        "A Martinez",
        "A Sanfeliu",
        "F Moreno-Noguer"
      ],
      "year": "2018",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "21",
      "title": "Denoising diffusion probabilistic models",
      "authors": [
        "J Ho",
        "A Jain",
        "P Abbeel"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "22",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "23",
      "title": "Emotalker: Emotionally editable talking face generation via diffusion model",
      "authors": [
        "B Zhang",
        "X Zhang",
        "N Cheng",
        "J Yu",
        "J Xiao",
        "J Wang"
      ],
      "year": "2024",
      "venue": "Emotalker: Emotionally editable talking face generation via diffusion model",
      "arxiv": "arXiv:2401.08049"
    },
    {
      "citation_id": "24",
      "title": "Enhancing conversational agents with empathic abilities",
      "authors": [
        "J Casas",
        "T Spring",
        "K Daher",
        "E Mugellini",
        "O Khaled",
        "P Cudré-Mauroux"
      ],
      "year": "2021",
      "venue": "Proceedings of the 21st ACM International Conference on Intelligent Virtual Agents"
    },
    {
      "citation_id": "25",
      "title": "Sequence to sequence learning with neural networks",
      "authors": [
        "I Sutskever",
        "O Vinyals",
        "Q Le"
      ],
      "year": "2014",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "26",
      "title": "Delete, retrieve, generate: A simple approach to sentiment and style transfer",
      "authors": [
        "J Li",
        "R Jia",
        "H He",
        "P Liang"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference of the North American Chapter"
    },
    {
      "citation_id": "27",
      "title": "Towards empathetic open-domain conversation models: A new benchmark and dataset",
      "authors": [
        "H Rashkin",
        "E Smith",
        "M Li",
        "Y.-L Boureau"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "28",
      "title": "A review on face reenactment techniques",
      "authors": [
        "S Dhere",
        "S Rathod",
        "S Aarankalle",
        "Y Lad",
        "M Gandhi"
      ],
      "year": "2020",
      "venue": "2020 International Conference on Industry 4.0 Technology (I4Tech"
    },
    {
      "citation_id": "29",
      "title": "Codeswap: Symmetrically face swapping based on prior codebook",
      "authors": [
        "X Luo",
        "X Zhang",
        "Y Xie",
        "X Tong",
        "W Yu",
        "H Chang",
        "F Ma",
        "F Yu"
      ],
      "year": "2024",
      "venue": "Proceedings of the 32nd ACM International Conference on Multimedia"
    },
    {
      "citation_id": "30",
      "title": "Consistency preservation and feature entropy regularization for gan-based face editing",
      "authors": [
        "W Xie",
        "W Lu",
        "Z Peng",
        "L Shen"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "31",
      "title": "Humancomputer interaction system: A survey of talking-head generation",
      "authors": [
        "R Zhen",
        "W Song",
        "Q He",
        "J Cao",
        "L Shi",
        "J Luo"
      ],
      "year": "2023",
      "venue": "Electronics"
    },
    {
      "citation_id": "32",
      "title": "Talking human face generation: A survey",
      "authors": [
        "M Toshpulatov",
        "W Lee",
        "S Lee"
      ],
      "year": "2023",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "33",
      "title": "An overview of voice conversion and its challenges: From statistical modeling to deep learning",
      "authors": [
        "B Sisman",
        "J Yamagishi",
        "S King",
        "H Li"
      ],
      "year": "2020",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "34",
      "title": "Emotional voice conversion: Theory, databases and esd",
      "authors": [
        "K Zhou",
        "B Sisman",
        "R Liu",
        "H Li"
      ],
      "year": "2022",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "35",
      "title": "Conventional and contemporary approaches used in text to speech synthesis: a review",
      "authors": [
        "N Kaur",
        "P Singh"
      ],
      "year": "2023",
      "venue": "Artificial Intelligence Review"
    },
    {
      "citation_id": "36",
      "title": "Fine-grained quantitative emotion editing for speech generation",
      "authors": [
        "S Inoue",
        "K Zhou",
        "S Wang",
        "H Li"
      ],
      "year": "2024",
      "venue": "Fine-grained quantitative emotion editing for speech generation",
      "arxiv": "arXiv:2403.02002"
    },
    {
      "citation_id": "37",
      "title": "Investigating critical frequency bands and channels for eeg-based emotion recognition with deep neural networks",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Autonomous Mental Development"
    },
    {
      "citation_id": "38",
      "title": "On the theory of the electrocardiogram",
      "authors": [
        "D Geselowitz"
      ],
      "year": "1989",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "39",
      "title": "Emotion in human-computer interaction",
      "authors": [
        "S Brave",
        "C Nass"
      ],
      "year": "2007",
      "venue": "The human-computer interaction handbook"
    },
    {
      "citation_id": "40",
      "title": "Functional accounts of emotions",
      "authors": [
        "D Keltner",
        "J Gross"
      ],
      "year": "1999",
      "venue": "Cognition & Emotion"
    },
    {
      "citation_id": "41",
      "title": "Measures of emotion: A reviews",
      "authors": [
        "I Mauss",
        "M Robinson"
      ],
      "year": "2010",
      "venue": "Measures of emotion: A reviews"
    },
    {
      "citation_id": "42",
      "title": "Emotion and decision making",
      "authors": [
        "J Lerner",
        "Y Li",
        "P Valdesolo",
        "K Kassam"
      ],
      "year": "2015",
      "venue": "Annual review of psychology"
    },
    {
      "citation_id": "43",
      "title": "Emotion in social relations: Cultural, group, and interpersonal processes",
      "authors": [
        "B Parkinson",
        "A Fischer",
        "A Manstead"
      ],
      "year": "2005",
      "venue": "Emotion in social relations: Cultural, group, and interpersonal processes"
    },
    {
      "citation_id": "44",
      "title": "Emotion recognition and artificial intelligence: A systematic review (2014-2023) and research recommendations",
      "authors": [
        "S Khare",
        "V Blanes-Vidal",
        "E Nadimi",
        "U Acharya"
      ],
      "year": "2023",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "45",
      "title": "Personality-aware personalized emotion recognition from physiological signals",
      "authors": [
        "S Zhao",
        "G Ding",
        "J Han",
        "Y Gao"
      ],
      "year": "2018",
      "venue": "IJCAI"
    },
    {
      "citation_id": "46",
      "title": "Personalized emotion recognition by personalityaware high-order learning of physiological signals",
      "authors": [
        "S Zhao",
        "A Gholaminejad",
        "G Ding",
        "Y Gao",
        "J Han",
        "K Keutzer"
      ],
      "year": "2019",
      "venue": "ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)"
    },
    {
      "citation_id": "47",
      "title": "An argument for basic emotions",
      "authors": [
        "P Ekman"
      ],
      "year": "1992",
      "venue": "Cognition & emotion"
    },
    {
      "citation_id": "48",
      "title": "Theories of emotion",
      "authors": [
        "R Plutchik",
        "H Kellerman"
      ],
      "year": "2013",
      "venue": "Theories of emotion"
    },
    {
      "citation_id": "49",
      "title": "Data augmentation for audio-visual emotion recognition with an efficient multimodal conditional gan",
      "authors": [
        "F Ma",
        "Y Li",
        "S Ni",
        "S.-L Huang",
        "L Zhang"
      ],
      "year": "2022",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "50",
      "title": "Real-time assessment of mental workload using psychophysiological measures and artificial neural networks",
      "authors": [
        "G Wilson",
        "C Russell"
      ],
      "year": "2003",
      "venue": "Human factors"
    },
    {
      "citation_id": "51",
      "title": "Pleasure-arousal-dominance: A general framework for describing and measuring individual differences in temperament",
      "authors": [
        "A Mehrabian"
      ],
      "year": "1996",
      "venue": "Current Psychology"
    },
    {
      "citation_id": "52",
      "title": "Generative adversarial networks for face generation: A survey",
      "authors": [
        "A Kammoun",
        "R Slama",
        "H Tabia",
        "T Ouni",
        "M Abid"
      ],
      "year": "2022",
      "venue": "ACM Computing Surveys"
    },
    {
      "citation_id": "53",
      "title": "Gan-based facial attribute manipulation",
      "authors": [
        "Y Liu",
        "Q Li",
        "Q Deng",
        "Z Sun",
        "M.-H Yang"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "54",
      "title": "An overview of affective speech synthesis and conversion in the deep learning era",
      "authors": [
        "A Triantafyllopoulos",
        "B Schuller",
        "G ˙iymen",
        "M Sezgin",
        "X He",
        "Z Yang",
        "P Tzirakis",
        "S Liu",
        "S Mertes",
        "E André"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "55",
      "title": "Generative adversarial networks for speech processing: A review",
      "authors": [
        "A Wali",
        "Z Alamgir",
        "S Karim",
        "A Fawaz",
        "M Ali",
        "M Adan",
        "M Mujtaba"
      ],
      "year": "2022",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "56",
      "title": "A survey of controllable text generation using transformer-based pre-trained language models",
      "authors": [
        "H Zhang",
        "H Song",
        "S Li",
        "M Zhou",
        "D Song"
      ],
      "year": "2023",
      "venue": "ACM Computing Surveys"
    },
    {
      "citation_id": "57",
      "title": "A survey on text generation using generative adversarial networks",
      "authors": [
        "G Rosa",
        "J Papa"
      ],
      "year": "2021",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "58",
      "title": "Emotion recognition and detection methods: A comprehensive survey",
      "authors": [
        "A Saxena",
        "A Khanna",
        "D Gupta"
      ],
      "year": "2020",
      "venue": "Journal of Artificial Intelligence and Systems"
    },
    {
      "citation_id": "59",
      "title": "A review on sentiment analysis and emotion detection from text",
      "authors": [
        "P Nandwani",
        "R Verma"
      ],
      "year": "2021",
      "venue": "Social network analysis and mining"
    },
    {
      "citation_id": "60",
      "title": "A survey on facial emotion recognition techniques: A state-of-the-art literature review",
      "authors": [
        "F Canal",
        "T Müller",
        "J Matias",
        "G Scotton",
        "A De Sa Junior",
        "E Pozzebon",
        "A Sobieranski"
      ],
      "year": "2022",
      "venue": "Information Sciences"
    },
    {
      "citation_id": "61",
      "title": "Deep generative models: Survey",
      "authors": [
        "A Oussidi",
        "A Elhassouny"
      ],
      "year": "2018",
      "venue": "2018 International conference on intelligent systems and computer vision (ISCV)"
    },
    {
      "citation_id": "62",
      "title": "Auto-encoding variational bayes",
      "authors": [
        "D Kingma",
        "M Welling"
      ],
      "year": "2013",
      "venue": "Auto-encoding variational bayes",
      "arxiv": "arXiv:1312.6114"
    },
    {
      "citation_id": "63",
      "title": "Improving language understanding by generative pre-training",
      "authors": [
        "A Radford",
        "K Narasimhan",
        "T Salimans",
        "I Sutskever"
      ],
      "year": "2018",
      "venue": "Improving language understanding by generative pre-training"
    },
    {
      "citation_id": "64",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "M.-W Kenton",
        "L Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of NAACL-HLT"
    },
    {
      "citation_id": "65",
      "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
      "authors": [
        "Z Yang",
        "Z Dai",
        "Y Yang",
        "J Carbonell",
        "R Salakhutdinov",
        "Q Le"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "66",
      "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "authors": [
        "C Raffel",
        "N Shazeer",
        "A Roberts",
        "K Lee",
        "S Narang",
        "M Matena",
        "Y Zhou",
        "W Li",
        "P Liu"
      ],
      "year": "2020",
      "venue": "Journal of machine learning research"
    },
    {
      "citation_id": "67",
      "title": "Language models are unsupervised multitask learners",
      "authors": [
        "A Radford",
        "J Wu",
        "R Child",
        "D Luan",
        "D Amodei",
        "I Sutskever"
      ],
      "year": "2019",
      "venue": "OpenAI blog"
    },
    {
      "citation_id": "68",
      "title": "Mllm-ta: Leveraging multimodal large language models for precise temporal video grounding",
      "authors": [
        "Y Liu",
        "H Hou",
        "F Ma",
        "S Ni",
        "F Yu"
      ],
      "year": "2024",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "69",
      "title": "Facial expression recognition from near-infrared videos",
      "authors": [
        "G Zhao",
        "X Huang",
        "M Taini",
        "S Li",
        "M Pietikäinen"
      ],
      "year": "2011",
      "venue": "Image and vision computing"
    },
    {
      "citation_id": "70",
      "title": "Presentation and validation of the radboud faces database",
      "authors": [
        "O Langner",
        "R Dotsch",
        "G Bijlstra",
        "D Wigboldus",
        "S Hawk",
        "A Knippenberg"
      ],
      "year": "2010",
      "venue": "Cognition and emotion"
    },
    {
      "citation_id": "71",
      "title": "The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression",
      "authors": [
        "P Lucey",
        "J Cohn",
        "T Kanade",
        "J Saragih",
        "Z Ambadar",
        "I Matthews"
      ],
      "year": "2010",
      "venue": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "72",
      "title": "Compound facial expressions of emotion",
      "authors": [
        "S Du",
        "Y Tao",
        "A Martinez"
      ],
      "year": "2014",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "73",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "74",
      "title": "Disfa: A spontaneous facial action intensity database",
      "authors": [
        "S Mavadati",
        "M Mahoor",
        "K Bartlett",
        "P Trinh",
        "J Cohn"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "75",
      "title": "Emotionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild",
      "authors": [
        "C Benitez-Quiroz",
        "R Srinivasan",
        "A Martinez"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "76",
      "title": "Seen and unseen emotional style transfer for voice conversion with a new emotional speech dataset",
      "authors": [
        "K Zhou",
        "B Sisman",
        "R Liu",
        "H Li"
      ],
      "year": "2021",
      "venue": "ICASSP 2021 -2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "77",
      "title": "The emotional voices database: Towards controlling the emotion dimension in voice generation systems",
      "authors": [
        "A Adigwe",
        "N Tits",
        "K Haddad",
        "S Ostadabbas",
        "T Dutoit"
      ],
      "year": "2018",
      "venue": "The emotional voices database: Towards controlling the emotion dimension in voice generation systems",
      "arxiv": "arXiv:1806.09514"
    },
    {
      "citation_id": "78",
      "title": "A database of german emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "Interspeech"
    },
    {
      "citation_id": "79",
      "title": "Et-gan: Cross-language emotion transfer based on cycle-consistent generative adversarial networks",
      "authors": [
        "X Jia",
        "J Tai",
        "H Zhou",
        "Y Li",
        "W Zhang",
        "H Du",
        "Q Huang"
      ],
      "year": "2019",
      "venue": "Et-gan: Cross-language emotion transfer based on cycle-consistent generative adversarial networks",
      "arxiv": "arXiv:1905.11173"
    },
    {
      "citation_id": "80",
      "title": "A canadian french emotional speech dataset",
      "authors": [
        "P Gournay",
        "O Lahaie",
        "R Lefebvre"
      ],
      "year": "2018",
      "venue": "Proceedings of the 9th ACM Multimedia Systems Conference"
    },
    {
      "citation_id": "81",
      "title": "Emoq-tts: Emotion intensity quantization for fine-grained controllable emotional text-to-speech",
      "authors": [
        "C.-B Im",
        "S.-H Lee",
        "S.-B Kim",
        "S.-W Lee"
      ],
      "year": "2022",
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "82",
      "title": "Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering",
      "authors": [
        "R He",
        "J Mcauley"
      ],
      "year": "2016",
      "venue": "Proceedings of the 25th International Conference on World Wide Web"
    },
    {
      "citation_id": "83",
      "title": "Mojitalk: Generating emotional responses at scale",
      "authors": [
        "X Zhou",
        "W Wang"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "84",
      "title": "Mead: A large-scale audio-visual dataset for emotional talking-face generation",
      "authors": [
        "K Wang",
        "Q Wu",
        "L Song",
        "Z Yang",
        "W Wu",
        "C Qian",
        "R He",
        "Y Qiao",
        "C Loy"
      ],
      "year": "2020",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "85",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "86",
      "title": "Emotion recognition in speech using cross-modal transfer in the wild",
      "authors": [
        "S Albanie",
        "A Nagrani",
        "A Vedaldi",
        "A Zisserman"
      ],
      "year": "2018",
      "venue": "Proceedings of the 26th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "87",
      "title": "Audio-visual feature selection and reduction for emotion classification",
      "authors": [
        "S Haq",
        "P Jackson",
        "J Edge"
      ],
      "year": "2008",
      "venue": "Proc. Int. Conf. on Auditory-Visual Speech Processing (AVSP'08)"
    },
    {
      "citation_id": "88",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PloS One"
    },
    {
      "citation_id": "89",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "90",
      "title": "Icface: Interpretable and controllable face reenactment using gans",
      "authors": [
        "S Tripathy",
        "J Kannala",
        "E Rahtu"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision"
    },
    {
      "citation_id": "91",
      "title": "Realistic face reenactment via self-supervised disentangling of identity and pose",
      "authors": [
        "X Zeng",
        "Y Pan",
        "M Wang",
        "J Zhang",
        "Y Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "92",
      "title": "Emotion editing in head reenactment videos using latent space manipulation",
      "authors": [
        "V Strizhkova",
        "Y Wang",
        "D Anghelone",
        "D Yang",
        "A Dantcheva",
        "F Brémond"
      ],
      "year": "2021",
      "venue": "2021 16th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2021)"
    },
    {
      "citation_id": "93",
      "title": "Altering the conveyed facial emotion through automatic reenactment of video portraits",
      "authors": [
        "C Groth",
        "J.-P Tauscher",
        "S Castillo",
        "M Magnor"
      ],
      "year": "2020",
      "venue": "International Conference on Computer Animation and Social Agents"
    },
    {
      "citation_id": "94",
      "title": "All-in-one: Facial expression transfer, editing and recognition using a single network",
      "authors": [
        "K Ali",
        "C Hughes"
      ],
      "year": "2019",
      "venue": "All-in-one: Facial expression transfer, editing and recognition using a single network",
      "arxiv": "arXiv:1911.07050"
    },
    {
      "citation_id": "95",
      "title": "Semantic prior guided finegrained facial expression manipulation",
      "authors": [
        "T Xue",
        "J Yan",
        "D Zheng",
        "Y Liu"
      ],
      "year": "2024",
      "venue": "Complex & Intelligent Systems"
    },
    {
      "citation_id": "96",
      "title": "Wp2-gan: Wavelet-based multi-level gan for progressive facial expression translation with parallel generators",
      "authors": [
        "J Shao",
        "T Bui"
      ],
      "venue": "Proc. British Mach. Vis. Conf., 2021"
    },
    {
      "citation_id": "97",
      "title": "Headgan: Oneshot neural head synthesis and editing",
      "authors": [
        "M Doukas",
        "S Zafeiriou",
        "V Sharmanska"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "98",
      "title": "Action unit driven facial expression synthesis from a single image with patch attentive gan",
      "authors": [
        "Y Zhao",
        "L Yang",
        "E Pei",
        "M Oveneke",
        "M Alioscha-Perez",
        "L Li",
        "D Jiang",
        "H Sahli"
      ],
      "year": "2021",
      "venue": "Computer Graphics Forum"
    },
    {
      "citation_id": "99",
      "title": "Local and global perception generative adversarial network for facial expression synthesis",
      "authors": [
        "Y Xia",
        "W Zheng",
        "Y Wang",
        "H Yu",
        "J Dong",
        "F.-Y Wang"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "100",
      "title": "Dft-net: Disentanglement of face deformation and texture synthesis for expression editing",
      "authors": [
        "J Wang",
        "J Zhang",
        "Z Lu",
        "S Shan"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Image Processing"
    },
    {
      "citation_id": "101",
      "title": "Sargan: Spatial attention-based residuals for facial expression manipulation",
      "authors": [
        "A Akram",
        "N Khan"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "102",
      "title": "Gated switchgan for multi-domain facial image translation",
      "authors": [
        "X Zhang",
        "Y Zhu",
        "W Chen",
        "W Liu",
        "L Shen"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "103",
      "title": "Us-gan: On the importance of ultimate skip connection for facial expression synthesis",
      "authors": [
        "A Akram",
        "N Khan"
      ],
      "year": "2024",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "104",
      "title": "Emostyle: One-shot facial expression editing using continuous emotion parameters",
      "authors": [
        "B Azari",
        "A Lim"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision"
    },
    {
      "citation_id": "105",
      "title": "Ganmut: Learning interpretable conditional space for gamut of emotions",
      "authors": [
        "S Apolito",
        "D Paudel",
        "Z Huang",
        "A Romero",
        "L Gool"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "106",
      "title": "Apprgan: Appearance-based gan for facial expression synthesis",
      "authors": [
        "Y Peng",
        "H Yin"
      ],
      "year": "2019",
      "venue": "IET Image Processing"
    },
    {
      "citation_id": "107",
      "title": "Geometry guided adversarial facial expression synthesis",
      "authors": [
        "L Song",
        "Z Lu",
        "R He",
        "Z Sun",
        "T Tan"
      ],
      "year": "2018",
      "venue": "Proceedings of the 26th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "108",
      "title": "Dualpathgan: Facial reenacted emotion synthesis",
      "authors": [
        "J Kong",
        "H Shen",
        "K Huang"
      ],
      "year": "2021",
      "venue": "IET Computer Vision"
    },
    {
      "citation_id": "109",
      "title": "Fine-grained expression manipulation via structured latent space",
      "authors": [
        "J Tang",
        "Z Shao",
        "L Ma"
      ],
      "year": "2020",
      "venue": "2020 IEEE International Conference on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "110",
      "title": "Styleclip: Text-driven manipulation of stylegan imagery",
      "authors": [
        "O Patashnik",
        "Z Wu",
        "E Shechtman",
        "D Cohen-Or",
        "D Lischinski"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "111",
      "title": "Evogan: An evolutionary computation assisted gan",
      "authors": [
        "F Liu",
        "H Wang",
        "J Zhang",
        "Z Fu",
        "A Zhou",
        "J Qi",
        "Z Li"
      ],
      "year": "2022",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "112",
      "title": "Expression conditional gan for facial expression-to-expression translation",
      "authors": [
        "H Tang",
        "W Wang",
        "S Wu",
        "X Chen",
        "D Xu",
        "N Sebe",
        "Y Yan"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Image Processing (ICIP)"
    },
    {
      "citation_id": "113",
      "title": "Unmasking your expression: Expressionconditioned gan for masked face inpainting",
      "authors": [
        "S Sola",
        "D Gera"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "114",
      "title": "Facial expression editing with continuous emotion labels",
      "authors": [
        "A Lindt",
        "P Barros",
        "H Siqueira",
        "S Wermter"
      ],
      "year": "2019",
      "venue": "2019 14th IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "115",
      "title": "Adversarial autoencoders",
      "authors": [
        "A Makhzani",
        "J Shlens",
        "N Jaitly",
        "I Goodfellow",
        "B Frey"
      ],
      "year": "2015",
      "venue": "Adversarial autoencoders",
      "arxiv": "arXiv:1511.05644"
    },
    {
      "citation_id": "116",
      "title": "Speech driven talking face generation from a single image and an emotion condition",
      "authors": [
        "S Eskimez",
        "Y Zhang",
        "Z Duan"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "117",
      "title": "Realistic speechdriven facial animation with gans",
      "authors": [
        "K Vougioukas",
        "S Petridis",
        "M Pantic"
      ],
      "year": "2020",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "118",
      "title": "Talking face generation with expression-tailored generative adversarial network",
      "authors": [
        "D Zeng",
        "H Liu",
        "H Lin",
        "S Ge"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "119",
      "title": "Efficient emotional adaptation for audio-driven talking-head generation",
      "authors": [
        "Y Gan",
        "Z Yang",
        "X Yue",
        "L Sun",
        "Y Yang"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "120",
      "title": "Flowvqtalker: High-quality emotional talking face generation through normalizing flow and quantization",
      "authors": [
        "S Tan",
        "B Ji",
        "Y Pan"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "121",
      "title": "Edtalk: Efficient disentanglement for emotional talking head synthesis",
      "authors": [
        "S Tan",
        "B Ji",
        "M Bi"
      ],
      "year": "2024",
      "venue": "Edtalk: Efficient disentanglement for emotional talking head synthesis",
      "arxiv": "arXiv:2404.01647"
    },
    {
      "citation_id": "122",
      "title": "2cet-gan: Pixel-level gan model for human facial expression transfer",
      "authors": [
        "X Hu",
        "N Aldausari",
        "G Mohammadi"
      ],
      "year": "2023",
      "venue": "Proceedings of the 1st International Workshop on Multimedia Content Generation and Evaluation: New Methods and Practice"
    },
    {
      "citation_id": "123",
      "title": "Stargan: Unified generative adversarial networks for multi-domain image-to-image translation",
      "authors": [
        "Y Choi",
        "M Choi",
        "M Kim",
        "J.-W Ha",
        "S Kim",
        "J Choo"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "124",
      "title": "Ugan: Untraceable gan for multi-domain face translation",
      "authors": [
        "D Zhu",
        "S Liu",
        "W Jiang",
        "C Gao",
        "T Wu",
        "Q Wang",
        "G Guo"
      ],
      "year": "2019",
      "venue": "Ugan: Untraceable gan for multi-domain face translation",
      "arxiv": "arXiv:1907.11418"
    },
    {
      "citation_id": "125",
      "title": "Exprgan: Facial expression editing with controllable expression intensity",
      "authors": [
        "H Ding",
        "K Sricharan",
        "R Chellappa"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "126",
      "title": "Generating realistic facial expressions through conditional cycle-consistent generative adversarial networks (ccyclegan)",
      "authors": [
        "G Tesei"
      ],
      "year": "2019",
      "venue": "Generating realistic facial expressions through conditional cycle-consistent generative adversarial networks (ccyclegan)"
    },
    {
      "citation_id": "127",
      "title": "Comp-gan: Compositional generative adversarial network in synthesizing and recognizing facial expression",
      "authors": [
        "W Wang",
        "Q Sun",
        "Y Fu",
        "T Chen",
        "C Cao",
        "Z Zheng",
        "G Xu",
        "H Qiu",
        "Y.-G Jiang",
        "X Xue"
      ],
      "year": "2019",
      "venue": "Proceedings of the 27th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "128",
      "title": "Self-supervised emotion representation disentanglement for speech-preserving facial expression manipulation",
      "authors": [
        "Z Xu",
        "T Chen",
        "Z Yang"
      ],
      "year": "2024",
      "venue": "Proceedings of ACM Multimedia 2024"
    },
    {
      "citation_id": "129",
      "title": "Emmn: Emotional motion memory network for audio-driven emotional talking face generation",
      "authors": [
        "S Tan",
        "B Ji",
        "Y Pan"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "130",
      "title": "Style2talker: High-resolution talking head generation with emotion style and art style",
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "131",
      "title": "Talking face generation with audio-deduced emotional landmarks",
      "authors": [
        "S Zhai",
        "M Liu",
        "Y Li",
        "Z Gao",
        "L Zhu",
        "L Nie"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "132",
      "title": "Stochastic latent talking face generation towards emotional expressions and head poses",
      "authors": [
        "Z Sheng",
        "L Nie",
        "M Zhang",
        "X Chang",
        "Y Yan"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "133",
      "title": "Dreamtalk: When expressive talking head generation meets diffusion probabilistic models",
      "authors": [
        "Y Ma",
        "S Zhang",
        "J Wang",
        "X Wang",
        "Y Zhang",
        "Z Deng"
      ],
      "year": "2023",
      "venue": "Dreamtalk: When expressive talking head generation meets diffusion probabilistic models",
      "arxiv": "arXiv:2312.09767"
    },
    {
      "citation_id": "134",
      "title": "Dream-talk: Diffusion-based realistic emotional audio-driven method for single image talking face generation",
      "authors": [
        "C Zhang",
        "C Wang",
        "J Zhang",
        "H Xu",
        "G Song",
        "Y Xie",
        "L Luo",
        "Y Tian",
        "X Guo",
        "J Feng"
      ],
      "year": "2023",
      "venue": "Dream-talk: Diffusion-based realistic emotional audio-driven method for single image talking face generation",
      "arxiv": "arXiv:2312.13578"
    },
    {
      "citation_id": "135",
      "title": "Continuously controllable facial expression editing in talking face videos",
      "authors": [
        "Z Sun",
        "Y Wen",
        "T Lv"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "136",
      "title": "Eat-face: Emotion-controllable audio-driven talking face generation via diffusion model",
      "authors": [
        "H Wang",
        "X Jia",
        "X Cao"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 IEEE 18th International Conference on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "137",
      "title": "Converting anyone's emotion: Towards speaker-independent emotional voice conversion",
      "authors": [
        "K Zhou",
        "B Sisman",
        "M Zhang",
        "H Li"
      ],
      "year": "2020",
      "venue": "Converting anyone's emotion: Towards speaker-independent emotional voice conversion",
      "arxiv": "arXiv:2005.07025"
    },
    {
      "citation_id": "138",
      "title": "Vaw-gan for disentanglement and recomposition of emotional elements in speech",
      "authors": [
        "K Zhou",
        "B Sisman",
        "H Li"
      ],
      "year": "2021",
      "venue": "2021 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "139",
      "title": "Transforming spectrum and prosody for emotional voice conversion with non-parallel training data",
      "year": "2020",
      "venue": "Transforming spectrum and prosody for emotional voice conversion with non-parallel training data",
      "arxiv": "arXiv:2002.00198"
    },
    {
      "citation_id": "140",
      "title": "Unpaired image-toimage translation using cycle-consistent adversarial networks",
      "authors": [
        "J.-Y Zhu",
        "T Park",
        "P Isola",
        "A Efros"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "141",
      "title": "An improved cycleganbased emotional voice conversion model by augmenting temporal dependency with a transformer",
      "authors": [
        "C Fu",
        "C Liu",
        "C Ishi",
        "H Ishiguro"
      ],
      "year": "2022",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "142",
      "title": "Stargan for emotional speech conversion: Validated by data augmentation of end-to-end emotion recognition",
      "authors": [
        "G Rizos",
        "A Baird",
        "M Elliott",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "143",
      "title": "Multi-speaker and multidomain emotional voice conversion using factorized hierarchical variational autoencoder",
      "authors": [
        "M Elgaar",
        "J Park",
        "S Lee"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "144",
      "title": "Nonparallel emotional speech conversion",
      "authors": [
        "J Gao",
        "D Chakraborty",
        "H Tembine",
        "O Olaleye"
      ],
      "year": "2018",
      "venue": "Nonparallel emotional speech conversion",
      "arxiv": "arXiv:1811.01174"
    },
    {
      "citation_id": "145",
      "title": "Attention-based interactive disentangling network for instance-level emotional voice conversion",
      "authors": [
        "Y Chen",
        "L Yang",
        "Q Chen",
        "J.-H Lai",
        "X Xie"
      ],
      "year": "2023",
      "venue": "Attention-based interactive disentangling network for instance-level emotional voice conversion",
      "arxiv": "arXiv:2312.17508"
    },
    {
      "citation_id": "146",
      "title": "Durflex-evc: Duration-flexible emotional voice conversion with parallel generation",
      "authors": [
        "H.-S Oh",
        "S.-H Lee",
        "D.-H Cho",
        "S.-W Lee"
      ],
      "year": "2024",
      "venue": "Durflex-evc: Duration-flexible emotional voice conversion with parallel generation",
      "arxiv": "arXiv:2401.08095"
    },
    {
      "citation_id": "147",
      "title": "Expressive voice conversion: A joint framework for speaker identity and emotional style transfer",
      "authors": [
        "Z Du",
        "B Sisman",
        "K Zhou",
        "H Li"
      ],
      "year": "2021",
      "venue": "2021 IEEE Automatic Speech Recognition and Understanding Workshop"
    },
    {
      "citation_id": "148",
      "title": "English emotional voice conversion using stargan model",
      "authors": [
        "A Meftah",
        "A Alashban",
        "Y Alotaibi",
        "S.-A Selouani"
      ],
      "year": "2023",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "149",
      "title": "Nonparallel emotional voice conversion for unseen speaker-emotion pairs using dual domain adversarial network & virtual domain pairing",
      "authors": [
        "N Shah",
        "M Singh",
        "N Takahashi",
        "N Onoe"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "150",
      "title": "Disentanglement of emotional style and speaker identity for expressive voice conversion",
      "authors": [
        "Z Du",
        "B Sisman",
        "K Zhou",
        "H Li"
      ],
      "year": "2021",
      "venue": "Disentanglement of emotional style and speaker identity for expressive voice conversion",
      "arxiv": "arXiv:2110.10326"
    },
    {
      "citation_id": "151",
      "title": "Emotion intensity and its control for emotional voice conversion",
      "authors": [
        "K Zhou",
        "B Sisman",
        "R Rana",
        "B Schuller",
        "H Li"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "152",
      "title": "Textless speech emotion conversion using discrete & decomposed representations",
      "authors": [
        "F Kreuk",
        "A Polyak",
        "J Copet",
        "E Kharitonov",
        "T Nguyen",
        "M Rivière",
        "W.-N Hsu",
        "A Mohamed",
        "E Dupoux",
        "Y Adi"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "153",
      "title": "Sequence-to-sequence emotional voice conversion with strength control",
      "authors": [
        "H Choi",
        "M Hahn"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "154",
      "title": "Fine-grained emotion strength transfer, control and prediction for emotional speech synthesis",
      "authors": [
        "Y Lei",
        "S Yang",
        "L Xie"
      ],
      "year": "2021",
      "venue": "2021 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "155",
      "title": "Diclet-tts: Diffusion model based cross-lingual emotion transfer for text-to-speech-a study between english and mandarin",
      "authors": [
        "T Li",
        "C Hu",
        "J Cong",
        "X Zhu",
        "J Li",
        "Q Tian",
        "Y Wang",
        "L Xie"
      ],
      "year": "2023",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "156",
      "title": "Exploring transfer learning for low resource emotional tts",
      "authors": [
        "N Tits",
        "K Haddad",
        "T Dutoit"
      ],
      "year": "2020",
      "venue": "Intelligent Systems and Applications: Proceedings of the 2019 Intelligent Systems Conference (IntelliSys)"
    },
    {
      "citation_id": "157",
      "title": "Improving emotional tts with an emotion intensity input from unsupervised extraction",
      "authors": [
        "B Schnell",
        "P Garner"
      ],
      "venue": "Proc. 11th ISCA Speech Synth. Workshop, 2021"
    },
    {
      "citation_id": "158",
      "title": "End-toend emotional speech synthesis using style tokens and semisupervised training",
      "authors": [
        "P Wu",
        "Z Ling",
        "L Liu",
        "Y Jiang",
        "H Wu",
        "L Dai"
      ],
      "year": "2019",
      "venue": "2019 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "159",
      "title": "Emotional speech synthesis with rich and granularized control",
      "authors": [
        "S.-Y Um",
        "S Oh",
        "K Byun",
        "I Jang",
        "C Ahn",
        "H.-G Kang"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "160",
      "title": "Gantron: Emotional speech synthesis with generative adversarial networks",
      "authors": [
        "E Hortal",
        "R Alarcia"
      ],
      "year": "2021",
      "venue": "Gantron: Emotional speech synthesis with generative adversarial networks",
      "arxiv": "arXiv:2110.03390"
    },
    {
      "citation_id": "161",
      "title": "Emodiff: Intensity controllable emotional text-to-speech with soft-label guidance",
      "authors": [
        "Y Guo",
        "C Du",
        "X Chen",
        "K Yu"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "162",
      "title": "Controllable emotion transfer for end-to-end speech synthesis",
      "authors": [
        "T Li",
        "S Yang",
        "L Xue",
        "L Xie"
      ],
      "year": "2021",
      "venue": "2021 12th International Symposium on Chinese Spoken Language Processing"
    },
    {
      "citation_id": "163",
      "title": "Msemotts: Multi-scale emotion transfer, prediction, and control for emotional speech synthesis",
      "authors": [
        "Y Lei",
        "S Yang",
        "X Wang",
        "L Xie"
      ],
      "year": "2022",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "164",
      "title": "Cross-speaker emotion disentangling and transfer for end-to-end speech synthesis",
      "authors": [
        "T Li",
        "X Wang",
        "Q Xie",
        "Z Wang",
        "L Xie"
      ],
      "year": "2022",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "165",
      "title": "Towards multiscale style control for expressive speech synthesis",
      "authors": [
        "X Li",
        "C Song",
        "J Li",
        "Z Wu",
        "J Jia",
        "H Meng"
      ],
      "year": "2021",
      "venue": "Towards multiscale style control for expressive speech synthesis",
      "arxiv": "arXiv:2104.03521"
    },
    {
      "citation_id": "166",
      "title": "Emomix: Emotion mixing via diffusion models for emotional speech synthesis",
      "authors": [
        "H Tang",
        "X Zhang",
        "J Wang",
        "N Cheng",
        "J Xiao"
      ],
      "year": "2023",
      "venue": "Emomix: Emotion mixing via diffusion models for emotional speech synthesis",
      "arxiv": "arXiv:2306.00648"
    },
    {
      "citation_id": "167",
      "title": "Controlling the strength of emotions in speech-like emotional sound generated by wavenet",
      "authors": [
        "K Matsumoto",
        "S Hara",
        "M Abe"
      ],
      "year": "2020",
      "venue": "Controlling the strength of emotions in speech-like emotional sound generated by wavenet"
    },
    {
      "citation_id": "168",
      "title": "Emotion selectable end-to-end text-based speech editing",
      "authors": [
        "T Wang",
        "J Yi",
        "R Fu",
        "J Tao",
        "Z Wen",
        "C Zhang"
      ],
      "year": "2024",
      "venue": "Artificial Intelligence"
    },
    {
      "citation_id": "169",
      "title": "Fastspeech: Fast, robust and controllable text to speech",
      "authors": [
        "Y Ren",
        "Y Ruan",
        "X Tan",
        "T Qin",
        "S Zhao",
        "Z Zhao",
        "T.-Y Liu"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "170",
      "title": "Towards realistic emotional voice conversion using controllable emotional intensity",
      "authors": [
        "T Qi",
        "S Wang",
        "C Lu"
      ],
      "year": "2024",
      "venue": "Towards realistic emotional voice conversion using controllable emotional intensity",
      "arxiv": "arXiv:2407.14800"
    },
    {
      "citation_id": "171",
      "title": "Zet-speech: Zero-shot adaptive emotion-controllable text-to-speech synthesis with diffusion and style-based models",
      "authors": [
        "M Kang",
        "W Han",
        "S Hwang",
        "E Yang"
      ],
      "year": "2023",
      "venue": "Zet-speech: Zero-shot adaptive emotion-controllable text-to-speech synthesis with diffusion and style-based models",
      "arxiv": "arXiv:2305.13831"
    },
    {
      "citation_id": "172",
      "title": "Controlling emotion strength with relative attribute for end-to-end speech synthesis",
      "authors": [
        "X Zhu",
        "S Yang",
        "G Yang",
        "L Xie"
      ],
      "year": "2019",
      "venue": "2019 IEEE Automatic Speech Recognition and Understanding Workshop"
    },
    {
      "citation_id": "173",
      "title": "Cross-speaker emotion transfer through information perturbation in emotional speech synthesis",
      "authors": [
        "Y Lei",
        "S Yang",
        "X Zhu",
        "L Xie",
        "D Su"
      ],
      "year": "2022",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "174",
      "title": "Emotion controllable speech synthesis using emotion-unlabeled dataset with the assistance of cross-domain speech emotion recognition",
      "authors": [
        "X Cai",
        "D Dai",
        "Z Wu",
        "X Li",
        "J Li",
        "H Meng"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "175",
      "title": "Fine-grained emotional control of text-to-speech: Learning to rank inter-and intra-class emotion intensities",
      "authors": [
        "S Wang",
        "J Guðnason",
        "D Borth"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "176",
      "title": "Speech synthesis with mixed emotions",
      "authors": [
        "K Zhou",
        "B Sisman",
        "R Rana",
        "B Schuller",
        "H Li"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "177",
      "title": "Emospeech: Guiding fastspeech2 towards emotional text to speech",
      "authors": [
        "D Diatlova",
        "V Shutov"
      ],
      "year": "2023",
      "venue": "Emospeech: Guiding fastspeech2 towards emotional text to speech",
      "arxiv": "arXiv:2307.00024"
    },
    {
      "citation_id": "178",
      "title": "Mm-tts: Multi-modal prompt based style transfer for expressive text-to-speech synthesis",
      "authors": [
        "W Guan",
        "Y Li",
        "T Li"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "179",
      "title": "Emotional dimension control in language model-based text-to-speech: Spanning a broad spectrum of human emotions",
      "authors": [
        "K Zhou",
        "Y Zhang",
        "S Zhao"
      ],
      "year": "2024",
      "venue": "Emotional dimension control in language model-based text-to-speech: Spanning a broad spectrum of human emotions",
      "arxiv": "arXiv:2409.16681"
    },
    {
      "citation_id": "180",
      "title": "Ed-tts: Multi-scale emotion modeling using cross-domain emotion diarization for emotional speech synthesis",
      "authors": [
        "H Tang",
        "X Zhang",
        "N Cheng"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "181",
      "title": "Enhancing emotional text-to-speech controllability with natural language guidance through contrastive learning and diffusion models",
      "authors": [
        "X Jing",
        "K Zhou",
        "A Triantafyllopoulos"
      ],
      "year": "2024",
      "venue": "Enhancing emotional text-to-speech controllability with natural language guidance through contrastive learning and diffusion models",
      "arxiv": "arXiv:2409.06451"
    },
    {
      "citation_id": "182",
      "title": "Rset: Remapping-based sorting method for emotion transfer speech synthesis",
      "authors": [
        "H Shi",
        "J Wang",
        "X Zhang"
      ],
      "year": "2024",
      "venue": "Proceedings of Asia-Pacific Web (APWeb) and Web-Age Information Management (WAIM) Joint International Conference on Web and Big Data"
    },
    {
      "citation_id": "183",
      "title": "Textsettr: Few-shot text style extraction and tunable targeted restyling",
      "authors": [
        "P Riley",
        "N Constant",
        "M Guo",
        "G Kumar",
        "D Uthus",
        "Z Parekh"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "184",
      "title": "Tet: Text emotion transfer",
      "authors": [
        "R Mohammadibaghmolaei",
        "A Ahmadi"
      ],
      "year": "2023",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "185",
      "title": "Imat: Unsupervised text attribute transfer via iterative matching and translation",
      "authors": [
        "Z Jin",
        "D Jin",
        "J Mueller",
        "N Matthews",
        "E Santus"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "186",
      "title": "Mask and infill: Applying masked language model to sentiment transfer",
      "authors": [
        "X Wu",
        "T Zhang",
        "L Zang",
        "J Han",
        "S Hu"
      ],
      "year": "2019",
      "venue": "Mask and infill: Applying masked language model to sentiment transfer"
    },
    {
      "citation_id": "187",
      "title": "Unsupervised text style transfer with padded masked language models",
      "authors": [
        "E Malmi",
        "A Severyn",
        "S Rothe"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "188",
      "title": "Unsupervised text style transfer using language models as discriminators",
      "authors": [
        "Z Yang",
        "Z Hu",
        "C Dyer",
        "E Xing",
        "T Berg-Kirkpatrick"
      ],
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "189",
      "title": "Style transfer from non-parallel text by cross-alignment",
      "authors": [
        "T Shen",
        "T Lei",
        "R Barzilay",
        "T Jaakkola"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "190",
      "title": "Emotional text generation based on cross-domain sentiment transfer",
      "authors": [
        "R Zhang",
        "Z Wang",
        "K Yin",
        "Z Huang"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "191",
      "title": "Cycleconsistent adversarial autoencoders for unsupervised text style transfer",
      "authors": [
        "Y Huang",
        "W Zhu",
        "D Xiong",
        "Y Zhang",
        "C Hu",
        "F Xu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "192",
      "title": "Towards fine-grained text sentiment transfer",
      "authors": [
        "F Luo",
        "P Li",
        "P Yang",
        "J Zhou",
        "Y Tan",
        "B Chang",
        "Z Sui",
        "X Sun"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "193",
      "title": "Enhancing the emotional generation capability of large language models via emotional chain-of-thought",
      "authors": [
        "Z Li",
        "G Chen",
        "R Shao",
        "D Jiang",
        "L Nie"
      ],
      "year": "2024",
      "venue": "Enhancing the emotional generation capability of large language models via emotional chain-of-thought",
      "arxiv": "arXiv:2401.06836"
    },
    {
      "citation_id": "194",
      "title": "Enhancing empathetic response generation by augmenting llms with small-scale empathetic models",
      "authors": [
        "Z Yang",
        "Z Ren",
        "W Yufeng",
        "S Peng",
        "H Sun",
        "X Zhu",
        "X Liao"
      ],
      "year": "2024",
      "venue": "Enhancing empathetic response generation by augmenting llms with small-scale empathetic models",
      "arxiv": "arXiv:2402.11801"
    },
    {
      "citation_id": "195",
      "title": "Rational sensibility: Llm enhanced empathetic response generation guided by self-presentation theory",
      "authors": [
        "L Sun",
        "N Xu",
        "J Wei",
        "B Yu",
        "L Bu",
        "Y Luo"
      ],
      "year": "2023",
      "venue": "Rational sensibility: Llm enhanced empathetic response generation guided by self-presentation theory",
      "arxiv": "arXiv:2312.08702"
    },
    {
      "citation_id": "196",
      "title": "Does gpt-3 generate empathetic dialogues? a novel in-context example selection method and automatic evaluation metric for empathetic dialogue generation",
      "authors": [
        "Y.-J Lee",
        "C.-G Lim",
        "H.-J Choi"
      ],
      "year": "2022",
      "venue": "Proceedings of the 29th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "197",
      "title": "Soulchat: Improving llms' empathy, listening, and comfort abilities through fine-tuning with multi-turn empathy conversations",
      "authors": [
        "Y Chen",
        "X Xing",
        "J Lin",
        "H Zheng",
        "Z Wang",
        "Q Liu",
        "X Xu"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2023"
    },
    {
      "citation_id": "198",
      "title": "Generating responses with a specific emotion in dialog",
      "authors": [
        "Z Song",
        "X Zheng",
        "L Liu",
        "M Xu",
        "X.-J Huang"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "199",
      "title": "Emotional chatting machine: Emotional conversation generation with internal and external memory",
      "authors": [
        "H Zhou",
        "M Huang",
        "T Zhang",
        "X Zhu",
        "B Liu"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "200",
      "title": "An adversarial approach to high-quality, sentiment-controlled neural dialogue generation",
      "authors": [
        "X Kong",
        "B Li",
        "G Neubig",
        "E Hovy",
        "Y Yang"
      ],
      "year": "2019",
      "venue": "An adversarial approach to high-quality, sentiment-controlled neural dialogue generation",
      "arxiv": "arXiv:1901.07129"
    },
    {
      "citation_id": "201",
      "title": "Dual-view conditional variational auto-encoder for emotional dialogue generation",
      "authors": [
        "M Li",
        "J Zhang",
        "X Lu",
        "C Zong"
      ],
      "year": "2021",
      "venue": "Transactions on Asian and Low-Resource Language Information Processing"
    },
    {
      "citation_id": "202",
      "title": "Generating emotional controllable response based on multi-task and dual attention framework",
      "authors": [
        "W Xu",
        "X Gu",
        "G Chen"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "203",
      "title": "Affective neural response generation",
      "authors": [
        "N Asghar",
        "P Poupart",
        "J Hoey",
        "X Jiang",
        "L Mou"
      ],
      "year": "2018",
      "venue": "Advances in Information Retrieval: 40th European Conference on IR Research, ECIR 2018"
    },
    {
      "citation_id": "204",
      "title": "Affect-driven dialog generation",
      "authors": [
        "P Colombo",
        "W Witon",
        "A Modi",
        "J Kennedy",
        "M Kapadia"
      ],
      "year": "2019",
      "venue": "Affect-driven dialog generation",
      "arxiv": "arXiv:1904.02793"
    },
    {
      "citation_id": "205",
      "title": "Automatic dialogue generation with expressed emotions",
      "authors": [
        "C Huang",
        "O Zaiane",
        "A Trabelsi",
        "N Dziri"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "206",
      "title": "Emotional dialogue generation based on transformer and conditional variational autoencoder",
      "authors": [
        "H Lin",
        "Z Deng"
      ],
      "year": "2022",
      "venue": "2022 IEEE 21st International Conference on Ubiquitous Computing and Communications"
    },
    {
      "citation_id": "207",
      "title": "Sentigan: Generating sentimental texts via mixture adversarial networks",
      "authors": [
        "K Wang",
        "X Wan"
      ],
      "year": "2018",
      "venue": "IJCAI"
    },
    {
      "citation_id": "208",
      "title": "Automatic generation of sentimental texts via mixture adversarial networks",
      "year": "2019",
      "venue": "Artificial Intelligence"
    },
    {
      "citation_id": "209",
      "title": "Emotional dialog generation via multiple classifiers based on a generative adversarial network",
      "authors": [
        "W Chen",
        "X Chen",
        "X Sun"
      ],
      "year": "2021",
      "venue": "Virtual Reality & Intelligent Hardware"
    },
    {
      "citation_id": "210",
      "title": "Diffusemp: A diffusion model-based framework with multigrained control for empathetic response generation",
      "authors": [
        "G Bi",
        "L Shen",
        "Y Cao",
        "M Chen",
        "Y Xie",
        "Z Lin",
        "X He"
      ],
      "year": "2023",
      "venue": "Diffusemp: A diffusion model-based framework with multigrained control for empathetic response generation",
      "arxiv": "arXiv:2306.01657"
    },
    {
      "citation_id": "211",
      "title": "Customizable text generation via conditional text generative adversarial network",
      "authors": [
        "J Chen",
        "Y Wu",
        "C Jia",
        "H Zheng",
        "G Huang"
      ],
      "year": "2020",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "212",
      "title": "A recipe for arbitrary text style transfer with large language models",
      "authors": [
        "E Reif",
        "D Ippolito",
        "A Yuan",
        "A Coenen",
        "C Callison-Burch",
        "J Wei"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "213",
      "title": "Text style transfer via learning style instance supported latent space",
      "authors": [
        "X Yi",
        "Z Liu",
        "W Li",
        "M Sun"
      ],
      "year": "2021",
      "venue": "Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence"
    },
    {
      "citation_id": "214",
      "title": "Dgst: A dual-generator network for text style transfer",
      "authors": [
        "X Li",
        "G Chen",
        "C Lin",
        "R Li"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "215",
      "title": "Reinforced rewards framework for text style transfer",
      "authors": [
        "A Sancheti",
        "K Krishna",
        "B Srinivasan",
        "A Natarajan"
      ],
      "year": "2020",
      "venue": "Advances in Information Retrieval: 42nd European Conference on IR Research, ECIR 2020"
    },
    {
      "citation_id": "216",
      "title": "Semi-supervised formality style transfer using language model discriminator and mutual information maximization",
      "authors": [
        "K Chawla",
        "D Yang"
      ],
      "venue": "Semi-supervised formality style transfer using language model discriminator and mutual information maximization"
    },
    {
      "citation_id": "217",
      "title": "Empathetic dialogue generation with pre-trained roberta-gpt2 and external knowledge",
      "authors": [
        "Y Liu",
        "W Maier",
        "W Minker",
        "S Ultes"
      ],
      "year": "2022",
      "venue": "Conversational AI for Natural Human-Centric Interaction: 12th International Workshop on Spoken Dialogue System Technology, IWSDS 2021"
    },
    {
      "citation_id": "218",
      "title": "Harnessing the power of large language models for empathetic response generation: Empirical investigations and improvements",
      "authors": [
        "Y Qian",
        "W Zhang",
        "T Liu"
      ],
      "year": "2023",
      "venue": "The 2023 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "219",
      "title": "Emosen: Generating sentiment and emotion controlled responses in a multimodal dialogue system",
      "authors": [
        "M Firdaus",
        "H Chauhan",
        "A Ekbal",
        "P Bhattacharyya"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "220",
      "title": "Mime: Mimicking emotions for empathetic response generation",
      "authors": [
        "N Majumder",
        "P Hong",
        "S Peng",
        "J Lu",
        "D Ghosal",
        "A Gelbukh",
        "R Mihalcea",
        "S Poria"
      ],
      "year": "2020",
      "venue": "Mime: Mimicking emotions for empathetic response generation",
      "arxiv": "arXiv:2010.01454"
    },
    {
      "citation_id": "221",
      "title": "Cem: Commonsense-aware empathetic response generation",
      "authors": [
        "S Sabour",
        "C Zheng",
        "M Huang"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "222",
      "title": "Reinforcement learning based emotional editing constraint conversation generation",
      "authors": [
        "J Li",
        "X Sun",
        "X Wei",
        "C Li",
        "J Tao"
      ],
      "year": "2019",
      "venue": "Reinforcement learning based emotional editing constraint conversation generation",
      "arxiv": "arXiv:1904.08061"
    },
    {
      "citation_id": "223",
      "title": "Emotional dialogue generation with generative adversarial networks",
      "authors": [
        "Y Li",
        "B Wu"
      ],
      "year": "2020",
      "venue": "2020 IEEE 4th Information Technology, Networking, Electronic and Automation Control Conference (ITNEC)"
    },
    {
      "citation_id": "224",
      "title": "Multi-level knowledge-enhanced prompting for empathetic dialogue generation",
      "authors": [
        "Z Gu",
        "Q Zhu",
        "H He"
      ],
      "year": "2024",
      "venue": "Proceedings of the 27th International Conference on Computer Supported Cooperative Work in Design (CSCWD)"
    },
    {
      "citation_id": "225",
      "title": "Cause-aware empathetic response generation via chain-of-thought fine-tuning",
      "authors": [
        "X Chen",
        "C Yang",
        "M Lan"
      ],
      "year": "2024",
      "venue": "Cause-aware empathetic response generation via chain-of-thought fine-tuning",
      "arxiv": "arXiv:2408.11599"
    },
    {
      "citation_id": "226",
      "title": "Audio-driven emotional video portraits",
      "authors": [
        "X Ji",
        "H Zhou",
        "K Wang",
        "W Wu",
        "C Loy",
        "X Cao",
        "F Xu"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "227",
      "title": "Gaussianpu: A hybrid 2d-3d upsampling framework for enhancing color point clouds via 3d gaussian splatting",
      "authors": [
        "Z Guo",
        "Y Xie",
        "W Xie",
        "P Huang",
        "F Ma",
        "F Yu"
      ],
      "year": "2024",
      "venue": "Gaussianpu: A hybrid 2d-3d upsampling framework for enhancing color point clouds via 3d gaussian splatting",
      "arxiv": "arXiv:2409.01581"
    },
    {
      "citation_id": "228",
      "title": "Interactive multi-level prosody control for expressive speech synthesis",
      "authors": [
        "T Cornille",
        "F Wang",
        "J Bekker"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "229",
      "title": "Tacotron: Towards endto-end speech synthesis",
      "authors": [
        "Y Wang",
        "R Skerry-Ryan",
        "D Stanton",
        "Y Wu",
        "R Weiss",
        "N Jaitly",
        "Z Yang",
        "Y Xiao",
        "Z Chen",
        "S Bengio"
      ],
      "year": "2017",
      "venue": "Tacotron: Towards endto-end speech synthesis",
      "arxiv": "arXiv:1703.10135"
    },
    {
      "citation_id": "230",
      "title": "Large language models (llms) and empathy-a systematic review",
      "authors": [
        "V Sorin",
        "D Brin",
        "Y Barash",
        "E Konen",
        "A Charney",
        "G Nadkarni",
        "E Klang"
      ],
      "year": "2023",
      "venue": "medRxiv"
    },
    {
      "citation_id": "231",
      "title": "Multimodal machine learning: A survey and taxonomy",
      "authors": [
        "T Baltrušaitis",
        "C Ahuja",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "232",
      "title": "Visualrwkv: Exploring recurrent neural networks for visual language models",
      "authors": [
        "H Hou",
        "P Zeng",
        "F Ma",
        "F Yu"
      ],
      "year": "2024",
      "venue": "Visualrwkv: Exploring recurrent neural networks for visual language models",
      "arxiv": "arXiv:2406.13362"
    },
    {
      "citation_id": "233",
      "title": "A comprehensive review of data-driven co-speech gesture generation",
      "authors": [
        "S Nyatsanga",
        "T Kucherenko",
        "C Ahuja",
        "G Henter",
        "M Neff"
      ],
      "year": "2023",
      "venue": "Computer Graphics Forum"
    },
    {
      "citation_id": "234",
      "title": "Virtual and augmented reality for developing emotional intelligence skills",
      "authors": [
        "C Papoutsi",
        "A Drigas",
        "C Skianis"
      ],
      "year": "2021",
      "venue": "Int. J. Recent Contrib. Eng. Sci. IT (IJES)"
    },
    {
      "citation_id": "235",
      "title": "An overview on edge computing research",
      "authors": [
        "K Cao",
        "Y Liu",
        "G Meng",
        "Q Sun"
      ],
      "year": "2020",
      "venue": "IEEE access"
    },
    {
      "citation_id": "236",
      "title": "Wearables and the internet of things (iot), applications, opportunities, and challenges: A survey",
      "authors": [
        "F Dian",
        "R Vahidnia",
        "A Rahmati"
      ],
      "year": "2020",
      "venue": "IEEE access"
    },
    {
      "citation_id": "237",
      "title": "The impact of artificial intelligence on animation filmmaking: Tools, trends, and future implications",
      "authors": [
        "M Izani",
        "A Razak",
        "D Rehad",
        "M Rosli"
      ],
      "year": "2024",
      "venue": "2024 International Visualization, Informatics and Technology Conference (IVIT)"
    },
    {
      "citation_id": "238",
      "title": "Original research article revolutionizing filmmaking: A comparative analysis of conventional and ai-generated film production in the era of virtual reality",
      "authors": [
        "A Channa",
        "A Sharma",
        "M Singh",
        "P Malhotra",
        "A Bajpai",
        "P Whig"
      ],
      "year": "2024",
      "venue": "Journal of Autonomous Intelligence"
    },
    {
      "citation_id": "239",
      "title": "The application and user acceptance of aigc in network audiovisual field: Based on the perspective of social cognitive theory",
      "authors": [
        "H Sun",
        "Y He"
      ],
      "year": "2023",
      "venue": "Proceedings of the 7th International Conference on Computer Science and Application Engineering"
    }
  ]
}