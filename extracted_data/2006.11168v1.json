{
  "paper_id": "2006.11168v1",
  "title": "Emotion Recognition On Large Video Dataset Based On Convolutional Feature Extractor And Recurrent Neural Network",
  "published": "2020-06-19T14:54:13Z",
  "authors": [
    "Denis Rangulov",
    "Muhammad Fahim"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "For many years, the emotion recognition task has remained one of the most interesting and important problems in the field of human-computer interaction. In this study, we consider the emotion recognition task as a classification as well as a regression task by processing encoded emotions in different datasets using deep learning models. Our model combines convolutional neural network (CNN) with recurrent neural network (RNN) to predict dimensional emotions on video data. At the first step, CNN extracts feature vectors from video frames. In the second step, we fed these feature vectors to train RNN for exploiting the temporal dynamics of video. Furthermore, we analyzed how each neural network contributes to the systems overall performance. The experiments are performed on publicly available datasets including the largest modern Aff-Wild2 database. It contains over sixty hours of video data. We discovered the problem of overfitting of the model on an unbalanced dataset with an illustrative example using confusion matrices. The problem is solved by downsampling technique to balance the dataset. By significantly decreasing training data, we balance the dataset, thereby, the overall performance of the model is improved. Hence, the study qualitatively describes the abilities of deep learning models exploring enough amount of data to predict facial emotions. Our proposed method is implemented using Tensorflow Keras. The code is publicly available in repository 1 .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Emotions are the main indicators of human feelings. They can describe a distinct set of changes in physical state such as blood pressure rising or a particular face muscle moving. Several classical views of emotions suggest classification on a few basic categories like anger or happiness. Such views assume that each emotion has a defining underlying pattern in the brain and body. Another prospective assumes that the brain analyse past experience and predicts what the body should do in a similar situation. This theory is known as constructed emotions  [1] . They are not triggered, on the contrary, we create them in our way.\n\nEmotion capturing from videos is the simplest approach due to the simplicity of video recording in comparison with recording some physiological measurements such as EEG, blood pressure, and so on. Emotion recognition from videos has been a challenge for researcher community for many decades. Traditional way to encode emotions is developed by psychologist Paul Ekman  [3] . He divided all possible emotions into six basic categories: anger, disgust, fear, happiness, sadness, and surprise. These emotions were selected because 1 https://github.com/DenisRang/Combined-CNN-RNNfor-emotion-recognition they were all perceived similarly regardless of culture. The prediction of these emotions already gives an abundant useful information about a person and enable further research of his or her condition. It is the oldest model of emotion recognition, so many existing technologies are based on this.\n\nThe main problem of previous encoding method is the constraint by only six categories. An effective way to deal with that can be to try to encode emotions in dimensional space to make it continuous. One coordinate shows how positive an emotion is and another tells how engaged or apathetic a subject appears. This way of encoding gives the ability to transform multiple dimensional coordinates to more complex emotion category. Such a complex emotion takes us closer to the theory of constructed emotions  [1] . The second advantage is to output continuous emotion labels for video data containing continuous sequences of frames. Such model of emotion space is much more realistic and described in  [18] . Fig.  1 , present the dimensional emotions in valencearousal space.\n\nFig.  1 : Dimensional structure for the semantic space of emotions  [18]  Prediction of emotions from videos is an analysis of sequences of frames for specific feature changes. These features can represent some facial muscles that can move in a particular order. Therefore, in every video particular emotional patterns are encoded. Features can be computed using traditional Computer Vision (CV) techniques  [16] . However, this way of solving emotion recognition task requires a significant effort from CV engineers such as deciding which features are important in each given frame for every kind of facial expression. As the number of emotions increases, feature extraction becomes more complicated.\n\nOn the other hand, the deep learning models can be fed with a whole dataset of annotated images. Thus, neural networks are able to discover the underlying patterns in video frames in a fast and accurate way. One of the main problems of such models for emotion recognition is the complexity of their architecture. If the complexity is too high, the model will take many computational resources for training and prediction. It is critical for embedded systems. The second consequence of using complex architectures is the possible overfitting of the model for recognizing emotions only from the subjects of the training set. In this research, we solve this challenge by creating a simple model with a low number of parameters to efficiently perform training and prediction of emotion recognition task.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Contribution And Paper Outline",
      "text": "This study has the following contribution into research community:\n\n• A simple model to provide better results on large video dataset.\n\n• Discover and address the problem of overfitting of the model on unbalanced dataset with an illustrative example using confusion matrices. • Introduced data balancing technique by decreased number of training samples by 36%, which contribute to improve the results . • Finally, we provide analysis of neural networks by explaining how much the CNN and the RNN individually contribute to the overall performance. The rest of this paper is organized as follows: we briefly describe the related work in Section III. Section IV provides the details about the proposed methodology. In Section V, we presents experimental details followed by results and discussion in Section VI. Finally, Section VII provides conclusions and possible future directions.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Related Work",
      "text": "For dimensional emotions in valence-arousal space, the good way to feed input images to deep learning models is to convert them into sequences like usual order of frames in videos. Training on such data can exploit the temporal dynamics of the video. Feeding neural networks with the sequences can be done in different ways ad describes as follows:\n\nA. CNN and RNN trained jointly Kollias et al.  [10]  developed an architecture by utilizing existing state-of-the-art CNN architectures together with different RNNs for Aff-Wild2 challenge. Nested CNN model are pretrained on different datasets with faces or random images. Then CNN and RNN are jointly trained in specific sequence to achieve good results on RECOLA  [17]  and Aff-Wild2  [7] ,  [9] -  [12] ,  [19]  datasets. This way of training has disadvantages such as the complexity of the overall combined model and therefore expensiveness of computational resources. Besides, they used attention mechanism by stacking an attention layer on top of the RNN. The attention mechanism deals with the problem of the limited shortterm memory of RNNs. The aforementioned mechanism is a game-changing innovation that addressed this problem.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Cnn And Rnn Trained Separately",
      "text": "Another approach is presented in  [6] , where author adapts CNN model to work with valence and arousal data from RECOLA dataset  [17] . The last dense layer was changed with softmax activation for categorical emotions and regression layer to work with dimensional emotions. They proposed mechanism to incorporate the temporal information by using an RNN to propagate information from one time point to the next. Each input to the RNN is comprised of features from the second to the last fully connected layer of a single frame CNN. The main difference of this approach from previous is in separate training of CNN and RNN. They provide an evaluation of how much the CNN and the RNN individually contribute to the overall performance. They extend this approach to take audio and physiological data into consideration. He discovers usefulness of different features for prediction and examined how adding the audio and physiological features affected performance. Such types of data is not significant, if compared to video data.\n\nOur approach utilized CNN architecture as a feature extractor and further connected with RNN gated recurrent units (GRU) to recognize the dimensional emotion in valencearousal space. Furthermore, obtained results are better then the baseline model of Aff-Wild2 challenge.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iv. Methodology",
      "text": "The proposed system is presented in Fig.  2 . It consists of feature extractor and exploiting temporal dynamics of video unit to predict the valence and arousal score. The feature extractor is based on CNN architecture  [6]  with few modifications. We choose this architecture because of its simplicity comparing to other complex ones, to achieve a trade-off between underfitting and overfitting. The CNN model is trained separately from RNN, to extract relevant feature vectors from images. Then RNN can learn useful temporal dynamic information of sequences of feature vectors. This approach takes lesser computation resources comparing with the others. The main reason is that we do not need to propagate frames every time during the training of the RNN. It allows us to use large batches with small feature vectors instead of large frames, which ensures better performance in terms of consumed memory.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Datasets",
      "text": "In this research, we consider three different datasets to perform the experiments. The first Aff-Wild2  [10]  dataset is the largest dataset among other existing emotion datasets. According to train-validation partition and annotations provided by the ABAW 2020 challenge organizers  [8] , there are 351 and 71 subjects in the training and validation subsets respectively for the valance-arousal estimation track. The number of samples with either low arousal or low valence is small, because they are weakly expressed or negative emotions are rare as compared to highly expressed positive emotions. Hence the dataset is highly unbalanced.\n\nThe other two oldest and classical datasets the Extended Cohn-Kanade Dataset (CK+)  [14]  and Japanese Female Facial Expression (JAFFE)  [15]",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Dataset Preprocessing",
      "text": "We use all sequences from 118 subjects of CK+ dataset as in  [6]  experiments. For each sequence, the first image (i.e., neutral face) and three peak frames are used for prototypic expression recognition. For each image face is detected via Haar cascades  [5]  then cropped and resized to 96 × 96. We use gray-scaled images. The same preprocessing are used for JAFFE dataset. For Aff-Wild2 dataset, we are provided with all cropped-aligned face images of 112 × 112 dimensions, so we only need to resize it to 96 × 96. We exploit the full available data (training and validation). For training RNN, frames are combined into continuous sequences with length 100. Such window size is the best in  [6] , during hyperparameter analysis experiments. Sequences are created only if there are no frames where a face has not been detected and both valence and arousal have been calculated. Thus, they do not have a gap between continuous frames. For frames where a model can not predict emotions, valence and arousal scores were later computed by linearly interpolating the scores from adjacent frames.\n\nBesides, for Aff-Wild2 dataset, additional preprocessing is applied to reduce the overfitting. The overfitting is a possible issue that has bad influence on overall performance of our model for such an unbalanced dataset. We performed the following preprocessing to solve the challenge.\n\n1) Data augmentation: We apply different data augmentations such as scaling, shifting, shearing, rotation, horizontal flips and changing brightness to make training data as diverse as possible.\n\n2) Balancing dataset: One way to balance a large dataset is to use downsampling. Firstly, we remove frames with both values of valence and arousal equal to zero. Secondly, we divide values of valence and arousal on 40 bins. Then we compute density of all 40x40 bins for all samples in the dataset. After that we select a frame for training with probability:\n\nwhere k current is a number of samples in bin of this frame and k max is a number of samples in bin of the most frequent frame. k max was differently chosen for three settings: 1) Number of samples in bin with maximum number of samples (subset 1).",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "2) Mean Number Of Samples Among All Bins (Subset 2). 3) Average Between First Two Option (Subset 3).",
      "text": "As a result, we get different downsampled subsets of whole Aff-Wild2 dataset. They are shown in Fig.  3 .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Feature Extractor",
      "text": "The main block of the overall system is feature extractor from images. Hence we evaluate different configuration of the CNN architecture proposed in  [6]  to solve a single frame regression task. Author solves the same emotion recognition task with smaller dataset. The best configuration is used further in RNNs training module.\n\n1) CNN architecture: Our CNN architecture is based on Khorrami  [6] . He used a classical feed-forward convolutional neural network. In our networ, we added batch normalization first layer for zero-centering and normalizing each input. The network consist of three convolutional layers with 64, 128, and 256 filters. All filters have the same kernel size of 5 × 5 followed by Rectified Linear Unit(ReLU) activation functions. After the first two convolutional layers there are max pooling layers where quadrant pooling  [2]  is applied after the third. Output of the quadrant pooling has 2×2 dimension per filter. Then a fully-connected layer with 300 hidden units is applied. This fully-connected layer is followed by dropout layer with probability 0.5.\n\nTwo different last layers are used for different tasks. The softmax layer is used for emotion classification on small datasets. This layer contains eight outputs corresponding to the number of expressions present in the CK+ training set. Architecture of the such model is in Fig.  4 . For regression task with data from Aff-Wild2 dataset, the dense layer with two units for predicting valence and arousal are used.\n\n2) Loss function: A loss function is used to optimize the parameter values in a neural network model. In our model, Eq. (  2 ) measures valence and arousal Concordance Correlation Coefficient(CCC)  [13]  value and Eq. (  3 ) is main loss function of dimensional emotion model.\n\n(2)",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "D. Exploiting Of Temporal Dynamics Of Video Data",
      "text": "Our feature extractor CNN is trained on single frames so it completely ignores dynamics of changing emotions in sequence of frames in a single video. To address this issue we transform our dataset from videos to sequences of feature vectors and then fit RNN on it. We visualize such model in Fig.  5 . Fig.  5 : A combined CNN with RNN. Given a time t in a video, we extract a window of length W frames ([t -W, t]) to predict valence and arousal at time t+1\n\nWe utlized the best performing number of units of RNN model from  [6] . They had three hidden RNN layers with 100 hidden units in the first and second recurrent layers and 50 in the third. We compare it with architecture with single hidden RNN layer with 100 hidden units. We do not know how many frames we should remember to predict emotion in the next frame. It can be a few previous frames or all previous frames in a video. Therefore, we consider different types of RNN cells: simple RNN and GRU, which can capture short-term and long-term patterns in sequences, respectively. Final RNN models are represented in two configurations. The first one contains 1 layer with 100 hidden units. The second one contains 3 layers with 100 hidden units in the first two recurrent layers and 50 in the third.\n\nV. EXPERIMENTS DETAIL We utilized Google Colab to work with CK+ and JAFFE dataset, while GPU server with NVIDIA Tesla V100 (16 GB) graphics processor were used to process the large Aff-Wild2 dataset. Firstly, we validated the CNN model to get the similar results on small CK+ dataset as acheived in paper  [6] . Then we evaluated this model on additional scopes by predicting emotion labels on unseen JAFFE dataset. Secondly, we experiment with large Aff-Wild2 dataset to find best configuration of feature extractor model. We compared separate predicting valence or arousal with predicting valence and arousal simultaneously by changing the number of units in the last dense layer of the CNN model from 1 to 2. Besides, we experimented with two loss functions: mean squared error (MSE) and difference between 1 and average CCC for valence and arousal described early. Lastly, we trained different RNN models on features from the best feature extractor model.\n\nTwo similar training strategies for the CNN model were used for emotion classification on CK+ dataset and regression tasks on Aff-Wild2 dataset. For all experiments we trained CNN and RNN models for 100 epochs with early stopping criteria. The batch size of 64 and 128 is used for CNN and RNN, respectively. They were trained from scratch using stochastic gradien descent (SGD) optimizer with momentum 0.9, and weight decay parameter as 1e -5. Along with that, we used constant learning rate of 0.01. The parameters of each layer were initialized by default Xavier initialization or Glorot initialization strategy  [4] .",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Vi. Results And Discussion",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Experiments On Small Ck+ And Jaffe Datasets",
      "text": "The CNN model trained over CK+ dataset was tested on test folder of CK+ and whole JAFFE dataset. The obtained results are presented in Table  I . As we can see, the accuracy on the CK+ dataset is much higher, that the one from the JAFFE dataset.  Our thoughts were, as CK+ dataset was unbalanced and contains more examples of the Neutral, Surprise classes than the other classes. The number of training samples from CK+ and the number of all samples from JAFFE with respect to emotion labels are shown in Table  II .\n\nThus, we also plotted the confusion matrix, to see, how well the model handles class with smaller amount of examples. In the Fig.  6 , it is obvious that almost all classes, the model handles well for CK+. However, if we look on the confusion matrix of JAFFE dataset in the Fig.  7   The best results was obtained by the CNN predicting valence and arousal simultaneously with average CCC loss function. It is a little bit better than the baseline. We obtained a similar ratio between the CNN and the baseline model results with the ratio in  [6]  where the model was tested on the RECOLA dataset. Results are shown in Table  III . As we can see, it is sufficient to train model over subset 3 instead of the whole dataset. Hence, we achieved slightly better training results using only subset 3 with 1,030,963 samples versus all 1,620,421 samples. We selected the CNN trained on subset 3 for feature extraction. After that we trained different RNN on the extracted features from all images. There are 1,496,928 sequences with 100 feature vectors of size 300 in total. From the results, different RNNs can efficiently exploit the temporal dynamics of the data. Simple RNN shows worse results than GRU. Therefore, capturing long-term patterns is more efficient than capturing short-term patterns in small number of sequential frames in video sequence.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Vii. Conclusion",
      "text": "In this paper, we developed a deep neural networks for the task of emotion recognition. This task was split into two separate subtasks: feature extraction and exploiting of temporal dynamics of video data. We performed experiment with simple architecture on the largest modern video database Aff-Wild2. We discovered the problem of overfitting of the model on the unbalanced dataset with an illustrative example using confusion matrices. Next, we tried to balance the dataset by different downsampling. Therefore, we decreased the number of training samples on 36% from 1,620,421 to 1,030,963 frames and achieved better performance of the model. We experimented with Aff-Wild2 database using the CNN and a combination of this model with different RNN models such as simple RNN and GRU. We obtain better results than baseline model with simple and general model, whose number of parameters is low. Therefore, such neural network can efficiently perform training and prediction emotion recognition task.\n\nFor future work, we are considering to adapt our model to multitask learning of different human affective behavior characteristics in-the-wild at the same time. These behavior characteristics are emotion encoded in one of seven basic categories, Action Units (AUs) describing all possible facial actions, and valence-arousal emotion encoding small changes in the intensity of each emotion on a continuous scale.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , present the dimensional emotions in valence-",
      "page": 1
    },
    {
      "caption": "Figure 1: Dimensional structure for the semantic space of",
      "page": 1
    },
    {
      "caption": "Figure 2: It consists of",
      "page": 2
    },
    {
      "caption": "Figure 2: The block diagram of proposed model",
      "page": 2
    },
    {
      "caption": "Figure 3: (a) Subset 1 with 1,267,932 sam-",
      "page": 3
    },
    {
      "caption": "Figure 3: 2D valence-arousal histograms of Aff-Wild2 and",
      "page": 3
    },
    {
      "caption": "Figure 4: CNN architecture for emotion classiﬁcation task on 8 categories",
      "page": 4
    },
    {
      "caption": "Figure 4: For regression",
      "page": 4
    },
    {
      "caption": "Figure 5: Fig. 5: A combined CNN with RNN. Given a time t in a",
      "page": 4
    },
    {
      "caption": "Figure 6: , it is obvious that almost all classes, the",
      "page": 5
    },
    {
      "caption": "Figure 6: Confusion matrix over CK+ dataset",
      "page": 5
    },
    {
      "caption": "Figure 7: Confusion matrix over JAFFE dataset",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "",
          "RMSE": "Valence\nArousal",
          "CCC": "Valence\nArousal",
          "Best # epochs": ""
        },
        {
          "Method": "Baseline",
          "RMSE": "-\n-",
          "CCC": "0.14\n0.24",
          "Best # epochs": "-"
        },
        {
          "Method": "CNN over whole Aff-Wild2",
          "RMSE": "0.47\n0.3",
          "CCC": "0.2\n0.23",
          "Best # epochs": "2"
        },
        {
          "Method": "CNN over subset 3",
          "RMSE": "0.43\n0.3",
          "CCC": "0.22\n0.22",
          "Best # epochs": "1"
        },
        {
          "Method": "CNN + Simple RNN, 1 layer",
          "RMSE": "0.49\n0.37",
          "CCC": "0.21\n0.22",
          "Best # epochs": "4"
        },
        {
          "Method": "CNN + GRU, 1 layer",
          "RMSE": "0.51\n0.32",
          "CCC": "0.2\n0.35",
          "Best # epochs": "8"
        },
        {
          "Method": "CNN + GRU, 3 layer",
          "RMSE": "0.45\n0.29",
          "CCC": "0.23\n0.39",
          "Best # epochs": "17"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "How emotions are made: The secret life of the brain",
      "authors": [
        "L Barrett"
      ],
      "year": "2017",
      "venue": "How emotions are made: The secret life of the brain"
    },
    {
      "citation_id": "2",
      "title": "An analysis of single-layer networks in unsupervised feature learning",
      "authors": [
        "A Coates",
        "A Ng",
        "H Lee"
      ],
      "year": "2011",
      "venue": "Proceedings of the fourteenth international conference on artificial intelligence and statistics"
    },
    {
      "citation_id": "3",
      "title": "Universal and cultural differences in facial expression of emotion",
      "authors": [
        "P Eckman"
      ],
      "year": "1972",
      "venue": "Nebraska symposium on motivation"
    },
    {
      "citation_id": "4",
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "authors": [
        "X Glorot",
        "Y Bengio"
      ],
      "year": "2010",
      "venue": "Proceedings of the thirteenth international conference on artificial intelligence and statistics"
    },
    {
      "citation_id": "5",
      "title": "Fast multi-view face detection",
      "authors": [
        "M Jones",
        "P Viola"
      ],
      "year": "2003",
      "venue": "Fast multi-view face detection"
    },
    {
      "citation_id": "6",
      "title": "How deep learning can help emotion recognition",
      "authors": [
        "P Khorrami"
      ],
      "year": "2017",
      "venue": "How deep learning can help emotion recognition"
    },
    {
      "citation_id": "7",
      "title": "Recognition of affect in the wild using deep neural networks",
      "authors": [
        "D Kollias",
        "M Nicolaou",
        "I Kotsia",
        "G Zhao",
        "S Zafeiriou"
      ],
      "year": "2017",
      "venue": "Computer Vision and Pattern Recognition Workshops (CVPRW), 2017 IEEE Conference on"
    },
    {
      "citation_id": "8",
      "title": "Analysing affective behavior in the first abaw 2020 competition",
      "authors": [
        "D Kollias",
        "A Schulc",
        "E Hajiyev",
        "S Zafeiriou"
      ],
      "year": "2020",
      "venue": "Analysing affective behavior in the first abaw 2020 competition",
      "arxiv": "arXiv:2001.11409"
    },
    {
      "citation_id": "9",
      "title": "Deep affect prediction in-thewild: Aff-wild database and challenge, deep architectures, and beyond",
      "authors": [
        "D Kollias",
        "P Tzirakis",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "B Schuller",
        "I Kotsia",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "10",
      "title": "Aff-wild2: Extending the aff-wild database for affect recognition",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2018",
      "venue": "Aff-wild2: Extending the aff-wild database for affect recognition",
      "arxiv": "arXiv:1811.07770"
    },
    {
      "citation_id": "11",
      "title": "A multi-task learning & generation framework: Valence-arousal, action units & primary expressions",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2018",
      "venue": "A multi-task learning & generation framework: Valence-arousal, action units & primary expressions",
      "arxiv": "arXiv:1811.07771"
    },
    {
      "citation_id": "12",
      "title": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "arxiv": "arXiv:1910.04855"
    },
    {
      "citation_id": "13",
      "title": "A concordance correlation coefficient to evaluate reproducibility",
      "authors": [
        "I Lawrence",
        "K Lin"
      ],
      "year": "1989",
      "venue": "Biometrics"
    },
    {
      "citation_id": "14",
      "title": "The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression",
      "authors": [
        "P Lucey",
        "J Cohn",
        "T Kanade",
        "J Saragih",
        "Z Ambadar",
        "I Matthews"
      ],
      "year": "2010",
      "venue": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition-Workshops"
    },
    {
      "citation_id": "15",
      "title": "Please get in touch beforehand concerning use of the images not covered by the conditions stated above",
      "authors": [
        "M Lyons",
        "M Kamachi",
        "J Gyoba"
      ],
      "year": "1998",
      "venue": "Please get in touch beforehand concerning use of the images not covered by the conditions stated above"
    },
    {
      "citation_id": "16",
      "title": "Deep learning vs. traditional computer vision",
      "authors": [
        "N Omahony",
        "S Campbell",
        "A Carvalho",
        "S Harapanahalli",
        "G Hernandez",
        "L Krpalkova",
        "D Riordan",
        "J Walsh"
      ],
      "year": "2019",
      "venue": "Science and Information Conference"
    },
    {
      "citation_id": "17",
      "title": "Introducing the recola multimodal corpus of remote collaborative and affective interactions",
      "authors": [
        "F Ringeval",
        "A Sonderegger",
        "J Sauer",
        "D Lalanne"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "18",
      "title": "What are emotions? and how can they be measured? Social science information",
      "authors": [
        "K Scherer"
      ],
      "year": "2005",
      "venue": "What are emotions? and how can they be measured? Social science information"
    },
    {
      "citation_id": "19",
      "title": "Aff-wild: Valence and arousal in-the-wildchallenge",
      "authors": [
        "S Zafeiriou",
        "D Kollias",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "I Kotsia"
      ],
      "year": "2017",
      "venue": "Computer Vision and Pattern Recognition Workshops (CVPRW), 2017 IEEE Conference on"
    }
  ]
}