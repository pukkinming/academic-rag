{
  "paper_id": "2212.12266v1",
  "title": "Large Raw Emotional Dataset With Aggregation Mechanism",
  "published": "2022-12-23T11:31:02Z",
  "authors": [
    "Vladimir Kondratenko",
    "Artem Sokolov",
    "Nikolay Karpov",
    "Oleg Kutuzov",
    "Nikita Savushkin",
    "Fyodor Minkin"
  ],
  "keywords": [
    "Emotion recognition",
    "speech analysis",
    "speech data set"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "We present a new data set for speech emotion recognition (SER) tasks called Dusha. The corpus contains approximately 350 hours of data, more than 300 000 audio recordings with Russian speech and their transcripts. Therefore it is the biggest open bi-modal data collection for SER task nowadays. It is annotated using a crowd-sourcing platform and includes two subsets: acted and real-life. Acted subset has a more balanced class distribution than the unbalanced real-life part consisting of audio podcasts. So the first one is suitable for model pre-training, and the second is elaborated for fine-tuning purposes, model approbation, and validation. This paper describes pre-processing routine, annotation, and experiment with a baseline model to demonstrate some actual metrics which could be obtained with the Dusha data set.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "There are a lot of recent studies in the field of human behavior analysis and automatic speech emotion recognition (SER). Many of them use various inputs such as speech, video, and transcript as multi-modal data. The popular approach of such research is to invent a new neural network architecture and train it on the open data sets and benchmarks  [1] ,  [2] . However, some aspects have a negative impact on the process of model training and evaluation. For instance, the small size of the open data aset frequently becomes a bottleneck for research. One more possible shortcoming is biasing between label annotation of data set and user emotions in the real world  [3] . It is highly desirable for a data set to involve as many label evaluators as possible but, practically, it is complicated enough to implement  [4] . Another issue is the lack of speaker diversity which leads to the model underperforming when it faces a new speaker in a training set or in a real-time speech.\n\nThese issues with the existing big open data sets motivated us to develop a new extensive database with Russian speech. We call it Dusha, which means Soul in Slavonic languages. It is designed to reveal such concepts as peace, openness and vast nature of the Eastern-European soul. We believe that our corpus can help to improve results in other languages using cross-corpus study  [5]  or transfer learning techniques on speech emotion recognition. The data set contains recordings of speech and their transcripts. That is why we call it bi-modal.\n\nTwo sources of speech are used: acted crowd-sourced records and real-life podcasts in the Russian language. We consider that such a combination of domains is common in a real-life scenario when a model developer has less data from a target domain and much more from another crowd-sourced one. We select the emotions that appear in the dialogue with a virtual assistant most frequently: Anger, Happiness, Neutral emotion, and Sadness.\n\nEach item has been labelled by several annotators using 4 emotional classes so that markup could be aggregated into one confident label or multi-labelled. Along with the data, we share aggregation mechanism, so that any data scientist could get access to them to conduct research.\n\nThis paper delivered to the open source an advanced speech emotion recognition data set with transcription. Also it describes approaches and methods for data set collection and markup. All data and processing scripts are released on a GitHub repository 1  .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Works",
      "text": "To highlight our contribution, we analyzed existing Speech Emotional databases and compared our benchmarks with those including corpora with the Russian language.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Emotional Speech Datasets",
      "text": "The interactive emotional dyadic motion capture database (IEMOCAP)  [6]  is a widely used multimodal data set that is de facto preferable for modern research comparison in emotion recognition and sentiment analysis. It contains visual data, audio tracks of dialogues, and transcribed text. Besides, this database includes motion data for faces and hands only. Five male and five female semi-professional actors recorded their voices for this data set. IEMOCAP exhibits the balanced distribution of emotions from the following list: happiness, anger, sadness, frustration, and neutral emotion. This material includes about 12 hours of an audio split in 5 dyadic sessions. Although the data set is balanced, its disadvantage is that it is not very extensive and has few speakers involved. Mostly, the benchmark is applicable for model comparing, yet it can cause an issue with precision during evaluation in live speech. It is a common researching practice to take a subset of IEMOCAP with four classes of emotions: happiness, sadness, anger and neutral emotion (where the excitement is combined with happiness)  [5] . This set is referred to as IEMOCAP4.\n\nThe CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI) database  [7]  is another human multi-modal language benchmark. The data set is the next generation of CMU-MOSI  [8]  and involves YouTube video recordings with the voices of 1000 distinct English speakers, text transcription of audio, and emotion annotation for each utterance. In addition to the size of CMU-MOSEI, one of its strong points is that emotions are not acted. However, the emotion annotation of this benchmark was conducted by only 3 crowdsourced persons. Potentially, such a few number annotators could lead to a gap in accuracy for the evaluation and include some bias compared to real data, even if they pass special training.\n\nAmong widely-spoken languages, Chinese (Mandarin) and Spanish are also covered by numerous data sets. German domain is widely represented in emotion databases too  [9] ,  [10] . The most famous one is EmoDB  [11] .\n\nAn attempt to create an enormous repository by joining several various languages was described in  [12] . The authors presented a united database that included subsets with English, German, Chinese, Turkish and other languages.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Datasets In Russian Domain",
      "text": "Currently, there are very few data collections for emotional speech recognition available in the Russian language.\n\nOne of the first attempts to organise a Russian emotional data set is described in  [13] . This set of audio utterances and their transcriptions is called Russian Language Affective speech database (RUSLANA). Students of various Russian universities, participating as speakers, dictated in total 6.400 utterances with the corresponding emotions.\n\nRussian Multimodal Corpus of Dyadic Interaction for Studying Emotion Recognition (RAMAS)  [14]  is another widely known Russian language data set. Similar to IEMO-CAP, it includes acted recordings with 7 hours of emotional speech. The corpus provides video and audio modality, transcripts, motion, and physiology data. It annotated the following emotions: Anger, Sadness, Disgust, Happiness, Fear, Surprise. Ten actors participated in the recording of the video clips for this benchmark.\n\nOne more Russian database which could be employed for SER is Multimodal Russian Corpus (MURCO)  [15]  which is a part of the Russian National Corpus (RNC). It stores clips from Russian cinematography, TV and radio programs, recordings of usual conversations, etc. Although MURCO has millions of recordings, it has quite obsolete and unfriendly interfaces for automatic data retrieving. The complete list of emotion classes is not defined.\n\nWe consider the problem of large-scale data sets for SER tasks. When faced with real-life emotions, the data set would become a framework to conduct research and establish a connection between obtained results in the laboratory and system behavior. In addition, MLS  [16]  and Golos  [17]  data sets play a major part in the automatic speech recognition (ASR) task. Therefore, we decided to collect and share a large multimodal (audio and text) data set in the Russian domain and involve both acted and real-life data.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Data Acquisition",
      "text": "The Dusha data set consists of two logical parts which are obtained in completely different ways. The first one is collected with the assistance of non-professional actors on a popular crowd-sourcing platform Yandex Tolloka 2  . Further in the text, we call it \"Crowd domain\" or \"Crowd\". The second part consists of a speech from various emotional podcasts. We call it \"Podcast domain\" or \"Podcast\".",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Crowd Subset Collection",
      "text": "The text for crowd recordings was chosen from genuine requests which users fulfilled via virtual voice assistant Salute and SmartSpeech 3  service for speech recognition. Raw data set included tens of millions of recordings and their transcriptions. It is evident that most voice requests involve an urge to do something like \"Salute, turn on YouTube\", \"Salute, sign me up for a hairdresser\" and other phrases and talks which users send to their voice assistant with neutral emotion. To balance our data, we filtered out requests and kept recordings with conversation (chit chat) because this subset could include more explicit emotional utterances. To do so we employed Salute internal intent classifier, which separates various types of voice commands and selects chatter requests when no action except response is required. The resulting subset was several millions of utterances.\n\nNext, we applied an emotional pseudo labelling of texts to establish what emotions could be acted for utterances. We employed a simple classifier on the top of a BERT-large version of well-known BERT architecture  [18]  which was trained from scratch internally and could classify our texts for 4 target sentiments: anger, happiness, sadness and neutral emo-tion. The investigation result demonstrates that neutral emotion dominated in a significant number of cases. To evaluate our pseudo labels we conducted a survey on a crowdsourcing platform where we asked to label manually a small part (˜10.000) of utterances and compare with classifier results. It shows that our pseudo labels are sufficiently accurate. We use them to sample emotional utterances and decrease the count of neutral recordings.\n\nNext, we carried out audio voicing with the help of nonprofessional actors on a crowd-source platform. We took pseudo labels predicted on the previous step into account and for each phrase we set one emotion from the label and one more with similar emotion valence or neutral sentiment. For instance, we organized emotions in pairs positive/neutral, sadness/neutral, anger/sadness etc.\n\nThus, the actors had to pronounce the text with one of the emotions from the pair. Also, we provided a description on how to better voice the emotion.\n\nTotally, we obtained 201 850 acted emotions with 2 068 unique speakers where, neutral emotion dominates as in reallife situations however other classes are quite balanced. Blue column on Figure  1  (a) represents the time length distribution. As people used their own equipment, the quality of audio files differs. Audio can contain background noises, such as children and animal voices or street sounds. Total length is about 255 hours.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Podcast Subset Collection",
      "text": "The Podcast subset was designed to diversify data in the Dusha database. Emotions in these recordings are not performed, but rather sincere. Furthermore, the distribution of emotions for this data set corresponds better to their distribution in usual human speech. Podcast domain is not balanced and the neutral emotion class substantially outnumbers other classes. Moreover, since acted emotions may differ slightly from the spontaneous real-life emotions, we consider it reasonable to keep this subset with natural class distribution in the Dusha. The Podcast could be used for fine-tuning goals and assessing the quality of the model for the production system.\n\nWe obtained a topic diversity and included entries on politics, IT, games, relationships, etc. We do not fulfil any specific podcast choosing or filtering and just trying to cover various conversation topics. Recordings were sliced into 5-second segments by a voice activity detector (VAD) to simplify emotion annotation (See Figure  1 (a) orange color). A total of 6240 podcasts were used, of which 102 113 samples were selected. In general, the Podcast audio is recorded with professional equipment and has a better quality than the Crowd. We normalized files to 16-bit, 16 000 Hz. Total length is greater than 90 hours.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Post-Processing And Annotation",
      "text": "To avoid implicit bias in annotation on crowd-sourcing platform each person took the training and passed the exam. All participants who had attained a passing score above 80% were allowed to evaluate data.\n\nParticipants listened to the audio only and did not have access to the transcript to evaluate emotions of Crowd and Podcast domain. Annotators were given instructions to choose their labels using one of the five options:\n\nPositive: the text is spoken with a smile, or laughter, or admiration, or a playful tone, or there are pronounced stresses on words emphasizing the positive.\n\nNeutral: the voice is still and calm, there is no emotion in the voice. At the same time, even if the text is clearly negative (for example, \"how tired you are\") or positive (for example, \"what a fine fellow you are\"), this emotion is not expressed in voices, it is necessary to mark the emotion as neutral.\n\nSadness: the text is pronounced with sadness, melancholy, a faded voice.\n\nAnger/Irritation: if the text is spoken with anger or annoyance, or the user is yelling or speaking through gritted teeth, or there are pronounced stresses on words emphasizing the negative.\n\nOther: the recording is too quiet, hissing, rattling or there is no voice.\n\nIn order to ensure the quality of markup, each person from time to time got a control task in which we knew the correct label. We named such control tasks \"honeypots\". If an answer to the control task was correct he or she would continue to mark up. During annotation 303 963 recordings were evaluated and 1 715 301 emotion labels were accumulated.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Data Set Overview",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Raw Data Set",
      "text": "Our raw metadata includes at least three labels given by independent annotators per sample and several fields for pure emotional markup without any aggregation. Independent annotators have an independent opinion about emotion labels. In case of disagreement more people were involved to mark one sample.\n\nA list of fields of raw metadata is provided below: wav path -relative path to audio file; annotator id -unique id of annotator; annotator emo -emotion mark given by annotator; annotator emo -emotion mark given by annotator; golden emo -emotion mark of control tasks (honeypots); speaker text -original speaker text to pronounce; speaker emo -intentional emotion of the audio; source idunique id of speaker or podcast;\n\nMetadata stores information about all applicable emotions to each recording, voting results and other specific data. It enables researchers to explore consistency of markup and try various methods to customise markup for data sampling with specific annotation confidence level. In order to get data set for machine learning purposes we have to group labels by audio files and aggregate into single-labels or multi-labels. We call this \"aggregation\" mechanism. For aggregation of raw data we use Dawid-Skene (DS) algorithm  [19]  with confidence threshold to limit the level of agreement. We choose an empirically selected threshold 0.9. Unlike raw corpus, subset we get could be employed for SER model.\n\nThe emotion distribution per domain of aggregated annotation are depicted on Figure  1 (b) and Table  1 . A list of fields of this metadata is provided below: wav path -relative path to audio file; emotion -aggregated emotion mark; speaker text -original text in the audio record; speaker emo -intentional emotion of the audio; source id -unique id of speaker. The number of items and duration in the aggregated training and test subsets are represented in Table  2 .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Baseline Implementation Details",
      "text": "We conduct experiments using the shallow baseline model in order to simplify the entry threshold for researchers who will benchmark using our data set.\n\nWe use common metrics for SER tasks: macro F1 score (F1), Unweighted Accuracy (UA), Weighted Accuracy (WA). These validation metrics are calculated on Crowd and Podcast testing sets, which are created using Dawid-Skene algorithm with confidence > 0.9.\n\nWe train a baseline model from scratch with both Dusha parts (Crowd and Podcast). Additionally, we train our baseline model on IEMOCAP4 to compare it with other state-ofthe-art (SOTA) solutions for speech emotion recognition.\n\nFor our experiments we employ an audio modality only. As input we pass 64 Mel-filterbank calculated from 20ms windows with a 10ms overlap. Next, features are received at a simple MobileNetV2  [20]  based architecture with a selfattention layer described in SAGAN  [21] . Input Mel features are passed through a sequence of inverted residual blocks as it is done in  [20] , but with custom layers configuration. Then we apply a convolutional self-attention layer followed by a global average pooling. After that, we pass the resulting vector (one number for each feature map) through a fully connected layer to get classification results.\n\nThe model is implemented in Pytorch, using the Adam  [22]  optimizer with learning rate 0.001, a weight decay of 10 -6 and without gradient clipping. We train models 100 epochs with batch size 64.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Benchmark Results",
      "text": "The results of our experiments are presented in Table  3 . For all test subsets UA is higher than WA. It could be explained by the neutral emotion dominance. The corpus includes emotion distribution as people faced it. However each researcher or engineer can filter out emotions as he/she wants.\n\nOur baseline model trained on IEMOCAP4 subset of IEMOCAP shows 0.59 ± 0.01 unweighted accuracy UA, 0.59 ± 0.01 weighted accuracy WA, and 0.59 ± 0.01 macro F1 score with 5 sessions cross testing. Actual SOTA result we showed with IEMOCAP were considerably better, but we didn't set the goal to obtain the best metrics. We demonstrated abilities of the utilized architecture for the popular data set.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "In this study, we introduce in details the novel speech data set for emotion recognition called \"Dusha\". The data has been taken from two different sources. The first one is 255 hours of audio with text transcriptions. This is an acted subset obtained and labeled via a crowd-sourcing platform. The second subset is taken from various podcasts and its size is about 90 hours.\n\nThe distinctive feature of Dusha is that we provide a raw emotional data set and an example of an aggregation mechanism. The Dusha's markup can be aggregated into singlelabels or multi-labels. The research community can use our example of a label aggregation or set-up in their own experiments with customized filtering. We open-sourced a code to benchmark models using Dusha and conduct an experiment with baseline model to demonstrate obtained metrics with default emotion distribution.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: (a) represents the time length distribu-",
      "page": 3
    },
    {
      "caption": "Figure 1: (a) orange color). A total of",
      "page": 3
    },
    {
      "caption": "Figure 1: 3.3. Post-processing and annotation",
      "page": 3
    },
    {
      "caption": "Figure 1: (b) and Table 1. A list of",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 1: A list of",
      "data": [
        {
          "Trainingfilesandhours": "150352 188h.44min.\n79825 71h.23min."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 1: A list of",
      "data": [
        {
          "Crowdtest\nUA WA F1": "0.83 0.76 0.77"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "End-to-end speech emotion recognition using deep neural networks",
      "authors": [
        "Panagiotis Tzirakis",
        "Zhang Jiehao",
        "Bjorn Schuller"
      ],
      "year": "2018",
      "venue": "IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "3",
      "title": "An efficient temporal modeling approach for speech emotion recognition by mapping varied duration sentences into fixed number of chunks",
      "authors": [
        "Wei-Cheng Lin",
        "Busso Carlos"
      ],
      "year": "2020",
      "venue": "An efficient temporal modeling approach for speech emotion recognition by mapping varied duration sentences into fixed number of chunks"
    },
    {
      "citation_id": "4",
      "title": "Speech emotion recognition: Two decades in a nutshell, benchmarks, and ongoing trends",
      "authors": [
        "W Björn",
        "Schuller"
      ],
      "year": "2018",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "5",
      "title": "Challenges in real-life emotion annotation and machine learning based detection",
      "authors": [
        "Laurence Devillers",
        "Vidrascu Laurence",
        "Lamel",
        "Lori"
      ],
      "year": "2005",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "6",
      "title": "A cross-corpus study on speech emotion recognition",
      "authors": [
        "Rosanna Milner",
        "Md Jalal",
        "Raymond Ng",
        "Thomas Hain"
      ],
      "year": "2019",
      "venue": "2019 IEEE Automatic Speech Recognition and Understanding Workshop"
    },
    {
      "citation_id": "7",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Busso Carlos",
        "Bulut Murtaza",
        "Lee Chi-Chun",
        "Kazemzadeh Abe",
        "Mower Emily",
        "Kim Samuel",
        "N Jeannette",
        "Lee Sungbok",
        "S Narayanan",
        "Shrikanth"
      ],
      "year": "2008",
      "venue": "Iemocap: Interactive emotional dyadic motion capture database"
    },
    {
      "citation_id": "8",
      "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "Zadeh Amir",
        "Liang Paul",
        "Vanbriesen Pu",
        "Poria Jonathan",
        "Tong Soujanya",
        "Cambria Edmund",
        "Chen Erik",
        "Louis-Philippe Minghai",
        "Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)"
    },
    {
      "citation_id": "9",
      "title": "Mosi: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos",
      "authors": [
        "Amir Zadeh",
        "Rowan Zellers",
        "Eli Pincus",
        "Louis-Philippe Morency"
      ],
      "year": "2016",
      "venue": "Mosi: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos"
    },
    {
      "citation_id": "10",
      "title": "The smartkom multimodal corpus at bas",
      "authors": [
        "Florian Schiel",
        "Silke Steininger",
        "Ulrich Türk"
      ],
      "year": "2002",
      "venue": "LREC. Citeseer"
    },
    {
      "citation_id": "11",
      "title": "Audiovisual behavior modeling by combined feature spaces",
      "authors": [
        "Bjorn Schuller",
        "Arsic Dejan",
        "Rigoll Gerhard",
        "Wimmer Matthias",
        "Radig Bernd"
      ],
      "year": "2007",
      "venue": "proceedings of the,\" International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "12",
      "title": "A database of german emotional speech",
      "authors": [
        "Felix Burkhardt",
        "Paeschke Astrid",
        "F Rolfes Miriam",
        "Sendlmeier",
        "Walter",
        "Weiss",
        "Benjamin"
      ],
      "year": "2005",
      "venue": "A database of german emotional speech"
    },
    {
      "citation_id": "13",
      "title": "Emonet: A transfer learning framework for multi-corpus speech emotion recognition",
      "authors": [
        "Maurice Gerczuk",
        "Amiriparian Shahin",
        "Ottl Sandra",
        "Bjorn Schuller"
      ],
      "venue": "IEEE Transactions on Affective Computing)"
    },
    {
      "citation_id": "14",
      "title": "Ruslana: A database of russian emotional utterances",
      "authors": [
        "Veronika Makarova",
        "Valery Petrushin"
      ],
      "year": "2002",
      "venue": "Ruslana: A database of russian emotional utterances"
    },
    {
      "citation_id": "15",
      "title": "Ramas: Russian multimodal corpus of dyadic interaction for affective computing",
      "authors": [
        "Olga Perepelkina",
        "Evdokia Kazimirova",
        "Maria Konstantinova"
      ],
      "year": "2018",
      "venue": "International Conference on Speech and Computer"
    },
    {
      "citation_id": "16",
      "title": "Multimodal russian corpus and its use in emotional studies",
      "authors": [
        "Svetlana Savchuk",
        "Alexandra Makhova"
      ],
      "year": "2021",
      "venue": "Russian Journal of Communication"
    },
    {
      "citation_id": "17",
      "title": "Mls: A large-scale multilingual dataset for speech research",
      "authors": [
        "Qiantong Vineel Pratap",
        "Anuroop Xu",
        "Gabriel Sriram",
        "Ronan Synnaeve",
        "Collobert"
      ],
      "year": "2020",
      "venue": "Mls: A large-scale multilingual dataset for speech research",
      "arxiv": "arXiv:2012.03411"
    },
    {
      "citation_id": "18",
      "title": "Golos: Russian dataset for speech research",
      "authors": [
        "Nikolay Karpov",
        "Alexander Denisenko",
        "Fedor Minkin"
      ],
      "year": "2021",
      "venue": "Golos: Russian dataset for speech research"
    },
    {
      "citation_id": "19",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2018",
      "venue": "Bert: Pre-training of deep bidirectional transformers for language understanding"
    },
    {
      "citation_id": "20",
      "title": "Maximum likelihood estimation of observer error-rates using the em algorithm",
      "authors": [
        "Alexander Philip",
        "Allan Skene"
      ],
      "year": "1979",
      "venue": "Journal of the Royal Statistical Society: Series C (Applied Statistics)"
    },
    {
      "citation_id": "21",
      "title": "Inverted residuals and linear bottlenecks: Mobile networks for classification, detection and segmentation",
      "authors": [
        "Mark Sandler",
        "Andrew Howard",
        "Menglong Zhu",
        "Andrey Zhmoginov",
        "Liang-Chieh Chen"
      ],
      "year": "2018",
      "venue": "Inverted residuals and linear bottlenecks: Mobile networks for classification, detection and segmentation"
    },
    {
      "citation_id": "22",
      "title": "Self-attention generative adversarial networks",
      "authors": [
        "Han Zhang",
        "Ian Goodfellow",
        "Dimitris Metaxas",
        "Augustus Odena"
      ],
      "year": "2019",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "23",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2015",
      "venue": "3rd International Conference on Learning Representations, ICLR 2015"
    }
  ]
}