{
  "paper_id": "2004.13236v1",
  "title": "Deep Auto-Encoders With Sequential Learning For Multimodal Dimensional Emotion Recognition",
  "published": "2020-04-28T01:25:00Z",
  "authors": [
    "Dung Nguyen",
    "Duc Thanh Nguyen",
    "Rui Zeng",
    "Thanh Thi Nguyen",
    "Son N. Tran",
    "Thin Nguyen",
    "Sridha Sridharan",
    "Clinton Fookes"
  ],
  "keywords": [
    "Multimodal emotion recognition",
    "dimensional emotion recognition",
    "auto-encoder",
    "long short term memory"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal dimensional emotion recognition has drawn a great attention from the affective computing community and numerous schemes have been extensively investigated, making a significant progress in this area. However, several questions still remain unanswered for most of existing approaches including: (i) how to simultaneously learn compact yet representative features from multimodal data, (ii) how to effectively capture complementary features from multimodal streams, and (iii) how to perform all the tasks in an end-to-end manner. To address these challenges, in this paper, we propose a novel deep neural network architecture consisting of a two-stream auto-encoder and a long short term memory for effectively integrating visual and audio signal streams for emotion recognition. To validate the robustness of our proposed architecture, we carry out extensive experiments on the multimodal emotion in the wild dataset: RECOLA. Experimental results show that the proposed method achieves state-of-the-art recognition performance and surpasses existing schemes by a significant margin.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "E MOTION recognition has become a core research field at the intersection of human communication and artificial intelligence. This research problem is challenging owing to emotions of human beings can be expressed in different forms such as visual, acoustic, and linguistic structures  [1] .\n\nAs shown in the literature, there are two main conceptualisations of emotions: categorical and dimensional conceptualisation. Categorical approach defines a small set of basic emotions (e.g., happiness, sadness, anger, surprise, fear, and disgust) relying on cross-culture studies that show humans perceive certain basic emotions in similar ways regardless of their culture  [2] . Alternatively, dimensional approach represents emotions into a multidimensional space where each dimension captures a fundamental property of the emotions (e.g., appraising human emotional states, behaviours and reactions displayed in real-world settings). These fundamental properties can be accomplished using continuous dimensions in the \"Circumplex Model of Affects\" (CMA)  [3]  including valence (i.e., how positive or negative an emotion is) and Fig.  1 . Two dimensional valence and arousal space (from https://www. pinterest.com.au/pin/354588170647498721/) arousal (i.e., the power of the activation of an emotion). Fig.  1  illustrates the CMA. However, this approach is more appropriate to represent subtle changes in emotions, which may not always happen in real-world conditions  [2] .\n\nRecently, deep neural networks have been proposed to effectively predict the continuous dimensions of emotions based on multimodal cues such as auditory and visual information  [2] -  [8] . These works combine convolutional and recurrent neural networks for feature integration, taking advantages of automatic feature learning in convolutional networks, while encoding temporal dynamics via the sequential layers in recurrent networks  [3] . However, feature integration is performed simply by concatenating domain-dependent features extracted from individual modalities. This scheme is straightforward and simple to implement yet may not be able to effectively learn compact and representative multimodal features. To address this issue, we propose a novel deep neural network for multimodal dimensional emotion recognition using a two-stream auto-encoder incorporating with a long short term memory to perform joint learning temporal and compact-representative features in multimodal data. Our architecture can be end-toend trainable and capable of learning multimodal representations, achieving state-of-the-art performance on benchmark dataset. Specifically, we make the following contributions,",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "Emotion recognition has been a well-studied research problem for several decades and numerous approaches have been proposed. In this section, we limit our review to recent multimodal dimensional emotion recognition methods using deep learning techniques such as convolutional neural networks and long short term memory due to their proven robustness and effectiveness in many applications.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Multimodal Emotion Recognition",
      "text": "Inspired by the capability of automatic feature learning in deep learning frameworks, Zhang et al.  [9]  proposed a hybrid deep learning system constructed by a convolutional neural network (CNN), a three-dimensional CNN (3DCNN), and a deep belief network (DBN) to learn audio-visual features for emotion recognition. In this work, the CNN was pre-trained on the large-scale ImageNet database  [10]  and used to learn audio features from speech signals. To capture the temporal information from video data, the 3DCNN model in  [11]  was adapted and fine-tuned on contiguous video frames. The learnt audio and visual features were subsequently fused into the DBN to generate audio-visual features that were finally fed to a linear SVM for emotion recognition.\n\nTo classify spontaneous multimodal emotional expressions, Barros et al.  [12]  proposed a so-called cross channel convolutional neural network (CC-CNN) to learn generic and specific features of emotions based on body movements and facial expressions. These features were further passed into crossconvolution channels to build cross-modal representations. Motivated by human perception in emotional expression, Barros and Wermter  [13]  developed a perception representation model capturing the correlation between different modalities. In this work, auditory and visual stimuli were firstly fused using the CC-CNN originally introduced in  [12]  to achieve a multimodal perception representation. A self-organising layer was then applied on top of the CC-CNN to further separate the perceived expression representations.\n\nRecently, Zheng et al.  [14]  proposed a multimodal framework including two Restricted Boltzmann Machines (RBMs) to capture eye movements and EEG signal. The RBMs were unfolded into a bimodal deep auto-encoder (BDAE)  [15]  to extract shared representations of the two modalities, which were finally fed to a linear SVM for emotion classification.\n\nConventionally, emotion recognition approaches classify person-independent emotions directly from observed data or determine the decreasing/increasing intensity of persondependent emotions relatively by comparing video segments. However, Liang et al.  [1]  proposed to combine both approaches for emotion recognition from audio-visual data. In this work, the emotion recognition task was divided into three subtasks including local relative emotion intensities ranking, global relative emotion intensities ranking, and incorporation of emotion predictions from observed multimodal expressions and relative emotion ranks from local-global rankings for emotion recognition.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Sequential Learning",
      "text": "When temporal information is available, sequential learning can be applied to improve the accuracy of emotion recognition. Long Short Term Memory (LSTM) is often used for sequential learning due to its capability of modelling human memory  [16] -  [18] .\n\nTechnically, LSTMs are recurrent neural networks (RNNs) integrated with some special gating architectures to control the access to memory cells  [17] . These gates can also be used to prevent modifying the contents in the cells. LSTMs are, therefore, able to encode much longer range of patterns and propagate errors better than original recurrent neural networks  [19] . Apart from being able to control the access to the contents in memory cells, the gates can also learn to target on specific parts of input sequences and refuse other parts. This feature allows LSTMs to be able to capture temporal information in sequential patterns.\n\nInspired by the advantages of LSTMs, many LSTM-based techniques have been developed for human emotion understanding from streaming data. For instance, Chen and Jin  [20]  and Wöollmer et al.  [21]  proposed LSTM-based networks for emotion recognition from data streams. Pei et al.  [22]  introduced a deep bidirectional LSTM-RNN, which was capable of modelling nonlinear relations in long-term history to handle both multimodal and unimodal emotion recognition tasks. In  [3] ,  [23] , the authors developed an affective recognition system consisting of several deep neural networks to learn features at discrete times and an LSTM to model the temporal evolution of features overtime.\n\nDespite recent promising achievements, current developments lack of ability to learn compact and representative features on individual domains and effectively learn multimodal features from multimodal data streams. To overcome these limitations, we propose to learn compact and representative features from individual domains using auto-encoders and fuse those domain-dependent features into multimodal features, integrated with temporal information using LSTM.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iii. Proposed Method",
      "text": "In this section, we present an end-to-end system for recognition of dimensional emotion from multimodal data including visual and audio data streams.\n\nA. Data Pre-processing Input of our system is a pair of video and audio streams.\n\nFor the video stream, we apply the Single Stage Headless (SSH) detector proposed in  [24]  to detect human faces in video frames. After that we resize the cropped faces to 96 × 96. The colour intensities in the cropped images are then normalised to [-1, 1].\n\nFor the audio stream, we segment the raw waveform signals of the stream into 0.2s long sequences after normalising the time-sequences in order to obtain zero mean and unit variance. The normalisation aims at taking into account the variation in the loudness among different speakers. For a given input stream sampled at 16 kHz, each 0.2s long sequence consists of 5 audio frames, each of which takes 0.04s and is represented by a 640-dimensional vector. Note that each audio frame corresponds to a video frame in the input video stream.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Network Architecture",
      "text": "The proposed network architecture is illustrated in Fig.  2 . Our architecture consists of two network branches, a 2D convolutional auto-encoder (2DConv-AE) and a 1D convolutional auto-encoder (1DConv-AE), and a long short term memory (LSTM). Each network branch takes its corresponding input, e.g., the 2DConv-AE receives input as an image while the 1DConv-AE receives input as a 0.04s audio frame. The latent layers of these branches are fused into a multimodal representation, which is then fed to the LSTM for prediction of two dimensional emotion scores: arousal and valence.\n\nGiven an input video stream (of a speaker) including an image sequence and audio sequence, each image frame in the sequence is passed into the SSH detector  [24]  to detect the speaker's face. The face image is then fed to the 2DConv-AE to learn visual features. Simultaneously, the corresponding audio frame is passed to the 1DConv-AE to learn audio features. The features extracted from the latent layers of these autoencoders are compact yet representative for their individual domains. Those features are then combined via a fusion layer before being fed to the LSTM for sequential learning of the features (for every 0.2s long sequence) from input streams. The reason for the LSTM is to model the temporal variation of the audio and visual features that provides the contextual information of the input data.\n\nThe combination of auto-encoders and LSTM ensures that the learnt representations are compact, rich and complementary and thus make the architecture optimal and robust towards the recognition task from multimodal data streams.\n\n1) 2DConv-AE: follows the common practice of 2D autoencoders, e.g.,  [25] . The encoder of the 2DCov-AE is composed of two residual blocks as in the ResNet architecture  [26] , then enclosed by a fully-connected layer. These residual blocks play a central role in feature enrichment. The decoder of the 2DCov-AE includes two residual blocks, which are designed by stacking six 2D de-convolutional layers. Leaky ReLU (LReLU) activation function is adopted after each convolu-\n\ntional layer and de-convolutional layer in our design. Table  I  provides a detailed description of the 2DConv-AE branch while Fig.  3  visualises its architecture.\n\n2) 1DConv-AE: realises an 1D auto-encoder applied to audio signal sequences. The encoder of the 1DConv-AE is an 1D convolutional neural network. We adopt the network architecture proposed by Tzirakis et al.  [3]  in our design for the encoder of the 1DConv-AE. Specifically, the 1DConv-AE's encoder includes two 1D convolutional layers, each of which is followed by a max pooling layer. Two fully-connected layers are subsequently attached. The decoder of the 1DConv-AE is formed by stacking one 1D de-convolutional layer, followed by one upsampling layer and then another 1D de-convolutional layer. We summarise the architecture of the 1DConv-AE in Table  II  and Fig.  4 .\n\n3) LSTM: has demonstrated a powerful capability of learning long-range contextual information in sequential patterns  [27] . Motivated by this power, we adopt a 2-layer LSTM with 512 cells for each layer to model the temporal and contextual information in multimodal features learnt by the 2DConv-AE and 1DConv-AE. Readers are referred to  [28]  for more detail on LSTM implementation.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "C. Joint Learning",
      "text": "Our goal is to learn compact and representative features commonly shared in both visual and auditory domain for prediction of dimensional emotion scores.\n\nFor the ease of presentation, we first introduce important notations used in our method. We denote the encoder and decoder of the 2DConv-AE as φ 2D and ϕ 2D respectively. Similarly, the encoder and decoder of the 1DConv-AE are denoted as φ 1D and ϕ 1D respectively. Let F denote the fusion layer which concatenates the features learnt by φ 2D and φ 1D . The LSTM is represented by G, receiving input from the fusion layer F . Let I t denote a facial image obtained by applying the SSH face detector on an input image frame at time step t. Let S t denote the corresponding sound segment of the facial image I t . Given a pair of input (I t , S t ), let a t and v t be the ground truth arousal and valence respectively, and ât and vt be the predicted arousal and valence of the input (I t , S t ). The training procedure can be described as follows.\n\nGiven the input pair (I t , S t ), to learn compact and representative visual and audio features in individual domains, the autoencoders 2DConv-AE and 1DConv-AE make use of the 2D and 1D encoders φ 2D and φ 1D , and the 2D and 1D decoders ϕ 2D and ϕ 1D , and result in an output pair of reconstructed image frame Ît and speech frame Ŝt where,\n\nThe quality of an auto-encoder can be measured via the similarity between an original signal and its reconstructed version after being processed through the auto-encoder. Autoencoders, thus, can ensure the representative quality of their encoded representations. In this work, we define the losses of our auto-encoders using 2 -norm as,\n\nwhere Ît and Ŝt is defined in Eq. (  1 ) and Eq. (  2 ) respectively, n is the number of samples (i.e., image/speech frames) in a current training batch.\n\nTo learn multimodal features, the fusion layer F is used to fuse features extracted from the latent layers of the autoencoders. Specifically, the multimodal feature vector for the input pair (I t , S t ) is denoted as m t and defined as,\n\nwhere ⊕ represents a concatenating operator. The multimodal feature vector m t is then fed to the LSTM G which estimates the arousal value ât and valence value vt of m t based on its precedent observations, i.e.,\n\nwhere k is the number of precedent observations used to determine the arousal and valence value of the observation at time step t.\n\nTo measure the quality of arousal and valence prediction, we adopt the Concordance Correlation Coefficient (CCC) proposed in  [29] . CCC has been widely used in measuring the performance of dimensional emotion recognition systems  [2] . It validates the agreement between two time series (e.g., predictions and their corresponding ground truth annotations) by scaling their correlation coefficient with their mean square difference. In this way, predictions that are well correlated with the annotations but shifted in the value are penalised proportionally to their deviations. CCC takes values in the range [-1, 1], where +1 denotes perfect concordance and -1 indicates perfect discordance. The higher the value of the CCC demonstrates the better the fit between predictions and ground truth annotations, and therefore high values are desired. Applying the CCC, we define the loss for emotion recognition L Rec as follows,\n\nwhere ρ a and ρ v is the CCC of the arousal and valence respectively, calculated on a current training batch. In particular, we define,\n\nwhere, for instance, σ â,a is the covariance of the predictions â and ground truth annotations of arousal a, σ 2 â and σ 2 a is the variance of â and a respectively, and µ â and µ a is the mean of â and a respectively. Note that those statistics are calculated on a current training batch. Similar interpretations can be applied to ρ v .\n\nFinally, the loss of entire network is defined as,\n\nwhere L 2D , L 1D , and L Rec is presented in Eq. (  3 ), Eq. (  4 ), and Eq. (  7 ), α, β, and γ are weights used to control the influence of sub networks and set empirically.\n\nAs shown in Eq. (  10 ), the individual auto-encoders and the LSTM are jointly trained. This scheme makes the features learnt through the entire network compact, representative, and complementary from different domains.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Iv. Experiments",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Dataset",
      "text": "We conducted our experiments on the REmote COLlaborative and Affective (RECOLA) dataset introduced by Ringeval et al.  [30] . This dataset consists of spontaneous and natural emotions represented by continuous values of arousal and valence. The dataset has four modalities including electro-cardiogram, electro-dermal activity, audio, and visual modality. There are 46 French speaking subjects involved in recordings of 9.5 hours in total. The recordings are labelled for every 5 minutes by three male and three female French-speaking annotators. The dataset is balanced among various factors including mother tongue, age, and gender. The dataset includes a training set with 16 subjects and a validation set with 15 subjects. Each subject is associated with a recording (including visual and audio signal). Each recording consists of 7,500 frames for each visual and audio channel. Fig.  5  illustrates an example in the RECOLA dataset.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Implementation Details",
      "text": "Our emotion recognition system receives input as a multimodal data stream including an image channel and a speech channel. The data stream of each channel is segmented into frames which are then passed to the proposed network architecture for processing.\n\nGiven a pair (I t , S t ) including an image frame I t and its corresponding speech frame S t at some time step t, the image frame I t is passed to the visual network branch (2DConv-AE) to extract 2,048 visual features via φ 2D (I t ). Similarly, the speech frame S t is forwarded to the speech network branch (1DConv-AE) to extract 1,280 auditory features via φ 1D (S t ). These output features are concatenated to form a 3,328 dimensional multimodal representation m t as defined in Eq. (  5 ). This representation is fed to the 2-layer LSTM (with 512 cells per layer) to extract long-range contextual information from the data stream. The output of the LSTM is finally attached to a fully-connected layer to predict the arousal and valence for the input data at time step t.\n\nTo predict the arousal and valence for the input pair (I t , S t ), the LSTM takes into account the last four time steps of t, i.e., k in Eq. (  6 ) is set to 4.\n\nOur proposed architecture was trained end-to-end via optimising the joint loss L defined in Eq. (  10 ) on the training set of the RECOLA database. In our implementation, we set the parameters in Eq. (  10 ) as, α = 1, β = 1, and γ = 0.01. Adam optimiser (with default values) was adopted. Mini-batch size was set to 32 and learning rate was set to 0.0001. The number of training steps was set to 50,000. All experiments in this paper were implemented in TensorFlow and conducted on 10 computing nodes: 3780 × 64-bit Intel Xeon Cores. Our proposed model was trained within 45 hours and required 10,485,760KB of memory usage. Fig.  6  shows the learning curve of our model.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "C. Evaluation Protocol",
      "text": "As commonly used in evaluation of dimensional emotion recognition  [2] , we measure the Root Mean Square Error (RMSE) of predicted dimensional emotion scores against the ground truth values as follow,\n\nwhere â and a is the predicted arousal and its ground truth value respectively, and v and v is the predicted valence and its ground truth value respectively, N is the total number of frames in an input sequence.\n\nTo further investigate the prediction performance in detail, we also calculate the RMSE on each individual emotion dimension as,\n\nThe RMSE gives us a rough indication of how the derived emotion model is behaving, providing a simple comparative evaluation metric. Small values of RMSE are desired.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "D. Evaluations And Comparisons",
      "text": "Since there are several technical contributions proposed in this work, e.g., auto-encoders for learning multimodal features, LSTM for sequential learning of multimodal data streams, etc., we evaluate each of these contributions in our experiments. In addition, for each dimension in the evaluations, we also compare our method with existing related works.\n\n1) Multimodal vs Unimodal: We first evaluate emotion recognition using multimodal and unimodal approach. Our proposed multimodal architecture combines two unimodal branches, each of which focuses on a single domain (visual or audio domain). To make a comprehensive comparison between muiltimodal and unimodal approaches, we respectively disabled one of the two branches in our architecture to achieve unimodal architectures. For instance, the unimodal architecture for the visual part is denoted as \"2DConv-AE-LSTM\" and obtained by taking off the speech network branch (see Fig.  2 ) while keeping the LSTM which receives input from only the latent layer of the 2DConv-AE. Similar action was applied to the audio part to create \"1DConv-AE-LSTM.\"\n\nTable  III  compares unimodal and multimodal approaches. As shown in the table, there is an inconsistency in the performance of the unimodal architectures. Specifically, the audio architecture (1DConv-AE-LSTM) outperforms the visual one (2DConv-AE-LSTM) in prediction of arousal while the visual architecture shows better performance than its counterpart in prediction of valence. Compared with these sub-models, our multimodal architecture (\"2D1DConv-AE-LSTM\") combining both the unimodal architectures shows superior performance on prediction of both emotion dimensions.  We also compared our multimodal architecture with other two unimodal architectures proposed in  [3] . Specifically, the authors in  [3]  investigated the combination of CNNs and LSTM on visual and audio domain separately. As shown in Table  III , our multimodal architecture outperforms all other unimodal ones on both arousal and valence prediction.\n\n2) Feature Learning with Auto-Encoders: We observed that the auto-encoders (2DConv-AE and 1DConvAE) also boosted up the prediction performance of our architecture. In particular, we compared our architecture with the one proposed in  [3] , where only CNNs were used to learn the multimodal features. Note that the same LSTM was used in both the architectures.\n\nWe report the prediction performance of our model and the one in  [3]  in Table  IV . As shown in our experimental results, compared with the network architecture proposed in  [3] , our model is slightly inferior in prediction of valence (see E v ) but significantly more prominent in prediction of arousal (see E a ), leading to a better overall performance.\n\nTo further investigate the impact of the auto-encoders on individual domains, we re-designed the unimodal branches by disabling the decoders in those branches and measured their performances. We observed that the 1DConv-AE improved the accuracy of the audio branch on both arousal and valence prediction. Specifically, the improvement was 4.6% on E a (from 0.517 to 0.493) and 5.6% on E v (from 0.251 to 0.237), leading to an overall improvement of 4.7% on E av (from 0.574 to 0.547). In contrast, the 2DConv-AE incurred a decrease of 11.5% on E a (from 0.476 to 0.538) and 10.3% on E v (from 0.192 to 0.214), and 11.2% on E av (from 0.514 to 0.579). However, as shown in our experimental results, the incarnation of both the 2DConv-AE and 1DConv-AE compensated the weakness of each individual component and achieved improved overall performance.\n\n3) Sequential Learning with LSTM: We propose the use of LSTM for learning long-range contextual and temporal information from streaming data. LSTM has also been used widely in dimensional emotion recognition from data streams. For instance, in  [23] , Kollias and Zafeiriou proposed to use two LSTMs, each of which for one unimodal data stream (e.g., visual or audio stream). We denote this approach as \"2D1D-2SLSTM.\" Unlike  [23] , in our architecture, we use only one LSTM receiving input from a fusion layer and producing predicted values for arousal and valence. Note that auto-encoders were not employed in  [23] . Therefore, to better study the effect of using one LSTM vs two stream LSTM, we applied the two stream LSTM in  [23]  to our architecture, to create a so-called \"2D1DConv-AE-2SLSTM\" variant. Table V compares different approaches using LSTM for sequential learning in dimensional emotion recognition. As shown in the table, our architecture achieves the best performance among all models on both emotion dimensions.\n\n4) Ablation Study: In this experiment, we study the effect of hidden nodes in the LSTM. Specifically, we investigate the prediction performance of our architecture with regard to various numbers of hidden nodes in each layer in the LSTM.\n\nWe report these results in Table  VI . In general, there is a fluctuation in the prediction performance when varying the number of hidden nodes in the LSTM. Although the best configuration for prediction of valence is the LSTM with 256 nodes in each hidden layer, our current setting with 512 nodes in each hidden layer shows better performance in prediction of arousal and also achieves the best overall performance.\n\nV. CONCLUSION This paper proposes a deep network architecture for end-toend dimensional emotion recognition from multimodal data streams. The proposed architecture incarnates auto-encoders for learning multimodal features from visual and audio domains, and LSTM for learning contextual and temporal information from streaming data. Our proposed architecture enables learning compact, representative, and complementary features from multimodal data source. We implemented various baseline models and conducted extensive experiments on the benchmark RECOLA dataset. Experimental results confirmed the contributions of our work and the superiority of our proposed architecture over the state-of-the-art.",
      "page_start": 7,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Two dimensional valence and arousal space (from https://www.",
      "page": 1
    },
    {
      "caption": "Figure 1: illustrates the CMA. However, this approach is more",
      "page": 1
    },
    {
      "caption": "Figure 2: Our proposed network architecture for multimodal dimensional emotion recognition.",
      "page": 3
    },
    {
      "caption": "Figure 3: Architecture of the 2DConv-AE.",
      "page": 4
    },
    {
      "caption": "Figure 3: visualises its architecture.",
      "page": 4
    },
    {
      "caption": "Figure 4: Architecture of the 1DConv-AE.",
      "page": 4
    },
    {
      "caption": "Figure 5: Arousal and valance annotations over a part of a video in the RECOLA dataset. Corresponding frames are also illustrated. This ﬁgure shows the",
      "page": 6
    },
    {
      "caption": "Figure 5: illustrates an example",
      "page": 6
    },
    {
      "caption": "Figure 6: Learning curve of our model.",
      "page": 6
    },
    {
      "caption": "Figure 6: shows the learning",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Metric\nArchitecture": "2DConv-AE-LSTM",
          "Ea": "0.538",
          "Ev": "0.214",
          "Eav": "0.579"
        },
        {
          "Metric\nArchitecture": "1DConv-AE-LSTM",
          "Ea": "0.493",
          "Ev": "0.237",
          "Eav": "0.547"
        },
        {
          "Metric\nArchitecture": "2D model\nin [3]",
          "Ea": "0.476",
          "Ev": "0.192",
          "Eav": "0.514"
        },
        {
          "Metric\nArchitecture": "1D model\nin [3]",
          "Ea": "0.517",
          "Ev": "0.251",
          "Eav": "0.574"
        },
        {
          "Metric\nArchitecture": "Our 2D1DConv-AE-LSTM",
          "Ea": "0.474",
          "Ev": "0.187",
          "Eav": "0.510"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Metric\nArchitecture": "2D1DConv-2SLSTM [23]",
          "Ea": "0.493",
          "Ev": "0.187",
          "Eav": "0.527"
        },
        {
          "Metric\nArchitecture": "2D1DConv-AE-2SLSTM",
          "Ea": "0.508",
          "Ev": "0.190",
          "Eav": "0.542"
        },
        {
          "Metric\nArchitecture": "Our 2D1DConv-AE-LSTM",
          "Ea": "0.474",
          "Ev": "0.187",
          "Eav": "0.510"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Metric\n#Hidden nodes": "32",
          "Ea": "0.509",
          "Ev": "0.190",
          "Eav": "0.543"
        },
        {
          "Metric\n#Hidden nodes": "64",
          "Ea": "0.540",
          "Ev": "0.195",
          "Eav": "0.574"
        },
        {
          "Metric\n#Hidden nodes": "128",
          "Ea": "0.502",
          "Ev": "0.205",
          "Eav": "0.542"
        },
        {
          "Metric\n#Hidden nodes": "256",
          "Ea": "0.496",
          "Ev": "0.184",
          "Eav": "0.529"
        },
        {
          "Metric\n#Hidden nodes": "512 (our architecture)",
          "Ea": "0.474",
          "Ev": "0.187",
          "Eav": "0.510"
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Multimodal local-global ranking fusion for emotion recognition",
      "authors": [
        "P Liang",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "2",
      "title": "Deep affect prediction in-thewild: Aff-wild database and challenge, deep architectures, and beyond",
      "authors": [
        "D Kollias",
        "P Tzirakis",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "B Schuller",
        "I Kotsia",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "3",
      "title": "End-to-end multimodal emotion recognition using deep neural networks",
      "authors": [
        "P Tzirakis",
        "G Trigeorgis",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2017",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "4",
      "title": "Aff-wild: Valence and arousal in-the-wildchallenge",
      "authors": [
        "S Zafeiriou",
        "D Kollias",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "I Kotsia"
      ],
      "year": "2017",
      "venue": "IEEE International Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "5",
      "title": "Recognition of affect in the wild using deep neural networks",
      "authors": [
        "D Kollias",
        "M Nicolaou",
        "I Kotsia",
        "G Zhao",
        "S Zafeiriou"
      ],
      "year": "2017",
      "venue": "IEEE International Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "6",
      "title": "Deep spatio-temporal features for multimodal emotion recognition",
      "authors": [
        "D Nguyen",
        "K Nguyen",
        "S Sridharan",
        "A Ghasemi",
        "D Dean",
        "C Fookes"
      ],
      "year": "2017",
      "venue": "2017 IEEE Winter Conference on Applications of Computer Vision (WACV)"
    },
    {
      "citation_id": "7",
      "title": "Aff-wild2: Extending the aff-wild database for affect recognition",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2018",
      "venue": "Aff-wild2: Extending the aff-wild database for affect recognition",
      "arxiv": "arXiv:1811.07770"
    },
    {
      "citation_id": "8",
      "title": "A multi-task learning & generation framework: Valence-arousal, action units & primary expressions",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2018",
      "venue": "A multi-task learning & generation framework: Valence-arousal, action units & primary expressions",
      "arxiv": "arXiv:1811.07771"
    },
    {
      "citation_id": "9",
      "title": "Learning affective features with a hybrid deep model for audio-visual emotion recognition",
      "authors": [
        "S Zhang",
        "S Zhang",
        "T Huang",
        "W Gao",
        "Q Tian"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "10",
      "title": "ImageNet Large Scale Visual Recognition Challenge",
      "authors": [
        "O Russakovsky",
        "J Deng",
        "H Su",
        "J Krause",
        "S Satheesh",
        "S Ma",
        "Z Huang",
        "A Karpathy",
        "A Khosla",
        "M Bernstein",
        "A Berg",
        "L Fei-Fei"
      ],
      "year": "2015",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "11",
      "title": "Learning spatiotemporal features with 3d convolutional networks",
      "authors": [
        "D Tran",
        "L Bourdev",
        "R Fergus",
        "L Torresani",
        "M Paluri"
      ],
      "year": "2015",
      "venue": "IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "12",
      "title": "Emotional expression recognition with a cross-channel convolutional neural network for human-robot interaction",
      "authors": [
        "P Barros",
        "C Weber",
        "S Wermter"
      ],
      "year": "2015",
      "venue": "IEEE-RAS International Conference on Humanoid Robots"
    },
    {
      "citation_id": "13",
      "title": "Developing crossmodal expression recognition based on a deep neural model",
      "authors": [
        "P Barros",
        "S Wermter"
      ],
      "year": "2016",
      "venue": "Adaptive Behavior -Animals, Animats, Software Agents, Robots, Adaptive Systems"
    },
    {
      "citation_id": "14",
      "title": "Emotionmeter: A multimodal framework for recognizing human emotions",
      "authors": [
        "W Zheng",
        "W Liu",
        "Y Lu",
        "B Lu",
        "A Cichocki"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Cybernetics"
    },
    {
      "citation_id": "15",
      "title": "Multimodal deep learning",
      "authors": [
        "J Ngiam",
        "A Khosla",
        "M Kim",
        "J Nam",
        "H Lee",
        "A Ng"
      ],
      "year": "2011",
      "venue": "International Conference on International Conference on Machine Learning"
    },
    {
      "citation_id": "16",
      "title": "Grid long short-term memory",
      "authors": [
        "N Kalchbrenner",
        "I Danihelka",
        "A Graves"
      ],
      "year": "2015",
      "venue": "CoRR"
    },
    {
      "citation_id": "17",
      "title": "Long short-term memory",
      "authors": [
        "S Hochreiter",
        "J Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural Computation"
    },
    {
      "citation_id": "18",
      "title": "Deep discovery of facial motions using a shallow embedding layer",
      "authors": [
        "A Bahmani",
        "M Baktashmotlagh",
        "S Denman",
        "S Sridharan",
        "D Tien",
        "C Fookes"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 IEEE International Conference on Image Processing"
    },
    {
      "citation_id": "19",
      "title": "Deep spatio-temporal feature fusion with compact bilinear pooling for multimodal emotion recognition",
      "authors": [
        "D Nguyen",
        "K Nguyen",
        "S Sridharan",
        "D Dean",
        "C Fookes"
      ],
      "year": "2018",
      "venue": "Computer Vision and Image Understanding"
    },
    {
      "citation_id": "20",
      "title": "Multi-modal dimensional emotion recognition using recurrent neural networks",
      "authors": [
        "S Chen",
        "Q Jin"
      ],
      "year": "2015",
      "venue": "International Workshop on Audio/Visual Emotion Challenge"
    },
    {
      "citation_id": "21",
      "title": "LSTM-Modeling of continuous emotions in an audiovisual affect recognition framework",
      "authors": [
        "M Wöllmer",
        "M Kaiser",
        "F Eyben",
        "B Schuller",
        "G Rigoll"
      ],
      "year": "2013",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "22",
      "title": "Multimodal dimensional affect recognition using deep bidirectional long short-term memory recurrent neural networks",
      "authors": [
        "E Pei",
        "L Yang",
        "D Jiang",
        "H Sahli"
      ],
      "year": "2015",
      "venue": "IEEE International Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "23",
      "title": "Exploiting multi-cnn features in cnn-rnn based dimensional emotion recognition on the omg in-the-wild dataset",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "Exploiting multi-cnn features in cnn-rnn based dimensional emotion recognition on the omg in-the-wild dataset",
      "arxiv": "arXiv:1910.01417"
    },
    {
      "citation_id": "24",
      "title": "SSH: Single stage headless face detector",
      "authors": [
        "M Najibi",
        "P Samangouei",
        "R Chellappa",
        "L Davis"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "25",
      "title": "Image restoration using very deep convolutional encoder-decoder networks with symmetric skip connections",
      "authors": [
        "X Mao",
        "C Shen",
        "Y.-B Yang"
      ],
      "year": "2016",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "26",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "IEEE International Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "27",
      "title": "Facing realism in spontaneous emotion recognition from speech: Feature enhancement by autoencoder with lstm neural networks",
      "authors": [
        "Z Zhang",
        "F Ringeval",
        "J Han",
        "J Deng",
        "E Marchi",
        "B Schuller"
      ],
      "year": "2016",
      "venue": "Facing realism in spontaneous emotion recognition from speech: Feature enhancement by autoencoder with lstm neural networks"
    },
    {
      "citation_id": "28",
      "title": "Supervised Sequence Labelling with Recurrent Neural Networks, ser. Studies in Computational Intelligence",
      "authors": [
        "A Graves"
      ],
      "year": "2012",
      "venue": "Supervised Sequence Labelling with Recurrent Neural Networks, ser. Studies in Computational Intelligence"
    },
    {
      "citation_id": "29",
      "title": "A concordance correlation coefficient to evaluate reproducibility",
      "authors": [
        "-K Lin"
      ],
      "year": "1989",
      "venue": "Biometrics"
    },
    {
      "citation_id": "30",
      "title": "Introducing the recola multimodal corpus of remote collaborative and affective interactions",
      "authors": [
        "F Ringeval",
        "A Sonderegger",
        "J Sauer",
        "D Lalanne"
      ],
      "year": "2013",
      "venue": "IEEE International Conference on Automatic Face and Gesture Recognition"
    }
  ]
}