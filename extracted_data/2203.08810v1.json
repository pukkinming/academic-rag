{
  "paper_id": "2203.08810v1",
  "title": "Semi-Fedser: Semi-Supervised Learning For Speech Emotion Recognition On Federated Learning Using Multiview Pseudo-Labeling",
  "published": "2022-03-15T21:50:43Z",
  "authors": [
    "Tiantian Feng",
    "Shrikanth Narayanan"
  ],
  "keywords": [
    "Speech Emotion Recognition",
    "Semi-supervised Learning",
    "Federated Learning",
    "Pseudo-labeling"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech Emotion Recognition (SER) application is frequently associated with privacy concerns as it often acquires and transmits speech data at the client-side to remote cloud platforms for further processing. These speech data can reveal not only speech content and affective information but the speaker's identity, demographic traits, and health status. Federated learning (FL) is a distributed machine learning algorithm that coordinates clients to train a model collaboratively without sharing local data. This algorithm shows enormous potential for SER applications as sharing raw speech or speech features from a user's device is vulnerable to privacy attacks. However, a major challenge in FL is limited availability of high-quality labeled data samples. In this work, we propose a semi-supervised federated learning framework, Semi-FedSER, that utilizes both labeled and unlabeled data samples to address the challenge of limited labeled data samples in FL. We show that our Semi-FedSER can generate desired SER performance even when the local label rate l = 20% using two SER benchmark datasets: IEMOCAP and MSP-Improv.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech emotion recognition (SER) has gained enormous interest in many diverse applications such as smart virtual assistants  [1] , medical diagnoses  [2, 3] , and education  [4] . SER aims to identify target emotional states conveyed in vocal expressions automatically. The SER system typically has three parts: data acquisition, data transfer, and classification  [5] . However, privacy is a major challenge that researchers or service providers often need to face before deploying the SER system. The speech signals are typically sensitive and can reveal a significant amount of information about an individual. These information include speech content and speaker's identity, traits (e.g., age, gender), and states (e.g., health status), many of which are deemed sensitive from an application point of view.\n\nFederated learning (FL) is a popular privacy-preserving distributed learning approach that allows clients to train a model collaboratively without sharing their local data  [6] . In an FL setting, a central server aggregates model updates from multiple clients during the training process. Each client generates such model updates by locally training a model on the private data available at the client. Furthermore, this machine learning approach reduces information leaks compared to classical centralized machine learning frameworks since personal data does not leave the client. Therefore, this distributed learning 1 This paper was submitted to Insterspeech 2022 for review.\n\nparadigm can be a natural choice for developing real-world multiuser SER applications as sharing raw speech or speech features from users' devices is vulnerable to privacy attacks.\n\nMost current works in FL have focused on fully supervised settings where all the input data are with labels. However, highquality labeled data samples do not often exist in real-life settings, and most data samples are indeed unlabeled. In order to address the limited labeled data samples in an FL setting, prior works have considered using semi-supervised learning (SSL), which utilizes unlabeled samples in addition to a small portion of labeled examples to obtain desired model performance  [7, 8] . SemiFed is an SSL framework that integrates supervised and unsupervised training during FL. This approach also generates pseudo-labels by ranking data samples using peer models. In addition, FedMatch  [9]  uses an inter-device consistency loss to enforce consistency between the pseudo-labeling predictions made across multiple devices. However, we argue that these methods are unrealistic in practice for two reasons: first, SemiFed significantly increases the communication costs occurring at each global training round, where the server needs to transfer additional models to each local client; additionally, sharing the local model to peers may cause potential breaches since the prior work has shown that data reconstruction is possible through accessing model updates  [10] .\n\nIn this work, we propose an SSL framework, Semi-FedSER, to address the challenge in limited labeled data samples for SER application in the FL setting. Semi-FedSER performs the model training, utilizing both labeled and unlabeled data samples at the local client following FixMatch framework  [11] . Furthermore, Semi-FedSER addresses the pseudolabeling issue mentioned earlier using the idea of multiview pseudo-labeling, and we adopt an efficient yet effective data augmentation technique called Stochastic Feature Augmentation (SFA)  [12] . In addition, we propose to use the past pseudo label information to improve the quality of the pseudo labels generated. To the best of our knowledge, the only similar work that exists in the literature is  [8]  for SER application. We show that our Semi-FedSER can generate desired SER performance even when the local label rate l = 20% and data distribution is completely non-iid using IEMOCAP  [13] , and MSP-Improv  [14]  datasets for the experiments.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ser Data Sets",
      "text": "In this work, we use two data sets for developing SER models. Table  1  shows the label distribution of utterances in these corpora used in this work.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Iemocap",
      "text": "The IEMOCAP database  [13]  is a multi-modal corpus collected from ten subjects (five male and five female) who target expressing categorical emotions. The data set contains measurements of motion, audio, and video of acted human interactions. The data set contains 10,039 utterances spoken under improvised and scripted conditions. Speakers used material from a fixed script in the scripted condition, while the utterance in the improvised condition is spontaneously spoken. In this work, we focus on the improvised sessions following the suggestions from  [15]  and our previous works. Due to the data imbalance issue in the IEMOCAP corpus, previous works use the four most frequently occurring emotion labels (neutral, sad, happiness, and anger) for training the SER model  [15] . In addition to this, we pick these four emotion classes because most other corpora contain these labels.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Msp-Improv",
      "text": "The MSP-Improv  [14]  corpus was used to study naturalistic emotions captured from improvised scenarios. The corpus includes audio and video data of utterances spoken in natural condition (2,785 utterances), target condition (652 target utterances in an improvised scenario), improvised condition (4,381 utterances from the remainder of the improvised scenarios), and read speech condition (620 utterances). The data were collected from 12 participants (six male and six female). Similarly to the IEMOCAP data set, we only use the data from the improvised conditions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Method",
      "text": "This section describes the proposed semi-supervised learning framework. To facilitate readability, we summarize the notations adopted in this paper in Table  2 .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Model Overview",
      "text": "In this work, we follow the model framework in FixMatch  [11] .\n\nWe perform two types of augmentations as part of FixMatch: strong and weak, denoted by Φ(•) and φ(•), respectively. We generate both weak and strong augmentations through noise addition. Specifically, we adopt the Stochastic Feature Augmentation (SFA)  [12]  where given a speech feature x, the weak augmentation is φ(x) = x ⊙ α + r, where α ∼ N (1, σ1), r ∼ N (0, σ2). We choose this method as this efficient yet straightforward feature augmentation technique has shown promising results in model generalization tasks  [12] . We choose a larger σ1 in Φ(•) than in φ(•).\n\nWe formulate the learning objectives to include labeled and unlabeled data information. Specifically, our learning function has a supervised loss term ls and an unsupervised loss term lu. The supervised loss on labeled data is the standard crossentropy loss, and given the k th client and its local model with parameter θ k , we aim to minimize the following ls with the weakly-augmented labeled data φ(x l ):\n\nWhen modeling the unlabeled data, our method computes a pseudo label for unlabeled data, which is then used in a standard cross-entropy loss. We obtain the pseudo label y ′ using the unlabeled weakly-augmented data. We explain our pseudo labeling process in the following subsection. We define the cross-entropy loss against the model's output using the stronglyaugmented version of unlabeled data x p :\n\nOur final goal is to minimize the combined loss below:",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Pseudo Labeling",
      "text": "Our Semi-FedSER framework adopts the idea of pseudo labeling proposed in  [7] . At each iteration, the model uses predictions from the previous iteration as target classes for unlabeled data samples as if they were true labels. The process goes through a fixed number of iterations until all unlabeled data are labeled. However, the generated pseudo labels can be inaccurate and negatively impact the training results. In order to improve the quality of the generated pseudo labels,  [7]  proposed to send the aggregated model θ t and K other local models θ t k to each client. Then for each local data sample, the local client produces K + 1 one-hot labels. This approach generates the final pseudo label only if a certain amount of models agrees with the sample label. However, we argue that this method is highly inefficient and unsafe in practice.\n\nMultiview pseudo-labeling (MvPL) Unlike the pseudolabeling process in  [7] , our framework uses the multiview pseudo-labeling (MvPL) approach proposed in  [16] . The MvPL takes in multiple complementary views of a single unlabeled data point and uses a shared model to generate predictions. Similarly, we decide the final pseudo label by aggregating predictions from all the views. Here, we generate the weaklyaugmented views of a speech feature sample using the SFA approach  [12] . Specifically, in our setting, the local client generates m weakly-augmented views using SFA for unlabeled data x u . Suppose there are m predicted class distributions q1, q2, ..., qm, and we can obtain the averaged predicted class distributions as q from all weakly-augmented views. We finally assign the pseudo label y ′ to x u if max(q) is above the confidence threshold τ . To further increase the confidence of a prediction, we apply the temperature scaling with a constant scalar temperature T to soften the logits output from the model. Specifically, the probability output for jth class qj is:\n\nCompared to the pseudo-labeling process in  [7] , our method is more efficient and does not require sharing local models.\n\nPseudo-labeling uncertainty To further increase the quality of the pseudo labels, we propose to use the idea of uncertaintyaware pseudo label selection process (UPS)  [17] . This pseudolabel selection process utilizes both the confidence and uncertainty of a network prediction. We obtain the uncertainty measure u(q) by calculating the standard deviation of m predicted class distributions. We compute the loss for an unlabeled strongly-augmented data Φ(x u ) as:\n\nFinally, we add data of the pseudo labels to the pseudo-label data and delete them from the unlabeled data. The entire pseudo labeling algorithm is presented in the algorithm 1.\n\nAlgorithm 1 MvPL with pseudo-labeling consistency\n\nfor i = 1, 2, ..., m do 6:\n\nφi(x u ) = x u ⊙ α + r where α ∼ N (1, σ1),\n\nr ∼ N (0, σ2)\n\nappend f (φi(x u ); θ t ) to q 9: q ← average(q) 10:\n\nu(q) ← std(q)\n\n11:\n\ny ′ ← arg max (q)\n\n12:\n\nif q ≥ τ and u(q) ≤ k then 13:",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Scaffold",
      "text": "We often encounter non-IID data at different clients in a typical FL setting. Thus, gradient drifting is a major challenge when training the FL algorithm in a non-IID data setup. To mitigate the impact of the gradient drifting, we propose to implement SCAFFOLD  [18] . SCAFFOLD is a robust algorithm that uses control variates c (variance reduction) to correct for the 'clientdrift' in its local updates. The details of the algorithm proof are in  [18] . Finally, the algorithm 2 shows the overall training procedure for our proposed Semi-FedSER pipeline.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "In this section, we describe our experimental setup and training details. The implementation of this work is at https://github.com/usc-sail/fed-ser-semi.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Data Preprocessing",
      "text": "To investigate the effectiveness of the proposed semi-supervised learning framework, we train our SER models on various speech representations. We first evaluate our proposed pipeline using the knowledge-based speech feature set, the Emo-Base feature set, computed from the OpenSMILE toolkit  [19] . In addition to the knowledge-based speech feature set, we propose to test our framework on SUPERB (Speech Processing Universal PERformance Benchmark)  [20] , which is designed to provide a standard and comprehensive testbed for pre-trained models on various downstream speech tasks. We compute the deep speech representations from the pre-trained models that are available in SUPERB including APC  [21] , TERA  [22] , and DeCoAR 2.0  [23]  and DistilHuBERT  [24] . We choose these three representations as they demonstrated better SER performance in our for Each client k ∈ S in parallel do 6: Lce(f (θ; Φ(x p )), y ′ )\n\nExecute Algorithm 1 21:\n\nreturn θ, c ′ , ∇c prior work  [25] . Finally, we calculate the global average of the last layer's hidden state as the final feature from the pre-trained model's output. Using the last hidden state is suggested in prior works for downstream tasks  [21, 26, 23, 27] . Our feature sizes are 988 in Emo-Base; 512 in APC; 768 in Tera, DistilHuBERT, and DeCoAR 2.0. We also apply z-normalization to the speech features within each client.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Non-Iid Data Setup",
      "text": "For simulating non-IID distributions over the clients in this experiment, we group the data by each speaker in the IEMOCAP and MSP-Improv data sets and split each speaker's data into 4 different shards. As a result, we produce pathological non-IID data for each partition by taking only 3 unique emotions from each speaker. We regard each partition as a client in federated learning. Consequently, each client in federated learning automatically gets 3 different emotions from one speaker. About 80% of the speakers are used for training, and the rest are for the test set. 20% of the data samples in each training client are used as validation, and the rest are training samples. We test our proposed semi-supervised learning pipeline at the labeling rate (training samples) of 20% and 40%. We repeat the experiments 5 times with test folds of different speakers in each data set, and we report the average results of the 5-fold experiments.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Model And Training Details",
      "text": "We use the multilayer perceptron (MLP) as the SER model architecture in this work. The model consists of 2 dense layers with layer sizes of {256, 128}. We choose ReLU as the activation function and the dropout rate as 0.2. We implement the SCAFFOLD algorithm in training the SER model. We conduct the experiments on a computer with two NVIDIA GeForce RTX 5000 GPUs to report the performance. We control only 10% of the clients participating in each global round. 80% of the data at a client is for local training, and the rest 20% is for valida- We set σ2 as 0.1 when generating weakly-augment samples and strongly-augment samples. We choose σ1 as 0.1 and 0.25 in generating weakly-augment samples and strongly-augment samples, respectively. We use a confidence score τ = 0.5 in the MvPL algorithm at the beginning of the training and linearly increase it to 0.9 at the 300th global training epoch. We apply a temperature value of 2 to soften the prediction output from weakly-augment samples during the MvPL. We empirically use the number of augmentations m and uncertainty score k as 10 and 0.005, respectively. To balance the pseudo label distribution, we add only one data sample with pseudo labels for each class in each training epoch.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Compared Baselines",
      "text": "We compare our semi-supervised learning framework with the following baselines: Supervised -Federated Here, we refer to supervised baseline as using only labeled data points at each local client with an assigned labeling rate. We train the supervised Federated baseline using the SCAFFOLD algorithm. Fully Supervised -Federated We train a fully supervised Federated baseline (use all the original labels of the training data) to measure the ideal accuracy we would obtain under federated learning settings. We train the fully supervised Federated baseline using both FedAvg and SCAFFOLD. Fully Supervised -Centralized Finally, we train a fully supervised centralized baseline to measure the upper bound SER model performance in different data sets.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Ser Performance",
      "text": "Baselines The emotion prediction results of FL training on different data sets are shown in Table  3 . We report the SER prediction results in unweighted average recall (UAR) scores. We observe that our fully-supervised federated SER performance using the SCAFFOLD algorithm yields similar performance to prior works in  [28, 29] . Moreover, we can identify that the performance of the fully supervised SER model trained in a centralized setup is slightly higher than the fully supervised SER model trained using the SCAFFOLD algorithm. Meanwhile, we find that the SCAFFOLD algorithm performs consistently better than the FedAvg algorithm in training the fully supervised SER model. These results suggest that the SCAFFOLD algorithm can significantly increase the model performance in the federated learning setup.\n\nSemi-FedSER When the label rate of the local client is at 20%, we can observe that the SER performance of the federated supervised model drops 5-10% compared to the fully federated supervised model on the different data sets and using different speech features. However, we can observe that SER models trained using our proposed Semi-FedSER framework generate similar UAR scores to fully federated supervised models. For example, the UAR score of the fully supervised model using FL is 66.70% in the IEMOCAP data set using the APC speech feature. In contrast, the SER model trained using our proposed Semi-FedSER framework on the same speech feature produces a similar UAR score of 63.11%. We can further observe that the performance of the Semi-FedSER framework is at the same level as the fully supervised FL model across all testing scenarios when the label rate is 40%. These comparisons demonstrate that the proposed Semi-FedSER can achieve desirable performance with few on-device labeled samples.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Conclusions",
      "text": "In this work, we propose a novel Semi-FedSER framework to address the challenges in limited labeled data sample settings for speech emotion recognition in FL setting. Semi-FedSER leverages both labeled and unlabeled data samples at the local client combined with pseudo-labeling. The pseudo-labeling approach in Semi-FedSER is based on the multiview pseudolabeling. We further improve the quality of the pseudo-labeling by incorporating the pseudo-labeling consistency measurement. We also implement the SCAFFOLD algorithm to address the non-IID data distribution issue in the FL setting. Our results show that the proposed Semi-FedSER framework effectively generates desired SER predictions even when the local label rate l = 20%. In the future, we plan to extend our current work to multi-modal data scenarios where not every modality is presented with labels. We also aim to design an automatic scheme for deciding the threshold τ in generating pseudo labels.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": ""
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": "Abstract"
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": ""
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": "Speech Emotion Recognition\n(SER)\napplication\nis\nfre-"
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": "quently associated with privacy concerns as it often acquires"
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": ""
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": "and transmits speech data at the client-side to remote cloud plat-"
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": ""
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": "forms for further processing. These speech data can reveal not"
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": ""
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": "only speech content and affective information but the speaker’s"
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": ""
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": "identity, demographic traits, and health status. Federated learn-"
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": ""
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": "ing (FL) is a distributed machine learning algorithm that coor-"
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": ""
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": "dinates clients to train a model collaboratively without sharing"
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": ""
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": "local data. This algorithm shows enormous potential\nfor SER"
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": ""
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": "applications as sharing raw speech or\nspeech features from a"
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": ""
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": "user’s device is vulnerable to privacy attacks. However, a major"
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": ""
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": "challenge in FL is limited availability of high-quality labeled"
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": ""
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": "data samples.\nIn this work, we propose a semi-supervised fed-"
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": ""
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": "erated learning framework, Semi-FedSER, that utilizes both la-"
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": ""
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": "beled and unlabeled data samples to address the challenge of"
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": ""
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": "limited labeled data samples in FL. We show that our Semi-"
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": ""
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": "FedSER can generate desired SER performance even when the"
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": ""
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": "local\nlabel\nrate l = 20% using two SER benchmark datasets:"
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": ""
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": "IEMOCAP and MSP-Improv."
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": ""
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": "Index Terms: Speech Emotion Recognition, Semi-supervised"
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": "Learning, Federated Learning, Pseudo-labeling"
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": "1"
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": ""
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": ""
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": "1.\nIntroduction"
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": ""
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": "Speech emotion recognition (SER) has gained enormous inter-"
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": "est in many diverse applications such as smart virtual assistants"
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": "[1], medical diagnoses [2, 3], and education [4]. SER aims to"
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": "identify target emotional states conveyed in vocal expressions"
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": "automatically. The SER system typically has three parts: data"
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": "acquisition, data transfer, and classiﬁcation [5]. However, pri-"
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": "vacy is a major challenge that researchers or service providers"
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": "often need to face before deploying the SER system.\nThe"
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": "speech signals are typically sensitive and can reveal a signiﬁ-"
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": "cant amount of information about an individual. These informa-"
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": "tion include speech content and speaker’s identity,\ntraits (e.g.,"
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": "age, gender), and states (e.g., health status), many of which are"
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": "deemed sensitive from an application point of view."
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": "Federated learning (FL) is a popular privacy-preserving dis-"
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": "tributed learning approach that allows clients to train a model"
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": "collaboratively without sharing their\nlocal data [6].\nIn an FL"
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": "setting, a central server aggregates model updates from multi-"
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": ""
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": "ple clients during the training process.\nEach client generates"
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": ""
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": "such model updates by locally training a model on the private"
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": ""
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": "data available at\nthe client.\nFurthermore,\nthis machine learn-"
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": "ing approach reduces information leaks compared to classical"
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": ""
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": "centralized machine learning frameworks\nsince personal data"
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": "does not\nleave the client.\nTherefore,\nthis distributed learning"
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": ""
        },
        {
          "1Signal Analysis and Interpretation Lab (SAIL), University of Southern California": "1This paper was submitted to Insterspeech 2022 for review."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 1: Statistics of emotion labels in two SER data sets.": ""
        },
        {
          "Table 1: Statistics of emotion labels in two SER data sets.": ""
        },
        {
          "Table 1: Statistics of emotion labels in two SER data sets.": "Neutral\nHappy\nSad\nAngry"
        },
        {
          "Table 1: Statistics of emotion labels in two SER data sets.": ""
        },
        {
          "Table 1: Statistics of emotion labels in two SER data sets.": "1099\n947\n608\n289"
        },
        {
          "Table 1: Statistics of emotion labels in two SER data sets.": ""
        },
        {
          "Table 1: Statistics of emotion labels in two SER data sets.": "2072\n1184\n739\n585"
        },
        {
          "Table 1: Statistics of emotion labels in two SER data sets.": ""
        },
        {
          "Table 1: Statistics of emotion labels in two SER data sets.": ""
        },
        {
          "Table 1: Statistics of emotion labels in two SER data sets.": "Table 2: Notation used in this paper."
        },
        {
          "Table 1: Statistics of emotion labels in two SER data sets.": ""
        },
        {
          "Table 1: Statistics of emotion labels in two SER data sets.": ""
        },
        {
          "Table 1: Statistics of emotion labels in two SER data sets.": "Global training epoch in FL."
        },
        {
          "Table 1: Statistics of emotion labels in two SER data sets.": "Client index."
        },
        {
          "Table 1: Statistics of emotion labels in two SER data sets.": "Global SER model at tth global training epoch."
        },
        {
          "Table 1: Statistics of emotion labels in two SER data sets.": ""
        },
        {
          "Table 1: Statistics of emotion labels in two SER data sets.": "Local SER model updates of kth client at tth global"
        },
        {
          "Table 1: Statistics of emotion labels in two SER data sets.": "training epoch."
        },
        {
          "Table 1: Statistics of emotion labels in two SER data sets.": "Labeled speech data of the kth client."
        },
        {
          "Table 1: Statistics of emotion labels in two SER data sets.": ""
        },
        {
          "Table 1: Statistics of emotion labels in two SER data sets.": "Speech data with hard pseudo labels of the kth client."
        },
        {
          "Table 1: Statistics of emotion labels in two SER data sets.": ""
        },
        {
          "Table 1: Statistics of emotion labels in two SER data sets.": "Unlabeled speech data of the kth client."
        },
        {
          "Table 1: Statistics of emotion labels in two SER data sets.": ""
        },
        {
          "Table 1: Statistics of emotion labels in two SER data sets.": ""
        },
        {
          "Table 1: Statistics of emotion labels in two SER data sets.": "Labeled speech data."
        },
        {
          "Table 1: Statistics of emotion labels in two SER data sets.": ""
        },
        {
          "Table 1: Statistics of emotion labels in two SER data sets.": "Unlabeled speech data."
        },
        {
          "Table 1: Statistics of emotion labels in two SER data sets.": ""
        },
        {
          "Table 1: Statistics of emotion labels in two SER data sets.": "Unlabeled speech data with pseudo labels."
        },
        {
          "Table 1: Statistics of emotion labels in two SER data sets.": "Weakly-augmentation function."
        },
        {
          "Table 1: Statistics of emotion labels in two SER data sets.": ""
        },
        {
          "Table 1: Statistics of emotion labels in two SER data sets.": "Strongly-augmentation function."
        },
        {
          "Table 1: Statistics of emotion labels in two SER data sets.": ""
        },
        {
          "Table 1: Statistics of emotion labels in two SER data sets.": "Emotion label."
        },
        {
          "Table 1: Statistics of emotion labels in two SER data sets.": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Weakly-augmentation function.\nφ(·)": ""
        },
        {
          "Weakly-augmentation function.\nφ(·)": "Strongly-augmentation function.\nΦ(·)"
        },
        {
          "Weakly-augmentation function.\nφ(·)": "y"
        },
        {
          "Weakly-augmentation function.\nφ(·)": "Emotion label."
        },
        {
          "Weakly-augmentation function.\nφ(·)": ""
        },
        {
          "Weakly-augmentation function.\nφ(·)": "of motion, audio, and video of acted human interactions. The"
        },
        {
          "Weakly-augmentation function.\nφ(·)": ""
        },
        {
          "Weakly-augmentation function.\nφ(·)": "data set contains 10,039 utterances\nspoken under\nimprovised"
        },
        {
          "Weakly-augmentation function.\nφ(·)": "and scripted conditions.\nSpeakers used material\nfrom a ﬁxed"
        },
        {
          "Weakly-augmentation function.\nφ(·)": "script in the scripted condition, while the utterance in the impro-"
        },
        {
          "Weakly-augmentation function.\nφ(·)": "vised condition is spontaneously spoken. In this work, we focus"
        },
        {
          "Weakly-augmentation function.\nφ(·)": "on the improvised sessions following the suggestions from [15]"
        },
        {
          "Weakly-augmentation function.\nφ(·)": "and our previous works. Due to the data imbalance issue in the"
        },
        {
          "Weakly-augmentation function.\nφ(·)": "IEMOCAP corpus, previous works use the four most frequently"
        },
        {
          "Weakly-augmentation function.\nφ(·)": "occurring emotion labels (neutral,\nsad, happiness, and anger)"
        },
        {
          "Weakly-augmentation function.\nφ(·)": "for\ntraining the SER model\n[15].\nIn addition to this, we pick"
        },
        {
          "Weakly-augmentation function.\nφ(·)": "these four emotion classes because most other corpora contain"
        },
        {
          "Weakly-augmentation function.\nφ(·)": "these labels."
        },
        {
          "Weakly-augmentation function.\nφ(·)": ""
        },
        {
          "Weakly-augmentation function.\nφ(·)": "2.2. MSP-Improv"
        },
        {
          "Weakly-augmentation function.\nφ(·)": ""
        },
        {
          "Weakly-augmentation function.\nφ(·)": "The MSP-Improv [14] corpus was used to study naturalistic"
        },
        {
          "Weakly-augmentation function.\nφ(·)": "emotions captured from improvised scenarios. The corpus in-"
        },
        {
          "Weakly-augmentation function.\nφ(·)": "cludes audio and video data of utterances\nspoken in natural"
        },
        {
          "Weakly-augmentation function.\nφ(·)": "condition (2,785 utterances),\ntarget condition (652 target utter-"
        },
        {
          "Weakly-augmentation function.\nφ(·)": "ances in an improvised scenario),\nimprovised condition (4,381"
        },
        {
          "Weakly-augmentation function.\nφ(·)": "utterances from the remainder of the improvised scenarios), and"
        },
        {
          "Weakly-augmentation function.\nφ(·)": "read speech condition (620 utterances). The data were collected"
        },
        {
          "Weakly-augmentation function.\nφ(·)": "from 12 participants (six male and six female). Similarly to the"
        },
        {
          "Weakly-augmentation function.\nφ(·)": "IEMOCAP data set, we only use the data from the improvised"
        },
        {
          "Weakly-augmentation function.\nφ(·)": "conditions."
        },
        {
          "Weakly-augmentation function.\nφ(·)": ""
        },
        {
          "Weakly-augmentation function.\nφ(·)": "3. Method"
        },
        {
          "Weakly-augmentation function.\nφ(·)": ""
        },
        {
          "Weakly-augmentation function.\nφ(·)": "This section describes the proposed semi-supervised learning"
        },
        {
          "Weakly-augmentation function.\nφ(·)": ""
        },
        {
          "Weakly-augmentation function.\nφ(·)": "framework.\nTo facilitate readability, we summarize the nota-"
        },
        {
          "Weakly-augmentation function.\nφ(·)": ""
        },
        {
          "Weakly-augmentation function.\nφ(·)": "tions adopted in this paper in Table 2."
        },
        {
          "Weakly-augmentation function.\nφ(·)": ""
        },
        {
          "Weakly-augmentation function.\nφ(·)": ""
        },
        {
          "Weakly-augmentation function.\nφ(·)": "3.1. Model Overview"
        },
        {
          "Weakly-augmentation function.\nφ(·)": ""
        },
        {
          "Weakly-augmentation function.\nφ(·)": "In this work, we follow the model framework in FixMatch [11]."
        },
        {
          "Weakly-augmentation function.\nφ(·)": "We perform two types of augmentations as part of FixMatch:"
        },
        {
          "Weakly-augmentation function.\nφ(·)": "respectively. We\nstrong and weak, denoted by Φ(·) and φ(·),"
        },
        {
          "Weakly-augmentation function.\nφ(·)": ""
        },
        {
          "Weakly-augmentation function.\nφ(·)": "generate both weak and strong augmentations through noise ad-"
        },
        {
          "Weakly-augmentation function.\nφ(·)": ""
        },
        {
          "Weakly-augmentation function.\nφ(·)": "dition. Speciﬁcally, we adopt\nthe Stochastic Feature Augmen-"
        },
        {
          "Weakly-augmentation function.\nφ(·)": "tation (SFA) [12] where given a speech feature x, the weak aug-"
        },
        {
          "Weakly-augmentation function.\nφ(·)": "mentation is φ(x) = x ⊙ α + r,\nwhere α ∼ N (1, σ1), r ∼"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "the pseudo labels, we propose to use the idea of uncertainty-",
          "Algorithm 2 Semi-FedSER": ""
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "",
          "Algorithm 2 Semi-FedSER": "1:\nInitialize: θ0, c0"
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "aware pseudo label selection process (UPS) [17]. This pseudo-",
          "Algorithm 2 Semi-FedSER": ""
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "",
          "Algorithm 2 Semi-FedSER": "2: Server executes:"
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "label\nselection process utilizes both the\nconﬁdence\nand un-",
          "Algorithm 2 Semi-FedSER": ""
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "",
          "Algorithm 2 Semi-FedSER": "3:\nfor Each round t = 0, ..., T − 1 do"
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "certainty of a network prediction. We obtain the uncertainty",
          "Algorithm 2 Semi-FedSER": ""
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "",
          "Algorithm 2 Semi-FedSER": "4:\nSample clients S ∈ {1, 2, ..., K}"
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "measure u(q) by calculating the standard deviation of m pre-",
          "Algorithm 2 Semi-FedSER": ""
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "",
          "Algorithm 2 Semi-FedSER": "5:\nfor Each client k ∈ S in parallel do"
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "dicted class distributions. We compute the loss for an unlabeled",
          "Algorithm 2 Semi-FedSER": ""
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "",
          "Algorithm 2 Semi-FedSER": "6:\nθt\nk ← θt"
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "strongly-augmented data Φ(xu) as:",
          "Algorithm 2 Semi-FedSER": ""
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "",
          "Algorithm 2 Semi-FedSER": "7:\nθt\nk, ck, ∇ck ← ClientLocalTraining(θt\nk, ct, ck)"
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "",
          "Algorithm 2 Semi-FedSER": "θt+1 ← 1"
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "",
          "Algorithm 2 Semi-FedSER": "8:\nk\n|S| Pk∈S θt"
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "(5)\n1(¯q ≥ τ ) · 1(u(q) ≤ k) · Lce(f (θk; Φ(xu)), y′)",
          "Algorithm 2 Semi-FedSER": ""
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "",
          "Algorithm 2 Semi-FedSER": "ct+1 ← ct + 1"
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "",
          "Algorithm 2 Semi-FedSER": "9:\n|S| Pk∈S ∇ck"
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "Finally, we add data of the pseudo labels to the pseudo-label",
          "Algorithm 2 Semi-FedSER": ""
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "",
          "Algorithm 2 Semi-FedSER": "10:\nfunction CLIENTLOCALTRAINING(θ, c, c′)"
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "data and delete them from the unlabeled data. The entire pseudo",
          "Algorithm 2 Semi-FedSER": ""
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "",
          "Algorithm 2 Semi-FedSER": "11:\nθold ← θ"
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "labeling algorithm is presented in the algorithm 1.",
          "Algorithm 2 Semi-FedSER": ""
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "",
          "Algorithm 2 Semi-FedSER": "12:\ncold ← c′"
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "",
          "Algorithm 2 Semi-FedSER": "13:\nfor Local epoch e from 0 to E − 1 do"
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "Algorithm 1 MvPL with pseudo-labeling consistency",
          "Algorithm 2 Semi-FedSER": "14:\nfor Iteration i from 0 to I − 1 do"
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "′",
          "Algorithm 2 Semi-FedSER": "15:\nSample mini-batch l from Dl"
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "1:\nInput: Dp\nk, θt\nk , m, τ , k\nk, Du",
          "Algorithm 2 Semi-FedSER": "k"
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "",
          "Algorithm 2 Semi-FedSER": "16:\nSample mini-batch p from Dp"
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "",
          "Algorithm 2 Semi-FedSER": "k"
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "2:\nfor each client k ∈ S in parallel do",
          "Algorithm 2 Semi-FedSER": ""
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "",
          "Algorithm 2 Semi-FedSER": "17:\nθ ← θ − η∇θ(Lce(f (θ; φ(xl)), y)+"
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "3:\nfor xu ∈ Du\nk do",
          "Algorithm 2 Semi-FedSER": ""
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "",
          "Algorithm 2 Semi-FedSER": "18:\nLce(f (θ; Φ(xp)), y′)"
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "4:\nq ← []",
          "Algorithm 2 Semi-FedSER": ""
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "",
          "Algorithm 2 Semi-FedSER": "19:\nθ ← θ − η(c − c′)"
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "5:\nfor i = 1, 2, ..., m do",
          "Algorithm 2 Semi-FedSER": ""
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "",
          "Algorithm 2 Semi-FedSER": "20:\nExecute Algorithm 1"
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "6:\nφi(xu) = xu ⊙ α + r where α ∼ N (1, σ1),",
          "Algorithm 2 Semi-FedSER": ""
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "",
          "Algorithm 2 Semi-FedSER": "1"
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "",
          "Algorithm 2 Semi-FedSER": "c′ ← c′ − c +"
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "7:\nr ∼ N (0, σ2)",
          "Algorithm 2 Semi-FedSER": "21:\nEIη (θold − θ)"
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "8:\nappend f (φi(xu); θt) to q",
          "Algorithm 2 Semi-FedSER": ""
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "",
          "Algorithm 2 Semi-FedSER": "22:\nend∇c = c′ − cold"
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "9:\nq ← average(q)",
          "Algorithm 2 Semi-FedSER": "23:\nreturn θ, c′, ∇c"
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "10:\nu(q) ← std(q)",
          "Algorithm 2 Semi-FedSER": ""
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "11:\ny′ ← arg max (¯q)",
          "Algorithm 2 Semi-FedSER": ""
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "12:\nif ¯q ≥ τ and u(q) ≤ k then",
          "Algorithm 2 Semi-FedSER": ""
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "",
          "Algorithm 2 Semi-FedSER": "prior work [25]. Finally, we calculate the global average of the"
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "13:\nDp\nk \\ xu\nk ∪ xu; Du",
          "Algorithm 2 Semi-FedSER": ""
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "",
          "Algorithm 2 Semi-FedSER": "last layer’s hidden state as the ﬁnal feature from the pre-trained"
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "",
          "Algorithm 2 Semi-FedSER": "model’s output. Using the last hidden state is suggested in prior"
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "3.3.\nSCAFFOLD",
          "Algorithm 2 Semi-FedSER": "works for downstream tasks [21, 26, 23, 27]. Our feature sizes"
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "",
          "Algorithm 2 Semi-FedSER": "are 988 in Emo-Base; 512 in APC; 768 in Tera, DistilHuBERT,"
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "We often encounter non-IID data at different clients in a typical",
          "Algorithm 2 Semi-FedSER": ""
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "",
          "Algorithm 2 Semi-FedSER": "and DeCoAR 2.0. We also apply z-normalization to the speech"
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "FL setting. Thus, gradient drifting is a major challenge when",
          "Algorithm 2 Semi-FedSER": ""
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "",
          "Algorithm 2 Semi-FedSER": "features within each client."
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "training the FL algorithm in a non-IID data setup. To mitigate",
          "Algorithm 2 Semi-FedSER": ""
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "the impact of\nthe gradient drifting, we propose to implement",
          "Algorithm 2 Semi-FedSER": "4.2. Non-IID Data setup"
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "SCAFFOLD [18]. SCAFFOLD is a robust algorithm that uses",
          "Algorithm 2 Semi-FedSER": ""
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "",
          "Algorithm 2 Semi-FedSER": "For simulating non-IID distributions over the clients in this ex-"
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "control variates c (variance reduction) to correct for the ’client-",
          "Algorithm 2 Semi-FedSER": ""
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "",
          "Algorithm 2 Semi-FedSER": "periment, we group the data by each speaker in the IEMOCAP"
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "drift’\nin its local updates.\nThe details of\nthe algorithm proof",
          "Algorithm 2 Semi-FedSER": ""
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "",
          "Algorithm 2 Semi-FedSER": "and MSP-Improv data sets and split each speaker’s data into 4"
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "are in [18]. Finally,\nthe algorithm 2 shows the overall\ntraining",
          "Algorithm 2 Semi-FedSER": ""
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "",
          "Algorithm 2 Semi-FedSER": "different shards. As a result, we produce pathological non-IID"
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "procedure for our proposed Semi-FedSER pipeline.",
          "Algorithm 2 Semi-FedSER": ""
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "",
          "Algorithm 2 Semi-FedSER": "data for each partition by taking only 3 unique emotions from"
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "",
          "Algorithm 2 Semi-FedSER": "each speaker. We regard each partition as a client\nin federated"
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "4. Experiments",
          "Algorithm 2 Semi-FedSER": ""
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "",
          "Algorithm 2 Semi-FedSER": "learning. Consequently, each client\nin federated learning auto-"
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "In\nthis\nsection,\nwe\ndescribe\nour\nexperimental\nsetup\nand",
          "Algorithm 2 Semi-FedSER": "matically gets 3 different emotions from one speaker. About"
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "training\ndetails.\nThe\nimplementation\nof\nthis work\nis\nat",
          "Algorithm 2 Semi-FedSER": "80% of the speakers are used for training, and the rest are for"
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "https://github.com/usc-sail/fed-ser-semi.",
          "Algorithm 2 Semi-FedSER": "the test set. 20% of the data samples in each training client are"
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "",
          "Algorithm 2 Semi-FedSER": "used as validation, and the rest are training samples. We test our"
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "4.1. Data Preprocessing",
          "Algorithm 2 Semi-FedSER": ""
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "",
          "Algorithm 2 Semi-FedSER": "proposed semi-supervised learning pipeline at\nthe labeling rate"
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "",
          "Algorithm 2 Semi-FedSER": "(training samples) of 20% and 40%. We repeat the experiments"
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "To investigate the effectiveness of the proposed semi-supervised",
          "Algorithm 2 Semi-FedSER": ""
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "",
          "Algorithm 2 Semi-FedSER": "5 times with test folds of different speakers in each data set, and"
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "learning\nframework, we\ntrain\nour\nSER models\non\nvarious",
          "Algorithm 2 Semi-FedSER": ""
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "",
          "Algorithm 2 Semi-FedSER": "we report the average results of the 5-fold experiments."
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "speech representations. We ﬁrst evaluate our proposed pipeline",
          "Algorithm 2 Semi-FedSER": ""
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "using the knowledge-based speech feature set,\nthe Emo-Base",
          "Algorithm 2 Semi-FedSER": ""
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "",
          "Algorithm 2 Semi-FedSER": "4.3. Model and training Details"
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "feature set, computed from the OpenSMILE toolkit [19]. In ad-",
          "Algorithm 2 Semi-FedSER": ""
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "dition to the knowledge-based speech feature set, we propose to",
          "Algorithm 2 Semi-FedSER": "We use the multilayer perceptron (MLP) as the SER model ar-"
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "test our framework on SUPERB (Speech Processing Universal",
          "Algorithm 2 Semi-FedSER": "chitecture in this work. The model consists of 2 dense layers"
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "PERformance Benchmark) [20], which is designed to provide a",
          "Algorithm 2 Semi-FedSER": "with layer sizes of {256, 128}. We choose ReLU as the acti-"
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "standard and comprehensive testbed for pre-trained models on",
          "Algorithm 2 Semi-FedSER": "vation function and the dropout rate as 0.2. We implement\nthe"
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "various downstream speech tasks. We compute the deep speech",
          "Algorithm 2 Semi-FedSER": "SCAFFOLD algorithm in training the SER model. We conduct"
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "representations from the pre-trained models that are available",
          "Algorithm 2 Semi-FedSER": "the experiments on a computer with two NVIDIA GeForce RTX"
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "in SUPERB including APC [21], TERA [22],\nand DeCoAR",
          "Algorithm 2 Semi-FedSER": "5000 GPUs to report the performance. We control only 10% of"
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "2.0 [23] and DistilHuBERT [24]. We choose these three repre-",
          "Algorithm 2 Semi-FedSER": "the clients participating in each global round. 80% of the data"
        },
        {
          "Pseudo-labeling uncertainty To further increase the quality of": "sentations as they demonstrated better SER performance in our",
          "Algorithm 2 Semi-FedSER": "at a client\nis for local\ntraining, and the rest 20% is for valida-"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "DistilHuBERT, and DeCoAR 2.0 computed using pre-trained models. L represents the label rate at the local client.": ""
        },
        {
          "DistilHuBERT, and DeCoAR 2.0 computed using pre-trained models. L represents the label rate at the local client.": ""
        },
        {
          "DistilHuBERT, and DeCoAR 2.0 computed using pre-trained models. L represents the label rate at the local client.": "Data set"
        },
        {
          "DistilHuBERT, and DeCoAR 2.0 computed using pre-trained models. L represents the label rate at the local client.": ""
        },
        {
          "DistilHuBERT, and DeCoAR 2.0 computed using pre-trained models. L represents the label rate at the local client.": ""
        },
        {
          "DistilHuBERT, and DeCoAR 2.0 computed using pre-trained models. L represents the label rate at the local client.": ""
        },
        {
          "DistilHuBERT, and DeCoAR 2.0 computed using pre-trained models. L represents the label rate at the local client.": "IEMOCAP"
        },
        {
          "DistilHuBERT, and DeCoAR 2.0 computed using pre-trained models. L represents the label rate at the local client.": ""
        },
        {
          "DistilHuBERT, and DeCoAR 2.0 computed using pre-trained models. L represents the label rate at the local client.": ""
        },
        {
          "DistilHuBERT, and DeCoAR 2.0 computed using pre-trained models. L represents the label rate at the local client.": ""
        },
        {
          "DistilHuBERT, and DeCoAR 2.0 computed using pre-trained models. L represents the label rate at the local client.": ""
        },
        {
          "DistilHuBERT, and DeCoAR 2.0 computed using pre-trained models. L represents the label rate at the local client.": ""
        },
        {
          "DistilHuBERT, and DeCoAR 2.0 computed using pre-trained models. L represents the label rate at the local client.": "MSP-Improv"
        },
        {
          "DistilHuBERT, and DeCoAR 2.0 computed using pre-trained models. L represents the label rate at the local client.": ""
        },
        {
          "DistilHuBERT, and DeCoAR 2.0 computed using pre-trained models. L represents the label rate at the local client.": ""
        },
        {
          "DistilHuBERT, and DeCoAR 2.0 computed using pre-trained models. L represents the label rate at the local client.": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "tion. We set\nthe local\ntraining batch size as 16. Expressly, we": "set the learning rate as 0.0001 and the local training epoch as 1",
          "model\ntrained using the SCAFFOLD algorithm. Meanwhile,": "we ﬁnd that\nthe SCAFFOLD algorithm performs consistently"
        },
        {
          "tion. We set\nthe local\ntraining batch size as 16. Expressly, we": "in the FL algorithm. The total global\ntraining epochs are 500.",
          "model\ntrained using the SCAFFOLD algorithm. Meanwhile,": "better\nthan the FedAvg algorithm in training the fully super-"
        },
        {
          "tion. We set\nthe local\ntraining batch size as 16. Expressly, we": "We set σ2\nas 0.1 when generating weakly-augment\nsamples",
          "model\ntrained using the SCAFFOLD algorithm. Meanwhile,": "vised SER model. These results suggest\nthat\nthe SCAFFOLD"
        },
        {
          "tion. We set\nthe local\ntraining batch size as 16. Expressly, we": "and strongly-augment samples. We choose σ1 as 0.1 and 0.25",
          "model\ntrained using the SCAFFOLD algorithm. Meanwhile,": "algorithm can signiﬁcantly increase the model performance in"
        },
        {
          "tion. We set\nthe local\ntraining batch size as 16. Expressly, we": "in generating weakly-augment\nsamples and strongly-augment",
          "model\ntrained using the SCAFFOLD algorithm. Meanwhile,": "the federated learning setup."
        },
        {
          "tion. We set\nthe local\ntraining batch size as 16. Expressly, we": "samples,\nrespectively. We use a conﬁdence score τ = 0.5 in",
          "model\ntrained using the SCAFFOLD algorithm. Meanwhile,": ""
        },
        {
          "tion. We set\nthe local\ntraining batch size as 16. Expressly, we": "",
          "model\ntrained using the SCAFFOLD algorithm. Meanwhile,": "Semi-FedSER When the label rate of the local client is at 20%,"
        },
        {
          "tion. We set\nthe local\ntraining batch size as 16. Expressly, we": "the MvPL algorithm at\nthe beginning of\nthe training and lin-",
          "model\ntrained using the SCAFFOLD algorithm. Meanwhile,": ""
        },
        {
          "tion. We set\nthe local\ntraining batch size as 16. Expressly, we": "",
          "model\ntrained using the SCAFFOLD algorithm. Meanwhile,": "we can observe that\nthe SER performance of the federated su-"
        },
        {
          "tion. We set\nthe local\ntraining batch size as 16. Expressly, we": "early increase it\nto 0.9 at\nthe 300th global\ntraining epoch. We",
          "model\ntrained using the SCAFFOLD algorithm. Meanwhile,": ""
        },
        {
          "tion. We set\nthe local\ntraining batch size as 16. Expressly, we": "",
          "model\ntrained using the SCAFFOLD algorithm. Meanwhile,": "pervised model drops 5-10% compared to the fully federated"
        },
        {
          "tion. We set\nthe local\ntraining batch size as 16. Expressly, we": "apply a temperature value of 2 to soften the prediction output",
          "model\ntrained using the SCAFFOLD algorithm. Meanwhile,": ""
        },
        {
          "tion. We set\nthe local\ntraining batch size as 16. Expressly, we": "",
          "model\ntrained using the SCAFFOLD algorithm. Meanwhile,": "supervised model on the different data sets and using different"
        },
        {
          "tion. We set\nthe local\ntraining batch size as 16. Expressly, we": "from weakly-augment\nsamples during the MvPL. We empiri-",
          "model\ntrained using the SCAFFOLD algorithm. Meanwhile,": ""
        },
        {
          "tion. We set\nthe local\ntraining batch size as 16. Expressly, we": "",
          "model\ntrained using the SCAFFOLD algorithm. Meanwhile,": "speech features.\nHowever, we can observe that SER models"
        },
        {
          "tion. We set\nthe local\ntraining batch size as 16. Expressly, we": "cally use the number of augmentations m and uncertainty score",
          "model\ntrained using the SCAFFOLD algorithm. Meanwhile,": ""
        },
        {
          "tion. We set\nthe local\ntraining batch size as 16. Expressly, we": "",
          "model\ntrained using the SCAFFOLD algorithm. Meanwhile,": "trained using our proposed Semi-FedSER framework generate"
        },
        {
          "tion. We set\nthe local\ntraining batch size as 16. Expressly, we": "respectively.\nTo balance the pseudo label\nk as 10 and 0.005,",
          "model\ntrained using the SCAFFOLD algorithm. Meanwhile,": ""
        },
        {
          "tion. We set\nthe local\ntraining batch size as 16. Expressly, we": "",
          "model\ntrained using the SCAFFOLD algorithm. Meanwhile,": "similar UAR scores to fully federated supervised models. For"
        },
        {
          "tion. We set\nthe local\ntraining batch size as 16. Expressly, we": "distribution, we add only one data sample with pseudo labels",
          "model\ntrained using the SCAFFOLD algorithm. Meanwhile,": ""
        },
        {
          "tion. We set\nthe local\ntraining batch size as 16. Expressly, we": "",
          "model\ntrained using the SCAFFOLD algorithm. Meanwhile,": "example,\nthe UAR score of\nthe fully supervised model using"
        },
        {
          "tion. We set\nthe local\ntraining batch size as 16. Expressly, we": "for each class in each training epoch.",
          "model\ntrained using the SCAFFOLD algorithm. Meanwhile,": ""
        },
        {
          "tion. We set\nthe local\ntraining batch size as 16. Expressly, we": "",
          "model\ntrained using the SCAFFOLD algorithm. Meanwhile,": "FL is 66.70% in the IEMOCAP data set using the APC speech"
        },
        {
          "tion. We set\nthe local\ntraining batch size as 16. Expressly, we": "",
          "model\ntrained using the SCAFFOLD algorithm. Meanwhile,": "feature.\nIn contrast, the SER model\ntrained using our proposed"
        },
        {
          "tion. We set\nthe local\ntraining batch size as 16. Expressly, we": "5. Results",
          "model\ntrained using the SCAFFOLD algorithm. Meanwhile,": ""
        },
        {
          "tion. We set\nthe local\ntraining batch size as 16. Expressly, we": "",
          "model\ntrained using the SCAFFOLD algorithm. Meanwhile,": "Semi-FedSER framework on the same speech feature produces"
        },
        {
          "tion. We set\nthe local\ntraining batch size as 16. Expressly, we": "5.1. Compared baselines",
          "model\ntrained using the SCAFFOLD algorithm. Meanwhile,": "a similar UAR score of 63.11%. We can further observe that"
        },
        {
          "tion. We set\nthe local\ntraining batch size as 16. Expressly, we": "",
          "model\ntrained using the SCAFFOLD algorithm. Meanwhile,": "the performance of the Semi-FedSER framework is at the same"
        },
        {
          "tion. We set\nthe local\ntraining batch size as 16. Expressly, we": "We compare our semi-supervised learning framework with the",
          "model\ntrained using the SCAFFOLD algorithm. Meanwhile,": ""
        },
        {
          "tion. We set\nthe local\ntraining batch size as 16. Expressly, we": "",
          "model\ntrained using the SCAFFOLD algorithm. Meanwhile,": "level as the fully supervised FL model across all testing scenar-"
        },
        {
          "tion. We set\nthe local\ntraining batch size as 16. Expressly, we": "following baselines:",
          "model\ntrained using the SCAFFOLD algorithm. Meanwhile,": ""
        },
        {
          "tion. We set\nthe local\ntraining batch size as 16. Expressly, we": "",
          "model\ntrained using the SCAFFOLD algorithm. Meanwhile,": "ios when the label rate is 40%. These comparisons demonstrate"
        },
        {
          "tion. We set\nthe local\ntraining batch size as 16. Expressly, we": "Supervised - Federated Here, we refer to supervised baseline",
          "model\ntrained using the SCAFFOLD algorithm. Meanwhile,": ""
        },
        {
          "tion. We set\nthe local\ntraining batch size as 16. Expressly, we": "",
          "model\ntrained using the SCAFFOLD algorithm. Meanwhile,": "that\nthe proposed Semi-FedSER can achieve desirable perfor-"
        },
        {
          "tion. We set\nthe local\ntraining batch size as 16. Expressly, we": "as using only labeled data points at each local client with an as-",
          "model\ntrained using the SCAFFOLD algorithm. Meanwhile,": ""
        },
        {
          "tion. We set\nthe local\ntraining batch size as 16. Expressly, we": "",
          "model\ntrained using the SCAFFOLD algorithm. Meanwhile,": "mance with few on-device labeled samples."
        },
        {
          "tion. We set\nthe local\ntraining batch size as 16. Expressly, we": "signed labeling rate. We train the supervised Federated baseline",
          "model\ntrained using the SCAFFOLD algorithm. Meanwhile,": ""
        },
        {
          "tion. We set\nthe local\ntraining batch size as 16. Expressly, we": "using the SCAFFOLD algorithm.",
          "model\ntrained using the SCAFFOLD algorithm. Meanwhile,": ""
        },
        {
          "tion. We set\nthe local\ntraining batch size as 16. Expressly, we": "Fully Supervised - Federated We train a fully supervised Fed-",
          "model\ntrained using the SCAFFOLD algorithm. Meanwhile,": "6. Conclusions"
        },
        {
          "tion. We set\nthe local\ntraining batch size as 16. Expressly, we": "erated baseline (use all\nthe original\nlabels of the training data)",
          "model\ntrained using the SCAFFOLD algorithm. Meanwhile,": ""
        },
        {
          "tion. We set\nthe local\ntraining batch size as 16. Expressly, we": "to measure the ideal accuracy we would obtain under federated",
          "model\ntrained using the SCAFFOLD algorithm. Meanwhile,": ""
        },
        {
          "tion. We set\nthe local\ntraining batch size as 16. Expressly, we": "",
          "model\ntrained using the SCAFFOLD algorithm. Meanwhile,": "In this work, we propose a novel Semi-FedSER framework to"
        },
        {
          "tion. We set\nthe local\ntraining batch size as 16. Expressly, we": "learning settings. We train the fully supervised Federated base-",
          "model\ntrained using the SCAFFOLD algorithm. Meanwhile,": ""
        },
        {
          "tion. We set\nthe local\ntraining batch size as 16. Expressly, we": "",
          "model\ntrained using the SCAFFOLD algorithm. Meanwhile,": "address the challenges in limited labeled data sample settings"
        },
        {
          "tion. We set\nthe local\ntraining batch size as 16. Expressly, we": "line using both FedAvg and SCAFFOLD.",
          "model\ntrained using the SCAFFOLD algorithm. Meanwhile,": ""
        },
        {
          "tion. We set\nthe local\ntraining batch size as 16. Expressly, we": "",
          "model\ntrained using the SCAFFOLD algorithm. Meanwhile,": "for\nspeech emotion recognition in FL setting.\nSemi-FedSER"
        },
        {
          "tion. We set\nthe local\ntraining batch size as 16. Expressly, we": "Fully Supervised - Centralized Finally, we train a fully su-",
          "model\ntrained using the SCAFFOLD algorithm. Meanwhile,": ""
        },
        {
          "tion. We set\nthe local\ntraining batch size as 16. Expressly, we": "",
          "model\ntrained using the SCAFFOLD algorithm. Meanwhile,": "leverages both labeled and unlabeled data samples at\nthe lo-"
        },
        {
          "tion. We set\nthe local\ntraining batch size as 16. Expressly, we": "pervised centralized baseline to measure the upper bound SER",
          "model\ntrained using the SCAFFOLD algorithm. Meanwhile,": ""
        },
        {
          "tion. We set\nthe local\ntraining batch size as 16. Expressly, we": "",
          "model\ntrained using the SCAFFOLD algorithm. Meanwhile,": "cal client combined with pseudo-labeling. The pseudo-labeling"
        },
        {
          "tion. We set\nthe local\ntraining batch size as 16. Expressly, we": "model performance in different data sets.",
          "model\ntrained using the SCAFFOLD algorithm. Meanwhile,": ""
        },
        {
          "tion. We set\nthe local\ntraining batch size as 16. Expressly, we": "",
          "model\ntrained using the SCAFFOLD algorithm. Meanwhile,": "approach in Semi-FedSER is based on the multiview pseudo-"
        },
        {
          "tion. We set\nthe local\ntraining batch size as 16. Expressly, we": "",
          "model\ntrained using the SCAFFOLD algorithm. Meanwhile,": "labeling. We further improve the quality of the pseudo-labeling"
        },
        {
          "tion. We set\nthe local\ntraining batch size as 16. Expressly, we": "5.2.\nSER performance",
          "model\ntrained using the SCAFFOLD algorithm. Meanwhile,": ""
        },
        {
          "tion. We set\nthe local\ntraining batch size as 16. Expressly, we": "",
          "model\ntrained using the SCAFFOLD algorithm. Meanwhile,": "by incorporating the pseudo-labeling consistency measurement."
        },
        {
          "tion. We set\nthe local\ntraining batch size as 16. Expressly, we": "Baselines The emotion prediction results of FL training on dif-",
          "model\ntrained using the SCAFFOLD algorithm. Meanwhile,": "We also implement\nthe SCAFFOLD algorithm to address the"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "7. References": "",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": "nich versatile\nand fast open-source\naudio feature\nextractor,”\nin"
        },
        {
          "7. References": "[1] M.-C. Lee, S.-Y. Chiang, S.-C. Yeh, and T.-F. Wen, “Study on",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": ""
        },
        {
          "7. References": "",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": "Proceedings of\nthe 18th ACM international conference on Mul-"
        },
        {
          "7. References": "emotion recognition and companion chatbot using deep neural",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": ""
        },
        {
          "7. References": "",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": "timedia, 2010, pp. 1459–1462."
        },
        {
          "7. References": "network,” Multimedia Tools and Applications, vol. 79, no. 27, pp.",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": ""
        },
        {
          "7. References": "19 629–19 657, 2020.",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,"
        },
        {
          "7. References": "",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": "Y\n. Y. Lin, A. T. Liu, J. Shi, X. Chang, G.-T. Lin, T.-H. Huang,"
        },
        {
          "7. References": "[2]\nS. Ramakrishnan and I. M. El Emary, “Speech emotion recogni-",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": ""
        },
        {
          "7. References": "",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": "W.-C. Tseng, K.\ntik Lee, D.-R. Liu, Z. Huang, S. Dong, S.-W."
        },
        {
          "7. References": "tion approaches in human computer interaction,” Telecommunica-",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": ""
        },
        {
          "7. References": "",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": "Li, S. Watanabe, A. Mohamed, and H. yi Lee, “SUPERB: Speech"
        },
        {
          "7. References": "tion Systems, vol. 52, no. 3, pp. 1467–1478, 2013.",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": ""
        },
        {
          "7. References": "",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": "Processing Universal PERformance Benchmark,” in Proc. Inter-"
        },
        {
          "7. References": "[3] D. Bone, C.-C. Lee, T. Chaspari,\nJ. Gibson, and S. Narayanan,",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": ""
        },
        {
          "7. References": "",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": "speech 2021, 2021, pp. 1194–1198."
        },
        {
          "7. References": "“Signal processing and machine\nlearning for mental health re-",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": ""
        },
        {
          "7. References": "",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": "[21] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsuper-"
        },
        {
          "7. References": "IEEE Signal Processing Mag-\nsearch and clinical applications,”",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": ""
        },
        {
          "7. References": "",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": "vised autoregressive model for speech representation learning,” in"
        },
        {
          "7. References": "azine, vol. 34, no. 5, pp. 189–196, September 2017.",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": ""
        },
        {
          "7. References": "",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": "Interspeech, 2019."
        },
        {
          "7. References": "[4] W. Li, Y. Zhang, and Y. Fu, “Speech emotion recognition in e-",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": ""
        },
        {
          "7. References": "learning system based on affective computing,” in Third Interna-",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": "[22] A. T. Liu, S.-W. Li, and H.-y. Lee, “Tera: Self-supervised learn-"
        },
        {
          "7. References": "tional Conference on Natural Computation (ICNC 2007), vol. 5.",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": "ing of transformer encoder representation for speech,” IEEE/ACM"
        },
        {
          "7. References": "IEEE, 2007, pp. 809–813.",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": "Transactions\non\nAudio,\nSpeech,\nand\nLanguage\nProcessing,"
        },
        {
          "7. References": "",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": "vol. 29, pp. 2351–2366, 2021."
        },
        {
          "7. References": "[5]\nS. G. Koolagudi\nand K. S. Rao,\n“Emotion\nrecognition\nfrom",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": ""
        },
        {
          "7. References": "International\njournal of\nspeech:\na review,”\nspeech technology,",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": "[23]\nS. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic"
        },
        {
          "7. References": "vol. 15, no. 2, pp. 99–117, 2012.",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": "representations with vector quantization,” 2020."
        },
        {
          "7. References": "[6] B. McMahan, E. Moore, D. Ramage, S. Hampson,\nand B. A.",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": "[24] H.-J. Chang, S.-w. Yang, and H.-y. Lee, “Distilhubert:\nSpeech"
        },
        {
          "7. References": "y Arcas,\n“Communication-efﬁcient\nlearning\nof deep networks",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": "representation learning by layer-wise distillation of hidden-unit"
        },
        {
          "7. References": "from decentralized data,” in Artiﬁcial\nintelligence and statistics.",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": "bert,” arXiv preprint arXiv:2110.01900, 2021."
        },
        {
          "7. References": "PMLR, 2017, pp. 1273–1282.",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": ""
        },
        {
          "7. References": "",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": "[25]\nT.\nFeng,\nH.\nHashemi,\nR.\nHebbar,\nM.\nAnnavaram,\nand"
        },
        {
          "7. References": "[7] H. Lin,\nJ. Lou, L. Xiong,\nand C. Shahabi,\n“Semifed:\nSemi-",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": "S. S. Narayanan,\n“Attribute\ninference\nattack\nof\nspeech\nemo-"
        },
        {
          "7. References": "supervised\nfederated\nlearning with\nconsistency\nand\npseudo-",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": "arXiv preprint\ntion recognition in federated learning settings,”"
        },
        {
          "7. References": "labeling,” arXiv preprint arXiv:2108.09412, 2021.",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": "arXiv:2112.13416, 2021."
        },
        {
          "7. References": "[8] V. Tsouvalas, T. Ozcelebi, and N. Meratnia, “Privacy-preserving",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": ""
        },
        {
          "7. References": "",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": "[26] A. Liu, Y.-A. Chung, and J. Glass, “Non-autoregressive predictive"
        },
        {
          "7. References": "speech emotion recognition\nthrough semi-supervised federated",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": ""
        },
        {
          "7. References": "",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": "coding for learning speech representations from local dependen-"
        },
        {
          "7. References": "learning,” arXiv preprint arXiv:2202.02611, 2022.",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": ""
        },
        {
          "7. References": "",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": "cies,” arXiv preprint arXiv:2011.00406, 2020."
        },
        {
          "7. References": "[9] W. Jeong, J. Yoon, E. Yang, and S. J. Hwang, “Federated semi-",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": ""
        },
        {
          "7. References": "",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": "[27] A. T. Liu, S.-w. Yang, P.-H. Chi, P.-c. Hsu, and H.-y. Lee, “Mock-"
        },
        {
          "7. References": "supervised learning with inter-client consistency & disjoint\nlearn-",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": ""
        },
        {
          "7. References": "",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": "ingjay: Unsupervised speech representation learning with deep"
        },
        {
          "7. References": "ing,” arXiv preprint arXiv:2006.12097, 2020.",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": ""
        },
        {
          "7. References": "",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": "bidirectional\ntransformer encoders,” ICASSP 2020 - 2020 IEEE"
        },
        {
          "7. References": "[10]\nL. Zhu and S. Han, “Deep leakage from gradients,” in Federated",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": "International Conference on Acoustics, Speech and Signal Pro-"
        },
        {
          "7. References": "learning.\nSpringer, 2020, pp. 17–31.",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": "cessing (ICASSP), May 2020."
        },
        {
          "7. References": "[11] K. Sohn, D. Berthelot, N. Carlini, Z. Zhang, H. Zhang, C. A. Raf-",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": ""
        },
        {
          "7. References": "",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": "[28] A. Satt, S. Rozenberg, and R. Hoory, “Efﬁcient emotion recogni-"
        },
        {
          "7. References": "fel, E. D. Cubuk, A. Kurakin, and C.-L. Li, “Fixmatch: Simpli-",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": ""
        },
        {
          "7. References": "",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": "tion from speech using deep learning on spectrograms.” in Inter-"
        },
        {
          "7. References": "fying semi-supervised learning with consistency and conﬁdence,”",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": ""
        },
        {
          "7. References": "",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": "speech, 2017, pp. 1089–1093."
        },
        {
          "7. References": "Advances in Neural Information Processing Systems, vol. 33, pp.",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": ""
        },
        {
          "7. References": "",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": "[29] G. Ramet,\nP. N. Garner, M. Baeriswyl,\nand A.\nLazaridis,"
        },
        {
          "7. References": "596–608, 2020.",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": ""
        },
        {
          "7. References": "",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": "“Context-aware attention mechanism for speech emotion recog-"
        },
        {
          "7. References": "[12]\nP. Li, D. Li, W. Li, S. Gong, Y. Fu, and T. M. Hospedales, “A",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": ""
        },
        {
          "7. References": "",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": "nition,”\nin 2018 IEEE Spoken Language Technology Workshop"
        },
        {
          "7. References": "simple feature augmentation for domain generalization,” in Pro-",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": ""
        },
        {
          "7. References": "",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": "(SLT).\nIEEE, 2018, pp. 126–131."
        },
        {
          "7. References": "ceedings of the IEEE/CVF International Conference on Computer",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": ""
        },
        {
          "7. References": "Vision, 2021, pp. 8886–8895.",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": ""
        },
        {
          "7. References": "[13] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower,",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": ""
        },
        {
          "7. References": "S. Kim, J. N. Chang, S. Lee, and S. S. Narayanan, “IEMOCAP:",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": ""
        },
        {
          "7. References": "Interactive emotional dyadic motion capture database,” Language",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": ""
        },
        {
          "7. References": "resources and evaluation, vol. 42, no. 4, pp. 335–359, 2008.",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": ""
        },
        {
          "7. References": "[14] C. Busso,\nS.\nParthasarathy, A. Burmania, M. AbdelWahab,",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": ""
        },
        {
          "7. References": "N. Sadoughi, and E. M. Provost, “Msp-improv: An acted corpus",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": ""
        },
        {
          "7. References": "of dyadic interactions to study emotion perception,” IEEE Trans-",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": ""
        },
        {
          "7. References": "actions on Affective Computing, vol. 8, no. 1, pp. 67–80, 2016.",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": ""
        },
        {
          "7. References": "[15] Y. Zhang, J. Du, Z. Wang, J. Zhang, and Y. Tu, “Attention based",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": ""
        },
        {
          "7. References": "fully convolutional network for speech emotion recognition,” in",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": ""
        },
        {
          "7. References": "2018 Asia-Paciﬁc Signal and Information Processing Association",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": ""
        },
        {
          "7. References": "Annual Summit and Conference (APSIPA ASC).\nIEEE, 2018, pp.",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": ""
        },
        {
          "7. References": "1771–1775.",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": ""
        },
        {
          "7. References": "[16] B. Xiong, H. Fan, K. Grauman, and C. Feichtenhofer, “Multiview",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": ""
        },
        {
          "7. References": "pseudo-labeling for semi-supervised learning from video,” in Pro-",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": ""
        },
        {
          "7. References": "ceedings of the IEEE/CVF International Conference on Computer",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": ""
        },
        {
          "7. References": "Vision, 2021, pp. 7209–7219.",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": ""
        },
        {
          "7. References": "[17] M. N. Rizve, K. Duarte, Y. S. Rawat,\nand M. Shah,\n“In de-",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": ""
        },
        {
          "7. References": "fense of pseudo-labeling: An uncertainty-aware pseudo-label se-",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": ""
        },
        {
          "7. References": "lection framework for semi-supervised learning,” arXiv preprint",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": ""
        },
        {
          "7. References": "arXiv:2101.06329, 2021.",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": ""
        },
        {
          "7. References": "[18]\nS. P. Karimireddy, S. Kale, M. Mohri, S. Reddi, S. Stich,\nand",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": ""
        },
        {
          "7. References": "A. T. Suresh, “Scaffold: Stochastic controlled averaging for feder-",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": ""
        },
        {
          "7. References": "ated learning,” in International Conference on Machine Learning.",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": ""
        },
        {
          "7. References": "PMLR, 2020, pp. 5132–5143.",
          "[19]\nF. Eyben, M. W¨ollmer,\nand B. Schuller,\n“Opensmile:\nthe mu-": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Study on emotion recognition and companion chatbot using deep neural network",
      "authors": [
        "M.-C Lee",
        "S.-Y Chiang",
        "S.-C Yeh",
        "T.-F Wen"
      ],
      "year": "2020",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "3",
      "title": "Speech emotion recognition approaches in human computer interaction",
      "authors": [
        "S Ramakrishnan",
        "I Emary"
      ],
      "year": "2013",
      "venue": "Telecommunication Systems"
    },
    {
      "citation_id": "4",
      "title": "Signal processing and machine learning for mental health research and clinical applications",
      "authors": [
        "D Bone",
        "C.-C Lee",
        "T Chaspari",
        "J Gibson",
        "S Narayanan"
      ],
      "year": "2017",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "5",
      "title": "Speech emotion recognition in elearning system based on affective computing",
      "authors": [
        "W Li",
        "Y Zhang",
        "Y Fu"
      ],
      "year": "2007",
      "venue": "Third International Conference on Natural Computation (ICNC 2007)"
    },
    {
      "citation_id": "6",
      "title": "Emotion recognition from speech: a review",
      "authors": [
        "S Koolagudi",
        "K Rao"
      ],
      "year": "2012",
      "venue": "International journal of speech technology"
    },
    {
      "citation_id": "7",
      "title": "Communication-efficient learning of deep networks from decentralized data",
      "authors": [
        "B Mcmahan",
        "E Moore",
        "D Ramage",
        "S Hampson",
        "B Arcas"
      ],
      "year": "2017",
      "venue": "Artificial intelligence and statistics"
    },
    {
      "citation_id": "8",
      "title": "Semifed: Semisupervised federated learning with consistency and pseudolabeling",
      "authors": [
        "H Lin",
        "J Lou",
        "L Xiong",
        "C Shahabi"
      ],
      "year": "2021",
      "venue": "Semifed: Semisupervised federated learning with consistency and pseudolabeling",
      "arxiv": "arXiv:2108.09412"
    },
    {
      "citation_id": "9",
      "title": "Privacy-preserving speech emotion recognition through semi-supervised federated learning",
      "authors": [
        "V Tsouvalas",
        "T Ozcelebi",
        "N Meratnia"
      ],
      "year": "2022",
      "venue": "Privacy-preserving speech emotion recognition through semi-supervised federated learning",
      "arxiv": "arXiv:2202.02611"
    },
    {
      "citation_id": "10",
      "title": "Federated semisupervised learning with inter-client consistency & disjoint learning",
      "authors": [
        "W Jeong",
        "J Yoon",
        "E Yang",
        "S Hwang"
      ],
      "year": "2020",
      "venue": "Federated semisupervised learning with inter-client consistency & disjoint learning",
      "arxiv": "arXiv:2006.12097"
    },
    {
      "citation_id": "11",
      "title": "Deep leakage from gradients",
      "authors": [
        "L Zhu",
        "S Han"
      ],
      "year": "2020",
      "venue": "Deep leakage from gradients"
    },
    {
      "citation_id": "12",
      "title": "Fixmatch: Simplifying semi-supervised learning with consistency and confidence",
      "authors": [
        "K Sohn",
        "D Berthelot",
        "N Carlini",
        "Z Zhang",
        "H Zhang",
        "C Raffel",
        "E Cubuk",
        "A Kurakin",
        "C.-L Li"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "13",
      "title": "A simple feature augmentation for domain generalization",
      "authors": [
        "P Li",
        "D Li",
        "W Li",
        "S Gong",
        "Y Fu",
        "T Hospedales"
      ],
      "year": "2021",
      "venue": "A simple feature augmentation for domain generalization"
    },
    {
      "citation_id": "14",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "15",
      "title": "Msp-improv: An acted corpus of dyadic interactions to study emotion perception",
      "authors": [
        "C Busso",
        "S Parthasarathy",
        "A Burmania",
        "M Abdelwahab",
        "N Sadoughi",
        "E Provost"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "16",
      "title": "Attention based fully convolutional network for speech emotion recognition",
      "authors": [
        "Y Zhang",
        "J Du",
        "Z Wang",
        "J Zhang",
        "Y Tu"
      ],
      "year": "2018",
      "venue": "Attention based fully convolutional network for speech emotion recognition"
    },
    {
      "citation_id": "17",
      "title": "Asia-Pacific Signal and Information Processing Association Annual Summit and Conference",
      "year": "2018",
      "venue": "Asia-Pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "18",
      "title": "Multiview pseudo-labeling for semi-supervised learning from video",
      "authors": [
        "B Xiong",
        "H Fan",
        "K Grauman",
        "C Feichtenhofer"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "19",
      "title": "In defense of pseudo-labeling: An uncertainty-aware pseudo-label selection framework for semi-supervised learning",
      "authors": [
        "M Rizve",
        "K Duarte",
        "Y Rawat",
        "M Shah"
      ],
      "year": "2021",
      "venue": "In defense of pseudo-labeling: An uncertainty-aware pseudo-label selection framework for semi-supervised learning",
      "arxiv": "arXiv:2101.06329"
    },
    {
      "citation_id": "20",
      "title": "Scaffold: Stochastic controlled averaging for federated learning",
      "authors": [
        "S Karimireddy",
        "S Kale",
        "M Mohri",
        "S Reddi",
        "S Stich",
        "A Suresh"
      ],
      "year": "2020",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "21",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "22",
      "title": "",
      "authors": [
        "S Yang",
        "P.-H Chi",
        "Y.-S Chuang",
        "C.-I Lai",
        "K Lakhotia",
        "Y Lin",
        "A Liu",
        "J Shi",
        "X Chang",
        "G.-T Lin",
        "T.-H Huang",
        "W.-C Tseng",
        "K Lee",
        "D.-R Liu",
        "Z Huang",
        "S Dong"
      ],
      "venue": ""
    },
    {
      "citation_id": "23",
      "title": "SUPERB: Speech Processing Universal PERformance Benchmark",
      "authors": [
        "S Li",
        "A Watanabe",
        "H Mohamed",
        "Yi Lee"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "24",
      "title": "An unsupervised autoregressive model for speech representation learning",
      "authors": [
        "Y.-A Chung",
        "W.-N Hsu",
        "H Tang",
        "J Glass"
      ],
      "year": "2019",
      "venue": "An unsupervised autoregressive model for speech representation learning"
    },
    {
      "citation_id": "25",
      "title": "Tera: Self-supervised learning of transformer encoder representation for speech",
      "authors": [
        "A Liu",
        "S.-W Li",
        "H.-Y Lee"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "26",
      "title": "Decoar 2.0: Deep contextualized acoustic representations with vector quantization",
      "authors": [
        "S Ling",
        "Y Liu"
      ],
      "year": "2020",
      "venue": "Decoar 2.0: Deep contextualized acoustic representations with vector quantization"
    },
    {
      "citation_id": "27",
      "title": "Distilhubert: Speech representation learning by layer-wise distillation of hidden-unit bert",
      "authors": [
        "H.-J Chang",
        "S.-W Yang",
        "H.-Y Lee"
      ],
      "year": "2021",
      "venue": "Distilhubert: Speech representation learning by layer-wise distillation of hidden-unit bert",
      "arxiv": "arXiv:2110.01900"
    },
    {
      "citation_id": "28",
      "title": "Attribute inference attack of speech emotion recognition in federated learning settings",
      "authors": [
        "T Feng",
        "H Hashemi",
        "R Hebbar",
        "M Annavaram",
        "S Narayanan"
      ],
      "year": "2021",
      "venue": "Attribute inference attack of speech emotion recognition in federated learning settings",
      "arxiv": "arXiv:2112.13416"
    },
    {
      "citation_id": "29",
      "title": "Non-autoregressive predictive coding for learning speech representations from local dependencies",
      "authors": [
        "A Liu",
        "Y.-A Chung",
        "J Glass"
      ],
      "year": "2020",
      "venue": "Non-autoregressive predictive coding for learning speech representations from local dependencies",
      "arxiv": "arXiv:2011.00406"
    },
    {
      "citation_id": "30",
      "title": "Mockingjay: Unsupervised speech representation learning with deep bidirectional transformer encoders",
      "authors": [
        "A Liu",
        "S -W. Yang",
        "P.-H Chi",
        "P.-C Hsu",
        "H.-Y Lee"
      ],
      "year": "2020",
      "venue": "ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "31",
      "title": "Efficient emotion recognition from speech using deep learning on spectrograms",
      "authors": [
        "A Satt",
        "S Rozenberg",
        "R Hoory"
      ],
      "year": "2017",
      "venue": "Interspeech"
    },
    {
      "citation_id": "32",
      "title": "Context-aware attention mechanism for speech emotion recognition",
      "authors": [
        "G Ramet",
        "P Garner",
        "M Baeriswyl",
        "A Lazaridis"
      ],
      "year": "2018",
      "venue": "2018 IEEE Spoken Language Technology Workshop (SLT)"
    }
  ]
}