{
  "paper_id": "2105.01502v2",
  "title": "Valence-Arousal Estimation On Affwild2 Dataset",
  "published": "2021-05-04T14:00:07Z",
  "authors": [
    "I-Hsuan Li"
  ],
  "keywords": [
    "to select the videos was \"monologue.\" Kollias et al.  [8]  built a large-scale Aff-Wild dataset",
    "collected from Youtube",
    "and proposed deep convolutional and recurrent neural architecture"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this work, we describe our method for tackling the valence-arousal estimation challenge from ABAW FG-2020 Competition. The competition organizers provide an in-thewild Aff-Wild2 dataset for participants to analyze affective behavior in real-life settings. We use MIMAMO Net [1] model to achieve information about micro-motion and macro-motion for improving video emotion recognition and achieve Concordance Correlation Coefficient (CCC) of 0.415 and 0.511 for valence and arousal on the reselected validation set. M. Shell was with the Department of Electrical and Computer Engineering,",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "P EOPLE study facial expression recognition research for a long history, but still have some challenges to be addressed, such as in-the-wild dataset improvements. No inthe-wild dataset contains complete and various sets of labels for human emotion estimation, due to the high cost and time requirement. And this causes the limitation of multi-task method progression and applications in real life. Recently, to tackle such problems, Kollias et al.  [2] ,  [3] ,  [4] ,  [5] ,  [6]  held Affective Behavior Analysis in-the-wild (ABAW) FG-2020 Competition and built the large-scale Aff-Wild2 dataset, which includes annotations of valence/arousal value, action unit (AU), and facial expression for three different recognition tasks. Valence represents how positive the person is while arousal describes how active the person is. AUs are the basic actions of individuals or groups of muscles for portraying emotions. As for facial expression, it classifies into seven categories, neutral, anger, disgust, fear, happiness, sadness, and surprise.\n\nThe challenges for ABAW FG-2020 Competition include valence-arousal estimation, facial action unit detection, and expression classification. We focus on valence-arousal estimation by using MIMAMO Net, which was proposed by Deng et al  [1] . The model, shown in Fig.  1 , gets state-of-the-art performance on the OMG  [7]  and Aff-Wild  [8]  dataset. It uses spatial-temporal feature learning to capture information about micro-motion and macro-motion and combine them by GRU network to improve video emotion recognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "In recent years, most of the existing research for facial expression recognition focused on valence-arousal estimation, facial action unit detection, and expression classification. We will introduce the latest related work of valence-arousal estimation study.\n\nMany data are in laboratory settings. However, models that perform well on controlled conditions don't necessarily work well on uncontrolled ones. The ideal is still far from reality. Therefore, in-the-wild datasets come to exist. Kossaif et al.  [9]  proposed a new dataset called AFEW-VA and found that geometric features performed well no matter what settings were. But it was unuseful for dynamic architecture since some of the clips in the dataset were too short to explore information between frame and frame. Barros et al.  [7]  proposed OMG dataset, collected from YouTube in real-world settings. The main keyword to select the videos was \"monologue.\" Kollias et al.  [8]  built a large-scale Aff-Wild dataset, collected from Youtube, and proposed deep convolutional and recurrent neural architecture, AffWildNet. A CNN extracted features while an RNN aimed to capture temporal information. Furthermore, their works not only got high performance on dimensional aspects but also for expression classification.\n\nChang et al.  [10]  proposed an integrated deep learning framework that used the concept of applying the information of facial action unit detection to estimate valence-arousal intensity. They had shown that exploring the relationship between AUs and V-A was helpful for V-A research. Pan et al.  [11]  proposed a two-stream network to utilize effective facial features. The model contained CNN and LSTM. For temporal stream, the former extracted temporal features; the latter resolved the temporal relation between frames. For spatial stream similar to temporal one, the former extracted spatial features; the latter analyzed the spatial association between frames. Kim et al.  [12]  tackled regression problems with adversarial learning, which enabled the model to better understand complex emotion and achieved person-independent facial expression recognition. Also, they proposed a contrastive loss function and improved the performance effectively. This study proved the potential of adversarial learning instead of conventional methods on emotion recognition.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Iii. Methodology & Experiments",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Dataset",
      "text": "We only use the large-scale in-the-wild Aff-Wild2 dataset for our experiments since Deng et al.  [13]  found that data balancing technique doesn't improve the performance of regression problem. This dataset contains 558 videos with framelevel annotations for valence-arousal estimation, facial action unit detection, and expression classification tasks. Our model focuses on estimating valence and arousal values, which take values in -1 to 1 and -5 represent no annotated values. In the VA set, there are 422 subjects with 1,932,935 images in the training and validation and 139 subjects with 714,986 Fig.  1 . MIMAMO Net  [1]  images in the test. These cropped and aligned images were all provided by ABAW FG-2020 Competition organizers.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "B. Model",
      "text": "In this research, we build our model based on MIMAMO Net  [1] , which uses a two-stream network with the GRU model to integrating information about micro-motion and macromotion. The inputs of the architecture separate into a spatial stream and temporal stream. It uses the pre-trained ResNet50 model to extract features and then fed pooling feature vector into fully connected layers to get the final feature vector in the spatial stream. While in the temporal stream, it utilizes time series of phase difference images to obtain the relationship between frames and then fed into the CNN network. The output of the two-stream network connects to the GRU model, which combines the information of the whole video to achieve frame-level predictions of valence and arousal values.\n\nTo measure the agreement between the outputs of the model and the ground truth, it uses Concordance Correlation Coefficient (CCC) metrics as follow:\n\nx + σ 2 y + (µ x -µ y ) 2 where x and y are the predictions and annotations, µ x and µ y are the mean values, σ 2\n\nx and σ 2 y are their variances, and ρ is the correlation coefficient.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Data Pre-Processing",
      "text": "We merge the training set and validation set and use the cross-validation method to acquire a more accurate estimate of model prediction performance. To apply the Aff-Wild2 dataset, we remove unannotated frames from the beginning and let the remaining ones match its annotation values. However, when testing, this approach may cause problems, such as missing data. We address those missing frames by using two different methods to deal with two situations. If disregarding frames are at the beginning of the video, we label -5 as the prediction. And if removed frames are not the case of above, we take the predicted value of the previous frame as its estimation.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "D. Results",
      "text": "Our valence CCC and arousal CCC are 0.415 and 0.511 on the reselected validation set. We find that the performance on arousal is better than the performance on valence. Since arousal describes how active the person is, it should be more related to facial motion than valence. Therefore, it is reasonable that we get higher accuracy on arousal.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iv. Conclusion",
      "text": "We have conducted valence-arousal estimation in Affwild2 dataset by using MIMAMO Net  [1] . In the future, we plan to improve the MIMAMO Net to achieve better result.",
      "page_start": 2,
      "page_end": 2
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , gets state-of-the-art",
      "page": 1
    },
    {
      "caption": "Figure 1: MIMAMO Net [1]",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "I-Hsuan Li": "Abstract—In this work, we describe our method for tackling"
        },
        {
          "I-Hsuan Li": "the valence-arousal\nestimation challenge\nfrom ABAW FG-2020"
        },
        {
          "I-Hsuan Li": ""
        },
        {
          "I-Hsuan Li": "Competition.\nThe\ncompetition\norganizers\nprovide\nan\nin-the-"
        },
        {
          "I-Hsuan Li": ""
        },
        {
          "I-Hsuan Li": "wild Aff-Wild2\ndataset\nfor\nparticipants\nto\nanalyze\naffective"
        },
        {
          "I-Hsuan Li": ""
        },
        {
          "I-Hsuan Li": "behavior in real-life settings. We use MIMAMO Net [1] model to"
        },
        {
          "I-Hsuan Li": "achieve information about micro-motion and macro-motion for"
        },
        {
          "I-Hsuan Li": "improving video emotion recognition and achieve Concordance"
        },
        {
          "I-Hsuan Li": "Correlation Coefﬁcient (CCC) of 0.415 and 0.511 for valence and"
        },
        {
          "I-Hsuan Li": ""
        },
        {
          "I-Hsuan Li": "arousal on the reselected validation set."
        },
        {
          "I-Hsuan Li": ""
        },
        {
          "I-Hsuan Li": ""
        },
        {
          "I-Hsuan Li": "I.\nINTRODUCTION"
        },
        {
          "I-Hsuan Li": ""
        },
        {
          "I-Hsuan Li": "P EOPLE study facial expression recognition research for"
        },
        {
          "I-Hsuan Li": ""
        },
        {
          "I-Hsuan Li": "addressed,\nsuch as\nin-the-wild dataset\nimprovements. No in-"
        },
        {
          "I-Hsuan Li": ""
        },
        {
          "I-Hsuan Li": "the-wild dataset contains complete and various sets of\nlabels"
        },
        {
          "I-Hsuan Li": ""
        },
        {
          "I-Hsuan Li": "for\nhuman\nemotion\nestimation,\ndue\nto\nthe\nhigh\ncost\nand"
        },
        {
          "I-Hsuan Li": ""
        },
        {
          "I-Hsuan Li": "time requirement. And this causes the limitation of multi-task"
        },
        {
          "I-Hsuan Li": ""
        },
        {
          "I-Hsuan Li": "method\nprogression\nand\napplications\nin\nreal\nlife. Recently,"
        },
        {
          "I-Hsuan Li": ""
        },
        {
          "I-Hsuan Li": "to tackle such problems, Kollias et al.\n[2],\n[3],\n[4],\n[5],\n[6]"
        },
        {
          "I-Hsuan Li": ""
        },
        {
          "I-Hsuan Li": "held Affective Behavior Analysis\nin-the-wild (ABAW) FG-"
        },
        {
          "I-Hsuan Li": ""
        },
        {
          "I-Hsuan Li": "2020 Competition and built\nthe large-scale Aff-Wild2 dataset,"
        },
        {
          "I-Hsuan Li": ""
        },
        {
          "I-Hsuan Li": "which includes\nannotations of valence/arousal value,\naction"
        },
        {
          "I-Hsuan Li": ""
        },
        {
          "I-Hsuan Li": "unit (AU), and facial expression for three different recognition"
        },
        {
          "I-Hsuan Li": ""
        },
        {
          "I-Hsuan Li": "tasks. Valence\nrepresents\nhow positive\nthe\nperson\nis while"
        },
        {
          "I-Hsuan Li": ""
        },
        {
          "I-Hsuan Li": "arousal describes how active the person is. AUs are the basic"
        },
        {
          "I-Hsuan Li": ""
        },
        {
          "I-Hsuan Li": "actions\nof\nindividuals\nor\ngroups\nof muscles\nfor\nportraying"
        },
        {
          "I-Hsuan Li": ""
        },
        {
          "I-Hsuan Li": "emotions. As\nfor\nfacial\nexpression,\nit\nclassiﬁes\ninto\nseven"
        },
        {
          "I-Hsuan Li": ""
        },
        {
          "I-Hsuan Li": "categories, neutral, anger, disgust, fear, happiness, sadness, and"
        },
        {
          "I-Hsuan Li": ""
        },
        {
          "I-Hsuan Li": "surprise."
        },
        {
          "I-Hsuan Li": ""
        },
        {
          "I-Hsuan Li": "The\nchallenges\nfor ABAW FG-2020 Competition include"
        },
        {
          "I-Hsuan Li": ""
        },
        {
          "I-Hsuan Li": "valence-arousal\nestimation,\nfacial\naction unit detection,\nand"
        },
        {
          "I-Hsuan Li": ""
        },
        {
          "I-Hsuan Li": "expression classiﬁcation. We focus on valence-arousal estima-"
        },
        {
          "I-Hsuan Li": ""
        },
        {
          "I-Hsuan Li": "tion by using MIMAMO Net, which was proposed by Deng"
        },
        {
          "I-Hsuan Li": ""
        },
        {
          "I-Hsuan Li": "et\nal\n[1]. The model,\nshown in Fig. 1, gets\nstate-of-the-art"
        },
        {
          "I-Hsuan Li": ""
        },
        {
          "I-Hsuan Li": "performance on the OMG [7] and Aff-Wild [8] dataset. It uses"
        },
        {
          "I-Hsuan Li": ""
        },
        {
          "I-Hsuan Li": "spatial-temporal feature learning to capture information about"
        },
        {
          "I-Hsuan Li": "micro-motion and macro-motion and combine them by GRU"
        },
        {
          "I-Hsuan Li": "network to improve video emotion recognition."
        },
        {
          "I-Hsuan Li": ""
        },
        {
          "I-Hsuan Li": "II. RELATED WORK"
        },
        {
          "I-Hsuan Li": ""
        },
        {
          "I-Hsuan Li": "In\nrecent\nyears, most\nof\nthe\nexisting\nresearch\nfor\nfacial"
        },
        {
          "I-Hsuan Li": ""
        },
        {
          "I-Hsuan Li": "expression recognition focused on valence-arousal estimation,"
        },
        {
          "I-Hsuan Li": ""
        },
        {
          "I-Hsuan Li": "facial action unit detection, and expression classiﬁcation. We"
        },
        {
          "I-Hsuan Li": ""
        },
        {
          "I-Hsuan Li": "will\nintroduce the latest\nrelated work of valence-arousal esti-"
        },
        {
          "I-Hsuan Li": ""
        },
        {
          "I-Hsuan Li": "mation study."
        },
        {
          "I-Hsuan Li": ""
        },
        {
          "I-Hsuan Li": "M. Shell was with the Department of Electrical and Computer Engineering,"
        },
        {
          "I-Hsuan Li": "Georgia\nInstitute\nof Technology, Atlanta, GA,\n30332 USA e-mail:\n(see"
        },
        {
          "I-Hsuan Li": ""
        },
        {
          "I-Hsuan Li": "http://www.michaelshell.org/contact.html)."
        },
        {
          "I-Hsuan Li": ""
        },
        {
          "I-Hsuan Li": "J. Doe and J. Doe are with Anonymous University."
        },
        {
          "I-Hsuan Li": "Manuscript\nreceived April 19, 2005;\nrevised August 26, 2015."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "frame-level predictions of valence and arousal values.": "To measure\nthe\nagreement\nbetween\nthe\noutputs\nof\nthe",
          "improve the MIMAMO Net": "",
          "to achieve better\nresult.": ""
        },
        {
          "frame-level predictions of valence and arousal values.": "model and the ground truth,\nit uses Concordance Correlation",
          "improve the MIMAMO Net": "",
          "to achieve better\nresult.": ""
        },
        {
          "frame-level predictions of valence and arousal values.": "",
          "improve the MIMAMO Net": "",
          "to achieve better\nresult.": "REFERENCES"
        },
        {
          "frame-level predictions of valence and arousal values.": "Coefﬁcient\n(CCC) metrics as follow:",
          "improve the MIMAMO Net": "",
          "to achieve better\nresult.": ""
        },
        {
          "frame-level predictions of valence and arousal values.": "",
          "improve the MIMAMO Net": "",
          "to achieve better\nresult.": "[1] D. Deng, Z. Chen, Y. Zhou, and B. Shi, “Mimamo net: Integrating micro-"
        },
        {
          "frame-level predictions of valence and arousal values.": "2ρσxσy",
          "improve the MIMAMO Net": "",
          "to achieve better\nresult.": ""
        },
        {
          "frame-level predictions of valence and arousal values.": "",
          "improve the MIMAMO Net": "",
          "to achieve better\nresult.": "and macro-motion for video emotion recognition,” in Proceedings of"
        },
        {
          "frame-level predictions of valence and arousal values.": "CCC =",
          "improve the MIMAMO Net": "",
          "to achieve better\nresult.": ""
        },
        {
          "frame-level predictions of valence and arousal values.": "",
          "improve the MIMAMO Net": "",
          "to achieve better\nresult.": "AAAI Conference on Artiﬁcial\nIntelligence, vol. 34, no. 03, 2020, pp."
        },
        {
          "frame-level predictions of valence and arousal values.": "x + σ2\ny + (µx − µy)2",
          "improve the MIMAMO Net": "",
          "to achieve better\nresult.": ""
        },
        {
          "frame-level predictions of valence and arousal values.": "",
          "improve the MIMAMO Net": "",
          "to achieve better\nresult.": "2621–2628."
        },
        {
          "frame-level predictions of valence and arousal values.": "where x and y are the predictions and annotations, µx and µy",
          "improve the MIMAMO Net": "",
          "to achieve better\nresult.": "[2] D. Kollias, A. Schulc, E. Hajiyev, and S. Zafeiriou, “Analysing affective"
        },
        {
          "frame-level predictions of valence and arousal values.": "",
          "improve the MIMAMO Net": "",
          "to achieve better\nresult.": "2020\n15th\nIEEE\nbehavior\nin\nthe\nﬁrst\nabaw 2020\ncompetition,”\nin"
        },
        {
          "frame-level predictions of valence and arousal values.": "are the mean values, σ2\nare their variances, and ρ is\nx and σ2",
          "improve the MIMAMO Net": "",
          "to achieve better\nresult.": ""
        },
        {
          "frame-level predictions of valence and arousal values.": "",
          "improve the MIMAMO Net": "",
          "to achieve better\nresult.": "International Conference on Automatic Face and Gesture Recognition"
        },
        {
          "frame-level predictions of valence and arousal values.": "the correlation coefﬁcient.",
          "improve the MIMAMO Net": "",
          "to achieve better\nresult.": "(FG 2020)(FG), pp. 794–800."
        },
        {
          "frame-level predictions of valence and arousal values.": "",
          "improve the MIMAMO Net": "[3] D. Kollias",
          "to achieve better\nresult.": "and S. Zafeiriou,\n“Expression,\naffect,\naction\nunit\nrecog-"
        },
        {
          "frame-level predictions of valence and arousal values.": "",
          "improve the MIMAMO Net": "",
          "to achieve better\nresult.": "arXiv\npreprint\nnition: Aff-wild2, multi-task\nlearning\nand\narcface,”"
        },
        {
          "frame-level predictions of valence and arousal values.": "C. Data Pre-processing",
          "improve the MIMAMO Net": "",
          "to achieve better\nresult.": "arXiv:1910.04855, 2019."
        },
        {
          "frame-level predictions of valence and arousal values.": "",
          "improve the MIMAMO Net": "",
          "to achieve better\nresult.": "[4] D. Kollias, V. Sharmanska, and S. Zafeiriou, “Face behavior a la carte:"
        },
        {
          "frame-level predictions of valence and arousal values.": "We merge\nthe training set\nand validation set and use\nthe",
          "improve the MIMAMO Net": "",
          "to achieve better\nresult.": ""
        },
        {
          "frame-level predictions of valence and arousal values.": "",
          "improve the MIMAMO Net": "",
          "to achieve better\nresult.": "Expressions, affect and action units in a single network,” arXiv preprint"
        },
        {
          "frame-level predictions of valence and arousal values.": "cross-validation method to acquire a more accurate estimate of",
          "improve the MIMAMO Net": "",
          "to achieve better\nresult.": "arXiv:1910.11111, 2019."
        },
        {
          "frame-level predictions of valence and arousal values.": "",
          "improve the MIMAMO Net": "[5]",
          "to achieve better\nresult.": "S. Zafeiriou, D. Kollias, M. A. Nicolaou, A. Papaioannou, G. Zhao,"
        },
        {
          "frame-level predictions of valence and arousal values.": "model prediction performance. To apply the Aff-Wild2 dataset,",
          "improve the MIMAMO Net": "",
          "to achieve better\nresult.": ""
        },
        {
          "frame-level predictions of valence and arousal values.": "",
          "improve the MIMAMO Net": "",
          "to achieve better\nresult.": "and I. Kotsia, “Aff-wild: Valence and arousal ‘in-the-wild’challenge,” in"
        },
        {
          "frame-level predictions of valence and arousal values.": "we remove unannotated frames from the beginning and let\nthe",
          "improve the MIMAMO Net": "",
          "to achieve better\nresult.": ""
        },
        {
          "frame-level predictions of valence and arousal values.": "",
          "improve the MIMAMO Net": "",
          "to achieve better\nresult.": "Computer Vision and Pattern Recognition Workshops\n(CVPRW), 2017"
        },
        {
          "frame-level predictions of valence and arousal values.": "remaining ones match its annotation values. However, when",
          "improve the MIMAMO Net": "",
          "to achieve better\nresult.": "IEEE Conference on.\nIEEE, 2017, pp. 1980–1987."
        },
        {
          "frame-level predictions of valence and arousal values.": "",
          "improve the MIMAMO Net": "[6] D. Kollias, M. A. Nicolaou,",
          "to achieve better\nresult.": "I. Kotsia, G. Zhao,\nand\nS. Zafeiriou,"
        },
        {
          "frame-level predictions of valence and arousal values.": "testing,\nthis approach may cause problems,\nsuch as missing",
          "improve the MIMAMO Net": "",
          "to achieve better\nresult.": ""
        },
        {
          "frame-level predictions of valence and arousal values.": "",
          "improve the MIMAMO Net": "",
          "to achieve better\nresult.": "“Recognition\nof\naffect\nin\nthe wild\nusing\ndeep\nneural\nnetworks,”\nin"
        },
        {
          "frame-level predictions of valence and arousal values.": "data. We address those missing frames by using two different",
          "improve the MIMAMO Net": "",
          "to achieve better\nresult.": ""
        },
        {
          "frame-level predictions of valence and arousal values.": "",
          "improve the MIMAMO Net": "",
          "to achieve better\nresult.": "Computer Vision and Pattern Recognition Workshops\n(CVPRW), 2017"
        },
        {
          "frame-level predictions of valence and arousal values.": "methods to deal with two situations. If disregarding frames are",
          "improve the MIMAMO Net": "",
          "to achieve better\nresult.": "IEEE Conference on.\nIEEE, 2017, pp. 1972–1979."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 1. MIMAMO Net\n[1]": "images in the test. These cropped and aligned images were all"
        },
        {
          "Fig. 1. MIMAMO Net\n[1]": "provided by ABAW FG-2020 Competition organizers."
        },
        {
          "Fig. 1. MIMAMO Net\n[1]": ""
        },
        {
          "Fig. 1. MIMAMO Net\n[1]": "B. Model"
        },
        {
          "Fig. 1. MIMAMO Net\n[1]": ""
        },
        {
          "Fig. 1. MIMAMO Net\n[1]": "In this\nresearch, we build our model based on MIMAMO"
        },
        {
          "Fig. 1. MIMAMO Net\n[1]": "Net [1], which uses a two-stream network with the GRU model"
        },
        {
          "Fig. 1. MIMAMO Net\n[1]": ""
        },
        {
          "Fig. 1. MIMAMO Net\n[1]": "to\nintegrating\ninformation\nabout micro-motion\nand macro-"
        },
        {
          "Fig. 1. MIMAMO Net\n[1]": ""
        },
        {
          "Fig. 1. MIMAMO Net\n[1]": "motion. The inputs of\nthe architecture separate into a spatial"
        },
        {
          "Fig. 1. MIMAMO Net\n[1]": ""
        },
        {
          "Fig. 1. MIMAMO Net\n[1]": "stream and temporal stream.\nIt uses the pre-trained ResNet50"
        },
        {
          "Fig. 1. MIMAMO Net\n[1]": ""
        },
        {
          "Fig. 1. MIMAMO Net\n[1]": "model\nto extract\nfeatures and then fed pooling feature vector"
        },
        {
          "Fig. 1. MIMAMO Net\n[1]": ""
        },
        {
          "Fig. 1. MIMAMO Net\n[1]": "into fully connected layers to get the ﬁnal feature vector in the"
        },
        {
          "Fig. 1. MIMAMO Net\n[1]": ""
        },
        {
          "Fig. 1. MIMAMO Net\n[1]": "spatial\nstream. While in the temporal\nstream,\nit utilizes\ntime"
        },
        {
          "Fig. 1. MIMAMO Net\n[1]": "series of phase difference\nimages\nto obtain the\nrelationship"
        },
        {
          "Fig. 1. MIMAMO Net\n[1]": ""
        },
        {
          "Fig. 1. MIMAMO Net\n[1]": "between\nframes\nand\nthen\nfed\ninto\nthe CNN network. The"
        },
        {
          "Fig. 1. MIMAMO Net\n[1]": "output of the two-stream network connects to the GRU model,"
        },
        {
          "Fig. 1. MIMAMO Net\n[1]": "which combines the information of the whole video to achieve"
        },
        {
          "Fig. 1. MIMAMO Net\n[1]": "frame-level predictions of valence and arousal values."
        },
        {
          "Fig. 1. MIMAMO Net\n[1]": "To measure\nthe\nagreement\nbetween\nthe\noutputs\nof\nthe"
        },
        {
          "Fig. 1. MIMAMO Net\n[1]": "model and the ground truth,\nit uses Concordance Correlation"
        },
        {
          "Fig. 1. MIMAMO Net\n[1]": ""
        },
        {
          "Fig. 1. MIMAMO Net\n[1]": "Coefﬁcient\n(CCC) metrics as follow:"
        },
        {
          "Fig. 1. MIMAMO Net\n[1]": ""
        },
        {
          "Fig. 1. MIMAMO Net\n[1]": "2ρσxσy"
        },
        {
          "Fig. 1. MIMAMO Net\n[1]": ""
        },
        {
          "Fig. 1. MIMAMO Net\n[1]": "CCC ="
        },
        {
          "Fig. 1. MIMAMO Net\n[1]": ""
        },
        {
          "Fig. 1. MIMAMO Net\n[1]": "x + σ2\ny + (µx − µy)2"
        },
        {
          "Fig. 1. MIMAMO Net\n[1]": ""
        },
        {
          "Fig. 1. MIMAMO Net\n[1]": "where x and y are the predictions and annotations, µx and µy"
        },
        {
          "Fig. 1. MIMAMO Net\n[1]": ""
        },
        {
          "Fig. 1. MIMAMO Net\n[1]": "are the mean values, σ2\nare their variances, and ρ is\nx and σ2"
        },
        {
          "Fig. 1. MIMAMO Net\n[1]": ""
        },
        {
          "Fig. 1. MIMAMO Net\n[1]": "the correlation coefﬁcient."
        },
        {
          "Fig. 1. MIMAMO Net\n[1]": ""
        },
        {
          "Fig. 1. MIMAMO Net\n[1]": ""
        },
        {
          "Fig. 1. MIMAMO Net\n[1]": "C. Data Pre-processing"
        },
        {
          "Fig. 1. MIMAMO Net\n[1]": ""
        },
        {
          "Fig. 1. MIMAMO Net\n[1]": "We merge\nthe training set\nand validation set and use\nthe"
        },
        {
          "Fig. 1. MIMAMO Net\n[1]": ""
        },
        {
          "Fig. 1. MIMAMO Net\n[1]": "cross-validation method to acquire a more accurate estimate of"
        },
        {
          "Fig. 1. MIMAMO Net\n[1]": ""
        },
        {
          "Fig. 1. MIMAMO Net\n[1]": "model prediction performance. To apply the Aff-Wild2 dataset,"
        },
        {
          "Fig. 1. MIMAMO Net\n[1]": ""
        },
        {
          "Fig. 1. MIMAMO Net\n[1]": "we remove unannotated frames from the beginning and let\nthe"
        },
        {
          "Fig. 1. MIMAMO Net\n[1]": ""
        },
        {
          "Fig. 1. MIMAMO Net\n[1]": "remaining ones match its annotation values. However, when"
        },
        {
          "Fig. 1. MIMAMO Net\n[1]": ""
        },
        {
          "Fig. 1. MIMAMO Net\n[1]": "testing,\nthis approach may cause problems,\nsuch as missing"
        },
        {
          "Fig. 1. MIMAMO Net\n[1]": ""
        },
        {
          "Fig. 1. MIMAMO Net\n[1]": "data. We address those missing frames by using two different"
        },
        {
          "Fig. 1. MIMAMO Net\n[1]": ""
        },
        {
          "Fig. 1. MIMAMO Net\n[1]": "methods to deal with two situations. If disregarding frames are"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[7]\nP. Barros, N. Churamani, E. Lakomkin, H. Siqueira, A. Sutherland, and": "S. Wermter, “The omg-emotion behavior dataset,” in 2018 International"
        },
        {
          "[7]\nP. Barros, N. Churamani, E. Lakomkin, H. Siqueira, A. Sutherland, and": "Joint Conference on Neural Networks (IJCNN).\nIEEE, 2018, pp. 1–7."
        },
        {
          "[7]\nP. Barros, N. Churamani, E. Lakomkin, H. Siqueira, A. Sutherland, and": "[8] D. Kollias, P. Tzirakis, M. A. Nicolaou, A. Papaioannou, G. Zhao,"
        },
        {
          "[7]\nP. Barros, N. Churamani, E. Lakomkin, H. Siqueira, A. Sutherland, and": "B. Schuller,\nI. Kotsia, and S. Zafeiriou, “Deep affect prediction in-the-"
        },
        {
          "[7]\nP. Barros, N. Churamani, E. Lakomkin, H. Siqueira, A. Sutherland, and": "wild: Aff-wild database and challenge, deep architectures, and beyond,”"
        },
        {
          "[7]\nP. Barros, N. Churamani, E. Lakomkin, H. Siqueira, A. Sutherland, and": "International Journal of Computer Vision, pp. 1–23, 2019."
        },
        {
          "[7]\nP. Barros, N. Churamani, E. Lakomkin, H. Siqueira, A. Sutherland, and": "[9]\nJ. Kossaiﬁ, G. Tzimiropoulos, S. Todorovic, and M. Pantic, “Afew-va"
        },
        {
          "[7]\nP. Barros, N. Churamani, E. Lakomkin, H. Siqueira, A. Sutherland, and": "Image\nand\ndatabase\nfor\nvalence\nand\narousal\nestimation\nin-the-wild,”"
        },
        {
          "[7]\nP. Barros, N. Churamani, E. Lakomkin, H. Siqueira, A. Sutherland, and": "Vision Computing, vol. 65, pp. 23–36, 2017."
        },
        {
          "[7]\nP. Barros, N. Churamani, E. Lakomkin, H. Siqueira, A. Sutherland, and": "[10] W.-Y. Chang, S.-H. Hsu, and J.-H. Chien, “Fatauva-net: An integrated"
        },
        {
          "[7]\nP. Barros, N. Churamani, E. Lakomkin, H. Siqueira, A. Sutherland, and": "deep\nlearning\nframework\nfor\nfacial\nattribute\nrecognition,\naction\nunit"
        },
        {
          "[7]\nP. Barros, N. Churamani, E. Lakomkin, H. Siqueira, A. Sutherland, and": "the IEEE\ndetection, and valence-arousal estimation,” in Proceedings of"
        },
        {
          "[7]\nP. Barros, N. Churamani, E. Lakomkin, H. Siqueira, A. Sutherland, and": "conference on computer vision and pattern recognition workshops, 2017,"
        },
        {
          "[7]\nP. Barros, N. Churamani, E. Lakomkin, H. Siqueira, A. Sutherland, and": "pp. 17–25."
        },
        {
          "[7]\nP. Barros, N. Churamani, E. Lakomkin, H. Siqueira, A. Sutherland, and": "[11] X. Pan, G. Ying, G. Chen, H. Li, and W. Li, “A deep spatial and temporal"
        },
        {
          "[7]\nP. Barros, N. Churamani, E. Lakomkin, H. Siqueira, A. Sutherland, and": "aggregation framework for video-based facial expression recognition,”"
        },
        {
          "[7]\nP. Barros, N. Churamani, E. Lakomkin, H. Siqueira, A. Sutherland, and": "IEEE Access, vol. 7, pp. 48 807–48 815, 2019."
        },
        {
          "[7]\nP. Barros, N. Churamani, E. Lakomkin, H. Siqueira, A. Sutherland, and": "[12] D. H. Kim and B. C. Song, “Contrastive adversarial\nlearning for person-"
        },
        {
          "[7]\nP. Barros, N. Churamani, E. Lakomkin, H. Siqueira, A. Sutherland, and": "independent\nfacial emotion recognition,” 2021."
        },
        {
          "[7]\nP. Barros, N. Churamani, E. Lakomkin, H. Siqueira, A. Sutherland, and": "[13] D. Deng, Z. Chen,\nand B. E.\nShi,\n“Multitask\nemotion\nrecognition"
        },
        {
          "[7]\nP. Barros, N. Churamani, E. Lakomkin, H. Siqueira, A. Sutherland, and": "with incomplete labels,” in 2020 15th IEEE International Conference"
        },
        {
          "[7]\nP. Barros, N. Churamani, E. Lakomkin, H. Siqueira, A. Sutherland, and": "on Automatic Face and Gesture Recognition (FG 2020)(FG).\nIEEE"
        },
        {
          "[7]\nP. Barros, N. Churamani, E. Lakomkin, H. Siqueira, A. Sutherland, and": "Computer Society, 2020, pp. 828–835."
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Mimamo net: Integrating microand macro-motion for video emotion recognition",
      "authors": [
        "D Deng",
        "Z Chen",
        "Y Zhou",
        "B Shi"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "2",
      "title": "Analysing affective behavior in the first abaw 2020 competition",
      "authors": [
        "D Kollias",
        "A Schulc",
        "E Hajiyev",
        "S Zafeiriou"
      ],
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)(FG)"
    },
    {
      "citation_id": "3",
      "title": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "arxiv": "arXiv:1910.04855"
    },
    {
      "citation_id": "4",
      "title": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "authors": [
        "D Kollias",
        "V Sharmanska",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "arxiv": "arXiv:1910.11111"
    },
    {
      "citation_id": "5",
      "title": "Aff-wild: Valence and arousal 'in-the-wild'challenge",
      "authors": [
        "S Zafeiriou",
        "D Kollias",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "I Kotsia"
      ],
      "year": "2017",
      "venue": "Computer Vision and Pattern Recognition Workshops (CVPRW), 2017 IEEE Conference"
    },
    {
      "citation_id": "6",
      "title": "Recognition of affect in the wild using deep neural networks",
      "authors": [
        "D Kollias",
        "M Nicolaou",
        "I Kotsia",
        "G Zhao",
        "S Zafeiriou"
      ],
      "year": "2017",
      "venue": "Computer Vision and Pattern Recognition Workshops (CVPRW), 2017 IEEE Conference"
    },
    {
      "citation_id": "7",
      "title": "The omg-emotion behavior dataset",
      "authors": [
        "P Barros",
        "N Churamani",
        "E Lakomkin",
        "H Siqueira",
        "A Sutherland",
        "S Wermter"
      ],
      "year": "2018",
      "venue": "2018 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "8",
      "title": "Deep affect prediction in-thewild: Aff-wild database and challenge, deep architectures, and beyond",
      "authors": [
        "D Kollias",
        "P Tzirakis",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "B Schuller",
        "I Kotsia",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "9",
      "title": "Afew-va database for valence and arousal estimation in-the-wild",
      "authors": [
        "J Kossaifi",
        "G Tzimiropoulos",
        "S Todorovic",
        "M Pantic"
      ],
      "year": "2017",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "10",
      "title": "Fatauva-net: An integrated deep learning framework for facial attribute recognition, action unit detection, and valence-arousal estimation",
      "authors": [
        "W.-Y Chang",
        "S.-H Hsu",
        "J.-H Chien"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition workshops"
    },
    {
      "citation_id": "11",
      "title": "A deep spatial and temporal aggregation framework for video-based facial expression recognition",
      "authors": [
        "X Pan",
        "G Ying",
        "G Chen",
        "H Li",
        "W Li"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "12",
      "title": "Contrastive adversarial learning for personindependent facial emotion recognition",
      "authors": [
        "D Kim",
        "B Song"
      ],
      "year": "2021",
      "venue": "Contrastive adversarial learning for personindependent facial emotion recognition"
    },
    {
      "citation_id": "13",
      "title": "Multitask emotion recognition with incomplete labels",
      "authors": [
        "D Deng",
        "Z Chen",
        "B Shi"
      ],
      "year": "2020",
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)(FG)"
    }
  ]
}