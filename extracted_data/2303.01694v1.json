{
  "paper_id": "2303.01694v1",
  "title": "Dwformer: Dynamic Window Transformer For Speech Emotion Recognition",
  "published": "2023-03-03T03:26:53Z",
  "authors": [
    "Shuaiqi Chen",
    "Xiaofen Xing",
    "Weibin Zhang",
    "Weidong Chen",
    "Xiangmin Xu"
  ],
  "keywords": [
    "speech emotion recognition",
    "transformer",
    "speech signal processing"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech emotion recognition is crucial to human-computer interaction. The temporal regions that represent different emotions scatter in different parts of the speech locally. Moreover, the temporal scales of important information may vary over a large range within and across speech segments. Although transformer-based models have made progress in this field, the existing models could not precisely locate important regions at different temporal scales. To address the issue, we propose Dynamic Window transFormer (DWFormer), a new architecture that leverages temporal importance by dynamically splitting samples into windows. Self-attention mechanism is applied within windows for capturing temporal important information locally in a fine-grained way. Cross-window information interaction is also taken into account for global communication. DWFormer is evaluated on both the IEMO-CAP and the MELD datasets. Experimental results show that the proposed model achieves better performance than the previous state-of-the-art methods.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech Emotion Recognition (SER) is the key to humancomputer interaction. To make human-computer interaction more natural, it is essential for machines to precisely capture emotions and respond in an appropriate manner.\n\nSER has been studied for decades. In recent years, transformer-based models have fostered huge improvement in SER field  [1, 2, 3, 4] . The vanilla transformer  [5]  is outstanding in modeling long-range dependencies in speech sequences. However, its core mechanism, global self-attention mechanism, is vulnerable to noise and may not be able to focus on the same areas as the location of the emotion  [1] . This limits the effectiveness of the transformer model. Sound events that prominently represent emotions, such as changes in intonation and speed, laughs and sighs, are located in local regions. Furthermore, the scales of important information are varied over a large range within and across speech segments (see Fig.  1 ).  [2]  applies local window attention mechanism Fig.  1 . Two examples are selected from IEMOCAP  [6] .  [lau]  represents laughter. Important sound events that indicates different emotions, such as laughter, sigh, sniffle and positive semantics etc., exist in local regions of speech and their duration varies.\n\nto enable models to focus more on local changes. However, immutable window lengths limit these models to capture sentiment information that varies with different temporal scales.\n\nIn computer vision field, dynamic designs  [7, 8, 9]  allow models to have a flexible perceptual field so that different shapes of targets can be captured. In SER field,  [10, 11, 12]   (i) A new architecture, named Dynamic Window trans-Former (DWFormer), is proposed to provide insights into the problem of capturing important temporal information of variable lengths for SER.\n\n(ii) We evaluate DWFormer on both the IEMOCAP and MELD  [13]    . IC represents Importance Calculation module (detailed in Fig.  3 ). The triangular sequence impt means the important weights. The deeper the color, the more important the token is. The rectangle sequence x represents feature map.\n\napproaches. The code will be published. 1",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Methodology",
      "text": "The architecture of DWFormer is shown in Fig. The input audio signal is first fed into the feature extractor to extract the features x 0 ∈ R T ×D , where T represents the number of feature tokens, D represents the feature dimension. Then x 0 is passed through a vanilla transformer encoder layer. The outputs of the encoder layer consists of the hidden feature x 11 and attention weights W ∈ R H×T ×T where H represents the number of heads. W is sent into the Importance Calculation module to obtain an temporal importance estimation which is necessary for the 1st DWFormer block.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Importance Calculation Module",
      "text": "The Importance Calculation (IC) module is proposed to measure the importance of token. Inspired by  [1] , IC module utilize the attention weights obtained from transformer for calculation. The process is shown in Fig.  3 , which is described as:\n\nwhere aw s represents attention weight from s-head, T 1 is the row length of the averaged matrix aw avg . Softmax function The importance score of each token impt 11 obtained by IC module, together with the hidden feature x 11 are then transferred to N stacked DWFormer blocks for further evaluation.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Dwformer Block",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Dynamic Local Window Transformer Module",
      "text": "The Dynamic Local Window Transformer (DLWT) module dynamically partitions regions for input feature and captures important information through local relationship modeling. The procedure is elaborated as bellows.\n\nFirstly, utilizing dynamic window splitting(DWS) operation, feature tokens are dynamically split into unequal-length windows according to their importance values obtained from the IC component. As shown in Fig.  4 , based on the importance scores calculated from the former block, tokens with importance scores above/below the threshold are grouped chronologically into several strong/weak emotional correlation windows. The threshold is set to the median of all the importance values. A strong emotional correlation windows and B strong emotional correlation windows are obtained from x a1 .\n\nTo process data in batches, the window division results are implemented by attention mask mechanism:\n\nwhere M ij is the value of ith row and jth column of the attention mask M ∈ R T ×T . b w k and e w k are the begin and the end indexes of the row and column of the kth window.  Then, each window passes through a transformer encoder for intra-window information communication, which is defined as:\n\nwhere FFN represents Feed Forward Network, Q a1 , K a1 , V a1 are the projection mapping of the feature x a1 , T means transposition operation, d h is a scaled factor.\n\nFor the weak emotional correlation windows, the prior knowledge learned from the former block indicates that they have a high probability to be redundant for emotion recognition, so the features of the tokens located in them are multiplied by a weight λ(≤ 1), while those in strong emotional correlation windows are multiplied by 1. The output is defined as x a2 ∈ R T ×D .\n\nTemporal importance of each token within window is calculated by IC module. Calculation results of all windows are then concatenated together into a sequence along the chronological order, which is noted as impt a2 ∈ R T . x a2 and impt a2 are passed to Dynamic Global Window Transformer module for further operation.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Dynamic Global Window Transformer Module",
      "text": "Dynamic Global Window Transformer (DGWT) module takes a holistic approach to remeasure the importance relationship between windows after DLWT. In detail, each window firstly generates a token through Window Weighted Sum operation, which is defined as:\n\nwhere p is the index of the token. wt ∈ R (A+B)×D is the sequence of window tokens.\n\nThen the sequence wt is passed through a transformer encoder for global interaction, which is defined as:\n\nwhere Q wt , K wt , V wt are the projection mapping of wt.\n\nNext, each window token is upsampled to the same length of the corresponding window by copying the vectors of the window. Then these tokens are concatenated together into a sequence, which is noted as x a3 ∈ R T ×D . The output of a DWFormer block x (a+1)1 is the summation of x a2 and x a3 so that each token obtains both local and global information.\n\nThe importance scores between windows impt a3 ∈ R A+B are calculated by IC. Through a DWFormer block, the importance of each token impt (a+1)1 is remeasured by:\n\nthe upsampling operation is the same as mentioned above.\n\nIn the next DWFormer block, x (a+1)1 is split into windows based on impt (a+1)1 . Finally, the emotion classification is performed by applying the temporal average pooling layer on the output feature x (N +1)1 of the N th DWFormer block, followed by a multi layer perception classifier.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiment",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiment Setup",
      "text": "We evaluate DWFormer on IEMOCAP and MELD datasets. On IEMOCAP dataset, DWFormer is evaluated using 5-fold leave-one-section-out cross validation. 4 emotions (happy &excited, angry, sad and neutral) are selected for classification. Weighted Accuracy (WA) and Unweighted Accuracy (UA) are the measuring metrics. On MELD corpus which contains 7 emotions (anger, disgust, fear, joy, neutral, sadness, surprise), the Weighted F1 (WF1) score is reported on test set.\n\nThe output feature of the 12th transformer encoder layer of Pre-trained WavLM-Large  [14]  model is used as the audio feature. The number of DWFormer blocks for IEMOCAP is 3 and for MELD is 2. The number of heads is 8. The activation function is ReLU. The number of batchsize is 32. The learning rate is initialized to be 3e-4 for IEMOCAP, while 5e-4 for MELD. The value of λ is 0.85. We employ an SGD optimizer for 120 epochs using a cosine decay learning rate scheduler with cosine warm-up scheduler. The optimization function is Cross Entropy Loss.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Comparison To Baseline Networks",
      "text": "The vanilla transformer, together with fixed window transformer which splits input feature into equal-length windows and applies self-attention within each window, are selected as the baseline networks. The parameters of baseline networks are the same as DWFormer. The window length of fixed window transformer is the same as the average length of the windows in the DWFormer. To verify the validity of the modules from DWFormer, ablation experiments are also conducted.\n\nResults in Table  1  demonstrate that DWFormer outperforms the Vanilla transformer and the fixed window transformer on both IEMOCAP and MELD datasets. Meanwhile, removing either the DLWT or the DGWT modules from DW-Former causes a significant decrease.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Model",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Comparison To Conventional Research& Visualization",
      "text": "Since our model employs the local-global architecture, we have conducted the comparison experiment with the conventional studies.  [10]  is open-source, so we first reproduce the results of  [10]  to ensure the correctness of the codes, and then we test the model under our experimental settings (Exp 1).\n\nSince the codes of  [11, 12]   In addition, Visualization results are shown in Fig.  5 . As shown in Fig.  5 , Vanilla Transformer, Fixed Window Transformer and ATDA are not as good as ours in locating important temporal information.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Comparison To Previous State-Of-The-Art Methods",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusions",
      "text": "We propose a new transformer-based framework, DWFormer, which aims at capturing important temporal regions at variable scales within and across samples in SER field. We empirically demonstrate that DWFormer outperforms the previous state-of-the-art methods. Ablation study proves the effectiveness of DLWT and DGWT modules. With the ability to locate important information, we plan to apply DWFormer in the pathological speech recognition field to assist researchers in understanding the impact of disease on pronunciation.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ). [2] applies local window attention mechanism",
      "page": 1
    },
    {
      "caption": "Figure 1: Two examples are selected from IEMOCAP[6]. [lau]",
      "page": 1
    },
    {
      "caption": "Figure 2: Model architecture of DWFormer. For simplicity, the residual connection and layer normalization are not plotted in the",
      "page": 2
    },
    {
      "caption": "Figure 3: ). The triangular sequence impt means the important",
      "page": 2
    },
    {
      "caption": "Figure 2: . The core",
      "page": 2
    },
    {
      "caption": "Figure 3: , which is described",
      "page": 2
    },
    {
      "caption": "Figure 3: The IC module calculates the importance from the",
      "page": 2
    },
    {
      "caption": "Figure 4: , based on the impor-",
      "page": 2
    },
    {
      "caption": "Figure 4: The operation of dynamic window splitting.",
      "page": 3
    },
    {
      "caption": "Figure 5: , Vanilla Transformer, Fixed Window Trans-",
      "page": 4
    },
    {
      "caption": "Figure 5: Visualization results of vanilla transformer, ﬁxed win-",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 1: demonstrate that DWFormer outper-",
      "data": [
        {
          "Model": "",
          "IEMOCAP": "WA(%)",
          "MELD": "WF1(%)"
        },
        {
          "Model": "Vanilla Transformer",
          "IEMOCAP": "70.7",
          "MELD": "47.1"
        },
        {
          "Model": "Fixed Window Transformer",
          "IEMOCAP": "71.2",
          "MELD": "47.6"
        },
        {
          "Model": "DWFormer (Ours) w/o DLWT",
          "IEMOCAP": "71.5",
          "MELD": "47.8"
        },
        {
          "Model": "DWFormer (Ours) w/o DGWT",
          "IEMOCAP": "71.5",
          "MELD": "47.7"
        },
        {
          "Model": "DWFormer (Ours)",
          "IEMOCAP": "72.3",
          "MELD": "48.5"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 1: demonstrate that DWFormer outper-",
      "data": [
        {
          "Dataset": "IEMOCAP",
          "Model": "[Chen et al., 2022][2]",
          "WA(%)": "62.9",
          "UA(%)": "64.5",
          "WF1(%)": "-"
        },
        {
          "Dataset": "",
          "Model": "[Li et al. 2022][15]",
          "WA(%)": "68.0",
          "UA(%)": "68.2",
          "WF1(%)": "-"
        },
        {
          "Dataset": "",
          "Model": "[Zou et al., 2022][16]",
          "WA(%)": "69.8",
          "UA(%)": "71.1",
          "WF1(%)": "-"
        },
        {
          "Dataset": "",
          "Model": "DWFormer(Ours)",
          "WA(%)": "72.3",
          "UA(%)": "73.9",
          "WF1(%)": "-"
        },
        {
          "Dataset": "MELD",
          "Model": "[Chudasama et al., 2022][17]",
          "WA(%)": "-",
          "UA(%)": "-",
          "WF1(%)": "35.8"
        },
        {
          "Dataset": "",
          "Model": "[Chen et al., 2022][2]",
          "WA(%)": "-",
          "UA(%)": "-",
          "WF1(%)": "41.9"
        },
        {
          "Dataset": "",
          "Model": "[Lian et al., 2022][18]",
          "WA(%)": "-",
          "UA(%)": "-",
          "WF1(%)": "45.2"
        },
        {
          "Dataset": "",
          "Model": "DWFormer (Ours)",
          "WA(%)": "-",
          "UA(%)": "-",
          "WF1(%)": "48.5"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 1: demonstrate that DWFormer outper-",
      "data": [
        {
          "Experimental Setting": "",
          "Model": "",
          "IEMOCAP": "WA(%)",
          "MELD": "WF1(%)"
        },
        {
          "Experimental Setting": "Exp 1",
          "Model": "[10]",
          "IEMOCAP": "59.6",
          "MELD": "39.8"
        },
        {
          "Experimental Setting": "",
          "Model": "DWFormer (Ours)",
          "IEMOCAP": "72.3",
          "MELD": "48.5"
        },
        {
          "Experimental Setting": "Exp 2",
          "Model": "[11]",
          "IEMOCAP": "69.4",
          "MELD": "-"
        },
        {
          "Experimental Setting": "",
          "Model": "[12]",
          "IEMOCAP": "70.3",
          "MELD": "-"
        },
        {
          "Experimental Setting": "",
          "Model": "DWFormer (Ours)",
          "IEMOCAP": "76.3",
          "MELD": "-"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Key-sparse transformer for multimodal speech emotion recognition",
      "authors": [
        "Weidong Chen",
        "Xiaofeng Xing",
        "Xiangmin Xu",
        "Jichen Yang",
        "Jianxin Pang"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "3",
      "title": "SpeechFormer: A Hierarchical Efficient Framework Incorporating the Characteristics of Speech",
      "authors": [
        "Weidong Chen",
        "Xiaofen Xing",
        "Xiangmin Xu",
        "Jianxin Pang",
        "Lan Du"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "4",
      "title": "A novel end-to-end speech emotion recognition network with stacked transformer layers",
      "authors": [
        "Xianfeng Wang",
        "Min Wang",
        "Wenbo Qi",
        "Wanqi Su",
        "Xiangqian Wang",
        "Huan Zhou"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "5",
      "title": "Learning mutual correlation in multimodal transformer for speech emotion recognition",
      "authors": [
        "Yuhua Wang",
        "Guang Shen",
        "Yuezhu Xu",
        "Jiahang Li",
        "Zhengdao Zhao"
      ],
      "year": "2021",
      "venue": "Learning mutual correlation in multimodal transformer for speech emotion recognition"
    },
    {
      "citation_id": "6",
      "title": "Advances in neural information processing systems",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Lukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "7",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "8",
      "title": "Beyond fixation: Dynamic window visual transformer",
      "authors": [
        "Pengzhen Ren",
        "Changlin Li",
        "Guangrun Wang",
        "Yun Xiao",
        "Qing Du",
        "Xiaodan Liang",
        "Xiaojun Chang"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "9",
      "title": "Glance and focus: a dynamic approach to reducing spatial redundancy in image classification",
      "authors": [
        "Yulin Wang",
        "Kangchen Lv",
        "Rui Huang",
        "Shiji Song",
        "Le Yang",
        "Gao Huang"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "10",
      "title": "Multiscale dynamic graph convolutional network for hyperspectral image classification",
      "authors": [
        "Sheng Wan",
        "Chen Gong",
        "Ping Zhong",
        "Bo Du",
        "Lefei Zhang",
        "Jian Yang"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Geoscience and Remote Sensing"
    },
    {
      "citation_id": "11",
      "title": "Atda: Attentional temporal dynamic activation for speech emotion recognition",
      "authors": [
        "Lu-Yao Liu",
        "Wen-Zhe Liu",
        "Jian Zhou",
        "Hui-Yuan Deng",
        "Lin Feng"
      ],
      "year": "2022",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "12",
      "title": "Speech emotion recognition with localglobal aware deep representation learning",
      "authors": [
        "Jiaxing Liu",
        "Zhilei Liu",
        "Longbiao Wang",
        "Lili Guo",
        "Jianwu Dang"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "13",
      "title": "Temporal attention convolutional network for speech emotion recognition with latent representation",
      "authors": [
        "Jiaxing Liu",
        "Zhilei Liu",
        "Longbiao Wang",
        "Yuan Gao",
        "Lili Guo",
        "Jianwu Dang"
      ],
      "year": "2020",
      "venue": "Temporal attention convolutional network for speech emotion recognition with latent representation"
    },
    {
      "citation_id": "14",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Gautam Naik",
        "Erik Cambria",
        "Rada Mihalcea"
      ],
      "year": "2018",
      "venue": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "15",
      "title": "Wavlm: Large-scale self-supervised pre-training for full stack speech processing",
      "authors": [
        "Sanyuan Chen",
        "Chengyi Wang",
        "Zhengyang Chen",
        "Yu Wu",
        "Shujie Liu",
        "Zhuo Chen",
        "Jinyu Li",
        "Naoyuki Kanda",
        "Takuya Yoshioka",
        "Xiong Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "16",
      "title": "Context-aware Multimodal Fusion for Emotion Recognition",
      "authors": [
        "Jinchao Li",
        "Shuai Wang",
        "Yang Chao",
        "Xunying Liu",
        "Helen Meng"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "17",
      "title": "Speech emotion recognition with co-attention based multi-level acoustic information",
      "authors": [
        "Heqing Zou",
        "Yuke Si",
        "Chen Chen",
        "Deepu Rajan",
        "Eng Siong"
      ],
      "year": "2022",
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "18",
      "title": "Multimodal emotion recognition with self-guided modality calibration",
      "authors": [
        "Mixiao Hou",
        "Zheng Zhang",
        "Guangming Lu"
      ],
      "year": "2022",
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "19",
      "title": "Smin: Semisupervised multi-modal interaction network for conversational emotion recognition",
      "authors": [
        "Zheng Lian",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    }
  ]
}