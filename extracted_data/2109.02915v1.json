{
  "paper_id": "2109.02915v1",
  "title": "Few-Shot Learning In Emotion Recognition Of Spontaneous Speech Using A Siamese Neural Network With Adaptive Sample Pair Formation",
  "published": "2021-09-07T08:04:02Z",
  "authors": [
    "Kexin Feng",
    "Theodora Chaspari"
  ],
  "keywords": [
    "Emotion recognition",
    "scripted/spontaneous speech",
    "few-shot learning",
    "metric learning",
    "siamese neural network"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech-based machine learning (ML) has been heralded as a promising solution for tracking prosodic and spectrotemporal patterns in real-life that are indicative of emotional changes, providing a valuable window into one's cognitive and mental state. Yet, the scarcity of labelled data in ambulatory studies prevents the reliable training of ML models, which usually rely on \"data-hungry\" distribution-based learning. Leveraging the abundance of labelled speech data from acted emotions, this paper proposes a few-shot learning approach for automatically recognizing emotion in spontaneous speech from a small number of labelled samples. Few-shot learning is implemented via a metric learning approach through a siamese neural network, which models the relative distance between samples rather than relying on learning absolute patterns of the corresponding distributions of each emotion. Results indicate the feasibility of the proposed metric learning in recognizing emotions from spontaneous speech in four datasets, even with a small amount of labelled samples. They further demonstrate superior performance of the proposed metric learning compared to commonly used adaptation methods, including network fine-tuning and adversarial learning. Findings from this work provide a foundation for the ambulatory tracking of human emotion in spontaneous speech contributing to the real-life assessment of mental health degradation.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "E MOTION tracking has been investigated as an approach to help individuals remain in psychologically healthy and to assist with mental diseases  [1] . Speech-based ambulatory monitoring is a promising method to longitudinal emotion tracking: prosodic and spectrotemporal patterns of speech reflect changes in muscle tension of the articulatory system, which are indicative of one's emotions  [2] . Supplemented with artificial intelligence (AI), speech has the potential to serve as a valuable biomarker for tracking emotions and triggering early mental health intervention mechanisms  [3] .\n\nDespite the potential of ambulatory speech monitoring for tracking human emotion, the reliable annotation of spontaneous emotional speech (i.e., speech elicited in realistic conditions without planning), usually conducted via selfreports or third-party evaluators, is an inherently challenging task. Self-reporting is subjective and potentially biased by social, cultural, and psychological factors  [4] , while thirdparty annotation can be erroneous and time-consuming  [5] . Due to these limitations, audio samples in many speechbased emotion datasets are collected through acted elicitation methods relying on individuals who engender a target emotion while uttering pre-determined linguistic contents, also known as scripted speech  [6] . Despite the fact that these methods tend to overlook subtle expression details, they provide ample data, based on which machine learning (ML) methodologies can recognize emotions  [7] .\n\nTransfer learning refers to leveraging knowledge from ample training examples from one domain to learn robust data representations for another (potentially related) domain  [8] . Various transfer learning algorithms have been proposed for speech emotion recognition, including adaptive support vector machines, neural network fine-tuning, progressive neural networks, and adversarial learning  [9] ,  [10] ,  [11] ,  [12] . Despite the promising results, these require a relatively large number of samples from the target domain to achieve promising performance. Few-shot learning has been proposed as an alternative to fully-supervised transfer learning, since it accounts for the potential shortage of labelled samples in the target domain. A promising approach in few-shot learning relies on Metric Learning (MeL). MeL learns a transferable distance-based embedding that models the relative distance between classes  [13] . In this way, it is easier to classify samples based on the new embedding rather than the original space. MeL has been explored for audio classification  [14] ,  [15] , speaker recognition  [16] , and image-based emotion and facial expression recognition  [17] ,  [18] . To the best of our knowledge, MeL has not been examined in speech-based recognition of spontaneous emotion.\n\nWe propose a MeL approach that transfers knowledge from scripted to spontaneous speech by learning emotionspecific speech embeddings with small supervision from the target domain. MeL conducts pairwise comparisons of samples between emotional classes and is implemented with a siamese neural network (SNN). Beyond the pairwise sample comparison, we impose an additional supervision to MeL that allows to directly obtain an emotional class output for a test sample, referred to as Metric Learning with Super-vision (MeL-S). Finally, we address issues related to training convergence that yield by the random pair formation in MeL and MeL-S through an adaptive procedure. The proposed Metric Learning with Supervision and Adaptive Sample Pair Formation (MeL-S-ASPF) iteratively trains a SNN using an adaptive selection likelihood of the input samples, assigning higher probability to consistently misclassified samples. The proposed few-shot learning is compared to in-domain learning, out-of-domain learning, as well as transfer learning implemented with feedforward neural network (FNN) finetuning and adversarial learning. Results indicate the ability of MeL-S to effectively transfer knowledge between acted and spontaneous speech relying only on a small number of labelled samples from the target domain (e.g., 70% unweighted average recall using 2-3 labelled samples per class on a 3-way classification task). Our findings further suggest that the proposed MeL-S-ASPF provides advantages compared to random sample pair formation of MeL and MeL-S, which are especially beneficial when using a small number of labelled samples.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Previous Work",
      "text": "Previously proposed transfer learning methods for speechbased emotion recognition include both supervised and unsupervised approaches. The first require labeled data from the target domain, in contrast to the latter. In terms of supervised learning, prior work has explored the effect of supervised domain adaptation in cross-corpus experiments using adaptive and incremental support vector machines  [19] , as well as auto-enconder architectures that learn a transferable low-dimensional feature space from the original features  [20] ,  [21] . Progressive neural networks have been also proposed as an alternative method to transfer knowledge between domains without forgetting the learned embeddings of the source domain  [9] ,  [10] . In terms of unsupervised transfer learning, adversarial and generative learning have been employed to tackle the distribution mismatch between emotional speech corpora  [22] ,  [23] ,  [24] . Results demonstrate that even including unlabelled data from the target domain can yield considerable improvement compared to not including any target data. A detailed review of transfer learning methodologies with focus on speech-based emotion recognition can be found in  [25] .\n\nFew-shot learning aims to classify samples from a target domain using a small number of labelled examples from that domain. A promising few-shot learning approach relies on metric learning algorithms, which aim to learn a transferable feature embedding by optimizing a distance loss metric between the classes of interest, rather than learning the distribution of each class separately  [26] ,  [27] . Metric learning, implemented through SNNs  [28]  and relation networks  [27] , has been explored in person re-identification, speaker verification, and recommendation systems  [29] ,  [30] ,  [31] . SNNs have also shown promising results in indomain emotion classification, not in the context of few-shot learning  [32] ,  [33] ,  [34] . As part of our preliminary work, we have explored the feasibility of metric learning implemented with SNN to few-shot emotion recognition  [35] .\n\nThe contributions of this paper are: (1) A novel formulation of MeL and MeL-S for transferring emotion-specific knowledge between a source and a target domain using a small number of labelled samples from the target domain. The proposed approach is likely to overcome limitations of conventional distribution-based learning due to its ability to model the relative distance between classes; (2) An iterative adaptive sample pair formation for SNN training, that promotes the selection of samples which are consistently misclassified by previous iterations. This has the potential to increase the robustness of metric learning compared to modeling the distance between random pairs of samples; and (3) An investigation of the feasibility of transferring knowledge between acted (source) and spontaneous (target) speech, providing a foundation into ways to leverage the large number of labelled samples from acted speech datasets in emotion tracking in spontaneous speech.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Data Description And Pre-Processing",
      "text": "In this study, we use four datasets, which were selected due to the fact that they contain utterances spoken in English and categorical emotional labels. These include the Interactive Emotional Dyadic Motion Capture (IEMOCAP)  [36] , Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)  [37] , Crowd-sourced Emotional Multimodal Actors Dataset (CREMA-D)  [38] , and Audio-Visual Emotion Database (eNTERFACE'05)  [39] . IEMOCAP  [36]  consists of dyadic sessions between 10 actors, who are engaged in both scripted (IEMOCAP-SC) and spontaneous (IEMOCAP-SP) interactions. RAVDESS  [37]  contains audiovisual recording from 24 actors, who uttered a list of scripted sentences with different emotions. CREMA-D  [38]  includes speech data from 91 actor participants, who were asked to express scripted sentences with target emotions until approved by an expert. The eNTERFACE'05 dataset  [39]  includes 42 speakers, who were asked to utter scripted sentences in response to listening short stories that were used to elicit various target emotions. Here, we explore the three most common emotions present in these datasets: anger, happiness, and sadness. Scripted speech samples from IEMOCAP-SC (133 minutes), RAVDESS (36 minutes), CREMA-D (159 minutes), and eNTERFACE'05 (30 minutes) are used as the source data, while spontaneous samples from IEMOCAP-SP (92 minutes) comprise the target data. Few-shot learning experiments will be conducted so that the samples from target dataset are gradually made available in the learning process.\n\nA 64-dimensional speech descriptor is extracted using the openSMILE toolkit  [40]  to capture prosodic and spectrotemporal variations relevant to human emotion  [41] . The features are extracted using the configuration file from the INTERSPEECH'09 Emotion Challenge  [42]  with default parameters of 25ms frame length and 10ms step length. The first 32 feature dimensions include the arithmetic mean and standard deviation of frame-based speech intensity, zerocrossing rate, voicing probability, fundamental frequency, and the first 12 Mel-frequency cepstral coefficients (MFCC). The last 32 dimensions include the first-order derivative of the above descriptors. Feature normalization is conducted using the scikit-learn  [43]  library within each dataset to initially mitigate potential domain differences. Each feature is standardized using the mean and standard deviation computed using the samples of each dataset. The scripted and spontaneous part of the IEMOCAP dataset is normalized separately to avoid potential information leaking.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Proposed Methodology",
      "text": "We will first introduce the fundamentals of the SNN architecture (Section 4.1) and then describe the proposed metric learning approaches (Sections 4.2-4.4). We will further provide details on the experimental design (Section 4.5), including the formulation of in-domain and out-of-domain learning (Section 4.5.1), the experimental setting of the metric learning approaches (Section 4.5.2), and the baseline approaches (Section 4.5.3).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Siamese Neural Network (Snn)",
      "text": "The Siamese neural network (SNN) is comprised of two input streams that compare a pair of input samples (x i , x j ) (Fig.  1 ). The hidden layers of the SNN learn a transformation f W , parameterized by weights W, that implements a similarity function between the two pairs of samples. The transformed input samples f W (x i ) and f W (x j ) are compared at the output through the distance function d(•, •). The parameters W of the SNN are the same between the two input streams and are learned so that they minimize the following distance loss function:\n\nwhere X c is the set of data belonging to class c, X c is the set of data belonging to class c different than c, and κ determines the trade-off between penalizing dissimilarity between samples belonging to the same class against similarity between samples belonging to different classes.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Metric Learning (Mel)",
      "text": "We learn emotion-specific speech representations through a MeL approach implemented with SNNs. Our SNN is initially trained on samples of scripted speech (i.e., source) based on  (1) . Knowledge transfer is performed by finetuning the weights W of the SNN using the small number of labelled samples from the spontaneous speech samples (i.e., target). Since the SNN is used to identify whether a pair of input samples belongs to the same emotion, it does yield explicit emotion labels. As a result, we obtain an emotion classification outcome for a test sample x by comparing its learned embedding to the embedding of the center xc of the labelled samples from the target data for each emotional class c. The final emotion label is obtained by identifying the class whose center depicts the lowest distance to the test sample:",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Metric Learning With Supervision (Mel-S)",
      "text": "We further impose an additional supervision constraint to the original SNN architecture, which allows us to directly obtain the emotional class outcome for a test sample (i.e., without having to compare the distance of the test sample with the center of samples from each class, as in Section 4.2). According to the MeL-S model, the transformed samples f W (x), where W is learned by the MeL approach through the loss function L d , are fed into an additional set of fully-connected hidden layers g V , which learns a mapping between the transformed space f W (x) and the final class outcome y. The transformation g V is implemented with a set of neural layers, but it could have been also implemented with any other linear or non-linear classification algorithm. The weights V are learned such that they minimize the cross-entropy loss of the emotion classification task:\n\nwhere X is the set of input samples and Y is the set of labels in the training set. The weights W are initially pre-trained using the MeL method on the source domain and refined using the Mel-S method on the target domain, such that {W * , V * } = arg min V min W (L e (V) + L d (W)).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Metric Learning With Supervision And Adaptive Sample Pair Formation (Mel-S-Aspf)",
      "text": "The pairs of samples that serve as an input to the SNN in MeL and MeL-S (Sections 4.2, 4.3) were randomly selected. In this way, convergence of the training process might require many iterations, since the algorithm has no information on how well specific samples are learned. To address this limitation, we propose the MeL-S-ASPF, an iterative learning process that alternates between training the SNN and assigning an importance factor to the input samples used for SNN training. The importance factor is assigned such that higher importance is given to samples that are not adequately learned by the SNN during previous iterations. Samples with higher importance have a higher chance to get selected in the next training iteration of the SNN (Fig.  2 ). According to MeL-S-ASPF, each sample of the training set x ∈ X , y ∈ Y is assigned to a selection likelihood π t (x) during training iteration t, which is updated such that:\n\nwhere f Wt and g Vt are the transformations learned by iteration t, λ is a constant used to control the update rate of the sample selection likelihood, and • 1 is the l1-norm. We initialized π 1 (x) to 1 for every sample x. The likelihood is proportional to the probability that a data sample is being selected. The adaptive sample pair formation process is outlined in Algorithm 1. We generate pairs of samples within a speaker s that either belong to the same or different emotions, each of the two cases occurring with probability equal to 0.5. We select a sample x ∈ X s,c uttered by speaker s with emotion c based on the following probability:\n\nThe proposed update in (3) renders the pair formation adaptive in two ways:  (1)  The selection probability of consistently correctly classified samples decreases over time; and (2)\n\nThe selection probability of misclassified samples increases proportionately to the error of the current iteration.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Design",
      "text": "Here, we describe the experimental design that was employed to validate the proposed approach.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "In-Domain And Out-Of-Domain Learning",
      "text": "In order to understand the inherent domain difference between the source and the target, we conduct two experiments without transfer learning. First, we conduct outof-domain learning by training an emotion classification model on the source data and testing on the target. All samples from the source are used to train the model and all samples from the target are used for testing. Second, we perform in-domain-training, according to which data from the target domain are used for training and testing the models. In-domain-training is evaluated using leave-onesubject-out cross-validation on the target data only. For both in-domain and out-of-domain experiments, we employed a 5-layer FNN with 64, 32, 16, and 16 nodes in the first, second, third, and fourth layers, as well as 3 nodes in the output layer. The rectified linear unit (ReLU) activation function is employed in the first three layers, while the sigmoid activation is used in the last decision making layer to yield a probability outcome for each emotional class. All models are trained to minimize the cross-entropy loss using the Adam optimization  [44]  with a learning rate of 0.0005, 250 epochs.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Few-Shot Learning",
      "text": "We examine the effectiveness of the proposed metric learning (i.e., MeL, MeL-S, MeL-ASPF; Sections 4.2-4.4) by gradually rendering available an increasing number of labelled samples from the target data in our experiments. We start by randomly selecting 1 labelled sample per speaker and per emotion from the target data and using the remaining samples in the test set. We repeat this process 10 times and compute the average accuracy in order to obtain an unbiased result. Then, we gradually increase the number of samples to 2, . . . , 10 clips per speaker and per emotion, to explore changes in performance when additional labeled data from the target domain is available.\n\nThe proposed MeL, MeL-S, and MeL-S-ASPF approaches employ a SNN, which is first trained on the source data and then fine-tuned on the labelled samples from the target data.\n\nTo be consistent with the in-domain and out-of-domain learning (Section 4.5.1), the proposed MeL, MeL-S, and MeL-S-ASPF approaches are implemented with a 5-layer SNN with 64, 32, 16, and 16 nodes and ReLU activation in the hidden layers, as well as a sigmoid activation in the single node of the output layer. Learning is performed using cross-entropy loss and Adam optimization with a learning rate of 0.0005 and 250 training epochs. The trade-off between penalizing dissimilarity between samples belonging to the same class against similarity between samples belonging to different classes, as shown in  (1) , is set to κ = 1. The supervision layer of the MeL-S model includes 1 hidden layer with 8 units and ReLU activation, and an output layer with 3 nodes and sigmoid activation. The adaptive sample pair formation in MeL-S-ASPF is performed with an update rate λ = 0.1 of the selection likelihood in (3), since our goal is to avoid an uninformed abrupt increase of the selection likelihoods in the first few training iterations. The MeL-S-ASPF approach further includes T = 25 iterations, during which the SNN is trained using 10 epochs. When training the model on the source data, we form pairs of samples without considering speaker identity, which allows us to maximize the generalization of the original model as well as to mitigate the difference between the source datasets. When fine-tuning on the target data, we form pairs of samples within a speaker (i.e., a pair containing samples from the same speaker) with a selection likelihood that follows a uniform distribution with half of the pairs being from the same emotion, while the other half from different emotions. Sample pair formation on the target data using random pairs of speakers was also performed, but provided slightly decreased performance.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Baseline Methods",
      "text": "We used two baseline methods to compare the proposed metric learning approaches. The first was implemented through FNN fine-tuning, a commonly used transfer learning technique  [25] ,  [45] . Fine-tuning is conducted by pretraining the FNN on the source samples and then refining the learned weights of all layers based on the available labelled target data. We employ 5-layer FNN, similar to Section 4.5.1. The second baseline follows the Adversarial Discriminative Domain Adaptation (ADDA) method  [46] , a recently proposed domain adaptation method, that aims to reduce the distribution difference between the source and target samples through an adversarial loss. We implemented the ADDA in a similar way to previous research on speech emotion recognition  [23]  using the time-frequency logscaled spectrogram patches as features. The 256 dimensional Mel Filter Banks (MFBs) are computed for each sample, with a frame length of 32ms and a 16ms shift using Scipy  [47] . Each patch is comprised of 30 frames, with an overlap of 15 frames, resulting in a 256 × 30 input to a convolutional neural network (CNN). The CNN uses a kernel size of 15 for the first convolutional layer, and a kernel size of 3 with a dilation rate of 2 for the second convolutional layer. Each convolutional layer has 128 channels, followed by a max pooling layer of pool size 3 and stride size 2. Then a global maximum is computed and resulted in a 128-dimensional feature. Two hidden layers with 128 units were applied to the extracted feature before the output softmax layer. Each layer used the ReLU activation except the output layer, which used sigmoid. Because ADDA does not require target labels during training, we fine-tuned the trained model using target data for a better comparison with other methods. The dropout rate during training was 0.2. The prediction for each audio sample is obtained through a majority voting based on all the patches of the corresponding sample.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Results",
      "text": "All approaches are evaluated on their ability to correctly classify the three considered emotions using the unweighted average recall (UAR), which is computed by taking the average of recall rates for each emotion (Figure  3 ). The chance UAR for the 3-way classification task is 33.3%. The in-domain training achieves UAR of 72.5% suggesting moderate to high differentiation between the three emotions when a significant number of target data is available. The out-of-domain performance ranges between 49% to 68% demonstrating the potential data mismatch between scripted and spontaneous speech. RAVDESS, CREMA-D, and IEMOCAP-SC provide relatively successful out-ofdomain training (i.e., 67.4%), while this is not the case for eNTERFACE'05, which appears to depict a large domain difference with the target IEMOCAP-SP dataset. This differential result might be attributed to the fact that eNTER-FACE'05 was the only dataset in which audio samples were recorded by regular participants rather than actors.\n\nIn regards to the proposed metric learning, the MeL approach does not yield satisfactory performance. However, the supervision added as part of the MeL-S and MeL-S-ASPF approaches significantly improves results. A potential reason for this is that MeL-S and MeL-S-ASPF explicitly model the emotional outcome of interest, which is not the case for MeL. The MeL does not outperform the FNN fine-tuning and ADDA potentially for the same reason. It is noteworthy that the MeL-S and MeL-S-ASPF methods depict good performance even when a small amount of labelled target data is available (e.g., 1-2 labelled samples per speaker and emotion for eNTERFACE'05, RAVDESS, and IEMOCAP-SC). Also the MeL-S and MeL-S-ASPF models appear to reach stability in their performance when increasing the number of labelled target data from the target domain. We further observe that the MeL-S-ASPF outperforms the MeL-S across many cases including the IEMOCAP, RAVDESS, and CREMA-D datasets, suggesting that the adaptive sample pair formation benefits performance compared to the random formation of sample pairs. This indicates that this adaptive process can potentially contribute to effectively learning challenging samples that are difficult to estimate when not taking into account the corresponding modeling error. The overall best performance is obtained by the MeL-S-ASPF model when using CREMA-D as the source with 10 samples per speaker and emotion, reaching UAR larger than the in-domain learning (i.e., 74%).\n\nWe further compare the performance of metric learning to the baseline. Both FNN fine-tuning and ADDA appear to be consistently lower compared to the MeL-S and MeL-ASPF. The UAR of FNN fine-tuning, is slowly increasing, as we use more labelled samples from the target data. When we only use one labelled sample per speaker and emotion, negative transfer occurs for the FNN fine-tuning in many cases (e.g., RAVDESS, IEMOCAP-SC), indicating that the corresponding approach may not be promising for transferring knowledge between scripted and spontaneous speech in emotion recognition. The performance increase of ADDA is less consistent compared with FNN fine-tuning, which could possibly be due to unstable training of the adversarial model. However, the best performance of ADDA is better compared to that of the FNN fine-tuning, indicating the potential of this method.\n\nNext, we visualize the embeddings learned by the proposed metric learning and the baseline non-metric learning approaches. For this, we perform principal component analysis (PCA) to the output of the last hidden layer of each model and provide scatter plots of the first two PCA dimensions corresponding to the largest data variance (Fig.  4 ). The transformed data samples depict high overlap among the three classes when using the baseline FNN fine-tuning, while the MeL-S-ASPF model results in more distinct distributions. This difference is depicted in our results, since the metric learning approaches with additional supervision outperform the FNN fine-tuning (Fig.  3 ).\n\nWe investigate how sample selection likelihoods vary throughout the adaptive sample pair formation process by tracking the selection likelihood π t (x n ) of training samples x n over different training iterations t (t = 5, 10, 15, 20, 25) (Fig.  5 ). When using the CREMA-D as the source, the selection likelihood of most samples is concentrated around 1. This suggests that samples from CREMA-D are fairly easily classified based on the MeL-S-ASPF, therefore there is no need to update the selection likelihood for many samples. In contrast, many samples from the RAVDESS dataset are updated with the corresponding selection likelihoods being in higher ranges compared to CREMA-D. This is also depicted in the emotion classification results (Fig.  3 ), where the MeL-S-ASFP presents clear benefits for CREMA-D.\n\nWe finally perform error analysis to identify consistently misclassified samples in the target data (independently of the source) and potential sources of error. Common sources of error correspond to noisy audio samples with almost ineligible speech, samples with high discrepancy among annotators, as well as samples for which the linguistic information played an important role in making the final emotion decision. We provide the specific files that correspond to the misclassified samples in Table  1 .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Discussion & Conclusions",
      "text": "We propose a few-shot learning approach that leverages the abundance of publicly available data from scripted speech in order to detect emotion in spontaneous speech. Our results indicate the feasibility of using scripted speech data to initialize emotion classification models, which can provide useful information for the target data. They further suggest that the proposed MeL-S and MeL-S-ASPF approaches can result in more effective transfer of knowledge compared to conventional distribution learning that models the absolute sample distribution. MeL-S and MeL-S-ASPF yield a relative improvement of 6-23% compared to out-of-domain learning and 1-7% compared to the FNN fine-tuning baseline. Indicatively prior work depicts approximate relative improvement of 6-8%  [22] ,  [23] ,  [48]  compared to out-of-domain training for similar emotion classification tasks. For a few cases, the proposed metric learning approach is able to outperform indomain learning, which suggests that including additional data with the appropriate handling of the potential domain mismatch can benefit learning. Furthermore, this method could potentially be used when detecting different emotions in the source and the target data. Since the SNN learns the relative distance between classes instead of the actual class labels, the learned relations could still provide a meaningful initialization of the weights of the network so that it can be effectively generalized to unseen classes. Results from this work can help toward reliable systems of emotional intelligence that can detect emotions from spontaneous speech with a small number of labelled samples from the target domain. This can be particularly useful in real-life applications that rely on tracking wellbeing from speech, since it demonstrates the ability of speech-based systems to generalize to a new domain using limited supervised experience, potentially mimicking the human ability to recognize emotions from few examples through relative comparison from previous experience. The  proposed methods could eventually benefit ambulatory monitoring applications in the clinical domain, where labelled ambulatory data with high temporal granularity are scarce. In this context, audio data collected in the clinic can robustly initialize emotion recognition models that can be generalized in real-life settings for emotion tracking. Since the experience of everyday emotions plays a significant role in psychopathology, this can contribute to effective mental health diagnosis and intervention tracking.\n\nOur study depicts the following limitations. First, the spontaneous speech data from IEMOCAP-SP are collected in laboratory conditions involving high quality microphones and low levels of noise. Despite the phonetic and prosodic differences between scripted and spontaneous speech, the fact that both source and target data have been collected in laboratory conditions has the potential to decrease the domain mismatch between them. As part of our future work, we plan to examine the effectiveness of the proposed approach with various types of real-life datasets, such as the EmotiW  [49] . Second, the source and target audio samples are uttered in English, however, emotion is expressed universally, so it would be worthwhile to explore transfer learning and few-shot learning techniques in crosslinguistic scenarios. Of particular interest would be few shot learning between languages of different families, such as between English (i.e., Indo-European language) and Mandarin Chinese (i.e., Sino-Tibetan language family). Datasets that could be used toward this purpose include the Chinese natural emotional audio-visual database (CHEAVD)  [50]  and the Mandarin Chinese Emotional Speech Dataset -Portrayed (MES-P)  [51] . Finally, we studied each of the source datasets in isolation. Inspired by prior findings  [10] ,  [23] , it would be beneficial to explore the extent to which the combination of multiple source datasets can benefit fewshot learning. It would be also interesting to investigate the interplay between the properties of the various source datasets (e.g., including complementary information) and their effectiveness in few-shot learning.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ). The hidden layers of the SNN learn a transforma-",
      "page": 3
    },
    {
      "caption": "Figure 1: Visualization of the metric learning (MeL) and metric learning with",
      "page": 4
    },
    {
      "caption": "Figure 2: An example of the formation of sample pairs in metric learning",
      "page": 4
    },
    {
      "caption": "Figure 5: ). When using the CREMA-D as the source, the selec-",
      "page": 5
    },
    {
      "caption": "Figure 3: Unweighted average recall (UAR) for in-domain, out-of-domain, and few shot learning methods using various source datasets and an",
      "page": 6
    },
    {
      "caption": "Figure 4: Data distributions learned by the feedforward neural network",
      "page": 6
    },
    {
      "caption": "Figure 5: The distribution of sample selection likelihoods πt(x) (t = 5, 10, 15, 20, 25) using the metric learning with supervision and adaptive sample",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Consistently misclassiﬁed samples from target dataset IEMOCAP-SP",
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotional regulation and depression: A potential mediator between heart and mind",
      "authors": [
        "A Compare",
        "C Zarbo",
        "E Shonin",
        "W Van Gordon",
        "C Marconi"
      ],
      "year": "2014",
      "venue": "Cardiovascular psychiatry and neurology"
    },
    {
      "citation_id": "2",
      "title": "Neural substrates of phonemic perception",
      "authors": [
        "E Liebenthal",
        "J Binder",
        "S Spitzer",
        "E Possing",
        "D Medler"
      ],
      "year": "2005",
      "venue": "Cerebral cortex"
    },
    {
      "citation_id": "3",
      "title": "Real-time tracking of speakers' emotions, states, and traits on mobile platforms",
      "authors": [
        "E Marchi",
        "F Eyben",
        "G Hagerer",
        "B Schuller"
      ],
      "year": "2016",
      "venue": "Real-time tracking of speakers' emotions, states, and traits on mobile platforms"
    },
    {
      "citation_id": "4",
      "title": "The science of self-report: Implications for research and practice",
      "authors": [
        "A Stone",
        "C Bachrach",
        "J Jobe",
        "H Kurtzman",
        "V Cain"
      ],
      "year": "1999",
      "venue": "The science of self-report: Implications for research and practice"
    },
    {
      "citation_id": "5",
      "title": "Annotation and processing of continuous emotional attributes: Challenges and opportunities",
      "authors": [
        "A Metallinou",
        "S Narayanan"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "6",
      "title": "Review on emotion recognition databases",
      "authors": [
        "R Haamer",
        "E Rusadze",
        "I Lsi",
        "T Ahmed",
        "S Escalera",
        "G Anbarjafari"
      ],
      "year": "2017",
      "venue": "Hum. Robot Interact. Theor. Appl"
    },
    {
      "citation_id": "7",
      "title": "Speech emotion recognition in acted and spontaneous context",
      "authors": [
        "F Chenchah",
        "Z Lachiri"
      ],
      "year": "2014",
      "venue": "Procedia Computer Science"
    },
    {
      "citation_id": "8",
      "title": "A survey on transfer learning",
      "authors": [
        "S Pan",
        "Q Yang"
      ],
      "year": "2009",
      "venue": "IEEE Transactions on Knowledge and Data Engineering"
    },
    {
      "citation_id": "9",
      "title": "Progressive neural networks for transfer learning in emotion recognition",
      "authors": [
        "J Gideon",
        "S Khorram",
        "Z Aldeneh",
        "D Dimitriadis",
        "E Provost"
      ],
      "year": "2017",
      "venue": "Progressive neural networks for transfer learning in emotion recognition",
      "arxiv": "arXiv:1706.03256"
    },
    {
      "citation_id": "10",
      "title": "Exploring transfer learning between scripted and spontaneous speech for emotion recognition",
      "authors": [
        "Q Li",
        "T Chaspari"
      ],
      "year": "2019",
      "venue": "2019 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "11",
      "title": "Transfer linear subspace learning for cross-corpus speech emotion recognition",
      "authors": [
        "P Song"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "12",
      "title": "Transferable positive/negative speech emotion recognition via class-wise adversarial domain adaptation",
      "authors": [
        "H Zhou",
        "K Chen"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "13",
      "title": "Few-shot learning with metric-agnostic conditional embeddings",
      "authors": [
        "N Hilliard",
        "L Phillips",
        "S Howland",
        "A Yankov",
        "C Corley",
        "N Hodas"
      ],
      "year": "2018",
      "venue": "Few-shot learning with metric-agnostic conditional embeddings",
      "arxiv": "arXiv:1802.04376"
    },
    {
      "citation_id": "14",
      "title": "Few-shot audio classification with attentional graph neural networks",
      "authors": [
        "S Zhang",
        "Y Qin",
        "K Sun",
        "Y Lin"
      ],
      "year": "2019",
      "venue": "Few-shot audio classification with attentional graph neural networks"
    },
    {
      "citation_id": "15",
      "title": "Few-shot sound event detection",
      "authors": [
        "Y Wang",
        "J Salamon",
        "N Bryan",
        "J Bello"
      ],
      "year": "2020",
      "venue": "2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "16",
      "title": "Automatic speaker recognition with limited data",
      "authors": [
        "R Li",
        "J.-Y Jiang",
        "J Li",
        "C.-C Hsieh",
        "W Wang"
      ],
      "year": "2020",
      "venue": "International Conference on Web Search and Data Mining"
    },
    {
      "citation_id": "17",
      "title": "Zeroshot emotion recognition via affective structural embedding",
      "authors": [
        "C Zhan",
        "D She",
        "S Zhao",
        "M.-M Cheng",
        "J Yang"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "18",
      "title": "Learning to augment expressions for few-shot fine-grained facial expression recognition",
      "authors": [
        "W Wang",
        "Y Fu",
        "Q Sun",
        "T Chen",
        "C Cao",
        "Z Zheng",
        "G Xu",
        "H Qiu",
        "Y.-G Jiang",
        "X Xue"
      ],
      "year": "2020",
      "venue": "Learning to augment expressions for few-shot fine-grained facial expression recognition",
      "arxiv": "arXiv:2001.06144"
    },
    {
      "citation_id": "19",
      "title": "Supervised domain adaptation for emotion recognition from speech",
      "authors": [
        "M Abdelwahab",
        "C Busso"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "20",
      "title": "Sparse autoencoderbased feature transfer learning for speech emotion recognition",
      "authors": [
        "J Deng",
        "Z Zhang",
        "E Marchi",
        "B Schuller"
      ],
      "year": "2013",
      "venue": "2013 Humaine Association Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "21",
      "title": "Adversarial auto-encoders for speech based emotion recognition",
      "authors": [
        "S Sahu",
        "R Gupta",
        "G Sivaraman",
        "W Abdalmageed",
        "C Espy-Wilson"
      ],
      "year": "2017",
      "venue": "Adversarial auto-encoders for speech based emotion recognition"
    },
    {
      "citation_id": "22",
      "title": "Domain adversarial for acoustic emotion recognition",
      "authors": [
        "M Abdelwahab",
        "C Busso"
      ],
      "year": "2018",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "23",
      "title": "Improving cross-corpus speech emotion recognition with adversarial discriminative domain generalization (ADDoG)",
      "authors": [
        "J Gideon",
        "M Mcinnis",
        "E Provost"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "24",
      "title": "Learning representations of emotional speech with deep convolutional generative adversarial networks",
      "authors": [
        "J Chang",
        "S Scherer"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "25",
      "title": "Transfer learning and generalizability in automatic emotion recognition",
      "authors": [
        "K Feng",
        "T Chaspari"
      ],
      "year": "2020",
      "venue": "Frontiers in Computer Science"
    },
    {
      "citation_id": "26",
      "title": "One shot learning of simple visual concepts",
      "authors": [
        "B Lake",
        "R Salakhutdinov",
        "J Gross",
        "J Tenenbaum"
      ],
      "year": "2011",
      "venue": "Proceedings of the annual meeting of the cognitive science society"
    },
    {
      "citation_id": "27",
      "title": "Learning to compare: Relation network for few-shot learning",
      "authors": [
        "F Sung",
        "Y Yang",
        "L Zhang",
        "T Xiang",
        "P Torr",
        "T Hospedales"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "28",
      "title": "Siamese neural networks for one-shot image recognition",
      "authors": [
        "G Koch",
        "R Zemel",
        "R Salakhutdinov"
      ],
      "year": "2015",
      "venue": "ICML deep learning workshop"
    },
    {
      "citation_id": "29",
      "title": "Cross-view asymmetric metric learning for unsupervised person re-identification",
      "authors": [
        "H.-X Yu",
        "A Wu",
        "W.-S Zheng"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "30",
      "title": "Latent relational metric learning via memory-based attention for collaborative ranking",
      "authors": [
        "Y Tay",
        "L Tuan",
        "S Hui"
      ],
      "year": "2018",
      "venue": "2018 World Wide Web Conference"
    },
    {
      "citation_id": "31",
      "title": "Triplet loss based cosine similarity metric learning for textindependent speaker recognition",
      "authors": [
        "S Novoselov",
        "V Shchemelinin",
        "A Shulipa",
        "A Kozlov",
        "I Kremnev"
      ],
      "year": "2018",
      "venue": "Triplet loss based cosine similarity metric learning for textindependent speaker recognition"
    },
    {
      "citation_id": "32",
      "title": "Speech emotion recognition via contrastive loss under siamese networks",
      "authors": [
        "Z Lian",
        "Y Li",
        "J Tao",
        "J Huang"
      ],
      "year": "2018",
      "venue": "Proceedings of the Joint Workshop of the 4th Workshop on Affective Social Multimedia Computing and first Multi-Modal Affective Computing of Large-Scale Multimedia Data"
    },
    {
      "citation_id": "33",
      "title": "Speech emotion recognition from variable-length inputs with triplet loss function",
      "authors": [
        "J Huang",
        "Y Li",
        "J Tao",
        "Z Lian"
      ],
      "year": "2018",
      "venue": "Interspeech"
    },
    {
      "citation_id": "34",
      "title": "Facial expression intensity estimation using siamese and triplet networks",
      "authors": [
        "M Sabri",
        "T Kurita"
      ],
      "year": "2018",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "35",
      "title": "A siamese neural network with modified distance loss for transfer learning in speech emotion recognition",
      "authors": [
        "K Feng",
        "T Chaspari"
      ],
      "year": "2020",
      "venue": "Proceedings of the 3rd Workshop on Affective Content Analysis (AffCon 2020) co-located with Thirty-Forth AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "36",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "37",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PLOS ONE"
    },
    {
      "citation_id": "38",
      "title": "CREMA-D: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "39",
      "title": "The eNTERFACE'05 audio-visual emotion database",
      "authors": [
        "O Martin",
        "I Kotsia",
        "B Macq",
        "I Pitas"
      ],
      "year": "2006",
      "venue": "22nd International Conference on Data Engineering Workshops (ICDEW'06)"
    },
    {
      "citation_id": "40",
      "title": "Opensmile: the munich vrsatile and fast open-source audio feature extractor",
      "authors": [
        "F Eyben",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "41",
      "title": "Analysis of speech features for emotion detection: a review",
      "authors": [
        "R Sudhakar",
        "M Anil"
      ],
      "year": "2015",
      "venue": "2015 International Conference on Computing Communication Control and Automation"
    },
    {
      "citation_id": "42",
      "title": "The interspeech 2009 emotion challenge",
      "authors": [
        "B Schuller",
        "S Steidl",
        "A Batliner"
      ],
      "year": "2009",
      "venue": "Tenth Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "43",
      "title": "API design for machine learning software: experiences from the scikitlearn project",
      "authors": [
        "L Buitinck",
        "G Louppe",
        "M Blondel",
        "F Pedregosa",
        "A Mueller",
        "O Grisel",
        "V Niculae",
        "P Prettenhofer",
        "A Gramfort",
        "J Grobler",
        "R Layton",
        "J Vanderplas",
        "A Joly",
        "B Holt",
        "G Varoquaux"
      ],
      "year": "2013",
      "venue": "ECML PKDD Workshop: Languages for Data Mining and Machine Learning"
    },
    {
      "citation_id": "44",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2015",
      "venue": "CoRR"
    },
    {
      "citation_id": "45",
      "title": "Speech emotion recognition using deep convolutional neural network and discriminant temporal pyramid matching",
      "authors": [
        "S Zhang",
        "S Zhang",
        "T Huang",
        "W Gao"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "46",
      "title": "Adversarial discriminative domain adaptation",
      "authors": [
        "E Tzeng",
        "J Hoffman",
        "K Saenko",
        "T Darrell"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "47",
      "title": "Scipy 1.0: fundamental algorithms for scientific computing in python",
      "authors": [
        "P Virtanen",
        "R Gommers",
        "T Oliphant",
        "M Haberland",
        "T Reddy",
        "D Cournapeau",
        "E Burovski",
        "P Peterson",
        "W Weckesser",
        "J Bright"
      ],
      "year": "2020",
      "venue": "Nature methods"
    },
    {
      "citation_id": "48",
      "title": "Cross-corpus speech emotion recognition based on domain-adaptive least-squares regression",
      "authors": [
        "Y Zong",
        "W Zheng",
        "T Zhang",
        "X Huang"
      ],
      "year": "2016",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "49",
      "title": "Automatic emotion, engagement and cohesion prediction tasks",
      "authors": [
        "A Dhall"
      ],
      "year": "2019",
      "venue": "2019 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "50",
      "title": "Cheavd: a chinese natural emotional audio-visual database",
      "authors": [
        "Y Li",
        "J Tao",
        "L Chao",
        "W Bao",
        "Y Liu"
      ],
      "year": "2017",
      "venue": "Journal of Ambient Intelligence and Humanized Computing"
    },
    {
      "citation_id": "51",
      "title": "Mes-p: an emotional tonal speech dataset in mandarin chinese with distal and proximal labels",
      "authors": [
        "Z Xiao",
        "Y Chen",
        "W Dou",
        "Z Tao",
        "L Chen"
      ],
      "year": "2018",
      "venue": "Mes-p: an emotional tonal speech dataset in mandarin chinese with distal and proximal labels",
      "arxiv": "arXiv:1808.10095"
    }
  ]
}