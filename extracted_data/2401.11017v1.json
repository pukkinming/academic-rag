{
  "paper_id": "2401.11017v1",
  "title": "Revealing Emotional Clusters In Speaker Embeddings: A Contrastive Learning Strategy For Speech Emotion Recognition",
  "published": "2024-01-19T20:31:53Z",
  "authors": [
    "Ismail Rasim Ulgen",
    "Zongyang Du",
    "Carlos Busso",
    "Berrak Sisman"
  ],
  "keywords": [
    "Speech emotion recognition",
    "speaker embeddings",
    "clustering",
    "contrastive learning",
    "multi-task learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speaker embeddings carry valuable emotion-related information, which makes them a promising resource for enhancing speech emotion recognition (SER), especially with limited labeled data. Traditionally, it has been assumed that emotion information is indirectly embedded within speaker embeddings, leading to their under-utilization. Our study reveals a direct and useful link between emotion and state-of-the-art speaker embeddings in the form of intra-speaker clusters. By conducting a thorough clustering analysis, we demonstrate that emotion information can be readily extracted from speaker embeddings. In order to leverage this information, we introduce a novel contrastive pretraining approach applied to emotionunlabeled data for speech emotion recognition. The proposed approach involves the sampling of positive and the negative examples based on the intra-speaker clusters of speaker embeddings. The proposed strategy, which leverages extensive emotion-unlabeled data, leads to a significant improvement in SER performance, whether employed as a standalone pretraining task or integrated into a multi-task pretraining setting.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech emotion recognition remains a challenging task due to its complexity and the subjective nature of emotional expression, compounded by the scarcity of labeled emotional data  [1] . These factors significantly hinder the development of effective SER methods, and encourage researchers to leverage auxiliary knowledge from closely related speech tasks, such as speaker verification (SV)  [2] [3] [4] [5] .\n\nIn contrast to SER, SV benefits from the availability of sufficient labeled data  [6, 7] . Although the tasks of recognizing emotions from speech and verifying speakers differ in their primary objectives, they both revolve around the identification of fundamental voice attributes, including pitch, tone, and phonation patterns. Consequently, speaker verification techniques with robust performance are now being explored as promising tools for enhancing the performance of speech emotion recognition systems  [2, 3, 8]  Emotion information within speaker features has been explored in various emotional speech tasks. Studies  [9] [10] [11]  revealed increased equal error rates in speaker verification for non-matching emotional conditions, highlighting the sensitivity of speaker features to emotional states  [12] . Research by  [13]  demonstrated emotion-related information in speaker embeddings via autoencoder-based reconstruction analysis and emotion classification. This finding was confirmed by  [8] , which also performed reconstruction analysis and used speaker embeddings as SER input features. Recent works  [2, 3]  employed deep speaker embedding networks to transfer knowledge from speaker verification to speech emotion recognition. However, the potential of recent deep speaker embeddings like d-vector  [14]  and ECAPA-TDNN  [15]  in encoding emotional information remains an area that requires comprehensive exploration. Previous studies are limited by the assumption that emotion information is indirectly encoded within speaker embeddings and can be utilized under supervision. In this paper, we aim to explore whether emotion-related information directly resides within the speaker embedding space and find effective ways to leverage this information in SER tasks.\n\nSelf-supervised speech models such as wav2vec2.0  [16]  can leverage large unlabeled speech datasets to enhance supervised SER frameworks  [5, 17, 18] . However, it's important to note that these pre-training objectives were not originally designed for SER, except for  [19]  which incorporated audiovisual features. Additionally, existing pretraining tasks utilized in SER are frame-level tasks while speech emotion is usually formulated as an uttterance-level task. Consequently, a significant gap exists in the field, particularly in the development of an utterance-level, unsupervised pre-training strategy explicitly tailored to SER, exclusively using speech-related features, which is one of the contributions of this paper. This paper marks the first attempt to investigate the direct accessibility of emotion-related information within state-of-the-art deep speaker embeddings. Our analysis reveals distinct intra-speaker clusters that reflect emotional states, suggesting a strong link between speaker and emotion recognition. To utilize this information, we propose a novel pretraining strategy using large-scale, emotion-unlabeled data. This approach employs contrastive learning, forming positive-negative pairs based on speaker embedding clusters, without the need for emotion labels. We apply this strategy both as the primary objective of pretraining and as an additional task for the existing pretraining methods in a multi-task ©2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Arxiv:2401.11017V1 [Eess.As] 19 Jan 2024",
      "text": "Table  1 : Intra-speaker clustering evaluation for emotion classification. setting. Our contributions can be summarized as follows: 1) We reveal readily available emotion information within speaker embeddings; 2) We introduce a unique, utterancelevel contrastive learning approach for SER, without relying on emotion labels; 3) We demonstrate that combination of pretraining tasks in a multi-task setting can further improve SER performance; and 4) Through our proposed training strategy, we enhance a very strong framework, wav2vec2.0, in terms of emotion recognition performance.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Revealing Emotion Clusters In Speaker Embeddings",
      "text": "In this section, we conduct clustering analysis on speaker embeddings to explore emotion discrimination within the speaker embedding space, aiming to establish a direct link between intra-speaker clusters of embeddings and emotional categories. This connection holds significant potential for various SER applications, particularly in harnessing extensive, emotion-unlabeled data. Our analysis is driven by the hypothesis that speaker embeddings, designed to capture voice characteristics, are sensitive to variations in a speaker's voice across different emotional states  [8] [9] [10] 13] , drawing inspiration from studies indicating distinct speaker patterns in different emotional contexts  [9] [10] [11] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Dataset, Speaker Embeddings And Evaluation Metrics",
      "text": "We applied k-means clustering to length-normalized speaker embeddings of using a maximum of 320 different utterances for each speaker within a dataset, using a fixed number of clusters to align with the four categorical emotions: neutral, happiness, sadness, and anger. We selected four widely used labeled emotion datasets: IEMOCAP  [20] , ESD  [21] , CREMA-D  [22] , and RAVDESS  [23] . Our choice of deep speaker embedding networks includes d-vector  [14]  and ECAPA-TDNN  [15] , both trained with metric-based objectives like generalized end-to-end loss and angular margin softmax loss on the voxceleb2 dataset  [7] . We evaluated the alignment between intra-speaker cluster labels and emotion categories using metrics such as Normalized Mutual Information (NMI)  [24] , Adjusted Rand Index (ARI)  [24] , Purity Score  [25] , and Silhouette Score  [26] , averaged over speakers and larger values indicate a stronger alignment.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Clustering And Evaluations",
      "text": "The clustering results are reported in Table  1 . Notably, the ESD dataset consistently demonstrates exceptionally high metrics, indicating a direct alignment between intra-speaker clusters and emotion categories in specific conditions where the utterances are very clean, linguistic content is normalized over emotion categories and emotion intensity tends to be high. While the metrics for other datasets are not as high as in ESD, a meaningful correlation exists across all datasets.\n\nThe IEMOCAP dataset, with challenges like reverberation and overlapping speech, exhibits the lowest metrics, possibly due to variance introduced into speaker embeddings. The distribution of embeddings can be observed in the t-SNE plots in Figure  1 , showing clear separation in the ESD dataset and some distinction in the IEMOCAP dataset. We've plotted t-SNE plots only for ESD and IEMOCAP due to similar trends in other databases. NMI values tend to be higher than ARI values, indicating uneven clustering errors. Higher purity values, compared to lower ARI values, suggest overlaps between specific emotion pairs, hinting at unique relationships between emotion categories. Low silhouette scores are expected due to closely spaced embeddings, aligning with their original goal of grouping speaker utterances together.\n\nIn general, the clustering results validate that speaker embeddings tend to group together for different emotional states in the embedding space due to distinct vocal characteristics for each emotion. The correspondence between emotion categories and intra-speaker clusters is limited in non-ideal conditions possibly due to other factors affecting the speech signal. The results show that even clusters with limited accuracy can serve as effective learning tasks  [27] [28] [29] . Inspired by these findings, we propose a contrastive learning strategy based on the trend of intra-speaker clustering of emotion categories. 3. CONTRASTIVE LEARNING FOR SER this study, we introduce a novel contrastive pretraining strategy without emotion labels, which capitalizes on emotion-related information present in the form of intraspeaker clusters within speaker embeddings. Our approach is based on contrastive learning, a technique well-known for its efficacy across various tasks  [30, 31] . The learning objective tries to maximize the similarity between positive pairs while minimize it for negative pairs. In our approach, positive pairs consist of utterances sampled from the same intra-speaker cluster, likely sharing the same emotion category. In contrast, negative examples are created from different intra-speaker clusters of the same speaker, likely to have different emotion categories given our analysis in Section 2. This setup inherently fosters an utterance-level emotion classification.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Contrastive Pretraining",
      "text": "In the pretraining stage, we obtain intra-speaker clusters of speaker embeddings in a separate process similar to the experiments in Section 2.1, where the only difference is in the number of clusters N since we don't have a prior about categories on emotion-unlabeled data. A variant NT-Xent  [30]  loss is used as an objective in the training:\n\nwhere z i , z j is the positive pair and z i , z k are the negative pairs for a given utterance. The similarity function sim(x, y) = x T y/||x||.||y|| calculates the cosine similarity and τ denotes the temperature parameter. Soft-sampling: For each utterance, we select one positive and N/2 negative utterances based on intra-speaker cluster labels. Due to rough clustering, when sampling the negative examples, we employ a soft-sampling strategy, selecting one negative sample from each of the N/2 intra-speaker clusters that are farthest from the positive cluster center. The model architecture consists of an encoder followed by a contrastive learning head, as shown in Fig.  2 (a) and Fig.  3 .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Contrastive Pretraining For Multi-Task Design",
      "text": "Given the success of the transfer learning from speaker recognition to SER due to their connection, we also propose a multi-task learning (MTL) strategy to utilize available speaker labels. The proposed multi-task framework includes shared encoder layers along with two separate heads: contrastive learning and speaker classification head which can be seen in Figure  2 (b). The contrastive learning head is trained with the proposed objective in Section 3.1; while the speaker classification head is trained with the cross-entropy loss with speaker labels. Along with the multi-task framework, speaker adversarial setting is also experimented, by including a Gradient Reversal Layer (GRL) just before the speaker classification head.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Speech Emotion Recognition",
      "text": "After pretraining on a large-scale, emotion-unlabeled dataset, the model is trained in a supervised manner on a smaller dataset with categorical emotion labels. During supervised training, we introduce a freshly initialized classification head on top of pre-trained encoder layers. This classification head comprises an average pooling layer, a dense projection layer with rectified linear unit (ReLU) activation, and a dense output layer with softmax activation. In this stage, we fine-tune the pre-trained layers in conjunction with the classification head, utilizing cross-entropy loss and emotion labels. The diagram can be seen in the Figure  2 (a).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "In this section, we report the effect of our proposed pretraining strategies with only contrastive loss and multi-task learning on SER performance when dealing with a limited amount of labeled data. We have evaluated our strategies independently and in conjunction with wav2vec2.0 to clearly discern their effect on emotion recognition performance.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Setup",
      "text": "Datasets: During pretraining, we utilize voxceleb2  [7]  as an emotion-unlabeled dataset, known for its diverse emotional contexts  [13] , aligning with our intra-speaker clustering approach. In supervised SER training, we separately employ two labeled emotion datasets, IEMOCAP and CREMA-D. We focus exclusively on Anger, Happiness, Neutral, Sadness, establishing a speaker-independent emotion recognition scenario. For the IEMOCAP corpus, we only use improvised utterances and create 5-fold training and test splits following the leave-one-session-out rule described in  [17]  and  [2] , excluding a small subset from one of the test speakers for validation.\n\nFor CREMA-D, we use training data from 64 speakers, with 8 for validation and 19 for testing.\n\nBaselines: In our basic SER experiments, we establish three baselines: No-pretraining, which involves initializing the model randomly before supervised SER training, without any pretraining; No-pretraining (small), which has a smaller architecture with only 2 transformer layers to assess the impact of overfitting; and Pretraining w/ spk classification which employs pretraining the model with encoder followed by only speaker classification head and loss, similar to the methodology in  [2] . For SER experiments based on wav2vec2.0, we utilize a smaller version of the original wav2vec2.0 as the baseline pretraining method.\n\nModel Architecture & Training: In our pretraining and basic SER experiments, our proposed methods and baseline models, have the same encoder architecture, which is based on wav2vec2.0  [16] . This encoder architecture includes a feature extractor and a transformer encoder, similar to wav2vec2.0, but with a more compact design only 6 transformer layers. The contrastive learning head includes an average pooling layer for frame-level outputs, followed by two dense layers featuring ReLU and tanh activation functions shown in Figure  2 , respectively. The speaker and emotion classification heads have a similar structure as the contrastive head but use softmax activation at the output layer, shown in Figure  2 . In the speaker adversarial setting, we introduce an additional GRL layer after pooling and before the speaker classification head. All the proposed models take the raw waveform of an utterance as input.\n\nDuring pretraining, we segment the input utterances into 4-second intervals and perform offline intra-speaker clustering with N = 20. The models are pretrained for 250k steps using the AdamW optimizer with a batch size of 8. In the supervised SER training that follows, the model undergoes 30 epochs of training with a learning rate of 1e-5, stopping based on the validation accuracy. We repeat each supervised SER training 5 times with different initialization seeds and measure unweighted average recall (UAR) during the evaluation. For the SER experiments based on wav2vec2.0 reported in Table  3 , the baseline wav2vec2.0 1 with 6 transformer layers, is pretrained for 400k steps on voxceleb2. We then fine-tune this model with our strategies on voxceleb2 for an extra 50k steps. The feature extractor and transformer layers of finetuned wav2vec2.0 are utilized in the supervised SER training.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Results And Discussion",
      "text": "According to the results in Table  2 , our proposed contrastive strategy, denoted as Pretraining w/ proposed contrastive, demonstrate a significant improvement in SER compared to cases with no pretraining in both datasets. We note that pretraining with supervised speaker classification also leads to   substantial improvements in both datasets, consistent with findings in  [2] . The proposed multi-task learning approach, denoted as Pretraining w/ proposed MTL, leverages the inherent connection between speaker and emotion recognition, while simultaneously considering intra-speaker variations and obtains the best performance in the IEMOCAP corpus. We observe that speaker adversarial network degrades performance, indicating that trying to remove speaker information has a negative impact and supports the connection between speaker and emotion recognition. In CREMA-D, the speaker classification baseline performs exceptionally well, possibly due to the presence of normalized linguistic content, creating ideal conditions for discriminating emotions through speaker embeddings, as discussed in Section 2. Overall, these results underscore the effectiveness of our multi-task learning method and highlight the strong relationship between emotion and speaker recognition.\n\nIn Table  3 , baseline wav2vec2.0 model, pretrained with voxceleb2, performs impressively well as a pretraining method for SER, underscoring its effectiveness. Fine-tuning this baseline with our contrastive learning strategy, FT wav2vec2.0 w/ proposed contrastive, seems leading to minor improvements in both datasets. Fine-tuning wav2vec2.0 with our proposed multi-task setting, FT wav2vec2.0 w/ proposed MTL, yields substantial enhancement, highlighting the effectiveness of our approach.\n\n5. CONCLUSION Our research reveals the potential of speaker embeddings for enhancing SER task, even with limited labeled data. Our study establishes a direct link between emotions and stateof-the-art speaker embeddings through intra-speaker clusters. Our novel contrastive pretraining approach on emotionunlabeled datasets, based on these clusters, significantly improves SER performance, whether used alone or in multi-task settings. Our findings not only advance our understanding of speaker embeddings and emotions but also provide practical solutions for data scarcity in SER. As a future work, we intend to extend the analysis of emotion information in speaker embeddings, analyzing other factors which potentially affect the appearance of that information.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Visualization of intra-speaker clusters in two datasets,",
      "page": 2
    },
    {
      "caption": "Figure 1: , showing clear separation in the ESD",
      "page": 2
    },
    {
      "caption": "Figure 2: a) Proposed contrastive pre-training and SER training,",
      "page": 3
    },
    {
      "caption": "Figure 3: The encoder architecture utilized in the networks.",
      "page": 3
    },
    {
      "caption": "Figure 3: 3.2. Contrastive Pretraining for Multi-Task Design",
      "page": 3
    },
    {
      "caption": "Figure 2: , respectively. The speaker",
      "page": 4
    },
    {
      "caption": "Figure 2: In the speaker adversarial setting,",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Notably, the goriesandintra-speakerclustersislimitedinnon-idealcondi-",
      "data": [
        {
          "d-vector": "NMI [0,1] ↑\nARI [0,1] ↑\nPurity [0,1] ↑\nSilhoutte [-1,1] ↑",
          "ECAPA-TDNN": "NMI [0,1] ↑\nARI [0,1] ↑\nPurity [0,1] ↑\nSilhoutte [-1,1] ↑"
        },
        {
          "d-vector": "0.76\n0.72\n0.89\n0.14\n0.29\n0.21\n0.66\n0.01\n0.43\n0.39\n0.63\n0.07\n0.59\n0.38\n0.67\n0.14",
          "ECAPA-TDNN": "0.89\n0.91\n0.97\n0.13\n0.31\n0.25\n0.67\n0.01\n0.36\n0.27\n0.57\n0.04\n0.51\n0.28\n0.62\n0.05"
        }
      ],
      "page": 2
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "The ambiguous world of emotion representation",
      "authors": [
        "Vidhyasaharan Sethu",
        "Emily Provost",
        "Julien Epps",
        "Carlos Busso",
        "Nicholas Cummins",
        "Shrikanth Narayanan"
      ],
      "year": "2019",
      "venue": "ArXiv"
    },
    {
      "citation_id": "3",
      "title": "X-vectors meet emotions: A study on dependencies between emotion and speaker recognition",
      "authors": [
        "R Pappagari",
        "Tianzi Wang",
        "Jesús Villalba",
        "Nanxin Chen",
        "Najim Dehak"
      ],
      "year": "2020",
      "venue": "ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "4",
      "title": "Improved speech emotion recognition using transfer learning and spectrogram augmentation",
      "authors": [
        "Sarala Padi",
        "Omid Seyed",
        "Dinesh Sadjadi",
        "Ram Manocha",
        "Sriram"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "5",
      "title": "A transfer learning method for speech emotion recognition from automatic speech recognition",
      "authors": [
        "Sitong Zhou",
        "S Homayoon",
        "Beigi"
      ],
      "year": "2008",
      "venue": "ArXiv"
    },
    {
      "citation_id": "6",
      "title": "Emotion Recognition from Speech Using wav2vec 2.0 Embeddings",
      "authors": [
        "Leonardo Pepino",
        "Pablo Riera",
        "Luciana Ferrer"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "7",
      "title": "Voxceleb: A large-scale speaker identification dataset",
      "authors": [
        "Arsha Nagrani",
        "Son Chung",
        "Andrew Zisserman"
      ],
      "year": "2017",
      "venue": "Voxceleb: A large-scale speaker identification dataset"
    },
    {
      "citation_id": "8",
      "title": "Voxceleb2: Deep speaker recognition",
      "authors": [
        "Son Joon",
        "Arsha Chung",
        "Andrew Nagrani",
        "Zisserman"
      ],
      "year": "2018",
      "venue": "Voxceleb2: Deep speaker recognition"
    },
    {
      "citation_id": "9",
      "title": "You're not you when you're angry: Robust emotion features emerge by recognizing speakers",
      "authors": [
        "Zakaria Aldeneh",
        "Emily Provost"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "10",
      "title": "A study of speaker verification performance with expressive speech",
      "authors": [
        "Srinivas Parthasarathy",
        "Chunlei Zhang",
        "H John",
        "Carlos Hansen",
        "Busso"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "11",
      "title": "Predicting speaker recognition reliability by considering emotional content",
      "authors": [
        "Srinivas Parthasarathy",
        "Carlos Busso"
      ],
      "year": "2017",
      "venue": "Seventh International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "12",
      "title": "Exploring the intersection between speaker verification and emotion recognition",
      "authors": [
        "Michelle Bancroft",
        "Reza Lotfian",
        "H John",
        "Carlos Hansen",
        "Busso"
      ],
      "year": "2019",
      "venue": "2019 8th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW)"
    },
    {
      "citation_id": "13",
      "title": "Front-end factor analysis for speaker verification",
      "authors": [
        "Najim Dehak",
        "Patrick Kenny",
        "Réda Dehak",
        "Pierre Dumouchel",
        "Pierre Ouellet"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "14",
      "title": "Disentangling style factors from speaker representations",
      "authors": [
        "Jennifer Williams",
        "Simon King"
      ],
      "year": "2019",
      "venue": "Disentangling style factors from speaker representations"
    },
    {
      "citation_id": "15",
      "title": "Generalized end-to-end loss for speaker verification",
      "authors": [
        "Li Wan",
        "Quan Wang",
        "Alan Papir",
        "Ignacio Lopez-Moreno"
      ],
      "year": "2017",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "16",
      "title": "ECAPA-TDNN: Emphasized Channel Attention, Propagation and Aggregation in TDNN Based Speaker Verification",
      "authors": [
        "Brecht Desplanques",
        "Jenthe Thienpondt",
        "Kris Demuynck"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "17",
      "title": "Wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Henry Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Proceedings of the 34th International Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "18",
      "title": "Speech emotion recognition using self-supervised features",
      "authors": [
        "Edmilson Da",
        "Silva Morais",
        "Ron Hoory",
        "Weizhong Zhu",
        "Itai Gat",
        "Matheus Damasceno",
        "Hagai Aronowitz"
      ],
      "year": "2022",
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "19",
      "title": "Contrastive unsupervised learning for speech emotion recognition",
      "authors": [
        "Mao Li",
        "Bo Yang",
        "Joshua Levy",
        "Andreas Stolcke",
        "Viktor Rozgic",
        "Spyros Matsoukas",
        "Constantinos Papayiannis",
        "Daniel Bone",
        "Chao Wang"
      ],
      "year": "2021",
      "venue": "ICASSP 2021 -2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "20",
      "title": "Improving Speech Emotion Recognition Using Self-Supervised Learning with Domain-Specific Audiovisual Tasks",
      "authors": [
        "Lucas Goncalves",
        "Carlos Busso"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "21",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "22",
      "title": "Emotional voice conversion: Theory, databases and esd",
      "authors": [
        "Kun Zhou",
        "Berrak Sisman",
        "Rui Liu",
        "Haizhou Li"
      ],
      "year": "2022",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "23",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "Houwei Cao",
        "David Cooper",
        "Michael Keutmann",
        "Ruben Gur",
        "Ani Nenkova",
        "Ragini Verma"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "24",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "R Steven",
        "Frank Livingstone",
        "Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "25",
      "title": "Information theoretic measures for clusterings comparison: Variants, properties, normalization and correction for chance",
      "authors": [
        "Xuan Vinh Nguyen",
        "Julien Epps",
        "James Bailey"
      ],
      "year": "2010",
      "venue": "J. Mach. Learn. Res"
    },
    {
      "citation_id": "26",
      "title": "A comparison of internal and external cluster validation indexes",
      "authors": [
        "Eréndira Rendón",
        "M Itzel",
        "Citlalih Abundez",
        "Sergio Gutierrez",
        "Alejandra Zagal",
        "Elvia Arizmendi",
        "H Quiroz",
        "Arzate"
      ],
      "year": "2011",
      "venue": "Proceedings of the 2011 American Conference on Applied Mathematics and the 5th WSEAS International Conference on Computer Engineering and Applications"
    },
    {
      "citation_id": "27",
      "title": "Silhouettes: a graphical aid to the interpretation and validation of cluster analysis",
      "authors": [
        "J Peter",
        "Rousseeuw"
      ],
      "year": "1987",
      "venue": "Journal of Computational and Applied Mathematics"
    },
    {
      "citation_id": "28",
      "title": "Hubert: Selfsupervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "Wei-Ning Hsu",
        "Benjamin Bolte",
        "Hubert Yao-Hung",
        "Kushal Tsai",
        "Ruslan Lakhotia",
        "Abdelrahman Salakhutdinov",
        "Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "29",
      "title": "Self-supervised learning with cluster-aware-dino for high-performance robust speaker verification",
      "authors": [
        "Bing Han",
        "Zhengyang Chen",
        "Yanmin Qian"
      ],
      "year": "2023",
      "venue": "ArXiv"
    },
    {
      "citation_id": "30",
      "title": "Self-supervised speaker recognition with loss-gated learning",
      "authors": [
        "Ruijie Tao",
        "Kong-Aik Lee",
        "Rohan Kumar Das",
        "Ville Hautamaki",
        "Haizhou Li"
      ],
      "year": "2021",
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "31",
      "title": "A simple framework for contrastive learning of visual representations",
      "authors": [
        "Ting Chen",
        "Simon Kornblith",
        "Mohammad Norouzi",
        "Geoffrey Hinton"
      ],
      "year": "2020",
      "venue": "Proceedings of the 37th International Conference on Machine Learning"
    },
    {
      "citation_id": "32",
      "title": "Momentum contrast for unsupervised visual representation learning",
      "authors": [
        "Kaiming He",
        "Haoqi Fan",
        "Yuxin Wu",
        "Saining Xie",
        "Ross Girshick"
      ],
      "year": "2019",
      "venue": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    }
  ]
}