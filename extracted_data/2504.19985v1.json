{
  "paper_id": "2504.19985v1",
  "title": "Real-Time Imitation Of Human Head Motions, Blinks And Emotions By Nao Robot: A Closed-Loop Approach",
  "published": "2025-04-28T17:01:54Z",
  "authors": [
    "Keyhan Rayati",
    "Amirhossein Feizi",
    "Alireza Beigy",
    "Pourya Shahverdi",
    "Mehdi Tale Masouleh",
    "Ahmad Kalhor"
  ],
  "keywords": [
    "Imitation",
    "Humanoid robot",
    "MediaPipe",
    "Deepface",
    "Emotion recognition Mediapipe"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This paper introduces a novel approach for enabling real-time imitation of human head motion by a Nao robot, with a primary focus on elevating human-robot interactions. By using the robust capabilities of the MediaPipe as a computer vision library and the DeepFace as an emotion recognition library, this research endeavors to capture the subtleties of human head motion, including blink actions and emotional expressions, and seamlessly incorporate these indicators into the robot's responses. The result is a comprehensive framework which facilitates precise head imitation within human-robot interactions, utilizing a closed-loop approach that involves gathering real-time feedback from the robot's imitation performance. This feedback loop ensures a high degree of accuracy in modeling head motion, as evidenced by an impressive R2 score of 96.3 for pitch and 98.9 for yaw. Notably, the proposed approach holds promise in improving communication for children with autism, offering them a valuable tool for more effective interaction. In essence, proposed work explores the integration of real-time head imitation and realtime emotion recognition to enhance human-robot interactions, with potential benefits for individuals with unique communication needs.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "The field of robotics has come a long way in recent years, with significant advancements in the development of humanoid robots. Humanoid robots are mostly designed to mimic human movement, behavior, and communication, making them ideal for a wide range of applications such as healthcare, education, entertainment, and more. In recent years, there has been a growing interest in imitating humanoid robots as researchers seek to improve the functionality and performance of these machines. Robot imitation involves three primary stages, namely, observation, representation, and reproduction. In the observation stage, Riley et al.  [1]  employed an advanced 3D vision system. This system utilized both external cameras and head-mounted cameras to capture human movements meticulously. The approach involved strategically placing colored markers on the human body, and subsequently, the precise positions of these markers were calculated and meticulously recorded. Similar marker-based visual capture methods are documented in  [2] . Furthermore, wearable devices like \"Xsens MVN\" and \"ShapeTape\" have gained prominence as effective tools for capturing motion  [3] -  [5] . Additionally, markerless visual capture systems, exemplified by the Microsoft Kinect sensor  [6] -  [11] , offer a cost-effective and user-friendly alternative. Such methods achieve robot imitation by harnessing the skeleton data provided by the Kinect. Different techniques exist for translating human motions into movements for humanoid robots. One strategy employs a geometrical analytical method grounded in link vectors within the robot's arms, hands, and head  [12] . Moreover, inverse kinematic motion models can also map human gestures onto robot actions. These models incorporate the constraints and capabilities of the robot's joints and actuators, facilitating the generation of trajectories that are both precise and faithful to reality  [1] ,  [13] -  [17] .\n\nThe approach proposed in this paper employs the robust capabilities of MediaPipe, a widely recognized and powerful computer vision framework. For the purpose of this study, MediaPipe is used to capture and analyze the intricate details of human head motion, including pitch and yaw angles, providing the essential input for the robotic imitation process. Leveraging the strengths of MediaPipe, this research extends its potential applications into the realm of humanrobot interaction. Moreover, the proposed approach enhances the robot's ability to seamlessly mimic human head movements by incorporating blink detection using MediaPipe. This additional feature enriches the capabilities of the proposed real-time imitation system and opens up new avenues for a more nuanced and interactive human-robot engagement  [18] . Alongside exploring MediaPipe's capabilities, the DeepFace  [19]  library is seamlessly integrated to make this research more comprehensive. DeepFace, a remarkable advancement in computer vision and facial recognition technology, represents a significant advancement in computer vision and facial recognition technology. Developed by Facebook's AI Research (FAIR) team, it is a deep learning-based facial recognition system designed to excel in the challenging task of face verification. DeepFace's core innovation lies in its ability to create a high-dimensional feature vector representation of facial images. This feature vector enables it to distinguish between different faces with remarkable accuracy, rivaling human-level performance. This breakthrough has paved the way for various applications in facial recognition, including the ability to detect and interpret human emotions, a capability which is integrated into the proposed approach in this paper for real-time imitation system of the Nao robot  [19] . In order to summarize, the contribution of this research revolves around achieving three key stages which are central to this work.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Mediapipe",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Proposed Framework",
      "text": "Naoqi API Data HTTP Request The primary goal consists of executing precise human head imitation through angle calculations. The study focuses on pioneering real-time blink detection and imitation techniques.\n\nLastly, an emotion response system is introduced in order to enhance human-robot interactions. As shown in Fig.  1 , these three stages collectively form the essence of the comprehensive real-time imitation system, poised to revolutionize humanrobot interaction.\n\nThe subsequent sections of this paper provide a detailed account of the three main phases of the research. In the following section, the implementation of the framework for head imitation, blink detection, and emotion recognition is discussed. Afterward, the results of the comprehensive testing process for each component are presented. Finally, the paper concludes by summarizing the findings and discussing their potential implications.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Ii. Human Imitation And Interaction Framework",
      "text": "In this section, a comprehensive framework for human imitation and interaction is presented, which leverages the capabilities of the MediaPipe for pose and face mesh detection, coupled with the Naoqi API. The framework aims to analyze various human behaviors such as head movements, eye blinks, and emotions, and then imitate or respond to these behaviors using the Nao Robot. The details of each component of this framework, challenges faced, and the strategies used to overcome them are discussed in the subsequent subsections.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Environment Setup",
      "text": "In this study, the MediaPipe version 0.10.2 is utilized. The framework offers functionalities for both face mesh and pose detection. The head rotation angles are determined based on eye and nose landmarks by a pose detection approach. Also, the face-mesh capability is employed to discern the inner and outer eye landmarks to track blinking movements.\n\nFig.  2  shows the data flow within the proposed framework. This data comprises landmarks from pose and face mesh, sourced from MediaPipe, which are then processed within the proposed framework. Then, the analyzed information including the yaw and pitch of the robot's head, eye status, and facial emotions is relayed to the Naoqi API. However, a challenge emerged during setup. MediaPipe operates on Python 3.10, while the Naoqi API runs on Python 2.7. In order to bridge this compatibility gap, some specific components are designed, which are integrated via HTTP requests. Additionally, it is noteworthy that the implementation leverages a simple camera or webcam, such as those found on a laptop, for data acquisition. Importantly, this approach is extendable to any other standard camera, contributing to the accessibility and adaptability of the proposed framework.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Head Imitation",
      "text": "The head imitation component aims at computing the subject's head's yaw and pitch orientation. By extracting face landmarks and plotting the landmarks, the eyes are observed to be aligned along the y-axis, which means the eyes have the same y and different x and z.\n\n1) Head Yaw Estimation: The estimation of head yaw is formulated as an angular measurement based on a predefined baseline vector and the vector formed by the subject's landmarks corresponding to the left and right eyes. Let Vbaseline be the baseline vector corresponding to a neutral head pose. Given the 3D coordinates of the subject's left and right eyes as P left and P right , respectively, In the approach proposed by this paper, the eye-based vector V eye is calculated as follows:\n\nThe vector V eye is then normalized to a unit length:\n\nThe angle θ between V baseline and Veye is obtained using the dot and cross products as:\n\nThe axis of rotation a is calculated as follows:\n\nA rotation vector r is then formed as:\n\nThis rotation vector is converted into Euler angles to extract the yaw(ϕ) component. In order to ensure robustness in real-world scenarios, ϕ is capped within the range [-119.5 • , 119.5 • ].\n\n2) Head Pitch Estimation: The head's pitch is determined based on the vector's orientation formed by the eyes' midpoint and the nose relative to a baseline vector corresponding to a neutral head pose. Let Vbaseline be the baseline vector, normalized to unit length. The 3D coordinates for the midpoint between the subject's left and right eyes M eye and the subject's nose P nose are given. Firstly, M eye is calculated as the mean of the left and right eye coordinates:\n\nThe vector Vnew from the mid-point of the eyes to the nose is then formulated as:\n\nThis vector is normalized to a unit length:\n\nThe angle α between Vbaseline and Vnew is calculated using the dot product as:\n\nThe axis of rotation a is calculated using the cross product:\n\nA rotation vector r is then formed as:\n\nThis rotation vector is converted into Euler angles, to extract the pitch(ψ) component. The allowable pitch angles are dependent on the current yaw angle value. To predict rotation angles within specified collision limits, a Support Vector Regression (SVR) model is trained using known values provided by the Nao robot manufacturer. This training enables the model to learn a mapping function, which predicts rotation angles for scenarios where exact values are unknown or fall between the provided training data, ensuring stability and preventing movements beyond the robot's defined collision thresholds.\n\nFor the sake of enhancing the robustness of the estimate against outliers, ψ is confined within a range [min, max]. The boundary conditions for ψ are defined as: The final pitch value is defined as:\n\nAfter this initial calculation of angles, the proposed framework enters a closed-loop system, as depicted in Fig.  3 .\n\nIn this closed-loop configuration, the observed head motions are not only commanded to be replicated by the Nao robot, but feedback is also continuously received from the robot's sensors, which monitor the accuracy and synchronization of the imitation. This feedback mechanism plays a crucial role in refining the precision of the proposed real-time imitation, ensuring that the robot's head movements closely mirror those of the human operator.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "C. Blink Detection",
      "text": "In order to improve the human imitation system, a blink detection framework is introduced, which determines the open or closed state of a subject's eyes. Using the face mesh model provided by MediaPipe, a geometry-based approach is employed to estimate the eye status. At least, blink estimation is applied to the Nao Robot, as shown in Fig.  4  1) Defining Eye Regions: Analyzing the face mesh, the distances between the top and bottom points of the eye's inner and outer sections are computed. these distances are denoted as d inner and d outer , respectively:\n\nwhere the it, ib, ot, and ob stand for the inner top, inner bottom, outer top, and outer bottom eye points, respectively. Finally, the eye ratio R, which will be used for defining the blinking threshold, can calculated as follows: 2) Blink Thresholding: A blink is detected when the computed eye ratio R is higher than a predefined threshold T . This threshold is empirically determined because of differences in eye shape:\n\nThis approach is applied independently to both eyes, allowing for the identification of blinks for each eye.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "D. Emotion Detection",
      "text": "The DeepFace library is seamlessly integrated into the proposed framework, employing it for real-time facial emotion detection. The webcam images are analyzed frame by frame, ensuring precision in the observations. In order to minimize noise and errors in the detection process, emotions over a sequence of 10 frames were collected, and then a majority feelings approach was adopted. As depicted in Figure  5 , the method allowed to effectively identify the prevailing emotions, encompassing a wide range of human expressions, including anger, fear, neutrality, sadness, disgust, happiness, and surprise. These detected emotions are subsequently associated with prepared textual responses for the Nao robot. Leveraging the robot's text-to-speech capabilities, the robot dynamically communicated these responses, enhancing its capacity to engage with users in real-time, tailored to their evolving emotional states. By dynamically responding to emotional signals, this innovative approach could potentially assist children with autism in enhancing their interactive experiences. Tailoring the robot's responses to their evolving emotional states, opens up the possibility of more effective and engaging interactions, providing valuable support and opportunities for these children.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iii. Results And Experiments",
      "text": "This section presents the results from the performed experiment, which was concerned with a Nao robot's ability to mimic human head motion in real-time accurately. Several experiments were run to assess the precision and efficacy of proposed strategy.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Test 1: Head Motion Imitation",
      "text": "In initial experiment, how well the robot's head movements replicated human subject motions was evaluated. The procedure involved the following steps:\n\n1) Data Collection: Head motion data is captured by manually moving the human head and recording the robot's head angles simultaneously. 2) Angle Calculation: Using proposed method, the yaw and pitch angles for the human subject are computed. 3) Data Comparison: The calculated angles to assess the degree of imitation by the robot are compared.\n\nIt can be observed from Fig.  6  that the plot illustrates the yaw angles of the human head and the robot's head over time, enabling a visual comparison of their movements. In order to assess the accuracy of imitation, the R-squared (R²) value was measured, which yielded an impressive accuracy score of 98.9 for yaw angles.   of differences in yaw angles between the human and robot heads over time, serving as a measure of imitation accuracy. In Fig.  8 , the pitch angles of the human head and the robot's head are illustrated, facilitating a visual assessment of their alignment. The distribution of both errors is calculated via 250 frames of the experiment we had. The distribution of differences in pitch angles between the human and robot heads over time reveals compelling insights into the accuracy of the proposed imitation method, which can be observed from Fig.  9 . In order to assess the accuracy of imitation, we measured the R-squared (R²) value, which yielded an impressive accuracy score of 98.9 for pitch angles. Performed analysis reveals that the majority of pitch angle differences fall within a range of -6 to +8 degrees. This range signifies that the robot's imitation closely mirrors the pitch movements of the human head with an acceptable level of variance. Furthermore, the distribution exhibits a near-normal distribution, with a noteworthy observation. The average pitch angle difference between the human and robot heads is approximately -2 degrees, indicating a slight offset of about 2 degrees from perfect alignment. This offset, while small, suggests a consistent trend in the robot's pitch angle behavior that can be further explored and potentially calibrated for even greater accuracy.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Test 2: Blink Imitation",
      "text": "A test focused on blink actions was conducted further to evaluate the imitation capabilities of the Nao robot. The experiment proceeded as follows:\n\n1) Data Collection: Utilizing a webcam, a series of blinks for a total of 50 occurrences was initiated. Throughout these blink actions, the robot's response was closely monitored to ascertain if it imitated them. 2) Results: In 48 out of the 50 blink occurrences, the robot exhibited a corresponding blink action, which exhibits its capability to imitate effectively human blinks in realtime.\n\n3) Noise Handling: The 48 successful blink imitations were attributed to the precision of the proposed method in detecting and replicating blink actions initiated by the human subject. However, it is noteworthy that the remaining 2 blink attempts failed due to their extremely brief duration, lasting less than 2 frames. In these instances, the rapid eye movement was considered noise by the Imitation approach of the Nao Robot, leading to a non-triggering of the robot's blink response. Test 2 demonstrated the Nao robot's ability to replicate blink actions initiated by a human subject accurately. In order to further evaluate the imitation capabilities of the Nao robot, a test focused on blink actions was conducted, and the test with an additional four participants was replicated to ensure the consistency of the findings. The experiment proceeded as follows. In addition, the same experiment with four more participants was conducted, and these subsequent tests yielded similar results. This consistency across multiple trials further shows the robustness and reliability of the framework. The high success rate of the proposed blink imitations algorithm, achieved through proposed method's precision, underscores the reliability and real-time capabilities of the framework. This experiment confirms the effectiveness of the proposed approach in imitating blink actions, a crucial aspect of human head motion imitation. The results from Test 2 further support the accuracy and real-time performance of the Imitation approach of the Nao Robot.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Test 3: Emotion Detection And Response",
      "text": "In this experiment, the Nao robot's ability to detect and respond to human emotions in real-time. The methodology involved the following steps:\n\n1) Emotion Expression: A human subject was positioned in front of the webcam and prompted to express a range of emotions, including happiness, sadness, anger, surprise, and fear through natural facial expressions.\n\n2) Emotion Detection: Real-time emotion detection was facilitated by the DeepFace library, enabling precise discernment and quantification of the human subject's emotions at any given moment. 3) Robot Response: The Nao robot, equipped with textto-speech capabilities, continuously analyzed the human subject's emotional state. The robot formulated and delivered contextually tailored verbal responses based on the detected emotions. Test 3 yielded highly promising outcomes, affirming the Nao robot's remarkable real-time emotion detection and response proficiency. Notably, the robot's recognition of expressions of happiness was a standout feature. Beyond happiness, the Nao robot effectively identified and responded to a spectrum of other emotions, encompassing sadness, anger, surprise, and fear. Moreover, the robot's responses were contextually relevant and attuned to the expressed emotions, enhancing the overall quality of human-robot interaction.\n\nIn summary, from the performed experimentation, it can be inferred that the Nao robot, equipped with the proposed realtime imitation approach, excels in multiple facets of humanrobot interaction. Across a series of tests encompassing head motion imitation, blink actions, and emotion detection and response, the Nao robot consistently showcased remarkable accuracy and adaptability. Notably, it accurately emulated human head motions, aligning closely with yaw and pitch angles. Additionally, the robot's capability to imitate blink actions, particularly in real-world scenarios, was highly reliable. Furthermore, the robot exhibited a deep understanding of human emotions, with the ability to detect and respond to a diverse range of emotional expressions. These findings collectively affirm the effectiveness and versatility of the framework, paving the way for enhanced applications in the realm of human-robot interaction.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Iv. Conclusions",
      "text": "This research has embarked on a journey to enhance human-robot interaction through a comprehensive framework designed for real-time imitation of human head motion by a Nao robot. This journey unfolded across three pivotal phases, each contributing to realizing a versatile and adaptable system. In the initial phase, a novel method was proposed utilizing captured webcam video frames and the robust capabilities of the MediaPipe library. This approach allowed to precisely calculate and replicate head motion angles in real-time, effectively closing the loop between the human operator and the Nao robot. The resulting system exhibited exceptional accuracy, providing valuable insights into the robot's imitation capabilities. The second phase focused on the intricacies of human blink actions. Leveraging Mediapipe's landmark tracking, a technique to detect and replicate blink movements initiated by the human subject was developed. The outcomes were resoundingly successful, highlighting the reliability of the proposed method in replicating real-world scenarios and underscoring the potential applications in human-robot interaction. In the third and final phase, the realm of human emotion tracking was delved into. Utilizing the capabilities of Deepface, the Nao robot was able to interpret human emotions and respond accordingly. This transformative addition to the proposed framework opened up new horizons for interactive experiences, with the robot adeptly responding to a spectrum of emotions, including happiness, sadness, anger, surprise, and fear. These three pivotal phases collectively culminated in creating a complete head imitation framework. This framework offers researchers and robot enthusiasts a powerful tool that seamlessly integrates head motion imitation, blink replication, and emotion recognition, all in real-time. While a broad range of potential applications is envisioned, one particularly notable avenue is the potential benefit to children with autism. The framework has the capacity to enhance their interactive experiences, fostering more effective communication.",
      "page_start": 6,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Framework overview: Real-time head motion imitation and emotion",
      "page": 1
    },
    {
      "caption": "Figure 2: Interaction diagram of MediaPipe, Naoqi API, and framework.",
      "page": 2
    },
    {
      "caption": "Figure 2: shows the data flow within the proposed framework.",
      "page": 2
    },
    {
      "caption": "Figure 3: Real-Time Imitation System’s Closed-Loop. (a) Capturing User’s",
      "page": 3
    },
    {
      "caption": "Figure 4: Eye closure interactions, (a) Both eyes open, (b) Right eye closed,",
      "page": 4
    },
    {
      "caption": "Figure 3: In this closed-loop configuration, the observed head motions",
      "page": 4
    },
    {
      "caption": "Figure 4: 1) Defining Eye Regions: Analyzing the face mesh, the",
      "page": 4
    },
    {
      "caption": "Figure 5: Emotion-to-Response Mapping for the Nao robot using DeepFace",
      "page": 4
    },
    {
      "caption": "Figure 5: , the method allowed to effectively identify the prevailing",
      "page": 4
    },
    {
      "caption": "Figure 6: Comparison of human and robot yaw angles.",
      "page": 5
    },
    {
      "caption": "Figure 7: Distribution of yaw angle differences.",
      "page": 5
    },
    {
      "caption": "Figure 6: that the plot illustrates the",
      "page": 5
    },
    {
      "caption": "Figure 7: depicts the distribution",
      "page": 5
    },
    {
      "caption": "Figure 8: Comparison of human and robot pitch angles.",
      "page": 5
    },
    {
      "caption": "Figure 9: Distribution of pitch angle differences.",
      "page": 5
    },
    {
      "caption": "Figure 8: , the pitch angles of the human head and the robot’s",
      "page": 5
    },
    {
      "caption": "Figure 9: In order to assess the accuracy",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(a)": "(c) (d)",
          "(b)": ""
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Enabling real-time full-body imitation: a natural way of transferring human movement to humanoids",
      "authors": [
        "M Riley",
        "A Ude",
        "K Wade",
        "C Atkeson"
      ],
      "year": "2003",
      "venue": "2003 IEEE International Conference on Robotics and Automation"
    },
    {
      "citation_id": "2",
      "title": "Image recognition and force measurement application in the humanoid robot imitation",
      "authors": [
        "H.-Y Liu",
        "W.-J Wang",
        "R.-J Wang",
        "C.-W Tung",
        "P.-J Wang",
        "I.-P Chang"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "3",
      "title": "Position teaching of a robot arm by demonstration with a wearable input device",
      "authors": [
        "J Aleotti",
        "A Skoglund",
        "T Duckett"
      ],
      "year": "2004",
      "venue": "Position teaching of a robot arm by demonstration with a wearable input device"
    },
    {
      "citation_id": "4",
      "title": "Real-time mimicking of human body motion by a humanoid robot",
      "authors": [
        "G Cheng",
        "Y Kuniyoshi"
      ],
      "year": "2000",
      "venue": "Real-time mimicking of human body motion by a humanoid robot"
    },
    {
      "citation_id": "5",
      "title": "Whole-body human-to-humanoid motion transfer",
      "authors": [
        "N Naksuk",
        "C Lee",
        "S Rietdyk"
      ],
      "year": "2005",
      "venue": "5th IEEE-RAS International Conference on Humanoid Robots"
    },
    {
      "citation_id": "6",
      "title": "A real-time human imitation system",
      "authors": [
        "F Wang",
        "C Tang",
        "Y Ou",
        "Y Xu"
      ],
      "year": "2012",
      "venue": "Proceedings of the 10th World Congress on Intelligent Control and Automation"
    },
    {
      "citation_id": "7",
      "title": "Whole-body humanoid robot imitation with pose similarity evaluation",
      "authors": [
        "J Lei",
        "M Song",
        "Z.-N Li",
        "C Chen"
      ],
      "year": "2015",
      "venue": "Signal Processing"
    },
    {
      "citation_id": "8",
      "title": "Nafome: Nao follows metracking, reproduction and simulation of human motion",
      "authors": [
        "S Franz",
        "R Nolte-Holube",
        "F Wallhoff"
      ],
      "year": "2013",
      "venue": "Jade University of Applied Sciences"
    },
    {
      "citation_id": "9",
      "title": "Recognition of human motions for imitation and control of a humanoid robot",
      "authors": [
        "F Zuher",
        "R Romero"
      ],
      "year": "2012",
      "venue": "2012 Brazilian Robotics Symposium and Latin American Robotics Symposium"
    },
    {
      "citation_id": "10",
      "title": "Support changes during online human motion imitation by a humanoid robot using task specification",
      "authors": [
        "L Poubel",
        "S Sakka",
        "D Ćehajić",
        "D Creusot"
      ],
      "year": "2014",
      "venue": "2014 IEEE International Conference on Robotics and Automation"
    },
    {
      "citation_id": "11",
      "title": "Telepresence using the kinect sensor and the nao robot",
      "authors": [
        "J Avalos",
        "S Cortez",
        "K Vasquez",
        "V Murray",
        "O Ramos"
      ],
      "year": "2016",
      "venue": "2016 IEEE 7th Latin American Symposium on Circuits and Systems (LASCAS)"
    },
    {
      "citation_id": "12",
      "title": "A real-time upperbody robot imitation system",
      "authors": [
        "Z Zhang",
        "Y Niu",
        "L Kong",
        "S Lin",
        "H Wang"
      ],
      "year": "2019",
      "venue": "International Journal of Robotics and Control"
    },
    {
      "citation_id": "13",
      "title": "Push recovery for nao humanoid robot",
      "authors": [
        "P Ghassemi",
        "M Masouleh",
        "A Kalhor"
      ],
      "year": "2014",
      "venue": "2014 Second RSI/ISM International Conference on Robotics and Mechatronics (ICRoM)"
    },
    {
      "citation_id": "14",
      "title": "Push recovery methods based on admittance control strategies for a nao-h25 humanoid",
      "authors": [
        "F Mousavi",
        "M Masouleh",
        "A Kalhor",
        "P Ghassemi"
      ],
      "year": "2018",
      "venue": "2018 6th RSI International Conference on Robotics and Mechatronics (IcRoM)"
    },
    {
      "citation_id": "15",
      "title": "Dynamic balance of a nao h25 humanoid robot based on model predictive control",
      "authors": [
        "F Mousavi",
        "P Ghassemi",
        "A Kalhor",
        "M Masouleh"
      ],
      "year": "2017",
      "venue": "2017 IEEE 4th International Conference on Knowledge-Based Engineering and Innovation"
    },
    {
      "citation_id": "16",
      "title": "An experimental study on a learning-based approach for the push recovery of nao humanoid robot",
      "authors": [
        "M Ghorbani",
        "F Kakavandi",
        "M Masouleh"
      ],
      "year": "2017",
      "venue": "2017 Artificial Intelligence and Signal Processing Conference"
    },
    {
      "citation_id": "17",
      "title": "Balance strategy for human imitation by a nao humanoid robot",
      "authors": [
        "P Shahverdi",
        "M Ansari",
        "M Masouleh"
      ],
      "year": "2017",
      "venue": "2017 5th RSI International Conference on Robotics and Mechatronics"
    },
    {
      "citation_id": "18",
      "title": "Mediapipe: A framework for building perception pipelines",
      "authors": [
        "C Lugaresi",
        "J Tang",
        "H Nash",
        "C Mcclanahan",
        "E Uboweja",
        "M Hays",
        "F Zhang",
        "C.-L Chang",
        "M Yong",
        "J Lee",
        "W.-T Chang",
        "W Hua",
        "M Georg",
        "M Grundmann"
      ],
      "year": "2019",
      "venue": "Mediapipe: A framework for building perception pipelines"
    },
    {
      "citation_id": "19",
      "title": "Deepface: Closing the gap to human-level performance in face verification",
      "authors": [
        "Y Taigman",
        "M Yang",
        "M Ranzato",
        "L Wolf"
      ],
      "year": "2014",
      "venue": "2014 IEEE Conference on Computer Vision and Pattern Recognition"
    }
  ]
}