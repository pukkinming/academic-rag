{
  "paper_id": "2009.04107v1",
  "title": "Multi-Modal Attention For Speech Emotion Recognition",
  "published": "2020-09-09T05:06:44Z",
  "authors": [
    "Zexu Pan",
    "Zhaojie Luo",
    "Jichen Yang",
    "Haizhou Li"
  ],
  "keywords": [
    "speech emotion recognition",
    "multi-modal attention",
    "early fusion",
    "hybrid fusion"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion represents an essential aspect of human speech that is manifested in speech prosody. Speech, visual, and textual cues are complementary in human communication. In this paper, we study a hybrid fusion method, referred to as multi-modal attention network (MMAN) to make use of visual and textual cues in speech emotion recognition. We propose a novel multimodal attention mechanism, cLSTM-MMA, which facilitates the attention across three modalities and selectively fuse the information. cLSTM-MMA is fused with other uni-modal subnetworks in the late fusion. The experiments show that speech emotion recognition benefits significantly from visual and textual cues, and the proposed cLSTM-MMA alone is as competitive as other fusion methods in terms of accuracy, but with a much more compact network structure. The proposed hybrid network MMAN achieves state-of-the-art performance on IEMOCAP database for emotion recognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotions play an important role in speech communication  [1] . The recent advancement of artificial intelligence has equipped machines with intelligence quotient. It is equally important for machines to understand emotions, and to improve their emotional intelligence.\n\nThe fact that voice call is more informative than text messaging suggests that the affective prosody of speech delivers additional information that includes emotion. Similarly, speaking face-to-face is more effective than text messaging and voice call, which suggests that visual cues play an important role. Humans express emotion through prosody, gesture, and lexical choice. Emotion is quantized by physiological arousal and hedonic valence level  [2] , which are only partially expressed through speech. The use of specific phrases further indicates our valence level and our body language carries the remaining arousal and valence. It is found that humans rely more on multimodalities than uni-modal  [3]  to understand emotions.\n\nMulti-modal speech emotion recognition has been an area of research for decades. Cho et al.  [4]  used text to aid speech in the MCNN network. Similarly Hossain et al.  [5]  and Xue et al.  [6]  used visual cues to augment speech using SVM and Sym-cHDP networks. It is evident that emotion recognition benefits from the fusion of speech, vision and text information  [7] [8] [9] [10] [11] . However, it has not been an easy task to fuse the information from different modalities. As the information coming from different modality is neither completely independent nor cor-related, the fusion mechanism is expected to pick up the right information from the right modality.\n\nEarly or late fusions are the typical options in multi-modal classifier design in emotion recognition. The state-of-theart method introduced the contextual long short-term memory block (cLSTM) and built a late fusion network (cLSTM-LF)  [12, 13] . The predictions of uni-modal models are fused to make a final prediction. It is effective at modelling modality-specific interactions but not cross-modal interactions  [14] .\n\nThere are also studies to explore the interaction between modalities with early fusion  [15] [16] [17] . Sebastian et al.  [15]  concatenated the low-level features and passed them through a convolutional neural network. Georgiou et al.  [17]  concatenated features from different modality at various levels and used multi-layer perceptron for emotion prediction. With early fusion, we are able to explore the interaction between raw features across modalities, that is good. However, the raw features represent different physical properties of the signals in the respective modalities. Therefore, the classifier network will have to learn both the feature abstraction of respective modalities, and the interaction of them at the same time, that is not easy. Furthermore, simple concatenation utilizes whatever information from the input streams that may or may not be relevant to the classification tasks. Early fusion also potentially suppresses modality-specific interactions  [18] . In general, concatenation based early fusion methods do not outperform the late fusion methods in emotion recognition  [14, 19] .\n\nTransformer has been effective in natural language processing that features a self-attention mechanism where each input feature embedding is first projected into query, key and value embeddings  [20] . In multi-modal situation, the query is from one modality while key and value are from another modality. The attentions between two modalities are computed by cosine similarities between the query and the key. The values are then fused based on the attention scores. The attention mechanism in Transformer is one of the effective solutions to learn crossmodality correlation  [21, 22] . Tsai et al.  [21]  used directional pairwise cross-modal attention for sentiment analysis. They show positive results with two-modalities attention. In this paper, we would like to explore a mechanism for three-modalities attention for the first time. We believe that speech, visual and text modalities provide complementary evidence for emotion. Three modalities cross-modal attention allows us to take advantage of such evidence.\n\nWe propose a multi-modal attention mechanism in place of concatenation to model the correlation between three modalities in cLSTM-MMA. As cLSTM-MMA takes the multi-modal features as input, we consider it as an early fusion sub-network. It",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Multi-Modal Attention Network",
      "text": "The proposed hybrid fusion network MMAN is shown on the left panel of Figure  1 . We have the speech, visual and text feature embeddings of the same utterance as the input. The MMAN consists of a cLSTM multi-modal attention sub-network (cLSTM-MMA) for early fusion, and three uni-modal sub-networks cLSTM-Speech, cLSTM-Visual and cLSTM-Text for late fusion. The outputs of the four subnetworks are fused with a dense and a softmax layer.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Multi-Modal Attention",
      "text": "The architecture of cLSTM-MMA sub-network is shown in the red dotted box on the right panel of Figure  1 . The cLSTM-MMA consists of three independent dense layers for uni-modal feature embeddings standardisation, multi-modal attention with three parallel directional multi-modal attention modules and finally a cLSTM with one LSTM layer inside.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Modality Dimension Standardization",
      "text": "The three inputs that represent one utterance are first encoded as the feature embeddings of different dimensions. We first stan-",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Directional Multi-Modal Attention Module",
      "text": "Taking the directional multi-modal attention module with speech query for illustration. It is represented by S -→ (S, V, T ) as shown in the blue module in Figure  1 . This module computes the directional attention from speech to visual and text as well as the self-attention of speech. The detail of this speech query module is illustrated in Figure  2 .\n\nWe use the query, key and value representation to compute the attention. We compute the query of speech qs through a learnable weights Wsq ∈ R d model ×dq as shown in Equation  1 .\n\nwhere dq is the dimension of the query vector.\n\nThe keys Ks and values Vs are computed using learnable weights W sk , W vk , W tk ∈ R d model ×d k , Wsv, Wvv, Wtv ∈ R d model ×dv , where d k , dv are dimensions of key and value vector. The computation is shown in Equation 2 and 3.\n\nThe cross-modal and self attention scores are computed by the dot product of the query qs and keys Ks. It is then used to compute the weighted sum of the values ẑi s , which represents the interaction of different modalities answering to speech query. The directional multi-modal attention from speech query D S-→(S,V,T ) is given in Equation 4 and illustrated in Figure  2 .\n\nThe same computing procedure is applied to text and visual directional multi-modal attention modules except that each module has its own learnable weights computing the query to facilitate the learning of different interactions based on different directional queries. The outputs from three parallel attention modules are concatenated with a skip connection.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Contextual Long Short-Term Memory Block",
      "text": "The output from multi-modal attention is passed through a cLSTM block with one LSTM layer as shown in Figure  1  to capture the contextual cues between consecutive utterances in a conversation  [13] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Uni-Modal Sub-Networks",
      "text": "The cLSTM-Speech, cLSTM-Visual and cLSTM-Text subnetworks are all built using cLSTM block with two LSTM layers except that their inputs are different. Their network hyperparameters are customized to suit different modalities. The cLSTM-MMA and three uni-modal sub-networks are separately trained. Their weights are fixed during the training of the late fusion dense layer in the MMAN.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "3. Experimental Setup",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "3.1. Dataset",
      "text": "The IEMOCAP dataset  [23]  is used to evaluate the proposed network. The dataset contains 10K videos split into 5 minutes of dyadic conversations for human emotion analysis. Each conversation is split into spoken utterance. Each utterance consists of corresponding transcription, speech waveform and visual frames. To align with previous works, we consider the emotion classes of angry, happy (excited), sad (frustrated) and neutral for multi-class classification but without excited and frustrated for binary sentiment classification system. The train and the test sets are disjoint for speakers. The speakers in the training set are not contained in the test set as we assume the speakers are unknown at the inference time. The details of the dataset are provided in Table  1 . Visual: We use a 3D-CNN  [26]  pre-trained from human action recognition to extract their body language. The 3D-CNN is applied to the consecutive visual frames of the speaker's upper body. It learns the relevant features of each frame and the changes among the given number of consecutive frames, which are the motion cues.\n\nText: Word2vec  [27]  is used to embed each word of an utterance's transcript into word2Vec vectors. The embedded words are concatenated, padded and standardized to a 1-dimensional vector by passing through a CNN  [28] .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Reference Baselines",
      "text": "Three baselines are constructed from the state-of-the-art model  [13] . They are all built based on cLSTM block. The first baseline uses speech data only while the other two uses speech, visual and text data. Speech-only cLSTM (cLSTM-Speech): The speech-only baseline receives speech features only, the speech features are passed through a cLSTM block with two LSTM layers for prediction.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Multi-Modal Clstm With Early Fusion (Clstm-Ef):",
      "text": "The cLSTM-EF baseline receives concatenated speech, visual and text feature embeddings as input. The concatenated features are passed through a cLSTM block with two LSTM layers for prediction.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Multi-Modal Clstm With Late Fusion (Clstm-Lf):",
      "text": "The cLSTM-LF baseline has a hierarchical structure. The lower level consists of three uni-modal networks cLSTM-Speech, cLSTM-Text and cLSTM-visual. At the higher level, the predictions of the three uni-modal networks are concatenated and passed through another cLSTM block for final prediction.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Evaluation Results",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Comparison With Baselines",
      "text": "We reported the results of our proposed models and baselines of multi-class recognition systems in terms of accuracy and recall rates in Table  2  and Figure  3 . We first presented the recognition accuracy of the Speech-only baseline cLSTM-Speech, which obtains a recognition accuracy of 57% which is more than 10 absolute percentage points lower than any other the multi-modal methods in Table  2 .\n\nWe compared its confusion matrix with our cLSTM-MMA model in Figure  3 , since they have similar network size. cLSTM-Speech's recall rates for neutral is very low but very high for sad as see in Figure  3 . This unbalance phenomenon is alleviated by cLSTM-MMA, which uses multi-modal information as seen from the cLSTM-MMA confusion matrix. This shows that the visual and textual cues do complement speech's ambiguity in emotion recognition.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Multi-Modal Attention Vs Concatenation",
      "text": "The cLSTM-MMA is 2% higher than cLSTM-EF in terms of accuracy as shown in Table  2 . This means that the proposed multi-modal attention is more prevalent in computing the interaction between modalities compared to concatenation method with early fusion. Besides, the cLSTM-MMA has 40% fewer parameters compared to the cLSTM-EF baseline.   The cLSTM-MMA achieves comparable accuracy with the state-of-the-art late-fusion model cLSTM-LF with only a quarter of its' parameters as shown in Table  2 . The proposed hybrid MMAN network outperforms all the multi-modal networks and achieves the state-of-the-art accuracy of 73.98% using the same amount of parameters as cLSTM-LF, suggesting that both modality-specific and cross-modal interactions are important in emotion recognition.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Hpy Sad Neu Ang",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Comparison With Previous Works",
      "text": "We compared the accuracies of our model with other binary sentiment classification systems using speech, visual and text in Table  3 . The cLSTM-MMA has superior performance over the pairwise correlation network MulT and others (with the exception of the sad emotion). Showing that correlation between three modalities is superior then pairwise correlation. Interestingly, MMAN has similar performance with cLSTM-MMA, suggesting that modality-specific interaction may not contribute much in binary sentiment classification case. Table  4  summaries the performance of previous multi-class emotion recognition network using speech visual and text. Our proposed MMAN achieves a state-of-the-art result of 73.94% on dataset IEMOCAP. Also, most of the methods of the previous work that achieved comparable results are based on BLSTM which have access to future utterance information when deciding for the current utterance, thus the comparison reported in this paper is in their favour. Nevertheless, MANN outperforms all reference baselines.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we presented a hybrid fusion model MMAN using visual and textual cues to aid speech in emotion recognition. We proposed the multi-modal attention in early fusion which features parallel directional attention between modalities in place of concatenation. The attention mechanism enables better data association between modalities and has a significantly less amount of parameters needed. Through experiments, we showed that the multi-modal attention alone is as competitive as other fusion methods with a much more compact network. Our hybrid model achieved the state-of-the-art result on IMEOCAP dataset for emotion recognition.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Acknowledgement",
      "text": "",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: On the left panel is the proposed multi-modal attention network (MMAN). It consists of a multi-modal attention sub-network",
      "page": 2
    },
    {
      "caption": "Figure 1: We have the speech, visual",
      "page": 2
    },
    {
      "caption": "Figure 1: The cLSTM-",
      "page": 2
    },
    {
      "caption": "Figure 2: The details of the directional multi-modal attention",
      "page": 2
    },
    {
      "caption": "Figure 1: , we standardize the outputs into the same",
      "page": 2
    },
    {
      "caption": "Figure 1: This module computes",
      "page": 2
    },
    {
      "caption": "Figure 2: We use the query, key and value representation to compute",
      "page": 2
    },
    {
      "caption": "Figure 3: , since they have similar network size.",
      "page": 4
    },
    {
      "caption": "Figure 3: This unbalance phenomenon",
      "page": 4
    },
    {
      "caption": "Figure 3: Normalised confusion matrix of the Speech-only base-",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4Osaka University, Osaka, Japan": "pan zexu@u.nus.edu,\nluo@irl.sys.es.osaka-u.ac.jp, {eleyji,"
        },
        {
          "4Osaka University, Osaka, Japan": "Abstract"
        },
        {
          "4Osaka University, Osaka, Japan": ""
        },
        {
          "4Osaka University, Osaka, Japan": "Emotion represents an essential aspect of human speech that is"
        },
        {
          "4Osaka University, Osaka, Japan": ""
        },
        {
          "4Osaka University, Osaka, Japan": "manifested in speech prosody. Speech, visual, and textual cues"
        },
        {
          "4Osaka University, Osaka, Japan": ""
        },
        {
          "4Osaka University, Osaka, Japan": "are complementary in human communication.\nIn this paper,"
        },
        {
          "4Osaka University, Osaka, Japan": ""
        },
        {
          "4Osaka University, Osaka, Japan": "we study a hybrid fusion method,\nreferred to as multi-modal"
        },
        {
          "4Osaka University, Osaka, Japan": ""
        },
        {
          "4Osaka University, Osaka, Japan": "attention network (MMAN)\nto make use of visual and textual"
        },
        {
          "4Osaka University, Osaka, Japan": ""
        },
        {
          "4Osaka University, Osaka, Japan": "cues in speech emotion recognition. We propose a novel multi-"
        },
        {
          "4Osaka University, Osaka, Japan": ""
        },
        {
          "4Osaka University, Osaka, Japan": "modal attention mechanism, cLSTM-MMA, which facilitates"
        },
        {
          "4Osaka University, Osaka, Japan": ""
        },
        {
          "4Osaka University, Osaka, Japan": "the attention across three modalities and selectively fuse the in-"
        },
        {
          "4Osaka University, Osaka, Japan": ""
        },
        {
          "4Osaka University, Osaka, Japan": "formation.\ncLSTM-MMA is fused with other uni-modal sub-"
        },
        {
          "4Osaka University, Osaka, Japan": ""
        },
        {
          "4Osaka University, Osaka, Japan": "networks in the late fusion. The experiments show that speech"
        },
        {
          "4Osaka University, Osaka, Japan": ""
        },
        {
          "4Osaka University, Osaka, Japan": "emotion recognition beneﬁts signiﬁcantly from visual and tex-"
        },
        {
          "4Osaka University, Osaka, Japan": ""
        },
        {
          "4Osaka University, Osaka, Japan": "tual cues, and the proposed cLSTM-MMA alone is as compet-"
        },
        {
          "4Osaka University, Osaka, Japan": ""
        },
        {
          "4Osaka University, Osaka, Japan": "itive as other\nfusion methods\nin terms of accuracy, but with"
        },
        {
          "4Osaka University, Osaka, Japan": ""
        },
        {
          "4Osaka University, Osaka, Japan": "a much more compact network structure.\nThe proposed hy-"
        },
        {
          "4Osaka University, Osaka, Japan": ""
        },
        {
          "4Osaka University, Osaka, Japan": "brid network MMAN achieves state-of-the-art performance on"
        },
        {
          "4Osaka University, Osaka, Japan": ""
        },
        {
          "4Osaka University, Osaka, Japan": "IEMOCAP database for emotion recognition."
        },
        {
          "4Osaka University, Osaka, Japan": ""
        },
        {
          "4Osaka University, Osaka, Japan": "Index Terms:\nspeech emotion recognition, multi-modal atten-"
        },
        {
          "4Osaka University, Osaka, Japan": ""
        },
        {
          "4Osaka University, Osaka, Japan": "tion, early fusion, hybrid fusion"
        },
        {
          "4Osaka University, Osaka, Japan": ""
        },
        {
          "4Osaka University, Osaka, Japan": ""
        },
        {
          "4Osaka University, Osaka, Japan": "1.\nIntroduction"
        },
        {
          "4Osaka University, Osaka, Japan": ""
        },
        {
          "4Osaka University, Osaka, Japan": ""
        },
        {
          "4Osaka University, Osaka, Japan": "Emotions play an important role in speech communication [1]."
        },
        {
          "4Osaka University, Osaka, Japan": ""
        },
        {
          "4Osaka University, Osaka, Japan": "The recent advancement of artiﬁcial\nintelligence has equipped"
        },
        {
          "4Osaka University, Osaka, Japan": ""
        },
        {
          "4Osaka University, Osaka, Japan": "machines with intelligence quotient.\nIt is equally important for"
        },
        {
          "4Osaka University, Osaka, Japan": ""
        },
        {
          "4Osaka University, Osaka, Japan": "machines to understand emotions, and to improve their emo-"
        },
        {
          "4Osaka University, Osaka, Japan": ""
        },
        {
          "4Osaka University, Osaka, Japan": "tional intelligence."
        },
        {
          "4Osaka University, Osaka, Japan": "The fact\nthat voice call\nis more informative than text mes-"
        },
        {
          "4Osaka University, Osaka, Japan": "saging suggests\nthat\nthe affective prosody of\nspeech delivers"
        },
        {
          "4Osaka University, Osaka, Japan": "additional information that includes emotion. Similarly, speak-"
        },
        {
          "4Osaka University, Osaka, Japan": "ing face-to-face is more effective than text messaging and voice"
        },
        {
          "4Osaka University, Osaka, Japan": "call, which suggests\nthat visual cues play an important\nrole."
        },
        {
          "4Osaka University, Osaka, Japan": "Humans express emotion through prosody, gesture, and lexi-"
        },
        {
          "4Osaka University, Osaka, Japan": "cal choice. Emotion is quantized by physiological arousal and"
        },
        {
          "4Osaka University, Osaka, Japan": "hedonic valence level\n[2], which are only partially expressed"
        },
        {
          "4Osaka University, Osaka, Japan": "through speech.\nThe use of speciﬁc phrases further\nindicates"
        },
        {
          "4Osaka University, Osaka, Japan": "our valence level and our body language carries the remaining"
        },
        {
          "4Osaka University, Osaka, Japan": "arousal and valence. It is found that humans rely more on multi-"
        },
        {
          "4Osaka University, Osaka, Japan": "modalities than uni-modal [3] to understand emotions."
        },
        {
          "4Osaka University, Osaka, Japan": ""
        },
        {
          "4Osaka University, Osaka, Japan": "Multi-modal speech emotion recognition has been an area"
        },
        {
          "4Osaka University, Osaka, Japan": ""
        },
        {
          "4Osaka University, Osaka, Japan": "of research for decades. Cho et al. [4] used text\nto aid speech"
        },
        {
          "4Osaka University, Osaka, Japan": ""
        },
        {
          "4Osaka University, Osaka, Japan": "in the MCNN network. Similarly Hossain et al. [5] and Xue et"
        },
        {
          "4Osaka University, Osaka, Japan": ""
        },
        {
          "4Osaka University, Osaka, Japan": "al. [6] used visual cues to augment speech using SVM and Sym-"
        },
        {
          "4Osaka University, Osaka, Japan": ""
        },
        {
          "4Osaka University, Osaka, Japan": "cHDP networks.\nIt is evident that emotion recognition beneﬁts"
        },
        {
          "4Osaka University, Osaka, Japan": "from the fusion of speech, vision and text\ninformation [7–11]."
        },
        {
          "4Osaka University, Osaka, Japan": "However,\nit has not been an easy task to fuse the information"
        },
        {
          "4Osaka University, Osaka, Japan": "from different modalities.\nAs\nthe information coming from"
        },
        {
          "4Osaka University, Osaka, Japan": "different modality is neither completely independent nor cor-"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Frames": "Multi-modal attention\ncLSTM"
        },
        {
          "Frames": "Speech\ncLSTM-Speech"
        },
        {
          "Frames": "Figure 1: On the left panel is the proposed multi-modal attention network (MMAN). It consists of a multi-modal attention sub-network"
        },
        {
          "Frames": "(cLSTM-MMA) for early fusion and three uni-modal sub-networks cLSTM-Text, cLSTM-Visual and cLSTM-Speech. The predictions"
        },
        {
          "Frames": "of the four sub-networks are fused with a dense and a softmax layer in late fusion. The architecture of\nthe cLSTM-MMA sub-network"
        },
        {
          "Frames": "is shown in the red dotted box on the right panel. The symbol ⊕ represents concatenation and S, V, T represents speech, visual and"
        },
        {
          "Frames": "text respectively.\nThe cLSTM-MMA consists of\nthree independent dense layers for uni-modal\nfeature embeddings standardisation,"
        },
        {
          "Frames": "multi-modal attention with three parallel directional multi-modal attention modules and ﬁnally a cLSTM with one LSTM layer inside."
        },
        {
          "Frames": "consists of three parallel directional multi-modal attention mod-"
        },
        {
          "Frames": "ules for multi-modal\nfusion.\nIn each module, a query is ﬁrst"
        },
        {
          "Frames": "Attention Scores"
        },
        {
          "Frames": "computed from a modality. It is then used to compute the cross-"
        },
        {
          "Frames": "x"
        },
        {
          "Frames": "modal attention and the self-attention scores to ﬁnd the relevant"
        },
        {
          "Frames": "x"
        },
        {
          "Frames": "information answering to this query. The three parallel modules"
        },
        {
          "Frames": "have distinct queries from three different modalities speciﬁcally."
        },
        {
          "Frames": "Thus, allowing the network to attend for different\ninteractions"
        },
        {
          "Frames": "based on the different queries jointly. The multi-modal atten-"
        },
        {
          "Frames": "tion can be easily scaled up if more than three modalities are"
        },
        {
          "Frames": "present.\nTo take advantage of both the late fusion and early"
        },
        {
          "Frames": "fusion to account for modality-speciﬁc and cross-modal\ninter-"
        },
        {
          "Frames": "actions, we propose a hybrid multi-modal attention network"
        },
        {
          "Frames": "(MMAN) which fuses the predictions of the cLSTM-MMA and"
        },
        {
          "Frames": "uni-modal cLSTM sub-networks for the ﬁnal prediction."
        },
        {
          "Frames": "the directional multi-modal attention\nFigure 2: The details of"
        },
        {
          "Frames": "The rest of\nthe paper\nis organized as follows.\nSection 2"
        },
        {
          "Frames": "module S −→ (S, V, T ) with query from speech. The inputs to"
        },
        {
          "Frames": "presents the details of the proposed multi-modal attention net-"
        },
        {
          "Frames": "this module are the uni-modal\nfeature embeddings (ˆsi, ˆvi, ˆti)"
        },
        {
          "Frames": "work. Section 3 describes the experimental setup. Section 4 re-"
        },
        {
          "Frames": "after the standardization dense layers"
        },
        {
          "Frames": "ports the results and evaluations. Finally, conclusions are drawn"
        },
        {
          "Frames": "in Section 5."
        },
        {
          "Frames": "dardize all feature embeddings into the same dimension dmodel\n2. Multi-modal attention network"
        },
        {
          "Frames": "to facilitate the subsequent processing."
        },
        {
          "Frames": "The\nproposed\nhybrid\nfusion\nnetwork MMAN is\nshown\non"
        },
        {
          "Frames": "Let’s denote the dataset as D = {si, vi, ti, yi}i=1:M where"
        },
        {
          "Frames": "the\nleft\npanel\nof\nFigure\n1.\nWe\nhave\nthe\nspeech,\nvisual"
        },
        {
          "Frames": "si, vi, ti and yi represent the speech, visual, text feature embed-"
        },
        {
          "Frames": "and text\nfeature embeddings of\nthe same utterance as the in-"
        },
        {
          "Frames": "dings and the emotion labels of utterance i. M is the number"
        },
        {
          "Frames": "put.\nThe MMAN consists of\na\ncLSTM multi-modal\natten-"
        },
        {
          "Frames": "of utterances\nin a conversation. With si ∈ Rds , vi ∈ Rdv"
        },
        {
          "Frames": "tion sub-network (cLSTM-MMA)\nfor early fusion, and three"
        },
        {
          "Frames": "and ti ∈ Rdt where ds, dv, dt are dimensions of corresponding"
        },
        {
          "Frames": "uni-modal\nsub-networks cLSTM-Speech,\ncLSTM-Visual and"
        },
        {
          "Frames": "speech, visual and text features. By passing the original feature"
        },
        {
          "Frames": "cLSTM-Text\nfor\nlate\nfusion.\nThe outputs of\nthe\nfour\nsub-"
        },
        {
          "Frames": "embeddings through the individual dense feed forward layers"
        },
        {
          "Frames": "networks are fused with a dense and a softmax layer."
        },
        {
          "Frames": "as shown in Figure 1, we standardize the outputs into the same"
        },
        {
          "Frames": "dimension ˆsi ∈ Rdmodel , ˆvi ∈ Rdmodel and ˆti ∈ Rdmodel ."
        },
        {
          "Frames": "2.1. Multi-modal attention"
        },
        {
          "Frames": "2.1.2. Directional multi-modal attention module\nThe architecture of cLSTM-MMA sub-network is shown in the"
        },
        {
          "Frames": "red dotted box on the right panel of Figure 1.\nThe cLSTM-"
        },
        {
          "Frames": "Taking\nthe\ndirectional multi-modal\nattention module with"
        },
        {
          "Frames": "MMA consists of three independent dense layers for uni-modal"
        },
        {
          "Frames": "speech query for illustration. It is represented by S −→ (S, V, T )"
        },
        {
          "Frames": "feature embeddings standardisation, multi-modal attention with"
        },
        {
          "Frames": "as shown in the blue module in Figure 1. This module computes"
        },
        {
          "Frames": "three parallel directional multi-modal attention modules and ﬁ-"
        },
        {
          "Frames": "the directional attention from speech to visual and text as well"
        },
        {
          "Frames": "nally a cLSTM with one LSTM layer inside."
        },
        {
          "Frames": "as the self-attention of speech. The detail of this speech query"
        },
        {
          "Frames": "module is illustrated in Figure 2."
        },
        {
          "Frames": "2.1.1. Modality dimension standardization"
        },
        {
          "Frames": "We use the query, key and value representation to compute"
        },
        {
          "Frames": "The three inputs that represent one utterance are ﬁrst encoded as\nthrough a\nthe attention. We compute the query of speech qs"
        },
        {
          "Frames": "the feature embeddings of different dimensions. We ﬁrst stan-\nlearnable weights Wsq ∈ Rdmodel×dq as shown in Equation 1."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 1: The number of utterances labelled happy (HPY), sad,": "neutral\n(NEU), angry\n(ANG),\nexcited (EXC) and frustrated"
        },
        {
          "Table 1: The number of utterances labelled happy (HPY), sad,": ""
        },
        {
          "Table 1: The number of utterances labelled happy (HPY), sad,": "(FRU) in the training and testing set of IEMOCAP"
        },
        {
          "Table 1: The number of utterances labelled happy (HPY), sad,": ""
        },
        {
          "Table 1: The number of utterances labelled happy (HPY), sad,": ""
        },
        {
          "Table 1: The number of utterances labelled happy (HPY), sad,": "HPY\nSAD\nNEU\nANG\nEXC\nFRU"
        },
        {
          "Table 1: The number of utterances labelled happy (HPY), sad,": ""
        },
        {
          "Table 1: The number of utterances labelled happy (HPY), sad,": "Train\n504\n839\n1324\n933\n742\n1468"
        },
        {
          "Table 1: The number of utterances labelled happy (HPY), sad,": "Test\n144\n245\n384\n170\n299\n381"
        },
        {
          "Table 1: The number of utterances labelled happy (HPY), sad,": ""
        },
        {
          "Table 1: The number of utterances labelled happy (HPY), sad,": "3.2. Uni-modal feature extraction"
        },
        {
          "Table 1: The number of utterances labelled happy (HPY), sad,": ""
        },
        {
          "Table 1: The number of utterances labelled happy (HPY), sad,": "We follow Poria et al. for low level feature extraction [13]. The"
        },
        {
          "Table 1: The number of utterances labelled happy (HPY), sad,": "input video of an utterance is ﬁrst separated into correspond-"
        },
        {
          "Table 1: The number of utterances labelled happy (HPY), sad,": "ing text, video frames and speech modalities and extraction is"
        },
        {
          "Table 1: The number of utterances labelled happy (HPY), sad,": "done by using individual pre-trained networks transferred from"
        },
        {
          "Table 1: The number of utterances labelled happy (HPY), sad,": "other tasks. The feature of each utterance is extracted as a ﬁxed-"
        },
        {
          "Table 1: The number of utterances labelled happy (HPY), sad,": "length vector for each modality."
        },
        {
          "Table 1: The number of utterances labelled happy (HPY), sad,": "Speech: OpenSMILE toolkit [24] with IS13-ComParE [25] is"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 2: Accuracy (%) and number of network parameters of",
      "data": [
        {
          "Table 2: Accuracy (%) and number of network parameters of": "the baselines and our models in multi-class classiﬁcation",
          "Table 4: A comparative study of multi-class emotion recognition": "with different multi-modal implementations"
        },
        {
          "Table 2: Accuracy (%) and number of network parameters of": "Model",
          "Table 4: A comparative study of multi-class emotion recognition": "Model"
        },
        {
          "Table 2: Accuracy (%) and number of network parameters of": "cLSTM-Speech",
          "Table 4: A comparative study of multi-class emotion recognition": "Rozgic et al. [31]"
        },
        {
          "Table 2: Accuracy (%) and number of network parameters of": "cLSTM-EF",
          "Table 4: A comparative study of multi-class emotion recognition": "Poria et al. [19]"
        },
        {
          "Table 2: Accuracy (%) and number of network parameters of": "cLSTM-LF",
          "Table 4: A comparative study of multi-class emotion recognition": "Tripathi et al. [12]"
        },
        {
          "Table 2: Accuracy (%) and number of network parameters of": "cLSTM-MMA",
          "Table 4: A comparative study of multi-class emotion recognition": "MMAN"
        },
        {
          "Table 2: Accuracy (%) and number of network parameters of": "MMAN",
          "Table 4: A comparative study of multi-class emotion recognition": ""
        },
        {
          "Table 2: Accuracy (%) and number of network parameters of": "",
          "Table 4: A comparative study of multi-class emotion recognition": "4.1.3. Comparison with late fusion"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: Accuracy (%) and number of network parameters of",
      "data": [
        {
          "Table 3: A comparative study of binary sentiment analysis with": "different multi-modal implementations in terms of classiﬁcation",
          "The": "",
          "cLSTM-MMA achieves": "state-of-the-art late-fusion model cLSTM-LF with only a quar-",
          "comparable": "",
          "accuracy with": "",
          "the": ""
        },
        {
          "Table 3: A comparative study of binary sentiment analysis with": "accuracy (%)",
          "The": "ter of",
          "cLSTM-MMA achieves": "",
          "comparable": "its’ parameters as shown in Table 2. The proposed hy-",
          "accuracy with": "",
          "the": ""
        },
        {
          "Table 3: A comparative study of binary sentiment analysis with": "",
          "The": "",
          "cLSTM-MMA achieves": "brid MMAN network outperforms all the multi-modal networks",
          "comparable": "",
          "accuracy with": "",
          "the": ""
        },
        {
          "Table 3: A comparative study of binary sentiment analysis with": "Model",
          "The": "",
          "cLSTM-MMA achieves": "and achieves the state-of-the-art accuracy of 73.98% using the",
          "comparable": "",
          "accuracy with": "",
          "the": ""
        },
        {
          "Table 3: A comparative study of binary sentiment analysis with": "",
          "The": "",
          "cLSTM-MMA achieves": "same amount of parameters as cLSTM-LF, suggesting that both",
          "comparable": "",
          "accuracy with": "",
          "the": ""
        },
        {
          "Table 3: A comparative study of binary sentiment analysis with": "MFM [29]",
          "The": "",
          "cLSTM-MMA achieves": "",
          "comparable": "",
          "accuracy with": "",
          "the": ""
        },
        {
          "Table 3: A comparative study of binary sentiment analysis with": "",
          "The": "",
          "cLSTM-MMA achieves": "modality-speciﬁc and cross-modal interactions are important in",
          "comparable": "",
          "accuracy with": "",
          "the": ""
        },
        {
          "Table 3: A comparative study of binary sentiment analysis with": "RAVEN [14]",
          "The": "",
          "cLSTM-MMA achieves": "",
          "comparable": "",
          "accuracy with": "",
          "the": ""
        },
        {
          "Table 3: A comparative study of binary sentiment analysis with": "",
          "The": "",
          "cLSTM-MMA achieves": "emotion recognition.",
          "comparable": "",
          "accuracy with": "",
          "the": ""
        },
        {
          "Table 3: A comparative study of binary sentiment analysis with": "FMT [30]",
          "The": "",
          "cLSTM-MMA achieves": "",
          "comparable": "",
          "accuracy with": "",
          "the": ""
        },
        {
          "Table 3: A comparative study of binary sentiment analysis with": "MulT [22]",
          "The": "",
          "cLSTM-MMA achieves": "",
          "comparable": "",
          "accuracy with": "",
          "the": ""
        },
        {
          "Table 3: A comparative study of binary sentiment analysis with": "",
          "The": "",
          "cLSTM-MMA achieves": "4.2. Comparison with previous works",
          "comparable": "",
          "accuracy with": "",
          "the": ""
        },
        {
          "Table 3: A comparative study of binary sentiment analysis with": "cLSTM-MMA",
          "The": "",
          "cLSTM-MMA achieves": "",
          "comparable": "",
          "accuracy with": "",
          "the": ""
        },
        {
          "Table 3: A comparative study of binary sentiment analysis with": "",
          "The": "",
          "cLSTM-MMA achieves": "We compared the accuracies of our model with other binary",
          "comparable": "",
          "accuracy with": "",
          "the": ""
        },
        {
          "Table 3: A comparative study of binary sentiment analysis with": "MMAN",
          "The": "",
          "cLSTM-MMA achieves": "",
          "comparable": "",
          "accuracy with": "",
          "the": ""
        },
        {
          "Table 3: A comparative study of binary sentiment analysis with": "",
          "The": "",
          "cLSTM-MMA achieves": "sentiment classiﬁcation systems using speech, visual and text",
          "comparable": "",
          "accuracy with": "",
          "the": ""
        },
        {
          "Table 3: A comparative study of binary sentiment analysis with": "",
          "The": "",
          "cLSTM-MMA achieves": "in Table 3. The cLSTM-MMA has superior performance over",
          "comparable": "",
          "accuracy with": "",
          "the": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: Accuracy (%) and number of network parameters of",
      "data": [
        {
          "ables better data association between modalities and has a sig-": "niﬁcantly less amount of parameters needed.\nThrough exper-"
        },
        {
          "ables better data association between modalities and has a sig-": "iments, we showed that\nthe multi-modal attention alone is as"
        },
        {
          "ables better data association between modalities and has a sig-": ""
        },
        {
          "ables better data association between modalities and has a sig-": "competitive as other\nfusion methods with a much more com-"
        },
        {
          "ables better data association between modalities and has a sig-": "pact network. Our hybrid model achieved the state-of-the-art"
        },
        {
          "ables better data association between modalities and has a sig-": "result on IMEOCAP dataset for emotion recognition."
        },
        {
          "ables better data association between modalities and has a sig-": ""
        },
        {
          "ables better data association between modalities and has a sig-": "6. Acknowledgement"
        },
        {
          "ables better data association between modalities and has a sig-": ""
        },
        {
          "ables better data association between modalities and has a sig-": "This\nresearch work\nis\npartially\nsupported\nby Programmatic"
        },
        {
          "ables better data association between modalities and has a sig-": ""
        },
        {
          "ables better data association between modalities and has a sig-": "Grant No. A1687b0033 from the Singapore Government’s Re-"
        },
        {
          "ables better data association between modalities and has a sig-": "search,\nInnovation and Enterprise 2020 plan (Advanced Man-"
        },
        {
          "ables better data association between modalities and has a sig-": ""
        },
        {
          "ables better data association between modalities and has a sig-": "ufacturing and Engineering domain), and in part by Human-"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "7. References": "",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": "Zadeh, and L.-P. Morency, “Efﬁcient low-rank multimodal fusion"
        },
        {
          "7. References": "[1] M. Sreeshakthy and J. Preethi, “Classiﬁcation of human emotion",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": ""
        },
        {
          "7. References": "",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": "with modality-speciﬁc factors,” in Proceedings of the 56th Annual"
        },
        {
          "7. References": "from deap eeg signal using hybrid improved neural networks with",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": ""
        },
        {
          "7. References": "",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": "Meeting of the Association for Computational Linguistics (Volume"
        },
        {
          "7. References": "cuckoo search,” BRAIN. Broad Research in Artiﬁcial Intelligence",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": ""
        },
        {
          "7. References": "",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": "1: Long Papers), 2018, pp. 2247–2256."
        },
        {
          "7. References": "and Neuroscience, vol. 6, no. 3-4, pp. 60–73, 2016.",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": ""
        },
        {
          "7. References": "",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": "[19]\nS. Poria, N. Majumder, D. Hazarika, E. Cambria, A. Gelbukh,"
        },
        {
          "7. References": "L. F. Barrett, “Solving the emotion paradox: Categorization and",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": ""
        },
        {
          "7. References": "",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": "and A. Hussain, “Multimodal sentiment analysis: Addressing key"
        },
        {
          "7. References": "the experience of emotion,” Personality and social psychology re-",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": ""
        },
        {
          "7. References": "",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": "issues and setting up the baselines,”\nIEEE Intelligent Systems,"
        },
        {
          "7. References": "view, vol. 10, no. 1, pp. 20–46, 2006.",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": ""
        },
        {
          "7. References": "",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": "vol. 33, no. 6, pp. 17–25, 2018."
        },
        {
          "7. References": "S. Shimojo and L. Shams, “Sensory modalities are not separate",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": ""
        },
        {
          "7. References": "",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": "[20] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N."
        },
        {
          "7. References": "modalities: plasticity and interactions,” Current opinion in neuro-",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": ""
        },
        {
          "7. References": "",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": "Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”"
        },
        {
          "7. References": "biology, vol. 11, no. 4, pp. 505–509, 2001.",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": ""
        },
        {
          "7. References": "",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": "in Advances in neural\ninformation processing systems, 2017, pp."
        },
        {
          "7. References": "",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": "5998–6008."
        },
        {
          "7. References": "J. Cho, R. Pappagari, P. Kulkarni,\nJ. Villalba, Y. Carmiel, and",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": ""
        },
        {
          "7. References": "N. Dehak, “Deep neural networks for emotion recognition com-",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": "[21] Y.-H. H. Tsai, S. Bai, P. P. Liang, J. Z. Kolter, L.-P. Morency, and"
        },
        {
          "7. References": "bining audio and transcripts,” arXiv preprint arXiv:1911.00432,",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": "R. Salakhutdinov, “Multimodal\ntransformer for unaligned multi-"
        },
        {
          "7. References": "2019.",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": "the 57th Annual\nmodal\nlanguage sequences,” in Proceedings of"
        },
        {
          "7. References": "",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": "Meeting of\nthe Association for Computational Linguistics, 2019,"
        },
        {
          "7. References": "[5] M. S. Hossain and G. Muhammad, “Emotion recognition using",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": ""
        },
        {
          "7. References": "",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": "pp. 6558–6569."
        },
        {
          "7. References": "deep learning approach from audio–visual emotional big data,”",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": ""
        },
        {
          "7. References": "Information Fusion, vol. 49, pp. 69–78, 2019.",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": "[22] H. Le, D. Sahoo, N. Chen, and S. Hoi, “Multimodal\ntransformer"
        },
        {
          "7. References": "",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": "networks\nfor end-to-end video-grounded dialogue systems,”\nin"
        },
        {
          "7. References": "J. Xue, Z. Luo, K. Eguchi, T. Takiguchi,\nand T. Omoto,\n“A",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": ""
        },
        {
          "7. References": "",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": "Proceedings of\nthe 57th Annual Meeting of\nthe Association for"
        },
        {
          "7. References": "bayesian nonparametric multimodal data modeling framework for",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": ""
        },
        {
          "7. References": "",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": "Computational Linguistics, 2019, pp. 5612–5623."
        },
        {
          "7. References": "video emotion recognition,” in 2017 IEEE International Confer-",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": ""
        },
        {
          "7. References": "ence on Multimedia and Expo (ICME).\nIEEE, 2017, pp. 601–606.",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": "[23] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower,"
        },
        {
          "7. References": "",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": "S. Kim, J. N. Chang, S. Lee, and S. S. Narayanan, “IEMOCAP:"
        },
        {
          "7. References": "S. Poria, E. Cambria, R. Bajpai, and A. Hussain, “A review of",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": ""
        },
        {
          "7. References": "",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": "Interactive emotional dyadic motion capture database,” Language"
        },
        {
          "7. References": "affective computing: From unimodal analysis to multimodal fu-",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": ""
        },
        {
          "7. References": "",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": "resources and evaluation, vol. 42, no. 4, p. 335, 2008."
        },
        {
          "7. References": "sion,” Information Fusion, vol. 37, pp. 98–125, 2017.",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": ""
        },
        {
          "7. References": "",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": "[24]\nF. Eyben, M. W¨ollmer, and B. Schuller, “Opensmile:\nthe mu-"
        },
        {
          "7. References": "[8] V. P´erez-Rosas, R. Mihalcea, and L.-P. Morency, “Utterance-level",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": ""
        },
        {
          "7. References": "",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": "nich versatile and fast open-source audio feature extractor,”\nin"
        },
        {
          "7. References": "the 51st An-\nmultimodal sentiment analysis,” in Proceedings of",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": ""
        },
        {
          "7. References": "",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": "Proceedings of\nthe 18th ACM international conference on Mul-"
        },
        {
          "7. References": "nual Meeting of\nthe Association for Computational Linguistics",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": ""
        },
        {
          "7. References": "",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": "timedia.\nACM, 2010, pp. 1459–1462."
        },
        {
          "7. References": "(Volume 1: Long Papers), 2013, pp. 973–982.",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": ""
        },
        {
          "7. References": "",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": "[25] B. Schuller, S. Steidl, A. Batliner, A. Vinciarelli, K. Scherer,"
        },
        {
          "7. References": "F. Weninger,\nT. Knaup, B.\nSchuller, C.\nSun,",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": ""
        },
        {
          "7. References": "",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": "F. Ringeval, M. Chetouani, F. Weninger, F. Eyben, E. Marchi"
        },
        {
          "7. References": "K. Sagae, and L.-P. Morency, “Youtube movie reviews:\nSenti-",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": ""
        },
        {
          "7. References": "",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": "et al., “The interspeech 2013 computational paralinguistics chal-"
        },
        {
          "7. References": "ment analysis in an audio-visual context,” IEEE Intelligent Sys-",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": ""
        },
        {
          "7. References": "",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": "lenge:\nsocial signals, conﬂict, emotion, autism,” in Proceedings"
        },
        {
          "7. References": "tems, vol. 28, no. 3, pp. 46–53, 2013.",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": ""
        },
        {
          "7. References": "",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": "INTERSPEECH 2013, 14th Annual Conference of\nthe Interna-"
        },
        {
          "7. References": "S. Poria, E. Cambria,\nand A. Gelbukh,\n“Deep\nconvolutional",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": "tional Speech Communication Association, Lyon, France, 2013."
        },
        {
          "7. References": "neural network textual\nfeatures and multiple kernel\nlearning for",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": ""
        },
        {
          "7. References": "",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": "[26]\nS. Ji, W. Xu, M. Yang, and K. Yu, “3d convolutional neural net-"
        },
        {
          "7. References": "utterance-level multimodal sentiment analysis,” in Proceedings of",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": ""
        },
        {
          "7. References": "",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": "works for human action recognition,” IEEE transactions on pat-"
        },
        {
          "7. References": "the 2015 conference on empirical methods in natural\nlanguage",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": ""
        },
        {
          "7. References": "",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": "tern analysis and machine intelligence, vol. 35, no. 1, pp. 221–"
        },
        {
          "7. References": "processing, 2015, pp. 2539–2544.",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": ""
        },
        {
          "7. References": "",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": "231, 2012."
        },
        {
          "7. References": "E. Cambria, D. Hazarika, S. Poria, A. Hussain, and R. Subra-",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": "[27]\nTomas, K. Mikolov, G. Chen, J. Corrado, and Dean, “Efﬁcient es-"
        },
        {
          "7. References": "manyam, “Benchmarking multimodal sentiment analysis,” in In-",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": "timation of word representations in vector space,” arXiv preprint"
        },
        {
          "7. References": "ternational Conference on Computational Linguistics and Intelli-",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": "arXiv:1301.3781, 2013."
        },
        {
          "7. References": "gent Text Processing.\nSpringer, 2017, pp. 166–179.",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": ""
        },
        {
          "7. References": "",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": "[28] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, and"
        },
        {
          "7. References": "S.\nTripathi\nand\nH.\nBeigi,\n“Multi-modal\nemotion\nrecogni-",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": "L. Fei-Fei, “Large-scale video classiﬁcation with convolutional"
        },
        {
          "7. References": "arXiv preprint\ntion on iemocap dataset using deep learning,”",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": "neural networks,” in Proceedings of the IEEE conference on Com-"
        },
        {
          "7. References": "arXiv:1804.05788, 2019.",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": "puter Vision and Pattern Recognition, 2014, pp. 1725–1732."
        },
        {
          "7. References": "S. Poria, E. Cambria, D. Hazarika, N. Majumder, A. Zadeh, and",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": "[29] Y. H. Tsai, P. P. Liang, A. Zadeh, L. Morency, and R. Salakhutdi-"
        },
        {
          "7. References": "L.-P. Morency,\n“Context-dependent\nsentiment analysis\nin user-",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": "nov, “Learning factorized multimodal representations,” in ICLR,"
        },
        {
          "7. References": "the 55th Annual Meeting of\ngenerated videos,” in Proceedings of",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": "2019."
        },
        {
          "7. References": "the Association for Computational Linguistics (Volume 1: Long",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": ""
        },
        {
          "7. References": "",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": "[30] A. Zadeh, C. Mao, K. Shi, Y. Zhang, P. P. Liang, S. Poria, and L.-"
        },
        {
          "7. References": "Papers), 2017, pp. 873–883.",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": ""
        },
        {
          "7. References": "",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": "P. Morency, “Factorized multimodal\ntransformer for multimodal"
        },
        {
          "7. References": "and L.-P.",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": "sequential learning,” arXiv preprint arXiv:1911.09826, 2019."
        },
        {
          "7. References": "Morency, “Words can shift: Dynamically adjusting word repre-",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": ""
        },
        {
          "7. References": "",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": "[31] V. Rozgi´c,\nS. Ananthakrishnan,\nS.\nSaleem,\nR. Kumar,\nand"
        },
        {
          "7. References": "sentations using nonverbal behaviors,” in Proceedings of the AAAI",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": ""
        },
        {
          "7. References": "",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": "R. Prasad, “Ensemble of svm trees for multimodal emotion recog-"
        },
        {
          "7. References": "Conference on Artiﬁcial\nIntelligence, vol. 33, 2019, pp. 7216–",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": ""
        },
        {
          "7. References": "",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": "nition,” in Proceedings of The 2012 Asia Paciﬁc Signal and Infor-"
        },
        {
          "7. References": "7223.",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": ""
        },
        {
          "7. References": "",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": "mation Processing Association Annual Summit and Conference."
        },
        {
          "7. References": "J. Sebastian and P. Pierucci,\n“Fusion techniques\nfor utterance-",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": "IEEE, 2012, pp. 1–4."
        },
        {
          "7. References": "level emotion recognition combining speech and transcripts,” in",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": ""
        },
        {
          "7. References": "Proc. Interspeech, 2019, pp. 51–55.",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": ""
        },
        {
          "7. References": "S. Poria, E. Cambria, N. Howard, G.-B. Huang,\nand A. Hus-",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": ""
        },
        {
          "7. References": "sain, “Fusing audio, visual and textual clues for sentiment analysis",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": ""
        },
        {
          "7. References": "from multimodal content,” Neurocomputing, vol. 174, pp. 50–59,",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": ""
        },
        {
          "7. References": "2016.",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": ""
        },
        {
          "7. References": "E. Georgiou, C. Papaioannou, and A. Potamianos, “Deep hierar-",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": ""
        },
        {
          "7. References": "chical fusion with application in sentiment analysis,” Proc. Inter-",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": ""
        },
        {
          "7. References": "speech 2019, pp. 1646–1650, 2019.",
          "[18]\nZ. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. B.": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Classification of human emotion from deap eeg signal using hybrid improved neural networks with cuckoo search",
      "authors": [
        "M Sreeshakthy",
        "J Preethi"
      ],
      "year": "2016",
      "venue": "BRAIN. Broad Research in Artificial Intelligence and Neuroscience"
    },
    {
      "citation_id": "3",
      "title": "Solving the emotion paradox: Categorization and the experience of emotion",
      "authors": [
        "L Barrett"
      ],
      "year": "2006",
      "venue": "Personality and social psychology review"
    },
    {
      "citation_id": "4",
      "title": "Sensory modalities are not separate modalities: plasticity and interactions",
      "authors": [
        "S Shimojo",
        "L Shams"
      ],
      "year": "2001",
      "venue": "Current opinion in neurobiology"
    },
    {
      "citation_id": "5",
      "title": "Deep neural networks for emotion recognition combining audio and transcripts",
      "authors": [
        "J Cho",
        "R Pappagari",
        "P Kulkarni",
        "J Villalba",
        "Y Carmiel",
        "N Dehak"
      ],
      "year": "2019",
      "venue": "Deep neural networks for emotion recognition combining audio and transcripts",
      "arxiv": "arXiv:1911.00432"
    },
    {
      "citation_id": "6",
      "title": "Emotion recognition using deep learning approach from audio-visual emotional big data",
      "authors": [
        "M Hossain",
        "G Muhammad"
      ],
      "year": "2019",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "7",
      "title": "A bayesian nonparametric multimodal data modeling framework for video emotion recognition",
      "authors": [
        "J Xue",
        "Z Luo",
        "K Eguchi",
        "T Takiguchi",
        "T Omoto"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "8",
      "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
      "authors": [
        "S Poria",
        "E Cambria",
        "R Bajpai",
        "A Hussain"
      ],
      "year": "2017",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "9",
      "title": "Utterance-level multimodal sentiment analysis",
      "authors": [
        "V Pérez-Rosas",
        "R Mihalcea",
        "L.-P Morency"
      ],
      "year": "2013",
      "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "10",
      "title": "Youtube movie reviews: Sentiment analysis in an audio-visual context",
      "authors": [
        "M Wöllmer",
        "F Weninger",
        "T Knaup",
        "B Schuller",
        "C Sun",
        "K Sagae",
        "L.-P Morency"
      ],
      "year": "2013",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "11",
      "title": "Deep convolutional neural network textual features and multiple kernel learning for utterance-level multimodal sentiment analysis",
      "authors": [
        "S Poria",
        "E Cambria",
        "A Gelbukh"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 conference on empirical methods in natural language processing"
    },
    {
      "citation_id": "12",
      "title": "Benchmarking multimodal sentiment analysis",
      "authors": [
        "E Cambria",
        "D Hazarika",
        "S Poria",
        "A Hussain",
        "R Subramanyam"
      ],
      "year": "2017",
      "venue": "International Conference on Computational Linguistics and Intelligent Text Processing"
    },
    {
      "citation_id": "13",
      "title": "Multi-modal emotion recognition on iemocap dataset using deep learning",
      "authors": [
        "S Tripathi",
        "H Beigi"
      ],
      "year": "2019",
      "venue": "Multi-modal emotion recognition on iemocap dataset using deep learning",
      "arxiv": "arXiv:1804.05788"
    },
    {
      "citation_id": "14",
      "title": "Context-dependent sentiment analysis in usergenerated videos",
      "authors": [
        "S Poria",
        "E Cambria",
        "D Hazarika",
        "N Majumder",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "15",
      "title": "Words can shift: Dynamically adjusting word representations using nonverbal behaviors",
      "authors": [
        "Y Wang",
        "Y Shen",
        "Z Liu",
        "P Liang",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "16",
      "title": "Fusion techniques for utterancelevel emotion recognition combining speech and transcripts",
      "authors": [
        "J Sebastian",
        "P Pierucci"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "17",
      "title": "Fusing audio, visual and textual clues for sentiment analysis from multimodal content",
      "authors": [
        "S Poria",
        "E Cambria",
        "N Howard",
        "G.-B Huang",
        "A Hussain"
      ],
      "year": "2016",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "18",
      "title": "Deep hierarchical fusion with application in sentiment analysis",
      "authors": [
        "E Georgiou",
        "C Papaioannou",
        "A Potamianos"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "19",
      "title": "Efficient low-rank multimodal fusion with modality-specific factors",
      "authors": [
        "Z Liu",
        "Y Shen",
        "V Lakshminarasimhan",
        "P Liang",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "20",
      "title": "Multimodal sentiment analysis: Addressing key issues and setting up the baselines",
      "authors": [
        "S Poria",
        "N Majumder",
        "D Hazarika",
        "E Cambria",
        "A Gelbukh",
        "A Hussain"
      ],
      "year": "2018",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "21",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "22",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Y.-H Tsai",
        "S Bai",
        "P Liang",
        "J Kolter",
        "L.-P Morency",
        "R Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "23",
      "title": "Multimodal transformer networks for end-to-end video-grounded dialogue systems",
      "authors": [
        "H Le",
        "D Sahoo",
        "N Chen",
        "S Hoi"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "24",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "25",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "26",
      "title": "The interspeech 2013 computational paralinguistics challenge: social signals, conflict, emotion, autism",
      "authors": [
        "B Schuller",
        "S Steidl",
        "A Batliner",
        "A Vinciarelli",
        "K Scherer",
        "F Ringeval",
        "M Chetouani",
        "F Weninger",
        "F Eyben",
        "E Marchi"
      ],
      "year": "2013",
      "venue": "Proceedings INTERSPEECH 2013, 14th Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "27",
      "title": "3d convolutional neural networks for human action recognition",
      "authors": [
        "S Ji",
        "W Xu",
        "M Yang",
        "K Yu"
      ],
      "year": "2012",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "28",
      "title": "Efficient estimation of word representations in vector space",
      "authors": [
        "K Tomas",
        "G Mikolov",
        "J Chen",
        "Dean Corrado"
      ],
      "year": "2013",
      "venue": "Efficient estimation of word representations in vector space",
      "arxiv": "arXiv:1301.3781"
    },
    {
      "citation_id": "29",
      "title": "Large-scale video classification with convolutional neural networks",
      "authors": [
        "A Karpathy",
        "G Toderici",
        "S Shetty",
        "T Leung",
        "R Sukthankar",
        "L Fei-Fei"
      ],
      "year": "2014",
      "venue": "Proceedings of the IEEE conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "30",
      "title": "Learning factorized multimodal representations",
      "authors": [
        "Y Tsai",
        "P Liang",
        "A Zadeh",
        "L Morency",
        "R Salakhutdinov"
      ],
      "year": "2019",
      "venue": "ICLR"
    },
    {
      "citation_id": "31",
      "title": "Factorized multimodal transformer for multimodal sequential learning",
      "authors": [
        "A Zadeh",
        "C Mao",
        "K Shi",
        "Y Zhang",
        "P Liang",
        "S Poria",
        "L.-P Morency"
      ],
      "year": "2019",
      "venue": "Factorized multimodal transformer for multimodal sequential learning",
      "arxiv": "arXiv:1911.09826"
    },
    {
      "citation_id": "32",
      "title": "Ensemble of svm trees for multimodal emotion recognition",
      "authors": [
        "V Rozgić",
        "S Ananthakrishnan",
        "S Saleem",
        "R Kumar",
        "R Prasad"
      ],
      "year": "2012",
      "venue": "Proceedings of The 2012 Asia Pacific Signal and Information Processing Association Annual Summit and Conference"
    }
  ]
}