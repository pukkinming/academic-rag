{
  "paper_id": "2303.11117v5",
  "title": "Emotionic: Emotional Inertia And Contagion-Driven Dependency Modeling For Emotion Recognition In Conversation",
  "published": "2023-03-20T13:58:35Z",
  "authors": [
    "Yingjian Liu",
    "Jiang Li",
    "Xiaoping Wang",
    "Zhigang Zeng"
  ],
  "keywords": [
    "emotion recognition in conversation",
    "emotional inertia and contagion",
    "multi-head attention",
    "gated recurrent unit",
    "conditional random field"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion Recognition in Conversation (ERC) has attracted growing attention in recent years as a result of the advancement and implementation of human-computer interface technologies. In this paper, we propose an emotional inertia and contagion-driven dependency modeling approach (EmotionIC) for ERC task. Our EmotionIC consists of three main components, i.e., Identity Masked Multi-Head Attention (IMMHA), Dialogue-based Gated Recurrent Unit (DiaGRU), and Skip-chain Conditional Random Field (SkipCRF). Compared to previous ERC models, EmotionIC can model a conversation more thoroughly at both the feature-extraction and classification levels. The proposed model attempts to integrate the advantages of attention-and recurrence-based methods at the feature-extraction level. Specifically, IMMHA is applied to capture identity-based global contextual dependencies, while DiaGRU is utilized to extract speaker-and temporal-aware local contextual information. At the classification level, SkipCRF can explicitly mine complex emotional flows from higher-order neighboring utterances in the conversation. Experimental results show that our method can significantly outperform the state-of-the-art models on four benchmark datasets. The ablation studies confirm that our modules can effectively model emotional inertia and contagion.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion Recognition in Conversation (ERC) is one of the most focusing research fields in Natural Language Processing (NLP), which aims to identify the emotion of each utterance in a conversation. This task has recently received considerable attention from NLP researchers due to its potential applications in multiple domains such as opinion mining in social media  [1, 2] , empathic dialogue system construction  [3, 4] , and smart home systems  [5, 6] . Emotions are often reflected in interpersonal interactions, and analyzing the emotions of a single utterance out of the conversational context may lead to ambiguity  [7] . Therefore, ERC incorporating conversational context information significantly contributes to model performance.\n\nThe effective use of contextual information in dialogues lies at the heart of ERC  [8] . There are numerous efforts have been developed to encode the contextual information in the dialogue, including graph-based methods  [9] [10] [11] , recurrence-based methods  [3, 12, 13] , and attention-based methods  [14] [15] [16] . Li et al.  [17]  proposed a psychological-knowledge-aware interaction graph, which established four relations in a local connectivity graph to simulate the psychological state of the speaker. Hu et al.  [18]  designed a multiturn reasoning module based on the Recurrent Neural Network (RNN), which iteratively performed the intuitive retrieval process and conscious reasoning process to extract and integrate emotional cues from a cognitive perspective. Zhu et al.  [15]  proposed a topic-driven and knowledge-aware Transformer model that incorporated topic representation and the commonsense knowledge from ATOMIC for emotion detection in dialogues.\n\nHowever, recurrence-based methods tend to use only relatively limited information from recent utterances to update the state of the current utterance, which makes them difficult to achieve satisfying performance; graph-and attention-based approaches diminish the importance of neighboring utterances because of global relevance, resulting in the loss of temporal sequential information in the conversation. According to the above analysis, a better way to implement ERC is to combine the strengths of attentionand recurrence-based models. The neighboring utterances tend to contain information about emotional inertia and contagion, and this information can diminish over time. Despite the weak influence of longdistance contexts on the current utterance, they imply abundant global information that can assist in classifying utterances without clear emotions. Thus, we model contexts of the current utterance by utilizing the speaker identity-based Multi-Head Attention (MHA)  [19]  and Gated Recurrent Unit (GRU)  [20]  to extract global and local information, respectively.\n\nDuring the conversation, a speaker's emotion is influenced by his/her own or others' historical emotions, indicating that there are significant dependencies between emotions in the conversation. Existing ERC models focus on contextual modeling at the feature-extraction level and rarely mine the emotional flows in the conversation at the classification level. To achieve this purpose, we draw on the effectiveness of Conditional Random Field (CRF) for modeling sequential dependencies to explicitly model emotional interactions in the conversation. Not only that, but in order to simulate the complex emotional propagations in the conversation, we introduce skip connections in CRF to capture emotion influence from higher-order neighboring utterances (i.e., indirect neighbors).\n\nYeah, but when the baby comes she's go want to move  [neutral]  She is? Yeah, you didn't expect her to live there with baby did you?  In summary, we propose an Emotional Inertia and Contagion-driven dependency modeling approach (EmotionIC) in this paper, which can adequately model contexts from the feature-extraction and classification levels. Figure  1  is an example of modeling context dependency at both the feature-extraction and classification levels. At the feature-extraction level, we design an Identity Masked MHA (IMMHA) to capture intra-and inter-speaker dependencies in the global contexts from two features subspaces, respectively; to further refine the contextual dependencies, we devise a speaker-and position-aware Dialogue GRU (DiaGRU). DiaGRU introduces the emotional tendencies of the current speaker and interlocutor at the previous moment into the single GRU cell. At the classification level, by introducing skip connections in CRF, we craft a novel structure called Skip-chain CRF (SkipCRF) to explicitly capture the emotional flows in the conversation. SkipCRF takes into account higher-order contextual dependencies from intra-and inter-speaker, and can model the complex emotional interactions of different participants in the conversation. It is worth noting that instead of using additional softmax layer, we directly utilize CRF for final emotion classification. Experimental results demonstrate the superiority of our model compared with state-of-the-art models, and several studies are conducted to illustrate the effectiveness of each module of EmotionIC. To put it briefly, our main contributions are summarized as follows:",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "Feature-Extraction Level",
      "text": "• We propose a novel model, EmotionIC, for the ERC task. EmotionIC models a conversation thoroughly at both the feature-extraction and classification levels and is mainly composed of IMMHA, Dia-GRU, and SkipCRF.\n\n• At the feature-extraction level, we combine the strengths of attention-and recurrence-based models. IMMHA extracts identity-based global contextual information, while DiaGRU captures participant-and temporal-aware local contextual information.\n\n• At the classification level, SkipCRF can extract complex emotional flows from higher-order neighboring utterances in the conversation while accomplishing final emotion classification.\n\n• We perform extensive experiments on the IEMOCAP, DailyDialog, MELD, and EmoryNLP datasets and obtain the most advanced performance, which demonstrates the superiority of the proposed method.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Related Work",
      "text": "Emotion recognition is an interdisciplinary field of research, with contributions from different fields such as natural language processing, computer vision, and psychological cognitive science  [21] . In this section, we mainly introduce the related works of emotion recognition in conversation and conditional random field. Moreover, we briefly introduce the applications of CRF in ERC tasks.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emotion Recognition In Conversation",
      "text": "Distinct from traditional emotion recognition which treats emotion as a static state, Emotion Recognition in Conversation (ERC) takes full consideration of emotion to be dynamic and flow between speaker interactions. Hazarika et al.  [22]  proposed a model based on Long-and Short-Term Memory (LSTM) to enable current utterance to capture contextual information in historical conversations. CMN  [23]  employed a skip attention mechanism to merge contextual information in a historical conversation. Jiao et al.  [12]  proposed a hierarchical GRU to address the difficulty of capturing long-distance contextual information effectively. By distinguishing specific speakers, DialogueRNN  [3]  modeled emotions dynamically based on the current speaker, contextual content, and emotional state. Zhong et al.  [14]  proposed Knowledge-Enriched Transformer, which dynamically exploited external commonsense knowledge through hierarchical self-attention and context-aware graph attention. By building directed graphical structures over the input utterance sequences with speaker information, DialogueGCN [9] applied graph convolution network to construct intra-and inter-dependencies among distant utterances. COSMIC  [13]  combined different commonsense knowledge and learned the interaction between the interlocutors in the dialogue. DialogXL  [24]  modified the memory block in XLNet  [25]  to store longer historical contexts and conversation-aware self-attention to handle multi-party structures. Wang et al.  [26]  proposed a relational graph attention network to encode the tree structure for sentiment prediction. DAG-ERC  [10]  treated the internal structure of dialogue as a directed acyclic graph, which intuitively model the way information flows between long and short distance contexts. Considering that utterances with similar semantics may have distinctive emotions under different contexts, CoG-BART  [27]  adopted supervised contrastive learning to enhance the model's ability to handle context information. GAR-Net  [28]  was an end-to-end graph attention reasoning network that took both word-level and utterance-level context into concern, aiming to emphasize the importance of contextual reasoning. Most prior efforts do not combine the strengths of global and local conceptual modeling and fail to explicitly consider self-and other-dependency based on emotional inertia and contagion.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Conditional Random Field",
      "text": "Conditional Random Fields (CRFs)  [29, 30]  are a class of probabilistic graphical modeling methods that aim to model the conditional distribution P(Y |X) by a set of observed variables\n\n, and the structural information among different variables. CRF relaxes the strong dependency assumptions in other Bayesian models based on directed graphical models and enables the establishment of higher-order dependency, which means that the result of CRF is closer to the real distribution of data  [31] . CRF has recently attracted the interest of researchers in the ERC field  [32] [33] [34] , and these methods demonstrate the effectiveness of CRF for modeling emotion dependency at the classification level. CRF utilizes the potential function with clusters on the graph structure to define the conditional probability P(Y |X). Since there are primarily two types of clusters on labels in the linear-chain CRF frequently employed in ERC models, two forms of exponential potential functions are added as feature functions. Suppose the random variable X and Y take the values x = (x 1 , x 2 , • • • , x T ) and y = (y 1 , y 2 , • • • , y T ), respectively, P(y|x) is defined formally as:\n\nwhere Z(x) represents the normalization factor; F n (•) is the feature function of linear-chain CRF, which consists of the local feature function g i (•) and the nodal feature function f l (•); g i (•) is defined on the context-connected edge of node Y , which means the state transition from y t-1 to y t ; f l (•) is defined on node Y , which means the state of y t ; ω n consists of λ i and µ l , which are learnable weights of the corresponding feature function. In the existing ERC methods for modeling this dependency, only the linear-chain CRF with first-order dependency is applied, i.e., only the dependency between neighbor tags are considered. This simple form is difficult to cope with complex interaction situations of different participants in dialogue scenarios. So we construct SkipCRF that introduces higher-order dependency through skip-chain connections to model emotional inertia and contagion at the classification level.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Our Approach",
      "text": "In this section, we will introduce the main components of our approach. First, we present the problem definitions and make certain transformations of the original problem according to the requirements of the proposed model. Then, we illustrate the architecture of our model as in Figure  2 , which contains three components: (1) Identity Masked Multi-head Attention (IMMHA), which captures global historical information from different participants; (2) Dialogue-based Gated Recurrent Unit (DiaGRU), which focuses on local intra-and inter-speaker dependencies of the current utterance; (3) Skip-chain Conditional Random Field (SkipCRF), which explicitly captures complex emotional flows at the classification level to obtain the optimal emotion sequence. In the following subsections, the key elements in these three components are described in detail.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Preliminaries",
      "text": "Problem definition. The objective of the ERC task is to predict the emotion label e t corresponding to the t-th utterance u pi t in the conversation\n\nHere, e t ∈ E, and E is the set of emotion labels in the dataset; p i ∈ {p 1 , p 2 , • • • , p m } is the speaker identity; t describes the order of the utterance and also represents the moment corresponding to the current utterance; T represents the length of conversation C. Emotional inertia and contagion. In addition to context dependency, speaker identity information is also shown to be critical to ERC  [35] . Research results in the field of psychology analyzed how emotions are transmitted in conversations  [36] . Specifically, the transmission of emotion in interpersonal communication and dialogue is mainly driven by two factors: emotional inertia which means that a speaker in the conversation tends to maintain a particular emotional state (i.e., intra-speaker dependency or selfdependency), and emotional contagion which describes the emotional stimulation of other participants' utterances (i.e., inter-speaker dependency or other-dependency). In this paper, these two factors run through our methodology. Higher-order dependency. Higher-Order  [31, 37]  is derived from concepts in the CRF domain. Refer to the definition of higher-order CRF, Order denotes the hop count of the current utterance's neighbor in our work. Higher-Order is a relative concept; in general, the order greater than or equal to 2 indicates Higher-Order. For instance, in conversation (u 1 , u 2 , u 3 , u 4 , u 5 ), u 2 is the 3rd-order neighbor (indirect/higher-order neighbor) of u 5 , and u 4 is the 1st-order neighbor (direct neighbor) of u 5 . Accordingly, the meaning of higher-order dependencies is the dependencies of the current utterance on higher-order neighbors. Taking advantage of the ability of global context-awareness in conversation, attention mechanism is widely applied in many ERC models  [14, 15] . However, these approaches didn't explicitly encode the speaker identity information based on emotional inertia and contagion. In this subsection, we design a new Identity Masked Multi-Head Attention (IMMHA) that can effectively combine the identity information of participants to capture intra-and inter-speaker global dependencies in the conversation.",
      "page_start": 4,
      "page_end": 6
    },
    {
      "section_name": "Identity Masked Multi-Head Attention",
      "text": "As shown on the left of Figure  4 , we add two mask matrices (i.e., M s and M o ) in IMMHA to capture intra-and inter-speaker contextual information, respectively. Here, M s is utilized to mask contextual dependencies from other participants (i.e., only the contextual information from the current participant is retained), and M o is implemented to mask contextual dependencies from the current participant (i.e., only the contextual information from other participants is maintained). Note that although we draw on multi-head self-attention  [19] , we treat utterances as inputs to IMMHA rather than tokens. For the capture of intra-speaker contextual information, we first matrix-multiply the utterance query matrix and utterance key matrix, and then mask the resulting result with the masked matrix M s . In the actual implementation, we perform the untied absolute position encoding before the mask operation by referring to Ke et al.  [38] . Similar steps to those described above can be adopted to capture the inter-speaker contextual information. Finally, the captured intra-and inter-speaker contextual information is summed, which in turn is passed through the softmax layer to obtain the attention matrix of IMMHA. The whole process can be seen on the right of Figure  4  and can be described with the following formulation:\n\nwhere Q s,u and Q o,u represent the utterance query matrices, which are the results of mapping the utterances to different subspaces (i.e., self-and other-dependent subspaces) through the fully connected layers; Q s,p and Q o,p indicate the position query matrices; K u and K p are the utterance key matrices, which are the results of mapping the utterances to the same subspace through the fully connected layer; ⊙ stands for the element-wise product.\n\nTo get the final output of IMMHA, the obtained attention matrix IM-Attn is matrix multiplied with the utterance value matrix. In addition, inspired by the excellent architecture of Transformer  [19] , we pass the output of IMMHA through the residual, normalization, and feedforward layers, while adopting the multi-head setting. Emotional inertia and contagion in a conversation are often susceptible to temporal sequence. For example, the current utterance relies more on the near contexts than on the long-distance contexts.\n\nAlthough IMMHA captures all historical information about the participants, it fails to take into account the temporal sequential information in the conversation. In this subsection, we design a new Dialoguebased Gated Recurrent Unit (DiaGRU) network with reference to the structure of GRU  [20]  to aggregate the intra-and inter-speaker dependencies in the conversation. The architecture of a single DiaGRU cell is shown in Figure  5(a) .\n\nDiaGRU sets the self-and other-dependent reset gates. Firstly, a single DiaGRU cell calculates the corresponding forgetting degrees (i.e., reset gates) s t and o t based on the hidden state h pi s(t) of self-context and the hidden state h pj o(t) of others-context, respectively. Here, s t approaches 1 indicates strong emotional inertia, while o t approaches 1 indicates strong emotional contagion. Then, the candidate hidden state ht and update gate z t are generated by the joint calculation of s t and o t . Finally, the hidden state h pi t is obtained by fusing candidate hidden states ht and h pi s(t) based on update gate z t . A single DiaGRU cell can be formalized as follows:\n\nwhere σ(•) is the sigmoid function, ⊕ denotes the concatenation operation; s t and o t are the weights of self-and other-dependent reset gates, respectively; z t is the weight of update gate;\n\n, and b h are the learnable parameters.\n\nIn addition, as shown in Figure  5 (b), a speaker-specific utterance block of the participant in a conversation may contain multiple utterances. If h pi s(t) and h pj o(t) are equally exerted on each DiaGRU cell, the others-dependence will dominate the hidden state h pi t at the current moment. Inspired by ECM  [7] , we apply two exponential decay factors, i.e., β s,t and β o,t , for self-and other-dependency according to the time interval between two utterances. Our aim is that the longer the interval, the smaller the decay factor. The formula is as follows:\n\n,\n\nwhere µ s and µ o are the position hyperparameters, γ s and γ o are the shape hyperparameters, and these hyperparameters are utilized to control the speed of emotional decay; s(t) denotes the moment of the previous utterance that belongs to the same speaker as the utterance at moment t and is the nearest to that utterance, and o(t) denotes the moment of the previous utterance that is spoken by the interlocutor and is the nearest to the utterance at moment t; DiaGRU(•) is a simplified function of Equation  3 .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Skip-Chain Conditional Random Field",
      "text": "At the feature-extraction level, the global contextual features extracted by IMMHA and the local contextual features extracted by DiaGRU are combined to obtain identity-enhanced utterance features.\n\nConsidering that there are significant dependencies between emotions in the conversation, we explicitly model the emotion interactions at the classification level leveraging CRF to capture emotional flows from different participants as well as to obtain the optimal sequence of emotions.\n\nBecause of the complexity of computing the normalized factor in the graphical model, only firstorder dependency is commonly exploited in existing ERC methods  [32, 33] . In other words, only the dependency between direct neighboring utterances is taken into account. This simple processing makes it challenging to capture the complex interaction information of distinct speakers in the dialogue, while not clearly expressing the meaning represented by the non-normalized transition probability matrix in CRF. Therefore, by introducing higher-order dependency into the CRF, we elaborate a novel strategy named Skip-chain Conditional Random Field (SkipCRF) to model the emotion interactions in the dialogue and enhance the performance of emotion classification.\n\nAccording to the relevant settings of the CRF, the value x = (x 1 , x 2 , ..., x T ) is defined as the utterance feature extracted by encoders at the feature-extraction level, and y = (y 1 , y 2 , • • • , y T ) is defined as the corresponding emotion, where y t ∈ E. Referring to Equation 1, g i (•) represents the contextual local feature function, and f l (•) is the nodal feature function. Subject to Markov property  [29, 30] , the contextual local feature function g i (•) of the linear-chain CRF adopt only the neighboring information with unseen identity, resulting in its inability to distinguish between the impacts brought by the speaker and the interlocutor. In contrast, SkipCRF, subdivides contextual local feature function g i (•) into the self-dependent feature function h i (y s(t) , y t ) and the others-dependent feature function g j (y o(t) , y t ) by introducing the speaker identity. The conditional probability P(y|x) is defined as follows:\n\nwhere Z(x) is the normalization factor of all state sequences; ω n consists of λ i , η j , and µ l , which represents the learnable weight of the feature function.\n\nIt is noted that CRF belongs to the probabilistic graphical model, and the difficulty lies in the calculation of the normalization factor with exponential complexity. Therefore, we employ forward-backward algorithm  [39]  to recursively calculate P(y|x), making it linearly complex. Distinct from the linear-chain CRFs that have been applied in existing efforts  [32] [33] [34] , the designed SkipCRF distinguishes the identity information of the interlocutor. We define α t (y emotion sequences before the moment t when the speaker's emotion is y pi t and the interlocutor's emotion is y pj o(t) . Assuming the total number of possible emotion labels is K, we define A t (x) as a forward matrix/tensor consisting of K × K values:\n\nHere, the value of the m-th row and n-th column in A t (x) denotes the non-normalized probability when the interlocutor's m-th emotion and the speaker's n-th emotion. Following the setting of the forwardbackward algorithm, the non-normalized transition probability m t is defined as:\n\nThen, the K × K × K-dimensional non-normalized transition probability tensor M t is constructed through transition probability m t . Therefore, the recursive equations for the forward probability matrix A t and backward probability matrix B t are as follows:\n\nSo the expression of the normalization factor Z(x) is as follows:\n\nwhere 1 denotes the K-dimensional vector whose elements are all 1.\n\nIn the training process, given the utterance feature x = (x 1 , x 2 , • • • , x T ) and ground-truth label sequence y * = (y * 1 , y * 2 , • • • , y * T ), the objective is to maximize P(y * |x) of the ground-truth label sequence. It is converted into the following minimization objective by the negative logarithmic function:\n\nwhere Θ consists of the classification parameter Θ cls and the feature-extraction parameter Θ ext , and these two types of parameters can be updated by the back propagation algorithm. After completing conditional probability modeling, the decoding problem of SkipCRF requires to be solved. Given the conditional probability P(y|x) and the input feature x, the emotion sequence y when P(y|x) takes the maximum is obtained. We adopt the Viterbi algorithm  [40]\n\nwhere k 1 , k 2 , k1 , and k2 denote emotions corresponding to utterances. In addition, when δ t (y\n\nIt is noted that the above analysis is discussed under the premise of dyadic conversation scenario. As can be noticed through Equation  6 , the order of the forward tensor is equal to the number of participants, so the computational complexity increases exponentially with the number of participants, which is not conducive to multi-person conversation scenario. Therefore, we do some simplifications, i.e., eliminating those skip-chain connections that span the dyadic conversation in order to make it a multi-segment dyadic conversation. Note that this is still a full undirected graphical model. The specific example presented in Figure  6  provides a visual illustration of the discrepancy before and after elimination. It can be seen from Figure  6 (b) that the connection between moment 1 and moment 6, and the connection between moment 3 and moment 7 have been removed. The shaded area shows that the multi-party conversation is converted into a three-segment dyadic conversation after eliminating these connections. The implication is that only the interaction between two participants needs to be explicitly considered in the multi-party conversation, while the influence of other participants' utterances is implicitly encoded.\n\n4 Experimental settings With reference to COSMIC  [13] , we evaluate EmotionIC on four benchmark ERC datasets including IEMOCAP  [41] , DailyDialog  [42] , MELD  [43] , and EmoryNLP  [44] . The statistics of these datasets are reported in Table  1 . For the IEMOCAP dataset, we choose the weighted F1 and accuracy to validate the proposed model; for the DailyDialog dataset, the macro F1 and micro F1 excluding the majority class (Neutral) are utilized to evaluate our model; for the MELD and EmoryNLP datasets, we select the weighted F1 and micro F1. In addition, all data segmentation and pre-processing is consistent with COSMIC  [13] .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Evaluation Metrics And Datasets",
      "text": "Table  2  Partial hyperparameter settings for distinct datasets. LR-ext and LR-cls denote the learning rates for the featureextraction and classification levels, respectively. BS and DR are the batch size and dropout rate, respectively. D-IMMHA and D-DiaGRU indicate the network depths for IMMHA and DiaGRU modules, respectively. our model on the DailyDialog dataset is insignificant.\n\n(3) MELD. The utterances in the MELD dataset are much shorter than those in the IEMOCAP dataset, which means that emotional modeling is highly context-dependent. In addition, the fact that there are often two or more speakers in a conversation and that the amount of statements uttered by each participant is relatively small makes it difficult to model based on emotional inertia and contagion. Our EmotionIC reaches the best F1 score, even surpassing that of COSMIC which adds additional commonsense knowledge. We attribute the improvement to our contextual modeling at the classification level, i.e., SkipCRF. (4) EmoryNLP. Compared to baseline models, our EmotionIC achieves competitive performance on the EmoryNLP dataset. However, compared with on the MELD dataset, EmotionIC shows limited improvement on the EmoryNLP dataset for the same problem. The probable reason may be that EmoryNLP requires more commonsense knowledge. To further analyze the performance of EmotionIC, we show the confusion matrices for the four benchmark datasets in Figure  7 . Overall, the proposed EmotionIC performs well on the IEMOCAP and DailyDialog datasets. For the IEMOCAP dataset, our EmotionIC can accurately recognize the emotions of utterances in most scenarios, exhibiting superior performance. It suggests that our proposed model can effectively capture intra-and inter-speaker contextual dependencies based on emotional inertia and contagion. In order to exclude the effect of extreme majority class, we remove the records related to Neutral in the DailyDialog dataset. It can be seen that our model has high accuracy. Even though the class number of Happy accounts for a high proportion in the DailyDialog dataset, EmotionIC can still distinguish it easily from other emotions.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Analysis For Confusion Matrices",
      "text": "In comparison to the first two datasets, the performance of the model on the MELD and EmoryNLP datasets is somewhat unsatisfactory. We examine the datasets and find that the MELD and EmoryNLP datasets are dialogue segments extracted from the TV series Friends. In these two datasets, the length of most conversations is short and the two neighboring conversations may not be consecutive. Therefore, it is difficult for EmotionIC to exploit the capability of contextual modeling. In future studies, we will explore the use of commonsense knowledge to solve this problem. In addition, our model suffers from class imbalance and similar emotion problems. On the MELD dataset, Fear and Disgust belong to the minority classes, so, like most ERC models, EmotionIC has difficulty identifying them correctly. On the IEMOCAP dataset, Angry is easily recognized as similar emotion Frustrated in some scenarios. Figure  8  shows the F1 scores of EmotionIC for each emotion on the IEMOCAP and MELD datasets. On the IEMOCAP dataset, the emotion that achieves the highest F1 score is Sad, which suggests that this class is more easily distinguished by EmotionIC relative to the other emotions. However, the F1 scores for Angry and Happy are lower in comparison to the other emotions, indicating that these two emotions are susceptible to being recognized as other emotions such as Frustrated. On the MELD dataset, Neutral achieves the highest F1 score by a significant margin, while Fear and Disgust obtain extremely low results. By examining the class distribution of MELD, we notice that the sample share (approximately 46.95%) of Neutral in the dataset is far higher than that of Fear or Disgust. Therefore, we believe that the above phenomenon is due to the class imbalance problem. On the whole, the performance of EmotionIC for each class on the IEMOCAP dataset is more balanced due to the influence of the class imbalance in the MELD dataset.",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "Result For Each Emotion",
      "text": "",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Ablation Studies",
      "text": "In order to analyze the impact of different modules in EmotionIC, we observe the performance after removing or replacing each module in this subsection. The experimental results are recorded in Tables  4  and 5 . Overall, removing or replacing any of these modules leads to the performance degradation of EmotionIC, which demonstrates that our designed modules help to extract the contextual dependencies adequately.\n\nTable  4  displays the experimental results after the removal of each module, from which the following findings can be drawn. (1) On all datasets, removing IMMHA causes more performance degradation than removing DiaGRU. This indicates that capturing global context dependencies is more crucial than  We record the F1 scores after replacing each module in Table  5 , from which the following conclusions can be derived. (1) Since MHA is unable to utilize the identity information of participants, it cannot sufficiently model global contexts. (2) GRU fails to distinguish between intra-and inter-speaker dependency information, leading to low performance of the model. (3) The direct use of softmax layer cannot explicitly mimic emotional propagations in the conversation and cannot effectively mine emotional flows. (4) Linear-chain CRF focuses only on first-order dependencies at the classification level and cannot explicitly mine emotional flows between indirect neighbors. Meanwhile, Linear-chain CRF does not introduce participant identity to distinguish the influence of distinct speakers on the current utterance.",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "Effectiveness Of Skipcrf",
      "text": "To further prove that considering emotional flows in the conversation at the classification level can effectively improve the performance of the model, we conduct a case study with a conversation in the MELD dataset, as shown in Figure  9 . The dialogue presents emotional contagion between participants and their own emotional inertia. It can be observed that the first utterance (Turn 2) of Person B is incorrectly classified as Surprise due to the lack of reliable historical emotional information. From the misclassification of the 3-th and 5-th utterances, Neutral emotion is easily misclassified as negative one by the model employing Softmax layer. Our elaborate SkipCRF has significant advantages in modeling based on emotional inertia and contagion, proving the effectiveness of capturing emotional flows between different speakers at the classification level.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Comparison Of Modeling Emotional Inertia And Contagion",
      "text": "In this subsection, we discuss the discrepancy in modeling based on between emotional inertia and contagion in the proposed model. First, we extract data samples with emotional inertia and contagion in the test set. Then, we test these samples by employing F1 scores, i.e., as evaluation indicators of emotional inertia and contagion. Note that in the training or testing phase, we use all the samples in the training or test set; whereas in the calculation of F1 scores, only the samples related to emotional inertia or contagion are employed. The data samples with emotional inertia and contagion are extracted in the following ways.\n\n(  Table  6  shows that the modeling ability of our EmotionIC in terms of emotional inertia is superior to that in terms of emotional contagion. It is intuitive that the ability of the ERC model to simulate emotional inertia and contagion is to some extent related to the number of corresponding samples. That is, the larger the sample size of emotional inertia or contagion, the better the model should perform on the corresponding samples. However, the results in Table  6  suggest that it is not the case. Except for the IEMOCAP dataset, all other datasets show the opposite results. In other words, although the sample size of emotional contagion is larger than that of emotional inertia, the corresponding F1 scores are still lower, suggesting that utterances involving emotional contagion in the conversation are more difficult to classify accurately. We further count the number of samples with emotional inertia and contagion for different emotions in the DailyDialog dataset, as shown in Table  7 . EmotionIC performs well on the samples involving emotional inertia. However, the results on the samples involving emotional contagion show a large variance due to the extreme class imbalance. This phenomenon is consistent with the findings obtained from Table  6 , i.e., EmotionIC is more prone to accurately classify utterances involving emotional inertia.\n\nOur proposed EmotionIC is a novel approach driven by emotional inertia and contagion for the ERC task. EmotionIC adequately models a conversation at both the feature-extraction and classification levels, and consists of three main modules: IMMHA, DiaGRU, and SkipCRF. At the feature-extraction level, we utilize IMMHA to capture global contextual dependencies with identity information, and DiaGRU to extract speaker-and temporal-aware local contextual information. At the classification level, the designed SkipCRF is leveraged to capture complex emotional flows from higher-order neighboring utterances, which can explicitly simulate emotional propagations in the conversation. Since the optimal sequence of emotion labels can be obtained by utilizing SkipCRF, ERC does not require an additional softmax layer for classification. Extensive experimental results on the benchmark datasets confirm that the proposed EmotionIC can efficiently model contexts based on emotional inertia and contagion, which outperforms all baseline models.\n\nCurrent ERC tasks confront several formidable challenges, such as: (1) the conversation length in the dataset is too short to facilitate contextual modeling; (2) the class imbalance in the dataset leads to recognition results being biased towards the majority class/emotion; and (3) the model has difficulty in distinguishing similar emotions. Therefore, we will strive to mitigate these issues in our future effort. Furthermore, in order to further strengthen the capacity of our model to emulate emotional contagion, we intend to investigate the effects of external commonsense knowledge and multimodal methods on emotional mutations in future work.",
      "page_start": 16,
      "page_end": 16
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Example of contextual dependency modeling at both the feature-extraction and classification levels. The dashed",
      "page": 2
    },
    {
      "caption": "Figure 1: is an example of modeling context dependency at both the feature-extraction and",
      "page": 2
    },
    {
      "caption": "Figure 2: , which contains",
      "page": 4
    },
    {
      "caption": "Figure 2: Architecture of our EmotionIC. Firstly, the global and local context dependencies are extracted through IMMHA",
      "page": 5
    },
    {
      "caption": "Figure 3: , assuming",
      "page": 5
    },
    {
      "caption": "Figure 3: Illustration of utterance moment function. upi",
      "page": 5
    },
    {
      "caption": "Figure 4: Network structure of IMMHA. Ms and Mo are two mask matrices that mask contextual dependencies from",
      "page": 6
    },
    {
      "caption": "Figure 4: , we add two mask matrices (i.e., Ms and Mo) in IMMHA to capture",
      "page": 6
    },
    {
      "caption": "Figure 4: and can be described with the following formulation:",
      "page": 6
    },
    {
      "caption": "Figure 5: Illustration of DiaGRU. (a) A single DiaGRU cell. Here, xpi",
      "page": 7
    },
    {
      "caption": "Figure 5: (b), a speaker-specific utterance block of the participant in a con-",
      "page": 7
    },
    {
      "caption": "Figure 6: Example of CRF modeling for a three-person conversation scenario (a) before and (b) after eliminating skip-",
      "page": 10
    },
    {
      "caption": "Figure 6: provides a visual illustration of the discrepancy before and after elimination. It can be seen from",
      "page": 10
    },
    {
      "caption": "Figure 6: (b) that the connection between moment 1 and moment 6, and the connection between moment 3",
      "page": 10
    },
    {
      "caption": "Figure 7: Confusion matrices of the testing set on the (a) IEMOCAP, (b) DailyDialog, (c) MELD, and (d) EmoryNLP",
      "page": 12
    },
    {
      "caption": "Figure 7: Overall, the proposed EmotionIC performs well on the IEMOCAP and",
      "page": 12
    },
    {
      "caption": "Figure 8: F1 score for each emotion on the (a) IEMOCAP and (b) MELD datasets. Note that Hap represents for the",
      "page": 13
    },
    {
      "caption": "Figure 8: shows the F1 scores of EmotionIC for each emotion on the IEMOCAP and MELD datasets.",
      "page": 13
    },
    {
      "caption": "Figure 9: The dialogue presents emotional contagion between participants",
      "page": 14
    },
    {
      "caption": "Figure 9: Case study on the MELD dataset. Prediction (SkipCRF) denotes the use of SkipCRF for emotion classification,",
      "page": 15
    },
    {
      "caption": "Figure 10: It can be seen that although the proportions of Sad and Fear are quite different in the dataset,",
      "page": 15
    },
    {
      "caption": "Figure 10: Effect of the class imbalance in the dialogue and that in the dataset on the performance. Note that we explore",
      "page": 15
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Statistics for these four emotion datasets. #Conversation and #Utterance denote the number of conversations",
      "data": [
        {
          "Datasets": "#Conversation",
          "IEMOCAP\nDailyDialog\nMELD\nEmoryNLP": "108\n11,118\n1,039\n659\n12\n1,000\n114\n89\n31\n1,000\n280\n79"
        },
        {
          "Datasets": "#Utterance",
          "IEMOCAP\nDailyDialog\nMELD\nEmoryNLP": "5,163\n87,170\n9,989\n7,551\n647\n8,069\n1,109\n954\n1,623\n7,740\n2,610\n984"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 3: shows the performance comparison of the proposed EmotionIC",
      "data": [
        {
          "Models": "",
          "IEMOCAP": "Accuracy\nWeighted-F1",
          "DailyDialog": "Macro-F1\nMicro-F1",
          "MELD": "Micro-F1\nWeighted-F1",
          "EmoryNLP": "Micro-F1\nWeighted-F1"
        },
        {
          "Models": "COSMIC\nRGAT-ERC\nDialogXL\nDAG-ERC\nI-GCN\nLR-GCN\nCauAIN\nGAR-Net\nCoG-BART\nEmoCaps-Text",
          "IEMOCAP": "65.28\n-\n65.22\n-\n65.94\n-\n68.03\n-\n66.28\n-\n68.30\n68.50\n67.61\n-\n67.41\n-\n66.18\n66.71\n69.49\n-",
          "DailyDialog": "51.05\n58.48\n-\n54.31\n-\n54.93\n-\n59.33\n-\n-\n-\n-\n53.85\n58.21\n45.81\n56.97\n-\n56.29\n-\n-",
          "MELD": "65.21\n-\n60.91\n-\n62.41\n-\n63.65\n-\n65.74\n-\n65.60\n-\n65.46\n-\n62.11\n-\n64.81\n65.95\n63.51\n-",
          "EmoryNLP": "38.11\n-\n34.42\n-\n34.73\n-\n39.02\n-\n-\n-\n-\n-\n-\n-\n-\n-\n39.04\n42.58\n-\n-"
        },
        {
          "Models": "EmotionIC",
          "IEMOCAP": "69.61\n69.44",
          "DailyDialog": "54.19\n60.13",
          "MELD": "66.32\n67.59",
          "EmoryNLP": "40.25\n44.31"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 4: Impactofremovingdifferentmodules. -indicatesremovalofthecorrespondingmodule.",
      "data": [
        {
          "Methods": "",
          "IEMOCAP": "Weighted-F1",
          "DailyDialog": "Micro-F1",
          "MELD": "Weighted-F1",
          "EmoryNLP": "Weighted-F1"
        },
        {
          "Methods": "EmotionIC",
          "IEMOCAP": "69.61",
          "DailyDialog": "60.13",
          "MELD": "66.32",
          "EmoryNLP": "40.25"
        },
        {
          "Methods": "-IMMHA\n-DiaGRU",
          "IEMOCAP": "66.61 (↓3.00)\n67.92 (↓1.69)",
          "DailyDialog": "58.35 (↓1.78)\n59.78 (↓0.35)",
          "MELD": "65.77 (↓0.55)\n65.95 (↓0.37)",
          "EmoryNLP": "38.26 (↓1.99)\n38.28 (↓1.97)"
        },
        {
          "Methods": "-Mask Matrix Ms\n-Mask Matrix Mo",
          "IEMOCAP": "67.88 (↓1.73)\n69.17 (↓0.44)",
          "DailyDialog": "59.44 (↓0.69)\n58.49 (↓1.64)",
          "MELD": "65.92 (↓0.40)\n65.93 (↓0.39)",
          "EmoryNLP": "38.83 (↓1.42)\n39.12 (↓1.13)"
        },
        {
          "Methods": "-Reset Gate st\n-Reset Gate ot",
          "IEMOCAP": "69.47 (↓0.14)\n68.92 (↓0.69)",
          "DailyDialog": "59.99 (↓0.14)\n59.60 (↓0.53)",
          "MELD": "65.52 (↓0.80)\n66.17 (↓0.15)",
          "EmoryNLP": "38.87 (↓1.38)\n39.08 (↓1.17)"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table 4: Impactofremovingdifferentmodules. -indicatesremovalofthecorrespondingmodule.",
      "data": [
        {
          "Methods": "",
          "IEMOCAP": "Weighted-F1",
          "DailyDialog": "Micro-F1",
          "MELD": "Weighted-F1",
          "EmoryNLP": "Weighted-F1"
        },
        {
          "Methods": "EmotionIC",
          "IEMOCAP": "69.61",
          "DailyDialog": "60.13",
          "MELD": "66.32",
          "EmoryNLP": "40.25"
        },
        {
          "Methods": "-IMMHA +MHA\n-DiaGRU +GRU\n-SkipCRF +Softmax\n-SkipCRF +Linear-chain CRF",
          "IEMOCAP": "67.57 (↓2.04)\n66.31 (↓3.30)\n68.13 (↓1.48)\n69.17 (↓0.44)",
          "DailyDialog": "57.85 (↓2.28)\n59.48 (↓0.65)\n56.86 (↓3.27)\n59.78 (↓0.35)",
          "MELD": "65.50 (↓0.82)\n65.79 (↓0.53)\n63.99 (↓2.33)\n66.01 (↓0.31)",
          "EmoryNLP": "38.17 (↓2.08)\n39.02 (↓1.23)\n39.56 (↓0.69)\n39.18 (↓1.07)"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Turn": "1",
          "Utterance from Person A": "Okay I-I just have to stop by my \nplace first.",
          "Utterance from Person B": "",
          "Ground Truth": "Neutral",
          "Prediction (SkipCRF)": "Neutral",
          "Prediction (Softmax)": "Neutral"
        },
        {
          "Turn": "2",
          "Utterance from Person A": "",
          "Utterance from Person B": "To tape the game? You do this every \ntime Ross, you’re.",
          "Ground Truth": "Disgust",
          "Prediction (SkipCRF)": "Surprise",
          "Prediction (Softmax)": "Surprise"
        },
        {
          "Turn": "3",
          "Utterance from Person A": "No-no, I-I have to see if this \napartment became available.",
          "Utterance from Person B": "",
          "Ground Truth": "Neutral",
          "Prediction (SkipCRF)": "Neutral",
          "Prediction (Softmax)": "Fear"
        },
        {
          "Turn": "4",
          "Utterance from Person A": "",
          "Utterance from Person B": "Oh, you’re switching apartments?",
          "Ground Truth": "Surprise",
          "Prediction (SkipCRF)": "Surprise",
          "Prediction (Softmax)": "Surprise"
        },
        {
          "Turn": "5",
          "Utterance from Person A": "'It’s not for me, it’s for Rachel",
          "Utterance from Person B": "",
          "Ground Truth": "Neutral",
          "Prediction (SkipCRF)": "Neutral",
          "Prediction (Softmax)": "Sadness"
        },
        {
          "Turn": "6",
          "Utterance from Person A": "",
          "Utterance from Person B": "But Rachel has an apartment.",
          "Ground Truth": "Neutral",
          "Prediction (SkipCRF)": "Neutral",
          "Prediction (Softmax)": "Neutral"
        },
        {
          "Turn": "7",
          "Utterance from Person A": "Yeah, but when the baby comes \nshe’s gonna want to move.",
          "Utterance from Person B": "",
          "Ground Truth": "Neutral",
          "Prediction (SkipCRF)": "Neutral",
          "Prediction (Softmax)": "Neutral"
        },
        {
          "Turn": "8",
          "Utterance from Person A": "",
          "Utterance from Person B": "She is?",
          "Ground Truth": "Surprise",
          "Prediction (SkipCRF)": "Surprise",
          "Prediction (Softmax)": "Surprise"
        },
        {
          "Turn": "9",
          "Utterance from Person A": "Yeah, you didn’t expect her to \nlive there with a baby did you?",
          "Utterance from Person B": "",
          "Ground Truth": "Surprise",
          "Prediction (SkipCRF)": "Surprise",
          "Prediction (Softmax)": "Surprise"
        },
        {
          "Turn": "10",
          "Utterance from Person A": "",
          "Utterance from Person B": "I guess I didn’t really think about it.",
          "Ground Truth": "Neutral",
          "Prediction (SkipCRF)": "Neutral",
          "Prediction (Softmax)": "Neutral"
        }
      ],
      "page": 15
    },
    {
      "caption": "Table 6: F1 scores of EmotionIC on the samples with emotional inertia and contagion for different datasets. We adopt",
      "data": [
        {
          "Datasets": "Emotional Inertia",
          "IEMOCAP\nDailyDialog\nMELD\nEmoryNLP": "1,151\n454\n861\n242\n77.12\n74.08\n72.59\n47.04"
        },
        {
          "Datasets": "Emotional Contagion",
          "IEMOCAP\nDailyDialog\nMELD\nEmoryNLP": "410\n670\n1,003\n497\n49.82\n73.60\n61.07\n31.66"
        }
      ],
      "page": 16
    },
    {
      "caption": "Table 6: F1 scores of EmotionIC on the samples with emotional inertia and contagion for different datasets. We adopt",
      "data": [
        {
          "Emotions": "Emotional Inertia",
          "Joy\nAnger\nSadness\nFear\nSurprise\nDisgust": "330\n60\n31\n5\n8\n20\n79.05\n63.27\n55.81\n75.00\n62.50\n57.14"
        },
        {
          "Emotions": "Emotional Contagion",
          "Joy\nAnger\nSadness\nFear\nSurprise\nDisgust": "483\n34\n46\n9\n86\n12\n80.54\n42.55\n57.58\n18.18\n66.67\n35.29"
        }
      ],
      "page": 16
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Sentiment analysis is a big suitcase",
      "authors": [
        "E Cambria",
        "S Poria",
        "A Gelbukh"
      ],
      "year": "2017",
      "venue": "IEEE Intelligent Systems",
      "doi": "10.1109/MIS.2017.4531228"
    },
    {
      "citation_id": "2",
      "title": "Sentiment analysis using deep learning approaches: an overview",
      "authors": [
        "O Habimana",
        "Y Li",
        "R Li"
      ],
      "year": "2019",
      "venue": "Science China Information Sciences",
      "doi": "10.1007/s11432-018-9941-6"
    },
    {
      "citation_id": "3",
      "title": "DialogueRNN: An attentive rnn for emotion detection in conversations",
      "authors": [
        "N Majumder",
        "S Poria",
        "D Hazarika"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "4",
      "title": "MoEL: Mixture of empathetic listeners",
      "authors": [
        "Z Lin",
        "A Madotto",
        "J Shin"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
      "doi": "10.18653/v1/D19-1012"
    },
    {
      "citation_id": "5",
      "title": "Augmenting end-to-end dialogue systems with commonsense knowledge",
      "authors": [
        "T Young",
        "E Cambria",
        "I Chaturvedi"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "6",
      "title": "Human guided cooperative robotic agents in smart home using beetle antennae search",
      "authors": [
        "T Khan",
        "S Li",
        "X Cao"
      ],
      "year": "2022",
      "venue": "Science China Information Sciences",
      "doi": "10.1007/s11432-020-3073-5"
    },
    {
      "citation_id": "7",
      "title": "Emotional chatting machine: Emotional conversation generation with internal and external memory",
      "authors": [
        "H Zhou",
        "M L Huang",
        "T Y Zhang"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "8",
      "title": "Emotion recognition in conversation: Research challenges, datasets, and recent advances",
      "authors": [
        "S Poria",
        "N Majumder",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "9",
      "title": "DialogueGCN: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "S Poria"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)"
    },
    {
      "citation_id": "10",
      "title": "Directed acyclic graph network for conversational emotion recognition",
      "authors": [
        "Z Shen",
        "S Wu",
        "Y Y Yang"
      ],
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
      "doi": "10.18653/v1/2021.acl-long.123"
    },
    {
      "citation_id": "11",
      "title": "GraphCFC: A directed graph based cross-modal feature complementation approach for multimodal conversational emotion recognition",
      "authors": [
        "J Li",
        "X Wang",
        "G Lv"
      ],
      "venue": "IEEE Transactions on Multimedia",
      "doi": "10.1109/TMM.2023.3260635"
    },
    {
      "citation_id": "12",
      "title": "HiGRU: Hierarchical gated recurrent units for utterance-level emotion recognition",
      "authors": [
        "W Jiao",
        "H Yang",
        "I King"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter"
    },
    {
      "citation_id": "13",
      "title": "COSMIC: Commonsense knowledge for emotion identification in conversations",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "A Gelbukh"
      ],
      "venue": "COSMIC: Commonsense knowledge for emotion identification in conversations",
      "doi": "10.18653/v1/2020.findings-emnlp.224"
    },
    {
      "citation_id": "14",
      "title": "Knowledge-enriched transformer for emotion detection in textual conversations",
      "authors": [
        "P Zhong",
        "D Wang",
        "C Miao"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)"
    },
    {
      "citation_id": "15",
      "title": "Topic-driven and knowledge-aware transformer for dialogue emotion detection",
      "authors": [
        "L Zhu",
        "G Pergola"
      ],
      "venue": "Inf Sci"
    },
    {
      "citation_id": "16",
      "title": "of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
      "venue": "of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "17",
      "title": "GA2MIF: Graph and attention based two-stage multi-source information fusion for conversational emotion detection",
      "authors": [
        "J Li",
        "X Wang",
        "G Lv"
      ],
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2023.3261279"
    },
    {
      "citation_id": "18",
      "title": "Past, present, and future: Conversational emotion recognition through structural modeling of psychological knowledge",
      "authors": [
        "J N Li",
        "Z Lin",
        "P Fu"
      ],
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021"
    },
    {
      "citation_id": "19",
      "title": "Contextual reasoning networks for emotion recognition in conversations",
      "authors": [
        "D Hu",
        "L W Wei",
        "X Huai",
        "Dialoguecrn"
      ],
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "20",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar"
      ],
      "year": "2017",
      "venue": "Proceedings of the 31st International Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "21",
      "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
      "authors": [
        "J Chung",
        "C Gulcehre",
        "K Cho"
      ],
      "year": "2014",
      "venue": "NIPS 2014 Workshop on Deep Learning"
    },
    {
      "citation_id": "22",
      "title": "Affective computing: from laughter to ieee",
      "authors": [
        "R Picard"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "23",
      "title": "ICON: Interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "D Hazarika",
        "S Poria",
        "R Mihalcea"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "24",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "D Hazarika",
        "S Poria",
        "A Zadeh"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference of the North American Chapter"
    },
    {
      "citation_id": "25",
      "title": "DialogXL: All-in-one xlnet for multi-party conversation emotion recognition",
      "authors": [
        "W Shen",
        "J Q Chen",
        "X J Quan"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "26",
      "title": "XLNet: Generalized autoregressive pretraining for language understanding",
      "authors": [
        "Z L Yang",
        "Z Dai",
        "Y M Yang"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "27",
      "title": "Relational graph attention network for aspect-based sentiment analysis",
      "authors": [
        "K Wang",
        "W Shen",
        "Y Y Yang"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "28",
      "title": "Contrast and generation make bart a good dialogue emotion recognizer",
      "authors": [
        "M Li",
        "H Yan",
        "X Qiu"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "29",
      "title": "GAR-Net: A graph attention reasoning network for conversation understanding",
      "authors": [
        "H Xu",
        "Z Q Yuan",
        "K Zhao"
      ],
      "year": "2022",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "30",
      "title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
      "authors": [
        "J Lafferty",
        "A Mccallum",
        "F Pereira"
      ],
      "year": "2001",
      "venue": "Proc. 18th International Conf. on Machine Learning"
    },
    {
      "citation_id": "31",
      "title": "An introduction to conditional random fields",
      "authors": [
        "C Sutton",
        "A Mccallum"
      ],
      "year": "2012",
      "venue": "Foundations and Trends® in Machine Learning",
      "doi": "10.1561/2200000013"
    },
    {
      "citation_id": "32",
      "title": "A comprehensive review of markov random field and conditional random field approaches in pathology image analysis",
      "authors": [
        "Y Li",
        "C Li",
        "X Li"
      ],
      "year": "2022",
      "venue": "Archives of Computational Methods in Engineering",
      "doi": "10.1007/s11831-021-09591-w"
    },
    {
      "citation_id": "33",
      "title": "Contextualized emotion recognition in conversation as sequence tagging",
      "authors": [
        "Y Wang",
        "J Zhang",
        "J Ma"
      ],
      "year": "2020",
      "venue": "Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue"
    },
    {
      "citation_id": "34",
      "title": "EmotionFlow: Capture the dialogue level emotion transitions",
      "authors": [
        "X H Song",
        "L Zang",
        "R Zhang"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "35",
      "title": "S+PAGE: A speaker and position-aware graph neural network model for emotion recognition in conversation",
      "authors": [
        "C Liang",
        "C Yang",
        "J Xu"
      ],
      "year": "2021",
      "venue": "S+PAGE: A speaker and position-aware graph neural network model for emotion recognition in conversation"
    },
    {
      "citation_id": "36",
      "title": "Interactive double states emotion cell model for textual dialogue emotion prediction. Knowledge-Based Systems",
      "authors": [
        "Y Li",
        "Y Li",
        "S Wang"
      ],
      "year": "2020",
      "venue": "Interactive double states emotion cell model for textual dialogue emotion prediction. Knowledge-Based Systems"
    },
    {
      "citation_id": "37",
      "title": "Emotional contagion",
      "authors": [
        "E Hatfield"
      ],
      "year": "1993",
      "venue": "Current Directions in Psychological Science"
    },
    {
      "citation_id": "38",
      "title": "A comprehensive review of conditional random fields: variants, hybrids and applications",
      "authors": [
        "B Yu"
      ],
      "year": "2020",
      "venue": "Artificial Intelligence Review",
      "doi": "10.1007/s10462-019-09793-6"
    },
    {
      "citation_id": "39",
      "title": "Rethinking positional encoding in language pre-training",
      "authors": [
        "G Ke",
        "D He",
        "T Liu"
      ],
      "year": "2021",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "40",
      "title": "Space-efficient inference in dynamic probabilistic networks",
      "authors": [
        "J Binder",
        "K Murphy",
        "S Russell"
      ],
      "year": "1997",
      "venue": "Bclr"
    },
    {
      "citation_id": "41",
      "title": "The viterbi algorithm",
      "authors": [
        "G Forney"
      ],
      "year": "1973",
      "venue": "Proceedings of the IEEE",
      "doi": "10.1109/PROC.1973.9030"
    },
    {
      "citation_id": "42",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database. Language resources and evaluation",
      "authors": [
        "C Busso",
        "M Bulut",
        "C C Lee"
      ],
      "year": "2008",
      "venue": "IEMOCAP: Interactive emotional dyadic motion capture database. Language resources and evaluation"
    },
    {
      "citation_id": "43",
      "title": "DailyDialog: A manually labelled multi-turn dialogue dataset",
      "authors": [
        "Y R Li",
        "H Su",
        "X Y Shen"
      ],
      "year": "2017",
      "venue": "Proceedings of the Eighth International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "44",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "45",
      "title": "Emotion detection on tv show transcripts with sequence-based convolutional neural networks",
      "authors": [
        "M Zahiri",
        "J Choi"
      ],
      "year": "2017",
      "venue": "AAAI Workshops"
    },
    {
      "citation_id": "46",
      "title": "Decoupled weight decay regularization",
      "authors": [
        "I Loshchilov",
        "F Hutter"
      ],
      "year": "2019",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "47",
      "title": "I-GCN: Incremental graph convolution network for conversation emotion detection",
      "authors": [
        "W Nie",
        "R Chang",
        "M Ren"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Multimedia",
      "doi": "10.1109/TMM.2021.3118881"
    },
    {
      "citation_id": "48",
      "title": "LR-GCN: Latent relation-aware graph convolutional network for conversational emotion recognition",
      "authors": [
        "M Ren",
        "X Huang",
        "W Li"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Multimedia",
      "doi": "10.1109/TMM.2021.3117062"
    },
    {
      "citation_id": "49",
      "title": "Causal aware interaction network for emotion recognition in conversations",
      "authors": [
        "W Zhao",
        "Y Zhao",
        "Lu Cauain"
      ],
      "venue": "Raedt L D. Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22",
      "doi": "10.24963/ijcai.2022/628"
    },
    {
      "citation_id": "50",
      "title": "EmoCaps: Emotion capsule based model for conversational emotion recognition",
      "authors": [
        "Z Li",
        "F Tang",
        "M Zhao"
      ],
      "venue": "Findings of the Association for Computational Linguistics: ACL 2022",
      "doi": "10.18653/v1/2022.findings-acl.126"
    }
  ]
}