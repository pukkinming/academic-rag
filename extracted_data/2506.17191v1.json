{
  "paper_id": "2506.17191v1",
  "title": "Facial Landmark Visualization And Emotion Recognition Through Neural Networks",
  "published": "2025-06-20T17:45:34Z",
  "authors": [
    "Israel Juárez-Jiménez",
    "Tiffany Guadalupe Martínez Paredes",
    "Jesús García-Ramírez",
    "Eric Ramos Aguilar"
  ],
  "keywords": [
    "Landmark visualization",
    "Emotion recognition",
    "Neural networks optimization"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition from facial images is a crucial task in human-computer interaction, enabling machines to learn human emotions through facial expressions. Previous studies have shown that facial images can be used to train deep learning models; however, most of these studies do not include a through dataset analysis. Visualizing facial landmarks can be challenging when extracting meaningful dataset insights; to address this issue, we propose facial landmark box plots, a visualization technique designed to identify outliers in facial datasets. Additionally, we compare two sets of facial landmark features: (i) the landmarks' absolute positions and (ii) their displacements from a neutral expression to the peak of an emotional expression. Our results indicate that a neural network achieves better performance than a random forest classifier.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition through facial expressions is a key area of research in human-computer interaction and artificial intelligence, as facial expressions are one of the primaries means of non-verbal communication. Different methods have addressed this challenge, including traditional models such as random forests  [10]  and modern approaches based on deep neural networks  [6] .\n\nSome of the related works are based on extracting geometric features from facial landmarks  [10] , while others have used convolutional neural network architectures to learn representations directly from images  [6] , and also, a combination of both  [7] . Statistical methods have previously been applied to identify patterns in facial points, focusing on discriminating between emotions such as happiness, sadness, or surprise  [9] . However, these techniques often face limitations, such as high data dimensionality or the need for large amounts of labeled data.\n\nDespite advances, one of the main challenges is efficiency in data processing and analysis. Traditional methods, such as decision trees, are computationally less expensive, but present limitations in accuracy and generalization capacity. Deep neural network models address these issues, but they require more training time and significant computational resources.\n\nThis work proposes an approach that combines statistical methods with neural networks to improve emotion recognition in facial images. Unlike other studies, this method uses facial landmark normalization based on nose centering and dimensionality reduction through quartile analysis. To visualize the facial dataset, we propose facial landmark box plots in order to determine atypical data.\n\nAdditionally, an improved neural network is implemented with Batch Normalization techniques and GELU activations, optimized to handle the extracted features efficiently. The proposed approach includes a comparison of two models: a decision tree and a neural network. Experimental results show that, while the decision tree achieved an accuracy of 80%, the optimized neural network performed exceptionally well with an accuracy of 98.48%.\n\nThis paper is organized as follows: in Section 2 we present the related work; section 3 introduce the proposed methodology; section 4 shows the experimental results for the data visualization and classification stage; finally, section 5 presents the conclusions and future work.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Works",
      "text": "Several works have addressed facial emotion recognition using methods based on landmarks and neural networks  [4, 13] . Among them, some studies have employed traditional statistical techniques to analyze the geometric features of the face  [15] , while others have explored more advanced models that use deep neural networks to learn representations directly from images.\n\nFacial landmark analysis has been widely used in previous research  [5] . For example, methods based on calculating distances between facial key points have proven effective in discriminating emotions such as happiness and sadness. However, these approaches often face limitations such as sensitivity to the initial position of landmarks and reliance on precise preprocessing to normalize the data.\n\nIn terms of neural networks, some works have implemented deep learning models that train convolutional networks to classify emotions directly from facial images. For instance, Picazo et al.  [12]  used manually labeled datasets to train networks for emotion recognition with high accuracy. However, these methods demand substantial labeled data and lengthy training times.\n\nOther approaches have explored hybrid architectures that combine traditional geometric processing techniques with deep neural networks. For example,  [11]  proposed a two-step scheme in which a hidden representation is learned using convolutional layers, which is then transferred to a target task using a custom loss function. Despite promising results, this approach has high computational complexity, as it involves training multiple models sequentially.\n\nIn addition, initial efforts have been made to apply transfer learning strategies to emotion recognition  [14] . Fine-tuning techniques have been experimented with to reuse convolutional layers from pre-trained models on different datasets  [1] .\n\nThe work most similar to ours is  [2] , who used a VGG neural network to find hidden representations that can be used as feature extractor to classify different emotions. Unlike previous studies, our approach reduces the dimensionality of the data using quartile analysis and normalizes facial landmarks, which significantly improves the accuracy and efficiency of the final model.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Proposed Methodology",
      "text": "This section presents the proposed methodology for emotion recognition and data visualization. The proposed methodology can be seen in Figure  1 . First, we perform a preprocessing stage in which we select the dataset. Next, we apply a deep learning model to extract facial landmarks from the images. In the following stage, we conduct a statistical analysis, introducing boxplots of the detected facial landmarks. Finally, we employ Random Forest and neural networks for emotion classification.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Preprocessing Stage",
      "text": "The preprocessing stage is described in this section. For this study, we use the CK+ dataset, which contains facial images transitioning from neutral expressions to maximum emotions. The CK+ datasets includes 593 sequences that had a nominal emotion labeled based on the subject impression of 7 emotions: anger, disgust, contempt, fear, happy, sadness and surprise  [8] . Some examples of the dataset can be seen in the Figure  2 .\n\nThe preprocessing consists of three main steps. First, we select two key images for each subject: the first frame, which represents a neutral expression, and the last frame, which captures the maximum emotional expression. Then, we perform a validation step to identify and remove incomplete entries or inconsistencies between emotion labels and images. The process involved selecting the first frame (neutral expression) and the last frame (peak emotion) from each sequence in the CK+ dataset  [8] . The decision was based on the dataset's predefined labels, and no additional annotators were required as the labels were already validated. The validation step ensured consistency by removing sequences with missing frames or mismatched labels, resulting in a curated dataset ready for analysis.\n\nAfter validation, the dataset consisted of paired images (neutral and peak emotion) for each of the 7 emotions: anger, disgust, fear, happiness, sadness, surprise, and contempt. The final dataset was balanced by addressing inconsistencies, ensuring each emotion had sufficient representation, though class imbalance remained (e.g., surprise was overrepresented, as shown in Fig.  3 ). This step was critical for reliable feature extraction and model training.\n\nAt the end of this process, we obtain a structured and curated dataset, ready for facial landmark extraction and further analysis.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Landmark Detection",
      "text": "Facial landmark detection and analysis involve several key steps to extract meaningful features from the images. First, the study employed Dlib's pre-trained model, which detects 68 facial landmarks using a histogram of oriented gradients (HOG) feature-based detector. This model was chosen for its robustness and accuracy in identifying key facial features. Once the face is detected, we apply a pre-trained model to identify 68 facial landmarks 1  . These landmarks correspond to key facial features, including the eyes, eyebrows, nose, and mouth.\n\nTo ensure consistency in the extracted features, we perform a normalization step. The landmarks are centered with respect to the midpoint between the eyes and scaled using Min-Max normalization, which eliminates variations due to differences in face position and size. Finally, for each subject, we compute the differences between the landmarks in the neutral and peak expression images. This process generates feature vectors that capture the facial deformations associated with different emotions.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Statistical Analysis",
      "text": "To enhance the quality of the extracted features, we perform a statistical analysis in two main stages. First, we identify outliers by computing quartiles and the interquartile range (IQR) for the landmark coordinates. Any values falling outside the limits defined by Q1 -1.5 • IQR and Q3 + 1.5 • IQR are considered outliers and handled accordingly, either by removing them or adjusting their values to mitigate their impact on the model.\n\nNext, we categorize the landmark points into three groups based on their statistical distribution. Points within the quartile range are considered normal and representative of the central distribution. Those located within the whiskers of the boxplot are near the quartile boundaries but are not classified as outliers. Finally, points falling outside the whiskers are identified as outliers and require special treatment.\n\nAdditionally, we generate various visualizations to better analyze and understand the data. Histograms and pie charts are used to illustrate the distribution of emotions within the dataset, ensuring a balanced representation of each class for classification purposes. Scatter plots and boxplots are also constructed to examine the distributions and variability of the distances between landmarks in neutral and peak emotion expressions.\n\nThis statistical analysis ensures the quality and representativeness of the features used in the classification models.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Classification Stage",
      "text": "Two models were implemented and evaluated for emotion classification: one based on decision trees and the other using neural networks with different configurations. We select this algorithms because we want to compare an algorithm that is much interpretable as Random Forest and other that obtain an acceptable performance in classification tasks as Neural Networks.\n\nThe first model is a decision tree, an interpretable method that organizes the features into a binary hierarchical structure  [3] . In this model, successive splits of the data are made based on their importance, determined using criteria such as gini or entropy. Key hyperparameters were tuned, such as the maximum depth of the tree, the splitting criterion, and the minimum number of samples per leaf. This model served as an initial reference to assess the complexity of the problem and to establish a baseline for comparison with more advanced models.\n\nSubsequently, two neural network models were implemented with different configurations. The first was a basic neural network, consisting of two fully connected layers: the first with 256 neurons and the second with 128 neurons, both using the ReLU activation function and regularized with 50% Dropout, this also stabilize the learning curves. The output layer contained 7 neurons, corresponding to the emotional classes, and used the Softmax activation function.\n\nFor training, the Adam optimizer, cross entropy loss function, and a training schedule of 50 epochs were employed.\n\nTo improve performance, an optimized neural network was designed, incorporating more advanced architectures. This network consisted of three fully connected layers: the first with 512 neurons and GELU activation, followed by a second layer with 256 neurons and a residual connection to enhance gradient propagation. The third layer had 128 neurons, maintaining the GELU activation across all layers. Additionally, Batch Normalization was applied to stabilize training, and 30% Dropout was used to prevent overfitting. The Adam optimizer with an initial learning rate of 0.001 and a learning rate scheduler were used to improve model convergence.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Experimental Results",
      "text": "In this section we present the experimental results for the data analysis and then we present the results of the training stage.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Data Visualization",
      "text": "In the Fig.  3  we show the distribution of the data of the used dataset, in this case CK+  [8] , where seven labeled emotions are contained. We can see that there is a clear imbalance of the classes, surprise is the class with more data, while contempt and fear are the classes with low instances, respectively. The imbalance can bias the model to the more representative classes.\n\nAlso, we introduce facial landmark boxplot, where the distribution of the data can be seen in a single figure instead of use one box plot for each landmark. The main idea is to plot the entire points of the landmarks taking into account the points outside the IQR in a different mark, in order to visualize how many of them are in the data distribution.\n\nFigure  4  illustrates the distribution of emotions in the CK+ dataset, revealing a significant imbalance. Surprise is the most represented emotion, accounting for 25.3% of the dataset, while Contempt and Fear are the least frequent, with 5.5% and 7.6%, respectively. This imbalance could bias the model toward the more common emotions, such as surprise and happiness, while making it more challenging to recognize the less frequent ones. Therefore, it is crucial to consider techniques to address this imbalance and ensure fair classification across all emotions.\n\nTable  1  presents the distribution of facial landmarks within the whiskers and the percentage of outliers for each emotion. Contempt exhibits the highest proportion of points within the whiskers (68.71%), indicating a more concentrated and uniform distribution. In contrast, fear shows the highest percentage of outliers (6.12%), suggesting greater variability in its expression. Surprise, with the largest number of total points (5644), maintains a high proportion of points within the whiskers (66.85%) and a low outlier rate (3.19%), reflecting its strong representation in the dataset. Emotions such as anger, disgust, and sadness display similar distributions, with approximately 64-66% of points inside the whiskers and around 3-4% classified as outliers. The results highlight the",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Classification Stage",
      "text": "In our experiments we use two popular algorithms, neural networks and random forests because these work well with similar data to ours. Nevertheless, random forests shows poor performance compared with neural networks. We perform the experiments using a 4-fold cross validation, that and we present the learning curves for the best model. In order to compare the classifiers we present the mean of the for experiments in the next paragraphs. The Random Forest model achieved an accuracy of 80% . While this performance is moderate, it suggests that the model effectively captures relevant patterns in the data. However, the complexity of the extracted features may have limited further improvements. In comparison, the Decision Tree model, despite its interpretability, exhibited suboptimal results, reinforcing the need for more sophisticated techniques to enhance classification performance. These findings highlight the importance of balancing model complexity and interpretability to achieve robust emotion recognition.\n\nIn the case of neural networks, we apply the configuration described in the previous section. In our experiments we observe that the loss on the test set stabilizes after several epochs and exhibits slight variability, which is expected in generalization processes. The minimal gap between training and test loss in the final epochs indicates that the model has achieved good generalization without significant overfitting. This suggests that the model can correctly classify new samples without being overly influenced by specific patterns in the training set.\n\nFigure  5  shows the training and test accuracy over epochs with the neural network that use ReLU activation function (Also, it can be seen the loss of the training in Figure  6 ). Similar to the loss behavior, the training accuracy improves rapidly in the initial epochs, reaching values close to 100% by the end of training.\n\nThis indicates that the model has effectively learned the patterns present in the training set. In the test set, accuracy increases quickly in the early epochs and stabilizes around 96.97%. The close proximity between training and test accuracy curves suggests that the model generalizes well, achieving high accuracy on unseen data without overfitting. This demonstrates the model's robustness and its ability to accurately classify emotions in facial images.\n\nFigure  7  illustrates the evolution of loss during training and evaluation with the GeLU activation function. Initially, the loss is high but rapidly decreases in the early epochs, indicating that the model quickly learns the fundamental patterns of emotions. As training progresses, the training loss continues to decrease, approaching zero, suggesting a good fit to the data.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Conclusions And Future Work",
      "text": "This work presented an integrated approach for facial emotion recognition, using the CK+ dataset for feature extraction and classification. The proposed method included key steps such as data preprocessing, facial landmark normalization, statistical analysis for feature enhancement, and classification using models of varying complexity.\n\nThe evaluation of different models provided valuable insights into their effectiveness. The Decision Tree classifier, while interpretable and computationally efficient, achieved a moderate accuracy of 80%, struggling with complex feature representations. The Basic Neural Network significantly outperformed it, reaching over 96% accuracy, making it a suitable choice for real-time applications due to its fast training and inference. The Optimized Neural Network emerged as the most effective model, achieving 98.48% accuracy, demonstrating strong generalization capabilities through advanced architecture, batch normalization, residual connections, and hyperparameter tuning. However, this higher accuracy came at the cost of increased training time.\n\nThe comparison highlights the trade-off between simplicity, speed, and accuracy when selecting a model. Simpler models are preferable for fast tasks, whereas more complex architectures offer higher accuracy but require greater computational resources.\n\nFuture work could explore transfer learning with pretrained models to improve performance in real-world scenarios, where lighting, pose variations, and demographic diversity impact facial expressions. Additionally, data augmentation techniques could enhance generalization by addressing the dataset's limited variability. Finally, developing lightweight models optimized for low-resource devices could enable practical applications in real-time monitoring and mobile systems.",
      "page_start": 10,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: First, we",
      "page": 3
    },
    {
      "caption": "Figure 1: Proposed methodology for emotion recognition.",
      "page": 3
    },
    {
      "caption": "Figure 2: The preprocessing consists of three main steps. First, we select two key images",
      "page": 3
    },
    {
      "caption": "Figure 2: Examples of the CK+ dataset, this dataset is used in our experiments.",
      "page": 4
    },
    {
      "caption": "Figure 3: ). This step",
      "page": 4
    },
    {
      "caption": "Figure 3: we show the distribution of the data of the used dataset, in this",
      "page": 6
    },
    {
      "caption": "Figure 4: illustrates the distribution of emotions in the CK+ dataset, revealing",
      "page": 6
    },
    {
      "caption": "Figure 3: Distribution of the classes in the CK+ dataset, disgust, happiness and",
      "page": 7
    },
    {
      "caption": "Figure 4: Examples of facial landmarks boxplots for Surprise, Fear, Happiness, Sad-",
      "page": 7
    },
    {
      "caption": "Figure 5: shows the training and test accuracy over epochs with the neural",
      "page": 8
    },
    {
      "caption": "Figure 6: ). Similar to the loss behavior, the training accuracy improves",
      "page": 8
    },
    {
      "caption": "Figure 5: Accuracy learning rate for the neural network that use ReLU activation",
      "page": 9
    },
    {
      "caption": "Figure 6: Loss for the neural network that use ReLU activation function.",
      "page": 9
    },
    {
      "caption": "Figure 7: illustrates the evolution of loss during training and evaluation with",
      "page": 9
    },
    {
      "caption": "Figure 7: Loss learning curve for the neural network that use GeLU activation",
      "page": 10
    },
    {
      "caption": "Figure 8: presents the accuracy throughout training and testing. The",
      "page": 10
    },
    {
      "caption": "Figure 8: Accuracy learning curve for the neural network that use GeLU activation",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Distribution of points within whiskers and outliers for each emotion.",
      "data": [
        {
          "Emotion": "Anger\nContempt\nDisgust\nFear\nHappiness\nSadness\nSurprise",
          "%\nin\nWhiskers": "64.51%\n68.71%\n64.36%\n58.71%\n62.47%\n65.70%\n66.85%",
          "%\nOut-\nliers": "2.78%\n3.76%\n3.91%\n6.12%\n3.39%\n3.83%\n3.19%",
          "Observations": "Expected distribution,\nlow variability.\nMore concentrated and uniform distribution.\nVariability within quartiles: 31.73%.\nHighest variability, more outliers.\nProportion within quartiles: 34.14%.\nModerate proportion within quartiles: 30.46%.\nLargest\ntotal number of points\n(5644), well-\nrepresented."
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Facial emotion recognition using transfer learning in the deep cnn",
      "authors": [
        "M Akhand",
        "S Roy",
        "N Siddique",
        "M Kamal",
        "T Shimamura"
      ],
      "year": "2021",
      "venue": "Electronics"
    },
    {
      "citation_id": "2",
      "title": "Reconocimiento de emociones usando multilayer perceptron neural network (mlp). 8vo",
      "authors": [
        "H Barboza"
      ],
      "year": "2020",
      "venue": "Reconocimiento de emociones usando multilayer perceptron neural network (mlp). 8vo"
    },
    {
      "citation_id": "3",
      "title": "Random forests",
      "authors": [
        "L Breiman"
      ],
      "year": "2001",
      "venue": "Machine Learning",
      "doi": "10.1023/A:1010933404324"
    },
    {
      "citation_id": "4",
      "title": "A randomized deep neural network for emotion recognition with landmarks detection",
      "authors": [
        "F Di Luzio",
        "A Rosato",
        "M Panella"
      ],
      "year": "2023",
      "venue": "Biomedical signal processing and control"
    },
    {
      "citation_id": "5",
      "title": "Real-time facial expression recognition using facial landmarks and neural networks",
      "authors": [
        "M Haghpanah",
        "E Saeedizade",
        "M Masouleh",
        "A Kalhor"
      ],
      "year": "2022",
      "venue": "2022 International Conference on Machine Vision and Image Processing"
    },
    {
      "citation_id": "6",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2020",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "7",
      "title": "Conditional convolution neural network enhanced random forest for facial expression recognition",
      "authors": [
        "Y Liu",
        "X Yuan",
        "X Gong",
        "Z Xie",
        "F Fang",
        "Z Luo"
      ],
      "year": "2018",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "8",
      "title": "The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression",
      "authors": [
        "E Lucas",
        "M Valstar",
        "J Snape",
        "M Pantic"
      ],
      "year": "2010",
      "venue": "Proceedings of the 3rd International Conference on Affective Computing and Intelligent Interaction (ACII 2010)",
      "doi": "10.1109/ACII.2010.5644614"
    },
    {
      "citation_id": "9",
      "title": "Locating facial landmarks using probabilistic random forest",
      "authors": [
        "C Luo",
        "Z Wang",
        "S Wang",
        "J Zhang",
        "J Yu"
      ],
      "year": "2015",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "10",
      "title": "Facial expression recognition using facial landmarks and random forest classifier",
      "authors": [
        "M Munasinghe"
      ],
      "year": "2018",
      "venue": "IEEE/ACIS 17th International Conference on Computer and Information Science (ICIS)"
    },
    {
      "citation_id": "11",
      "title": "Análisis semántico en rostros utilizando redes neuronales profundas",
      "authors": [
        "N Pellejero",
        "G Grinblat",
        "L Uzal"
      ],
      "year": "2017",
      "venue": "XVIII Simposio Argentino de Inteligencia Artificial (ASAI)-JAIIO"
    },
    {
      "citation_id": "12",
      "title": "Redes Neuronales Convolucionales Profundas para el reconocimiento de emociones en imágenes",
      "authors": [
        "Picazo Montoya"
      ],
      "year": "2018",
      "venue": "Redes Neuronales Convolucionales Profundas para el reconocimiento de emociones en imágenes"
    },
    {
      "citation_id": "13",
      "title": "I know how you feel: Emotion recognition with facial landmarks",
      "authors": [
        "I Tautkute",
        "T Trzcinski",
        "A Bielski"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition workshops"
    },
    {
      "citation_id": "14",
      "title": "Transfer: Learning relation-aware facial expression representations with transformers",
      "authors": [
        "F Xue",
        "Q Wang",
        "G Guo"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "15",
      "title": "Action unit classification for facial expression recognition using active learning and svm",
      "authors": [
        "L Yao",
        "Y Wan",
        "H Ni",
        "B Xu"
      ],
      "year": "2021",
      "venue": "Multimedia Tools and Applications"
    }
  ]
}