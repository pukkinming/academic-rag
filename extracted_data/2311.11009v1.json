{
  "paper_id": "2311.11009v1",
  "title": "Joyful: Joint Modality Fusion And Graph Contrastive Learning For Multimodal Emotion Recognition",
  "published": "2023-11-18T08:21:42Z",
  "authors": [
    "Dongyuan Li",
    "Yusong Wang",
    "Kotaro Funakoshi",
    "Manabu Okumura"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal emotion recognition aims to recognize emotions for each utterance of multiple modalities, which has received increasing attention for its application in human-machine interaction. Current graph-based methods fail to simultaneously depict global contextual features and local diverse uni-modal features in a dialogue. Furthermore, with the number of graph layers increasing, they easily fall into over-smoothing. In this paper, we propose a method for joint modality fusion and graph contrastive learning for multimodal emotion recognition (JOYFUL), where multimodality fusion, contrastive learning, and emotion recognition are jointly optimized. Specifically, we first design a new multimodal fusion mechanism that can provide deep interaction and fusion between the global contextual and uni-modal specific features. Then, we introduce a graph contrastive learning framework with inter-view and intra-view contrastive losses to learn more distinguishable representations for samples with different sentiments. Extensive experiments on three benchmark datasets indicate that JOYFUL achieved state-of-the-art (SOTA) performance compared to all baselines. 1",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Multimodal Emotion Recognition",
      "text": "Depending on how to model the context of utterances, existing MERC methods are categorized into three classes: Recurrent-based methods  (Majumder et al., 2019; Mao et al., 2022)  adopt RNN or LSTM to model the sequential context for each utterance. Transformers-based methods  (Ling et al., 2022; Liang et al., 2022; Le et al., 2022)  use Transformers with cross-modal attention to model the intra-and inter-speaker dependencies. Graphbased methods  (Joshi et al., 2022; Zhang et al., 2021; Fu et al., 2021)  can control context information for each utterance and provide accurate intraand inter-speaker dependencies, achieving SOTA performance on many MERC benchmark datasets.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Multimodal Fusion Mechanism",
      "text": "Learning effective fusion mechanisms is one of the core challenges in multimodal learning  (Shankar, 2022) . By capturing the interactions between different modalities more reasonably, deep models can acquire more comprehensive information. Current fusion methods can be classified into aggregationbased  (Wu et al., 2021; Guo et al., 2021) , alignmentbased  (Liu et al., 2020; Li et al., 2022e) , and their mixture  (Wei et al., 2019; Nagrani et al., 2021) . Aggregation-based fusion methods  (Zadeh et al., 2017; Chen et al., 2021)  adopt concatenation, tensor fusion and memory fusion to combine multiple modalities. Alignment-based fusion centers on latent cross-modal adaptation, which adapts streams from one modality to another  (Wang et al., 2022a) . Different from the above methods, we learn global contextual information by concatenation while fully exploring the specific patterns of each modality in an alignment manner.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Graph Contrastive Learning",
      "text": "GCL aims to learn representations by maximizing feature consistency under differently augmented views, that exploit data-or task-specific augmentations, to inject the desired feature invariance  (You et al., 2020) . GCL has been well used in the NLP community via self-supervised and supervised settings. Self-supervised GCL first creates augmented graphs by edge/node deletion and insertion  (Zeng and Xie, 2021) , or attribute masking  (Zhang et al., 2022) . It then captures the intrinsic patterns and properties in the augmented graphs without using human provided labels. Supervised GCL designs adversarial  (Sun et al., 2022)  or geometric  (Li et al., 2022d)  contrastive loss to make full use of label in-formation. For example,  Li et al. (2022c)  first used supervised CL for emotion recognition, greatly improving the performance. Inspired by previous studies, we jointly consider self-supervised (suitable graph augmentation) and supervised (crossentropy) manners to fully explore graph structural information and downstream supervisory signals.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Methodology",
      "text": "Figure  2  shows an overview of JOYFUL, which mainly consists of four components: (A) a unimodal extractor, (B) a multimodal fusion (MF) module, (C) a graph contrastive learning module, and (D) a classifier. Hereafter, we give formal notations and the task definition of JOYFUL, and introduce each component subsequently in detail.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Notations And Task Definition",
      "text": "In dialogue emotion recognition, a training dataset\n\n, X t be the visual, audio, and text feature spaces, respectively. The goal of MERC is to learn a function F : X v × X a × X t → Y that can recognize the emotion label for each utterance. We utilize three widely used multimodal conversational benchmark datasets, namely IEMO-CAP, MOSEI, and MELD, to evaluate the performance of our model. Please see Section 4.1 for their detailed statistical information.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Uni-Modal Extractor",
      "text": "For IEMOCAP  (Busso et al., 2008) , video features x v ∈ R 512 , audio features x a ∈ R 100 , and text fea-tures x t ∈ R 768 are obtained from OpenFace  (Baltrusaitis et al., 2018) , OpenSmile  (Eyben et al., 2010)  and SBERT  (Reimers and Gurevych, 2019) , respectively. For MELD  (Poria et al., 2019a) , x v ∈ R 342 , x a ∈ R 300 , and x t ∈ R 768 are obtained from DenseNet  (Huang et al., 2017) , OpenSmile, and TextCNN  (Kim, 2014) . For MOSEI  (Zadeh et al., 2018) , x v ∈ R 35 , x a ∈ R 80 , and x t ∈ R 768 are obtained from TBJE  (Delbrouck et al., 2020) , LibROSA  (Raguraman et al., 2019) , and SBERT. Textual features are sentence-level static features. Audio and visual modalities are utterance-level features by averaging all the token features.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Multimodal Fusion Module",
      "text": "Though the uni-modal extractors can capture longterm temporal context, they are unable to handle feature redundancy and noise due to the modality gap. Thus, we design a new multimodal fusion module (Figure  2 (B) ) to inherently separate multiple modalities into two disjoint parts, contextual representations and specific representations, to extract the consistency and specificity of heterogeneous modalities collaboratively and individually.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Contextual Representation Learning",
      "text": "Contextual representation learning aims to explore and learn hidden contextual intent/topic knowledge of the dialogue, which can greatly improve the performance of JOYFUL. In Figure  2  (B1), we first project all uni-modal inputs x {v,a,t} into a latent space by using three separate connected deep neural networks f g {v,a,t} (•) to obtain hidden representations z g {v,a,t} . Then, we concatenate them as z g m and apply it to a multi-layer transformer to maximize the correlation between multimodal features, where we learn a global contextual multimodal representation ẑg m . Considering that the contextual information will change over time, we design a temporal smoothing strategy for ẑg m as\n\nwhere z con is the topic-related vector describing the high-level global contextual information without requiring topic-related inputs, following the definition in  Joshi et al. (2022) . We update the (i+1)-th utterance as z con ← z con +e η * i ẑg m , and η is the exponential smoothing parameter  (Shazeer and Stern, 2018) , indicating that more recent information will be more important.\n\nTo ensure fused contextual representations capture enough details from hidden layers,  Hazarika et al. (2020)  minimized the reconstruction error between fused representations with hidden representations. Inspired by their work, to ensure that ẑg m contains essential modality cues for downstream emotion recognition, we reconstruct z g m from ẑg m by minimizing their Euclidean distance:\n\n(2)",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Specific Representation Learning",
      "text": "Specific representation learning aims to fully explore specific information from each modality to complement one another. Figure  2  (B2) shows that we first use three fully connected deep neural networks f ℓ {v,a,t} (•) to project uni-modal embeddings x {v,a,t} into a hidden space with representations as z ℓ {v,a,t} . Considering that visual, audio, and text features are extracted with different encoding methods, directly applying multiple specific features as an input for the downstream emotion recognition task will degrade the model's accuracy. To solve it, the multimodal features are projected into a shared subspace, and a shared trainable basis matrix is designed to learn aligned representations for them. Therefore, the multimodal features can be fully integrated and interacted to mitigate feature discontinuity and remove noise across modalities. We define a shared trainable basis matrix B with q basis vectors as B = (b 1 , . . . , b q ) T ∈ R q×d b with d b representing the dimensionality of each basis vector. Here, T indicates transposition. Then, z ℓ {v,a,t} and B are projected into the shared subspace:\n\nwhere W {v,a,t,b} are trainable parameters. To learn new representations for each modality, we calculate the cosine similarity between them and B as\n\nwhere S v ij denotes the similarity between the i-th visual feature ( zℓ v ) i and the j-th basis vector representation b j . To prevent inaccurate representation learning caused by an excessive weight of a certain item, the similarities are further normalized by\n\nThen, the new representations are obtained as\n\nwhere ẑℓ {v,a,t} are new representations, and we also use reconstruction loss for their combinations\n\nwhere Concat( , ) indicating the concatenation, i.e.,\n\n. Finally, we define the multimodal fusion loss by combining Eqs.(  1 ), (2), and (7) as:",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Graph Contrastive Learning Module",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Graph Construction",
      "text": "Graph construction aims to establish relations between past and future utterances that preserve both intra-and inter-speaker dependencies in a dialogue. We define the i-th dialogue with P speakers as C i = {U S 1 , . . . , U S P }, where U S i = {u S i 1 , . . . , u S i m } represents the set of utterances spoken by speaker S i . Following  Ghosal et al. (2019) , we define a graph with nodes representing utterances and directed edges representing their relations: R ij = u i → u j , where the arrow represents the speaking order. Intra-Dependency (R intra ∈ {U S i → U S i }) represents intra-relations between the utterances (red lines), and Inter-Dependency (R inter ∈ {U S i → U S j }, i ̸ = j) represents the inter-relations between the utterances (purple lines), as shown in Figure  3 . All nodes are initialized by concatenating contextual and specific representations as h m = Concat( ẑg m , ẑℓ m ). And we show that window size is a hyper-parameter that controls the context information for each utterance and provide accurate intraand inter-speaker dependencies.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Graph Augmentation",
      "text": "Graph Augmentation (GA): Inspired by  Zhu et al. (2020) , creating two augmented views by using different ways to corrupt the original graph can provide highly heterogeneous contexts for nodes. By maximizing the mutual information between two augmented views, we can improve the robustness of the model and obtain distinguishable node representations  (You et al., 2020) . However, there are no universally appropriate GA methods for various downstream tasks  (Xu et al., 2021) , which motivates us to design specific GA strategies for MERC. Considering that MERC is sensitive to initialized representations of utterances, intra-speaker and inter-speaker dependencies, we design three corresponding GA methods:\n\n-Feature Masking (FM): given the initialized representations of utterances, we randomly select p dimensions of the initialized representations and mask their elements with zero, which is expected to enhance the robustness of JOYFUL to multimodal feature variations;\n\n-Edge Perturbation (EP): given the graph G, we randomly drop and add p% of intra-and inter-speaker edges, which is expected to enhance the robustness of JOYFUL to local structural variations;\n\n-Global Proximity (GP): given the graph G, we first use the Katz index  (Katz, 1953)  to calculate high-order similarity between intra-and inter-speakers, and randomly add p% highorder edges between speakers, which is expected to enhance the robustness of JOYFUL to global structural variations (Examples in Appendix A).\n\nWe propose a hybrid scheme for generating graph views on both structure and attribute levels to provide diverse node contexts for the contrastive objective. Figure  2 (C ) shows that the combination of (FM & EP) and (FM & GP) are adopted to obtain two correlated views.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Graph Contrastive Learning",
      "text": "Graph contrastive learning adopts an L-th layer GCNs as a graph encoder to extract node hidden representations\n\nm } for two augmented graphs, where h i is the hidden representation for the i-th node. We follow an iterative neighborhood aggregation (or message passing) scheme to capture the structural information within the nodes' neighborhood. Formally, the propagation and aggregation of the ℓ-th GCN layer is:\n\nwhere h (i, ℓ) is the embedding of the i-th node at the ℓ-th layer, h (i, 0) is the initialization of the ith utterance, N i represents all neighbour nodes of the i-th node, and AGG (ℓ) (•) and COM (ℓ) (•) are aggregation and combination of the ℓ-th GCN layer  (Hamilton et al., 2017) . For convenience, we define h i = h (i,L) . After the L-th GCN layer, final node representations of two views are H (1) / H (2) .\n\nIn Figure  2  (C3), we design the intra-and interview graph contrastive losses to learn distinctive node representations. We start with the inter-view contrastiveness, which pulls closer the representations of the same nodes in two augmented views while pushing other nodes away, as depicted by the red and blue dash lines in Figure  2 (C3) . Given the definition of our positive and negative pairs as (h\n\n(2) j ) -, where i ̸ = j, the inter-view loss for the i-th node is formulated as:\n\ni ))\n\n,\n\nwhere sim(•, •) denotes the similarity between two vectors, i.e., the cosine similarity in this paper. Intra-view contrastiveness regards all nodes except the anchor node as negatives within a particular view (green dash lines in Figure  2  (C3)), as defined (h (1) i , h\n\n(1) j ) -where i ̸ = j. The intra-view contrastive loss for the i-th node is defined as:\n\ni , h\n\n(1) j ))\n\n.\n\nBy combining the inter-and intra-view contrastive losses of Eqs.(  11 ) and (  12 ), the contrastive objective function L ct is formulated as:",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Emotion Recognition Classifier",
      "text": "We use cross-entropy loss for classification as: where k is the number of emotion classes, m is the number of utterances, ŷj i is the i-th predicted label, and y j i is the i-th ground truth of j-th class. Above all, combining the MF loss of Eq.(  8 ), contrastive loss of Eq.(  13 ), and classification loss of Eq.(  14 ) together, the final objective function is\n\nwhere α and β are the trade-off hyper-parameters. We give our pseudo-code in Appendix F. Please note that the detailed label distribution of the datasets is given in Appendix I. Implementation Details. We selected the augmentation pairs (FM & EP) and (FM & GP) for two views. We set the augmentation ratio p=20% and smoothing parameter η=0.2, and applied the Adam (Kingma and Ba, 2015) optimizer with an initial learning rate of 3e-5. For a fair comparison, we followed the default parameter settings of the baselines and repeated all experiments ten times to report the average accuracy. We conducted the significance by t-test with Benjamini-Hochberg (Benjamini and Hochberg, 1995) correction (Please see details in Appendix G).\n\nBaselines. Different MERC datasets have different best system results, following COGMEN, we selected SOTA baselines for each dataset.\n\nFor IEMOCAP-4, we selected Mult  (Tsai et al., 2019a) , RAVEN  (Wang et al., 2019) , MTAG  (Yang et al., 2021) , PMR  (Lv et al., 2021) , COG-MEN and MICA  (Liang et al., 2021)  as our baselines. For IEMOCAP-6, we selected Mult, FE2E  (Dai et al., 2021) , DiaRNN  (Majumder et al., 2019) , COSMIC  (Ghosal et al., 2020) , Af-CAN  (Wang et al., 2021) , AGHMN  (Jiao et al., 2020) , COGMEN and RGAT  (Ishiwatari et al., 2020)  as our baselines. For MELD, we selected DiaGCN  (Ghosal et al., 2019) , DiaCRN  (Hu et al., 2021) , MMGCN  (Wei et al., 2019) , UniMSE  (Hu et al., 2022b) , COGMEN and MM-DFN  (Hu et al., 2022a)  as baselines. For MOSEI, we selected Mul-Net  (Shenoy et al., 2020) , TBJE  (Delbrouck et al., 2020) , COGMEN and MR  (Tsai et al., 2020) .",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Parameter Sensitive Study",
      "text": "We first examined whether applying different data augmentation methods improves JOYFUL. We observed in Figure  4  (A) that 1) all data augmentation strategies are effective 2) applying augmentation pairs of the same type cannot result in the best performance; and 3) applying augmentation pairs of different types improves performance. Thus, we selected (FM & EP) and (FM & GP) as the default augmentation strategy since they achieved the best performance (More details please see Appendix C). JOYFUL has three hyperparameters. α and β determine the importance of MF and GCL in Eq.(  15 ), and window size controls the contextual length of conversations. In Figure  4 (B) , we observed how α and β affect the performance of JOYFUL by varying α from 0.02 to 0.10 in 0.02 intervals and β from 0.1 to 0.5 in 0.1 intervals. The results indicated that JOYFUL achieved the best performance when α ∈ [0.06, 0.08] and β = 0.3. Figure  4 (C)  shows that when window_size = 8, JOYFUL achieved the best performance. A small window size will miss much contextual information, and a longer one contains too much noise, we set it as 8 in experiments (Details in Appendix D).",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Performance Of Joyful",
      "text": "Tables  2  & 3  show that JOYFUL outperformed all baselines in terms of accuracy and WF1, improving 5.0% and 1.3% in WF1 for 6-way and 4-way, respectively. Graph-based methods, COGMEN and JOYFUL, outperform Transformers-based methods, Mult and FE2E. Transformers-based methods cannot distinguish intra-and inter-speaker dependencies, distracting their attention to important utterances. Furthermore, they use the cross-modal attention layer, which can enhance common features among modalities while losing uni-modal specific features  (Rajan et al., 2022) . JOYFUL outperforms other GNN-based methods since it explored features from both the contextual and specific levels, and used GCL to obtain more distinguishable features. However, JOYFUL cannot improve in Happy for 4-way and in Excited for 6-way since samples in IEMOCAP were insufficient for distinguishing these similar emotions (Happy is 1/3 of Neutral in Fig.  4 (D) ). Without labels' guidance to re-sample or re-weight the underrepresented samples, selfsupervised GCL, utilized in JOYFUL, cannot ensure distinguishable representations for samples of minor classes by only exploring graph topological information and vertex attributes. Tables  4  & 5  show that JOYFUL outperformed  the baselines in more complex scenes with multiple speakers or various emotional labels. Compared with COGMEN and MM-DFN, which directly aggregate multimodal features, JOYFUL can fully explore features from each uni-modality by specific representation learning to improve the performance.\n\nThe GCL module can better aggregate similar emotional features for utterances to obtain better performance for multi-label classification. We cannot improve in Happy on MOSEI since the samples are imbalanced and Happy has only 1/6 of Surprise, making JOYFUL hard to identify it.\n\nTo verify the performance gain from each component, we conducted additional ablation studies. We deepened the GNN layers to verify JOYFUL's ability to alleviate the over-smoothing. In Table  7 , COGMEN with four-layer GNN was 9.24% lower than that with one-layer, demonstrating that the over-smoothing decreases performance, while JOY-FUL relieved this issue by using the GCL framework. To verify the robustness, following  Tan et al. (2022) , we randomly added 5%∼20% noisy edges to the training data. In Table  7 , COGMEN was  easily affected by the noise, decreasing 10.8% performance in average with 20% noisy edges, while JOYFUL had strong robustness with only an average 2.8% performance reduction for 20% noisy edges.\n\nTo show the distinguishability of the node representations, we visualize the node representations of FE2E, COGMEN, and JOYFUL on 6-way IEMO-CAP. In Figure  5 , COGMEN and JOYFUL obtained more distinguishable node representations than FE2E, demonstrating that graph structure is more suitable for MERC than Transformers. JOYFUL performed better than COGMEN, illustrating the effectiveness of GCL. In Figure  6 , we randomly sampled one example from each emotion of IEMOCAP (6-way) and chose best-performing COGMEN for comparison. JOYFUL obtained more discriminate prediction scores among emotion classes, showing GCL can push samples from different emotion class farther apart.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Conclusion",
      "text": "We proposed a joint learning model (JOYFUL) for MERC, that involves a new multimodal fusion mechanism and GCL module to effectively improve the performance of MERC. The MR mechanism can extract and fuse contextual and uni-modal specific emotion features, and the GCL module can help learn more distinguishable representations.\n\nFor future work, we plan to investigate the performance of using supervised GCL for JOYFUL on unbalanced and small-scale emotional datasets.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "A Example For Global Proximity",
      "text": "In Figure  7 , given the network G and a modified p, we first used the Katz index  (Katz, 1953)  to calculate a high-order similarity between the vertices. We considered the arbitrary number of high-order distances. For example, second-order similarity between u A 1 and u B 4 as u A 1 → u B 4 = 0.83, third-order similarity between u A 1 and u B 5 as u A 1 → u B 5 = 0.63, and fourth-order similarity between u A 1 and u B 7 as u A 1 → u B 7 = 0.21. We then define the threshold score as 0.5, where a high-order similarity score less than the threshold will not be selected as added edges. Finally, we randomly selected p% edges (whose scores are higher than the threshold score) and added them to the original graph G to construct the augmented graph.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "B Dimensions Of Mathematical Symbols",
      "text": "Since we do not have much space to introduce details about the dimensions of the mathematical symbols in our main body. We carefully list all the dimensions of the mathematical symbols of IEMOCAP in Table  8 . Mathematical symbols for other two datasets please see our source code.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "C Observations Of Graph Augmentation",
      "text": "As shown in Figure  8 , when we consider the combinations of (FM & EP) and (FP & GP) as two graph augmentation methods of the original graph, we could achieve the best performance. Furthermore, we have the following observations:\n\nObs.1: Graph augmentations are crucial. Without any data augmentation, GCL module will not improve\n\nParameters of Aggregation Layer COM ∈ R 2,760×5,520\n\nInput/Output of Combination Layer W graph ∈ R 5,520×2,760 Dimention Reduction after COM hm ∈ R 2,760\n\nNode Features of GCN Layer    (You et al., 2020) , without augmentation, GCL simply compares two original samples as a negative pair with the positive pair loss becoming zero, which leads to homogeneously pushes all graph representations away from each other. Appropriate augmentations can enforce the model to learn representations invariant to the desired perturbations through maximizing the agreement between a graph and its augmentation.\n\nObs.2: Composing different augmentations benefits the model's performance more. Applying augmentation pairs of the same type does not often result in the best performance (see diagonals in Figure  8 ). In contrast, applying augmentation pairs of different types result in better performance gain (see offdiagonals of Figure  8 ). Similar observations were in SimCSE  (Gao et al., 2021) . As mentioned in that study, composing augmentation pairs of different types correspond to a \"harder\" contrastive were greater than 0.05. This indicates that the results of the baselines and our model all adhere to the assumption of normality. For example, in  RAVEN, MTAG, PMR, MICA, COGMEN, JOYFUL] are [0.903, 0.957, 0.858, 0.978, 0.970, 0.969, 0.862] . Furthermore, we used the Levene's test  (Schultz, 1985)  to check for homogeneity of variances between baselines and our model. Under the constraint of a significance level (alpha = 0.05), we found that our p-values are greater than 0.05, indicating the homogeneity of the variances between the baselines and our model. For example, we obtained p-values 0.3101 and 0.3848 for group-based baselines on IEMOCAP-4 and IEMOCAP-6, respectively. Since we were able to demonstrate that all baselines and our model conform to the assumptions of normality and homogeneity of variances, we believe that the significance tests we reported are accurate.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "H Representation Visualization",
      "text": "We visualized the node features to understand the function of the multimodal fusion mechanism and the GCL-based node representation learning component, as shown in Figure  10 .   10 )) and before the pre-softmax layer (Eq.(  11 )). We observed that utterances could be roughly separated after the feature fusion mechanism, which indicates that the multimodal fusion mechanism can learn distinctive features to a certain extent. After GCL-based module, JOYFUL can be easily separated, demonstrating that GCL can provide distinguishable representation by exploring vertex attributes, graph structure, and contextual information from datasets.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "I Labels Distribution Of Datasets",
      "text": "In this section, we list the detailed label distribution of the three multimodal emotion recognition datasets MELD (Table  12 ), IEMOCAP 4-way (Table 13), IEMOCAP 6-way (Table  14 ) and MOSEI (Table  15 ) in the draft.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "J Multimodal Sentiment Analysis",
      "text": "We conducted experiments on two publicly available datasets, MOSI  (Zadeh et al., 2016)  and MO-",
      "page_start": 18,
      "page_end": 18
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Emotions are affected by multiple uni-modal,",
      "page": 1
    },
    {
      "caption": "Figure 1: shows that emotions expressed in a",
      "page": 1
    },
    {
      "caption": "Figure 2: Overview of JOYFUL. We first extract uni-modal features, fuse them using a multimodal fusion module,",
      "page": 3
    },
    {
      "caption": "Figure 2: shows an overview of JOYFUL, which",
      "page": 3
    },
    {
      "caption": "Figure 2: (B)) to inherently separate mul-",
      "page": 4
    },
    {
      "caption": "Figure 2: (B1), we first",
      "page": 4
    },
    {
      "caption": "Figure 2: (B2) shows that",
      "page": 4
    },
    {
      "caption": "Figure 3: An example of graph construction.",
      "page": 5
    },
    {
      "caption": "Figure 3: All nodes are initialized by concatenating contex-",
      "page": 5
    },
    {
      "caption": "Figure 2: (C) shows that the combina-",
      "page": 5
    },
    {
      "caption": "Figure 2: (C3), we design the intra- and inter-",
      "page": 6
    },
    {
      "caption": "Figure 2: (C3). Given",
      "page": 6
    },
    {
      "caption": "Figure 2: (C3)), as",
      "page": 6
    },
    {
      "caption": "Figure 4: (A) that 1) all data augmentation",
      "page": 7
    },
    {
      "caption": "Figure 4: (B), we observed how α",
      "page": 7
    },
    {
      "caption": "Figure 4: (C) shows",
      "page": 7
    },
    {
      "caption": "Figure 4: (D)). Without labels’ guidance to re-sample",
      "page": 7
    },
    {
      "caption": "Figure 4: (A) WF1 gain with different augmentation pairs; (B∼C) Parameter tuning; (D) Imbalanced dataset.",
      "page": 8
    },
    {
      "caption": "Figure 5: t-SNE visualization of IEMOCAP (6-way).",
      "page": 9
    },
    {
      "caption": "Figure 6: Visualization of emotion probability, each first",
      "page": 9
    },
    {
      "caption": "Figure 5: , COGMEN and JOYFUL obtained",
      "page": 9
    },
    {
      "caption": "Figure 6: , we randomly sam-",
      "page": 9
    },
    {
      "caption": "Figure 7: , given the network G and a modified",
      "page": 14
    },
    {
      "caption": "Figure 7: Example of adding p% high-order edges to",
      "page": 14
    },
    {
      "caption": "Figure 8: , when we consider the combi-",
      "page": 14
    },
    {
      "caption": "Figure 8: In contrast, composing an original",
      "page": 14
    },
    {
      "caption": "Figure 8: Similar observa-",
      "page": 14
    },
    {
      "caption": "Figure 8: ). Similar observations were",
      "page": 14
    },
    {
      "caption": "Figure 8: Average WF1 gain when contrasting different augmentation pairs, compared with training without graph",
      "page": 15
    },
    {
      "caption": "Figure 9: , we observed how α and β affect the",
      "page": 15
    },
    {
      "caption": "Figure 9: Parameters tuning for α and β on validation datasets for all multimodal emotion recognition tasks.",
      "page": 16
    },
    {
      "caption": "Figure 10: Figure 10 (A) shows",
      "page": 17
    },
    {
      "caption": "Figure 10: (B) shows the representation of",
      "page": 17
    },
    {
      "caption": "Figure 10: t-SNE visualization of IEMOCAP (6-way) features.",
      "page": 18
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "I just miss him. (Sad)": "It does look really beautiful over the water. (Happy)"
        },
        {
          "I just miss him. (Sad)": "Oh, thanks. move here before you get married. (Excited)"
        },
        {
          "I just miss him. (Sad)": "(cid:48)aybe we can find you something with juggling. (Neutral)"
        },
        {
          "I just miss him. (Sad)": "You above all have got to believe. (Angry)"
        },
        {
          "I just miss him. (Sad)": "So what, I'm not fast with women. (Frustrated)"
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Controlling the false discovery rate: a practical and powerful approach to multiple testing",
      "authors": [
        "Tadas Baltrusaitis",
        "Amir Zadeh",
        "Chong Lim",
        "Louis-Philippe Morency"
      ],
      "year": "1995",
      "venue": "Yoav Benjamini and Yosef Hochberg",
      "doi": "10.1109/FG.2018.00019"
    },
    {
      "citation_id": "2",
      "title": "IEMOCAP: interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth Narayanan"
      ],
      "year": "2008",
      "venue": "Lang. Resour. Evaluation",
      "doi": "10.1007/s10579-008-9076-6"
    },
    {
      "citation_id": "3",
      "title": "Cross-modal memory networks for radiology report generation",
      "authors": [
        "Zhihong Chen",
        "Yaling Shen",
        "Yan Song",
        "Xiang Wan"
      ],
      "year": "2021",
      "venue": "Proc. of ACL/IJCNLP",
      "doi": "10.18653/v1/2021.acl-long.459"
    },
    {
      "citation_id": "4",
      "title": "Multimodal end-to-end sparse model for emotion recognition",
      "authors": [
        "Wenliang Dai",
        "Samuel Cahyawijaya",
        "Zihan Liu",
        "Pascale Fung"
      ],
      "year": "2021",
      "venue": "Proc. of NAACL-HLT",
      "doi": "10.18653/v1/2021.naacl-main.417"
    },
    {
      "citation_id": "5",
      "title": "A transformer-based joint-encoding for emotion recognition and sentiment analysis",
      "authors": [
        "Jean-Benoit Delbrouck",
        "Noé Tits",
        "Mathilde Brousmiche",
        "Stéphane Dupont"
      ],
      "year": "2020",
      "venue": "Workshop on Multimodal Language (Challenge-HML)",
      "doi": "10.18653/v1/2020.challengehml-1.1"
    },
    {
      "citation_id": "6",
      "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "Proc. of NAACL-HLT",
      "doi": "10.18653/v1/n19-1423"
    },
    {
      "citation_id": "7",
      "title": "The hitchhiker's guide to testing statistical significance in natural language processing",
      "authors": [
        "Rotem Dror",
        "Gili Baumer",
        "Segev Shlomov",
        "Roi Reichart"
      ],
      "year": "2018",
      "venue": "Proc. of ACL",
      "doi": "10.18653/v1/P18-1128"
    },
    {
      "citation_id": "8",
      "title": "Opensmile: The munich versatile and fast open-source audio feature extractor",
      "authors": [
        "Florian Eyben",
        "Martin Wöllmer",
        "Björn Schuller"
      ],
      "year": "2010",
      "venue": "Proc. of MM",
      "doi": "10.1145/1873951.1874246"
    },
    {
      "citation_id": "9",
      "title": "CONSK-GCN: conversational semantic-and knowledge-oriented graph convolutional network for multimodal emotion recognition",
      "authors": [
        "Yahui Fu",
        "Shogo Okada",
        "Longbiao Wang",
        "Lili Guo",
        "Yaodong Song",
        "Jiaxing Liu",
        "Jianwu Dang"
      ],
      "year": "2021",
      "venue": "Proc. of ICME",
      "doi": "10.1109/ICME51207.2021.9428438"
    },
    {
      "citation_id": "10",
      "title": "Simcse: Simple contrastive learning of sentence embeddings",
      "authors": [
        "Tianyu Gao",
        "Xingcheng Yao",
        "Danqi Chen"
      ],
      "year": "2021",
      "venue": "Proc. of EMNLP",
      "doi": "10.18653/v1/2021.emnlp-main.552"
    },
    {
      "citation_id": "11",
      "title": "COSMIC: COmmonSense knowledge for eMotion identification in conversations",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder"
      ],
      "year": "2020",
      "venue": "Findings of EMNLP",
      "doi": "10.18653/v1/2020.findings-emnlp.224"
    },
    {
      "citation_id": "12",
      "title": "Di-alogueGCN: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Soujanya Poria",
        "Niyati Chhaya",
        "Alexander Gelbukh"
      ],
      "year": "2019",
      "venue": "Proc. of EMNLP-IJCNLP",
      "doi": "10.18653/v1/D19-1015"
    },
    {
      "citation_id": "13",
      "title": "Unimodal and crossmodal refinement network for multimodal sequence fusion",
      "authors": [
        "Xiaobao Guo",
        "Adams Kong",
        "Huan Zhou",
        "Xianfeng Wang",
        "Min Wang"
      ],
      "year": "2021",
      "venue": "Proc. of EMNLP",
      "doi": "10.18653/v1/2021.emnlp-main.720"
    },
    {
      "citation_id": "14",
      "title": "Aniket Bera, Debdoot Mukherjee, and Dinesh Manocha. 2022. 3massiv: Multilingual, multimodal and multi-aspect dataset of social media short videos",
      "authors": [
        "Vikram Gupta",
        "Trisha Mittal",
        "Puneet Mathur",
        "Vaibhav Mishra",
        "Mayank Maheshwari"
      ],
      "venue": "Proc. of CVPR",
      "doi": "10.1109/CVPR52688.2022.02039"
    },
    {
      "citation_id": "15",
      "title": "Inductive representation learning on large graphs",
      "authors": [
        "William Hamilton",
        "Zhitao Ying",
        "Jure Leskovec"
      ],
      "year": "2017",
      "venue": "Proc. of NeurIPS"
    },
    {
      "citation_id": "16",
      "title": "Louis-Philippe Morency, and Soujanya Poria. 2021a. Bi-bimodal modality fusion for correlationcontrolled multimodal sentiment analysis",
      "authors": [
        "Wei Han",
        "Hui Chen",
        "Alexander Gelbukh",
        "Amir Zadeh"
      ],
      "venue": "Proc. of ICMI"
    },
    {
      "citation_id": "17",
      "title": "2021b. Improving multimodal fusion with hierarchical mutual information maximization for multimodal sentiment analysis",
      "authors": [
        "Wei Han",
        "Hui Chen",
        "Soujanya Poria"
      ],
      "venue": "Proc. of EMNLP"
    },
    {
      "citation_id": "18",
      "title": "Misa: Modality-invariant and -specific representations for multimodal sentiment analysis",
      "authors": [
        "Devamanyu Hazarika",
        "Roger Zimmermann",
        "Soujanya Poria"
      ],
      "year": "2020",
      "venue": "Proc. of MM",
      "doi": "10.1145/3394171.3413678"
    },
    {
      "citation_id": "19",
      "title": "2022a. MM-DFN: multimodal dynamic fusion network for emotion recognition in conversations",
      "authors": [
        "Dou Hu",
        "Xiaolong Hou",
        "Lingwei Wei",
        "Lian-Xin Jiang",
        "Yang Mo"
      ],
      "venue": "Proc. of ICASSP"
    },
    {
      "citation_id": "20",
      "title": "Dia-logueCRN: Contextual reasoning networks for emotion recognition in conversations",
      "authors": [
        "Dou Hu",
        "Lingwei Wei",
        "Xiaoyong Huai"
      ],
      "year": "2021",
      "venue": "Proc. of ACL",
      "doi": "10.18653/v1/2021.acl-long.547"
    },
    {
      "citation_id": "21",
      "title": "2022b. UniMSE: Towards unified multimodal sentiment analysis and emotion recognition",
      "authors": [
        "Guimin Hu",
        "Ting-En Lin",
        "Yi Zhao",
        "Guangming Lu",
        "Yuchuan Wu",
        "Yongbin Li"
      ],
      "venue": "Proc. of EMNLP"
    },
    {
      "citation_id": "22",
      "title": "Densely connected convolutional networks",
      "authors": [
        "Gao Huang",
        "Zhuang Liu",
        "Laurens Van Der Maaten",
        "Kilian Weinberger"
      ],
      "year": "2017",
      "venue": "Proc. of CVPR",
      "doi": "10.1109/CVPR.2017.243"
    },
    {
      "citation_id": "23",
      "title": "Relation-aware graph attention networks with relational position encodings for emotion recognition in conversations",
      "authors": [
        "Taichi Ishiwatari",
        "Yuki Yasuda",
        "Taro Miyazaki",
        "Jun Goto"
      ],
      "year": "2020",
      "venue": "Proc. of EMNLP",
      "doi": "10.18653/v1/2020.emnlp-main.597"
    },
    {
      "citation_id": "24",
      "title": "Real-time emotion recognition via attention gated hierarchical memory network",
      "authors": [
        "Wenxiang Jiao",
        "Michael Lyu",
        "Irwin King"
      ],
      "year": "2020",
      "venue": "Proc. of AAAI"
    },
    {
      "citation_id": "25",
      "title": "COGMEN: COntextualized GNN based multimodal emotion recognitioN",
      "authors": [
        "Abhinav Joshi",
        "Ashwani Bhat",
        "Ayush Jain",
        "Atin Singh",
        "Ashutosh Modi"
      ],
      "year": "2022",
      "venue": "Proc. of NAACL",
      "doi": "10.18653/v1/2022.naacl-main.306"
    },
    {
      "citation_id": "26",
      "title": "A new status index derived from sociometric analysis",
      "authors": [
        "Leo Katz"
      ],
      "year": "1953",
      "venue": "Psychometrika"
    },
    {
      "citation_id": "27",
      "title": "Convolutional neural networks for sentence classification",
      "authors": [
        "Yoon Kim"
      ],
      "year": "2014",
      "venue": "Proc. of EMNLP",
      "doi": "10.3115/v1/d14-1181"
    },
    {
      "citation_id": "28",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2015",
      "venue": "Proc. of ICLR"
    },
    {
      "citation_id": "29",
      "title": "Semisupervised classification with graph convolutional networks",
      "authors": [
        "Thomas Kipf",
        "Max Welling"
      ],
      "year": "2017",
      "venue": "Proc. of ICLR"
    },
    {
      "citation_id": "30",
      "title": "Multimodal dialogue state tracking",
      "authors": [
        "Hung Le",
        "Nancy Chen",
        "Steven Hoi"
      ],
      "year": "2022",
      "venue": "Proc. of NAACL",
      "doi": "10.18653/v1/2022.naacl-main.248"
    },
    {
      "citation_id": "31",
      "title": "Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks",
      "authors": [
        "Yann Lecun",
        "Yoshua Bengio"
      ],
      "year": "1995",
      "venue": "Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks"
    },
    {
      "citation_id": "32",
      "title": "Contributions to probability and statistics",
      "authors": [
        "Howard Levene"
      ],
      "year": "1960",
      "venue": "Essays in honor of Harold Hotelling"
    },
    {
      "citation_id": "33",
      "title": "A-TIP: attribute-aware text infilling via pre-trained language model",
      "authors": [
        "Dongyuan Li",
        "Jingyi You",
        "Kotaro Funakoshi",
        "Manabu Okumura"
      ],
      "year": "2022",
      "venue": "Proc. of COLING"
    },
    {
      "citation_id": "34",
      "title": "Enhanced knowledge selection for grounded dialogues via document semantic graphs",
      "authors": [
        "Sha Li",
        "Madhi Namazifar",
        "Di Jin",
        "Mohit Bansal",
        "Heng Ji",
        "Yang Liu",
        "Dilek Hakkani-Tur"
      ],
      "year": "2022",
      "venue": "NAACL 2022"
    },
    {
      "citation_id": "35",
      "title": "2022c. Contrast and generation make BART a good dialogue emotion recognizer",
      "authors": [
        "Shimin Li",
        "Hang Yan",
        "Xipeng Qiu"
      ],
      "venue": "Proc. of AAAI"
    },
    {
      "citation_id": "36",
      "title": "Geomgcl: Geometric graph contrastive learning for molecular property prediction",
      "authors": [
        "Shuangli Li",
        "Jingbo Zhou",
        "Tong Xu",
        "Dejing Dou",
        "Hui Xiong"
      ],
      "year": "2022",
      "venue": "Proc. of AAAI"
    },
    {
      "citation_id": "37",
      "title": "CLMLF:a contrastive learning and multilayer fusion method for multimodal sentiment detection",
      "authors": [
        "Zhen Li",
        "Bing Xu",
        "Conghui Zhu",
        "Tiejun Zhao"
      ],
      "year": "2022",
      "venue": "Findings of NAACL",
      "doi": "10.18653/v1/2022.findings-naacl.175"
    },
    {
      "citation_id": "38",
      "title": "Modular and parameter-efficient multimodal fusion with prompting",
      "authors": [
        "Sheng Liang",
        "Mengjie Zhao",
        "Hinrich Schuetze"
      ],
      "year": "2022",
      "venue": "Findings of ACL",
      "doi": "10.18653/v1/2022.findings-acl.234"
    },
    {
      "citation_id": "39",
      "title": "Attention is not enough: Mitigating the distribution discrepancy in asynchronous multimodal sequence fusion",
      "authors": [
        "Tao Liang",
        "Guosheng Lin",
        "Lei Feng",
        "Yan Zhang",
        "Fengmao Lv"
      ],
      "year": "2021",
      "venue": "Proc. of ICCV",
      "doi": "10.1109/ICCV48922.2021.00804"
    },
    {
      "citation_id": "40",
      "title": "Summary-oriented vision modeling for multimodal abstractive summarization",
      "authors": [
        "Yunlong Liang",
        "Fandong Meng",
        "Jinan Xu",
        "Jiaan Wang",
        "Yufeng Chen",
        "Jie Zhou"
      ],
      "year": "2023",
      "venue": "Proc. of ACL",
      "doi": "10.18653/v1/2023.acl-long.165"
    },
    {
      "citation_id": "41",
      "title": "Visionlanguage pre-training for multimodal aspect-based sentiment analysis",
      "authors": [
        "Yan Ling",
        "Jianfei Yu",
        "Rui Xia"
      ],
      "year": "2022",
      "venue": "Proc. of ACL",
      "doi": "10.18653/v1/2022.acl-long.152"
    },
    {
      "citation_id": "42",
      "title": "Multistage fusion with forget gate for multimodal summarization in open-domain videos",
      "authors": [
        "Nayu Liu",
        "Xian Sun",
        "Hongfeng Yu",
        "Wenkai Zhang",
        "Guangluan Xu"
      ],
      "year": "2020",
      "venue": "Proc. of EMNLP",
      "doi": "10.18653/v1/2020.emnlp-main.144"
    },
    {
      "citation_id": "43",
      "title": "Roberta: A robustly optimized BERT pretraining approach",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized BERT pretraining approach"
    },
    {
      "citation_id": "44",
      "title": "Progressive modality reinforcement for human multimodal emotion recognition from unaligned multimodal sequences",
      "authors": [
        "Fengmao Lv",
        "Xiang Chen",
        "Yanyong Huang",
        "Lixin Duan",
        "Guosheng Lin"
      ],
      "year": "2021",
      "venue": "Proc. of CVPR",
      "doi": "10.1109/CVPR46437.2021.00258"
    },
    {
      "citation_id": "45",
      "title": "Dialoguernn: An attentive RNN for emotion detection in conversations",
      "authors": [
        "Navonil Majumder",
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Rada Mihalcea",
        "Alexander Gelbukh",
        "Erik Cambria"
      ],
      "year": "2019",
      "venue": "Proc. of AAAI",
      "doi": "10.1609/aaai.v33i01.33016818"
    },
    {
      "citation_id": "46",
      "title": "M-SENA: An integrated platform for multimodal sentiment analysis",
      "authors": [
        "Huisheng Mao",
        "Ziqi Yuan",
        "Hua Xu",
        "Wenmeng Yu",
        "Yihe Liu",
        "Kai Gao"
      ],
      "year": "2022",
      "venue": "Proc. of ACL",
      "doi": "10.18653/v1/2022.acl-demo.20"
    },
    {
      "citation_id": "47",
      "title": "Attention bottlenecks for multimodal fusion",
      "authors": [
        "Arsha Nagrani",
        "Shan Yang",
        "Anurag Arnab",
        "Aren Jansen",
        "Cordelia Schmid",
        "Chen Sun"
      ],
      "year": "2021",
      "venue": "Proc. of NeurIPS"
    },
    {
      "citation_id": "48",
      "title": "2023. e-health CSIRO at radsum23: Adapting a chest x-ray report generator to multimodal radiology report summarisation",
      "authors": [
        "Aaron Nicolson",
        "Jason Dowling",
        "Bevan Koopman"
      ],
      "venue": "The 22nd Workshop on BioNLP@ACL",
      "doi": "10.18653/v1/2023.bionlp-1.56"
    },
    {
      "citation_id": "49",
      "title": "Retrieving multimodal prompts for generative visual question answering",
      "authors": [
        "Timothy Ossowski",
        "Junjie Hu"
      ],
      "year": "2023",
      "venue": "Findings of the ACL",
      "doi": "10.18653/v1/2023.findings-acl.158"
    },
    {
      "citation_id": "50",
      "title": "Communication goes multimodal",
      "authors": [
        "Sarah Partan",
        "Peter Marler"
      ],
      "year": "1999",
      "venue": "Science",
      "doi": "10.1126/science.283.5406.1272"
    },
    {
      "citation_id": "51",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika"
      ],
      "year": "2019",
      "venue": "Proc. of ACL",
      "doi": "10.18653/v1/p19-1050"
    },
    {
      "citation_id": "52",
      "title": "Recognizing emotion cause in conversations",
      "authors": [
        "Soujanya Poria",
        "Navonil Majumder",
        "Devamanyu Hazarika",
        "Deepanway Ghosal",
        "Rishabh Bhardwaj",
        "Samson Yu Bai Jian",
        "Pengfei Hong",
        "Romila Ghosh",
        "Abhinaba Roy",
        "Niyati Chhaya",
        "Alexander Gelbukh",
        "Rada Mihalcea"
      ],
      "year": "2021",
      "venue": "Cogn. Comput",
      "doi": "10.1007/s12559-021-09925-7"
    },
    {
      "citation_id": "53",
      "title": "Emotion recognition in conversation: Research challenges, datasets, and recent advances",
      "authors": [
        "Soujanya Poria",
        "Navonil Majumder",
        "Rada Mihalcea",
        "Eduard Hovy"
      ],
      "year": "2019",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2019.2929050"
    },
    {
      "citation_id": "54",
      "title": "Influence of multiple hypothesis testing on reproducibility in neuroimaging research: A simulation study and python-based software",
      "authors": [
        "Tuomas Puoliväli",
        "Satu Palva",
        "J Palva"
      ],
      "year": "2020",
      "venue": "Journal of Neuroscience Methods",
      "doi": "10.1016/j.jneumeth.2020.108654"
    },
    {
      "citation_id": "55",
      "title": "Librosa based assessment tool for music information retrieval systems",
      "authors": [
        "Preeth Raguraman",
        "Mohan Ramasundaram",
        "Midhula Vijayan"
      ],
      "year": "2019",
      "venue": "Proc. of MIPR",
      "doi": "10.1109/MIPR.2019.00027"
    },
    {
      "citation_id": "56",
      "title": "Integrating multimodal information in large pretrained transformers",
      "authors": [
        "Md Wasifur Rahman",
        "Sangwu Hasan",
        "Amirali Lee",
        "Chengfeng Bagher Zadeh",
        "Louis-Philippe Mao",
        "Mohammed Morency",
        "Hoque"
      ],
      "year": "2020",
      "venue": "Proc. of ACL",
      "doi": "10.18653/v1/2020.acl-main.214"
    },
    {
      "citation_id": "57",
      "title": "Is cross-attention preferable to self-attention for multi-modal emotion recognition?",
      "authors": [
        "Alessio Vandana Rajan",
        "Andrea Brutti",
        "Cavallaro"
      ],
      "year": "2022",
      "venue": "Proc. of ICASSP",
      "doi": "10.1109/ICASSP43922.2022.9746924"
    },
    {
      "citation_id": "58",
      "title": "Sentence-BERT: Sentence embeddings using Siamese BERTnetworks",
      "authors": [
        "Nils Reimers",
        "Iryna Gurevych"
      ],
      "year": "2019",
      "venue": "Proc. of EMNLP-IJCNLP",
      "doi": "10.18653/v1/D19-1410"
    },
    {
      "citation_id": "59",
      "title": "The graph neural network model",
      "authors": [
        "Franco Scarselli",
        "Marco Gori",
        "Ah Chung Tsoi",
        "Markus Hagenbuchner",
        "Gabriele Monfardini"
      ],
      "year": "2009",
      "venue": "IEEE Trans. Neural Networks",
      "doi": "10.1109/TNN.2008.2005605"
    },
    {
      "citation_id": "60",
      "title": "Levene's test for relative variation",
      "authors": [
        "Brian Schultz"
      ],
      "year": "1985",
      "venue": "Systematic Zoology"
    },
    {
      "citation_id": "61",
      "title": "Multimodal fusion via cortical network inspired losses",
      "authors": [
        "Shiv Shankar"
      ],
      "year": "2022",
      "venue": "Proc. of ACL",
      "doi": "10.18653/v1/2022.acl-long.83"
    },
    {
      "citation_id": "62",
      "title": "An analysis of variance test for normality (complete samples)",
      "authors": [
        "Sanford Samuel",
        "Martin Shapiro",
        "Wilk"
      ],
      "year": "1965",
      "venue": "Biometrika"
    },
    {
      "citation_id": "63",
      "title": "Adafactor: Adaptive learning rates with sublinear memory cost",
      "authors": [
        "Noam Shazeer",
        "Mitchell Stern"
      ],
      "year": "2018",
      "venue": "Proc. of ICML"
    },
    {
      "citation_id": "64",
      "title": "Dialogxl: All-in-one xlnet for multiparty conversation emotion recognition",
      "authors": [
        "Weizhou Shen",
        "Junqing Chen",
        "Xiaojun Quan",
        "Zhixian Xie"
      ],
      "year": "2021",
      "venue": "Proc. of AAAI"
    },
    {
      "citation_id": "65",
      "title": "2021b. Directed acyclic graph network for conversational emotion recognition",
      "authors": [
        "Weizhou Shen",
        "Siyue Wu",
        "Yunyi Yang",
        "Xiaojun Quan"
      ],
      "venue": "Proc. of ACL/IJCNLP",
      "doi": "10.18653/v1/2021.acl-long.123"
    },
    {
      "citation_id": "66",
      "title": "Summarize before aggregate: A global-to-local heterogeneous graph inference network for conversational emotion recognition",
      "authors": [
        "Dongming Sheng",
        "Dong Wang",
        "Ying Shen",
        "Haitao Zheng",
        "Haozhuang Liu"
      ],
      "year": "2020",
      "venue": "Proc. of COLING",
      "doi": "10.18653/v1/2020.coling-main.367"
    },
    {
      "citation_id": "67",
      "title": "Multilogue-net: A context-aware RNN for multimodal emotion detection and sentiment analysis in conversation",
      "authors": [
        "Aman Shenoy",
        "Ashish Sardana"
      ],
      "year": "2020",
      "venue": "Workshop on Multimodal Language (Challenge-HML)",
      "doi": "10.18653/v1/2020.challengehml-1.3"
    },
    {
      "citation_id": "68",
      "title": "Anamitra Singha, and Sriparna Saha. 2022. Sentiment and emotionaware multi-modal complaint identification",
      "authors": [
        "Apoorva Singh",
        "Soumyodeep Dey"
      ],
      "venue": "Proc. of AAAI"
    },
    {
      "citation_id": "69",
      "title": "Rumor detection on social media with graph adversarial contrastive learning",
      "authors": [
        "Tiening Sun",
        "Zhong Qian",
        "Sujun Dong",
        "Peifeng Li",
        "Qiaoming Zhu"
      ],
      "year": "2022",
      "venue": "Proc. of WWW",
      "doi": "10.1145/3485447.3511999"
    },
    {
      "citation_id": "70",
      "title": "Temporality-and frequency-aware graph contrastive learning for temporal network",
      "authors": [
        "Shiyin Tan",
        "Jingyi You",
        "Dongyuan Li"
      ],
      "year": "2022",
      "venue": "Proc. of CIKM",
      "doi": "10.1145/3511808.3557469"
    },
    {
      "citation_id": "71",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Shaojie Bai",
        "Paul Pu Liang",
        "J Kolter",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proc. of ACL",
      "doi": "10.18653/v1/p19-1656"
    },
    {
      "citation_id": "72",
      "title": "Learning factorized multimodal representations",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Paul Liang",
        "Amir Zadeh",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Learning factorized multimodal representations"
    },
    {
      "citation_id": "73",
      "title": "Multimodal routing: Improving local and global interpretability of multimodal language analysis",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Martin Ma",
        "Muqiao Yang",
        "Ruslan Salakhutdinov",
        "Louis-Philippe Morency"
      ],
      "year": "2020",
      "venue": "Proc. of EMNLP",
      "doi": "10.18653/v1/2020.emnlp-main.143"
    },
    {
      "citation_id": "74",
      "title": "A contextual attention network for multimodal emotion recognition in conversation",
      "authors": [
        "Tana Wang",
        "Yaqing Hou",
        "Dongsheng Zhou",
        "Qiang Zhang"
      ],
      "year": "2021",
      "venue": "Proc. of IJCNN",
      "doi": "10.1109/IJCNN52387.2021.9533718"
    },
    {
      "citation_id": "75",
      "title": "Contextualized emotion recognition in conversation as sequence tagging",
      "authors": [
        "Yan Wang",
        "Jiayu Zhang",
        "Jun Ma",
        "Shaojun Wang",
        "Jing Xiao"
      ],
      "year": "2020",
      "venue": "Proc. of SIGDIAL"
    },
    {
      "citation_id": "76",
      "title": "Words can shift: Dynamically adjusting word representations using nonverbal behaviors",
      "authors": [
        "Yansen Wang",
        "Ying Shen",
        "Zhun Liu",
        "Paul Liang",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2019",
      "venue": "Proc. of AAAI",
      "doi": "10.1609/aaai.v33i01.33017216"
    },
    {
      "citation_id": "77",
      "title": "2022a. Multimodal token fusion for vision transformers",
      "authors": [
        "Yikai Wang",
        "Xinghao Chen",
        "Lele Cao",
        "Wenbing Huang",
        "Fuchun Sun",
        "Yunhe Wang"
      ],
      "venue": "Proc. of CVPR",
      "doi": "10.1109/CVPR52688.2022.01187"
    },
    {
      "citation_id": "78",
      "title": "Emp: Emotion-guided multi-modal fusion and contrastive learning for personality traits recognition",
      "authors": [
        "Yusong Wang",
        "Dongyuan Li",
        "Kotaro Funakoshi",
        "Manabu Okumura"
      ],
      "year": "2023",
      "venue": "Proc. of ICMR",
      "doi": "10.1145/3591106.3592243"
    },
    {
      "citation_id": "79",
      "title": "Modern question answering datasets and benchmarks: A survey",
      "authors": [
        "Zhen Wang"
      ],
      "year": "2022",
      "venue": "Modern question answering datasets and benchmarks: A survey",
      "doi": "10.48550/arXiv.2206.15030"
    },
    {
      "citation_id": "80",
      "title": "2022b. N24news: A new dataset for multimodal news classification",
      "authors": [
        "Zhen Wang",
        "Xu Shan",
        "Xiangxie Zhang",
        "Jie Yang"
      ],
      "venue": "Proc. of LREC"
    },
    {
      "citation_id": "81",
      "title": "MMGCN: multi-modal graph convolution network for personalized recommendation of micro-video",
      "authors": [
        "Yinwei Wei",
        "Xiang Wang",
        "Liqiang Nie",
        "Xiangnan He",
        "Richang Hong",
        "Tat-Seng Chua"
      ],
      "year": "2019",
      "venue": "Proc. of MM",
      "doi": "10.1145/3343031.3351034"
    },
    {
      "citation_id": "82",
      "title": "Multimodal fusion with coattention networks for fake news detection",
      "authors": [
        "Yang Wu",
        "Pengwei Zhan",
        "Yunjian Zhang",
        "Liming Wang",
        "Zhen Xu"
      ],
      "year": "2021",
      "venue": "Findings of the ACL/IJCNLP",
      "doi": "10.18653/v1/2021.findings-acl.226"
    },
    {
      "citation_id": "83",
      "title": "Infogcl: Informationaware graph contrastive learning",
      "authors": [
        "Dongkuan Xu",
        "Wei Cheng",
        "Dongsheng Luo",
        "Haifeng Chen",
        "Xiang Zhang"
      ],
      "year": "2021",
      "venue": "Proc. of NeurIPS"
    },
    {
      "citation_id": "84",
      "title": "MTAG: modaltemporal attention graph for unaligned human multimodal language sequences",
      "authors": [
        "Jianing Yang",
        "Yongxin Wang",
        "Ruitao Yi",
        "Yuying Zhu",
        "Azaan Rehman",
        "Amir Zadeh",
        "Soujanya Poria",
        "Louis-Philippe Morency"
      ],
      "year": "2021",
      "venue": "Proc. of NAACL-HLT",
      "doi": "10.18653/v1/2021.naacl-main.79"
    },
    {
      "citation_id": "85",
      "title": "A reexamination of text categorization methods",
      "authors": [
        "Yiming Yang",
        "Xin Liu"
      ],
      "year": "1999",
      "venue": "Proc. of SIGIR",
      "doi": "10.1145/312624.312647"
    },
    {
      "citation_id": "86",
      "title": "JPG -jointly learn to align: Automated disease prediction and radiology report generation",
      "authors": [
        "Jingyi You",
        "Dongyuan Li",
        "Manabu Okumura",
        "Kenji Suzuki"
      ],
      "year": "2022",
      "venue": "Proc. of COLING"
    },
    {
      "citation_id": "87",
      "title": "Graph contrastive learning with augmentations",
      "authors": [
        "Yuning You",
        "Tianlong Chen",
        "Yongduo Sui",
        "Ting Chen",
        "Zhangyang Wang",
        "Yang Shen"
      ],
      "year": "2020",
      "venue": "Proc. of NeurIPS"
    },
    {
      "citation_id": "88",
      "title": "Learning modality-specific representations with selfsupervised multi-task learning for multimodal sentiment analysis",
      "authors": [
        "Wenmeng Yu",
        "Hua Xu",
        "Ziqi Yuan",
        "Jiele Wu"
      ],
      "year": "2021",
      "venue": "Proc. of AAAI"
    },
    {
      "citation_id": "89",
      "title": "Tensor fusion network for multimodal sentiment analysis",
      "authors": [
        "Amir Zadeh",
        "Minghai Chen",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proc. of EMNLP",
      "doi": "10.18653/v1/d17-1115"
    },
    {
      "citation_id": "90",
      "title": "Multimodal language analysis in the wild: CMU-MOSEI dataset and interpretable dynamic fusion graph",
      "authors": [
        "Amir Zadeh",
        "Paul Liang",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proc. of ACL",
      "doi": "10.18653/v1/P18-1208"
    },
    {
      "citation_id": "91",
      "title": "Multimodal sentiment intensity analysis in videos: Facial gestures and verbal messages",
      "authors": [
        "Amir Zadeh",
        "Rowan Zellers",
        "Eli Pincus",
        "Louis-Philippe Morency"
      ],
      "year": "2016",
      "venue": "IEEE Intelligent Systems",
      "doi": "10.1109/MIS.2016.94"
    },
    {
      "citation_id": "92",
      "title": "Contrastive selfsupervised learning for graph classification",
      "authors": [
        "Jiaqi Zeng",
        "Pengtao Xie"
      ],
      "year": "2021",
      "venue": "Proc. of AAAI"
    },
    {
      "citation_id": "93",
      "title": "Multi-modal multi-label emotion recognition with heterogeneous hierarchical message passing",
      "authors": [
        "Dong Zhang",
        "Xincheng Ju",
        "Wei Zhang",
        "Junhui Li",
        "Shoushan Li",
        "Qiaoming Zhu",
        "Guodong Zhou"
      ],
      "year": "2021",
      "venue": "Proc. of AAAI"
    },
    {
      "citation_id": "94",
      "title": "COSTA: covariance-preserving feature augmentation for graph contrastive learning",
      "authors": [
        "Yifei Zhang",
        "Hao Zhu",
        "Zixing Song",
        "Piotr Koniusz",
        "Irwin King"
      ],
      "year": "2022",
      "venue": "Proc. of KDD",
      "doi": "10.1145/3534678.3539425"
    },
    {
      "citation_id": "95",
      "title": "Bidirectional transformer reranker for grammatical error correction",
      "authors": [
        "Ying Zhang",
        "Hidetaka Kamigaito",
        "Manabu Okumura"
      ],
      "year": "2023",
      "venue": "Findings of ACL",
      "doi": "10.18653/v1/2023.findings-acl.234"
    },
    {
      "citation_id": "96",
      "title": "Deep Graph Contrastive Representation Learning",
      "authors": [
        "Yanqiao Zhu",
        "Yichen Xu",
        "Feng Yu",
        "Qiang Liu",
        "Shu Wu",
        "Liang Wang"
      ],
      "year": "2020",
      "venue": "ICML Workshop on Graph Representation Learning and Beyond"
    }
  ]
}