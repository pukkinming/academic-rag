{
  "paper_id": "2102.04029v1",
  "title": "Non-Linear Frequency Warping Using Constant-Q Transformation For Speech Emotion Recognition",
  "published": "2021-02-08T06:57:16Z",
  "authors": [
    "Premjeet Singh",
    "Goutam Saha",
    "Md Sahidullah"
  ],
  "keywords": [
    "Speech emotion recognition (SER)",
    "Constant-Q transform (CQT)",
    "Mel frequency analysis",
    "Cross-corpora evaluation"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this work, we explore the constant-Q transform (CQT) for speech emotion recognition (SER). The CQT-based time-frequency analysis provides variable spectro-temporal resolution with higher frequency resolution at lower frequencies. Since lower-frequency regions of speech signal contain more emotion-related information than higher-frequency regions, the increased low-frequency resolution of CQT makes it more promising for SER than standard short-time Fourier transform (STFT). We present a comparative analysis of short-term acoustic features based on STFT and CQT for SER with deep neural network (DNN) as a back-end classifier. We optimize different parameters for both features. The CQT-based features outperform the STFT-based spectral features for SER experiments. Further experiments with cross-corpora evaluation demonstrate that the CQT-based systems provide better generalization with out-ofdomain training data.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "The speech emotion recognition (SER) is the task for recognizing emotion from human speech. The potential applications of SER include human-computer interaction, sentiment analysis and health-care  [1] -  [4] . Humans naturally sense the emotions in speech while machines find it difficult to characterize them  [5] ,  [6] . Techniques proposed till date have significantly increased the machine's ability to recognize speech emotions. However, the task is still challenging mainly due to the presence of large interpersonal and intrapersonal variability and the differences in speech quality used to train and evaluate the system. The goal of this work is to develop an improved SER system by considering emotion-specific acoustic parameters from speech that are assumed to be more robust to unwanted variabilities.\n\nPrevious studies in SER research have shown that spectral and prosodic characteristics of speech contain emotion-related information. Spectral features include lower formants frequencies (F1 and F2), speech amplitude and energy, zero crossing rate (ZCR) and spectral parameters, e.g., like spectral flux and spectral roll-off  [7] -  [9] . Prosodic features include pitch, pitch harmonics, intonation, and speaking rate  [8] ,  [9] . The acoustic front-ends are used with Gaussian mixture model (GMM) or support vector machines (SVMs) as back-end classifiers for SER tasks  [10] . Studies with prosody reveal that high arousal emotions, such as Angry, Happy and Fear, have higher average pitch values with abrupt pitch variations whereas low arousal emotions like Sadness and Neutral have lower pitch values with consistent pitch contours  [1] ,  [11] -  [14] . The authors in  [15]  have reported that recognition accuracy of Anger is higher near F2 (1250-1750 Hz) and that of Neutral is higher near F1 (around 200-1000 Hz). Authors in  [16]  report that center frequencies of F2 and F3 are reduced in depressed individuals. In  [17] , the authors report that high arousal emotions have higher mean F1 and lower F2 and high (positive) valence emotions have high mean F2. In  [18] , authors report discrimination between idle and negative emotions using temporal patterns in formants. In  [19] , the authors have demonstrated that non-linear frequency scales, such as logarithmic, mel and equivalent rectangular bandwidth (ERB), have considerable impact in SER performance over linear frequency scale.\n\nRecent works with deep learning methods such as convolutional neural networks (CNNs) or CNN with recurrent neural networks (CNN-RNNs), use spectrogram or raw waveform as input and have shown impressive results  [20] -  [22] . These data-driven methods automatically learn the emotion-related representation, however, the role of individual speech attributes in the decision making process is not clear due to the lack of explainability. On the other hand, the generalization of these methods remains an open problem, especially when the audiodata for train and test are substantially different in terms of language and speech quality  [23] .\n\nWe address this generalization issue by capturing emotionrelated information from speech before processing with a neural network back-end. Given the fact that the low and mid frequency regions of speech spectrum contain pitch harmonics and lower formants that are relevant for emotion recognition, we propose to use a more appropriate approach for timefrequency analysis that produces emotion-oriented speech representation in the first place. Even though the processing with mel frequency warping introduces non-linearity in some sense, the power spectrum from the speech is essentially computed with a uniform frequency resolution. We propose to use a time-frequency analysis method called constant-Q transform (CQT). This transformation offers higher frequency resolution at low-frequency regions and higher time resolution at highfrequency regions. As the pitch harmonics and lower formants reside in the low-frequency regions of speech spectrum, we  Fig.  1 . F-ratios of spectrograms based on CQT (top), mel-filter (middle), and standard STFT (bottom) corresponding to the frequency bins. We use the speech sentences with fixed text 'a02' from EmoDB database (discussed in Section III-A). We select the same text assuming spectra characteristics of emotions to be text-dependent. First column shows the values over the entire frequency range while the second column focuses only on the lower-frequency regions.\n\nhypothesize that keeping high resolution in this region may efficiently capture emotion-related information.\n\nThe CQT was initially proposed for music signal processing  [24] . Then it was also applied in different speech processing tasks, e.g., anti-spoofing  [25] ,  [26] , speaker verification  [27]  and acoustic scene classification  [28] . Recently, the CQT has also been studied for SER  [29] , but without success. This is possibly due to the lack of optimization of CQT parameters and/or the applied end-to-end model fails to exploit the advantages of CQT. Recent studies show that CNNbased models are suitable for SER including cross-corpora evaluation  [23] ,  [30] . In this work, we also adopt a CNN-based approach for modeling SER systems. Our main contributions in this work are summarized as follows: (i) We propose a new framework for CQT-based SER by optimizing CQT extraction parameters for reduced redundancy and improved performance, (ii) We investigate CNN architecture known as time-delay neural networks (TDNNs) suitable for speech pattern classification tasks  [31] ,  [32]  for SER, and (iii) We perform cross-corpora evaluation with three different speech corpora to assess the generalization ability of the proposed method. Our results demonstrate that the optimized CQT features not only outperform short-time Fourier transform (STFT) features but also provide better generalization.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Methodology",
      "text": "In this section, we discuss the CQT-based feature extraction framework and the TDNN architecture for emotion recognition.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Constant-Q Transform",
      "text": "The CQT of a time-domain signal x[n] is defined as,\n\nwhere , where Q is the (constant) Q factor of the filter banks  [24] . In CQT computation, the window length N [k] varies for different values of k. Hence, x[n] is correlated with sinusoids of different lengths with equal cycles of oscillation. This leads to constant-Q filter bank representation with geometrically spaced center frequencies over frequency octaves. Hence, we obtain a time-frequency representation which has frequency resolution varying from high to low towards increasing frequencies.\n\nThe CQT representation of an audio signal depends on the number of octaves of frequencies and the number of frequency bins per each octave. The number of octaves depends upon the chosen minimum frequency (F min ) and the maximum frequency (F max ) of operation, and this equals to log 2 Fmax Fmin  [25] . The CQT representation with reduced number of total frequency bins over a fixed number of octaves will provide detailed information for lower frequency region with reduced redundancy. Conversely, due to linearly spaced frequency bins, short-time Fourier transform (STFT) does not offer this flexibility. Fixing F min to 32.7 Hz and F max to Nyquist frequency gives us approximately 8 octaves. Hop length in CQT computation defines the number of time samples by which CQT computation window moves. The CQT also has resemblance with continuous wavelet transform (CWT) which provides variable time-frequency resolution and has been found helpful for SER  [33] .\n\nDuring the CQT-based feature extraction process, the CQT coefficients are uniformly resampled and then processed with discrete cosine transform (DCT) to compute speech features known as constant-Q cepstral coefficients (CQCCs).\n\nWe perform class separability analysis of the timefrequency representations by computing the F-ratios  [34] . The Fig.  1  shows the F-ratio obtained at different frequency bins. The higher F-ratios at lower bins for CQT and STFT show the presence of more discriminative information. The figure also indicates that CQT-spectrogram has more number of discriminative coefficients on an average over others due to higher resolution in low-frequency regions.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Cnn Architecture",
      "text": "The time-frequency representation of speech-like signal is suitable to be used with 1-D CNN, popularly known as TDNN in speech processing literature. Our method is inspired by the TDNN-based x-vector system  [32]  developed for speaker verification task. This processes speech information at frame and segment level. In frame level, the TDNN captures contextual information by applying kernel over adjacent frames and by processing each speech frame in an identical manner. This also applies dilation in the temporal domain to reduce redundancy and to make it computationally efficient. The frame-level information is processed with several TDNN layers having different kernel sizes and dilation parameters. Finally, temporal pooling aggregates frame-level information into segment-level and this is followed by processing with fully connected (FC) and softmax layer for classification objective. The standard x-vector system computes the segment-level intermediate representation referred as embeddings which are further processed with another system for classification. In contrast, our proposed method trains the network in an end-toend fashion for which the emotion for a test speech is obtained from the output of the trained network.\n\nWe empirically optimize the parameters for TDNN architecture. Finally, we use four TDNN layers, followed by statistics pooling with mean and standard deviation, and one FC layer before softmax. Table  I",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Experimental Setup",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Speech Corpora",
      "text": "In our experiments, we use three different speech corpora which are described in Table  II . We downsample speech files at sampling rate of 16 kHz when required. The EmoDB is a German language corpora while RAVDESS and IEMOCAP are in English. For IEMOCAP database, we select only four emotions (Angry, Happy, Sad and Neutral) as some of the emotion class have inadequate data for training neural network models  [30] . We perform cross-corpora SER experiments by selecting the same four emotions.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Experimental Details & Evaluation Methodology",
      "text": "First, we optimize the parameters of the features on EmoDB. We perform experiments on this corpus using leave-onespeaker-out (LOSO) cross validation by keeping one speaker in test. Out of the remaining speakers, we use two of them in validation and seven in training. We also apply five-fold data augmentation by corrupting training set with additive noises and room reverberation effect following the Kaldi recipe 1  for x-vector training  [32] .\n\nWe extract features from each speech utterance and discard the non-speech frames with a simple energy-based speech activity detector (SAD). We apply utterance-level cepstral mean variance normalization (CMVN) before creating the training and validation samples with chunks of 100 consecutive frames. We consider multiple non-overlapping chunks from the speech utterances depending on the length. We use LibROSA 2  python library for feature extraction.\n\nWe do not apply chunking for testing and consider the full utterance for computing the test accuracy. We report the final performances with accuracy as well as unweighted average recall (UAR). The accuracy is computed as the ratio between the number of correctly classified sentences to the total number of sentences in test. The UAR is given as  [38] ,\n\nwhere A refers to the contingency matrix, A ij corresponds to number of samples in class i classified into class j and K is the total number of classes. As accuracy is considered  unintuitive for databases with uneven samples across different classes, we optimize the feature extraction parameters based on the UAR metric.\n\nIn DNN, we use ReLU activation function and batch normalization for all the hidden layers. For regularization, we apply dropout with probability 0.3 on the FC layer only. We use Adam optimizer with learning rate 0.001. The mini-batch size is 64. We train the models for 50 epochs and finally testing is done with the model which achieves the highest UAR on the validation set. We repeat each experiment multiple times and report the average performance.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iv. Results",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Experiments On Emodb",
      "text": "First, we conduct experiments on EmoDB and optimize the CQT parameters. We vary the number of bins per octave from 1 to 96. We also perform the experiments with three different hop lengths: 64, 128, and 192. The top row of Fig.  2  shows the standard accuracy and UAR for CQT. We observe improved performance for lower bins per octave and lower hop length.\n\nThe performance remains very similar for bins per octaves between 2 and 5. We select 3 bins per octave as the optimum observing the consistency in different runs of the experiment. We fix the hop size 64 as the optimum since the performance is consistently better with this hop size, especially, for lower bins per octave. Since the optimized CQT features use 24 filters and hop length as 64, we apply similar configuration for STFT-based mel frequency cepstral coefficients (MFCCs) as well as mel frequency spectral coefficients (MFSCs) (i.e., MFCCs without DCT). The SER performances with CQT and STFT-based features are illustrated as a bar plot in Fig.  2 . We observe that CQT coefficients as well as CQCCs consistently outperform MFCCs and MFSCs. We also notice that the optimized MFSC outperforms baseline MFSC. The DCT slightly degrades performance in both CQT and STFTbased approaches. We chose the best configuration for both features for the remaining experiments.\n\nFigure  3  shows the confusion matrices obtained for CQT and MFSC in experiments with EmoDB. We observe that CQT is better capable of discriminating emotions such as Fear,",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Cross-Corpora Evaluation",
      "text": "Table  III  shows the performance obtained after cross corpus testing. The optimized CQT shows better performance than optimized MFSC for most cases except when the train-test pair are IEMOCAP-EmoDB and IEMOCAP-RAVDESS. The obtained results consolidate our hypothesis that CQT helps in better capturing of emotion-dependent information leading to better generality across databases.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "V. Discussion And Conclusion",
      "text": "We notice that increasing the frequency resolution at lower frequency regions led to substantial improvement in SER performance. This also confirms that low-frequency region containing pitch harmonics and lower formants convey important emotion-specific information. At the same time, the CQT with lower high-frequency resolution does not degrade the overall SER performance which indicates that high-frequency regions are less important from emotion perspective. Also, better performance with fewer frequency bins in both CQT and MFSC indicates less redundant time-frequency representation is more effective for emotion discrimination. Though STFT with optimized parameters generates spectrograms with higher frequency resolution, the performance degrades most likely due to increased redundancy caused by capturing details of high-frequency region. Cross-corpora evaluation suggests that CQT-based time-frequency representation provides better generalization for SER task with different speech corpora in training and test.\n\nWe conclude that CQT is a better choice of time-frequency representation in terms of both recognition performance and generalization ability. However, the SER performance is still poor for real-world deployment. We also gain no improvement over MFSC for all the seven emotions included in EmoDB corpus. This indicates that the time-frequency representation needs further investigation for SER. This work can also be extended by exploring CQT representation with recurrent architecture and attention mechanisms which are lacking within our TDNN framework but found useful for SER.",
      "page_start": 5,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: F-ratios of spectrograms based on CQT (top), mel-ﬁlter (middle), and standard STFT (bottom) corresponding to the frequency bins. We use the speech",
      "page": 2
    },
    {
      "caption": "Figure 1: shows the F-ratio obtained at different frequency bins.",
      "page": 3
    },
    {
      "caption": "Figure 2: Performance comparison of CQT for different parameter values. Optimized CQT shows the response of CQT with 3 bins per octave and hop length of",
      "page": 4
    },
    {
      "caption": "Figure 3: Confusion matrices for emotion classiﬁcation experiment with optimized CQT and MFSC features in EmoDB corpus. Given values are the ratio of",
      "page": 4
    },
    {
      "caption": "Figure 2: shows the",
      "page": 4
    },
    {
      "caption": "Figure 2: We observe that CQT coefﬁcients as well as CQCCs",
      "page": 4
    },
    {
      "caption": "Figure 3: shows the confusion matrices obtained for CQT",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Hop len:64": "Hop len:128\nHop len:192"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Train on": "EmoDB",
          "Test on": "RAVDESS",
          "MFSC": "0.41 / 0.44",
          "CQT": "0.44 / 0.46"
        },
        {
          "Train on": "",
          "Test on": "IEMOCAP",
          "MFSC": "0.36 / 0.37",
          "CQT": "0.38 / 0.39"
        },
        {
          "Train on": "RAVDESS",
          "Test on": "EmoDB",
          "MFSC": "0.45 / 0.42",
          "CQT": "0.48 / 0.48"
        },
        {
          "Train on": "",
          "Test on": "IEMOCAP",
          "MFSC": "0.30 / 0.32",
          "CQT": "0.32 / 0.34"
        },
        {
          "Train on": "IEMOCAP",
          "Test on": "EmoDB",
          "MFSC": "0.64 / 0.50",
          "CQT": "0.63 / 0.50"
        },
        {
          "Train on": "",
          "Test on": "RAVDESS",
          "MFSC": "0.38 / 0.39",
          "CQT": "0.38 / 0.39"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Speech emotion recognition: Emotional models, databases, features, preprocessing methods, supporting modalities, and classifiers",
      "authors": [
        "Mehmet Berkehan",
        "Akc ¸ay",
        "Kaya Oguz"
      ],
      "year": "2020",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "2",
      "title": "Speech emotion recognition: a review",
      "authors": [
        "Sreenivasa Rao",
        "Krothapalli Shashidhar",
        "G Koolagudi"
      ],
      "year": "2013",
      "venue": "Emotion recognition using speech features"
    },
    {
      "citation_id": "3",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "Moataz Ayadi",
        "Mohamed Kamel",
        "Fakhri Karray"
      ],
      "year": "2011",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "4",
      "title": "New approach in quantification of emotional intensity from the speech signal: emotional temperature",
      "authors": [
        "Josué Jesús B Alonso",
        "Manuel Cabrera",
        "Carlos Medina",
        "Travieso"
      ],
      "year": "2015",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "5",
      "title": "Affective computing",
      "authors": [
        "Rosalind Picard"
      ],
      "year": "2000",
      "venue": "Affective computing"
    },
    {
      "citation_id": "6",
      "title": "Affective computing: challenges",
      "authors": [
        "Rosalind Picard"
      ],
      "year": "2003",
      "venue": "International Journal of Human-Computer Studies"
    },
    {
      "citation_id": "7",
      "title": "Speech emotion recognition: Features and classification models",
      "authors": [
        "Lijiang Chen",
        "Xia Mao",
        "Yuli Xue",
        "Lee Cheng"
      ],
      "year": "2012",
      "venue": "Digital Signal Processing"
    },
    {
      "citation_id": "8",
      "title": "Towards a standard set of acoustic features for the processing of emotion in speech",
      "authors": [
        "Florian Eyben",
        "Anton Batliner",
        "Bjoern Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of Meetings on Acoustics 159ASA"
    },
    {
      "citation_id": "9",
      "title": "The Geneva minimalistic acoustic parameter set (GeMAPS) for voice research and affective computing",
      "authors": [
        "Florian Eyben"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "10",
      "title": "Application of speakerand language identification state-of-the-art techniques for emotion recognition",
      "authors": [
        "M Kockmann",
        "L Burget",
        "J Cernocký"
      ],
      "year": "2011",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "11",
      "title": "Emotions and speech: Some acoustical correlates",
      "authors": [
        "E Carl",
        "Kenneth Williams",
        "Stevens"
      ],
      "year": "1972",
      "venue": "The Journal of the Acoustical Society of America"
    },
    {
      "citation_id": "12",
      "title": "Acoustic profiles in vocal emotion expression",
      "authors": [
        "Rainer Banse",
        "Klaus Scherer"
      ],
      "year": "1996",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "13",
      "title": "Automatic statistical analysis of the signal and prosodic signs of emotion in speech",
      "authors": [
        "Roddy Cowie",
        "Ellen Douglas-Cowie"
      ],
      "year": "1996",
      "venue": "Proc. ICSLP. IEEE"
    },
    {
      "citation_id": "14",
      "title": "Multiscale amplitude feature and significance of enhanced vocal tract information for emotion classification",
      "authors": [
        "Suman Deb",
        "Samarendra Dandapat"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Cybernetics"
    },
    {
      "citation_id": "15",
      "title": "A comparative study of traditional and newly proposed features for recognition of speech under stress",
      "authors": [
        "E Sahar",
        "John Hl Bou-Ghazale",
        "Hansen"
      ],
      "year": "2000",
      "venue": "IEEE Transactions on Speech and Audio Processing"
    },
    {
      "citation_id": "16",
      "title": "Acoustical properties of speech as indicators of depression and suicidal risk",
      "authors": [
        "Daniel Joseph",
        "Richard Shiavi",
        "Stephen Silverman",
        "Marilyn Silverman",
        "M Wilkes"
      ],
      "year": "2000",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "17",
      "title": "Emotion dimensions and formant position",
      "authors": [
        "Martijn Goudbeek",
        "Philippe Goldman",
        "Klaus Scherer"
      ],
      "year": "2009",
      "venue": "Proc. INTERPSEECH"
    },
    {
      "citation_id": "18",
      "title": "Formant position based weighted spectral features for emotion recognition",
      "authors": [
        "Elif Bozkurt",
        "Engin Erzin"
      ],
      "year": "2011",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "19",
      "title": "Amplitude-frequency analysis of emotional speech using transfer learning and classification of spectrogram images",
      "authors": [
        "Margaret Lech",
        "Melissa Stolar",
        "Robert Bolia",
        "Michael Skinner"
      ],
      "year": "2018",
      "venue": "Advances in Science, Technology and Engineering Systems Journal"
    },
    {
      "citation_id": "20",
      "title": "Speech emotion recognition using deep convolutional neural network and discriminant temporal pyramid matching",
      "authors": [
        "Shiqing Zhang",
        "Shiliang Zhang",
        "Tiejun Huang",
        "Wen Gao"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "21",
      "title": "Learning salient features for speech emotion recognition using convolutional neural networks",
      "authors": [
        "Qirong Mao",
        "Ming Dong",
        "Zhengwei Huang",
        "Yongzhao Zhan"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "22",
      "title": "Characterizing types of convolution in deep convolutional recurrent neural networks for robust speech emotion recognition",
      "authors": [
        "Che-Wei Huang",
        "Shrikanth Narayanan"
      ],
      "year": "2017",
      "venue": "Characterizing types of convolution in deep convolutional recurrent neural networks for robust speech emotion recognition",
      "arxiv": "arXiv:1706.02901"
    },
    {
      "citation_id": "23",
      "title": "Analysis of deep learning architectures for cross-corpus speech emotion recognition",
      "authors": [
        "Jack Parry",
        "Dimitri Palaz",
        "Georgia Clarke",
        "Pauline Lecomte",
        "Rebecca Mead",
        "Michael Berger",
        "Gregor Hofer"
      ],
      "year": "2019",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "24",
      "title": "Calculation of a constant Q spectral transform",
      "authors": [
        "Judith Brown"
      ],
      "year": "1991",
      "venue": "The Journal of the Acoustical Society of America"
    },
    {
      "citation_id": "25",
      "title": "Constant Q cepstral coefficients: A spoofing countermeasure for automatic speaker verification",
      "authors": [
        "Massimiliano Todisco",
        "Héctor Delgado",
        "Nicholas Evans"
      ],
      "year": "2017",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "26",
      "title": "Synthetic speech detection using fundamental frequency variation and spectral features",
      "authors": [
        "Monisankha Pal",
        "Dipjyoti Paul",
        "Goutam Saha"
      ],
      "year": "2018",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "27",
      "title": "Further optimisations of constant Q cepstral processing for integrated utterance and text-dependent speaker verification",
      "authors": [
        "Héctor Delgado"
      ],
      "year": "2016",
      "venue": "Proc. IEEE SLT"
    },
    {
      "citation_id": "28",
      "title": "CQT-based convolutional neural networks for audio scene classification",
      "authors": [
        "T Lidy",
        "A Schindler"
      ],
      "year": "2016",
      "venue": "Proc. the Detection and Classification of Acoustic Scenes and Events 2016 Workshop"
    },
    {
      "citation_id": "29",
      "title": "An end-to-end deep learning framework for speech emotion recognition of atypical individuals",
      "authors": [
        "Dengke Tang",
        "Junlin Zeng",
        "Ming Li"
      ],
      "year": "2018",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "30",
      "title": "Speech emotion recognition with deep convolutional neural networks",
      "authors": [
        "Dias Issa",
        "M Fatih Demirci",
        "Adnan Yazici"
      ],
      "year": "2020",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "31",
      "title": "Phoneme recognition using time-delay neural networks",
      "authors": [
        "A Waibel",
        "T Hanazawa",
        "G Hinton",
        "K Shikano",
        "K Lang"
      ],
      "year": "1989",
      "venue": "IEEE Transactions on Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "32",
      "title": "X-vectors: Robust DNN embeddings for speaker recognition",
      "authors": [
        "David Snyder",
        "Daniel Garcia-Romero",
        "Gregory Sell",
        "Daniel Povey",
        "Sanjeev Khudanpur"
      ],
      "year": "2018",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "33",
      "title": "Continuous wavelet transform based speech emotion recognition",
      "authors": [
        "P Shegokar",
        "P Sircar"
      ],
      "year": "2016",
      "venue": "Proc. ICSPCS"
    },
    {
      "citation_id": "34",
      "title": "Evaluating feature set performance using the F-ratio and J-measures",
      "authors": [
        "Simon Nicholson",
        "Ben Milner",
        "Stephen Cox"
      ],
      "year": "1997",
      "venue": "Proc. EUROSPEECH"
    },
    {
      "citation_id": "35",
      "title": "A database of German emotional speech",
      "authors": [
        "Felix Burkhardt",
        "Astrid Paeschke",
        "Miriam Rolfes",
        "Walter Sendlmeier",
        "Benjamin Weiss"
      ],
      "year": "2005",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "36",
      "title": "The Ryerson audiovisual database of emotional speech and song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English",
      "authors": [
        "R Steven",
        "Frank Livingstone",
        "Russo"
      ],
      "year": "2018",
      "venue": "PLOS One"
    },
    {
      "citation_id": "37",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "38",
      "title": "Classifying skewed data: Importance weighting to optimize average recall",
      "authors": [
        "Andrew Rosenberg"
      ],
      "year": "2012",
      "venue": "Proc. INTERSPEECH"
    }
  ]
}