{
  "paper_id": "2010.14602v2",
  "title": "Copypaste: An Augmentation Method For Speech Emotion Recognition",
  "published": "2020-10-27T20:52:47Z",
  "authors": [
    "Raghavendra Pappagari",
    "Jesús Villalba",
    "Piotr Żelasko",
    "Laureano Moro-Velazquez",
    "Najim Dehak"
  ],
  "keywords": [
    "emotion recognition",
    "data augmentation",
    "Copy-Paste",
    "x-vector",
    "transfer learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Data augmentation is a widely used strategy for training robust machine learning models. It partially alleviates the problem of limited data for tasks like speech emotion recognition (SER), where collecting data is expensive and challenging. This study proposes Copy-Paste, a perceptually motivated novel augmentation procedure for SER. Assuming that the presence of emotions other than neutral dictates a speaker's overall perceived emotion in a recording, concatenation of an emotional (emotion E) and a neutral utterance can still be labeled with emotion E. We hypothesize that SER performance can be improved using these concatenated utterances in model training. To verify this, three CopyPaste schemes are tested on two deep learning models: one trained independently and another using transfer learning from an x-vector model, a speaker recognition model. We observed that all three CopyPaste schemes improve SER performance on all the three datasets considered: MSP-Podcast, Crema-D, and IEMOCAP. Additionally, CopyPaste performs better than noise augmentation and, using them together improves the SER performance further. Our experiments on noisy test sets suggested that CopyPaste is effective even in noisy test conditions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech emotion recognition (SER) deals with recognizing the perceived emotion of a speaker in a recording. Emotion of a speaker can be categorized into many classes such as angry, happy, sad, neutral etc. Recognizing the emotion automatically is challenging as it depends on many factors such as context and demographics  [1] . Some of the applications of SER include mental health analysis  [2] , detecting patient's emotion  [3]  and detecting hate speech in social media  [4] .\n\nMost studies follow a two stage approach to recognize emotion from speech recordings: (1) finding a suitable speech representation and (2) building a model to predict the emotion, given the representations. Some examples like  [5, 6, 7, 8] , explore the use of standard feature representations such as MFCC and OpenSMILE  [9]  features with deep learning models like CNN and LSTM. Few recent studies explore the possibility of learning the speech representations jointly with the emotion recognition model  [10, 11, 12] -using raw waveform or spectrogram as the input. While these works are focused on exploiting only datasets with emotion-related annotations or labels for SER, few other research groups explored leveraging large datasets with annotations unrelated to emotion such as phoneme and speaker identity labels  [13, 14] . In this respect, authors in  [13]  show that knowledge learned in an ASR model trained to predict phones can be transferred to SER. Similarly, features extracted from speaker recognition models such as x-vector model are shown to contain emotion information  [14] .\n\nMajority of these studies use deep learning models which thrive with more data i.e., they are data hungry  [15] . However, current emotion datasets are small and collecting more data with emotion labels is expensive. Moreover, ambiguous nature of emotion makes it difficult to annotate speech recordings. Some of the previous works attempted to tackle this problem by artificially creating more data using data augmentation techniques. Authors in  [14, 16]  show that adding noise to the clean recordings help the model to better recognize emotions. Altering the speaking rate of speech  [16]  and vocal tract length perturbation  [17]  are also shown to help SER. Few recent studies  [18, 19]  ventured into generating emotional speech features using advance techniques such as CycleGANs and StarGANs.\n\nIn this paper, we propose a novel augmentation method for SER. We postulate that the presence of emotions other than neutral emotion dictate the overall perceived emotion of a speaker in a recording. In other words, if the speaker expresses an emotion other than neutral even for a short duration in a longer utterance, then that speaker is perceived to be expressing that emotion. Consequently, concatenation of an emotional utterance with an emotion E and a neutral utterance produces a new emotional utterance that can be labelled with emotion E. We hypothesize that using the new concatenated utterances along with the original ones in SER model training will improve the SER performance. We evaluate our hypothesis using two models: one model trained independently and another using transfer learning from a speaker recognition model. We follow the framework presented in  [14]  to build the models. We first present results with the proposed augmentation and noise augmentation, then we investigate effectiveness of these augmentation techniques in noisy conditions.\n\nThe paper is organized as follows. First, we present the proposed augmentation procedure in Section 2 and then the models we use to conduct experiments in Section 3. We present our datasets and results in Section 4 and 5 respectively followed by conclusions in Section 6.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Copypaste",
      "text": "When trying to classify the emotion of an utterance, some segments might have more emotional information than others. To this respect, some authors observed that when an emotionally neutral speech segment and an emotional segment with emotion E are played in sequence, the human listeners commonly classified the whole sequence as emotion E  [20] . Therefore, the emotional segments (non-neutral) in an utterance might define listener's perception. For example, consider a 10 s recording where the speaker is angry for the first 3 s and manifests a neutral emotion for the remaining 7 s. We surmise that a human annotator might label the utterance as angry even though the speaker expresses neutral emotion for the most part of the recording. In these cases, the recognition of the angry emotion by machine learning models might be difficult, as the neutral emotion dominates the overall statistics of the utterance referred in the previous example. In this work, we address this problem by proposing CopyPaste augmentation technique. This technique considers that a speaker is perceived to be expressing an emotion E (non-neutral) even if that emotion is exhibited for a short duration. Hence, we propose a data augmentation methodology consisting of the concatenation of an emotional utterance with emotion E and a neutral utterance. The resulting concatenated utterance is then labelled with emotion E for model training. As we copy one utterance and paste (concatenate) it at the beginning or ending of another utterance to produce a new one, we have called this process CopyPaste augmentation. Under this method, we present three data augmentation schemes for model training:\n\n1. Concatenation of an emotional utterance (say emotion E) and a neutral utterance to produce another utterance with emotion E. We refer to this scheme as Neutral CopyPaste (N-CP)\n\n2. Concatenation of two emotional utterances with same emotion E to produce another utterance with emotion E. We refer to this scheme as Same Emotion CopyPaste (SE-CP)",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Using N-Cp And Se-Cp Together During Model Training. We Denote This Scheme With N+Se-Cp",
      "text": "Through CopyPaste schemes, we can produce a greater variety in the training data which can help the model generalize better. We expect that the N-CP scheme i.e., concatenating emotional utterances with neutral utterances forces the model to focus more on emotional parts of an utterance. For example, if the input is a concatenation of angry utterance and neutral utterance then N-CP scheme forces the model to focus more on the angry part of the utterance compared to neutral part of the utterance. For details of CopyPaste schemes implementation in our models and it's comparison with standard noise augmentation technique, please refer to the Section 3.2.2 and 5.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Our Models",
      "text": "In this section, we present the details of our SER models employed to evaluate the proposed CopyPaste schemes. We use the same ResNet-34 architecture detailed in  [14] , where speaker recognition models such as x-vector model are utilized for SER.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Resnet Model Architecture",
      "text": "In this paper, we use ResNet model reported in  [14, 21]  for utterance level representation specific to speaker. Our ResNet model consists of three modules: frame-level representation learning network, pooling network, and utterance-level classifier. Frame-level representation learning network operates on input frame-level features such as Mel-frequency cepstral coefficients (MFCC) and filter-bank coefficients. We use ResNet-34  [22]  structure consisting of a sequence of 2D convolutional layers with residual connections between them. The pooling network comprises a multi-head attention layer and operates on ResNet output xt. Normalized attention scores w h,t for each head h are calculated as follows:\n\n.\n\n(1) Output embedding for head h is the weighted average over its inputs along the time axis:\n\nDifferent heads are designed to capture different speech aspects of the input signal. We concatenate the attention heads output and pass it through a fully connected layer to obtain a single vector embedding summarizing the input. Then, the output of the fully connected layer is passed through an utterance-level classifier to obtain model decision. The whole network structure is illustrated in Table  1 .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Speech Emotion Recognition (Ser)",
      "text": "For achieving SER using the ResNet architecture, we train two models: one model trained with random initialization and another initialized with a model pre-trained for speaker classification. We use 23-dim MFCC representation as input to the ResNet model. Before passing them through ResNet, energy-based voice activity detection and mean normalization are applied. We minimize the cross-entropy loss function during training using Adam optimizer with default parameters in PyTorch. The epoch with the best weighted f1-score on the development set is chosen for evaluating on test set. We report average of weighted f1-scores from 3-runs on the test set for each emotion dataset considered.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Transfer Learning From A Model Pre-Trained For Speaker Classification",
      "text": "For pre-training, we use several datasets with speaker labels. In this work, we use VoxCeleb1, VoxCeleb2, NIST SRE4-10 and Switchboard datasets which together contain approximately 12000 speakers. We use noise and music augmentation in pre-training to improve speaker classification performance. In speaker recognition community this model is commonly referred as x-vector model. For more details, please refer to  [21] .\n\nTo transfer the knowledge learned in this model for SER, we follow the fine-tuning procedure described in  [14]  i.e., replace the final speaker-discriminative layer with one emotion-discriminative layer and fine-tune to minimize emotion loss. In other words, we use the weights learned in pre-training for all the layers except the last layer and then optimize all the weights for emotion classification.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Copypaste Schemes Implementation",
      "text": "During training, we randomly sample a batch of 128 utterances and perform CopyPaste based on the emotion class labels. For SE-CP augmentation scheme, we pick the utterances with the same emotion labels and randomly pair them for concatenation. For N-CP augmentation scheme, we pick utterances with neutral emotion and randomly pair them with all utterances in the batch including neutral utterances. In this scenario, there is a risk that the resulting models are biased against the neutral emotion, as 50% of each augmented utterance is of neutral emotion, and yet we force the model to predict the emotion of the other 50% of the augmented utterance. To avoid that danger, we perform CopyPaste augmentation only for 80% of the batches in each epoch. With the same premises, in the N+SE-CP scheme, we follow each of N-CP and SE-CP schemes for 40% of the batches in each epoch amounting to 80% of batches with CopyPaste augmentation. To avoid overfitting, we randomly pick 4 s from each recording for concatenation instead of whole recording. We note that the average length of the training recordings in our datasets is less than 6 s. Hence, our hypothesis is affected only with negligible likelihood by picking only 4 s of each recording for concatenation.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Noise Augmentation",
      "text": "In this work, we augment the training data by adding noise and music from MUSAN corpus  [23] . Our augmented data contains six copies of training set with SNRs of 10 dB, 5 dB and 0 dB after adding noise and music. We denote the models trained with clean and augmented data as Clean+Noise. As researchers showed that effectiveness of adding noise to the training data is more evident on noisy test data compared to clean test data  [24] , we compare noise augmentation with CopyPaste in noisy test conditions. As emotion datasets are usually clean and have higher SNR, adding noise to the test data is considered. We create two sets of test data, one with SNR level of 10 dB and another with 0 dB for comparison with CopyPaste.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Datasets",
      "text": "We evaluate the CopyPaste augmentation schemes on three contrasting datasets: MSP-Podcast (natural and no restriction on spoken content), Crema-D (acted and restricted to 12 sentences), and IEMO-CAP (acted -scripted and improvised). The details are as follows.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Msp-Podcast Dataset",
      "text": "MSP-Podcast dataset 1 is collected from podcast recordings which discuss about a variety of topics like politics, sports and movies  [25] . The annotations are obtained using crowd-sourced workers with a minimum of 5 workers for each utterance. In this work, we used 5 emotion classes: angry, happy, sad, neutral, disgust for classification as in  [14] . We used the standard splits in Release 1.4 for training, development, and testing. This dataset has 610 speakers (14200 utterances) in the training split, 30 (2893 utterances) in the development, and 50 speakers (6482 utterances) in the test split.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Crema-D Dataset",
      "text": "Crema-D dataset 2  is a multimodal dataset (audio and visual) containing utterances with enacted emotions by 91 professional actors from a pre-defined list of 12 sentences. This dataset includes 48 male and 48 female actors with diverse ethnicity and age distribution. We follow the same data splits as in  [14]  with 51 actors for training, 8 for development and 32 for testing. We experiment with 4 emotion categories: angry, happy, neutral and sad.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Iemocap",
      "text": "IEMOCAP dataset is a multimodal conversational dataset recorded with 5 female and 5 male actors  [26] . It contains improvised and scripted conversations about pre-defined topics in 5 sessions. Each utterance is annotated by at least 3 annotators who were asked to choose from 8 emotion categories namely angry, happy, neutral, sad, disgust, fear and excited. In this work, we choose a subset of data with labels angry, happy, neutral and sad as in the previous works. We perform 5-fold cross-validation (CV) on this dataset as it is relatively small and contains only 10 speakers. For model training on each fold, we use three sessions for training the model, one session for development, and the remaining session for testing. In each fold, we use weighted f1-score as our metric, and hence, we report an average of weighted f-scores of 5-fold CV for each experiment.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Results",
      "text": "In this work, we used weighted f1-score as metric (higher the better) to measure emotion model classification performance. We first show the effectiveness of CopyPaste schemes on clean data and noise augmented data. Then, we present results on artificially created noisy test data to compare CopyPaste and noise augmentation. General considerations: Tables  2  and 3  show the results of CopyPaste schemes on randomly initialized ResNet model and speaker pre-trained ResNet model respectively. Comparing both tables, we can observe that pre-training improves the model performance significantly on all datasets as is the case in  [14] . Models trained with noise augmented data perform better compared to models trained only on clean data corroborating with previous research  [14, 16] . Comparison of models trained with and without CopyPaste schemes (4th-6th columns vs. 3rd column) reveals that our models perform better on all datasets with all schemes. Though application of CopyPaste schemes provide performance improvement in most cases, we do not observe a single best scheme across datasets and models except on Crema-D where N-CP scheme consistently performs best. We can observe that CopyPaste schemes are effective on both clean data as well as noise augmented data. We note that the improvements obtained with CopyPaste schemes on the randomly initialized ResNet model are relatively higher compared to the improvements on the pre-trained ResNet model.\n\nPer-class analysis: As noted in the Section 3.2, there is a risk that the model can get biased to not predict neutral when N-CP scheme is employed during model training. Hence, we examined class-wise f1-scores of our models to identify the main source of improvements and observed that in most cases performance improved for all emotion classes. As an example, we show in Table  4  classwise f1-scores of emotion classes on Crema-D dataset. These scores are obtained with ResNet model pre-trained for speaker classification and trained on clean data. We can observe improvements for all emotion classes with CopyPaste schemes during training. Among CopyPaste schemes, N-CP is performing best for all classes except for sad emotion for which N+SE-CP performs best.\n\nNoise augmentation: Comparing the augmentation techniques, CopyPaste and noise augmentation, we can observe from Tables 2 and 3 that CopyPaste schemes performs better in most cases suggesting that concatenating utterances based on emotion helps the model generalize better compared to adding noise to the training data. As noted in Section 3.2.3, we compare noise augmentation and Copy-Paste in noisy test conditions too. Tables  5  and 6  show the results on the noisy test data with SNR level of 10 dB and 0 dB respectively. We used the model pre-trained with speaker classification for this experiment as it is performing the best on all the datasets. As expected, SER performance degraded on the noisy test data suggesting that our models are sensitive to noisy test conditions. Models trained with noise augmentation are more robust compared to models trained with only clean data which illustrates the benefits of augmenting training data with noise. We can also observe that noise augmentation, in most cases, outperforms CopyPaste in the noisy conditions. However, our best models on all the datasets are when used both augmentations together which showcases the effectiveness of proposed CopyPaste schemes even in noisy test conditions.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Conclusions",
      "text": "In this work, we proposed CopyPaste, a perceptually motivated augmentation technique, for speech emotion recognition (SER) task.\n\nWe postulated that for emotions other than neutral, a speaker is perceived to be expressing an emotion E even if it is expressed within a short segment in a longer utterance. We presented three CopyPaste schemes for model training based on the concatenation of utterances w.r.t. emotion labels. We have evaluated CopyPaste augmentation on 3 contrasting emotion datasets using 2 neural net (ResNet) models: one trained independently and another using transfer learning from a model pre-trained to classify speakers. We have shown that all schemes of CopyPaste improve the SER on all the datasets and perform better than standard speech augmentation technique of adding noise to the signal. Our results suggested that CopyPaste is effective even in noisy test conditions. Our best performing models use both CopyPaste and noise augmentation for training. We note that even though CopyPaste schemes are presented for SER task in this paper, the underlying idea can be applied to other utterance based classification tasks such as language identification and age prediction.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Component": "Frame-level\nRepresentation\nLearning",
          "Layer": "7 × 7, 16",
          "Output Size": "T × 23"
        },
        {
          "Component": "",
          "Layer": "3 × 3, 16\n× 3\n(cid:20)\n3 × 3, 16(cid:21)",
          "Output Size": "T × 23"
        },
        {
          "Component": "",
          "Layer": "3 × 3, 32\n× 4, stride 2\n3 × 3, 32(cid:21)\n(cid:20)",
          "Output Size": "T2\n× 12"
        },
        {
          "Component": "",
          "Layer": "3 × 3, 64\n× 6, stride 2\n(cid:20)\n3 × 3, 64(cid:21)",
          "Output Size": "T4\n× 6"
        },
        {
          "Component": "",
          "Layer": "3 × 3, 128\n× 3, stride 2\n(cid:20)\n3 × 3, 128(cid:21)",
          "Output Size": "T8\n× 3"
        },
        {
          "Component": "",
          "Layer": "average pool 1 × 3",
          "Output Size": "T8"
        },
        {
          "Component": "Pooling",
          "Layer": "32 heads attention",
          "Output Size": "32 × 128"
        },
        {
          "Component": "Utterance-level\nClassiﬁer",
          "Layer": "FC",
          "Output Size": "400"
        },
        {
          "Component": "",
          "Layer": "FC",
          "Output Size": "#classes"
        }
      ],
      "page": 2
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion recognition from speech: a review",
      "authors": [
        "G Shashidhar",
        "K Sreenivasa Koolagudi",
        "Rao"
      ],
      "year": "2012",
      "venue": "International journal of speech technology"
    },
    {
      "citation_id": "2",
      "title": "Clinical state tracking in serious mental illness through computational analysis of speech",
      "authors": [
        "Daniel Armen C Arevian",
        "Nikolaos Bone",
        "Malandrakis",
        "Kenneth Victor R Martinez",
        "David Wells",
        "Shrikanth Miklowitz",
        "Narayanan"
      ],
      "year": "2020",
      "venue": "PLoS one"
    },
    {
      "citation_id": "3",
      "title": "Feature selection for automatic analysis of emotional response based on nonlinear speech modeling suitable for diagnosis of alzheimer's disease",
      "authors": [
        "Karmele Lopez-De Ipiña",
        "J Alonso-Hernández",
        "Jordi Solé-Casals",
        "Carlos Manuel Travieso-González",
        "Aitzol Ezeiza",
        "Marcos Faundez-Zanuy",
        "Pilar Calvo",
        "Blanca Beitia"
      ],
      "year": "2015",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "4",
      "title": "A survey on hate speech detection using natural language processing",
      "authors": [
        "Anna Schmidt",
        "Michael Wiegand"
      ],
      "year": "2017",
      "venue": "Proceedings of the Fifth International workshop on natural language processing for social media"
    },
    {
      "citation_id": "5",
      "title": "Speech emotion recognition using cnn",
      "authors": [
        "Zhengwei Huang",
        "Ming Dong",
        "Qirong Mao",
        "Yongzhao Zhan"
      ],
      "year": "2014",
      "venue": "Proceedings of the 22nd ACM international conference on Multimedia"
    },
    {
      "citation_id": "6",
      "title": "Speech emotion recognition using convolutional and recurrent neural networks",
      "authors": [
        "Wootaek Lim",
        "Daeyoung Jang",
        "Taejin Lee"
      ],
      "year": "2016",
      "venue": "2016 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "7",
      "title": "Deep neural networks for emotion recognition combining audio and transcripts",
      "authors": [
        "Jaejin Cho",
        "Raghavendra Pappagari",
        "Purva Kulkarni",
        "Jesús Villalba",
        "Yishay Carmiel",
        "Najim Dehak"
      ],
      "year": "2018",
      "venue": "Deep neural networks for emotion recognition combining audio and transcripts"
    },
    {
      "citation_id": "8",
      "title": "Speech emotion recognition using deep 1d & 2d cnn lstm networks",
      "authors": [
        "Jianfeng Zhao",
        "Xia Mao",
        "Lijiang Chen"
      ],
      "year": "2019",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "9",
      "title": "Recent developments in opensmile, the munich open-source multimedia feature extractor",
      "authors": [
        "Florian Eyben",
        "Felix Weninger",
        "Florian Gross",
        "Björn Schuller"
      ],
      "year": "2013",
      "venue": "Proceedings of the 21st ACM international conference on Multimedia"
    },
    {
      "citation_id": "10",
      "title": "End-to-end multimodal emotion recognition using deep neural networks",
      "authors": [
        "Panagiotis Tzirakis",
        "George Trigeorgis",
        "A Mihalis",
        "Björn Nicolaou",
        "Stefanos Schuller",
        "Zafeiriou"
      ],
      "year": "2017",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "11",
      "title": "Emotion identification from raw speech signals using dnns",
      "authors": [
        "Mousmita Sarma",
        "Pegah Ghahremani",
        "Daniel Povey",
        "Kumar Nagendra",
        "Kandarpa Goel",
        "Najim Sarma",
        "Dehak"
      ],
      "year": "2018",
      "venue": "Emotion identification from raw speech signals using dnns"
    },
    {
      "citation_id": "12",
      "title": "Adieu features? end-to-end speech emotion recognition using a deep convolutional recurrent network",
      "authors": [
        "George Trigeorgis",
        "Fabien Ringeval",
        "Raymond Brueckner",
        "Erik Marchi",
        "A Mihalis",
        "Björn Nicolaou",
        "Stefanos Schuller",
        "Zafeiriou"
      ],
      "year": "2016",
      "venue": "2016 IEEE international conference on acoustics, speech and signal processing"
    },
    {
      "citation_id": "13",
      "title": "Reusing neural speech representations for auditory emotion recognition",
      "authors": [
        "Egor Lakomkin",
        "Cornelius Weber",
        "Sven Magg",
        "Stefan Wermter"
      ],
      "year": "2018",
      "venue": "Reusing neural speech representations for auditory emotion recognition",
      "arxiv": "arXiv:1803.11508"
    },
    {
      "citation_id": "14",
      "title": "x-vectors meet emotions: A study on dependencies between emotion and speaker recognition",
      "authors": [
        "Raghavendra Pappagari",
        "Tianzi Wang",
        "Jesus Villalba",
        "Nanxin Chen",
        "Najim Dehak"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "15",
      "title": "Deep learning scaling is predictable, empirically",
      "authors": [
        "Joel Hestness",
        "Sharan Narang",
        "Newsha Ardalani",
        "Gregory Diamos",
        "Heewoo Jun",
        "Hassan Kianinejad",
        "Md Patwary",
        "Mostofa Ali",
        "Yang Yang",
        "Yanqi Zhou"
      ],
      "year": "2017",
      "venue": "Deep learning scaling is predictable, empirically",
      "arxiv": "arXiv:1712.00409"
    },
    {
      "citation_id": "16",
      "title": "On the robustness of speech emotion recognition for human-robot interaction with deep neural networks",
      "authors": [
        "Egor Lakomkin",
        "Mohammad Ali Zamani",
        "Cornelius Weber",
        "Sven Magg",
        "Stefan Wermter"
      ],
      "year": "2018",
      "venue": "2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)"
    },
    {
      "citation_id": "17",
      "title": "Cnn+ lstm architecture for speech emotion recognition with data augmentation",
      "authors": [
        "Caroline Etienne",
        "Guillaume Fidanza",
        "Andrei Petrovskii",
        "Laurence Devillers",
        "Benoit Schmauch"
      ],
      "year": "2018",
      "venue": "Cnn+ lstm architecture for speech emotion recognition with data augmentation",
      "arxiv": "arXiv:1802.05630"
    },
    {
      "citation_id": "18",
      "title": "Cycleganbased emotion style transfer as data augmentation for speech emotion recognition",
      "authors": [
        "Fang Bao",
        "Michael Neumann",
        "Ngoc Vu"
      ],
      "year": "2019",
      "venue": "Cycleganbased emotion style transfer as data augmentation for speech emotion recognition"
    },
    {
      "citation_id": "19",
      "title": "Stargan for emotional speech conversion: Validated by data augmentation of end-to-end emotion recognition",
      "authors": [
        "Georgios Rizos",
        "Alice Baird",
        "Max Elliott",
        "Björn Schuller"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "20",
      "title": "Speech emotion perception by human and machine",
      "authors": [
        "Levente Szabolcs",
        "David Tóth",
        "Klára Sztahó",
        "Vicsi"
      ],
      "year": "2008",
      "venue": "Verbal and Nonverbal Features of Human-Human and Human-Machine Interaction"
    },
    {
      "citation_id": "21",
      "title": "Stateof-the-art speaker recognition for telephone and video speech: The jhu-mit submission for nist sre18",
      "authors": [
        "Jesús Villalba",
        "Nanxin Chen",
        "David Snyder",
        "Daniel Garcia-Romero",
        "Alan Mccree",
        "Gregory Sell",
        "Jonas Borgstrom",
        "Fred Richardson",
        "Suwon Shon",
        "Franc ¸ois Grondin"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "22",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "23",
      "title": "Musan: A music, speech, and noise corpus",
      "authors": [
        "David Snyder",
        "Guoguo Chen",
        "Daniel Povey"
      ],
      "year": "2015",
      "venue": "Musan: A music, speech, and noise corpus",
      "arxiv": "arXiv:1510.08484"
    },
    {
      "citation_id": "24",
      "title": "Robust speech recognition in unknown reverberant and noisy conditions",
      "authors": [
        "Roger Hsiao",
        "Jeff Ma",
        "William Hartmann",
        "Martin Karafiát",
        "František Grézl",
        "Lukáš Burget",
        "Igor Szöke",
        "Jan Honza Černockỳ",
        "Shinji Watanabe",
        "Zhuo Chen"
      ],
      "year": "2015",
      "venue": "2015 IEEE Workshop on Automatic Speech Recognition and Understanding"
    },
    {
      "citation_id": "25",
      "title": "Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings",
      "authors": [
        "Reza Lotfian",
        "Carlos Busso"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "26",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    }
  ]
}