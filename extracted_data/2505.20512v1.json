{
  "paper_id": "2505.20512v1",
  "title": "A Feature-Level Bias Evaluation Framework For Facial Expression Recognition Models",
  "published": "2025-05-26T20:26:07Z",
  "authors": [
    "Tangzheng Lian",
    "Oya Celiktutan"
  ],
  "keywords": [
    "Facial expression recognition",
    "Algorithmic fairness",
    "Demographic bias evaluation"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Recent studies on fairness have shown that Facial Expression Recognition (FER) models exhibit biases toward certain visually perceived demographic groups. However, the limited availability of human-annotated demographic labels in public FER datasets has constrained the scope of such bias analysis. To overcome this limitation, some prior works have resorted to pseudo-demographic labels, which may distort bias evaluation results. Alternatively, in this paper, we propose a feature-level bias evaluation framework for evaluating demographic biases in FER models under the setting where demographic labels are unavailable in the test set. Extensive experiments demonstrate that our method more effectively evaluates demographic biases compared to existing approaches that rely on pseudodemographic labels. Furthermore, we observe that many existing studies do not include statistical testing in their bias evaluations, raising concerns that some reported biases may not be statistically significant but rather due to randomness. To address this issue, we introduce a plug-and-play statistical module to ensure the statistical significance of biased evaluation results. A comprehensive bias analysis based on the proposed module is then conducted across three sensitive attributes (age, gender, and race), seven facial expressions, and multiple network architectures on a largescale dataset, revealing the prominent demographic biases in FER and providing insights on selecting a fairer network architecture.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "The advancement of deep learning models relies on obtaining extensive datasets in the wild, which are usually gathered through crowd-sourcing or web crawling initiatives  [1] . However, despite its efficacy in gathering a broad spectrum of data, it unintentionally introduces demographic biases 1 in the dataset  [2] . Without explicit intervention, deep learning models trained on biased datasets have been shown to inherit  [3]  or even amplify  [4]  the biases present in the training data. Consequently, these models exhibit unequal performance across different subgroups of data  [5] ,  [6] . This issue is particularly concerning when the dataset contains faces, as they inherently include sensitive information. Disparities in the performance of models trained on such face data can lead to discriminatory outcomes against certain perceived 2 demographic groups, thereby undermining fairness, a fundamental 1 By demographic biases, we refer to two aspects: at the dataset level, they refer to imbalances in demographic distributions; at the model level, they refer to disparities in performance across different demographic groups. 2 The demographic groups in this paper are based on visual perception from facial appearances rather than biological sex, self-identified gender, race, or age of the subject. Please see Section VI-A for a detailed discussion. Fig.  1 . Race (left column) and age (right column) distribution for each facial expression in the RAF-DB  [10]  dataset. African-American faces showing fear and very young (0-3 years) or older (70+) faces expressing fear or anger are severely underrepresented, each with fewer than 20 samples. These categories still need to be further split into train/val/test sets.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Surprise Fear Disgust Happy Sad Anger Neutral",
      "text": "principle in the implementation of Trustworthy AI  [7] , Ethical AI  [8] , or Responsible AI  [9]  systems.\n\nRecent studies have highlighted disparities in model performance across perceived age, gender, and racial groups in various facial image analysis tasks  [11] , including facial attribute classification  [12] -  [14] , face detection  [15] ,  [16] , and facial recognition  [17] -  [21] . Facial expression recognition (FER) is an essential facial image analysis task to recognize emotional signals from faces and has broad applications in augmented and virtual reality (AR/VR)  [22] ,  [23] , humancomputer interaction (HCI)  [24] ,  [25] , and interactive therapy systems  [22] ,  [26] . Having a reliable and effective method for evaluating demographic biases in FER models is essential for the responsible development of these models and plays a critical role in preventing such biases from propagating into downstream applications. Previous fairness research in FER  [27] -  [31]  often relies on the availability of demographic labels in the test set to calculate performance disparities for bias evaluation (Please see Section III-A for a detailed discussion). However, such demographic labels are typically not available in public FER datasets.\n\nThis limitation has narrowed the scope of prior research on fairness in FER. For instance, some studies have limited their focus to a single sensitive attribute  [32] -  [37] , while others analyze only a subset of facial expressions  [38] ,  [39] . Even studies that consider all seven basic facial expressions and three sensitive attributes  [27] -  [30] ,  [40]  often restrict their experiments to the RAF-DB dataset  [10] . However, RAF-DB is relatively small, and as shown in Fig.  1 , some categories contain too few samples to draw statistically significant conclusions or capture the full diversity of a visual concept. As a result, bias evaluations in these underrepresented categories may be unreliable due to small sample sizes. Larger FER datasets, such as AffectNet  [41] , need to be annotated with perceived demographic labels to enable more robust and reliable bias evaluations  [42] . However, annotating perceived demographic labels for large and diverse FER datasets requires significant human effort. Therefore, it is important to develop methods that can still effectively evaluate demographic biases in FER models in cases where the test set does not include demographic labels.\n\nSome studies have attempted to address this issue by training facial attribute classifiers to assign pseudo-demographic labels to images in the test set and then evaluating bias based on these labels  [35] ,  [36] ,  [39] ,  [43] . However, the quality of these pseudo-demographic labels has not been thoroughly evaluated, and they may distort the results of bias evaluation. Additionally, prior studies  [28] -  [31] ,  [42]  have not applied statistical tests for bias evaluation, raising concerns that some reported biases may not be statistically significant but rather due to randomness. Unlike previous methods that rely on pseudo-demographic labels, we evaluate biases in the feature space of FER models. This idea is inspired by recent advancements in fair representation learning (FRL)  [14] ,  [44] , which suggest that biases in deep learning models primarily originate from learned representations in the feature space rather than the final classification layer. Bias evaluation in the feature space has been explored in computer vision research  [45] ,  [46]  through the Image Embedding Association Test (iEAT)  [47] . However, iEAT is constrained by a fixed set of target-attribute pairs and is designed exclusively for self-supervised models, making it challenging to apply to FER.\n\nTo address these limitations, we propose a feature-level bias evaluation framework to evaluate the demographic biases in FER under the setting where demographic labels are missing in the test set. Our method evaluates biases by using a probe dataset to compute differential associations in the feature space of FER models, through a reformulation of the iEAT. In addition, we introduce a statistical module to ensure the statistical significance of the reported biases in FER, which can be applied to both existing evaluation pipelines and our proposed framework that analyzes the feature space. A comprehensive bias analysis is then conducted based on the proposed module. The contributions of this paper can be summarized as follows:\n\n1. We propose a feature-level bias evaluation framework to evaluate the demographic biases in FER in scenarios where demographic labels are not available in the test set.\n\n2. Extensive experiments show that our method can more effectively evaluate demographic biases in FER models compared to existing methods using pseudo-demographic labels.\n\n3. We introduce a plug-and-play statistical module to ensure statistically significant bias evaluation results, which can be applied to both existing evaluation pipelines and our proposed framework that analyzes the feature space.\n\n4. A comprehensive bias analysis based on the statistical module is then conducted across three sensitive attributes (age, gender, and race), seven facial expressions, and multiple network architectures on a large-scale dataset, revealing the prominent demographic biases in FER and providing insights on selecting a fairer network architecture.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "A. Demographic Biases in FER 1) Biases in FER Datasets: Biases in FER datasets can emerge from two primary sources. The first source is data-level bias, which arises during the collection of facial images with various expressions. These images are often obtained through web search engines or web crawling  [41] . However, web search engines often reflect gender, age, or racial biases  [48] , resulting in uneven perceived demographic distributions in the facial images retrieved for each facial expression query. The second source is annotation-level bias, which arises during the labeling process, where annotators assign facial expressions to collected images. Psychological studies have shown that implicit biases, shaped by societal and perceptual factors, significantly influence the interpretation of facial expressions across demographic groups, including age  [49] , gender  [50] , and race  [51] . For example, research indicates that women are often perceived as happier than men  [50] , and angry expressions are more accurately detected on Black faces, while happy expressions are more easily recognized on White faces  [51] . These perception biases distort individual annotations and embed systemic biases into the datasets  [39] .\n\n2) Evaluating and Mitigating Biases in FER Models: Deep learning models trained on biased datasets often inherit  [3]  or even amplify  [4]  biases present in the training data. This results in unequal model performance across different subsets of data  [5] ,  [6] . For instance, if male faces dominate the training data for anger expression, the model may learn to associate anger predominantly with male faces. Consequently, during testing, the model will likely perform better for male faces when predicting anger. Fairness studies in FER can broadly be categorized into two groups: those that aim to evaluate bias  [32] ,  [34] ,  [35] ,  [37] ,  [38] ,  [42] ,  [43] ,  [52]  and those that further propose methods to mitigate bias  [27] -  [31] ,  [39] .\n\nChen and Joo  [39] , as well as Suresh and Ong  [31] , have proposed using facial action units (AUs) to address biases in FER. However, existing evidence suggests that AU recognition models may themselves be biased  [30] ,  [38] ,  [52] . Moreover, the lack of demographic labels in public FER datasets has constrained prior studies, limiting the scope of their analysis. For instance, some studies have exclusively focused on a single sensitive attribute, such as age  [32] , gender  [33] -  [36] , or race  [37] . Others have attempted to analyze multiple sensitive attributes collectively but often restricted their analysis to specific facial expressions, such as happiness  [38]  or anger  [39] . Even research exploring seven basic facial expressions and three sensitive attributes (i.e., age, gender, and race)  [27] -  [30] ,  [40]  has been limited to the RAF-DB dataset  [10] , whose limitations are discussed in Fig.  1 . In contrast, we provide a more comprehensive bias analysis that includes all three sensitive attributes and seven facial expressions across multiple deep learning models using a large-scale FER dataset.\n\nAnnotating perceived demographic labels for large and diverse FER datasets requires significant human effort. Therefore, previous studies have resorted to training facial attribute classifiers to generate pseudo-demographic labels for bias evaluation  [35] ,  [36] ,  [39] ,  [43] . However, as shown in our experiments, this approach does not evaluate the biases effectively, particularly for age and race, whereas our proposed feature-level bias evaluation framework addresses this limitation and produces more reliable results. Additionally, prior studies  [28] -  [31] ,  [42]  have not applied statistical tests for bias evaluation, raising concerns that some reported biases may result from randomness rather than actual disparities. We address this gap by proposing a statistical module to ensure the statistical significance of the reported biases in FER.",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "B. Evaluating Bias By Association Test",
      "text": "The concept of association tests, originally derived from the Implicit Association Test (IAT)  [53] ,  [54]  in psychology, measures biases by having participants rapidly pair concepts with attributes, where shorter response times suggest stronger associations. This approach is commonly used to detect implicit biases in human perception, including those related to how people perceive facial expressions  [55] -  [59] . Building upon this foundation, Caliskan et al.  [60]  adapted the IAT for machine learning, specifically to assess biases within word embeddings, leading to the development of the Word Embedding Association Test (WEAT).\n\nThis approach was extended to the visual domain through the Image Embedding Association Test (iEAT), which measures association biases in computer vision models  [47] , and has also been applied to multimodal models that combine language and vision to examine bias in these systems  [61] ,  [62] . For example, Sirotkin et al.  [45]  conducted a comprehensive study in self-supervised learning visual models, while Brinkmann et al.  [46]  engaged in a multidimensional analysis of social biases in Vision Transformers. However, iEAT is constrained by a fixed set of binary target-attribute pairs and is designed for self-supervised models. In this paper, we reformulate iEAT, tailoring it for multi-class image classification tasks like FER with sensitive attributes involving multiple groups.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Preliminaries",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Previous Bias Evaluation Pipeline In Fer",
      "text": "The previous bias evaluation pipeline in FER  [27] -  [31]  for a FER model f is illustrated in Fig.  2 . The evaluation is performed on a test set D ts = {(x i , y i , s i )} n i=1 , where x i is a face image, y i is its facial expression label, and s i is its label or pseudo label for a sensitive attribute S. To examine whether f behaves differently across each attribute group in S for each facial expression, the test set can be stratified by each facial expression e ∈ E and S:\n\nD ts e,S = D ts e, s1 , D ts e, s2 , . . . , D ts e, sn ,\n\nwhere S ∈ {s 1 , s 2 , . . . , s n }. We denote D ts e,sj as the subset of test set images labeled with facial expression e and an attribute group s j ∈ S. For instance, D ts anger, gender represents the stratification of test samples labeled anger by gender, with D ts anger, male containing samples labeled anger and male while D ts anger, female containing samples labeled anger and female. We then evaluate f using a performance metric M (•), such as the true positive rate (TPR), within each D ts e,sj . Bias is identified if the disparities in M (D ts e,sj ) across different s j exceed a predefined threshold. This pipeline has two main limitations. First, the test set must be equipped with demographic or pseudo-demographic labels to enable stratification. Second, the chosen threshold does not guarantee the statistical significance of the observed performance disparities. As a result, the observed disparities may simply be due to randomness.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Ieat",
      "text": "iEAT  [47]  is designed to measure differential associations between pairs of target concepts and attributes using image embeddings. It consists of 15 association tests aimed at evaluating human-like social biases. For instance, one target pair, X and Y, could be flowers vs. insects, while an attribute pair, A and B, could be pleasant vs. unpleasant. The differential association between X, Y, A, and B is defined as:\n\nwhere s(w, A, B) (w ∈ {x, y}) represents the differential association of a single image embedding of a target concept with attributes and is calculated as:\n\nwhere a and b represent the image embedding for attributes and cos(•) is the cosine similarity. Statistical tests are then performed to evaluate the significance of the observed associations. However, iEAT is designed for self-supervised models and is limited to a fixed set of 15 binary target-attribute pairs.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Iv. Methodology",
      "text": "A. Evaluating Biases in the Feature Space of FER Models Our objective is to evaluate demographic biases in FER models under the setting where demographic labels are unavailable in the test set. Unlike prior FER studies that use pseudo-demographic labels  [35] ,  [36] ,  [39] ,  [43] , we propose a method, as shown in Fig.  2 , that uses a \"probe\" dataset to evaluate biases in the feature space of FER models. A \"probe\" dataset is a collection of diverse images that represent the visual concepts of interest. Since we aim to evaluate biases in FER models toward specific facial appearances associated with visually perceived demographic groups (e.g., age, gender, and race), we use existing public facial attribute datasets as probe datasets in this paper, as these datasets are specifically designed to cover a broad range of demographic-related facial Previous Bias Evaluation Pipeline in FER Our Proposed Framework FER Model ❄ Fig.  2 . Illustration of the previous bias evaluation pipeline in FER (green) and our proposed framework (orange), which evaluates biases in the feature space. A statistical module is also introduced, applicable to both the previous pipeline and our framework, to ensure the statistical significance of observed performance disparities and differential associations. The FER model remains frozen throughout the evaluation.\n\nfeatures. Alternatively, one could also construct this probe dataset from web search engines.\n\nThis approach is further supported by observations from our pilot study, which show that FER models can effectively encode unseen images from different attribute groups across gender, age, and race in the feature space (see Section V-C for details). Once we have a probe dataset and a test set with facial expression labels, we can evaluate bias in FER models by measuring differential associations in the feature space. Given a facial expression e, we feed all images showing facial expression e in the test set into the FER model and extract their embeddings, denoted as Z e . Similarly, for an attribute group s j of a sensitive attribute S, we feed images labeled as s j in the probe dataset through the FER model, obtaining a set of embeddings for s j , denoted as Z sj . By reformulating the Eq. (  2 ) & (3), we compute the association between facial expression e and s j as:\n\nwhere z e and z sj represent each embedding in their respective sets and cos(•) is the cosine similarity. For each facial expression e, the differential association (DiA) between two attribute groups s j and s j ′ is defined as:\n\nDiA e (j,j ′ ) = A(e, s j ) -A(e, s j ′ ).\n\n(\n\nThe intuition is that if the embeddings of the facial expression e are more strongly associated with s j in the feature space of the FER model being evaluated, then the model is more likely to prefer s j when predicting e. Since any two random sets of embeddings will exhibit some level of differ-ential association, a statistical module is needed to ensure the statistical significance of the observed differential association.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "B. Evaluating Biases In Fer With A Statistical Module",
      "text": "Both observed disparities in performance and differential associations require a statistical module to determine whether they are greater than what would occur by chance. Similar to differential association, for each facial expression e, we define the disparity in performance (DiP) under a metric M (•) between two attribute groups s j and s j ′ as:\n\nFor each facial expression e and a sensitive attribute S = {s 1 , s 2 , . . . , s n } in the test set, the objective is to identify the attribute groups of S on which the model underperforms and to quantify the extent of this underperformance in a statistically significant manner. To achieve this, for a given facial expression e, we first select the attribute group s e max with the highest performance:\n\nSimilarly, in terms of differential association, the attribute group s e max is the one most strongly associated with facial expression e: s e max = arg max sj ∈{s1,s2,...,sn} A e, s j .\n\nFor each facial expression e, this results in n -1 remaining attribute groups, which either underperform or show a weaker association compared to s e max . For each group s k among these n-1 remaining groups, we set s e max as the reference group and compute disparities relative to it. The disparity in performance for each s k is denoted as DiP e (max,k) while the differential association is denoted as DiA e (max,k) , where k ∈ [1, n -1]. This strategy of comparing each group pairwise to a reference group aligns with the notion of fairness proposed by Feldman et al.  [63] , but with a differential formulation. While it does not capture zero-sum effects between groups, it reflects the pairwise concept of current legal perspectives on discrimination and is well-suited to sensitive attributes that are either binary or involve multiple groups. Furthermore, this approach ensures that both DiA e (max,k) and DiP e (max,k) fall within the range [0, 1] with fixed directions. As a result, one-sided p-values are sufficient for statistical testing.\n\nTo establish a statistical module that applies to both DiA e (max,k) and DiP e (max,k) , we first denote V e k as an observed value that represents either DiA e (max,k) or DiP e (max,k) . The goal is to check whether V e k could have arisen by chance. To achieve this, we first formulate a null hypothesis H e k , stating that the observed value V e k is purely due to randomness. We then conduct a permutation test for V e k under H e j . Specifically, we randomly permute the labels between the attribute groups s max and s k while keeping the group sizes unchanged. For each permutation, we obtain a permuted value V k under H e j , is computed as:\n\nwhere l(•) is the indicator function. If p e k < α for a chosen significance threshold α (Type I Error Rate), we reject H e k and conclude that V e k is statistically significant. If p e k ≥ α, we fail to reject H e k , considering V e k as a result of random fluctuation. We then denote V e k as the statistically validated value of V e k , where it is retained only if it is statistically significant; otherwise, it is set to zero.\n\nV. EXPERIMENTS",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "A. Datasets",
      "text": "The datasets used in our study include two FER datasets: RAF-DB and AffectNet, and two facial attribute datasets: FairFace and UTKFace. Among these, RAF-DB and UTKFace are used for the pilot study, while AffectNet, UTKFace, and FairFace are used in the main experiments. All of them are in-the-wild face datasets. We selected in-the-wild datasets because lab-controlled FER datasets often lack participant diversity, making them less suitable for this analysis due to their limited representation of real-world variability and demographic inclusiveness.\n\nRAF-DB contains in-the-wild face images annotated with facial expressions by 40 expert annotators. Our study focuses on seven basic expressions: neutral, happiness, surprise, sadness, anger, disgust, and fear, resulting in approximately 15,000 face images. AffectNet  [41]  is one of the largest FER datasets and is divided into two subsets: AffectNet-8, which includes eight annotated expressions, and AffectNet-7, which excludes \"contempt.\" In our experiments, we use AffectNet-7, which contains around 290,000 internet-sourced face images. UTKFace  [64]  is a facial attribute dataset consisting of 23,704 face images, representing diverse facial appearances across different visually perceived demographic groups. Each image is annotated with visually perceived age, gender, and racial group. FairFace  [65]  is a facial attribute dataset designed to reduce demographic bias. It contains 108,000 images and provides a balanced distribution of visually perceived gender, age, and racial groups.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "B. Data Pre-Processing",
      "text": "All face images were aligned and resized to 224×224. For AffectNet, we followed the data-splitting protocol proposed by Hu et al.  [42] , dividing the dataset into training, validation, and independent test sets. Age labels in FairFace and UTKFace were grouped into the same five categories: 0-3, 4-19, 20-39, 40-69, and 70+ years. In UTKFace, images labeled as \"others\" under racial labels were excluded. For FairFace, race labels were grouped into four categories: White, Black, Asian (East Asian), and Indian. To summarize, the attribute group categorizations used in this paper are as follows: race categories include White, Black, Asian (East Asian), and Indian; age groups are defined as 0 to 3, 4 to 19, 20 to 39, 40 to 69, and 70 years and above; and gender is categorized as Female and Male. Since both facial expression and facial attribute labels are subjective, we identified visually ambiguous samples that were likely misannotated across the datasets we used. These samples were excluded from our analysis. A list of the excluded images is provided in the supplementary material to support reproducibility. For more details on the data cleaning process, please refer to the Supplementary Material.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Pilot Study: Fer Models Are Good Attribute Encoders",
      "text": "FER models are naturally effective facial attribute encoders. This is because, although they are not explicitly trained to recognize sensitive attributes, these features are inherently present in face images and are therefore indirectly encoded during training. We conduct a small pilot study as a proof of concept. First, we train a FER model based on ResNet34 using the RAF-DB dataset with only facial expression labels. We then keep the trained model frozen and use it to encode images from the UTKFace dataset. As shown in Fig.  3 , the resulting image embeddings form distinct clusters corresponding to gender, race, and age. This supports that differential associations in our framework can be effectively computed in the feature space of the FER models being evaluated.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "D. Settings For The Main Experiments",
      "text": "In our main experiments, we use AffectNet to ensure sufficient sample size and statistically significant results, as it is nearly 20 times larger than RAF-DB. Specifically, we train FER models on the AffectNet training and validation sets and evaluate bias on the test set. Furthermore, we implement two versions of our proposed framework using UTKFace and FairFace as probe datasets, respectively, to examine the sensitivity of our approach to probe datasets of different scales (FairFace is nearly four times larger than UTKFace).\n\nFor a fair comparison with existing fair FER studies  [35] ,  [36] ,  [39] ,  [43]  that rely on pseudo-demographic labels, we implement them by training facial attribute classifiers using the same datasets. Please refer to the Supplementary Material for implementation details. These classifiers are then used to generate pseudo-demographic labels for the AffectNet test set. The generated pseudo-labels are used to conduct bias evaluation. This setup results in four bias evaluation methods for comparison in our experiments: 1. Bias evaluation using pseudo-demographic labels generated by an attribute classifier trained on UTKFace, denoted as UTKFace (pseudo).\n\n2. Bias evaluation using pseudo-demographic labels generated by an attribute classifier trained on FairFace, denoted as FairFace (pseudo).\n\n3. Bias evaluation using UTKFace as a probe dataset to compute differential associations, denoted as UTKFace (ours).\n\n4. Bias evaluation using FairFace as a probe dataset to compute differential associations, denoted as FairFace (ours).\n\n1) Implementation Details: The FER models we train and evaluate include ResNet18, ResNet34, ResNet101, and ResNet152, which represent convolutional neural networks (CNNs)  [66] , and Vision Transformer Base, Swin Transformer Tiny, Swin Transformer Small, and Swin Transformer Base, which represent Transformer-based architectures  [67] ,  [68] . We choose these architectures because they are among the most commonly used backbones for feature extraction in FER. All models are optimized using stochastic gradient descent (SGD) with the same hyperparameters. Feature embeddings are extracted from the penultimate layer, with a dimension of 512. In this paper, we present detailed results using the Swin Transformer (Base)  [67]  as a representative architecture, chosen for its strong performance in FER and related areas  [42] ,  [69] ,  [70] . Please refer to the Supplementary Material for the training details and the results of other models.\n\nWe use the true positive rate (TPR) as the performance metric M (•), which aligns with the concept of Equal Opportunity, a fairness metric widely adopted in prior studies on fairness in FER  [27] -  [31] . In this context, DiP e (max,k)\n\nrepresents the difference in Equal Opportunity, which we refer to as DEO e (max,k) for the remainder of the paper. Following prior research in iEAT  [45] -  [47] ,  [71] , we generate 10,000 permutations for each test (B = 10000) and set the significance threshold in the statistical module to α = 0.05. All implementations are carried out using PyTorch and two NVIDIA A100 GPUs.\n\n2) Evaluating the Bias Evaluation Methods: To evaluate the effectiveness of the four aforementioned bias evaluation methods, we use the bias evaluation results based on humanannotated demographic labels provided by Hu et al.  [42]  on the AffectNet test set to serve as the ground truth bias evaluation results. If a method M, which operates under the setting where demographic labels are unavailable in the test set, produces results closely aligned with those based on human annotations, it indicates that M is more effective. We pass the bias evaluation results from the four bias evaluation methods, along with the ground truth, through our proposed statistical module to ensure that all the reported biases are not the result of random fluctuations.\n\nTo quantify the closeness, for each facial expression e and a sensitive attribute S with n groups, let s e,M max be the reference group identified by M as having the highest performance or strongest association with e, and let s e,GT max be the corresponding group in the ground truth. If s e,M max = s e,GT max , we compute the average L 1 distance between the statistically validated values V e,M k produced by M and the corresponding ground truth values V e,GT k for the remaining n-1 groups, as follows:\n\nwhere     ground truth, which means M fails entirely for this expression.\n\nIn such cases, we do not compute the L e,M 1 and mark the result as NaN.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "E. Experimental Results",
      "text": "In this section, we present the bias evaluation results of the four bias evaluation methods and the ground truth across gender, race, and age. We report V e,GT k and L e,M 1 as percentages since they both fall within the range [0, 1]. We note that a value of V e,GT k = 0 does not indicate that the observed value is zero. Instead, it means that the observed value is not statistically significant, as we set it to zero when p ≥ 0.05 to simplify the presentation of our experimental results. Similarly, L e,M 1 = 0 does not imply that the results of M perfectly match the ground truth. It simply indicates that M also found the result to be not statistically significant. For detailed values of V e,M k and the corresponding p-values across facial expressions and sensitive attributes for each method, please refer to the Supplementary Material.\n\n1) Gender: Our analysis of gender is shown in Table  I  and Table  II . As shown in Table  I , the ground truth results show no statistically significant gender bias for the expressions of surprise and fear. However, the FER model performs better on female-looking faces for disgust, happiness, sadness, and neutral. In contrast, for anger, it shows a preference for malelooking faces. As shown in Table  II , all four evaluation meth-ods correctly capture these trends as there are no NaN values. When comparing the exact values, for disgust, happiness, anger, and neutral, our proposed methods using UTKFace and FairFace (ours) are closer to the ground truth than UTKFace and FairFace (pseudo), as measured by the L 1 distance.\n\nOur findings on gender bias in happiness resonate with previous research showing that females are often perceived as generally happier than males  [34] ,  [35] ,  [39] . This is further supported by studies indicating that females typically exhibit higher AU12 (smile muscle) intensity than males  [38] , and psychological investigations using the IAT  [56] , which suggest that cosmetics may also play a role. Similarly, for anger, earlier studies also report that FER models tend to favor male-looking faces  [34] ,  [39] .\n\n2) Race: Our analysis of racial bias is summarized in Table  III  and Table  IV . As shown in Table  III , the ground truth results indicate no statistically significant bias across racial groups for the expression of happiness. For surprise, the FER model predicts White faces more accurately than Black, Asian, and Indian faces. For fear, the model performs best on Indian faces, while prediction accuracy is lower for other groups. In the case of disgust, there is no statistically significant bias among White, Black, and Asian groups, all of which outperform the Indian group. For sadness, Asian faces are predicted more accurately than all other groups. For anger, there is no statistically significant bias for White, Indian, and   Asian groups, but the model underperforms on Black faces. For the neutral expression, no statistically significant bias is found for Black and Asian groups, whereas White and Indian groups are predicted less accurately.\n\nAs shown in Table  IV , UTKFace (pseudo) fails to identify the reference group for surprise, fear, and sadness, resulting in NaN values for these expressions. Similarly, FairFace (pseudo) fails to identify the reference group for fear and sadness, also producing NaN values. In contrast, both UTKFace and FairFace (ours) correctly identify the reference group across all expressions. When comparing exact values, UTKFace (ours) is closest to the ground truth for surprise and sadness, while FairFace (ours) aligns most closely with the ground truth for fear, disgust, sadness, anger, and neutral. Our finding for the anger expression echoes psychological research showing that people are more likely to associate Black faces with hostility  [51]  or anger  [58] ,  [59] . This pattern is also observed by Rhue et al.  [72] , who reported that modern FER APIs more frequently classified Black NBA players as angry compared to their White counterparts.\n\n3) Age: Our analysis of age bias is presented in Table  V  and Table  VI . As shown in Table  V , for the expression surprise, the ground truth indicates no statistically significant bias for the 0-3 and 70+ age groups, while other groups are predicted less accurately. For fear, the model performs best on the 0-3 age group, with all other groups showing lower accuracy. For disgust, the model predicts the 4-39 age range more accurately than other groups. For happiness, the model underperforms only for the 40-69 age group, with no statistically significant bias for the remaining groups. For sadness, the model favors the 0-3 age group, with all others performing worse by comparison. For anger, the model favors the 0-19 age group, and prediction accuracy decreases for older groups. The model shows the highest accuracy for the neutral expression for the 4-39 age range, while other groups are predicted less accurately.\n\nAs shown in Table VI, UTKFace (pseudo) fails to identify the reference group for surprise, fear, disgust, sadness, and anger, resulting in NaN values for these expressions. Similarly, FairFace (pseudo) fails to identify the reference group for surprise, sadness, and anger, also producing NaN values. In contrast, both UTKFace and FairFace (ours) correctly identify the reference group across all expressions. When comparing the exact values, UTKFace (ours) is closest to the ground truth for surprise, fear, and happiness, while FairFace (ours) aligns best with the ground truth for disgust, sadness, anger, and neutral. Our finding for the fear expression aligns with psychological research using the Implicit Association Test (IAT)  [55]  to measure implicit biases in human perception of facial expressions, which suggests an implicit association between fearful expressions and infant faces.\n\n4) Summary: This section highlights key takeaways and observations in our experiments from Tables I to VI, which are also consistent across all evaluated network architectures.\n\nBiases are more pronounced in age and race than in gender. Based on the ground truth results shown in Table  I  Methods based on pseudo-demographic labels perform worse on age and race. As shown in Table  II , Table  IV , and Table  VI , methods based on pseudo-labels tend to be more error-prone and less reliable for race and age compared to gender, often resulting in a higher number of NaN values and larger L 1 distances from the ground truth. We attribute this to the high accuracy of facial attribute classifiers in distinguishing male and female faces, which makes genderbased pseudo-labels more reliable. However, the performance of these classifiers for race and age is substantially lower compared to gender. The reduced accuracy in predicting racial and age groups makes bias evaluation based on unreliable pseudo-labels and may skew the bias evaluation results. In contrast, our proposed method performs consistently well across gender, race, and age. We attribute this to the fact that our method evaluates bias directly in the feature space of the FER model being evaluated. This space inherently reflects how the model encodes and separates input features, allowing us to capture biased behavior more effectively.\n\nMethods based on pseudo-demographic labels are more sensitive to the facial attribute datasets. We also observe that the performance of these methods varies depending on the dataset used to train the facial attribute classifier. For example, FairFace, which contains nearly four times more samples than UTKFace, results in better bias evaluation performance in these methods. It produces fewer NaN values and lower L 1 scores because the pseudo-demographic labels are more accurate when trained on a larger dataset. In comparison, our method yields similarly strong bias evaluation results using either UTKFace or FairFace as the probe dataset. This indicates that a smaller dataset like UTKFace is already sufficient for producing reliable bias evaluations for FER models in our framework. Unlike methods based on pseudo labels, our approach depends less on large-scale data to be effective. Similar patterns of biases are observed across studies. As discussed in previous sections, several of our findings are consistent with results reported in prior studies on demographic bias in FER models, as well as in psychological research. Although our experiments are conducted under different settings, these similarities may indicate that such biases are widespread in FER datasets, frequently emerge during the annotation process, or are further amplified by deep learning models. This underscores the importance of mitigating these biases with greater care.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "F. A Sensitivity Analysis Of Threshold Α",
      "text": "As shown in Fig.  4 , we conducted a sensitivity analysis on the threshold α used in our statistical module. For each bias evaluation method M and the ground truth, we compute the average of V e,M k and V e,GT k across the n-1 underperforming or less associated groups, the seven facial expressions, and the three sensitive attributes. This yields a single average value, defined as:\n\nwhere each sensitive attribute S ∈ S = {gender, race, age} and V e,S k represents either V e,M k or V e,GT k under the sensitive attribute S. NaN values are excluded from the calculations. This average reflects the overall level of bias across underperforming attribute groups, sensitive attributes, and facial expressions. We analyze how this average changes as the threshold α varies from 0.01 to 0.1 in increments of 0.01.\n\nIntuitively, as α decreases toward 0.01, the criterion for statistical significance becomes more stringent. Consequently, more observed V e,S k values become statistically insignificant and are set to zero, leading to a decrease in AvgBias, as shown in Fig.  4 . Conversely, as α increases toward 0.1, the criterion becomes less stringent, allowing more values to be considered statistically significant and increasing AvgBias. methods based on pseudo-labels show larger deviations from the ground truth compared to our method.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "G. Exploring More Network Architectures",
      "text": "As shown in Fig.  5 , we evaluate the effectiveness of our bias evaluation methods across eight network architectures based on the AvgBias. All models were trained using the same set of hyperparameters to ensure that any differences in AvgBias are due solely to architectural differences. The network architectures are ordered by model size. As shown in Fig.  5 , our proposed methods remain highly consistent with the ground truth across both CNN and Transformer families. Our results also indicate that Transformer-based models exhibit higher FER bias compared to their CNN-based counterparts. A similar observation is noted by Mandal et al.  [73] , who reported that Vision Transformers tend to amplify gender bias. We believe this may be due to their global attention mechanism and the larger receptive fields introduced by multi-headed selfattention, which can lead the models to capture broader visual attributes such as race, age, and gender, rather than focusing primarily on localized facial action units or expressions. In addition, we observe that bias tends to decrease as model size increases for both the ResNet and Transformer families. A similar trend was reported by Brinkmann et al.  [46] , who suggested that larger models are better at capturing the core semantic content of images, which may reduce reliance on sensitive attributes.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Vi. Discussion",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "A. On The Use Of Visually Perceived Demographic Labels",
      "text": "Image-level demographic annotations are essential for advancing fairness research in computer vision. However, in large-scale datasets such as the \"people\" subtree in ImageNet  [2] , these annotations are often based on perceived attributes rather than self-identified demographics. It is important to note that these two types of information are fundamentally different. While self-reported data is highly valuable, collecting it at scale is extremely difficult, as it would require participation from a large number of volunteers. Consequently, much of the fairness research in computer vision  [28] ,  [29] ,  [39] ,  [43]  relies on annotations derived from the perceptions of external annotators. Evaluating fairness based on perceived attributes is an important step for computer vision systems that rely solely on image-based inputs. For FER models, consistent performance across facial appearances associated with perceived gender, race, and age is essential to prevent discrimination against demographic-related facial features in downstream applications, thereby supporting the responsible development of this technology.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Vii. Conclusion",
      "text": "In this paper, we propose a feature-level bias evaluation framework to evaluate demographic biases in FER models in scenarios where demographic labels are not available in the test set. Extensive experiments demonstrate that our method more effectively evaluates demographic biases compared to existing approaches that rely on pseudo-demographic labels. To ensure the statistical significance of the evaluation results, we introduce a plug-and-play statistical module that can be integrated into both existing evaluation pipelines and our proposed method that analyzes the feature space. Furthermore, a comprehensive bias analysis based on the statistical module is then conducted across three sensitive attributes (age, gender, and race), seven facial expressions, and multiple network architectures on a large-scale dataset, revealing the prominent demographic biases in FER and providing insights on selecting a fairer network architecture.\n\nAn additional strength of our method is its ability to evaluate biases embedded in the feature space of FER models. Identifying such biases is important, as they can extend beyond FER. For example, biases in the feature space may propagate to downstream tasks during fine-tuning  [74]  or through feature fusion in multi-modal emotion recognition systems  [75] . In future work, we plan to investigate how these feature-level biases transfer to other tasks through fine-tuning and feature fusion, and to develop mitigation strategies that prevent this transfer of bias.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Viii. Ethical Statement",
      "text": "Demographic categories are socially constructed, contextdependent, and continue to be the focus of ethical and academic debate regarding their appropriateness and feasibility in computational systems  [76] -  [78] . In this work, demographic categories are used only as descriptors of facial appearance, rather than as indicators of biological sex, self-identified gender, race, or age of the individual. These labels are used solely to examine how FER models respond to different facial characteristics. This work does not support or endorse the use of such annotations to determine or assign the actual gender or race of any person. There is a clear ethical responsibility to treat demographic information with care, and this study is committed to advancing inclusive and respectful practices in the community. study. This includes eight FER models with different architectures that we have trained and evaluated on the Affect-Net dataset. The architectures include ResNet-18, ResNet-34, ResNet-101, ResNet-152, Swin Transformer models (Tiny, Small, and Base), and Vision Transformer Base. All models are trained using the same set of hyperparameters. We also include six facial attribute classifiers: three trained on the FairFace dataset for gender, age, and race, and three trained on the UTKFace dataset for the same attributes. All attribute classifiers are based on ResNet-34 to maintain consistency with prior fairness studies that use these models to generate pseudo-demographic labels. These classifiers are included solely for the purpose of reproducing such previous studies. The hyperparameters used for training are listed in Table  I . The performance of the FER models is reported in Table  II , and the performance of the facial attribute classifiers on the AffectNet test set is shown in Table  III . The code is also make available at https://github.com/Supltz/FairFER.    V  and VI . These tables serve as a supplement to the results presented in the main paper across facial expressions and sensitive attributes for each method in our main experiments based on Swin-B.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Iv. Additional Network Architectures",
      "text": "All detailed results for the other network architectures are included in the JSON files located in the ./other networks/ directory within suppl.7z. Please refer to readme.txt for more information.\n\nAlgorithm 1 Centroids-based filtering 1: Input: Set of data points X = {x1, x2, . . . , xn}, corresponding labels L = {l1, l2, . . . , ln}, scale factor s 2: Output: Filtered data points X f iltered 3: function FILTERDATA(X, L, s) 4:\n\nfor each unique label l in L do 6:",
      "page_start": 13,
      "page_end": 13
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Race (left column) and age (right column) distribution for each facial",
      "page": 1
    },
    {
      "caption": "Figure 1: In contrast, we provide",
      "page": 2
    },
    {
      "caption": "Figure 2: The evaluation is",
      "page": 3
    },
    {
      "caption": "Figure 2: Illustration of the previous bias evaluation pipeline in FER (green) and our proposed framework (orange), which evaluates biases in the feature",
      "page": 4
    },
    {
      "caption": "Figure 3: , the resulting image",
      "page": 5
    },
    {
      "caption": "Figure 3: t-SNE visualization from our pilot study: A FER model trained solely on expression labels effectively encodes unseen face images into well-defined",
      "page": 6
    },
    {
      "caption": "Figure 4: Sensitivity analysis of the threshold α across four bias evaluation",
      "page": 9
    },
    {
      "caption": "Figure 4: , we conducted a sensitivity analysis on",
      "page": 9
    },
    {
      "caption": "Figure 4: Conversely, as α increases toward 0.1, the criterion",
      "page": 9
    },
    {
      "caption": "Figure 5: Experimental results of our proposed methods across multiple network",
      "page": 10
    },
    {
      "caption": "Figure 1: For reproducibility, all discarded samples are located",
      "page": 13
    },
    {
      "caption": "Figure 1: Selected potentially misannotated samples in the four datasets.",
      "page": 14
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Expression\nMethod": "",
          "Surprise\nFear\nDisgust\nHappiness\nSadness\nAnger\nNeutral": "Le,\n(%)\nM1"
        },
        {
          "Expression\nMethod": "FairFace (ours)",
          "Surprise\nFear\nDisgust\nHappiness\nSadness\nAnger\nNeutral": "0"
        },
        {
          "Expression\nMethod": "UTKFace (ours)",
          "Surprise\nFear\nDisgust\nHappiness\nSadness\nAnger\nNeutral": "0"
        },
        {
          "Expression\nMethod": "FairFace (pseudo)",
          "Surprise\nFear\nDisgust\nHappiness\nSadness\nAnger\nNeutral": "0"
        },
        {
          "Expression\nMethod": "UTKFace (pseudo)",
          "Surprise\nFear\nDisgust\nHappiness\nSadness\nAnger\nNeutral": "0"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Expression\nMethod": "",
          "Surprise\nFear\nDisgust\nHappiness\nSadness\nAnger\nNeutral": "Le,\n(%)\nM1\n↓"
        },
        {
          "Expression\nMethod": "FairFace (ours)",
          "Surprise\nFear\nDisgust\nHappiness\nSadness\nAnger\nNeutral": "0.89"
        },
        {
          "Expression\nMethod": "UTKFace (ours)",
          "Surprise\nFear\nDisgust\nHappiness\nSadness\nAnger\nNeutral": "0.71"
        },
        {
          "Expression\nMethod": "FairFace (pseudo)",
          "Surprise\nFear\nDisgust\nHappiness\nSadness\nAnger\nNeutral": "4.95"
        },
        {
          "Expression\nMethod": "UTKFace (pseudo)",
          "Surprise\nFear\nDisgust\nHappiness\nSadness\nAnger\nNeutral": "NaN"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Expression\nMethod": "",
          "Surprise\nFear\nDisgust\nHappiness\nSadness\nAnger\nNeutral": "Le,\n(%)\nM1\n↓"
        },
        {
          "Expression\nMethod": "FairFace (ours)",
          "Surprise\nFear\nDisgust\nHappiness\nSadness\nAnger\nNeutral": "0.44"
        },
        {
          "Expression\nMethod": "UTKFace (ours)",
          "Surprise\nFear\nDisgust\nHappiness\nSadness\nAnger\nNeutral": "0.31"
        },
        {
          "Expression\nMethod": "FairFace (pseudo)",
          "Surprise\nFear\nDisgust\nHappiness\nSadness\nAnger\nNeutral": "NaN"
        },
        {
          "Expression\nMethod": "UTKFace (pseudo)",
          "Surprise\nFear\nDisgust\nHappiness\nSadness\nAnger\nNeutral": "NaN"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Expressions": "",
          "FairFace (pseudo) UTFFace (ours)\nMethods Ground Truth UTKFace (pseudo)": "DEOe\n(%)\n/ p-value\n(max,k)",
          "FairFace (ours)": "DiAe\n(%)\n/ p-value\n(max,k)"
        },
        {
          "Expressions": "Surprise",
          "FairFace (pseudo) UTFFace (ours)\nMethods Ground Truth UTKFace (pseudo)": "M/F",
          "FairFace (ours)": "M/F"
        },
        {
          "Expressions": "",
          "FairFace (pseudo) UTFFace (ours)\nMethods Ground Truth UTKFace (pseudo)": "0/0.1234",
          "FairFace (ours)": "0/0.1762"
        },
        {
          "Expressions": "Fear",
          "FairFace (pseudo) UTFFace (ours)\nMethods Ground Truth UTKFace (pseudo)": "M/F",
          "FairFace (ours)": "M/F"
        },
        {
          "Expressions": "",
          "FairFace (pseudo) UTFFace (ours)\nMethods Ground Truth UTKFace (pseudo)": "0/0.1453",
          "FairFace (ours)": "0/0.1245"
        },
        {
          "Expressions": "Disgust",
          "FairFace (pseudo) UTFFace (ours)\nMethods Ground Truth UTKFace (pseudo)": "Male",
          "FairFace (ours)": "Male"
        },
        {
          "Expressions": "",
          "FairFace (pseudo) UTFFace (ours)\nMethods Ground Truth UTKFace (pseudo)": "9.01/0.0234",
          "FairFace (ours)": "8.59/0.0412"
        },
        {
          "Expressions": "Happiness",
          "FairFace (pseudo) UTFFace (ours)\nMethods Ground Truth UTKFace (pseudo)": "Male",
          "FairFace (ours)": "Male\n5.07/0.0189"
        },
        {
          "Expressions": "",
          "FairFace (pseudo) UTFFace (ours)\nMethods Ground Truth UTKFace (pseudo)": "4.94/0.0167",
          "FairFace (ours)": ""
        },
        {
          "Expressions": "Sadness",
          "FairFace (pseudo) UTFFace (ours)\nMethods Ground Truth UTKFace (pseudo)": "Male",
          "FairFace (ours)": "Male"
        },
        {
          "Expressions": "",
          "FairFace (pseudo) UTFFace (ours)\nMethods Ground Truth UTKFace (pseudo)": "3.22/0.0312",
          "FairFace (ours)": "3.54/0.0467"
        },
        {
          "Expressions": "Anger",
          "FairFace (pseudo) UTFFace (ours)\nMethods Ground Truth UTKFace (pseudo)": "Female",
          "FairFace (ours)": "Female"
        },
        {
          "Expressions": "",
          "FairFace (pseudo) UTFFace (ours)\nMethods Ground Truth UTKFace (pseudo)": "9.30/0.0356",
          "FairFace (ours)": "9.63/0.0267"
        },
        {
          "Expressions": "Neutral",
          "FairFace (pseudo) UTFFace (ours)\nMethods Ground Truth UTKFace (pseudo)": "Male",
          "FairFace (ours)": "Male"
        },
        {
          "Expressions": "",
          "FairFace (pseudo) UTFFace (ours)\nMethods Ground Truth UTKFace (pseudo)": "2.41/0.0156",
          "FairFace (ours)": "2.29/0.0345"
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Methods\nExpressions": "",
          "Ground Truth\nUTKFace (pseudo)\nFairFace (pseudo)": "DEOe\n(%)\n(max,k)\n(p-value)",
          "UTKFace (ours)\nFairFace (ours)": "DiAe\n(%)\n(max,k)\n(p-value)"
        },
        {
          "Methods\nExpressions": "Surprise",
          "Ground Truth\nUTKFace (pseudo)\nFairFace (pseudo)": "B\nA\nI",
          "UTKFace (ours)\nFairFace (ours)": "B\nA\nI\n8.94\n11.63\n10.34"
        },
        {
          "Methods\nExpressions": "",
          "Ground Truth\nUTKFace (pseudo)\nFairFace (pseudo)": "8.23\n10.92\n9.63",
          "UTKFace (ours)\nFairFace (ours)": ""
        },
        {
          "Methods\nExpressions": "Fear",
          "Ground Truth\nUTKFace (pseudo)\nFairFace (pseudo)": "W\nB\nA",
          "UTKFace (ours)\nFairFace (ours)": "W\nB\nA"
        },
        {
          "Methods\nExpressions": "",
          "Ground Truth\nUTKFace (pseudo)\nFairFace (pseudo)": "7.24\n5.62\n16.63",
          "UTKFace (ours)\nFairFace (ours)": "8.13\n6.51\n17.52"
        },
        {
          "Methods\nExpressions": "Disgust",
          "Ground Truth\nUTKFace (pseudo)\nFairFace (pseudo)": "W/B\nW/A\nI",
          "UTKFace (ours)\nFairFace (ours)": "W/B\nW/A\nI"
        },
        {
          "Methods\nExpressions": "",
          "Ground Truth\nUTKFace (pseudo)\nFairFace (pseudo)": "0\n0\n5.72",
          "UTKFace (ours)\nFairFace (ours)": "0\n0\n6.12"
        },
        {
          "Methods\nExpressions": "Happiness",
          "Ground Truth\nUTKFace (pseudo)\nFairFace (pseudo)": "W/B\nW/A\nW/I",
          "UTKFace (ours)\nFairFace (ours)": "W/B\nW/A\nW/I"
        },
        {
          "Methods\nExpressions": "",
          "Ground Truth\nUTKFace (pseudo)\nFairFace (pseudo)": "0\n0\n0",
          "UTKFace (ours)\nFairFace (ours)": "0\n0\n0"
        },
        {
          "Methods\nExpressions": "Sadness",
          "Ground Truth\nUTKFace (pseudo)\nFairFace (pseudo)": "W\nB\nI",
          "UTKFace (ours)\nFairFace (ours)": "W\nB\nI\n8.79\n11.49\n12.38"
        },
        {
          "Methods\nExpressions": "",
          "Ground Truth\nUTKFace (pseudo)\nFairFace (pseudo)": "8.23\n10.93\n11.82",
          "UTKFace (ours)\nFairFace (ours)": ""
        },
        {
          "Methods\nExpressions": "Anger",
          "Ground Truth\nUTKFace (pseudo)\nFairFace (pseudo)": "B\nW/A\nW/I",
          "UTKFace (ours)\nFairFace (ours)": "B\nW/A\nW/I"
        },
        {
          "Methods\nExpressions": "",
          "Ground Truth\nUTKFace (pseudo)\nFairFace (pseudo)": "7.81\n0\n0",
          "UTKFace (ours)\nFairFace (ours)": "8.25\n0\n0"
        },
        {
          "Methods\nExpressions": "Neutral",
          "Ground Truth\nUTKFace (pseudo)\nFairFace (pseudo)": "W\nB/A\nI",
          "UTKFace (ours)\nFairFace (ours)": "W\nB/A\nI"
        },
        {
          "Methods\nExpressions": "",
          "Ground Truth\nUTKFace (pseudo)\nFairFace (pseudo)": "4.23\n0\n5.82",
          "UTKFace (ours)\nFairFace (ours)": "4.70\n0\n6.29"
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Methods\nExpressions": "",
          "Ground Truth\nUTKFace (pseudo)\nFairFace (pseudo)": "DEOe\n(max,k) (%)\n(p-value)",
          "UTFFace (ours)\nFairFace (ours)": "DiAe\n(max,k) (%)\n(p-value)"
        },
        {
          "Methods\nExpressions": "Surprise",
          "Ground Truth\nUTKFace (pseudo)\nFairFace (pseudo)": "A1/A5\nA2\nA3\nA4",
          "UTFFace (ours)\nFairFace (ours)": "A1/A5\nA2\nA3\nA4\n0\n13.55\n15.61\n14.54"
        },
        {
          "Methods\nExpressions": "",
          "Ground Truth\nUTKFace (pseudo)\nFairFace (pseudo)": "0\n13.24\n15.31\n14.23",
          "UTFFace (ours)\nFairFace (ours)": ""
        },
        {
          "Methods\nExpressions": "Fear",
          "Ground Truth\nUTKFace (pseudo)\nFairFace (pseudo)": "A2\nA3\nA4\nA5",
          "UTFFace (ours)\nFairFace (ours)": "A2\nA3\nA4\nA5\n13.69\n14.48\n14.77\n25.66"
        },
        {
          "Methods\nExpressions": "",
          "Ground Truth\nUTKFace (pseudo)\nFairFace (pseudo)": "13.34\n14.13\n14.42\n25.31",
          "UTFFace (ours)\nFairFace (ours)": ""
        },
        {
          "Methods\nExpressions": "Disgust",
          "Ground Truth\nUTKFace (pseudo)\nFairFace (pseudo)": "A1\nA2/A3\nA4\nA5",
          "UTFFace (ours)\nFairFace (ours)": "A1\nA2/A3\nA4\nA5"
        },
        {
          "Methods\nExpressions": "",
          "Ground Truth\nUTKFace (pseudo)\nFairFace (pseudo)": "14.32\n0\n17.14\n34.23",
          "UTFFace (ours)\nFairFace (ours)": "14.90\n0\n17.72\n34.81"
        },
        {
          "Methods\nExpressions": "Happiness",
          "Ground Truth\nUTKFace (pseudo)\nFairFace (pseudo)": "A1/A2 A2/A3\nA4\nA2/A5",
          "UTFFace (ours)\nFairFace (ours)": "A1/A2 A2/A3\nA4\nA2/A5\n0\n0\n3.44\n0"
        },
        {
          "Methods\nExpressions": "",
          "Ground Truth\nUTKFace (pseudo)\nFairFace (pseudo)": "0\n0\n3.14\n0",
          "UTFFace (ours)\nFairFace (ours)": ""
        },
        {
          "Methods\nExpressions": "Sadness",
          "Ground Truth\nUTKFace (pseudo)\nFairFace (pseudo)": "A2\nA3\nA4\nA5",
          "UTFFace (ours)\nFairFace (ours)": "A2\nA3\nA4\nA5"
        },
        {
          "Methods\nExpressions": "",
          "Ground Truth\nUTKFace (pseudo)\nFairFace (pseudo)": "17.23\n33.14\n31.42\n28.13",
          "UTFFace (ours)\nFairFace (ours)": "17.75\n33.66\n31.94\n28.65"
        },
        {
          "Methods\nExpressions": "Anger",
          "Ground Truth\nUTKFace (pseudo)\nFairFace (pseudo)": "A1/A2\nA3\nA4\nA5",
          "UTFFace (ours)\nFairFace (ours)": "A1/A2\nA3\nA4\nA5"
        },
        {
          "Methods\nExpressions": "",
          "Ground Truth\nUTKFace (pseudo)\nFairFace (pseudo)": "0\n21.31\n14.23\n30.14",
          "UTFFace (ours)\nFairFace (ours)": "0\n21.82\n14.74\n30.65"
        },
        {
          "Methods\nExpressions": "Neutral",
          "Ground Truth\nUTKFace (pseudo)\nFairFace (pseudo)": "A1\nA2/A3\nA4\nA5",
          "UTFFace (ours)\nFairFace (ours)": "A1\nA2/A3\nA4\nA5"
        },
        {
          "Methods\nExpressions": "",
          "Ground Truth\nUTKFace (pseudo)\nFairFace (pseudo)": "13.42\n0\n14.31\n25.24",
          "UTFFace (ours)\nFairFace (ours)": "13.88\n0\n14.77\n25.70"
        }
      ],
      "page": 17
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Imagenet: A large-scale hierarchical image database",
      "authors": [
        "J Deng",
        "W Dong",
        "R Socher",
        "L.-J Li",
        "K Li",
        "L Fei-Fei"
      ],
      "year": "2009",
      "venue": "Imagenet: A large-scale hierarchical image database"
    },
    {
      "citation_id": "2",
      "title": "Towards fairer datasets: Filtering and balancing the distribution of the people subtree in the imagenet hierarchy",
      "authors": [
        "K Yang",
        "K Qinami",
        "L Fei-Fei",
        "J Deng",
        "O Russakovsky"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 conference on fairness, accountability, and transparency"
    },
    {
      "citation_id": "3",
      "title": "Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization",
      "authors": [
        "S Sagawa",
        "P Koh",
        "T Hashimoto",
        "P Liang"
      ],
      "year": "2020",
      "venue": "The International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "4",
      "title": "Directional bias amplification",
      "authors": [
        "A Wang",
        "O Russakovsky"
      ],
      "year": "2021",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "5",
      "title": "Domino: Discovering systematic errors with cross-modal embeddings",
      "authors": [
        "S Eyuboglu",
        "M Varma",
        "K Saab",
        "J.-B Delbrouck",
        "C Lee-Messer",
        "J Dunnmon",
        "J Zou",
        "C Ré"
      ],
      "venue": "The International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "6",
      "title": "No subclass left behind: Fine-grained robustness in coarse-grained classification problems",
      "authors": [
        "N Sohoni",
        "J Dunnmon",
        "G Angus",
        "A Gu",
        "C Ré"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "7",
      "title": "Trustworthy artificial intelligence",
      "authors": [
        "S Thiebes",
        "S Lins",
        "A Sunyaev"
      ],
      "year": "2021",
      "venue": "Electronic Markets"
    },
    {
      "citation_id": "8",
      "title": "The global landscape of ai ethics guidelines",
      "authors": [
        "A Jobin",
        "M Ienca",
        "E Vayena"
      ],
      "year": "2019",
      "venue": "Nature machine intelligence"
    },
    {
      "citation_id": "9",
      "title": "Explainable artificial intelligence (xai): Concepts, taxonomies, opportunities and challenges toward responsible ai",
      "authors": [
        "A Arrieta",
        "N Díaz-Rodríguez",
        "J Del",
        "A Ser",
        "S Bennetot",
        "A Tabik",
        "S Barbado",
        "S García",
        "D Gil-López",
        "R Molina",
        "Benjamins"
      ],
      "year": "2020",
      "venue": "Information fusion"
    },
    {
      "citation_id": "10",
      "title": "Reliable crowdsourcing and deep localitypreserving learning for expression recognition in the wild",
      "authors": [
        "S Li",
        "W Deng",
        "J Du"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "11",
      "title": "Anatomizing bias in facial analysis",
      "authors": [
        "R Singh",
        "P Majumdar",
        "S Mittal",
        "M Vatsa"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "12",
      "title": "Fairgrape: Fairness-aware gradient pruning method for face attribute classification",
      "authors": [
        "X Lin",
        "S Kim",
        "J Joo"
      ],
      "year": "2022",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "13",
      "title": "Fair contrastive learning for facial attribute classification",
      "authors": [
        "S Park",
        "J Lee",
        "P Lee",
        "S Hwang",
        "D Kim",
        "H Byun"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "14",
      "title": "Fair attribute classification through latent space de-biasing",
      "authors": [
        "V Ramaswamy",
        "S Kim",
        "O Russakovsky"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "15",
      "title": "Bias and fairness in face detection",
      "authors": [
        "H Menezes",
        "A Ferreira",
        "E Pereira",
        "H Gomes"
      ],
      "year": "2021",
      "venue": "2021 34th SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI)"
    },
    {
      "citation_id": "16",
      "title": "Enhancing fairness in face detection in computer vision systems by demographic bias mitigation",
      "authors": [
        "Y Yang",
        "A Gupta",
        "J Feng",
        "P Singhal",
        "V Yadav",
        "Y Wu",
        "P Natarajan",
        "V Hedau",
        "J Joo"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society"
    },
    {
      "citation_id": "17",
      "title": "Gender shades: Intersectional accuracy disparities in commercial gender classification",
      "authors": [
        "J Buolamwini",
        "T Gebru"
      ],
      "year": "2018",
      "venue": "Conference on fairness, accountability and transparency"
    },
    {
      "citation_id": "18",
      "title": "Mitigating bias in face recognition using skewness-aware reinforcement learning",
      "authors": [
        "M Wang",
        "W Deng"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "19",
      "title": "Racial faces in the wild: Reducing racial bias by information maximization adaptation network",
      "authors": [
        "M Wang",
        "W Deng",
        "J Hu",
        "X Tao",
        "Y Huang"
      ],
      "year": "2019",
      "venue": "Proceedings of the ieee/cvf international conference on computer vision"
    },
    {
      "citation_id": "20",
      "title": "Mitigating face recognition bias via group adaptive classifier",
      "authors": [
        "S Gong",
        "X Liu",
        "A Jain"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "21",
      "title": "Consistent instance false positive improves fairness in face recognition",
      "authors": [
        "X Xu",
        "Y Huang",
        "P Shen",
        "S Li",
        "J Li",
        "F Huang",
        "Y Li",
        "Z Cui"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "22",
      "title": "Farapy: An augmented reality feedback system for facial paralysis using action unit intensity estimation",
      "authors": [
        "G Dell'olio",
        "M Sra"
      ],
      "year": "2021",
      "venue": "The 34th Annual ACM Symposium on User Interface Software and Technology"
    },
    {
      "citation_id": "23",
      "title": "Partially occluded facial action recognition and interaction in virtual reality applications",
      "authors": [
        "U Ciftci",
        "X Zhang",
        "L Tin"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "24",
      "title": "Real time face detection and facial expression recognition: Development and applications to human computer interaction",
      "authors": [
        "M Bartlett",
        "G Littlewort",
        "I Fasel",
        "J Movellan"
      ],
      "year": "2003",
      "venue": "2003 Conference on computer vision and pattern recognition workshop"
    },
    {
      "citation_id": "25",
      "title": "Emotion recognition in human-computer interaction",
      "authors": [
        "R Cowie",
        "E Douglas-Cowie",
        "N Tsapatsoulis",
        "G Votsis",
        "S Kollias",
        "W Fellenz",
        "J Taylor"
      ],
      "year": "2001",
      "venue": "IEEE Signal processing magazine"
    },
    {
      "citation_id": "26",
      "title": "Automated detection of facial expressions during computer-assisted instruction in individuals on the autism spectrum",
      "authors": [
        "A Ahmed",
        "M Goodwin"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems"
    },
    {
      "citation_id": "27",
      "title": "Toward fair facial expression recognition with improved distribution alignment",
      "authors": [
        "M Kolahdouzi",
        "A Etemad"
      ],
      "year": "2023",
      "venue": "Proceedings of the 25th International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "28",
      "title": "Investigating bias and fairness in facial expression recognition",
      "authors": [
        "T Xu",
        "J White",
        "S Kalkan",
        "H Gunes"
      ],
      "year": "2020",
      "venue": "Computer Vision-ECCV 2020 Workshops: Glasgow, UK"
    },
    {
      "citation_id": "29",
      "title": "Counterfactual fairness for facial expression recognition",
      "authors": [
        "J Cheong",
        "S Kalkan",
        "H Gunes"
      ],
      "year": "2022",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "30",
      "title": "Domain-incremental continual learning for mitigating bias in facial expression and action unit recognition",
      "authors": [
        "N Churamani",
        "O Kara",
        "H Gunes"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "31",
      "title": "Using positive matching contrastive loss with facial action units to mitigate bias in facial expression recognition",
      "authors": [
        "V Suresh",
        "D Ong"
      ],
      "year": "2022",
      "venue": "2022 10th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "32",
      "title": "Facial emotion recognition analysis based on age-biased data",
      "authors": [
        "H Park",
        "Y Shin",
        "K Song",
        "C Yun",
        "D Jang"
      ],
      "year": "2022",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "33",
      "title": "Responsible ai: Gender bias assessment in emotion recognition",
      "authors": [
        "A Domnich",
        "G Anbarjafari"
      ],
      "year": "2021",
      "venue": "Responsible ai: Gender bias assessment in emotion recognition",
      "arxiv": "arXiv:2103.11436"
    },
    {
      "citation_id": "34",
      "title": "Facial expression recognition: Impact of gender on fairness and expressions",
      "authors": [
        "C Manresa-Yee",
        "S Guarinos",
        "J Buades Rubio"
      ],
      "year": "2022",
      "venue": "Proceedings of the XXII International Conference on Human Computer Interaction"
    },
    {
      "citation_id": "35",
      "title": "Gender stereotyping impact in facial expression recognition",
      "authors": [
        "I Dominguez-Catena",
        "D Paternain",
        "M Galar"
      ],
      "year": "2022",
      "venue": "Joint European Conference on Machine Learning and Knowledge Discovery in Databases"
    },
    {
      "citation_id": "36",
      "title": "Less can be more: representational vs. stereotypical gender bias in facial expression recognition",
      "authors": [
        "I Dominguez-Catena",
        "D Paternain",
        "A Jurio",
        "M Galar"
      ],
      "year": "2024",
      "venue": "Progress in Artificial Intelligence"
    },
    {
      "citation_id": "37",
      "title": "Ethical ai in facial expression analysis: Racial bias",
      "authors": [
        "A Sham",
        "K Aktas",
        "D Rizhinashvili",
        "D Kuklianov",
        "F Alisinanoglu",
        "I Ofodile",
        "C Ozcinar",
        "G Anbarjafari"
      ],
      "year": "2023",
      "venue": "Signal, Image and Video Processing"
    },
    {
      "citation_id": "38",
      "title": "Demographic effects on facial emotion expression: an interdisciplinary investigation of the facial action units of happiness",
      "authors": [
        "Y Fan",
        "J Lam",
        "V Li"
      ],
      "year": "2021",
      "venue": "Scientific reports"
    },
    {
      "citation_id": "39",
      "title": "Understanding and mitigating annotation bias in facial expression recognition",
      "authors": [
        "Y Chen",
        "J Joo"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "40",
      "title": "Causal structure learning of bias for fair affect recognition",
      "authors": [
        "J Cheong",
        "S Kalkan",
        "H Gunes"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision"
    },
    {
      "citation_id": "41",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "42",
      "title": "Rethinking affect analysis: A protocol for ensuring fairness and consistency",
      "authors": [
        "G Hu",
        "D Kollias",
        "E Papadopoulou",
        "P Tzouveli",
        "J Wei",
        "X Yang"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Biometrics, Behavior, and Identity Science"
    },
    {
      "citation_id": "43",
      "title": "Metrics for dataset demographic bias: A case study on facial expression recognition",
      "authors": [
        "I Dominguez-Catena",
        "D Paternain",
        "M Galar"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "44",
      "title": "Discovering fair representations in the data domain",
      "authors": [
        "N Quadrianto",
        "V Sharmanska",
        "O Thomas"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "45",
      "title": "A study on the distribution of social biases in self-supervised learning visual models",
      "authors": [
        "K Sirotkin",
        "P Carballeira",
        "M Escudero-Viñolo"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "46",
      "title": "A multidimensional analysis of social biases in vision transformers",
      "authors": [
        "J Brinkmann",
        "P Swoboda",
        "C Bartelt"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "47",
      "title": "Image representations learned with unsupervised pre-training contain human-like biases",
      "authors": [
        "R Steed",
        "A Caliskan"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency"
    },
    {
      "citation_id": "48",
      "title": "Fairness with overlapping groups; a probabilistic perspective",
      "authors": [
        "F Yang",
        "M Cisse",
        "S Koyejo"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "49",
      "title": "Facial age affects emotional expression decoding",
      "authors": [
        "M Fölster",
        "U Hess",
        "K Werheid"
      ],
      "year": "2014",
      "venue": "Frontiers in psychology"
    },
    {
      "citation_id": "50",
      "title": "Do we expect women to look happier than they are? a test of gender-dependent perceptual correction",
      "authors": [
        "J Steephen",
        "S Mehta",
        "R Bapi"
      ],
      "year": "2018",
      "venue": "Perception"
    },
    {
      "citation_id": "51",
      "title": "Ambiguity in social categorization: The role of prejudice and facial affect in race categorization",
      "authors": [
        "K Hugenberg",
        "G Bodenhausen"
      ],
      "year": "2004",
      "venue": "Psychological Science"
    },
    {
      "citation_id": "52",
      "title": "Female, white, 27? bias evaluation on data and algorithms for affect recognition in faces",
      "authors": [
        "J Pahl",
        "I Rieger",
        "A Möller",
        "T Wittenberg",
        "U Schmid"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency"
    },
    {
      "citation_id": "53",
      "title": "Measuring individual differences in implicit cognition: the implicit association test",
      "authors": [
        "A Greenwald",
        "D Mcghee",
        "J Schwartz"
      ],
      "year": "1998",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "54",
      "title": "Understanding and using the implicit association test: I. an improved scoring algorithm",
      "authors": [
        "A Greenwald",
        "B Nosek",
        "M Banaji"
      ],
      "year": "2003",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "55",
      "title": "Why do fearful facial expressions elicit behavioral approach? evidence from a combined approach-avoidance implicit association test",
      "authors": [
        "J Hammer",
        "A Marsh"
      ],
      "year": "2015",
      "venue": "Emotion"
    },
    {
      "citation_id": "56",
      "title": "Facial make-up elicits positive attitudes at the implicit level: Evidence from the implicit association test",
      "authors": [
        "J Richetin",
        "J.-C Croizet",
        "P Huguet"
      ],
      "year": "2004",
      "venue": "Current Research in Social Psychology"
    },
    {
      "citation_id": "57",
      "title": "Implicit racial attitudes influence perceived emotional intensity on other-race faces",
      "authors": [
        "Q Wang",
        "G Chen",
        "Z Wang",
        "C Hu",
        "X Hu",
        "G Fu"
      ],
      "year": "2014",
      "venue": "PLoS One"
    },
    {
      "citation_id": "58",
      "title": "Not always black and white: The effect of race and emotional expression on implicit attitudes",
      "authors": [
        "J Steele",
        "M George",
        "M Cease",
        "T Fabri",
        "J Schlosser"
      ],
      "year": "2018",
      "venue": "Social Cognition"
    },
    {
      "citation_id": "59",
      "title": "Not at Face Value: Exploring Whether Whites Exhibit Bias in Categorizing Angry and Happy Black Versus White Expressions",
      "authors": [
        "T Lesick"
      ],
      "year": "2023",
      "venue": "Not at Face Value: Exploring Whether Whites Exhibit Bias in Categorizing Angry and Happy Black Versus White Expressions"
    },
    {
      "citation_id": "60",
      "title": "Semantics derived automatically from language corpora contain human-like biases",
      "authors": [
        "A Caliskan",
        "J Bryson",
        "A Narayanan"
      ],
      "year": "2017",
      "venue": "Science"
    },
    {
      "citation_id": "61",
      "title": "American== white in multimodal languageand-image ai",
      "authors": [
        "R Wolfe",
        "A Caliskan"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society"
    },
    {
      "citation_id": "62",
      "title": "Contrastive languagevision ai models pretrained on web-scraped multimodal data exhibit sexual objectification bias",
      "authors": [
        "R Wolfe",
        "Y Yang",
        "B Howe",
        "A Caliskan"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency"
    },
    {
      "citation_id": "63",
      "title": "Certifying and removing disparate impact",
      "authors": [
        "M Feldman",
        "S Friedler",
        "J Moeller",
        "C Scheidegger",
        "S Venkatasubramanian"
      ],
      "year": "2015",
      "venue": "proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining"
    },
    {
      "citation_id": "64",
      "title": "Age progression/regression by conditional adversarial autoencoder",
      "authors": [
        "S Zhang",
        "H Qi"
      ],
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "65",
      "title": "Fairface: Face attribute dataset for balanced race, gender, and age for bias measurement and mitigation",
      "authors": [
        "K Karkkainen",
        "J Joo"
      ],
      "year": "2021",
      "venue": "Proceed-ings of the IEEE/CVF winter conference on applications of computer vision"
    },
    {
      "citation_id": "66",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "67",
      "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
      "authors": [
        "Z Liu",
        "Y Lin",
        "Y Cao",
        "H Hu",
        "Y Wei",
        "Z Zhang",
        "S Lin",
        "B Guo"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF international conference on computer vision"
    },
    {
      "citation_id": "68",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly"
      ],
      "year": "2020",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "arxiv": "arXiv:2010.11929"
    },
    {
      "citation_id": "69",
      "title": "Learning multidimensional edge feature-based au relation graph for facial action unit recognition",
      "authors": [
        "C Luo",
        "S Song",
        "W Xie",
        "L Shen",
        "H Gunes"
      ],
      "year": "2022",
      "venue": "Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22"
    },
    {
      "citation_id": "70",
      "title": "Supervised contrastive learning with identity-label embeddings for facial action unit recognition",
      "authors": [
        "T Lian",
        "D Adama",
        "P Machado",
        "D Vinkemeier"
      ],
      "year": "2023",
      "venue": "Supervised contrastive learning with identity-label embeddings for facial action unit recognition"
    },
    {
      "citation_id": "71",
      "title": "Statistical power analysis for the behavioral sciences",
      "authors": [
        "J Cohen"
      ],
      "year": "2013",
      "venue": "Statistical power analysis for the behavioral sciences"
    },
    {
      "citation_id": "72",
      "title": "Racial influence on automated perceptions of emotions",
      "authors": [
        "L Rhue"
      ],
      "year": "2018",
      "venue": "SSRN 3281765"
    },
    {
      "citation_id": "73",
      "title": "Biased attention: Do vision transformers amplify gender bias more than convolutional neural networks?",
      "authors": [
        "A Mandal",
        "S Leavy",
        "S Little"
      ],
      "venue": "British Machine Vision Conference (BMVC)"
    },
    {
      "citation_id": "74",
      "title": "Overwriting pretrained bias with finetuning data",
      "authors": [
        "A Wang",
        "O Russakovsky"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "75",
      "title": "Transformer-based multimodal emotional perception for dynamic facial expression recognition in the wild",
      "authors": [
        "X Zhang",
        "M Li",
        "S Lin",
        "H Xu",
        "G Xiao"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "76",
      "title": "The misgendering machines: Trans/hci implications of automatic gender recognition",
      "authors": [
        "O Keyes"
      ],
      "year": "2018",
      "venue": "Proceedings of the ACM on human-computer interaction"
    },
    {
      "citation_id": "77",
      "title": "Gender recognition or gender reductionism? the social implications of embedded gender recognition systems",
      "authors": [
        "F Hamidi",
        "M Scheuerman",
        "S Branham"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 chi conference on human factors in computing systems"
    },
    {
      "citation_id": "78",
      "title": "Gender as a variable in natural-language processing: Ethical considerations",
      "authors": [
        "B Larson"
      ],
      "year": "2017",
      "venue": "Gender as a variable in natural-language processing: Ethical considerations"
    }
  ]
}