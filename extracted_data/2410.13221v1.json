{
  "paper_id": "2410.13221v1",
  "title": "Investigating Effective Speaker Property Privacy Protection In Federated Learning For Speech Emotion Recognition",
  "published": "2024-10-17T05:03:34Z",
  "authors": [
    "Chao Tan",
    "Sheng Li",
    "Yang Cao",
    "Zhao Ren",
    "Tanja Schultz"
  ],
  "keywords": [
    "property inference attack",
    "differential privacy",
    "federated learning",
    "speech emotion recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Federated Learning (FL) is a privacy-preserving approach that allows servers to aggregate distributed models transmitted from local clients rather than training on user data. More recently, FL has been applied to Speech Emotion Recognition (SER) for secure humancomputer interaction applications. Recent research has found that FL is still vulnerable to inference attacks. To this end, this paper focuses on investigating the security of FL for SER concerning property inference attacks. We propose a novel method to protect the property information in speech data by decomposing various properties in the sound and adding perturbations to these properties. Our experiments show that the proposed method offers better privacy-utility trade-offs than existing methods. The trade-offs enable more effective attack prevention while maintaining similar FL utility levels. This work can guide future work on privacy protection methods in speech processing. \n CCS CONCEPTS â€¢ Human-centered computing â†’ Human computer interaction (HCI); â€¢ Security and privacy;",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech, the natural and comfortable mean in human communication  [28] , contains significant hidden paralinguistic information, such as emotional cues  [25] . Recognizing emotional states from speech is essential for machines to understand humans' speech and effectively interact with humans in many applications, such as speech assistants, mental health monitoring, education, etc  [12, 17, 18] . In the realm of human-computer interaction, Speech Emotion Recognition (SER) has garnered significant attention, particularly due to the utilization of Deep Neural Networks (DNNs) featuring intricate model structures.  [14] .\n\nDNNs have been successfully used in multiple areas, like image processing and audio processing tasks, as their deep structures can capture effective abstract information for those prediction tasks. Training DNNs often require large-scale datasets, thereby numerous models parameters can be sufficiently learned. However, it is challenging to gather personal data due to strict data protection laws and privacy concerns  [31] , resulting in a bottleneck of training DNNs on personal speech data.\n\nFederated Learning (FL), proposed by Google, enables distributed model training to tackle this issue  [15] . Within the realm of FL, the exchange of information between local devices and the central server during each training iteration is limited to transmitting model parameters or gradients rather than the actual data. Subsequently, the central server aggregates these local models to construct a unified global model that serves as the foundation for the subsequent training round. This approach helps to protect personal data while still enabling the model to learn from diverse data sources. Specifically, in applying FL for SER, clients gather users' speech data from individual mobile devices, perform local model training, and subsequently transmit the models to a central server. However, recent research has revealed that FL may be susceptible to property inference attacks  [34] . Despite only transmitting the model parameters or gradients during FL, sensitive information can still be inferred, as demonstrated in several studies  [10, 20, 21, 33, 36] . In this context, FL may not provide rigorous privacy protection.\n\nProperty inference refers to the attacker learning information about the subject's data property, which is unrelated to the original learning task  [11] . Notably, the property inference in this work aims to learn a subject's attribute rather than the global data property  [11] . While it has been studied in natural language processing and computer vision  [10, 21, 33, 36] , its impact on speech signals has not been well-investigated. The principle of property inference through survey has been used in various attack methods  [4]  that are already known. If the attackers use property inference to attack the system, the profiles of the users (such as age, race, and gender) are very easily stolen. To foster awareness within the speech community, the imperative lies in developing robust protective measures against such attacks. Regrettably, a scarcity of studies has been dedicated to comprehensively evaluating and comparing various safeguarding techniques.\n\nThis study aims to assess and compare the efficacy of diverse privacy protection mechanisms in the context of FL-based SER. Our contributions are as follows. First, we introduce a novel privacy preservation technique called Property-Indistinguishability (Pro-Ind), and present experimental findings obtained by implementing various privacy protection strategies to thwart potential attacks. Second, we provide an empirical analysis of the outcomes stemming from using existing protection methodologies  [8, 32]  in conjunction with our proposed Pro-Ind method within the realm of FL-based SER (FL-SER). Third, this work verifies the proposed approach on inference attacks of multiple types of properties, offering a comprehensive comparison and guidance for future research works. In sum, the novelty of our work is that we have proposed a scheme to protect property privacy in voice, and its effectiveness has been verified through experiments.\n\nThe rest of the paper is organized as follows: Section 2 introduces the relevant research studies. Section 3 describes the proposed method in this work. Section 4 shows the experimental evaluation. Finally, this work is concluded, and potential future work is discussed in Section 5.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work 2.1 Federated Learning For Ser",
      "text": "More recently, FL has been applied to SER for protecting users' privacy in several studies. The study in  [16]  demonstrated good performance of FL for SER compared to the state-of-the-art, and FL was shown to enhance the performance of audiovisual emotion recognition in  [26] . An FL-based system was proposed to monitor users' mental health by analyzing their emotional states from facial expressions and speech signals in  [2] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Privacy Protection In Federated Learning",
      "text": "Despite the capability of FL to protect data by only uploading model parameters or gradients to the server, it has been shown that FL may still expose user information to malicious attacks, such as property inference attacks discussed in this work. Attribute inference attacks were proposed to successfully attack an FL system for SER in  [5] . The authors in  [6]  further evaluated User-level differential privacy (UDP) on attribute inference attacks in a framework of FL for SER. In  [34] , a feature attention mechanism was proposed to guide the SER models to focus on learning emotion-related representations, thereby hiding emotion-irrelevant attributes. Paillier homomorphic encryption was applied to FL in the task of SER. Therefore, data remains private during training  [22]  A method to protect gender information from inference attacks was proposed in  [29] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Preliminary Knowledge 3.1 Federated Learning",
      "text": "As aforementioned, in the FL-based SER (FL-SER), clients employ their speech data to train localized models on individual devices and transmit them to the central server. In this setting, the server has a potential malicious adversary that is curious (adhering to the FL protocol but wanting to get some information from clients' gradients). The server is expected to perform inference attacks using snapshots of each user's model parameters. This assumption is also confirmed in the recent work  [30] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Property Attacks",
      "text": "We adopted the attack model shown in Figure  1 . This choice was made due to the model's standardization and ease of implementation. It's worth noting that attacks in the context of FL can be categorized into two main categories: those originating from within the system and those from external sources  [20] . Consequently, in the specific scenario of this paper, both the server, despite its honest but curious nature, and an external eavesdropper possess the capability to execute the attack. The above-mentioned framework outlining the attack process is visually represented in Figure  1 . This attack process has three distinct stages:\n\nFirstly, Private Training is a procedure where the model's training takes place locally on the clients' devices. These clients engage with their localized training dataset, denoted as ğ‘« ğ’‘ , and associated emotional labels, ğ’š ğ’‘ .\n\nThe central server can coordinate the FL process through two primary algorithms: FedSGD and FedAvg  [13] . Only gradients or model parameters, rather than the raw data, are transmitted to the central server. In this setup, the gradients exposed by each client are represented as ğ’ˆ ğ’• ğ’‘ , where ğ’• specifies the training epoch and ğ’‘ identifies the client.\n\nThe central server coordinates the FL process using two main algorithms: FedSGD and FedAvg  [13] . Instead of transmitting raw data, only gradients or model parameters are sent to the central server. In this configuration, the gradients contributed by each client are denoted as ğ’ˆ Thirdly, Attack Model is denoted as ğ‘´ ğ’‚ , the training dataset is comprised of gradients ğ’ˆ ğ’• ğ’‚ and corresponding property labels ğ’š ğ’‚ , both of which are acquired during the shadow training phase. The model is subsequently trained using these datasets and optimized to the parameter set Î¨. The objective is represented as L:\n\nUpon the completion of the training process for the Attack Model ğ‘´ ğ’‚ , it becomes feasible to predict the gender of client ğ’‘ by feeding the gradient ğ’ˆ ğ’• ğ’‘ into the trained attack model.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Protection Methods",
      "text": "The principle of Differential Privacy ensures that the inclusion or omission of any single data point in a dataset does not substantially affect the conclusions drawn from the dataset. Formally, Differential Privacy is defined as per equation (  2 )  [3] . An randomized algorithm denoted as ğ‘´, which operates on a domain ğ‘µ |ğ‘¿ | with a corresponding range ğ‘¹, gives (ğ, ğœ¹)-differential privacy. This property holds true for any subset ğ‘º within the range of ğ‘´ and for all adjacent data points ğ’™, ğ’™â€² âˆˆ ğ‘µ |ğ‘¿ | as described below:\n\nA comprehensive survey focusing on Differential Privacy (DP) for unstructured data has been presented in  [35] . This study uncovers that voiceprint 1  combined with normal DP technique could be  applied to speech data. However, as the principal objective of the current work is to safeguard client privacy against potential attacks-bearing in mind that the clients are actual human individuals-a more apt DP method, termed User-level DP, has been adopted for client-level privacy preservation. Figure  2  offers a graphical representation illustrating the modifications in the attack process when client-side protection measures are considered and incorporated.\n\nUser-Level Differential Privacy (UDP): The primary concept of UDP is to introduce perturbation in local data using mechanism ğ‘´ ğ’– . The perturbed data is ensured to be protected against privacy attacks with the help of parameter values ğ and ğœ¹. This protection technique is based on the research presented in Fenget al.'s study  [6] .\n\nIn such a configuration, UDP introduces perturbations to the gradient ğ’ˆ ğ’• ğ’Œ by appending Gaussian noise. This noise is characterized by a zero mean and a variance of ğˆ 2 ğ’Œ ğ‘° for a given gradient ğ’ˆ ğ’• ğ’Œ . The mathematical representation of this operation is detailed in equation  (3) .\n\nThe algorithmic specifics for UDP are elaborated upon in existing research  [6] . For a particular client, denoted as ğ’Œ, the variance parameter ğˆ ğ’Œ is delineated using equation (  4 ). The âˆ‡ğ‘™ denotes the constraint on the gradient variance between consecutive datasets from a specific client ğ’Œ  [6] . Furthermore, ğ‘ and ğ‘‡ denote the proportion of client data sampled in each training epoch and the number of global training epochs, respectively. ğœ¹ ğ’Œ and ğ ğ’Œ are used to control differential privacy.\n\nvoiceprint with DP: operates on the foundational concept of differential privacy, yet its privacy definition is customized for voiceprints, specifically x-vectors  [27] . To uphold privacy, noise is introduced into the voiceprint.\n\nThe voiceprint with DP approach, as described in  [7, 8] , is specifically designed for the context of speech data publication. Given an initial data point ğ’” 0 , the mechanism, denoted as ğ‘´ ğ’— , initially extracts an x-vector ğ’— 0 . Subsequently, this x-vector is transformed into a perturbed version, ğ’—, which belongs to the dataset ğ‘« ğ’— . The dataset ğ‘« ğ’— consists of x-vectors extracted from a yet-tobe-published speech dataset ğ‘« ğ’” . The transition from ğ’— 0 to ğ’— occurs with a probability that is a function of the distance between these two vectors. Finally, the perturbed x-vector ğ’— is combined with the original speech data ğ’” 0 to synthesize a modified speech data point, ğ’”, which is then released for publication.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "The Proposed Approach: Property-Indistinguishability",
      "text": "While voiceprints contain property information, every property information is coupled together  [24] . Individual users exhibit varying privacy requirements in the FL context. For instance, while some users may wish to withhold information about their age, others may prefer to conceal data related to their gender or ethnicity. Apart from these specific information that clients choose not to disclose, they are generally amenable to contributing other personal information for FL tasks. Consequently, utilizing voiceprint with DP as a mechanism to thwart property inference attacks is suboptimal. This is because the very act of preserving a particular facet of privacy necessitates obfuscating that specific data. When clients engage in specific FL applications, such as SER, they must furnish information about vocal characteristics and emotional states. The intentional obfuscation of this data for privacy protection could potentially impede the optimization of FL accuracy. As a result, we propose a new protection method called Pro-Ind, which is specifically designed for property privacy preservation.\n\nDefinition 4.1 (Property-Indistinguishability, i.e., Pro-Ind). A mechanism ğ‘´ ğ’ˆ is said to fulfill ğ-property-indistinguishability under the following conditions: For any resulting property embedding ğ’„ and any two feasible input embeddings ğ’„, ğ’„â€² âˆˆ ğ‘ª as follows:\n\nwhere ğ‘ª denotes the set of properties represented in the forms of embeddings. The function ğ’… ğ‘ª quantifies the distance between angulars of two embeddings ğ’„ and ğ’„â€².\n\nIt is reasonable to suppose that clients should possess equivalent capabilities, especially considering that an adversary could potentially leverage property embeddings from publicly accessible datasets.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Property Embedding Protection Under Pro-Ind",
      "text": "The method proposed in this paper, termed Pro-Ind, is architected specifically to safeguard property-related information. This is achieved by injecting noise into the property embeddings derived from an x-vector framework trained using property identifiers rather than speaker identifiers. Unlike the data release scenario elaborated in  [7, 8] , the present framework is engineered for application within FL. It accommodates the client's use of public datasets, mirroring the adversary's capabilities and thus reinforcing privacy protection.\n\nWith an initial property embedding denoted as ğ’„ 0 , the mechanism ğ‘´ ğ’ˆ introduces perturbations by probabilistically selecting a property embedding ğ’„ from the dataset ğ‘« ğ’ˆ . The selection process is governed by predefined probability distributions, thereby affording a degree of plausible deniability for the original property embedding ğ’„ 0 . Their definitions are as follows: The synthesis model employed in this work is an end-to-end neural network, trained by utilizing the ESPnet (End-to-end Speech Processing Toolkit) framework  [9] , specifically the example setup in egs/libritts/tts1. This model is designed to generate Mel Spectrogram (Mel-spec) features  [19] , which serve as the input for a waveform vocoder  [23] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Privacy Analysis",
      "text": "The crux of privacy leakage resides in the gap between an adversary's prior and posterior knowledge concerning a targeted individual. Although Differential Privacy (DP) is not capable of entirely thwarting adversaries from accumulating knowledge, it can impede their progress through the judicious selection of parameters (the detailed description can be found in  [3] ).\n\nGiven two distinct property embeddings, ğ’„ and ğ’„ â€² , and an output property embedding, c, generated by the protective mechanism:\n\nbecause the distance metric ğ’… C (ğ’„, ğ’„ â€² ) is the angular distance, which satisfies the triangle inequality, so there are definitions as follows:\n\ntherefore\n\nwhich implies that the property embedding protection method complies with the Pro-Ind criterion.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiments 5.1 Data Description",
      "text": "The CREMA-D database  [1]  contains 7,442 original audio clips provided by 91 actors. The actor demographics are varied, with 48 male and 43 female participants representing diverse racial and ethnic backgrounds, including African American, Asian, Caucasian, Hispanic, and Unspecified. The age range of the contributors spans from 20 to 74 years. Each clip includes one of twelve distinct sentences and is annotated with mood level (in total four levels) and emotional state (in total six states: sad, neutral, happy, fear, disgust, angry). For the present experiment, we focused on four specific elemental emotional categories: neutral, pleased, joyful, and furious. Evaluation Metrics. In this paper, we use multiple metrics for evaluating the results, including Accuracy (ğ´ğ¶ğ¶), Unweighted Average Recall (ğ‘ˆ ğ´ğ‘…), Success Rate of Property Attack (ğ‘†ğ‘… ğ‘ƒ ), and Unweighted Average Success Recall of Property Attack (ğ‘ˆ ğ´ğ‘†ğ‘… ğ‘ƒ ). Their calculation procedures are in the following.\n\nwhere ğ¸ denotes the ğ‘ ğ¸ emotional classes, ğ‘ ğ¸ ğ‘– is the number of samples in the ğ‘–-th emotional class, N ğ¸ ğ‘– indicates the number of correctly classified samples, Ã‘ ğ¸ ğ‘– is number of predicted class at the ğ‘–-th index, and ğ‘Ÿ ğ¸ ğ‘– is the ratio of the ğ‘–-th class in the whole dataset. Correspondingly, ğ‘ƒ means the ğ‘ ğ‘ƒ property groups, ğ‘ ğ‘ƒ ğ‘— is number of samples in the ğ‘—-th property class, N ğ‘ƒ ğ‘— means number of samples with correctly predicted properties, Ã‘ ğ‘ƒ ğ‘— is number of predicted property class at the ğ‘—-th index, and ğ‘Ÿ ğ‘ƒ ğ‘— is the ratio of the ğ‘—-th property class.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Results Of Unprotected Fl",
      "text": "The private training methodology deployed in this study is consistent with approaches presented in prior research  [6] . The DNN architecture designated for SER is structured with two dense layers, with hidden layer dimensions of 256 and 128 units, respectively. The activation function incorporated within the network is the Rectified Linear Unit (ReLU), and a dropout rate of 0.2 is implemented to mitigate overfitting. For the FL coordination, the most popular FedAvg algorithm is used as the foundational framework. In the shadow training phase, a total of 30 speakers are employed, with the remaining speakers allocated for private training. The client participation rate in private training is set at 10%. For each client, 80% of the available data is designated for local training, and the remaining 20% is reserved for validation purposes. The local learning rate is established at 0.0005, the number of local epochs is fixed at 1, and the total number of global training epochs, denoted as ğ‘‡ , is set at 200. Furthermore, the local training batch size is configured to be 20.\n\nAligned with the configurations in prior work  [6] , the architecture remains constant, and additional layer experimentation is foregone. This choice is guided by the understanding that the first layer is the most susceptible to privacy leakage  [30] . We also adhere to the default configuration for voiceprint with DP as in  [7, 8] .\n\nTo assess the efficacy of various defense strategies, multiple values of differential privacy's ğ are tested, specifically {1, 5, 10, 25, 50}. 100 gradients are extracted to assess the performance of the attack model. Table  1  presents detailed performance for both the FL-SER model and the attack model without a protection mechanism.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Results Of Protected Fl",
      "text": "In parallel to the methodologies proposed in Feng's research  [4] , this work expands the scope of investigation by designing two additional types of attack models, specifically focusing on age and race. For each variety of attack, three distinct defense mechanisms-UDP, voiceprint with DP, and Pro-Ind-are employed, each tailored to defend against the attributes targeted by the attack model in question. As illustrated in Figure  3 , Pro-Ind ostensibly outperforms the other defense strategies in safeguarding privacy. It is noteworthy to mention that in the absence of any defense mechanisms, the success rate for each type of attack model hovers around 83% -85%, as summarized in Table  1 . Figure  5  reveals a nuanced impact of employing various privacypreserving techniques on the accuracy of the FL model. Notably, the use of voiceprint with DP and Pro-Ind incurs a marginal degradation in model performance, affirming their efficiency as privacypreserving methods with minimal trade-offs in accuracy. Conversely, the employment of UDP demonstrates a more pronounced impact on the FL model, especially when the parameter ğ is set to a smaller value, implying stronger privacy protections. Under such circumstances, the model's performance experiences a significant decline. For a comprehensive understanding, the baseline accuracy of the FL model without any privacy-preserving techniques is cataloged in Table  1 .\n\nOn the other hand, the experiment also compares the privacy protection of UDP, voiceprint with DP, pro-Ind for gender, pro-Ind for age, and pro-Ind for race against gender attacks. As shown in Figure  4 , it can be seen that good results will not be achieved when misusing non-corresponding pro-Ind. This interesting result also indirectly proves the accuracy of pro-Ind from the side. Figures  3  and 4  present comparative evaluations of the efficacy of various privacy-preserving techniques, namely, UDP, voiceprint with DP, and Pro-Ind. Among these, Pro-Ind emerges as the most proficient method for safeguarding user privacy without significantly compromising the performance of the FL model. Specifically, UDP introduces noise at the gradient level, thereby obfuscating information globally across all users. Voiceprint with DP, on the other hand, perturbs only the voiceprint data. In contrast, Pro-Ind focuses on adding noise to attribute embeddings. This targeted approach allows Pro-Ind to offer a more nuanced balance, ensuring robust privacy protection while minimally affecting the utility of the FL model.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Discussion",
      "text": "It is critical to design and deploy privacy-preserving machine learning algorithms in distributed computing environments like FL. In light of the experimental findings elucidated, we can conclusively assert that to preserve specific facets of privacy within the domain of speech-emotion FL, the judicious strategy involves explicitly defining the specific attributes to be protected and safeguarding property-relevant embeddings through mechanisms like Property-Indistinguishability. The proposed approach can effectively prevent the attackers from getting the user profile. Deploying such targeted privacy-preservation mechanisms demonstrates advantages over a blanket approach of indiscriminate noise addition to the entire model. This specialized method fosters enhanced accuracy in the FL model and ensures more effective privacy preservation. Consequently, it represents an optimal trade-off between utility and privacy.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusion And Future Work",
      "text": "This paper proposed a novel privacy-preserving method within the context of Federated Learning-based Speech Emotion Recognition (FL-SER). Empirical evidence substantiated that more effective privacy safeguards and improved FL-SER accuracy are realized when privacy protection mechanisms are architected in alignment with precisely tailored privacy definitions. This insight provided a fertile ground for subsequent research endeavors to develop nuanced definitions for speech privacy. In extending this line of inquiry, we postulate a broader hypothesis: that the alignment of protection",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: This choice was",
      "page": 2
    },
    {
      "caption": "Figure 1: The framework of attack.",
      "page": 3
    },
    {
      "caption": "Figure 2: The framework of protection.",
      "page": 3
    },
    {
      "caption": "Figure 3: , Pro-Ind ostensibly outperforms the",
      "page": 5
    },
    {
      "caption": "Figure 5: reveals a nuanced impact of employing various privacy-",
      "page": 5
    },
    {
      "caption": "Figure 4: , it can be seen that good results will not be achieved when",
      "page": 5
    },
    {
      "caption": "Figure 3: The success rates of attack models (age, gender, race) with corresponding Pro-Ind",
      "page": 6
    },
    {
      "caption": "Figure 4: The success rate of the gender attack model with different",
      "page": 6
    },
    {
      "caption": "Figure 5: The accuracy of FL-SER model.",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Gender Attack model": "ğ‘†ğ‘…ğ‘ƒ\nğ‘ˆ ğ´ğ‘†ğ‘…ğ‘ƒ",
          "Age Attack model": "ğ‘†ğ‘…ğ‘ƒ\nğ‘ˆ ğ´ğ‘†ğ‘…ğ‘ƒ",
          "Race Attack model": "ğ‘ˆ ğ´ğ‘†ğ‘…ğ‘ƒ\nğ‘ˆ ğ´ğ‘†ğ‘…ğ‘ƒ",
          "FL-SER model": "ğ´ğ¶ğ¶\nğ‘ˆ ğ´ğ‘…"
        },
        {
          "Gender Attack model": "0.84\n0.83",
          "Age Attack model": "0.85\n0.84",
          "Race Attack model": "0.82\n0.85",
          "FL-SER model": "0.66\n0.60"
        },
        {
          "Gender Attack model": "0.85\n0.84",
          "Age Attack model": "0.85\n0.86",
          "Race Attack model": "0.83\n0.80",
          "FL-SER model": "0.67\n0.60"
        },
        {
          "Gender Attack model": "0.82\n0.80",
          "Age Attack model": "0.86\n0.87",
          "Race Attack model": "0.85\n0.78",
          "FL-SER model": "0.66\n0.62"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "H Cao"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "2",
      "title": "Federated learning meets human emotions: A decentralized framework for human-computer interaction for IoT applications",
      "authors": [
        "Prateek Chhikara",
        "Prabhjot Singh",
        "Rajkumar Tekchandani",
        "Neeraj Kumar",
        "Mohsen Guizani"
      ],
      "year": "2020",
      "venue": "IEEE Internet of Things Journal"
    },
    {
      "citation_id": "3",
      "title": "The algorithmic foundations of differential privacy",
      "authors": [
        "Cynthia Dwork",
        "Aaron Roth"
      ],
      "year": "2014",
      "venue": "Figure 5. The accuracy of FL-SER model"
    },
    {
      "citation_id": "4",
      "title": "Attribute Inference Attack of Speech Emotion Recognition in Federated Learning Settings",
      "authors": [
        "Tiantian Feng",
        "Hanieh Hashemi",
        "Rajat Hebbar",
        "Murali Annavaram",
        "Shrikanth Narayanan"
      ],
      "year": "2021",
      "venue": "Attribute Inference Attack of Speech Emotion Recognition in Federated Learning Settings"
    },
    {
      "citation_id": "5",
      "title": "Attribute inference attack of speech emotion recognition in federated learning settings",
      "authors": [
        "Tiantian Feng",
        "Hanieh Hashemi",
        "Rajat Hebbar",
        "Murali Annavaram",
        "Shrikanth S Narayanan"
      ],
      "year": "2021",
      "venue": "Attribute inference attack of speech emotion recognition in federated learning settings",
      "arxiv": "arXiv:2112.13416"
    },
    {
      "citation_id": "6",
      "title": "User-level differential privacy against attribute inference attack of speech emotion recognition in federated learning",
      "authors": [
        "Tiantian Feng",
        "Raghuveer Peri",
        "Shrikanth Narayanan"
      ],
      "year": "2022",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "7",
      "title": "Voice-Indistinguishability -Protecting Voiceprint with Differential Privacy under an Untrusted Server",
      "authors": [
        "Yaowei Han",
        "Yang Cao",
        "Sheng Li",
        "Qiang Ma",
        "Masatoshi Yoshikawa"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security (Virtual Event, USA) (CCS '20)",
      "doi": "10.1145/3372297.3420025"
    },
    {
      "citation_id": "8",
      "title": "Voice-Indistinguishability: Protecting Voiceprint In Privacy-Preserving Speech Data Release",
      "authors": [
        "Yaowei Han",
        "Sheng Li",
        "Yang Cao",
        "Qiang Ma",
        "Masatoshi Yoshikawa"
      ],
      "year": "2020",
      "venue": "Proc. ICME"
    },
    {
      "citation_id": "9",
      "title": "Espnet-TTS: Unified, reproducible, and integratable open source end-to-end text-tospeech toolkit",
      "authors": [
        "Tomoki Hayashi",
        "Ryuichi Yamamoto",
        "Katsuki Inoue",
        "Takenori Yoshimura",
        "Shinji Watanabe",
        "Tomoki Toda",
        "Kazuya Takeda",
        "Yu Zhang",
        "Xu Tan"
      ],
      "year": "2020",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "10",
      "title": "Deep models under the GAN: Information Leakage from Collaborative Deep Learning",
      "authors": [
        "Briland Hitaj",
        "Giuseppe Ateniese",
        "Fernando Perez-Cruz"
      ],
      "year": "2017",
      "venue": "Proc. ACM SIGSAC CCS"
    },
    {
      "citation_id": "11",
      "title": "Membership inference attacks on machine learning: A survey",
      "authors": [
        "Hongsheng Hu",
        "Zoran Salcic",
        "Lichao Sun",
        "Gillian Dobbie",
        "Philip Yu",
        "Xuyun Zhang"
      ],
      "year": "2022",
      "venue": "ACM CSUR"
    },
    {
      "citation_id": "12",
      "title": "Speech emotion recognition",
      "authors": [
        "B Ashish",
        "Ingale",
        "Chaudhari"
      ],
      "year": "2012",
      "venue": "International Journal of Soft Computing and Engineering"
    },
    {
      "citation_id": "13",
      "title": "Rachel Cummings, et al. 2021. Advances and open problems in federated learning",
      "authors": [
        "Peter Kairouz",
        "Brendan Mcmahan",
        "Brendan Avent",
        "AurÃ©lien Bellet",
        "Mehdi Bennis",
        "Nitin Arjun",
        "Kallista Bhagoji",
        "Zachary Bonawitz",
        "Graham Charles",
        "Cormode"
      ],
      "year": "2021",
      "venue": "Foundations and TrendsÂ® in Machine Learning"
    },
    {
      "citation_id": "14",
      "title": "Speech emotion recognition using deep learning techniques: A review",
      "authors": [
        "Edward Ruhul Amin Khalil",
        "Mohammad Jones",
        "Tariqullah Inayatullah Babar",
        "Mohammad Jan",
        "Thamer Haseeb Zafar",
        "Alhussain"
      ],
      "year": "2019",
      "venue": "IEEE access"
    },
    {
      "citation_id": "15",
      "title": "Federated Learning: Strategies for Improving Communication Efficiency. NIPS Workshop on Private Multi-Party Machine Learning",
      "authors": [
        "H Jakub KonecnÃ½",
        "Felix Mcmahan",
        "Peter Yu",
        "Ananda RichtÃ¡rik",
        "Dave Theertha Suresh",
        "Bacon"
      ],
      "year": "2016",
      "venue": "Federated Learning: Strategies for Improving Communication Efficiency. NIPS Workshop on Private Multi-Party Machine Learning"
    },
    {
      "citation_id": "16",
      "title": "Federated learning for speech emotion recognition applications",
      "authors": [
        "Siddique Latif",
        "Sara Khalifa",
        "Rajib Rana",
        "Raja Jurdak"
      ],
      "year": "2020",
      "venue": "Proc. IPSN"
    },
    {
      "citation_id": "17",
      "title": "Study on emotion recognition and companion Chatbot using deep neural network",
      "authors": [
        "Ming-Che Lee",
        "Shu-Yin Chiang",
        "Sheng-Cheng Yeh",
        "Ting-Feng Wen"
      ],
      "year": "2020",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "18",
      "title": "Speech emotion recognition in e-learning system based on affective computing",
      "authors": [
        "Wu Li",
        "Yanhui Zhang",
        "Yingzi Fu"
      ],
      "year": "2007",
      "venue": "Proc. ICNC"
    },
    {
      "citation_id": "19",
      "title": "Mel frequency cepstral coefficients for music modeling",
      "authors": [
        "Beth Logan"
      ],
      "year": "2000",
      "venue": "Proc. ISMIR"
    },
    {
      "citation_id": "20",
      "title": "Threats to Federated Learning",
      "authors": [
        "Lingjuan Lyu",
        "Han Yu",
        "Jun Zhao",
        "Qiang Yang"
      ],
      "year": "2020",
      "venue": "Threats to Federated Learning"
    },
    {
      "citation_id": "21",
      "title": "Exploiting Unintended Feature Leakage in Collaborative Learning",
      "authors": [
        "Luca Melis",
        "Congzheng Song",
        "Emiliano De Cristofaro",
        "Vitaly Shmatikov"
      ],
      "year": "2019",
      "venue": "Proc. SP"
    },
    {
      "citation_id": "22",
      "title": "Secure and efficient federated learning by combining homomorphic encryption and gradient pruning in speech emotion recognition",
      "authors": [
        "Samaneh Mohammadi",
        "Sima Sinaei",
        "Ali Balador",
        "Francesco Flammini"
      ],
      "year": "2023",
      "venue": "Proc. ISPEC"
    },
    {
      "citation_id": "23",
      "title": "Wavenet: A generative model for raw audio",
      "authors": [
        "Aaron Van Den Oord",
        "Sander Dieleman",
        "Heiga Zen",
        "Karen Simonyan",
        "Oriol Vinyals",
        "Alex Graves",
        "Nal Kalchbrenner",
        "Andrew Senior",
        "Koray Kavukcuoglu"
      ],
      "year": "2016",
      "venue": "Wavenet: A generative model for raw audio",
      "arxiv": "arXiv:1609.03499"
    },
    {
      "citation_id": "24",
      "title": "Probing the Information Encoded in X-Vectors",
      "authors": [
        "Desh Raj",
        "David Snyder",
        "Daniel Povey",
        "Sanjeev Khudanpur"
      ],
      "year": "2019",
      "venue": "Proc. ASRU"
    },
    {
      "citation_id": "25",
      "title": "Computational paralinguistics: emotion, affect and personality in speech and language processing",
      "authors": [
        "BjÃ¶rn Schuller",
        "Anton Batliner"
      ],
      "year": "2013",
      "venue": "Computational paralinguistics: emotion, affect and personality in speech and language processing"
    },
    {
      "citation_id": "26",
      "title": "Enhancing Emotion Recognition through Federated Learning: A Multimodal Approach with Convolutional Neural Networks",
      "authors": [
        "Nikola SimiÄ‡",
        "SiniÅ¡a SuziÄ‡",
        "Nemanja MiloÅ¡eviÄ‡",
        "Vuk Stanojev",
        "Tijana Nosek",
        "Branislav PopoviÄ‡",
        "Dragana BajoviÄ‡"
      ],
      "year": "2024",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "27",
      "title": "X-vectors: Robust DNN embeddings for speaker recognition",
      "authors": [
        "D Snyder"
      ],
      "year": "2018",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "28",
      "title": "Speech in human system interaction",
      "authors": [
        "Ryszard Tadeusiewicz"
      ],
      "year": "2010",
      "venue": "Proc. ICHSI"
    },
    {
      "citation_id": "29",
      "title": "General or Specific? Investigating Effective Privacy Protection in Federated Learning for Speech Emotion Recognition",
      "authors": [
        "Chao Tan",
        "Yang Cao",
        "Sheng Li",
        "Masatoshi Yoshikawa"
      ],
      "year": "2023",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "30",
      "title": "Privacy Attacks for Automatic Speech Recognition Acoustic Models in A Federated Learning Framework",
      "authors": [
        "Natalia Tomashenko",
        "Salima Mdhaffar",
        "Marc Tommasi",
        "Y EstÃ¨ve",
        "Jean-FranÃ§ois Bonastre"
      ],
      "year": "2022",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "31",
      "title": "The eu general data protection regulation (gdpr)",
      "authors": [
        "Paul Voigt",
        "Axel Von Dem Bussche"
      ],
      "year": "2017",
      "venue": "The eu general data protection regulation (gdpr)"
    },
    {
      "citation_id": "32",
      "title": "User-Level Privacy-Preserving Federated Learning: Analysis and Performance Optimization",
      "authors": [
        "Kang Wei",
        "Jun Li",
        "Ming Ding",
        "Chuan Ma",
        "Hang Su",
        "Bo Zhang",
        "H Vincent Poor"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Mobile Computing"
    },
    {
      "citation_id": "33",
      "title": "Improved Deep Leakage from Gradients",
      "authors": [
        "Bo Zhao",
        "Konda Reddy Mopuri",
        "Hakan Bilen"
      ],
      "year": "2020",
      "venue": "Improved Deep Leakage from Gradients",
      "arxiv": "arXiv:2001.02610"
    },
    {
      "citation_id": "34",
      "title": "Privacyenhanced federated learning against attribute inference attack for speech emotion recognition",
      "authors": [
        "Huan Zhao",
        "Haijiao Chen",
        "Yufeng Xiao",
        "Zixing Zhang"
      ],
      "year": "2023",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "35",
      "title": "A survey on differential privacy for unstructured data content",
      "authors": [
        "Ying Zhao",
        "Jinjun Chen"
      ],
      "year": "2022",
      "venue": "Comput. Surveys"
    },
    {
      "citation_id": "36",
      "title": "Deep leakage from gradients",
      "authors": [
        "Ligeng Zhu",
        "Zhijian Liu",
        "Song Han"
      ],
      "year": "2019",
      "venue": "Proc. NeurlPS"
    }
  ]
}