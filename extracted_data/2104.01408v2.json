{
  "paper_id": "2104.01408v2",
  "title": "Reinforcement Learning For Emotional Text-To-Speech Synthesis With Improved Emotion Discriminability",
  "published": "2021-04-03T13:52:47Z",
  "authors": [
    "Rui Liu",
    "Berrak Sisman",
    "Haizhou Li"
  ],
  "keywords": [
    "Reinforcement Learning",
    "Emotional Text-to-Speech Synthesis",
    "Speech Emotion Recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotional text-to-speech synthesis (ETTS) has seen much progress in recent years. However, the generated voice is often not perceptually identifiable by its intended emotion category. To address this problem, we propose a new interactive training paradigm for ETTS, denoted as i-ETTS, which seeks to directly improve the emotion discriminability by interacting with a speech emotion recognition (SER) model. Moreover, we formulate an iterative training strategy with reinforcement learning to ensure the quality of i-ETTS optimization. Experimental results demonstrate that the proposed i-ETTS outperforms the state-of-the-art baselines by rendering speech with more accurate emotion style. To our best knowledge, this is the first study of reinforcement learning in emotional text-to-speech synthesis.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotional text-to-speech (ETTS) seeks to synthesize humanlike natural-sounding voice for a given input text with desired emotional expression. The recent advances have enabled many applications such as virtual assistants, call centers, dubbing of movies and games, audiobook narration, and online education.\n\nThe early studies of ETTS are based on hidden Markov models  [1] [2] [3] . For example, we can synthesize speech with a desired emotion through model interpolation  [2]  or by incorporating unsupervised expression cluster during training  [3] . Recently, deep learning opens up many possibilities for ETTS  [4, 5] , where emotion codes can be used as control vectors to change text-to-speech (TTS) output in different ways. Successful attempts include global style tokens (GST) that have been used to control the expressiveness in emotional TTS  [6] [7] [8] . Such approaches typically use style embedding to indicate the emotion rendering and can learn speech variations in an unsupervised manner. We note that the latent style embeddings has no explicit meaning, hence lacks interpretability. Therefore, GST-Tacotron studies often face emotion confusion problem  [9] , i.e., the projected emotion in the synthetic speech is not rendered accurately.\n\nRecently, some approaches  [10] [11] [12]  were proposed to enhance the interpretability of the style embedding. These approaches add an additional emotion recognition loss  [10, 11]  or perceptual loss  [12]  to force the latent style embedding to pay more attention to the emotion rendering. These works have made a great contribution to the development of ETTS. However, they mostly focus on the hidden features  [13] , not the explicit output; and optimize the output acoustic features Speech samples: https://ttslr.github.io/i-ETTS.\n\nwith mean square error (MSE) loss, which calculates the distance between the synthesized emotional speech and natural reference. We note that such MSE-based objective function does not focus directly on emotion discriminability, hence often face emotion confusion problem in generated speech.\n\nWe are motivated by the fact that the interaction of speech emotion recognition model with ETTS can improve the emotion discriminability of synthesized speech, hence overcome emotion confusion problem. We note that ETTS can be trained with SER classification result in a supervised manner. However, this may require a large amount of emotion-labeled speech data, that limits the scope of applications. Therefore, this paper studies the use of reinforcement learning for ETTS in an interactive manner for improved emotion discriminability.\n\nThe reinforcement learning (RL) algorithm  [14]  learns how to achieve a complex goal in an interactive manner. Specifically, RL involves agents to learn their behavior by trial and error  [14] . RL agents aim to learn decision-making by successfully interacting with the environment where they operate. It has enabled speech processing systems  [15]  through a well designed feedback that reflects appropriate perceptual metrics in speech enhancement  [16, 17] , speech recognition  [18]  and speaker recognition  [19] . We note that the use of interactive paradigm with RL algorithm in emotional TTS remains to be explored, which will be the focus of this paper.\n\nIn this paper, we propose a novel ETTS framework with reinforcement learning, denoted as i-ETTS. The proposed approach aims to overcome emotion confusion problem of synthesized speech by optimizing the ETTS model through an interaction with a SER model. The proposed idea is expected to reduce the requirement of emotion-labeled training data size. Experimental results demonstrate that i-ETTS consistently outperforms the state-of-the-art baselines, by achieving improved emotion discriminability of synthesized speech.\n\nThe main contributions of this paper are summarized as follows: 1) we introduce a novel interactive emotional text-tospeech synthesis paradigm that overcomes emotion confusion problem; 2) we formulate the training problem with RL, and optimize the ETTS model with policy gradient and a reward function correlated with the SER accuracy; 3) we utilize an iterative training strategy for a stable training procedure. To our best knowledge, this is the first study of reinforcement learning in emotional speech synthesis.\n\nThis paper is organized as follows. We motivate our study through the comparison with existing RL and ETTS frameworks in Section 2. We formulate the proposed paradigm in Section 3. We report the results of a systematic evaluation and comparison in Sections 4. Finally, Section 5 concludes the study.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "The reinforcement learning (RL) algorithm takes an action in an environment in order to maximize the cumulative reward  [14] . It has been successfully applied to various tasks, such as computer game  [20] , natural language processing  [21] , and speech processing  [15] [16] [17] [18] [19] . For example, RL has been used to optimize speech enhancement system based on character recognition error. Moreover, it has also been used for DNNbased source enhancement  [16]  where the objective sound quality assessment scores are given as the reward. Last but not least, RL-based methods have also been proposed for speech recognition  [18]  and speaker recognition  [19] . We note that these approaches utilize limited or unlabeled data and optimize the target model by interacting with the feedback, which correlates with an evaluation metric from a third party directly.\n\nWith the advent of deep learning, end-to-end TTS systems, such as Tacotron  [22] , Tacotron2  [23]  and their variants  [24, 25]  greatly improve the voice quality of the synthesized speech. However, it remains a challenge as to how we generate speech with an intended emotion due to the fact that it is hard to interpret the learned style embedding.\n\nSome recent studies make use of SER loss  [10, 11]  and perceptual loss  [12]  to assign specific emotional information to the style embedding. However, the commonly used MSE loss for output feature does not directly improve the perceptual discriminability of the synthesized speech. In this paper, we devise a training strategy under reinforcement learning, which employs an interactive game between the ETTS system and a SER to directly improve emotion discriminability.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Interactive Etts: Methodology",
      "text": "We propose a novel interactive training paradigm for emotional TTS under the reinforcement learning framework, denoted as i-ETTS. Our method is a combination of ETTS and RL, hence we first present the main components of proposed i-ETTS that includes agent, policy, action and reward. We then introduce the action update method, denoted as policy gradient. Finally, we describe the iterative training algorithm.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "I-Etts: Interactive Etts With Rl",
      "text": "As in Fig.  1 , the ETTS model can be viewed as an agent under the reinforcement learning framework. The parameters of this agent define a policy. The policy aims to predict the emotional acoustic features at each time step. The emotional acoustic features define the action. After the emotional acoustic feature prediction is finished, the pre-trained SER starts to feedback the emotion recognition accuracy, which is denoted as reward. The policy gradient strategy was used to perform back-propagation smoothly, and optimize the ETTS model to achieves maximum reward.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Agent: Etts",
      "text": "As shown in the left panel of Fig.  1 , our emotional text-tospeech synthesis model is built on the GST-based Tacotron network  [24] . It consists of text encoder, reference encoder with a GST module attached, attention-based decoder and Post-Net. We use Griffin-Lim algorithm  [26]  to reconstruct the speech waveform.\n\nText encoder shares a similar architecture with that of Tacotron2  [23] , which takes the character embeddings as input and generates high-level encoder output embeddings. The reference encoder and GST module were defined to encode the emotion of reference audio into a fixed-length emotion embedding. Their architectures are also similar to that of  [24] , except for the definition of style tokens. To force the GST module to pay more attention to learn the emotion-related style, we set the number of tokens to the number of emotion categories in the corpus. The attention-based decoder takes the encoder output and emotion embedding as input and predicts the acoustic features frame-by-frame. Note that we also adopt a similar decoder structure to Tacotron2  [24] , except that the post-net as in  [22]  was added to convert the mel spectrum to linear spectrum. The GMM attention  [27]  was utilized to learn the <text, wav> alignment.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Reward: Ser Accuracy",
      "text": "In order to achieve emotion expressiveness and discriminability, we introduce a reward function correlated with the SER evaluation metric.\n\nAs shown in the right panel of Fig.  1 , the SER network includes a CNN layer, a BiLSTM layer, an attention layer, a fully connected (FC) layer and a softmax layer. The above architecture is proven effective to learn discriminative features for SER  [28] . The CNN first reads a mel spectrum from input utterance and then outputs a fixed size latent representation. The BiLSTM summarizes the temporal information into another latent representation. The attention layer learns the weights for each frame. The final softmax layer outputs the probability of five emotion types, i.e., happy, angry, neutral, sad and surprise.\n\nFor ETTS training, given an input text x i and reference audio ŷi with emotion label l ŷi , the goal is to generate an emotional speech y i that is not only natural, but also in accordance with the desired emotion l ŷi . One way to employ the pre-trained SER as an expert system in place of a human judge is to evaluate the emotion category of the generated speech y i . We then use the recognition accuracy as the reward to update the ETTS model. Specifically, upon generating the end-of-sequence (EOS) token, the pre-trained SER is used to evaluate how well the generated mel spectrum feature y i matches the reference emotion label l ŷi . Mathematically, the recognition probability p i about the target emotion l ŷi of y i is formulated as:\n\nwhere ϕ represents all the parameters of SER, that are pretrained before the training of ETTS. Note that the value of p i ranges from 0 to 1.\n\nDuring ETTS training, we sample a fixed number of predicted mel spectrum features y from a batch of data to calculate the SER reward, which is formulated as:\n\nwhere R is the reward of the sampled mel spectrum features y from ETTS model. Note that the value of reward R also ranges from 0 to 1. K denotes the sample size. λ is a threshold that is set to 0.5 in this work. N represents the number of samples of which the probability p i of the target emotion exceeds the threshold λ. 1(•) is an indicator function of a set {0, 1}.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Action Update: Policy Gradient",
      "text": "We use policy gradient algorithm  [29]  to estimate the gradient that leads to a larger expected reward E[R] of the generated acoustic features y for input text x. Note that the gradient of the ETTS model P (•) w.r.t. the model parameters θ is estimated by sampling as follows:\n\nUnlike the traditional gradient descent algorithm, the policy gradient algorithm estimates the weights of an optimal policy through gradient ascent. Note that it can assign an explicit emotion-aware supervised signal to the gradient during the model training, resulting in emotional speech with accurate emotion category.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iterative Training",
      "text": "In practice, it is hard to train the whole network from scratch with policy gradient since the ETTS model may find an unexpected way to achieve a high reward but fail to guarantee the naturalness or robustness of the synthesized speech  [21] . Therefore, we formulate an iterative training algorithm, reported in Algorithm 1, that contains two main steps: 1) pre-training, and 2) iterative-training. First, we use MSE Loss to pre-train the ETTS model with text-wav pairs from the training set. After the pre-training phase, iterative-training phase aims to optimize the ETTS model with reward and the MSE loss alternatively.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "We report emotional TTS experiments on ESD database  [30] , which is a new publicly available emotional speech dataset for emotional speech synthesis. ESD is a multi-lingual dataset and has 350 parallel utterances spoken by 10 native English and 10 native Mandarin speakers. We use the English corpora with a total of nearly 13 hours of speech by 5 male and 5 female speakers in five emotions, namely happy, angry, neutral, sad and surprise. In ESD, scripts are provided, speech data are sampled Compute reward R based on Eq. 2 8. Update θ using R based on Eq. 3 9. Update θ using MSE Loss with (x, y, l) 10: end for End at 16 kHz and coded in 16 bits. We note that ESD is considered to be large enough for various voice conversion tasks  [31] , while it provides limited data for TTS and ETTS tasks.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Comparative Study",
      "text": "We implement two baseline systems together with the proposed i-ETTS, as summarized next.\n\n• MTL-ETTS  [11] : An emotional TTS model that jointly trains an auxiliary SER task with the TTS model;\n\n• CET-ETTS  [12] : An emotional TTS model that uses two reference encoders with SER module and perceptual loss to enhance the emotion-discriminative ability;\n\n• i-ETTS: the proposed i-ETTS that optimizes the ETTS model with a reward function correlated with the SER accuracy.\n\nFor a fair comparison, all frameworks use the same SER module, as also illustrated in Section 3.1.2, and Griffin-Lim algorithm  [26]  is used for waveform generation. We note that the use of neural vocoder will further improve the speech quality  [32] , which is not the main focus of this paper.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Setup",
      "text": "For all systems, the encoder takes the character sequence as input. The 80-channel mel spectrum and 1025-channel linearspectrum are extracted with 12.5ms frame shift and 50ms frame length. The decoder predicts 5 output frames at each decoding step. We randomly split the audio set of each emotion in each speaker into training/validation/test set with the number of 300/30/20, resulting in a total of 15,000/1,500/1,000 training/validation/test proportion for whole dataset. We use the Adam optimizer with β1 = 0.9, β2 = 0.999 to optimize the model. The learning rate is set to 10 -3 before 100k steps, then exponentially decays to 10 -5 after 100k steps. We set the batch size to 32. The sample size in Eq. 3 is set to 20. The MTL-ETTS and CET-ETTS systems are trained with 300k steps.\n\nFor our i-ETTS, we first pre-train a SER  [28] , that reports a classification accuracy of 90.4% for all emotions on the test set. During the iterative learning, i-ETTS model is first pre-trained  200k steps via MLE, then the iterative training runs until the performance on validation set converges. Note that we set the number of tokens in GST module to 5 to be consistent with the number of emotion categories in ESD dataset. The GST module produces a 256-dimensional emotion embedding. At run-time, the task is to generate emotional speech by selecting a reference audio with the desired style. In this paper, we follow  [33]  and manually specify the weight of style tokens by averaging the style token weights of the test set for each emotion. All systems use Griffin-Lim algorithm  [26]  with 64 iterations.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Experiment Results",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Objective Evaluation",
      "text": "We first evaluate the emotion-discriminative ability of the synthesized speech with SER accuracy among the baselines MTL-ETTS, CET-ETTS and the proposed i-ETTS. As reported in Fig.  2 , the proposed i-ETTS consistently outperforms two baselines by achieving average accuracy of 82.8%, which is much higher than that of the MTL-ETTS (73.8%) and CET-ETTS (78.2%). The results show the effectiveness of our proposed model in terms of emotion expressiveness and discriminability.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Subjective Evaluation",
      "text": "We first conduct mean opinion score (MOS) listening experiments to compare overall emotion expression among different frameworks. The MOS values are calculated by taking the arithmetic average of all scores provided by the subjects. We keep the linguistic content same among different models to exclude other interference factors. We invite 15 subjects to participate in these experiments. Each listener listened to 100 synthesized speech samples. As shown in Fig.  3 , we observe that the proposed i-ETTS achieves remarkable results by outperforming both MTL-ETTS and CET-ETTS baseline systems for all emotion categories.\n\nWe also conduct A/B preference experiments as reported in 4 to evaluate the emotion expressiveness among MTL-ETTS, CET-ETTS and i-ETTS systems. We ask listeners to listen to all speech samples from different systems, and then choose the preferred one in terms of emotion expression. We also invite 15 subjects to this experiment. Each listener listened to 100 synthesized speech samples. Consistent with the previous experiments, the preference test results in Fig.  4  show that our proposed i-ETTS system can generate more expressive emotional speech, and always achieves better results than MTL-ETTS and CET-ETTS systems in all five emotion categories. All the above observations validate the effectiveness of our proposed i-ETTS system in terms of emotion expressiveness and discriminability.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "We have studied an interactive training paradigm for emotional TTS, denoted as i-ETTS, to synthesize emotional speech with accurate emotion category. In doing so, we devise a training strategy under reinforcement learning, which employs an interactive game between ETTS and SER. We formulate a policy gradient strategy and a reward function correlated with the SER accuracy. A series of experiments were conducted to evaluate the emotion expression. The proposed i-ETTS achieves remarkable performance by consistently outperforming the ETTS baseline systems in terms of voice quality and emotion discriminability. Future work includes improving the performance by investigating more effective ways to optimize the ETTS model.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Block diagram of the proposed interactive paradigm for emotional text-to-speech synthesis with reinforcement learning.",
      "page": 2
    },
    {
      "caption": "Figure 1: , the ETTS model can be viewed as an agent under",
      "page": 2
    },
    {
      "caption": "Figure 1: , our emotional text-to-",
      "page": 2
    },
    {
      "caption": "Figure 1: , the SER network",
      "page": 2
    },
    {
      "caption": "Figure 2: Confusion matrices of synthesized speech from MTL-ETTS, CET-ETTS and the proposed i-ETTS. The X-axis and Y-axis of",
      "page": 4
    },
    {
      "caption": "Figure 3: The MOS test results of MTL-ETTS, CET-ETTS and i-",
      "page": 4
    },
    {
      "caption": "Figure 2: , the proposed i-ETTS consistently outperforms two baselines",
      "page": 4
    },
    {
      "caption": "Figure 4: The preference test results between 1) MTL-ETTS",
      "page": 4
    },
    {
      "caption": "Figure 4: show that",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "haizhou.li@nus.edu.sg\nliurui imu@163.com, berrak sisman@sutd.edu.sg,"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "with mean\nsquare\nerror\n(MSE)\nloss, which\ncalculates\nthe\nAbstract"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "distance between the synthesized emotional speech and natural"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "Emotional\ntext-to-speech\nsynthesis\n(ETTS)\nhas\nseen much"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "reference. We note that\nsuch MSE-based objective function"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "progress in recent years. However, the generated voice is often"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "does not focus directly on emotion discriminability, hence often"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "not perceptually identiﬁable by its intended emotion category."
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "face emotion confusion problem in generated speech."
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "To address this problem, we propose a new interactive training"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "We are motivated by the fact\nthat\nthe interaction of speech"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "paradigm for ETTS, denoted as i-ETTS, which seeks to directly"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "emotion recognition model with ETTS can improve the emotion"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "improve\nthe\nemotion\ndiscriminability\nby\ninteracting with\na"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "discriminability of synthesized speech, hence overcome emo-"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "speech emotion recognition (SER) model. Moreover, we for-"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "tion confusion problem. We note that ETTS can be trained with"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "mulate an iterative training strategy with reinforcement learning"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "SER classiﬁcation result in a supervised manner. However, this"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "to ensure the quality of\ni-ETTS optimization.\nExperimental"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "may require a large amount of emotion-labeled speech data, that"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "results demonstrate that\nthe proposed i-ETTS outperforms the"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "limits the scope of applications. Therefore,\nthis paper studies"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "state-of-the-art baselines by rendering speech with more accu-"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "the use of\nreinforcement\nlearning for ETTS in an interactive"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "rate emotion style. To our best knowledge, this is the ﬁrst study"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "manner for improved emotion discriminability."
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "of reinforcement learning in emotional text-to-speech synthesis."
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "The\nreinforcement\nlearning\n(RL)\nalgorithm [14]\nlearns\nIndex Terms:\nReinforcement Learning, Emotional Text-to-"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "how to\nachieve\na\ncomplex\ngoal\nin\nan\ninteractive manner.\nSpeech Synthesis, Speech Emotion Recognition."
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "Speciﬁcally, RL involves\nagents\nto\nlearn\ntheir\nbehavior\nby"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "trial and error\n[14]. RL agents aim to learn decision-making"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "1.\nIntroduction"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "by successfully interacting with the environment where they"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "Emotional\ntext-to-speech (ETTS) seeks to synthesize human-\noperate. It has enabled speech processing systems [15] through"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "like natural-sounding voice for a given input\ntext with desired\na well designed feedback that\nreﬂects appropriate perceptual"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "emotional expression. The recent advances have enabled many\nmetrics in speech enhancement [16,17], speech recognition [18]"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "applications such as virtual assistants, call centers, dubbing of\nand speaker recognition [19]. We note that the use of interactive"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "movies and games, audiobook narration, and online education.\nparadigm with RL algorithm in emotional TTS remains to be"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "The early studies of ETTS are based on hidden Markov\nexplored, which will be the focus of this paper."
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "models\n[1–3].\nFor example, we can synthesize speech with"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "In this paper, we propose a novel ETTS framework with"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "a desired emotion through model\ninterpolation [2] or by in-"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "reinforcement\nlearning, denoted as i-ETTS. The proposed ap-"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "corporating\nunsupervised\nexpression\ncluster\nduring\ntraining"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "proach aims to overcome emotion confusion problem of syn-"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "[3].\nRecently, deep learning opens up many possibilities\nfor"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "thesized speech by optimizing the ETTS model\nthrough an"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "ETTS [4, 5], where\nemotion\ncodes\ncan\nbe\nused\nas\ncontrol"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "interaction with a SER model. The proposed idea is expected"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "vectors to change text-to-speech (TTS) output in different ways."
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "to reduce the requirement of emotion-labeled training data size."
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "Successful attempts include global style tokens (GST) that have"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "Experimental results demonstrate that i-ETTS consistently out-"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "been used to control\nthe expressiveness in emotional TTS\n[6–"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "performs the state-of-the-art baselines, by achieving improved"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "8]. Such approaches typically use style embedding to indicate"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "emotion discriminability of synthesized speech."
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "the emotion rendering and can learn speech variations\nin an"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "The main contributions of\nthis paper are summarized as"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "unsupervised manner. We note that the latent style embeddings"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "follows: 1) we introduce a novel\ninteractive emotional\ntext-to-"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "has no explicit meaning, hence lacks interpretability. Therefore,"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "speech synthesis paradigm that overcomes emotion confusion"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "GST-Tacotron studies often face emotion confusion problem"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "problem; 2) we formulate the training problem with RL, and"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "[9],\ni.e.,\nthe projected emotion in the synthetic speech is not"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "optimize the ETTS model with policy gradient and a reward"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "rendered accurately."
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "function correlated with the SER accuracy;\n3) we utilize an"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "Recently, some approaches [10–12] were proposed to en-"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "iterative training strategy for a stable training procedure. To our"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "hance the interpretability of\nthe style embedding.\nThese ap-"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "best knowledge, this is the ﬁrst study of reinforcement learning"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "proaches add an additional emotion recognition loss\n[10, 11]"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "in emotional speech synthesis."
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "or perceptual\nloss [12]\nto force the latent style embedding to"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "This\npaper\nis\norganized\nas\nfollows.\nWe motivate\nour"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "pay more\nattention to the\nemotion rendering.\nThese works"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "study\nthrough\nthe\ncomparison with\nexisting RL and ETTS"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "have made a great contribution to the development of ETTS."
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "frameworks in Section 2. We formulate the proposed paradigm"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "However,\nthey mostly focus on the hidden features\n[13], not"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "in Section 3. We report\nthe results of a systematic evaluation"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "the explicit output; and optimize the output acoustic features"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "and comparison in Sections 4. Finally, Section 5 concludes the"
        },
        {
          "3 Machine Listening Lab, University of Bremen, Germany": "Speech samples: https://ttslr.github.io/i-ETTS.\nstudy."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Decoder": "Sequence\nEncoder"
        },
        {
          "Decoder": ""
        },
        {
          "Decoder": ""
        },
        {
          "Decoder": "2. Related Work"
        },
        {
          "Decoder": "The reinforcement\nlearning (RL) algorithm takes an action in"
        },
        {
          "Decoder": "an environment\nin order\nto maximize the cumulative reward"
        },
        {
          "Decoder": ""
        },
        {
          "Decoder": "[14].\nIt has been successfully applied to various\ntasks,\nsuch"
        },
        {
          "Decoder": ""
        },
        {
          "Decoder": "as computer game [20], natural\nlanguage processing [21], and"
        },
        {
          "Decoder": ""
        },
        {
          "Decoder": "speech processing [15–19].\nFor example, RL has been used"
        },
        {
          "Decoder": ""
        },
        {
          "Decoder": "to optimize\nspeech enhancement\nsystem based on character"
        },
        {
          "Decoder": ""
        },
        {
          "Decoder": "recognition error. Moreover,\nit has also been used for DNN-"
        },
        {
          "Decoder": ""
        },
        {
          "Decoder": "based\nsource\nenhancement\n[16] where\nthe\nobjective\nsound"
        },
        {
          "Decoder": ""
        },
        {
          "Decoder": "quality assessment\nscores are given as\nthe reward.\nLast but"
        },
        {
          "Decoder": ""
        },
        {
          "Decoder": "not\nleast, RL-based methods\nhave\nalso\nbeen\nproposed\nfor"
        },
        {
          "Decoder": ""
        },
        {
          "Decoder": "speech\nrecognition\n[18]\nand\nspeaker\nrecognition\n[19].\nWe"
        },
        {
          "Decoder": ""
        },
        {
          "Decoder": "note that these approaches utilize limited or unlabeled data and"
        },
        {
          "Decoder": ""
        },
        {
          "Decoder": "optimize\nthe\ntarget model by interacting with the\nfeedback,"
        },
        {
          "Decoder": ""
        },
        {
          "Decoder": "which correlates with an evaluation metric from a third party"
        },
        {
          "Decoder": ""
        },
        {
          "Decoder": "directly."
        },
        {
          "Decoder": ""
        },
        {
          "Decoder": "With the advent of deep learning, end-to-end TTS systems,"
        },
        {
          "Decoder": ""
        },
        {
          "Decoder": "such as Tacotron [22], Tacotron2 [23] and their variants [24,25]"
        },
        {
          "Decoder": ""
        },
        {
          "Decoder": "greatly improve the voice quality of\nthe synthesized speech."
        },
        {
          "Decoder": ""
        },
        {
          "Decoder": "However,\nit remains a challenge as to how we generate speech"
        },
        {
          "Decoder": ""
        },
        {
          "Decoder": "with an intended emotion due\nto the\nfact\nthat\nit\nis hard to"
        },
        {
          "Decoder": ""
        },
        {
          "Decoder": "interpret the learned style embedding."
        },
        {
          "Decoder": ""
        },
        {
          "Decoder": "Some recent\nstudies make use of SER loss\n[10, 11] and"
        },
        {
          "Decoder": ""
        },
        {
          "Decoder": "perceptual\nloss\n[12]\nto assign speciﬁc emotional\ninformation"
        },
        {
          "Decoder": ""
        },
        {
          "Decoder": "to the style embedding.\nHowever,\nthe commonly used MSE"
        },
        {
          "Decoder": ""
        },
        {
          "Decoder": "loss for output feature does not directly improve the perceptual"
        },
        {
          "Decoder": ""
        },
        {
          "Decoder": "discriminability of\nthe synthesized speech.\nIn this paper, we"
        },
        {
          "Decoder": ""
        },
        {
          "Decoder": "devise a training strategy under reinforcement\nlearning, which"
        },
        {
          "Decoder": ""
        },
        {
          "Decoder": "employs an interactive game between the ETTS system and a"
        },
        {
          "Decoder": ""
        },
        {
          "Decoder": "SER to directly improve emotion discriminability."
        },
        {
          "Decoder": ""
        },
        {
          "Decoder": "3.\nInteractive ETTS: Methodology"
        },
        {
          "Decoder": "We propose a novel interactive training paradigm for emotional"
        },
        {
          "Decoder": "TTS under\nthe reinforcement\nlearning framework, denoted as"
        },
        {
          "Decoder": "i-ETTS. Our method is a combination of ETTS and RL, hence"
        },
        {
          "Decoder": "we ﬁrst present\nthe main components of proposed i-ETTS that"
        },
        {
          "Decoder": "includes agent, policy, action and reward. We then introduce"
        },
        {
          "Decoder": "the action update method, denoted as policy gradient. Finally,"
        },
        {
          "Decoder": "we describe the iterative training algorithm."
        },
        {
          "Decoder": "3.1.\ni-ETTS: Interactive ETTS with RL"
        },
        {
          "Decoder": "As in Fig. 1, the ETTS model can be viewed as an agent under"
        },
        {
          "Decoder": "the reinforcement\nlearning framework. The parameters of this"
        },
        {
          "Decoder": "agent deﬁne a policy. The policy aims to predict the emotional"
        },
        {
          "Decoder": "acoustic features at each time step.\nThe emotional acoustic"
        },
        {
          "Decoder": "features deﬁne the action. After the emotional acoustic feature"
        },
        {
          "Decoder": "prediction is ﬁnished, the pre-trained SER starts to feedback the"
        },
        {
          "Decoder": "emotion recognition accuracy, which is denoted as reward. The"
        },
        {
          "Decoder": "policy gradient strategy was used to perform back-propagation"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "token, the pre-trained SER is used to evaluate how well the gen-",
          "Algorithm 1:Iterative Training Algorithm.": ""
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "",
          "Algorithm 1:Iterative Training Algorithm.": "Input: Training set: D = {x, y, l}"
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "erated mel spectrum feature y(cid:48)\ni matches the reference emotion",
          "Algorithm 1:Iterative Training Algorithm.": ""
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "",
          "Algorithm 1:Iterative Training Algorithm.": "x: character sequence"
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "label l ˆyi . Mathematically, the recognition probability pi about",
          "Algorithm 1:Iterative Training Algorithm.": ""
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "",
          "Algorithm 1:Iterative Training Algorithm.": "y: acoustic feature sequence"
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "i is formulated as:\nthe target emotion l ˆyi of y(cid:48)",
          "Algorithm 1:Iterative Training Algorithm.": ""
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "",
          "Algorithm 1:Iterative Training Algorithm.": "l: emotion label"
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "| y(cid:48)\n(1)\ni; ϕ(cid:1)\npi = SER (cid:0)l ˆyi",
          "Algorithm 1:Iterative Training Algorithm.": "Output: ETTS model: θ"
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "",
          "Algorithm 1:Iterative Training Algorithm.": "Begin"
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "where ϕ represents all\nthe parameters of SER,\nthat are pre-",
          "Algorithm 1:Iterative Training Algorithm.": ""
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "",
          "Algorithm 1:Iterative Training Algorithm.": "0. Pre-train SER model ϕ with (y, l)"
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "trained before the training of ETTS. Note that\nthe value of pi",
          "Algorithm 1:Iterative Training Algorithm.": ""
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "",
          "Algorithm 1:Iterative Training Algorithm.": "(cid:46) Pre-training"
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "ranges from 0 to 1.",
          "Algorithm 1:Iterative Training Algorithm.": ""
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "",
          "Algorithm 1:Iterative Training Algorithm.": "1: Pre-train ETTS model θ using MSE Loss with"
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "During ETTS training, we sample a ﬁxed number of pre-",
          "Algorithm 1:Iterative Training Algorithm.": ""
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "",
          "Algorithm 1:Iterative Training Algorithm.": "(x, y, l)"
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "dicted mel spectrum features y(cid:48) from a batch of data to calculate",
          "Algorithm 1:Iterative Training Algorithm.": ""
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "",
          "Algorithm 1:Iterative Training Algorithm.": "(cid:46) Iterative-training"
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "the SER reward, which is formulated as:",
          "Algorithm 1:Iterative Training Algorithm.": ""
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "",
          "Algorithm 1:Iterative Training Algorithm.": "3: for epoch = 1,2,...N do"
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "(cid:80)K",
          "Algorithm 1:Iterative Training Algorithm.": ""
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "",
          "Algorithm 1:Iterative Training Algorithm.": "4.\nSample a batch B in D"
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "i=1 1(pi > λ)",
          "Algorithm 1:Iterative Training Algorithm.": ""
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "N K\nR =\n=\n(2)",
          "Algorithm 1:Iterative Training Algorithm.": ""
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "",
          "Algorithm 1:Iterative Training Algorithm.": "5.\nSample (x, y, l) from B"
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "K",
          "Algorithm 1:Iterative Training Algorithm.": ""
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "",
          "Algorithm 1:Iterative Training Algorithm.": "6.\nGenerate mel spectrum features y(cid:48) via θ"
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "where R is the reward of the sampled mel spectrum features y(cid:48)",
          "Algorithm 1:Iterative Training Algorithm.": ""
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "",
          "Algorithm 1:Iterative Training Algorithm.": "7.\nCompute reward R based on Eq. 2"
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "from ETTS model. Note that the value of reward R also ranges",
          "Algorithm 1:Iterative Training Algorithm.": ""
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "",
          "Algorithm 1:Iterative Training Algorithm.": "8.\nUpdate θ using R based on Eq. 3"
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "from 0 to 1. K denotes the sample size. λ is a threshold that",
          "Algorithm 1:Iterative Training Algorithm.": ""
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "",
          "Algorithm 1:Iterative Training Algorithm.": "9.\nUpdate θ using MSE Loss with (x, y, l)"
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "is set\nto 0.5 in this work. N represents the number of samples",
          "Algorithm 1:Iterative Training Algorithm.": ""
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "",
          "Algorithm 1:Iterative Training Algorithm.": "10: end for"
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "the target emotion exceeds the\nof which the probability pi of",
          "Algorithm 1:Iterative Training Algorithm.": ""
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "",
          "Algorithm 1:Iterative Training Algorithm.": "End"
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "threshold λ. 1(·) is an indicator function of a set {0, 1}.",
          "Algorithm 1:Iterative Training Algorithm.": ""
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "3.1.3. Action update: Policy Gradient",
          "Algorithm 1:Iterative Training Algorithm.": ""
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "",
          "Algorithm 1:Iterative Training Algorithm.": "at 16 kHz and coded in 16 bits. We note that ESD is considered"
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "We use policy gradient algorithm [29] to estimate the gradient",
          "Algorithm 1:Iterative Training Algorithm.": ""
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "",
          "Algorithm 1:Iterative Training Algorithm.": "to be large enough for various voice conversion tasks [31], while"
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "that\nleads\nto a larger expected reward E[R] of\nthe generated",
          "Algorithm 1:Iterative Training Algorithm.": ""
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "",
          "Algorithm 1:Iterative Training Algorithm.": "it provides limited data for TTS and ETTS tasks."
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "acoustic features y(cid:48)\nfor input\ntext x. Note that\nthe gradient of",
          "Algorithm 1:Iterative Training Algorithm.": ""
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "the ETTS model P (·) w.r.t.\nthe model parameters θ is estimated",
          "Algorithm 1:Iterative Training Algorithm.": ""
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "",
          "Algorithm 1:Iterative Training Algorithm.": "4.1. Comparative Study"
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "by sampling as follows:",
          "Algorithm 1:Iterative Training Algorithm.": ""
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "",
          "Algorithm 1:Iterative Training Algorithm.": "We implement two baseline systems together with the proposed"
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "| x; θ(cid:1) R\n∇θE[R] = ∇θP (cid:0)y(cid:48)",
          "Algorithm 1:Iterative Training Algorithm.": "i-ETTS, as summarized next."
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "= P (cid:0)y(cid:48)\n| x; θ(cid:1)(cid:1)\n(3)\n| x; θ(cid:1) R ∇θ log (cid:0)P (cid:0)y(cid:48)",
          "Algorithm 1:Iterative Training Algorithm.": "• MTL-ETTS [11]: An emotional TTS model\nthat\njointly"
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "",
          "Algorithm 1:Iterative Training Algorithm.": "trains an auxiliary SER task with the TTS model;"
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "| x; θ(cid:1)(cid:1)\n(cid:39) R ∇θ log (cid:0)P (cid:0)y(cid:48)",
          "Algorithm 1:Iterative Training Algorithm.": ""
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "",
          "Algorithm 1:Iterative Training Algorithm.": "• CET-ETTS [12]: An emotional TTS model that uses two"
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "Unlike the traditional gradient descent algorithm, the policy",
          "Algorithm 1:Iterative Training Algorithm.": "reference encoders with SER module and perceptual loss"
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "gradient algorithm estimates the weights of an optimal policy",
          "Algorithm 1:Iterative Training Algorithm.": "to enhance the emotion-discriminative ability;"
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "through gradient ascent.\nNote that\nit can assign an explicit",
          "Algorithm 1:Iterative Training Algorithm.": ""
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "",
          "Algorithm 1:Iterative Training Algorithm.": "•\ni-ETTS:\nthe proposed i-ETTS that optimizes the ETTS"
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "emotion-aware\nsupervised\nsignal\nto\nthe\ngradient\nduring\nthe",
          "Algorithm 1:Iterative Training Algorithm.": ""
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "",
          "Algorithm 1:Iterative Training Algorithm.": "model with a reward function correlated with the SER"
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "model\ntraining,\nresulting in emotional\nspeech with accurate",
          "Algorithm 1:Iterative Training Algorithm.": ""
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "",
          "Algorithm 1:Iterative Training Algorithm.": "accuracy."
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "emotion category.",
          "Algorithm 1:Iterative Training Algorithm.": ""
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "",
          "Algorithm 1:Iterative Training Algorithm.": "For a fair comparison, all\nframeworks use the same SER"
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "3.2.\nIterative Training",
          "Algorithm 1:Iterative Training Algorithm.": "module,\nas also illustrated in Section 3.1.2,\nand Grifﬁn-Lim"
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "",
          "Algorithm 1:Iterative Training Algorithm.": "algorithm [26] is used for waveform generation. We note that"
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "In practice,\nit\nis hard to train the whole network from scratch",
          "Algorithm 1:Iterative Training Algorithm.": ""
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "",
          "Algorithm 1:Iterative Training Algorithm.": "the use of neural vocoder will further improve the speech quality"
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "with policy gradient since the ETTS model may ﬁnd an unex-",
          "Algorithm 1:Iterative Training Algorithm.": ""
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "",
          "Algorithm 1:Iterative Training Algorithm.": "[32], which is not the main focus of this paper."
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "pected way to achieve a high reward but\nfail\nto guarantee the",
          "Algorithm 1:Iterative Training Algorithm.": ""
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "naturalness or robustness of the synthesized speech [21]. There-",
          "Algorithm 1:Iterative Training Algorithm.": ""
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "",
          "Algorithm 1:Iterative Training Algorithm.": "4.2. Experimental Setup"
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "fore, we formulate an iterative training algorithm,\nreported in",
          "Algorithm 1:Iterative Training Algorithm.": ""
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "Algorithm 1, that contains two main steps: 1) pre-training, and",
          "Algorithm 1:Iterative Training Algorithm.": "For all\nsystems,\nthe encoder\ntakes\nthe character\nsequence as"
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "2)\niterative-training.\nFirst, we use MSE Loss to pre-train the",
          "Algorithm 1:Iterative Training Algorithm.": "input. The 80-channel mel spectrum and 1025-channel\nlinear-"
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "ETTS model with text-wav pairs from the training set. After",
          "Algorithm 1:Iterative Training Algorithm.": "spectrum are\nextracted with\n12.5ms\nframe\nshift\nand\n50ms"
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "the pre-training phase, iterative-training phase aims to optimize",
          "Algorithm 1:Iterative Training Algorithm.": "frame length.\nThe decoder predicts 5 output\nframes at each"
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "the ETTS model with reward and the MSE loss alternatively.",
          "Algorithm 1:Iterative Training Algorithm.": "decoding\nstep.\nWe\nrandomly\nsplit\nthe\naudio\nset\nof\neach"
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "",
          "Algorithm 1:Iterative Training Algorithm.": "emotion in each speaker into training/validation/test set with the"
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "",
          "Algorithm 1:Iterative Training Algorithm.": "number of 300/30/20, resulting in a total of 15,000/1,500/1,000"
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "4. Experiments",
          "Algorithm 1:Iterative Training Algorithm.": ""
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "",
          "Algorithm 1:Iterative Training Algorithm.": "training/validation/test proportion for whole dataset. We use"
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "We report emotional TTS experiments on ESD database [30],",
          "Algorithm 1:Iterative Training Algorithm.": "the Adam optimizer with β1 = 0.9, β2 = 0.999 to optimize the"
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "which is a new publicly available emotional speech dataset for",
          "Algorithm 1:Iterative Training Algorithm.": "model. The learning rate is set to 10−3 before 100k steps, then"
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "emotional speech synthesis. ESD is a multi-lingual dataset and",
          "Algorithm 1:Iterative Training Algorithm.": "exponentially decays to 10−5 after 100k steps. We set the batch"
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "has 350 parallel utterances spoken by 10 native English and 10",
          "Algorithm 1:Iterative Training Algorithm.": "size to 32. The sample size in Eq.\n3 is set\nto 20. The MTL-"
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "native Mandarin speakers. We use the English corpora with",
          "Algorithm 1:Iterative Training Algorithm.": "ETTS and CET-ETTS systems are trained with 300k steps."
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "a total of nearly 13 hours of speech by 5 male and 5 female",
          "Algorithm 1:Iterative Training Algorithm.": "For our i-ETTS, we ﬁrst pre-train a SER [28], that reports a"
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "speakers in ﬁve emotions, namely happy, angry, neutral, sad and",
          "Algorithm 1:Iterative Training Algorithm.": "classiﬁcation accuracy of 90.4% for all emotions on the test set."
        },
        {
          "Speciﬁcally, upon generating the end-of-sequence (EOS)": "surprise. In ESD, scripts are provided, speech data are sampled",
          "Algorithm 1:Iterative Training Algorithm.": "During the iterative learning,\ni-ETTS model\nis ﬁrst pre-trained"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "The preference test\nresults between 1) MTL-ETTS\nFigure 4:": "i-ETTS for ﬁve\nemotion\nvs.\ni-ETTS, and 2) CET-ETTS vs."
        },
        {
          "The preference test\nresults between 1) MTL-ETTS\nFigure 4:": ""
        },
        {
          "The preference test\nresults between 1) MTL-ETTS\nFigure 4:": "categories, with 95% conﬁdence interval."
        },
        {
          "The preference test\nresults between 1) MTL-ETTS\nFigure 4:": ""
        },
        {
          "The preference test\nresults between 1) MTL-ETTS\nFigure 4:": "the preferred one in terms of emotion expression. We also"
        },
        {
          "The preference test\nresults between 1) MTL-ETTS\nFigure 4:": ""
        },
        {
          "The preference test\nresults between 1) MTL-ETTS\nFigure 4:": "invite 15 subjects to this experiment. Each listener listened to"
        },
        {
          "The preference test\nresults between 1) MTL-ETTS\nFigure 4:": ""
        },
        {
          "The preference test\nresults between 1) MTL-ETTS\nFigure 4:": "100 synthesized speech samples. Consistent with the previous"
        },
        {
          "The preference test\nresults between 1) MTL-ETTS\nFigure 4:": ""
        },
        {
          "The preference test\nresults between 1) MTL-ETTS\nFigure 4:": "experiments,\nthe preference test\nresults\nin Fig.\n4 show that"
        },
        {
          "The preference test\nresults between 1) MTL-ETTS\nFigure 4:": ""
        },
        {
          "The preference test\nresults between 1) MTL-ETTS\nFigure 4:": "i-ETTS\nour\nproposed\nsystem can\ngenerate more\nexpressive"
        },
        {
          "The preference test\nresults between 1) MTL-ETTS\nFigure 4:": ""
        },
        {
          "The preference test\nresults between 1) MTL-ETTS\nFigure 4:": "emotional speech, and always achieves better results than MTL-"
        },
        {
          "The preference test\nresults between 1) MTL-ETTS\nFigure 4:": ""
        },
        {
          "The preference test\nresults between 1) MTL-ETTS\nFigure 4:": "ETTS and CET-ETTS systems in all ﬁve emotion categories."
        },
        {
          "The preference test\nresults between 1) MTL-ETTS\nFigure 4:": ""
        },
        {
          "The preference test\nresults between 1) MTL-ETTS\nFigure 4:": "All\nthe\nabove observations validate\nthe\neffectiveness of our"
        },
        {
          "The preference test\nresults between 1) MTL-ETTS\nFigure 4:": ""
        },
        {
          "The preference test\nresults between 1) MTL-ETTS\nFigure 4:": "proposed i-ETTS system in terms of emotion expressiveness and"
        },
        {
          "The preference test\nresults between 1) MTL-ETTS\nFigure 4:": ""
        },
        {
          "The preference test\nresults between 1) MTL-ETTS\nFigure 4:": "discriminability."
        },
        {
          "The preference test\nresults between 1) MTL-ETTS\nFigure 4:": "5. Conclusion"
        },
        {
          "The preference test\nresults between 1) MTL-ETTS\nFigure 4:": ""
        },
        {
          "The preference test\nresults between 1) MTL-ETTS\nFigure 4:": "We have studied an interactive training paradigm for emotional"
        },
        {
          "The preference test\nresults between 1) MTL-ETTS\nFigure 4:": ""
        },
        {
          "The preference test\nresults between 1) MTL-ETTS\nFigure 4:": "TTS, denoted as i-ETTS,\nto synthesize emotional speech with"
        },
        {
          "The preference test\nresults between 1) MTL-ETTS\nFigure 4:": "accurate\nemotion category.\nIn doing so, we devise\na\ntrain-"
        },
        {
          "The preference test\nresults between 1) MTL-ETTS\nFigure 4:": "ing strategy under\nreinforcement\nlearning, which employs an"
        },
        {
          "The preference test\nresults between 1) MTL-ETTS\nFigure 4:": "interactive\ngame\nbetween ETTS and SER. We\nformulate\na"
        },
        {
          "The preference test\nresults between 1) MTL-ETTS\nFigure 4:": "policy gradient strategy and a reward function correlated with"
        },
        {
          "The preference test\nresults between 1) MTL-ETTS\nFigure 4:": "the SER accuracy.\nA series of experiments were conducted"
        },
        {
          "The preference test\nresults between 1) MTL-ETTS\nFigure 4:": "i-ETTS\nto\nevaluate\nthe\nemotion\nexpression.\nThe\nproposed"
        },
        {
          "The preference test\nresults between 1) MTL-ETTS\nFigure 4:": "achieves remarkable performance by consistently outperform-"
        },
        {
          "The preference test\nresults between 1) MTL-ETTS\nFigure 4:": "ing the ETTS baseline systems in terms of voice quality and"
        },
        {
          "The preference test\nresults between 1) MTL-ETTS\nFigure 4:": "emotion discriminability. Future work includes improving the"
        },
        {
          "The preference test\nresults between 1) MTL-ETTS\nFigure 4:": "performance by investigating more effective ways to optimize"
        },
        {
          "The preference test\nresults between 1) MTL-ETTS\nFigure 4:": "the ETTS model."
        },
        {
          "The preference test\nresults between 1) MTL-ETTS\nFigure 4:": ""
        },
        {
          "The preference test\nresults between 1) MTL-ETTS\nFigure 4:": ""
        },
        {
          "The preference test\nresults between 1) MTL-ETTS\nFigure 4:": "6. Acknowledgements"
        },
        {
          "The preference test\nresults between 1) MTL-ETTS\nFigure 4:": ""
        },
        {
          "The preference test\nresults between 1) MTL-ETTS\nFigure 4:": "The\nresearch\nis\nfunded\nby SUTD Start-up Grant Artiﬁcial"
        },
        {
          "The preference test\nresults between 1) MTL-ETTS\nFigure 4:": "Intelligence\nfor Human Voice Conversion (SRG ISTD 2020"
        },
        {
          "The preference test\nresults between 1) MTL-ETTS\nFigure 4:": "158) and SUTD AI Project\n(SGPAIRS1821) Discovery by AI"
        },
        {
          "The preference test\nresults between 1) MTL-ETTS\nFigure 4:": "- The Understanding and Synthesis of Expressive Speech by"
        },
        {
          "The preference test\nresults between 1) MTL-ETTS\nFigure 4:": "AI. This research by Haizhou Li supported by the Science and"
        },
        {
          "The preference test\nresults between 1) MTL-ETTS\nFigure 4:": "Engineering Research Council, Agency of Science, Technol-"
        },
        {
          "The preference test\nresults between 1) MTL-ETTS\nFigure 4:": "ogy and Research, Singapore,\nthrough the National Robotics"
        },
        {
          "The preference test\nresults between 1) MTL-ETTS\nFigure 4:": "Program under Grant No.\n192 25 00054 and Programmatic"
        },
        {
          "The preference test\nresults between 1) MTL-ETTS\nFigure 4:": "Grant No.\nA18A2b0046 from the Singapore Government’s"
        },
        {
          "The preference test\nresults between 1) MTL-ETTS\nFigure 4:": "Research,\nInnovation\nand Enterprise\n2020\nplan\n(Advanced"
        },
        {
          "The preference test\nresults between 1) MTL-ETTS\nFigure 4:": "Manufacturing and Engineering domain). Project Title: Human"
        },
        {
          "The preference test\nresults between 1) MTL-ETTS\nFigure 4:": "Robot Collaborative AI for AME."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "forcement\nlearning based speech enhancement for robust speech"
        },
        {
          "7. References": "[1] K. Tokuda, H. Zen, and A. W. Black, “An hmm-based speech",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "ICASSP 2019-2019\nIEEE International Con-\nrecognition,”\nin"
        },
        {
          "7. References": "synthesis system applied to english,” in IEEE Speech Synthesis",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "ference on Acoustics, Speech and Signal Processing (ICASSP)."
        },
        {
          "7. References": "Workshop, 2002, pp. 227–230.",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "IEEE, 2019, pp. 6750–6754."
        },
        {
          "7. References": "[2]\nJ. Yamagishi, K. Onishi, T. Masuko, and T. Kobayashi, “Modeling",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "[18]\nT. Kala\nand T. Shinozaki,\n“Reinforcement\nlearning of\nspeech"
        },
        {
          "7. References": "of various speaking styles and emotions for hmm-based speech",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "recognition\nsystem based\non\npolicy\ngradient\nand\nhypothesis"
        },
        {
          "7. References": "synthesis,” in Eighth European Conference on Speech Communi-",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "selection,” in ICASSP 2018-2018 IEEE International Conference"
        },
        {
          "7. References": "cation and Technology, 2003.",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "on Acoustics, Speech and Signal Processing (ICASSP).\nIEEE,"
        },
        {
          "7. References": "[3]\nF. Eyben, S. Buchholz, N. Braunschweiler,\nJ. Latorre, V. Wan,",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "2018, pp. 5759–5763."
        },
        {
          "7. References": "M. J. Gales, and K. Knill, “Unsupervised clustering of emotion",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "[19] M. Seurin, F. Strub, P. Preux, and O. Pietquin, “A machine of"
        },
        {
          "7. References": "ICASSP\n2012-2012\nand\nvoice\nstyles\nfor\nexpressive\ntts,”\nin",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "few words:\nInteractive speaker\nrecognition with reinforcement"
        },
        {
          "7. References": "IEEE International Conference on Acoustics, Speech and Signal",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "learning,” Proc. Interspeech 2020, pp. 4323–4327, 2020."
        },
        {
          "7. References": "Processing (ICASSP).\nIEEE, 2012, pp. 4009–4012.",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "[20] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G."
        },
        {
          "7. References": "[4]\nJ.\nLorenzo-Trueba, G.\nE. Henter,\nS.\nTakaki,\nJ. Yamagishi,",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Os-"
        },
        {
          "7. References": "Y\n. Morino, and Y. Ochiai, “Investigating different representations",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "trovski et al., “Human-level control\nthrough deep reinforcement"
        },
        {
          "7. References": "for modeling\nand controlling multiple\nemotions\nin\ndnn-based",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "learning,” nature, vol. 518, no. 7540, pp. 529–533, 2015."
        },
        {
          "7. References": "speech synthesis,” Speech Communication, 2018.",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "[21]\nF. Luo, P. Li,\nJ. Zhou, P. Yang, B. Chang, Z. Sui, and X. Sun,"
        },
        {
          "7. References": "[5] H. Choi, S. Park, J. Park, and M. Hahn, “Multi-speaker emotional",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "“A dual reinforcement\nlearning framework for unsupervised text"
        },
        {
          "7. References": "acoustic modeling for cnn-based speech synthesis,”\nin ICASSP",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "the 28th International\nJoint\nstyle\ntransfer,”\nin Proceedings of"
        },
        {
          "7. References": "2019-2019 IEEE International Conference on Acoustics, Speech",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "Conference on Artiﬁcial Intelligence, IJCAI 2019, 2019."
        },
        {
          "7. References": "and Signal Processing (ICASSP).\nIEEE, 2019, pp. 6950–6954.",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "[22] Y. Wang, R. Skerry-Ryan, D. Stanton, Y. Wu, R.\nJ. Weiss,"
        },
        {
          "7. References": "[6] W.-N. Hsu, Y. Zhang, R.\nJ. Weiss, H. Zen, Y. Wu, Y. Wang,",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "N. Jaitly, Z. Yang, Y. Xiao, Z. Chen, S. Bengio et al., “Tacotron:"
        },
        {
          "7. References": "Y\n. Cao, Y. Jia, Z. Chen, J. Shen et al., “Hierarchical generative",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "Towards end-to-end speech synthesis,” Proc.\nInterspeech 2017,"
        },
        {
          "7. References": "International\nmodeling\nfor\ncontrollable\nspeech\nsynthesis,”\nin",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "pp. 4006–4010, 2017."
        },
        {
          "7. References": "Conference on Learning Representations, 2018.",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "[23]\nJ. Shen, R. Pang, R. J. Weiss, M. Schuster, N. Jaitly, Z. Yang,"
        },
        {
          "7. References": "[7] Y.\nZhang,\nS.\nPan,\nL. He,\nand\nZ.\nLing,\n“Learning\nlatent",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "Z. Chen, Y. Zhang, Y. Wang, R. Skerrv-Ryan et al.,\n“Natural"
        },
        {
          "7. References": "representations for style control and transfer in end-to-end speech",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "tts\nsynthesis by conditioning wavenet on mel\nspectrogram pre-"
        },
        {
          "7. References": "synthesis,” in ICASSP 2019 - 2019 IEEE International Confer-",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "dictions,” in ICASSP 2018-2018 IEEE International Conference"
        },
        {
          "7. References": "ence on Acoustics, Speech and Signal Processing (ICASSP), 2019,",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "on Acoustics, Speech and Signal Processing (ICASSP).\nIEEE,"
        },
        {
          "7. References": "pp. 6945–6949.",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "2018, pp. 4779–4783."
        },
        {
          "7. References": "[8]\nS.-Y. Um,\nS. Oh,\nK. Byun,\nI.\nJang,\nC. Ahn,\nand H.-G.",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "[24] Y. Wang, D. Stanton, Y. Zhang, R.-S. Ryan, E. Battenberg,"
        },
        {
          "7. References": "Kang,\n“Emotional\nspeech synthesis with rich and granularized",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "J. Shor, Y. Xiao, Y.\nJia,\nF. Ren,\nand R. A. Saurous,\n“Style"
        },
        {
          "7. References": "control,” in ICASSP 2020-2020 IEEE International Conference",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "tokens: Unsupervised style modeling, control and transfer in end-"
        },
        {
          "7. References": "on Acoustics, Speech and Signal Processing (ICASSP).\nIEEE,",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "to-end speech synthesis,” in International Conference on Machine"
        },
        {
          "7. References": "2020, pp. 7254–7258.",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "Learning, 2018, pp. 5180–5189."
        },
        {
          "7. References": "[9] M. El Ayadi, M. S. Kamel,\nand F. Karray,\n“Survey on speech",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "[25] R. Liu, B. Sisman, J. Li, F. Bao, G. Gao, and H. Li, “Teacher-"
        },
        {
          "7. References": "emotion\nrecognition:\nFeatures,\nclassiﬁcation\nschemes,\nand",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "student training for robust tacotron-based TTS,” in ICASSP 2020-"
        },
        {
          "7. References": "databases,” Pattern recognition, vol. 44, no. 3, pp. 572–587, 2011.",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "2020 IEEE International Conference on Acoustics, Speech and"
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "Signal Processing (ICASSP).\nIEEE, 2020, pp. 6274–6278."
        },
        {
          "7. References": "[10]\nP. Wu, Z. Ling, L. Liu, Y.\nJiang, H. Wu,\nand L. Dai,\n“End-",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "to-end emotional speech synthesis using style tokens and semi-",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "[26] G. D and L.\nJ. S, “Signal estimation from modiﬁed short-time"
        },
        {
          "7. References": "supervised training,” in 2019 Asia-Paciﬁc Signal and Information",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "IEEE Transactions on Acoustics Speech &\nfourier\ntransform,”"
        },
        {
          "7. References": "Processing Association Annual Summit and Conference (APSIPA",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "Signal Processing, vol. 32, no. 2, pp. 236–243, 1984."
        },
        {
          "7. References": "ASC).\nIEEE, 2019, pp. 623–627.",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "[27] A. Graves,\n“Generating\nsequences with\nrecurrent\nneural\nnet-"
        },
        {
          "7. References": "[11] X. Cai, D. Dai, Z. Wu, X. Li, J. Li, and H. Meng, “Emotion con-",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "works,” arXiv preprint arXiv:1308.0850, 2013."
        },
        {
          "7. References": "trollable speech synthesis using emotion-unlabeled dataset with",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "[28] M. Chen, X. He,\nJ. Yang,\nand H. Zhang,\n“3-d convolutional"
        },
        {
          "7. References": "the assistance of cross-domain speech emotion recognition,” in",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "recurrent\nneural\nnetworks with\nattention model\nfor\nspeech"
        },
        {
          "7. References": "ICASSP 2021-2021 IEEE International Conference on Acoustics,",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "emotion recognition,” IEEE Signal Processing Letters, vol. 25,"
        },
        {
          "7. References": "Speech and Signal Processing (ICASSP).\nIEEE, 2021, pp. 1–5.",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "no. 10, pp. 1440–1444, 2018."
        },
        {
          "7. References": "[12]\nT. Li, S. Yang, L. Xue, and L. Xie, “Controllable emotion transfer",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "[29] R. J. Williams, “Simple statistical gradient-following algorithms"
        },
        {
          "7. References": "2021\n12th\nInternational\nfor\nend-to-end\nspeech\nsynthesis,”\nin",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "for\nconnectionist\nreinforcement\nlearning,” Machine\nlearning,"
        },
        {
          "7. References": "Symposium on Chinese Spoken Language Processing (ISCSLP).",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "vol. 8, no. 3-4, pp. 229–256, 1992."
        },
        {
          "7. References": "IEEE, 2021, pp. 1–5.",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "[30] K. Zhou, B. Sisman, R. Liu,\nand H. Li,\n“Seen\nand\nunseen"
        },
        {
          "7. References": "[13] R. Liu, B. Sisman, G. Gao, and H. Li, “Expressive tts training with",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "emotional style transfer for voice conversion with a new emotional"
        },
        {
          "7. References": "frame and style reconstruction loss,” IEEE/ACM Transactions on",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "speech dataset,” arXiv preprint arXiv:2010.14794, 2020."
        },
        {
          "7. References": "Audio, Speech, and Language Processing, vol. 29, pp. 1806–1818,",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "[31] B. Sisman, S. King,\nJ. Yamagishi, and H. Li, “An overview of"
        },
        {
          "7. References": "2021.",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "voice conversion and its challenges: From statistical modeling to"
        },
        {
          "7. References": "Reinforcement\nlearning:\nAn\n[14] R. S. Sutton\nand A. G. Barto,",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "deep learning,” IEEE/ACM Transactions on Audio, Speech, and"
        },
        {
          "7. References": "introduction.\nMIT press, 2018.",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "Language Processing, vol. 29, pp. 132–157, 2021."
        },
        {
          "7. References": "[15]\nS. Latif, H. Cuay´ahuitl, F. Pervez, F. Shamshad, H. S. Ali, and",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "[32] Y. Zhou, X. Tian,\nand H. Li,\n“Multi-task wavernn with\nan"
        },
        {
          "7. References": "E. Cambria, “A survey on deep reinforcement learning for audio-",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "integrated architecture for cross-lingual voice conversion,” IEEE"
        },
        {
          "7. References": "based applications,” arXiv preprint arXiv:2101.00240, 2021.",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "Signal Processing Letters, vol. 27, pp. 1310–1314, 2020."
        },
        {
          "7. References": "[16] Y. Koizumi, K. Niwa, Y. Hioka, K. Kobayashi, and Y. Haneda,",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "[33] O. Kwon,\nI. Jang, C. Ahn, and H.-G. Kang, “An effective style"
        },
        {
          "7. References": "“Dnn-based\nsource\nenhancement\nto\nincrease\nobjective\nsound",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "token weight control\ntechnique for end-to-end emotional speech"
        },
        {
          "7. References": "IEEE/ACM Transactions on Audio,\nquality assessment\nscore,”",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "synthesis,” IEEE Signal Processing Letters, vol. 26, no. 9, pp."
        },
        {
          "7. References": "Speech, and Language Processing, vol. 26, no. 10, pp. 1780–",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "1383–1387, 2019."
        },
        {
          "7. References": "1792, 2018.",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "forcement\nlearning based speech enhancement for robust speech"
        },
        {
          "7. References": "[1] K. Tokuda, H. Zen, and A. W. Black, “An hmm-based speech",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "ICASSP 2019-2019\nIEEE International Con-\nrecognition,”\nin"
        },
        {
          "7. References": "synthesis system applied to english,” in IEEE Speech Synthesis",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "ference on Acoustics, Speech and Signal Processing (ICASSP)."
        },
        {
          "7. References": "Workshop, 2002, pp. 227–230.",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "IEEE, 2019, pp. 6750–6754."
        },
        {
          "7. References": "[2]\nJ. Yamagishi, K. Onishi, T. Masuko, and T. Kobayashi, “Modeling",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "[18]\nT. Kala\nand T. Shinozaki,\n“Reinforcement\nlearning of\nspeech"
        },
        {
          "7. References": "of various speaking styles and emotions for hmm-based speech",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "recognition\nsystem based\non\npolicy\ngradient\nand\nhypothesis"
        },
        {
          "7. References": "synthesis,” in Eighth European Conference on Speech Communi-",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "selection,” in ICASSP 2018-2018 IEEE International Conference"
        },
        {
          "7. References": "cation and Technology, 2003.",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "on Acoustics, Speech and Signal Processing (ICASSP).\nIEEE,"
        },
        {
          "7. References": "[3]\nF. Eyben, S. Buchholz, N. Braunschweiler,\nJ. Latorre, V. Wan,",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "2018, pp. 5759–5763."
        },
        {
          "7. References": "M. J. Gales, and K. Knill, “Unsupervised clustering of emotion",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "[19] M. Seurin, F. Strub, P. Preux, and O. Pietquin, “A machine of"
        },
        {
          "7. References": "ICASSP\n2012-2012\nand\nvoice\nstyles\nfor\nexpressive\ntts,”\nin",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "few words:\nInteractive speaker\nrecognition with reinforcement"
        },
        {
          "7. References": "IEEE International Conference on Acoustics, Speech and Signal",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "learning,” Proc. Interspeech 2020, pp. 4323–4327, 2020."
        },
        {
          "7. References": "Processing (ICASSP).\nIEEE, 2012, pp. 4009–4012.",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "[20] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G."
        },
        {
          "7. References": "[4]\nJ.\nLorenzo-Trueba, G.\nE. Henter,\nS.\nTakaki,\nJ. Yamagishi,",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Os-"
        },
        {
          "7. References": "Y\n. Morino, and Y. Ochiai, “Investigating different representations",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "trovski et al., “Human-level control\nthrough deep reinforcement"
        },
        {
          "7. References": "for modeling\nand controlling multiple\nemotions\nin\ndnn-based",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "learning,” nature, vol. 518, no. 7540, pp. 529–533, 2015."
        },
        {
          "7. References": "speech synthesis,” Speech Communication, 2018.",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "[21]\nF. Luo, P. Li,\nJ. Zhou, P. Yang, B. Chang, Z. Sui, and X. Sun,"
        },
        {
          "7. References": "[5] H. Choi, S. Park, J. Park, and M. Hahn, “Multi-speaker emotional",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "“A dual reinforcement\nlearning framework for unsupervised text"
        },
        {
          "7. References": "acoustic modeling for cnn-based speech synthesis,”\nin ICASSP",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "the 28th International\nJoint\nstyle\ntransfer,”\nin Proceedings of"
        },
        {
          "7. References": "2019-2019 IEEE International Conference on Acoustics, Speech",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "Conference on Artiﬁcial Intelligence, IJCAI 2019, 2019."
        },
        {
          "7. References": "and Signal Processing (ICASSP).\nIEEE, 2019, pp. 6950–6954.",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "[22] Y. Wang, R. Skerry-Ryan, D. Stanton, Y. Wu, R.\nJ. Weiss,"
        },
        {
          "7. References": "[6] W.-N. Hsu, Y. Zhang, R.\nJ. Weiss, H. Zen, Y. Wu, Y. Wang,",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "N. Jaitly, Z. Yang, Y. Xiao, Z. Chen, S. Bengio et al., “Tacotron:"
        },
        {
          "7. References": "Y\n. Cao, Y. Jia, Z. Chen, J. Shen et al., “Hierarchical generative",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "Towards end-to-end speech synthesis,” Proc.\nInterspeech 2017,"
        },
        {
          "7. References": "International\nmodeling\nfor\ncontrollable\nspeech\nsynthesis,”\nin",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "pp. 4006–4010, 2017."
        },
        {
          "7. References": "Conference on Learning Representations, 2018.",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "[23]\nJ. Shen, R. Pang, R. J. Weiss, M. Schuster, N. Jaitly, Z. Yang,"
        },
        {
          "7. References": "[7] Y.\nZhang,\nS.\nPan,\nL. He,\nand\nZ.\nLing,\n“Learning\nlatent",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "Z. Chen, Y. Zhang, Y. Wang, R. Skerrv-Ryan et al.,\n“Natural"
        },
        {
          "7. References": "representations for style control and transfer in end-to-end speech",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "tts\nsynthesis by conditioning wavenet on mel\nspectrogram pre-"
        },
        {
          "7. References": "synthesis,” in ICASSP 2019 - 2019 IEEE International Confer-",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "dictions,” in ICASSP 2018-2018 IEEE International Conference"
        },
        {
          "7. References": "ence on Acoustics, Speech and Signal Processing (ICASSP), 2019,",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "on Acoustics, Speech and Signal Processing (ICASSP).\nIEEE,"
        },
        {
          "7. References": "pp. 6945–6949.",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "2018, pp. 4779–4783."
        },
        {
          "7. References": "[8]\nS.-Y. Um,\nS. Oh,\nK. Byun,\nI.\nJang,\nC. Ahn,\nand H.-G.",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "[24] Y. Wang, D. Stanton, Y. Zhang, R.-S. Ryan, E. Battenberg,"
        },
        {
          "7. References": "Kang,\n“Emotional\nspeech synthesis with rich and granularized",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "J. Shor, Y. Xiao, Y.\nJia,\nF. Ren,\nand R. A. Saurous,\n“Style"
        },
        {
          "7. References": "control,” in ICASSP 2020-2020 IEEE International Conference",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "tokens: Unsupervised style modeling, control and transfer in end-"
        },
        {
          "7. References": "on Acoustics, Speech and Signal Processing (ICASSP).\nIEEE,",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "to-end speech synthesis,” in International Conference on Machine"
        },
        {
          "7. References": "2020, pp. 7254–7258.",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "Learning, 2018, pp. 5180–5189."
        },
        {
          "7. References": "[9] M. El Ayadi, M. S. Kamel,\nand F. Karray,\n“Survey on speech",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "[25] R. Liu, B. Sisman, J. Li, F. Bao, G. Gao, and H. Li, “Teacher-"
        },
        {
          "7. References": "emotion\nrecognition:\nFeatures,\nclassiﬁcation\nschemes,\nand",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "student training for robust tacotron-based TTS,” in ICASSP 2020-"
        },
        {
          "7. References": "databases,” Pattern recognition, vol. 44, no. 3, pp. 572–587, 2011.",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "2020 IEEE International Conference on Acoustics, Speech and"
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "Signal Processing (ICASSP).\nIEEE, 2020, pp. 6274–6278."
        },
        {
          "7. References": "[10]\nP. Wu, Z. Ling, L. Liu, Y.\nJiang, H. Wu,\nand L. Dai,\n“End-",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "to-end emotional speech synthesis using style tokens and semi-",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "[26] G. D and L.\nJ. S, “Signal estimation from modiﬁed short-time"
        },
        {
          "7. References": "supervised training,” in 2019 Asia-Paciﬁc Signal and Information",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "IEEE Transactions on Acoustics Speech &\nfourier\ntransform,”"
        },
        {
          "7. References": "Processing Association Annual Summit and Conference (APSIPA",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "Signal Processing, vol. 32, no. 2, pp. 236–243, 1984."
        },
        {
          "7. References": "ASC).\nIEEE, 2019, pp. 623–627.",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "[27] A. Graves,\n“Generating\nsequences with\nrecurrent\nneural\nnet-"
        },
        {
          "7. References": "[11] X. Cai, D. Dai, Z. Wu, X. Li, J. Li, and H. Meng, “Emotion con-",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "works,” arXiv preprint arXiv:1308.0850, 2013."
        },
        {
          "7. References": "trollable speech synthesis using emotion-unlabeled dataset with",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "[28] M. Chen, X. He,\nJ. Yang,\nand H. Zhang,\n“3-d convolutional"
        },
        {
          "7. References": "the assistance of cross-domain speech emotion recognition,” in",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "recurrent\nneural\nnetworks with\nattention model\nfor\nspeech"
        },
        {
          "7. References": "ICASSP 2021-2021 IEEE International Conference on Acoustics,",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "emotion recognition,” IEEE Signal Processing Letters, vol. 25,"
        },
        {
          "7. References": "Speech and Signal Processing (ICASSP).\nIEEE, 2021, pp. 1–5.",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "no. 10, pp. 1440–1444, 2018."
        },
        {
          "7. References": "[12]\nT. Li, S. Yang, L. Xue, and L. Xie, “Controllable emotion transfer",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "[29] R. J. Williams, “Simple statistical gradient-following algorithms"
        },
        {
          "7. References": "2021\n12th\nInternational\nfor\nend-to-end\nspeech\nsynthesis,”\nin",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "for\nconnectionist\nreinforcement\nlearning,” Machine\nlearning,"
        },
        {
          "7. References": "Symposium on Chinese Spoken Language Processing (ISCSLP).",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "vol. 8, no. 3-4, pp. 229–256, 1992."
        },
        {
          "7. References": "IEEE, 2021, pp. 1–5.",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "[30] K. Zhou, B. Sisman, R. Liu,\nand H. Li,\n“Seen\nand\nunseen"
        },
        {
          "7. References": "[13] R. Liu, B. Sisman, G. Gao, and H. Li, “Expressive tts training with",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "emotional style transfer for voice conversion with a new emotional"
        },
        {
          "7. References": "frame and style reconstruction loss,” IEEE/ACM Transactions on",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "speech dataset,” arXiv preprint arXiv:2010.14794, 2020."
        },
        {
          "7. References": "Audio, Speech, and Language Processing, vol. 29, pp. 1806–1818,",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "[31] B. Sisman, S. King,\nJ. Yamagishi, and H. Li, “An overview of"
        },
        {
          "7. References": "2021.",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "voice conversion and its challenges: From statistical modeling to"
        },
        {
          "7. References": "Reinforcement\nlearning:\nAn\n[14] R. S. Sutton\nand A. G. Barto,",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        },
        {
          "7. References": "",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "deep learning,” IEEE/ACM Transactions on Audio, Speech, and"
        },
        {
          "7. References": "introduction.\nMIT press, 2018.",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "Language Processing, vol. 29, pp. 132–157, 2021."
        },
        {
          "7. References": "[15]\nS. Latif, H. Cuay´ahuitl, F. Pervez, F. Shamshad, H. S. Ali, and",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "[32] Y. Zhou, X. Tian,\nand H. Li,\n“Multi-task wavernn with\nan"
        },
        {
          "7. References": "E. Cambria, “A survey on deep reinforcement learning for audio-",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "integrated architecture for cross-lingual voice conversion,” IEEE"
        },
        {
          "7. References": "based applications,” arXiv preprint arXiv:2101.00240, 2021.",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "Signal Processing Letters, vol. 27, pp. 1310–1314, 2020."
        },
        {
          "7. References": "[16] Y. Koizumi, K. Niwa, Y. Hioka, K. Kobayashi, and Y. Haneda,",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "[33] O. Kwon,\nI. Jang, C. Ahn, and H.-G. Kang, “An effective style"
        },
        {
          "7. References": "“Dnn-based\nsource\nenhancement\nto\nincrease\nobjective\nsound",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "token weight control\ntechnique for end-to-end emotional speech"
        },
        {
          "7. References": "IEEE/ACM Transactions on Audio,\nquality assessment\nscore,”",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "synthesis,” IEEE Signal Processing Letters, vol. 26, no. 9, pp."
        },
        {
          "7. References": "Speech, and Language Processing, vol. 26, no. 10, pp. 1780–",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": "1383–1387, 2019."
        },
        {
          "7. References": "1792, 2018.",
          "[17] Y. Shen, C. Huang, S. Wang, Y. Tsao, H. Wang, and T. Chi, “Rein-": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "An hmm-based speech synthesis system applied to english",
      "authors": [
        "K Tokuda",
        "H Zen",
        "A Black"
      ],
      "year": "2002",
      "venue": "IEEE Speech Synthesis Workshop"
    },
    {
      "citation_id": "3",
      "title": "Modeling of various speaking styles and emotions for hmm-based speech synthesis",
      "authors": [
        "J Yamagishi",
        "K Onishi",
        "T Masuko",
        "T Kobayashi"
      ],
      "year": "2003",
      "venue": "Eighth European Conference on Speech Communication and Technology"
    },
    {
      "citation_id": "4",
      "title": "Unsupervised clustering of emotion and voice styles for expressive tts",
      "authors": [
        "F Eyben",
        "S Buchholz",
        "N Braunschweiler",
        "J Latorre",
        "V Wan",
        "M Gales",
        "K Knill"
      ],
      "year": "2012",
      "venue": "ICASSP 2012-2012 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "5",
      "title": "Investigating different representations for modeling and controlling multiple emotions in dnn-based speech synthesis",
      "authors": [
        "J Lorenzo-Trueba",
        "G Henter",
        "S Takaki",
        "J Yamagishi",
        "Y Morino",
        "Y Ochiai"
      ],
      "year": "2018",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "6",
      "title": "Multi-speaker emotional acoustic modeling for cnn-based speech synthesis",
      "authors": [
        "H Choi",
        "S Park",
        "J Park",
        "M Hahn"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "7",
      "title": "Hierarchical generative modeling for controllable speech synthesis",
      "authors": [
        "W.-N Hsu",
        "Y Zhang",
        "R Weiss",
        "H Zen",
        "Y Wu",
        "Y Wang",
        "Y Cao",
        "Y Jia",
        "Z Chen",
        "J Shen"
      ],
      "year": "2018",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "8",
      "title": "Learning latent representations for style control and transfer in end-to-end speech synthesis",
      "authors": [
        "Y Zhang",
        "S Pan",
        "L He",
        "Z Ling"
      ],
      "year": "2019",
      "venue": "ICASSP 2019 -2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "9",
      "title": "Emotional speech synthesis with rich and granularized control",
      "authors": [
        "S.-Y Um",
        "S Oh",
        "K Byun",
        "I Jang",
        "C Ahn",
        "H.-G Kang"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "10",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "M Ayadi",
        "M Kamel",
        "F Karray"
      ],
      "year": "2011",
      "venue": "Pattern recognition"
    },
    {
      "citation_id": "11",
      "title": "Endto-end emotional speech synthesis using style tokens and semisupervised training",
      "authors": [
        "P Wu",
        "Z Ling",
        "L Liu",
        "Y Jiang",
        "H Wu",
        "L Dai"
      ],
      "year": "2019",
      "venue": "2019 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "12",
      "title": "Emotion controllable speech synthesis using emotion-unlabeled dataset with the assistance of cross-domain speech emotion recognition",
      "authors": [
        "X Cai",
        "D Dai",
        "Z Wu",
        "X Li",
        "J Li",
        "H Meng"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "13",
      "title": "Controllable emotion transfer for end-to-end speech synthesis",
      "authors": [
        "T Li",
        "S Yang",
        "L Xue",
        "L Xie"
      ],
      "year": "2021",
      "venue": "2021 12th International Symposium on Chinese Spoken Language Processing"
    },
    {
      "citation_id": "14",
      "title": "Expressive tts training with frame and style reconstruction loss",
      "authors": [
        "R Liu",
        "B Sisman",
        "G Gao",
        "H Li"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "15",
      "title": "Reinforcement learning: An introduction",
      "authors": [
        "R Sutton",
        "A Barto"
      ],
      "year": "2018",
      "venue": "Reinforcement learning: An introduction"
    },
    {
      "citation_id": "16",
      "title": "A survey on deep reinforcement learning for audiobased applications",
      "authors": [
        "S Latif",
        "H Cuayáhuitl",
        "F Pervez",
        "F Shamshad",
        "H Ali",
        "E Cambria"
      ],
      "year": "2021",
      "venue": "A survey on deep reinforcement learning for audiobased applications",
      "arxiv": "arXiv:2101.00240"
    },
    {
      "citation_id": "17",
      "title": "Dnn-based source enhancement to increase objective sound quality assessment score",
      "authors": [
        "Y Koizumi",
        "K Niwa",
        "Y Hioka",
        "K Kobayashi",
        "Y Haneda"
      ],
      "year": "2018",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "18",
      "title": "Reinforcement learning based speech enhancement for robust speech recognition",
      "authors": [
        "Y Shen",
        "C Huang",
        "S Wang",
        "Y Tsao",
        "H Wang",
        "T Chi"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "19",
      "title": "Reinforcement learning of speech recognition system based on policy gradient and hypothesis selection",
      "authors": [
        "T Kala",
        "T Shinozaki"
      ],
      "year": "2018",
      "venue": "ICASSP 2018-2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "20",
      "title": "A machine of few words: Interactive speaker recognition with reinforcement learning",
      "authors": [
        "M Seurin",
        "F Strub",
        "P Preux",
        "O Pietquin"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech 2020"
    },
    {
      "citation_id": "21",
      "title": "Human-level control through deep reinforcement learning",
      "authors": [
        "V Mnih",
        "K Kavukcuoglu",
        "D Silver",
        "A Rusu",
        "J Veness",
        "M Bellemare",
        "A Graves",
        "M Riedmiller",
        "A Fidjeland",
        "G Ostrovski"
      ],
      "year": "2015",
      "venue": "nature"
    },
    {
      "citation_id": "22",
      "title": "A dual reinforcement learning framework for unsupervised text style transfer",
      "authors": [
        "F Luo",
        "P Li",
        "J Zhou",
        "P Yang",
        "B Chang",
        "Z Sui",
        "X Sun"
      ],
      "year": "2019",
      "venue": "Proceedings of the 28th International Joint Conference on Artificial Intelligence"
    },
    {
      "citation_id": "23",
      "title": "Tacotron: Towards end-to-end speech synthesis",
      "authors": [
        "Y Wang",
        "R Skerry-Ryan",
        "D Stanton",
        "Y Wu",
        "R Weiss",
        "N Jaitly",
        "Z Yang",
        "Y Xiao",
        "Z Chen",
        "S Bengio"
      ],
      "year": "2017",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "24",
      "title": "Natural tts synthesis by conditioning wavenet on mel spectrogram predictions",
      "authors": [
        "J Shen",
        "R Pang",
        "R Weiss",
        "M Schuster",
        "N Jaitly",
        "Z Yang",
        "Z Chen",
        "Y Zhang",
        "Y Wang",
        "R Skerrv-Ryan"
      ],
      "year": "2018",
      "venue": "ICASSP 2018-2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "25",
      "title": "Style tokens: Unsupervised style modeling, control and transfer in endto-end speech synthesis",
      "authors": [
        "Y Wang",
        "D Stanton",
        "Y Zhang",
        "R.-S Ryan",
        "E Battenberg",
        "J Shor",
        "Y Xiao",
        "Y Jia",
        "F Ren",
        "R Saurous"
      ],
      "year": "2018",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "26",
      "title": "Teacherstudent training for robust tacotron-based TTS",
      "authors": [
        "R Liu",
        "B Sisman",
        "J Li",
        "F Bao",
        "G Gao",
        "H Li"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "27",
      "title": "Signal estimation from modified short-time fourier transform",
      "year": "1984",
      "venue": "IEEE Transactions on Acoustics Speech & Signal Processing"
    },
    {
      "citation_id": "28",
      "title": "Generating sequences with recurrent neural networks",
      "authors": [
        "A Graves"
      ],
      "year": "2013",
      "venue": "Generating sequences with recurrent neural networks",
      "arxiv": "arXiv:1308.0850"
    },
    {
      "citation_id": "29",
      "title": "3-d convolutional recurrent neural networks with attention model for speech emotion recognition",
      "authors": [
        "M Chen",
        "X He",
        "J Yang",
        "H Zhang"
      ],
      "year": "2018",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "30",
      "title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
      "authors": [
        "R Williams"
      ],
      "year": "1992",
      "venue": "Machine learning"
    },
    {
      "citation_id": "31",
      "title": "Seen and unseen emotional style transfer for voice conversion with a new emotional speech dataset",
      "authors": [
        "K Zhou",
        "B Sisman",
        "R Liu",
        "H Li"
      ],
      "year": "2020",
      "venue": "Seen and unseen emotional style transfer for voice conversion with a new emotional speech dataset",
      "arxiv": "arXiv:2010.14794"
    },
    {
      "citation_id": "32",
      "title": "An overview of voice conversion and its challenges: From statistical modeling to deep learning",
      "authors": [
        "B Sisman",
        "S King",
        "J Yamagishi",
        "H Li"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "33",
      "title": "Multi-task wavernn with an integrated architecture for cross-lingual voice conversion",
      "authors": [
        "Y Zhou",
        "X Tian",
        "H Li"
      ],
      "year": "2020",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "34",
      "title": "An effective style token weight control technique for end-to-end emotional speech synthesis",
      "authors": [
        "O Kwon",
        "I Jang",
        "C Ahn",
        "H.-G Kang"
      ],
      "year": "2019",
      "venue": "IEEE Signal Processing Letters"
    }
  ]
}