{
  "paper_id": "2106.11532v3",
  "title": "Key-Sparse Transformer For Multimodal Speech Emotion Recognition",
  "published": "2021-06-22T04:02:58Z",
  "authors": [
    "Weidong Chen",
    "Xiaofeng Xing",
    "Xiangmin Xu",
    "Jichen Yang",
    "Jianxin Pang"
  ],
  "keywords": [
    "speech emotion recognition",
    "sparse network",
    "modality interaction"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech emotion recognition is a challenging research topic that plays a critical role in human-computer interaction. Multimodal inputs further improve the performance as more emotional information is used. However, existing studies learn all the information in the sample while only a small portion of it is about emotion. The redundant information will become noises and limit the system performance. In this paper, a keysparse Transformer is proposed for efficient emotion recognition by focusing more on emotion related information. The proposed method is evaluated on the IEMOCAP and LSSED. Experimental results show that the proposed method achieves better performance than the state-of-the-art approaches.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech emotion recognition (SER) is fast becoming a key instrument in human-computer interaction (HCI)  [1] . SER also sheds new light on autism and the elderly care and so on, which are collectively referred to healthcare  [2] . For example, the people who suffer from severe speech and language disorder have difficulty expressing their emotions. An emotion recognition system can help to treat the patients and improve their emotional communication skills.\n\nSpeech is multimodal as it contains text information by its nature. Latest researches  [3, 4]  have also proved that multimodal methods outperform the uni-modal methods. Consequently, multimodal SER has been a hot research topic in recent years. For example, Yoon et al.  [5]  use dual recurrent neural networks to combine the information from audio and text. In the same way, Krishna et al.  [6]  use raw audio waveform as audio features and GloVe word embeddings as text features for multimodal learning. Moreover, Peri et al.  [7]  combine audio and video information and utilize multitask setting for emotion recognition. In this paper, we use both audio and text information for SER.\n\nPre-trained Self Supervised Learning (SSL) has made great success in many fields such as natural language processing  [8, 9]  and speech recognition  [10] . Meanwhile, recent works  [11, 12]  that use SSL model have obtained promising results in SER. Nowadays, wav2vec  [10]  and RoBERTa  [9]  are the most commonly used pre-trained SSL models in the literature. Thus, in this paper, we use them to extract audio and text embeddings, respectively. Inspired by the attention mechanism, Transformer  [13] , which is outstanding in modeling long sequence, is proposed and has achieved great success in natural language processing  [11] . Meanwhile, several Transformer based architectures have been introduced for SER. Tarantino et al.  [14]  use global windowing system in Transformer to capture deep relationships within the utterance. Moreover, Huang et al.  [15]  use Transformer to fuse different modalities for sentiment analysis. In this paper, we use Transformer as our basic structure to implement emotion recognition.\n\nHowever, few works have paid attention to that not all the information in audio or text is related to emotion. For example, considering a text \"Okay, look it's a beautiful day. Why are we arguing?\" in IEMOCAP  [16] , the attention weights in vanilla Transformer are shown in Figure  1 . We can see that the attention weights in Transformer are assigned to all the words. However, words \"beautiful\" and \"arguing\" contain the majority of emotional information in this sentence. And the words that are not related to emotion such as \"it\", \"a\" and \"look\", are unnecessary for SER task and become noises, leading to the limitation of system performance. To address this issue, we propose a novel method, named key-sparse Transformer (KS-Transformer), to judge the importance of each word or speech frame in the sample and help the model focus more on the emotion related information. Based on KS-Transformer, we further design a cascaded cross-attention block to fuse different modalities with high efficiency.\n\nThe contributions of this paper can be summarized as follows:\n\n• We propose KS-Transformer to judge the importance of each frame or word that helps the model focus more on the emotional information. Based on KS-Transformer, we further design a cascaded cross-attention block to achieve interaction between different modalities.\n\n• We evaluate the proposed method on IEMOCAP and LSSED, and demonstrate that it achieves better results than the existing state-of-the-art approaches.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Proposed Method",
      "text": "The proposed model, as shown in Figure  2 , mainly consists of three modules. In which, feature extraction module is used to learn the input features, modality interaction module is used for learning interactive information and deep fusion module aims to further combine the information from audio and text. Specifically, the first module (gray parts) is based on vanilla Transformer and the last two modules (yellow parts) are based on KS-Transformer. More details will be introduced in the following subsections.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Feature Extraction Module",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Roberta Text Embeddings",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Modality Interaction Module",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Deep Fusion Module",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Concatenation",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Feature Extraction Module",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Wav2Vec Speech Embeddings",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Modality Interaction Module",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Deep Fusion Module",
      "text": "Average pooling Fig.  2 . Overview structure of the proposed model.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Key-Sparse Transformer",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Vanilla Transformer",
      "text": "Vanilla Transformer consists of encoder and decoder originally. In this paper, we use Transformer to represent the encoder part, since it is the one needed for the implementation of our proposed architecture. The inputs of Transformer are divided into Q, K and V, which consist of Query, Key and Value vectors, respectively. The attention mechanism in vanilla Transformer is depicted as follows:\n\nwhere d Q is the dimension of the Query vector, W is the weight matrix and attn is the attention output. For multi-head attention mechanism, we combine the attention outputs from all the heads. More details can be found in  [13] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Key-Sparse Attention Mechanism",
      "text": "The key-sparse Transformer aims to find the emotional information automatically. Assume the number of Query vectors in Q is i while that of Key vectors in K is j, the key-sparse attention mechanism is illustrated in Figure  3 . It should be noted that K and V are always the same in Transformer. Key-sparse attention mechanism, which is used in KS-Transformer, is capable of judging the importance of each speech frame or word automatically. As shown in Figure  3 , the weight matrix W is obtained by multiplying Q and K, and each row in W are the weights of Value vectors in V. As a Value vector represents a frame in audio or a word in text, we add up all the weights of the same Value vector and the summation is used as a discriminator for the importance of the speech frame or word in the sample. We select k Value vectors with top-k largest summation and keep their attention weights in weight matrix unchanged while the others are reset to zero. This operation makes the weight matrix from dense to sparse and reduces the redundancy, that's why we call the Transformer used here as KS-Transformer. The top-k mask is calculated by Equation  3 .\n\nwhere threshold is the k th largest summation and z ∈ [1, j].",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Modality Interaction Module",
      "text": "Because modality interaction module is based on cascaded cross-attention block (CCAB), we introduce CCAB's structure first. As shown in the left part of Figure  4 , CCAB is a cascade of two KS-Transformers, in which, the first KS-Transformer creates Q from modality A and K, V from modality B. With this special input method, the key-sparse attention mechanism will find out the most relevant part in B for A and produce an output which has combined A with B information.\n\nSince the emotional information between different modalities is often complementary  [3, 17, 18] , neither A nor B can represent the accurate emotion. Therefore, the second KS-Transformer in CCAB takes the fused features as input and considers the information from both modality A and modality B when applying key-sparse attention. Benefited from CCAB, A and B are fused more comprehensively and accurately.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Modality Interaction Module",
      "text": "Cascaded cross-attention block (CCAB) Fig.  4 . The details of CCAB (left) and modality interaction module (right).\n\nAs shown in the right part of Figure  4 , modality interaction module consists of a stack of CCABs, wherein the later CCAB takes the output of the former CCAB as Q input while K and V inputs are always from modality B. That the information from B goes through one CCAB is regarded as one interaction because the information from B had flowed into A by the key-sparse attention. More than one CCAB are applied for multiple times interactions. A skip connection is utilized for the features' stability.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Deep Fusion Module",
      "text": "Most researches take the fused features to predict emotions after the interaction  [12, 19] . However, we argue that the fused features maybe not the best and can be deep fused to further improve the system performance. In detail, deep fusion module consists of several KS-Transformers, in which, they take the fused features as input and utilize key-sparse attention to enhance the interaction between audio and text and implement deep fusion.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Database Introduction",
      "text": "IEMOCAP contains five sessions, every of which has one male and one female speaker, respectively. To stay consistent with the previous works  [6, 17, 18] , we use 5,531 utterances from four emotions: angry, neutral, happy (& excited) and sad. We conduct experiments in leave-one-session-out crossvalidation strategy.\n\nLSSED  [20]  is a new released large-scale English speech emotion dataset, which has data collected from 820 subjects and contains 147,025 samples. Consistent with  [20] , we use four emotion categories, including angry, neutral, happy and sad. For each emotion class, its associated samples are randomly split into train/development/test in ratio of 7/1/2, respectively. Every experiment is run for 10 times to avoid randomness, and the averaged result is used as the final accuracy.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experimental Setup",
      "text": "The pre-trained wav2vec and RoBERTa are available online  1  . The max lengths of the audio and text feature sequence are set to 460 and 20, respectively. SGD optimizer with a learning rate of 5×10 -4 on IEMOCAP and 1×10 -4 on LSSED is applied to optimize the model. The learning rate drops to 50% of the original every 30 epochs. Dropout with p = 0.5 is utilized to alleviate over-fitting. The batch size is 32.\n\nFeature extraction module is used to learn the input features, which are extracted from pre-trained SSL models, aims to obtain suitable features for SER task. For modeling rich contexts, this module is based on vanilla Transformer. Q, K and V inputs here are the same, which is known as selfattention  [13] . The number of vanilla Transformers in feature extraction module is 5 and the number of KS-Transformers in deep fusion module is 2. Eight attention heads are used in multi-head attention. The number of CCABs used in modality interaction module will be discussed later.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Results And Analysis",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Key-Sparse Attention Analysis",
      "text": "To demonstrate the effectiveness of the key-sparse attention, we consider a sample in IEMOCAP and compare the attention weights in vanilla Transformer and KS-Transformer by visualization. As shown in Figure  5 , the vanilla Transformer takes note of all the words, including the noisy words which are not related to emotion, and trends to over-fitting. However, the KS-Transformer makes the connections from dense to sparse, which is able to ignore most of the noises and focus more on the emotional information. Meanwhile, the sparsity in KS-Transformer can reduces the complexity in the model and alleviates over-fitting.\n\nOkay , look it 's a beautiful day . Why are we arguing ? Okay , look it 's a beautiful day . Why are we arguing ?  To explore the optimal sparsity in KS-Transformer, we vary k from 0.1 to 0.9. The larger k we set, the less attention weights are reset to zero and less sparsity we have. Because LSSED suffers from sample imbalance, we use unweighted accuracy (UA) as criterion. The results are shown in Figure  6 .\n\nSince IEMOCAP is a relatively small corpus, the model is prone to over-fitting when k is larger than 0.5, causing the UA scores to remain constant. However, on the large-scale dataset LSSED, a significant drop is appeared when k is larger than 0.5 because of the redundant information. In contrast, when k is smaller than 0.5, the model uses too little information and might converge to an unsatisfactory local minimum. Considering the UA performance curves on IEMOCAP and LSSED corpora, k is set to 0.5, which means 50% attention weights are reset to zero in each KS-Transformer as default.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Multimodal Interaction Analysis",
      "text": "Modality interaction is vital for multimodal system. To investigate the effectiveness of the stack of CCABs, we change the number of CCABs used from zero to four, where zero means that the modality interaction module is removed, and the results are shown in Table  1 . Weighted accuracy (WA) and UA are used as criteria. It should be noted that the number of CCABs used represents the times of interactions performed.\n\nFrom Table  1 , we show that the interaction between different modalities is shallow and insufficient when only one CCAB is applied. The performance improves as the number of CCABs increases. The best performances are obtained when the number is three, which confirms the effectiveness of CCAB and the necessity of multiple times interactions.   2  gives the performance comparison among the proposed method with some known systems on IEMOCAP and LSSED, in which, all the systems apply audio and text as inputs except that PyResNet  [20]  only takes audio information.\n\nFrom Table  2 , it can be observed that our method gives the best WA and UA on IEMOCAP. Moreover, our method achieves the highest UA on LSSED, where UA is a more important criterion because of the sample imbalance issue.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, KS-Transformer, using a novel key-sparse attention mechanism, has been proposed for speech emotion recognition. Only the emotion related speech frames in audio or words in text can be considered and assigned with attention weights. And based on KS-Transformer, we further present CCAB to fuse different modalities and achieve deep interaction. Experimental results on IEMOCAP and LSSED demonstrate the effectiveness of KS-Transformer and CCAB.\n\nIn the future, we plan to combine more modalities to further improve the system performance.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The attention weights of the utterance “OK, look it’s a",
      "page": 1
    },
    {
      "caption": "Figure 1: We can see that",
      "page": 1
    },
    {
      "caption": "Figure 2: , mainly consists of",
      "page": 2
    },
    {
      "caption": "Figure 2: Overview structure of the proposed model.",
      "page": 2
    },
    {
      "caption": "Figure 3: It should be",
      "page": 2
    },
    {
      "caption": "Figure 3: The key-sparse attention in KS-Transformer.",
      "page": 2
    },
    {
      "caption": "Figure 4: The details of CCAB (left) and modality interaction",
      "page": 3
    },
    {
      "caption": "Figure 4: , modality interac-",
      "page": 3
    },
    {
      "caption": "Figure 5: , the vanilla Transformer",
      "page": 3
    },
    {
      "caption": "Figure 5: Visualization of the attention weights.",
      "page": 4
    },
    {
      "caption": "Figure 6: Effects of hyperparameter k.",
      "page": 4
    },
    {
      "caption": "Figure 6: Since IEMOCAP is a relatively small corpus, the model is",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0": "0",
          "W12": "W22",
          ". . .": ". . ."
        },
        {
          "0": ". . .",
          "W12": ". . .",
          ". . .": ""
        },
        {
          "0": "0",
          "W12": "Wi2",
          ". . .": ". . ."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "W11 W12": "W22",
          ". . . W1j": "W2j",
          "Softmax": ""
        },
        {
          "W11 W12": ". . .",
          ". . . W1j": ". . .",
          "Softmax": "S1\nS2"
        },
        {
          "W11 W12": "Wi2",
          ". . . W1j": "Wij",
          "Softmax": ""
        },
        {
          "W11 W12": "",
          ". . . W1j": "",
          "Softmax": ""
        },
        {
          "W11 W12": "",
          ". . . W1j": "",
          "Softmax": ""
        },
        {
          "W11 W12": "",
          ". . . W1j": "",
          "Softmax": ""
        },
        {
          "W11 W12": "",
          ". . . W1j": "",
          "Softmax": ""
        },
        {
          "W11 W12": "",
          ". . . W1j": "",
          "Softmax": "0\n1"
        },
        {
          "W11 W12": "",
          ". . . W1j": "",
          "Softmax": ""
        }
      ],
      "page": 2
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Speech emotion recognition: Two decades in a nutshell, benchmarks, and ongoing trends",
      "authors": [
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Commun. ACM"
    },
    {
      "citation_id": "3",
      "title": "Usage of emotion recognition in military health care",
      "authors": [
        "S Tokuno",
        "G Tsumatori",
        "S Shono",
        "E Takei",
        "T Yamamoto",
        "G Suzuki",
        "S Mituyoshi",
        "M Shimura"
      ],
      "year": "2011",
      "venue": "Defense Science Research Conference and Expo (DSR)"
    },
    {
      "citation_id": "4",
      "title": "Multi-Modal Attention for Speech Emotion Recognition",
      "authors": [
        "Z Pan",
        "Z Luo",
        "J Yang",
        "H Li"
      ],
      "venue": "Proc. Interspeech 2020"
    },
    {
      "citation_id": "5",
      "title": "What makes multimodal learning better than single (provably)",
      "authors": [
        "Y Huang",
        "C Du",
        "Z Xue",
        "X Chen",
        "H Zhao",
        "L Huang"
      ],
      "year": "2021",
      "venue": "What makes multimodal learning better than single (provably)",
      "arxiv": "arXiv:2106.04538"
    },
    {
      "citation_id": "6",
      "title": "Multimodal speech emotion recognition using audio and text",
      "authors": [
        "S Yoon",
        "S Byun",
        "K Jung"
      ],
      "year": "2018",
      "venue": "2018 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "7",
      "title": "Multimodal Emotion Recognition Using Cross-Modal Attention and 1D Convolutional Neural Networks",
      "authors": [
        "D Krishna",
        "A Patil"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "8",
      "title": "Disentanglement for audio-visual emotion recognition using multitask setup",
      "authors": [
        "R Peri",
        "S Parthasarathy",
        "C Bradshaw",
        "S Sundaram"
      ],
      "year": "2021",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "9",
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "authors": [
        "J Devlin",
        "M Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2018",
      "venue": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "10",
      "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal",
        "J Du",
        "M Joshi",
        "D Chen",
        "O Levy",
        "M Lewis",
        "L Zettlemoyer",
        "V Stoyanov"
      ],
      "year": "2019",
      "venue": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "11",
      "title": "wav2vec: Unsupervised pre-training for speech recognition",
      "authors": [
        "S Schneider",
        "A Baevski",
        "R Collobert",
        "M Auli"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "12",
      "title": "Multimodal emotion recognition with transformer-based self supervised feature fusion",
      "authors": [
        "S Siriwardhana",
        "T Kaluarachchi",
        "M Billinghurst",
        "S Nanayakkara"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "13",
      "title": "Jointly Fine-Tuning \"BERT-Like\" Self Supervised Models to Improve Multimodal Speech Emotion Recognition",
      "authors": [
        "S Siriwardhana",
        "A Reis",
        "R Weerasekera",
        "S Nanayakkara"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "14",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Proceedings of the 31st International Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "15",
      "title": "Self-Attention for Speech Emotion Recognition",
      "authors": [
        "L Tarantino",
        "P Garner",
        "A Lazaridis"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "16",
      "title": "Multimodal transformer fusion for continuous emotion recognition",
      "authors": [
        "J Huang",
        "J Tao",
        "B Liu",
        "Z Lian",
        "M Niu"
      ],
      "year": "2020",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "17",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "18",
      "title": "Group Gated Fusion on Attention-Based Bidirectional Alignment for Multimodal Emotion Recognition",
      "authors": [
        "P Liu",
        "K Li",
        "H Meng"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "19",
      "title": "A Multi-Scale Fusion Framework for Bimodal Speech Emotion Recognition",
      "authors": [
        "M Chen",
        "X Zhao"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "20",
      "title": "M3er: Multiplicative multimodal emotion recognition using facial, textual, and speech cues",
      "authors": [
        "T Mittal",
        "U Bhattacharya",
        "R Chandra",
        "A Bera",
        "D Manocha"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "21",
      "title": "Lssed: A large-scale dataset and benchmark for speech emotion recognition",
      "authors": [
        "W Fan",
        "X Xu",
        "X Xing",
        "W Chen",
        "D Huang"
      ],
      "year": "2021",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    }
  ]
}