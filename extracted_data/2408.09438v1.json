{
  "paper_id": "2408.09438v1",
  "title": "Enhancing Modal Fusion By Alignment And Label Matching For Multimodal Emotion Recognition",
  "published": "2024-08-18T11:05:21Z",
  "authors": [
    "Qifei Li",
    "Yingming Gao",
    "Yuhua Wen",
    "Cong Wang",
    "Ya Li"
  ],
  "keywords": [
    "multimoal emotion recognition",
    "multitask learning",
    "contrastive learning",
    "cross-attention"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "To address the limitation in multimodal emotion recognition (MER) performance arising from inter-modal information fusion, we propose a novel MER framework based on multitask learning where fusion occurs after alignment, called Foal-Net. The framework is designed to enhance the effectiveness of modality fusion and includes two auxiliary tasks: audiovideo emotion alignment (AVEL) and cross-modal emotion label matching (MEM). First, AVEL achieves alignment of emotional information in audio-video representations through contrastive learning. Then, a modal fusion network integrates the aligned features. Meanwhile, MEM assesses whether the emotions of the current sample pair are the same, providing assistance for modal information fusion and guiding the model to focus more on emotional information. The experimental results conducted on IEMOCAP corpus show that Foal-Net outperforms the state-of-the-art methods and emotion alignment is necessary before modal fusion. The code is open-source 1 .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition is an important part of human-computer interaction (HCI). In order to improve the interaction experience, it is necessary to fully utilize the information between different modalities to improve the recognition performance of the system  [1] . Hence, efficient modal fusion methods is one of the current research hotspots.\n\nIn the MER task, there are many challenges that need to be investigated. This challenges include the need for improved feature representation, developing reasonable model structure to fit the feature representation, exploring effective modal fusion methods to realize inter-modal information complementary and improve the performance of MER, and addressing or alleviating the problem of model performance degradation caused by missing modal information.\n\nThe commonly used modalities for emotion recognition include audio, video, and text. In recent years, most of researchers have extracted deep representations of pre-trained models to represent modal information  [2, 3, 4, 5] , such as HuBERT  [6] , WavLM  [7] , BERT  [8] , RoBERT  [9] , Resnet-FER2013  [10]  and MAE  [11] . Compared with the conventional features, such as Mel Frequency Cepstrum Coefficient, one-hot representation and Face Action Unit, the deep representations have better performance and generalization. For modeling the modal information, Wang et al.  [12]  proposed a modality-sensitive MER framework to exploring complementary features. Mitra et al.  [13]  utilized TC-GRU which consists of time convolution network and GRU to capture the spatio-temporal properties of the inputs. Maji et al.  [14]  proposed a cross-model Transformer model to model the information of audio and text modalities and achieved remarkable performance. The cross-attention mechanism is currently the most commonly used for modal fusion  [15] . The above studies all made use of cross-attention to fuse the audio and lexical information to realize MER. Lian et al.  [16]  exploited graph neural networks to fuse three modalities and attained excellent performance. In order to alleviate the effects of modal absence, Lian et al.  [16]  and Wang et al.  [12]  employed different strategies individually to simulate scenarios of modality absence during the training process, enhancing the robustness of the emotion recognition model.\n\nRecently, remarkable performance have been achieved in research related to utilizing auxiliary tasks for modality fusion. In many image-text tasks, such as image-text retrieval, visual entailment and visual question answering (VAQ), many researchers always take advantage of auxiliary tasks to assist image-text information to fuse  [17, 18, 19, 20, 21] . These auxiliary tasks include image-text matching, image-text contrastive learning, maskd language model, etc. In particular, the authors of ALBEF  [17]  point out that it is very effective and necessary to perform modal alignment prior to modal fusion. Similarly, in the MER task, Ghosh et al.  [5]  and Sun et al.  [22]  have respectively proposed different auxiliary tasks for multimodal emotion recognition, both achieving outstanding performance.\n\nInspired by the above studies, we propose a novel MER framework based on multitask learning for MER, named Foal-Net, which consists of two unimoal sub-networks, multiple layers of cross-attention and two auxiliary tasks. The outputs of two sub-networks first need to undergo audio-visual emotion alignment, where the audio-visual emotion alignment contrastive loss (AVEL) is computed. Subsequently, they are fed into the cross-attention network for modalities fusion. Simultaneously, we calculate negative samples for each current modality sample based on AVEL's similarity matrix and hard negative technique, forming pairs of positive-negative samples. Next, these sample pairs, along with the initially matched pairs, are used to implement corss-modal emotion label matching (MEM). The MEM serves two purposes. Firstly, it promotes modality fusion, preventing insufficient integration of modal information and recognizing the emotion of the current sample solely based on a single modality. Secondly, it directs the model to focus more on the emotional information within the current input. For modal information representation, we use CLIP embeddings instead of traditional facial expression features. This is because these embeddings not only encompass facial information but also include body language information. We have validated the effectiveness of our proposed method on",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Proposed Method",
      "text": "In this section, we will provide a detailed introduction to the proposed AVEL, MEM auxiliary tasks, and the modal fusion module used.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Audio-Video Emotion Aligning",
      "text": "Inspired by ALBEF  [17] , in order to achieve better modal fusion for complementary emotional information between modalities in the later stage, we first align the emotional information between the audio and video modalities. In other words, the goal of AVEL is to enhance the similarity between sample pairs with the same emotional category through contrastive learning, while reducing the similarity between sample pairs with different emotional categories. The input audio-video sample pairs are {X a i , X v i }, where i ∈ [0, N ], N is batch size and a, v represent audio, video respectively. First, Foal-Net leverages WavLM and CLIP to extract audio embeddings Z a ∈ R N×T ×Da and video embeddings Z v ∈ R N×F ×Dv , where T, F represent the number of frames in the audio and video, respectively, Da, Dv are the dimension of audio and video embeddings. Then, we conduct average pooling along with time dimension of Z a and Z v to ob- tain Za ∈ R N×Da and Zv ∈ R N×Dv . The Za and Zv are fed into projection blocks, which are designed to map their inputs to feature vectors of the same dimension, enabling the calculation of inter-modal similarity matrices. These operations are measured as:\n\nwhere the M LPa(•) and M LPv(•) are projection modules, which consist of two linear layers. E a ∈ R N×D and E v ∈ R N×D denote the scaled audio and video feature vectors, the ǫ is temperature hyper-parameter. The C a2v ∈ R N×N and C v2a ∈ R N×N represent inter-modal similarity matrices.\n\nThe labels Ce ∈ R N×N for the contrastive loss, as shown in Figure  2 , are set such that if the true labels are the same for different sample pairs within the same batch, the corresponding ground truth values are set to 1. Otherwise, these ground truth values are set to 0. Finally, the inter-modal emotion alignment loss is defined as:\n\nwhere the σ is log-softmax function.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Audio-Video Emotion Matching",
      "text": "The research by Sun et al.  [22]  indicates that during the training process, for some simple samples, the model can identify emotions using only the information from a single modality. This may result in insufficient fusion of modal information. Therefore, errors are more likely to occur when the model predicts complex samples. Hence, we propose the MEM auxiliary task to alleviate this issue. MEM is a binary classification task designed to enable the model to fully leverage information from both modalities to determine whether the emotional labels for the current input sample pair are consistent. We adopt the hard negative technique to generate the negative samples required for binary classification. Hard negative refers to identifying samples in other modalities that have inconsistent emotional information with the current modality sample but the highest similarity. First, we will set the value of C a2v and C v2a to negative infinity at the corresponding position where Ce has a value of 1. Then, the steps for finding the most similar negative sample from other modalities for the current sample are as follows:\n\nwhere id a2v and Z v neg respectively represent the index and CLIP embeddings of samples in the video modality. They are selected to serve as negative samples for the audio modality. Similarly, id v2a and Z a neg represent the index and WavLM embedding of samples from the audio modality, serving as negative samples for the video modality. Then, they are fed into fusion network for MEM. The operations are as follows:\n\nwhere fa(•) and fv(•) denote fusion networks for audio and video modalities respectively. The δ means average pooling along with time dimension. The CE represents Cross Entropy loss. M k p and M k n (k ∈ {a, v}) denote the output of paired positive and unpaired negative samples from the fusion network, respectively. Me ∈ R 2N×1 represents the labels used for MEM, where the first half takes the value of 1, and the second half takes the value of 0.\n\nFinally, the loss function of Foal-Net is represented as Equation  10 , where Lce is the emotion classification loss, and the value of λ is 0.01.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Modal Fusion Module",
      "text": "The fusion module we used is based on multi-head crossattention  [15] . Here, we use the representation of the speech modality as the query to illustrate the calculation method of the cross-attention mechanism. The calculation process of multihead cross-attention is as follows:\n\nwhere F a and F v denotes the embeddings from pretrained audio and video models respectively. LN , M H and C mean Layer Normalization, Multi-Head and Concatenate. The h represents the hth cross-attention. When calculating F v out , the method remains consistent with F a out , but the query is now based on the representation of the video modality. In this paper, we utilize two multi-head cross-attention layers for modal fusion, with each layer consisting of four heads. The F a out and F v out will undergo an average pooling layer along with time dimension, and then be concatenated together for MER.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments And Results",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Setup Dataset",
      "text": "The IEMOCAP  [23]  multimodal corpus is one of the most well-known databases for emotion recognition, encompassing data from three modalities: audio, video, and text. In total, it comprises 5 sessions, each including one male and one female speaker. To be consistent and compare with previous studies, we conduct experiments with 5,531 audio utterances of four emotion categories happy (happy & excited, 1,636), angry (1,103), sad (1,084) and neutral  (1, 708) . We perform five-fold cross-validation using a leave-one-session-out strategy on the corpus to evaluate the effectiveness of our proposed method. The weighted accuracy (WA) and unweighted accuracy (UA) are used as metrics in line with previous methods.\n\nExperimental Details The feature dimensions of the WavLM 2  and CLIP 3  image encoders are 1024 and 768, respectively. The Projection module has 512 neurons, with a dropout rate of 0.5. During training, the batch size is set to 64, the learning rate is a constant 1e-4, and the optimizer is AdamW. The dropout rate for cross-attention is 0.1. The input for the audio modality consists of 6 seconds of speech with a sampling rate",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Methods",
      "text": "Type Year UA(%) WA(%) FTSLLM  [24]  A+T 2023 78.50 77.70 MSMSER  [12]  A+T 2023 76.40 75.20 ATMF  [22]  A+T 2023 79.71 78.42 RMER  [25]  A+T 2023 77.00 76.00 MTTX  [26]  A+T 2023 75.00 74.50 CRNN-SAN  [14]  A+T 2023 79.95 78.82 BAM  [27]  A+T 2023 77.00 75.50 MSER  [28]  A+T 2024 76.56 77.20\n\nMMAN  [29]  A+V 2020 -73.94 AM-FBP  [30]  A+V 2021 75.49 -MCWSA-CMHA  [31]  A+V 2022 78.90 -GCNet  [16]  A+V 2023 78.36 -Foal-Net A+V -80.10 79.45 of 16 kHz. Due to the simultaneous appearance of two speakers in the video recording, it is necessary to separate the speakers in the frame. In the end, the input for the video modality consists of 180 individual images. Any questions regarding the parameters in the method we proposed, please refer to the github repository.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Performance Comparison With Previous Methods",
      "text": "The effectiveness of the method we proposed can be underscored by contrasting it with the latest significant findings derived from the IEMOCAP corpus, as show in Table  1 . It demonstrates that the best UA (80.10%) and WA (79.45%) are achieved by Foal-Net. Research on facial expression recognition in the IEMOCAP corpus is relatively scarce, primarily due to two reasons. Firstly, the video frames are relatively blurry, making it challenging to extract facial expression features. Secondly, the issue of speakers appearing in the same frame in the videos complicates the data processing. However, based on the experimental results, the performance of the audio-video combination is not inferior to that of the audio-text combination. This suggests that besides facial information, other details in the images also contribute to emotion recognition. Meanwhile, it highlights the outstanding performance of the Foal-Net.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiments And Analysis",
      "text": "We conducted a series of ablation experiments to validate the effectiveness of the features we used, the proposed auxiliary tasks, and the final model. The baseline is the Foal-Net without two auxiliary tasks. To validate the effectiveness of global image encoding for emotion recognition compared to local facial information encoding, we extracted deep representations from three pretrained models (EmoNet  [32] , Resnet-FER2013  [10] , SENet-FER2013  [10] ) for facial expression recognition and utilized them for emotion recognition. As shown in Table  2 , their performance is significantly lower than that of the CLIP model's features. This indicates that, in addition to facial expressions, body movements and other information in the images also contribute to emotion recognition. Furthermore, another reason for the superior performance of CLIP features is the inclusion of semantic information.\n\nIn addition, we observe that after incorporating the AVEL task, UA and WA improved by 2.2% and 1.36%, respectively, compared to the baseline. This indicates the crucial necessity of cross-modal emotion alignment before modal fusion. On the contrary, the performance improvement is not very significant when introducing the MEM task alone. This is because the use of hard negative for finding negative samples heavily relies on the similarity matrix from the AVEL task. When the similarity matrix is not optimized, it cannot bring positive benefits to the MEM task. This also indirectly confirms the necessity of alignment before fusion. When we introduce both the AVEL and MEM tasks simultaneously, the model's performance is further improved compared to using AVEL or MEM alone. This demonstrates that with the assistance of AVEL, MEM can facilitate the fusion of modal information and enhance the model's capability for emotion recognition.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper, we propose a novel framework called Foal-Net for MER, which includes two auxiliary tasks: AVEL and MEM. Our research on the AVEL task has demonstrated the effectiveness and necessity of initially aligning emotional information across modalities before integrating inter-modality information. The MEM task can guide modality fusion and make fusion module focus more on emotional information during the fusion process with the assistance of the AVEL task. Under the influence of both tasks, Foal-Net achieves SOTA performance. Moreover, we show that the embeddings of CLIP outperform facial expression features in MER. In the future work, we will optimize the MEM task as its performance relies on AVEL and validate the universality of our method by substituting the video modality with the text modality.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: An overview of Foal-Net architecture, which mainly consists of two auxiliary tasks (AVEL, MEM), and modal fusion module.",
      "page": 2
    },
    {
      "caption": "Figure 2: The example for AVEL. The horizontal and vertical",
      "page": 2
    },
    {
      "caption": "Figure 2: , are set such that if the true labels are the same for",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "!": "\"",
          "\"": "\""
        },
        {
          "!": "!",
          "\"": "\""
        },
        {
          "!": "\"",
          "\"": "\""
        },
        {
          "!": "\"",
          "\"": "!"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "!!\"! !!\"&": "!&\"! !&\"&",
          "!!\"# !!\"$ !!\"%": "!&\"# !&\"$ !&\"%"
        },
        {
          "!!\"! !!\"&": "!#\"! !#\"&",
          "!!\"# !!\"$ !!\"%": "!#\"# !#\"$ !#\"%"
        },
        {
          "!!\"! !!\"&": "!$\"! !$\"&",
          "!!\"# !!\"$ !!\"%": "!$\"# !$\"$ !$\"%"
        },
        {
          "!!\"! !!\"&": "!%\"! !%\"&",
          "!!\"# !!\"$ !!\"%": "!%\"# !%\"$ !%\"%"
        }
      ],
      "page": 2
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Cold fusion: Calibrated and ordinal latent distribution fusion for uncertainty-aware multimodal emotion recognition",
      "authors": [
        "M Tellamekala",
        "S Amiriparian",
        "B Schuller",
        "E André",
        "T Giesbrecht",
        "M Valstar"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "3",
      "title": "Episodic Memory For Domain-Adaptable, Robust Speech Emotion Recognition",
      "authors": [
        "J Tavernor",
        "M Perez",
        "E Provost"
      ],
      "year": "2023",
      "venue": "Proc. INTERSPEECH 2023"
    },
    {
      "citation_id": "4",
      "title": "Personalized Adaptation with Pre-trained Speech Encoders for Continuous Emotion Recognition",
      "authors": [
        "M Tran",
        "Y Yin",
        "M Soleymani"
      ],
      "year": "2023",
      "venue": "Proc. INTERSPEECH 2023"
    },
    {
      "citation_id": "5",
      "title": "SWRR: Feature Map Classifier Based on Sliding Window Attention and High-Response Feature Reuse for Multimodal Emotion Recognition",
      "authors": [
        "Z Zhao",
        "T Gao",
        "H Wang",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "Proc. INTERSPEECH 2023"
    },
    {
      "citation_id": "6",
      "title": "MMER: Multimodal Multi-task Learning for Speech Emotion Recognition",
      "authors": [
        "S Ghosh",
        "U Tyagi",
        "S Ramaneswaran",
        "H Srivastava",
        "D Manocha"
      ],
      "year": "2023",
      "venue": "Proc. INTERSPEECH 2023"
    },
    {
      "citation_id": "7",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "8",
      "title": "Wavlm: Large-scale selfsupervised pre-training for full stack speech processing",
      "authors": [
        "S Chen",
        "C Wang",
        "Z Chen",
        "Y Wu",
        "S Liu",
        "Z Chen",
        "J Li",
        "N Kanda",
        "T Yoshioka",
        "X Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "9",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "M.-W Kenton",
        "L Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of NAACL-HLT"
    },
    {
      "citation_id": "10",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal",
        "J Du",
        "M Joshi",
        "D Chen",
        "O Levy",
        "M Lewis",
        "L Zettlemoyer",
        "V Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "11",
      "title": "Facial expression recognition based on optimized resnet,\" in 2020 2nd World Symposium on Artificial Intelligence (WSAI)",
      "authors": [
        "Y Zhong",
        "S Qiu",
        "X Luo",
        "Z Meng",
        "J Liu"
      ],
      "year": "2020",
      "venue": "Facial expression recognition based on optimized resnet,\" in 2020 2nd World Symposium on Artificial Intelligence (WSAI)"
    },
    {
      "citation_id": "12",
      "title": "Masked autoencoders are scalable vision learners",
      "authors": [
        "K He",
        "X Chen",
        "S Xie",
        "Y Li",
        "P Dollár",
        "R Girshick"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "13",
      "title": "Exploring complementary features in multi-modal speech emotion recognition",
      "authors": [
        "S Wang",
        "Y Ma",
        "Y Ding"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "14",
      "title": "Pre-trained model representations and their robustness against noise for speech emotion analysis",
      "authors": [
        "V Mitra",
        "V Kowtha",
        "H.-Y Chien",
        "E Azemi",
        "C Avendano"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "15",
      "title": "Multimodal emotion recognition based on deep temporal features using crossmodal transformer and self-attention",
      "authors": [
        "B Maji",
        "M Swain",
        "R Guha",
        "A Routray"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "16",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "17",
      "title": "Gcnet: graph completion network for incomplete multimodal learning in conversation",
      "authors": [
        "Z Lian",
        "L Chen",
        "L Sun",
        "B Liu",
        "J Tao"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "18",
      "title": "Align before fuse: Vision and language representation learning with momentum distillation",
      "authors": [
        "J Li",
        "R Selvaraju",
        "A Gotmare",
        "S Joty",
        "C Xiong",
        "S Hoi"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "19",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "C Hallacy",
        "A Ramesh",
        "G Goh",
        "S Agarwal",
        "G Sastry",
        "A Askell",
        "P Mishkin",
        "J Clark"
      ],
      "year": "2021",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "20",
      "title": "Blip: Bootstrapping languageimage pre-training for unified vision-language understanding and generation",
      "authors": [
        "J Li",
        "D Li",
        "C Xiong",
        "S Hoi"
      ],
      "year": "2022",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "21",
      "title": "Image as a foreign language: Beit pretraining for vision and vision-language tasks",
      "authors": [
        "W Wang",
        "H Bao",
        "L Dong",
        "J Bjorck",
        "Z Peng",
        "Q Liu",
        "K Aggarwal",
        "O Mohammed",
        "S Singhal",
        "S Som"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "22",
      "title": "Vlmo: Unified vision-language pre-training with mixture-of-modality-experts",
      "authors": [
        "H Bao",
        "W Wang",
        "L Dong",
        "Q Liu",
        "O Mohammed",
        "K Aggarwal",
        "S Som",
        "S Piao",
        "F Wei"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "23",
      "title": "Using auxiliary tasks in multimodal fusion of wav2vec 2.0 and bert for multimodal emotion recognition",
      "authors": [
        "D Sun",
        "Y He",
        "J Han"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "24",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "25",
      "title": "Exploiting fine-tuning of self-supervised learning models for improving bi-modal sentiment analysis and emotion recognition",
      "authors": [
        "W Yang",
        "S Fukayama",
        "P Heracleous",
        "J Ogata"
      ],
      "year": "2022",
      "venue": "INTER-SPEECH"
    },
    {
      "citation_id": "26",
      "title": "Robust multi-modal speech emotion recognition with asr error adaptation",
      "authors": [
        "B Lin",
        "L Wang"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "27",
      "title": "Multilevel transformer for multimodal emotion recognition",
      "authors": [
        "J He",
        "M Wu",
        "M Li",
        "X Zhu",
        "F Ye"
      ],
      "year": "2023",
      "venue": "ICASSP 2023 -2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "28",
      "title": "Knowledge-aware bayesian coattention for multimodal emotion recognition",
      "authors": [
        "Z Zhao",
        "Y Wang",
        "Y Wang"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "29",
      "title": "Mser: Multimodal speech emotion recognition using cross-attention with deep fusion",
      "authors": [
        "M Khan",
        "W Gueaieb",
        "A Saddik",
        "S Kwon"
      ],
      "year": "2024",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "30",
      "title": "Multi-Modal Attention for Speech Emotion Recognition",
      "authors": [
        "Z Pan",
        "Z Luo",
        "J Yang",
        "H Li"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "31",
      "title": "Information fusion in attention networks using adaptive and multilevel factorized bilinear pooling for audio-visual emotion recognition",
      "authors": [
        "H Zhou",
        "J Du",
        "Y Zhang",
        "Q Wang",
        "Q.-F Liu",
        "C.-H Lee"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "32",
      "title": "Multichannel weight-sharing autoencoder based on cascade multi-head attention for multimodal emotion recognition",
      "authors": [
        "J Zheng",
        "S Zhang",
        "Z Wang",
        "X Wang",
        "Z Zeng"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "33",
      "title": "Emonets: Multimodal deep learning approaches for emotion recognition in video",
      "authors": [
        "S Kahou",
        "X Bouthillier",
        "P Lamblin",
        "C Gulcehre",
        "V Michalski",
        "K Konda",
        "S Jean",
        "P Froumenty",
        "Y Dauphin",
        "N Boulanger-Lewandowski"
      ],
      "year": "2016",
      "venue": "Journal on Multimodal User Interfaces"
    }
  ]
}