{
  "paper_id": "2403.16540v1",
  "title": "Enhancing Cross-Dataset Eeg Emotion Recognition: A Novel Approach With Emotional Eeg Style Transfer Network",
  "published": "2024-03-25T08:31:54Z",
  "authors": [
    "Yijin Zhou",
    "Fu Li",
    "Yang Li",
    "Youshuo Ji",
    "Lijian Zhang",
    "Yuanfang Chen"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Recognizing the pivotal role of EEG emotion recognition in the development of affective Brain-Computer Interfaces (aBCIs), considerable research efforts have been dedicated to this field. While prior methods have demonstrated success in intra-subject EEG emotion recognition, a critical challenge persists in addressing the style mismatch between EEG signals from the source domain (training data) and the target domain (test data). To tackle the significant inter-domain differences in cross-dataset EEG emotion recognition, this paper introduces an innovative solution known as the Emotional EEG Style Transfer Network (E 2 STN). The primary objective of this network is to effectively capture content information from the source domain and the style characteristics from the target domain, enabling the reconstruction of stylized EEG emotion representations. These representations prove highly beneficial in enhancing cross-dataset discriminative prediction. Concretely, E 2 STN consists of three key modules-transfer module, transfer evaluation module, and discriminative prediction module-which address the domain style transfer, transfer quality evaluation, and discriminative prediction, respectively. Extensive experiments demonstrate that E 2 STN achieves state-of-the-art performance in cross-dataset EEG emotion recognition tasks.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "In the 21st century, brain-computer interface (BCI) technology emerges as a novel avenue for human-computer interaction, offering a novel communication paradigm against the background of the burgeoning metaverse  [Guo and Gao, 2022] . Given the pivotal role of emotion in humancomputer interaction, affective Brain-Computer Interfaces (aBCIs) have attracted significant attention across interdisciplinary fields  [Fiorini et al., 2020] . The aBCIs predominantly rely on two modalities-behavioral signals and physiological signals-for emotion recognition  [He et al., 2020] . Compared with behavioral signals, such as facial expressions, speech, and text, it is more reliable to distinguish the spontaneous emotion state through physiological signals, such as electrocardiogram (ECG), electrooculogram (EOG), electromyogram (EMG), and electroencephalogram (EEG)  [Song et al., 2021] . Among these physiological signals, EEG signals originating in the cerebral cortex are particularly associated with spontaneous emotional states  [He et al., 2020] . And with the development of wearable non-invasive EEG acquisition equipment in recent years, more and more researches are focusing on the field of EEG emotion recognition. The existing research on EEG emotion recognition has predominantly concentrated on intra-subject tasks  [Xiao et al., 2022] . For instance, considering the abundant saptial information in EEG signals,  Song et al. proposed  to convert multi-channel EEG signals into an image format, which converts the question of EEG emotion recognition into image recognition. In this regard, they introduced a novel EEGto-image method and a graph-embedded convolutional neural network (GECNN) approach. The effectiveness of GECNN was validated through extensive experiments on four public datasets  [Song et al., 2022] . Additionally,  Zhou et al. introduced a Progressive Graph Convolution Network (PGCN)  for EEG emotion recognition, leveraging insights from neuroscience on dynamic brain relationships. PGCN achieves state-of-the-art performance by progressively learning discriminative features from coarse-to fine-grained emotion categories  [Zhou et al., 2023] . Despite the numerous methods proposed for EEG emotion recognition in recent years, there are significant issues that merit thorough investigation to advance this field. The primary concern is the proto-col for EEG emotion recognition. Existing protocols for EEG emotion recognition mostly involve intra-subject and cross-subject classification tasks, where training and test EEG data originate from the same experimental environment. The performance variation across different experimental environments, especially in cross-dataset EEG emotion recognition, needs further exploration. To clearly and intuitively show the differences in EEG data distribution, T-SNE technology was employed to visualize the EEG data of different subjects in different datasets, as illustrated in Fig.  1 . Notably, substantial differences exist in the EEG data distribution among different subjects of the same dataset, which are more significant among diverse datasets.\n\nThe second critical issue involves addressing domain differences. Recent studies have attempted to tackle domain shift in cross-subject EEG emotion recognition tasks. For example, Li et al. proposed a multisource transfer learning method, treating existing subjects as sources and the new subject as a target to achieve style transfer mapping  [Li et al., 2020] . Advanced performance in addressing distribution differences between training and test data in cross-subject EEG emotion recognition tasks has been demonstrated by methods like  BiDANN [Li et al., 2018]  and  TANN [Li et al., 2021] . However, the inter-domain differences in crossdataset EEG emotion recognition surpass those observed in the cross-subject EEG emotion recognition task, as depicted in Fig.  1 . Minimizing these differences between domains holds promise for improving cross-dataset EEG emotion recognition and enhancing generalization to new emotional EEG data.\n\nTo tackle these issues, we propose an Emotional EEG Style Transfer Network (E 2 STN) in this study to obtain stylized emotional EEG representations. These representations encapsulate emotion content of the source domain and style characteristics of the target domain, enabling the model to make discriminative predictions for cross-dataset emotional EEG samples. Specifically, E 2 STN comprises three unique modules: the transfer module, transfer evaluation module, and discriminative prediction module. The transfer module reorganizes emotional pattern information from the source domain and statistical style from the target domain to generate new stylized EEG representations. The transfer evaluation module, incorporating content-aware loss, style-aware loss, and identity loss, ensures the precise fusion of information from both domains. The discriminative prediction module, utilizing a dynamic graph convolutional network and fully connected layers, extracts deep features for discriminative predictions. Joint optimization with cross-entropy loss and transfer evaluation losses guides the entire model for comprehensive cross-dataset EEG emotion recognition.\n\nTo the best of our knowledge, this work is the first to reorganize emotional content information from the source domain and statistical characteristic style from the target domain into new stylized EEG representations to enhance cross-dataset EEG emotion recognition. The proposed E 2 STN generates stylized emotional EEG representations and further performs discriminative predictions from source domain and stylized representations. The joint loss optimization ensures precise fusion of complementary information and guides discrimi-native prediction for cross-dataset EEG emotion recognition. Extensive experiments validate E 2 STN's state-of-the-art performance in cross-dataset EEG emotion recognition tasks.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Proposed Method For Emotion Recognition",
      "text": "To elucidate the proposed method, we present the framework of E 2 STN in Fig.  2 . The primary objective of this network is to restructure the emotional content information from the source domain and the statistical characteristic style from the target domain, yielding newly stylized source domain EEG representations. These representations are crucial for the successful execution of cross-dataset EEG emotion recognition tasks. We adopt three key modules to achieve this goal, i.e., the transfer module, transfer evaluation module, and discriminative prediction module. The transfer module focuses on generating stylized emotional EEG representations that encapsulate both the emotional content information from the source domain and the statistical characteristic style from the target domain. Subsequently, the discriminative prediction module processes the source domain and stylized EEG representations for effective cross-dataset EEG emotion recognition. Simultaneously, the transfer evaluation module extracts multi-scale spatio-temporal features from the source domain and stylized EEG representations, constructing multidimensional losses that intricately guide the emotional EEG style transfer process. In the following, we introduce the details of the proposed E 2 STN model.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Obtaining Stylized Emotional Eeg Samples",
      "text": "To obtain emotional EEG representations that contain both the emotional content information of the source domain and the statistical characteristics style of the target domain, the transfer process is divided into two essential steps. The initial step involves constructing transfer encoders corresponding to the source and target domains. The two encoders capture the global dependencies within the domain-specific information of distinct fields (i.e., the emotional content of the source domain and the style characteristics of the target domain), respectively. Drawing inspiration from the Transformer method  [Vaswani et al., 2017] , the encoders of E 2 STN employ multi-head self-attention layers to assign dynamic weights to different EEG channels. This dynamic weighting, guided by domain-specific information, highlights the more significant electrode dependencies within the specific domain. Such dynamic dependencies, enriched with domainspecific information, have stronger capabilities in representing their corresponding domain characteristics. The second crucial step in the transfer process is the fusion of source domain content information and target domain style information within the decoder, yielding stylized emotional EEG features. The decoder achieves this by iteratively combining content and style information through a multi-layer structure, applying the target domain style to the source domain EEG features. The use of the residual connection method in the decoder layer ensures that the content information of the source domain remains undistorted throughout the fusion process. Finally, a CNN decoder, comprising multiple convolutional layers, reconstructs the stylized EEG features into representations of the same dimension as the input. Specifically, we utilize the B frequency bands EEG representations after pre-decomposition of the raw EEG signals.\n\nThe input emotional EEG representations of the proposed model corresponding to the source and target domains are denoted as X s ∈ R C×B and X t ∈ R C×B , respectively, where C is the number of EEG channels. Two corresponding encoders are employed to extract domain-specific information from their respective source and target domain EEG representations. The structure of the encoder layer is illustrated in Fig.  3  (a). X s and X t undergo encoding into query (Q), key (K), and value (V) vectors within their respective encoders. The subsequent explanation focuses on X s to illustrate the encoding process, as shown in fomula (1), (  2 ), (3), (4).",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Multi-Head Attention",
      "text": "Linear Projection\n\nMulti-head Attention\n\nMulti-head Attention\n\nwhere W q s , W k s , W v s ∈ R B×m are trainable linear projection matrices. To enable the encoder to pay attention to the in-\n\np is the number of attention heads. Then the multi-head self-attention (MSA) can be calculated by:\n\nwhere\n\nrepresents the output of each attention head.\n\nTo maintain domain-specific information, the MSA matrix is added to the Q vector and subsequently subjected to layer normalization. This operation can be expressed as:\n\nSimilarly, we can easily obtain the domain-specific features of the target domain H E t through the above formulas. To integrate the emotional content information of the source domain and the statistical characteristic style of the target domain, we devise a three-layer transfer decoder, which applies the style of the target domain to the emotional features of the source domain progressively. The structure of a single decoder layer is depicted in Fig.  3 (b) . The source domain features H E s , encapsulating the emotional content information of the source domain, serve as the primary focus of transfer. These features are utilized as the query vectors for the first decoder layer. To make the source domain features more similar to the taget domain style, the target domain features H E t are employed as key and value vectors for the first decoder layer, which calculates a similarity matrix with the query vectors to weigh the emotional content features H E s . Specifically, Q D 1 , K D 1 , and V D 1 are obtained through linear projection, as illustrated in formula (5).\n\n(5) Subsequently, two MSA layers and one fully connected feedforward network (FFN) are employed in the first decoder layer with residual connections. The output of the first decoder layer is then passed on to the second decoder layer, and so forth. Consequently, we can readily derive the output H D ∈ R C×m of the transfer decoder using formulas (  5 ), (  3 ), (4).\n\nTo restore the dimension of the stylized features, a twolayer Convolutional Neural Network (CNN) decoder is employed to refine the output of the transfer decoder H D . This process allows reshaping of the stylized EEG features H D ∈ R C×m into generated stylized EEG representations Xs ∈ R C×B . These stylized EEG representations Xs have the same emotion labels as their corresponding source domain EEG representations.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Obtaining Discriminative Features And Predictions",
      "text": "After obtaining the stylized source-domain EEG representations, a dynamic graph network is constructed to extract deep features, enabling E 2 STN to learn discriminative features from the source domain and stylized EEG representations. Both the source EEG representations X s and the corresponding stylized EEG representations Xs are jointly input into the discriminative prediction module to obtain discriminative features and predictions. Concretely, the data-driven graph of the dynamic graph network is characterized by an adjacency matrix G, which dynamically adjusts based on the input representations X s and Xs . To ensure that G contains intrachannel spatial information and frequency band information, two trainable matrices W s ∈ R C×C and W f ∈ R B×(C * B) are left-and right-multiplied by the input features, respectively. This process can be expressed as follows:\n\nwhere X = X s / Xs , ReLU is applied to the output to guarantee non-negative elements, the bias matrix B ∈ R C×B is used to increase the flexibility of graph structure representation. Then, the adjacency matrix G is reshaped into B adjacency matrices, i.e., G = [G * 1 , . . . , G * B ] ∈ R C×C×B , to represent graphs in B frequency bands.\n\nTo avoid the high computational complexity associated with direct graph Fourier transform based on graph filtering theories, we adopt Chebyshev polynomials to approximate the graph convolution operation  [Kipf and Welling, 2017] . Let φ k (G) = G k denotes the k-order polynomial of the adjacency matrix G. Consequently, the high-level features extracted by the dynamic Graph Convolutional Network (GCN) can be expressed as follows:\n\nwhere φ k (G) is the k-th level graph, F is the output dimension for the graph convolution operation.\n\nFollowing that, the discriminative prediction module, acting as the supervision term of the E 2 STN, employs two fully connected (FC) layers to predict the class labels. Therefore, the output of the second FC layer H FC ∈ R 1×P can be easily deduced, where P is the output dimension of the FC layer. Consequently, the cross-entropy loss of E 2 STN, aiming to achieve cross-dataset EEG emotion recognition, can be expressed as:\n\nwhere Y = {y 1 , . . . , y P } ∈ R 1×P represents the groundtruth label; Ŷ = { ŷ1 , . . . , ŷP } ∈ R 1×P is the discriminative prediction from the softmax layer of E 2 STN.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Multi-Objective Joint Optimization",
      "text": "To optimize stylized emotional EEG representations, we specially propose a transfer evaluation module to constrain the style transfer process. In the emotional style transfer process, we primarily consider three factors, leading to the formulation of three corresponding losses: content-aware loss L c , style-aware loss L s , and identity loss L id . The first crucial consideration is preserving the emotional content information of the source domain during the transfer process. We regard the features extracted by the convolutional layer as containing the content information of the respective domain  [Gatys et al., 2016] . Therefore, the content-aware loss is constructed from the features extracted by multiple unique convolutional layers in the transfer evaluation module. This loss can be expressed as:\n\nwhere f i (•) denotes the convolution operation function of the i-th layer in the transfer evaluation module, and ∥•∥ 2 represents the ℓ 2 -norm. Another crucial aspect in the transfer process is ensuring that the style characteristics of the stylized emotional EEG representations closely resemble those of the target domain. The Gram matrix of features extracted by the convolutional layer is considered representative of the statistical characteristics of the target domain  [Gatys et al., 2016] . Consequently, we construct the style-aware loss for the target domain by considering the statistics (e.g., mean and variance) of each convolutional layer in the transfer evaluation module.\n\nwhere µ(•) and σ(•) denote the mean and variance of the features, respectively. In the final consideration, aiming to preserve more accurate content and style information in self-style transfer, we introduce an identity loss to ensure the undistorted nature of stylized EEG representations during the progressive transfer process. Specifically, for lossless and unbiased transfer of emotional EEG representations, we input the same representation X s /X t into the source and target domain transfer encoders. The resulting stylized emotional EEG representation, denoted as Xss / Xtt , should be identical to the original X s /X t . Consequently, the identity loss L id can be defined as:\n\nTo ensure that the features extracted by multiple convolutional layers in the transfer evaluation module contain multidimensional and multi-scale spatio-temporal information, we employ three distinct convolution convolutional kernels to construct the convolutional network.  D) , where D is a depth parameter controlling the number of spatial filters. 3. Separable convolution: Building upon depthwise convolution, separable convolution utilizes a convolution kernel of size (1, 3) and F 2 pointwise convolutions to optimally merge the spatial features. This process compresses the feature H dc into H sc ∈ R 1×B×F2 in the channel dimension. where H c , H dc , and H sc correspond to the features extracted by each convolutional layers in formulas (  9 ), (10), and (11), respectively.\n\nIn conclusion, the transfer losses in the transfer evaluation module and the cross-entropy loss in the discriminative prediction module collectively form a multi-objective joint optimization loss function L. L = L c + λL s + νL id + ξL ce , (12) where λ, ν, ξ are hyper-parameters used to control the proportion between the optimization loss functions. E 2 STN is iteratively optimized by minimizing L, and the emphasis on transferring tasks and classification tasks is achieved by adjusting the hyperparameters. The training procedure for E 2 STN is outlined in the Algorithm 1 of Appendix A. Further implementation details of E 2 STN are provided in Appendix C.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiment Protocol",
      "text": "The objective of this paper is to investigate cross-dataset EEG emotion recognition tasks. In alignment with the principles of previous experiments, we establish groups of experiment protocols for cross-dataset EEG emotion recognition based on SEED  [Zheng and Lu, 2015] , SEED-IV  [Zheng et al., 2018] , and MPED  [Song et al., 2019]  datasets, with detailed information provided in Appendix B. Table  I  of Appendix B summarizes the setup details of the cross-dataset EEG emotion recognition experiments. In brief, to maintain category balance in the training samples, we choose neutral, sad, and happy (joy) emotions of SEED, SEED-IV, and MPED datasets for 3-category cross-dataset experiments. For 4-category cross-dataset experiments, we choose neutral, happy (joy), sad, and fear emotions from SEED-IV and MPED datasets. All subjects' samples from one dataset are considered as source domain data, while one subject's samples from another dataset are utilized as target domain data. This approach allows us to conduct experiments on two types of cross-dataset EEG emotion recognition tasks, involving three and four categories of emotions. For instance, we denote the 3-category cross-dataset EEG emotion recognition experiment using the MPED 3 dataset as the source domain and the SEED 3 dataset as the target domain as MPED 3 → SEED 3 in this paper.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiment Results",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "3-Category Cross-Dataset Eeg Emotion Recognition",
      "text": "To evaluate the performance of our model in cross-dataset EEG emotion recognition, we conduct extensive experiments following the specified protocols. In comparison with other advanced methods of EEG emotion recognition, we replicate the same experiments using 7 alternative methods. We either quote or reproduce their results from the literature to ensure a convincing comparison with the proposed method. The evaluation criteria for all subjects in the test dataset include mean accuracy (ACC) and standard deviation (STD). The experiment results are presented in Table  1 . (2) 4-category Table  1 : 3-category cross-dataset classification performance for EEG emotion recognition on SEED, SEED-IV, and MPED.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Method",
      "text": "ACC / STD (%)  and Vandewalle, 1999]  48 Table  1  showcases the superior performance of the proposed E 2 STN model in cross-dataset EEG emotion recognition experiments, confirming the efficacy of the method in transfer and recognition tasks. Notably, E 2 STN achieves the highest accuracy of 73.51% in the MPED 3 → SEED 3 task, surpassing compared advanced algorithms significantly. In comparison with the domain adaptation method TANN, E 2 STN demonstrates a notable accuracy improvement of 09.28% (73.51% vs 64.23%) in the 3-category classification of the MPED 3 → SEED 3 task. Additionally, in the SEED-IV 3 → SEED 3 task, E 2 STN achieves a 02.06% (60.51% vs 58.45%) enhancement compared to the advanced method PGCN.\n\nIn MPED 3 → SEED-IV 3 and SEED 3 → SEED-IV 3 experiments (4th and 5th columns of Table  1 ), the recognition performance of E 2 STN experiences a decline, potentially due to reduced emotional feature discrimination when the SEED-IV dataset captures finer emotion nuances. Meanwhile, the similar classification performance in these tasks (62.32% vs. 61.24%) underscores the effectiveness of the proposed method in eliminating the domain shift problem. Regarding the MPED dataset, which contains more emotion categories and exhibits less discrimination between emotions, resulting in a further decline in model performance (6th and 7th columns of Table  1 ), E 2 STN still outperforms other advanced methods. To validate the confidence of our experimental results, we perform the t-test statistical analysis  [Hanusz et al., 2016]  on each reproduced accuracy result. The Shapro-Wilk test (S-W test)  [Semenick, 1990]  is initially conducted to eliminate accuracy data that does not follow the normal distribution hypothesis. The results show that our proposed E 2 STN exhibits significantly better (p < 0.05) performance in each cross-dataset task. This statistical analysis indicates that our proposed method effectively reduces interdomain differences among different datasets, achieving efficient cross-dataset EEG emotion recognition.\n\nTo explore which emotion is more easily recognized by the proposed model, we present confusion matrices based on the results of E 2 STN, depicted in Fig.  4  (1). Several observations can be made from these matrices. Except for the SEED 3 → MPED 3 experiment (Fig.  4  (f)), the recognition accuracy of 'happiness' emotion is consistently higher than that of 'sadness', with an average difference of 24.56%. This suggests that 'happiness' is more distinguishable than 'sadness' across different datasets, indicating that 'happiness' emotion is more universally induced. Furthermore, compared with 'happi-ness', the average accuracy of 'sadness' is 40.38%. The lower recognition accuracy for 'sadness' is attributed to its tendency to be mistaken for 'neutrality', especially in Fig.  4 (a), (b), (d), and (f). This phenomenon may be due to the weak stimulation of \"sadness\" in these experiments. In the MPED 3 → SEED-IV 3 and SEED 3 → SEED-IV 3 experiments, similar observations between Fig.  4(c ) and (d) are made when SEED-IV is the target dataset. The recognition accuracies of E 2 STN for the three emotions follow this order: 'neutrality' >'happiness' >'sadness', suggesting that increased emotional categories in the SEED-IV dataset result in more subtle emotional changes and increased difficulty in recognition. For the SEED 3 → MPED 3 experiment in Fig.  4 (f), the recognition accuracy for 'sadness' is highest, contrary to other experiment results. The greater difference in emotional categories between the SEED and MPED datasets may contribute to the more pronounced recognition of 'sadness' emotions.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "4-Category Cross-Dataset Eeg Emotion Recognition",
      "text": "To assess the effectiveness of E 2 STN across a broader range of emotional categories, we conduct additional 4-category cross-dataset EEG emotion recognition experiments. In parallel, we performed comparative experiments with the same advanced methods. The results of these experiments are detailed in Table  2 . In contrast to the 3-category cross-dataset EEG emotion recognition, the expansion of emotion categories results in a performance decrease for E 2 STN. However, E 2 STN consistently achieves the highest accuracy (53.75% and 36.78%) compared to other advanced methods. In the MPED 4 → SEED-IV 4 experiment, E 2 STN outperforms the state-of-theart method GECNN by 02.89%. Similarly, it surpasses the state-of-the-art method PGCN by 0.27% in the SEED-IV 4 →  MPED 4 task. Consistent with the 3-category cross-dataset EEG emotion recognition experiment, the accuracy of the MPED dataset as the target domain is slightly lower than that of the SEED-IV dataset. This may be attributed to the MPED dataset collecting more emotion categories, resulting in subtle feature differences between emotions and making transfer more challenging.\n\nFor the 4-category cross-dataset EEG emotion recognition experiments, confusion matrices in Fig.  4  (2) reveal that 'happiness' and 'fear' emotions are easier to be recognized than 'sadness'. The 'sadness' emotion is prone to confusion with 'fear', especially in Fig.  4 (h), aligning with neuroscience research  [Kragel and LaBar, 2016]  that negative emotions (such as 'sadness' and 'fear') have quite similar Euclidean distances. Additionally, the recognition accuracy of 'neutrality' in the MPED 4 → SEED-IV 4 experiment is higher than in SEED-IV 4 → MPED 4 (54.06% vs 29.12%), which is reflected in the overall recognition results (53.75% vs 36.78% in Table  2 ).",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Discussion",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Effect Of The Transfer Module",
      "text": "To validate the effectiveness of the proposed transfer module, we modify the E 2 STN framework, retaining only the discriminative prediction module, denoted as E 2 STN-t. E 2 STNt follows the same experimental protocols as E 2 STN but is trained solely on labeled source domain samples rather than source domain and stylized EEG samples. The experiment results are presented in Table  3  and 4 . Compared with E 2 STNt, E 2 STN has a substantial improvement in the performance of 3-and 4-category cross-dataset EEG emotion recognition experiments. In Table  3 , E 2 STN enhances the recognition accuracy by an average of 05.69%, while in Table  4  of the 4-category cross-dataset experiments, the average increase is 04.56%. This demonstrates that stylized emotional EEG representations effectively enhance the performance of E 2 STN for cross-dataset EEG emotion recognition, providing further confirmation of the proposed transfer module's effectiveness.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Exploring The Importance Of Emotion-Related Brain Regions",
      "text": "To explore a more explicit understanding of the contribution of different brain functional regions for EEG emotion recognition, we depict the electrode activity maps in Fig.  5 . The contribution of each brain region is evident in the visualization of advanced features H DG , extracted by the dynamic graph convolutional layer in the discriminative prediction module. Darker red areas in the figure signify higher contributions from corresponding brain regions. The activation of the frontal and temporal lobes is prominently visible, aligning with established neuroscience research  [Alarcão and Fonseca, 2019] . This observation indicates that E 2 STN captures the most crucial emotion-related features in both source domain and stylized EEG representations, providing further evidence of the excellent performance of the proposed method for cross-dataset EEG emotion recognition.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Conclusion",
      "text": "In this study, we introduce an emotional EEG style transfer network, E 2 STN, designed to facilitate effective crossdataset EEG emotion recognition. Three modules are constructed to accomplish the tasks of transfer, transfer evaluation, and discriminative prediction. The transfer module effectively minimizes inter-domain differences in data distribution across diverse datasets, generating stylized emotional EEG representations. The transfer evaluation module extracts multi-scale spatio-temporal features from the source domain and stylized EEG representations, constructing multidimensional losses to guide the emotional EEG style transfer process. The discriminative prediction module is jointly trained using both source domain and stylized EEG representations to achieve accurate prediction for cross-dataset experiments. Extensive experiments prove the effectiveness of E 2 STN in cross-dataset EEG emotion recognition tasks. Additionally, our exploration of important brain regions related to emotion provides valuable insights into neurophysiology. For future research, we aim to delve deeper into the transfer rules of emotional EEG signals to further enhance the performance of cross-dataset EEG emotion recognition.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Distribution of EEG data in different subjects and",
      "page": 1
    },
    {
      "caption": "Figure 1: Notably, substantial",
      "page": 2
    },
    {
      "caption": "Figure 1: Minimizing these differences between do-",
      "page": 2
    },
    {
      "caption": "Figure 2: The primary objective of this network",
      "page": 2
    },
    {
      "caption": "Figure 2: Framework of E2STN. E2STN consists of three modules to obtain stylized emotional EEG representations containing emotion",
      "page": 3
    },
    {
      "caption": "Figure 3: (a). Xs and Xt undergo encoding into query (Q), key",
      "page": 3
    },
    {
      "caption": "Figure 3: Architecture of the encoder and decoder layer in the trans-",
      "page": 3
    },
    {
      "caption": "Figure 3: (b). The source",
      "page": 3
    },
    {
      "caption": "Figure 4: Confusion matrices of E2STN results on cross-dataset ex-",
      "page": 5
    },
    {
      "caption": "Figure 4: (1). Several observations",
      "page": 6
    },
    {
      "caption": "Figure 4: (f)), the recognition accuracy of",
      "page": 6
    },
    {
      "caption": "Figure 4: (a), (b), (d),",
      "page": 6
    },
    {
      "caption": "Figure 4: (c) and (d) are made when SEED-",
      "page": 6
    },
    {
      "caption": "Figure 4: (f), the recognition",
      "page": 6
    },
    {
      "caption": "Figure 4: (2) reveal that ’hap-",
      "page": 7
    },
    {
      "caption": "Figure 4: (h), aligning with neuroscience",
      "page": 7
    },
    {
      "caption": "Figure 5: The contribution of each brain region is evident in the vi-",
      "page": 7
    },
    {
      "caption": "Figure 5: Visualization of the dynamic graph distribution in the dis-",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table 1: showcases the superior performance of the pro- ness’,theaverageaccuracyof’sadness’is40.38%.Thelower",
      "data": [
        {
          "Method": "",
          "ACC / STD (%)": "MPED3 → SEED3"
        },
        {
          "Method": "SVM [Suykens and Vandewalle, 1999]\nBiDANN [Li et al., 2018]\nA-LSTM [Song et al., 2019]\nIAG [Song et al., 2020]\nTANN [Li et al., 2021]\nGECNN [Song et al., 2022]\nPGCN-c [Zhou et al., 2023]\nE2STN",
          "ACC / STD (%)": "48.94/04.96\n61.30/09.14\n47.55/07.46\n60.89*/–\n64.23/09.63\n62.90/06.58\n63.02/09.37\n73.51/07.23"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Fiorini et al., 2020] Laura Fiorini, Gianmaria Mancioppi, Francesco Semeraro, Hamido Fujita, and Filippo Cavallo. Unsupervised emotional state classification through physiological parameters for social robotics applications",
      "authors": [
        "Alarcão",
        "M Fonseca ; Soraia",
        "Manuel Alarcão",
        "Fonseca"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "2",
      "title": "Guo and Gao, 2022] Hongyu Guo and Wurong Gao. Metaverse-powered experiential situational englishteaching design: An emotion-based analysis method",
      "authors": [
        "Gatys"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "3",
      "title": "A novel neural network model based on cerebral hemispheric asymmetry for eeg emotion recognition",
      "authors": [
        "Labar Kragel",
        "Philip Kragel",
        "Kevin Labar",
        "Li"
      ],
      "year": "2016",
      "venue": "IJCAI"
    },
    {
      "citation_id": "4",
      "title": "Mped: A multi-modal physiological emotion database for discrete emotion recognition",
      "authors": [
        "; Semenick",
        "Song"
      ],
      "year": "1990",
      "venue": "AAAI-20"
    },
    {
      "citation_id": "5",
      "title": "Suykens and Vandewalle, 1999] Johan AK Suykens and Joos Vandewalle. Least squares support vector machine classifiers",
      "authors": [
        "Song"
      ],
      "year": "1999",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "6",
      "title": "Investigating critical frequency bands and channels for eeg-based emotion recognition with deep neural networks",
      "year": "2015",
      "venue": "IEEE Transactions on autonomous mental development"
    },
    {
      "citation_id": "7",
      "title": "Emotionmeter: A multimodal framework for recognizing human emotions",
      "authors": [
        "Zheng"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Cybernetics"
    },
    {
      "citation_id": "8",
      "title": "Progressive graph convolution network for eeg emotion recognition",
      "authors": [
        "Zhou"
      ],
      "year": "2023",
      "venue": "Neurocomputing"
    }
  ]
}