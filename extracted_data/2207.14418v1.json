{
  "paper_id": "2207.14418v1",
  "title": "Domain Specific Wav2Vec 2.0 Fine-Tuning For The Se&R 2022 Challenge",
  "published": "2022-07-29T00:48:40Z",
  "authors": [
    "Alef Iury Siqueira Ferreira",
    "Gustavo dos Reis Oliveira"
  ],
  "keywords": [
    "speech recognition",
    "portuguese",
    "prepared speech",
    "spontaneous speech",
    "wild data"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This paper presents our efforts to build a robust ASR model for the shared task Automatic Speech Recognition for spontaneous and prepared speech & Speech Emotion Recognition in Portuguese (SE&R 2022). The goal of the challenge is to advance the ASR research for the Portuguese language, considering prepared and spontaneous speech in different dialects. Our method consist on fine-tuning an ASR model in a domain-specific approach, applying gain normalization and selective noise insertion. The proposed method improved over the strong baseline provided on the test set in 3 of the 4 tracks available.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The performance of Automatic Speech Recognition systems (ASRs) has increased significantly with the development of modern neural network topologies and the use of massive amount of data to train the models  [1] . Although the accuracy of recent models improved for high-resource languages, such as English, the development of ASR models in other languages is still a difficult task using the same technologies  [2, 3] . In this scenario, Self-Supervised Learning (SSL), a method in which representations with semantic information are learned by using unlabelled data, emerged as an important advance, allowing the training of deeper models using less labelled data  [4, 5] . In this line of work, this paper explores the use of the Wav2vec 2.0  [6] , a framework for self-supervised learning of discrete representations from raw audio data. Wav2vec 2.0 (Figure  1 ) is inspired by previous works in unsupervised pre-training for speech recognition, that is, Wav2vec  [7]  and Vq-Wav2vec  [4] . During pre-training the model learns speech representations solving a contrastive task which requires identifying the correct quantized latent speech representations of a masked time step among a set of distractors. After the self-supervised pre-training, the model can be fine-tuned on labeled data in a supervised task, like ASR, adding a randomly initialized linear projection on top of the context network with N classes and a loss function specific to the task at hand, like CTC, for instance.\n\nThe model shows important results among low resource languages. In Portuguese, for example,  [8]  and  [9]  demonstrated that the fine-tuning of the Wav2vec 2.0 model achieves state-of-the-art (SOTA) results only using publicly available datasets.\n\nAn important aspect to consider when training an ASR model is the quality and the domain of the data  [10, 11, 12] . While most of the available public datasets are composed of prepared speech  [9] , mostly read sentences  [13, 14] , the domain of real ASRs are far more complex, mainly because it is formed by spontaneous speech and different speech dialects. Quality is another issue: most of ASR use cases involve high noise environments or low recording equipment, which is not adressed in most of the public datasets available  [9] .\n\nTo stimulate research that can advance the present SOTA in ASR in Portuguese, for both prepared and spontaneous speech, the shared-task Automatic Speech Recognition for spontaneous and prepared speech & Speech Emotion In this work, we investigate the fine-tuning of the baseline model  [9]  proposed by the shared task, a fine-tuned model based on the Wav2vec 2.0 XLSR-53  [15] , using only public available Portuguese datasets, including the CORAA ASR dataset. We conducted several experiments in different domains for the challenge and explored the use of selective noise insertion and audio normalization during training. This work is organized as follows: Section 2 discuss the proposed methods and Section 3 shows and discusses the obtained results. Finally, Section 4 presents the conclusions of this work.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Methods",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Datasets",
      "text": "We used several publicly available datasets in Portuguese. Besides CORAA ASR, most of them is composed by prepared speech. In general, we opted to use all the data in the gathered datasets for training, except the dev part of the CORAA ASR, as presented in Table  1 . The datasets used in this work are:\n\n• CETUC  [13] : contains approximately 145 hours of Brazilian Portuguese speech distributed among 50 male and 50 female speakers, each pronouncing approximately 1,000 phonetically balanced sentences selected from the CETEN-Folha 1  corpus;\n\n• Common Voice (CV) 7.0  [16] : is a project proposed by Mozilla Foundation with the goal to create a wide open dataset in different languages. In this project, volunteers donate and validate speech using the official site 2  ;\n\n• Multilingual LibriSpeech (MLS)  [14] : a massive dataset available in many languages. The MLS is based on audiobook recordings in public domain like LibriVox 3  . The dataset contains a total of 6k hours of transcribed data in many languages. The set in Portuguese used in this work  4  (mostly Brazilian variant) has approximately 284 hours of speech, obtained from 55 audiobooks read by 62 speakers;\n\n• Multilingual TEDx  [17] : a collection of audio recordings from TEDx talks in 8 source languages. The Portuguese set (mostly Brazilian Portuguese variant) contains 164 hours of transcribed speech.\n\n• Corpus of Annotated Audios (CORAA ASR) v1  [9] : is a public available dataset that contains 290.77 hours of validated pairs (audio-transcription) in Portuguese (mostly Brazilian Portuguese variant) and is comprised by five other corpora: ALIP  [18] , C-ORAL Brasil I  [19] , NURC-Recife  [20] , SP2010  [21]  and TEDx Portuguese talks.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "Our experiments consists on the fine-tuning of the baseline model of  [9] . For each experiment, we trained the model for 5 epochs, using a batch size of 192, and using Adam  [22]  where the learning rate is 3 e-05 that is warmed up for the first 400 updates, then linearly decayed for the remained. For the experiments, we used a NVIDIA TESLA V100 32GB, a NVIDIA TESLA Tesla P100 16GB and a NVIDIA A100 80GB, depending on the type of audio pre-processing used. The code to replicate the results is available at https://github.com/alefiury/SE-R_2022_Challenge_Wav2vec2.\n\nIn total, we conducted five main experiments to test our methods:\n\n• Experiment 1: Wav2vec 2.0 XLSR-53 -Base: For this experiment, the model was fine-tuned considering the whole train set, but did not receive neither normalization nor noise addition;\n\n• Experiment 2: Wav2vec 2.0 XLSR-53 -Norm: The model was fine-tuned with the whole train set with normalization. For the normalization, the mean gain of all the audios in the train set was considered;\n\n• Experiment 3: Wav2vec 2.0 XLSR-53 -Norm and SNA: The model was fine-tuned with gain normalization and selective noise addition. The audios were normalized considering the mean gain of all the audios in the train set, and those audios pertaining to datasets that were considered to have a low presence of noise, namely MLS and CETUC, received randomly one of the following 5 possible types of noises: additive noise, being music or nonspeech noises from the MUSAN Corpus  [23] , Room impulse responses  [24] , Addition or reduction of gain, Pitch shift and Gaussian noise;\n\n• Experiment 4: Wav2vec 2.0 XLSR-53 -Norm + Prepared Speech: Model fine-tuned based on the final trained model of Experiment 2, but considering just the prepared speech data from the CORAA ASR dataset, trained for more 5 epochs;\n\n• Experiment 5: Wav2vec 2.0 XLSR-53 -Norm + Spontaneous Speech: Model fine-tuned based on the final trained model of Experiment 2, but considering just the spontaneous speech data from the CORAA ASR dataset, trained for more 5 epochs.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results And Discussion",
      "text": "The shared-task consists of 4 tracks. Each track has a domain specific scenario, that includes prepared speech and spontaneous speech. In this regard, we conducted a prior analysis (Section 3.1) using the dev set to select the best approaches based on 3 of the 4 tracks available: Mixed, Prepared Speech PT_BR and Spontaneous Speech. The best models were selected and then submitted for evaluation. Our final results are presented in Section 3.2.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dev Set Analysis",
      "text": "Overall, our models did not show a huge improvement in performance when compared to the baseline model. Even though we fine-tuned a model that is considered the state-of-the-art in Brazilian Portuguese, we suspect that the number of training epochs might have been insufficient to obtain an increase in performance, or that the baseline model might have already reached a local optima.\n\nFurthermore, as presented in Table  2 , the model that was fine-tuned using prepared speech clearly improved the results in the Prepared Speech subset (and consequently the Mixed subset). The same phenomenon could not be seen in the Spontaneous Speech subset. A possible explanation to this fact is that most of the data of the datasets that were added to the train set are comprised of prepared speech, which might have contributed to the increase in performance in this particular domain. Another possible explanation is the low number of training epochs used to train the models.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Final Results",
      "text": "Table  4  compares the baseline with our selected models in the test set. The model Wav2vec 2.0 XLSR-53 -Norm + Prepared Speech surpassed the strong baseline in the Prepared Speech PT_BR, Prepared Speech PT_PT and the Mixed tracks. As seen in the results based on the dev set, the fact that most of the data of the datasets that were added to the train set are comprised of prepared speech, might have contributed to the increase in performance in this domain in both Portuguese variants. Lastly, even though we were not able to surpass the baseline model in the Spontaneous Speech track, we achieved competitive results with both submitted models.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Additional Experiments",
      "text": "After selecting and submitting the best results, we performed some additional experiments to further explore our proposed methods using the dev set. We tried to use text correction in the outputs of the ASR models, and train the normalized models for a longer period of time using early stopping considering the prepared speech and spontaneous speech data from the CORAA ASR dataset.\n\nThe text correction was done with an additional post-processing step, using a KenLM  [25]  model. For the different tasks, we used 2 different KenLM models: one for spontaneous speech, which was built using subsets of the CORAA ASR dataset containing spontaneous speech phrases. And the other one was built considering wikipedia in portuguese texts, as proposed by  [3] . Both were 4-grams. We found that this post-processing of the predictions from the ASR models did not improve the results from the dev set on the Prepared Speech PT_BR and Spontaneous Speech tracks, as can be seen in Table  5 , in fact they were worse. One possible explanation is that some of the decoder hyper-parameters did not work well with our ASR models. Another possibility is that the 4-gram trained with spontaneous text was built with a small amount of text, which might have decreased the performance of the model. However, the results on the Prepared Speech PT_PT track were much better compared to the previous experiments. This result suggests that the LM might improve results when there are few domain data used to train the Wav2vec model, since most of our training data was composed by Brazilian Portuguese audios.\n\nFurthermore, as we had suspected earlier, the model with gain normalization that were trained considering the spontaneous speech data from the CORAA ASR corpus and for a longer period of time performed better in its respective subtrack, strengthening our hypothesis that our results did not improve in the main experiments due to a low number of training epochs.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Conclusions",
      "text": "In this work we presented our efforts to build a robust ASR model using multiple approaches, such as selective noise insertion and domain specific fine-tuning. In our experiments we found that fine-tuning a strong baseline with additional public available data in multiple domains and using normalization, even for a few epochs, can improve performance.\n\nWith our results we were able to improve on the test set in 3 of the 4 tracks available over the strong baseline provided.\n\nAs future works, we plan to train a ASR model using a dynamic noise insertion approach that do not depend on choosing specific datasets previously.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ) is inspired by previous works in unsupervised pre-training for speech recognition, that is,",
      "page": 1
    },
    {
      "caption": "Figure 1: Illustration of the Wav2vec 2.0 framework [6].",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ABSTRACT": "This paper presents our efforts to build a robust ASR model for the shared task Automatic Speech"
        },
        {
          "ABSTRACT": "Recognition for spontaneous and prepared speech & Speech Emotion Recognition in Portuguese"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "considering prepared and spontaneous speech in different dialects. Our method consist on ﬁne-tuning"
        },
        {
          "ABSTRACT": "in a domain-speciﬁc approach, applying gain normalization and selective noise"
        },
        {
          "ABSTRACT": "insertion. The proposed method improved over the strong baseline provided on the test set in 3 of the"
        },
        {
          "ABSTRACT": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Keywords\nspeech recognition · portuguese · prepared speech · spontaneous speech · wild data": "1\nIntroduction"
        },
        {
          "Keywords\nspeech recognition · portuguese · prepared speech · spontaneous speech · wild data": "The performance of Automatic Speech Recognition systems (ASRs) has increased signiﬁcantly with the development"
        },
        {
          "Keywords\nspeech recognition · portuguese · prepared speech · spontaneous speech · wild data": "of modern neural network topologies and the use of massive amount of data to train the models [1]. Although the"
        },
        {
          "Keywords\nspeech recognition · portuguese · prepared speech · spontaneous speech · wild data": "accuracy of recent models improved for high-resource languages, such as English, the development of ASR models in"
        },
        {
          "Keywords\nspeech recognition · portuguese · prepared speech · spontaneous speech · wild data": "other languages is still a difﬁcult task using the same technologies [2, 3]. In this scenario, Self-Supervised Learning"
        },
        {
          "Keywords\nspeech recognition · portuguese · prepared speech · spontaneous speech · wild data": "(SSL), a method in which representations with semantic information are learned by using unlabelled data, emerged as"
        },
        {
          "Keywords\nspeech recognition · portuguese · prepared speech · spontaneous speech · wild data": "an important advance, allowing the training of deeper models using less labelled data [4, 5]. In this line of work, this"
        },
        {
          "Keywords\nspeech recognition · portuguese · prepared speech · spontaneous speech · wild data": "paper explores the use of the Wav2vec 2.0 [6], a framework for self-supervised learning of discrete representations"
        },
        {
          "Keywords\nspeech recognition · portuguese · prepared speech · spontaneous speech · wild data": "from raw audio data."
        },
        {
          "Keywords\nspeech recognition · portuguese · prepared speech · spontaneous speech · wild data": "Wav2vec 2.0 (Figure 1) is inspired by previous works in unsupervised pre-training for speech recognition,\nthat\nis,"
        },
        {
          "Keywords\nspeech recognition · portuguese · prepared speech · spontaneous speech · wild data": "Wav2vec [7] and Vq-Wav2vec [4]. During pre-training the model learns speech representations solving a contrastive"
        },
        {
          "Keywords\nspeech recognition · portuguese · prepared speech · spontaneous speech · wild data": "task which requires identifying the correct quantized latent speech representations of a masked time step among a set of"
        },
        {
          "Keywords\nspeech recognition · portuguese · prepared speech · spontaneous speech · wild data": "distractors. After the self-supervised pre-training, the model can be ﬁne-tuned on labeled data in a supervised task, like"
        },
        {
          "Keywords\nspeech recognition · portuguese · prepared speech · spontaneous speech · wild data": "ASR, adding a randomly initialized linear projection on top of the context network with N classes and a loss function"
        },
        {
          "Keywords\nspeech recognition · portuguese · prepared speech · spontaneous speech · wild data": "speciﬁc to the task at hand, like CTC, for instance."
        },
        {
          "Keywords\nspeech recognition · portuguese · prepared speech · spontaneous speech · wild data": "The model shows important results among low resource languages. In Portuguese, for example, [8] and [9] demonstrated"
        },
        {
          "Keywords\nspeech recognition · portuguese · prepared speech · spontaneous speech · wild data": "that the ﬁne-tuning of the Wav2vec 2.0 model achieves state-of-the-art (SOTA) results only using publicly available"
        },
        {
          "Keywords\nspeech recognition · portuguese · prepared speech · spontaneous speech · wild data": "datasets."
        },
        {
          "Keywords\nspeech recognition · portuguese · prepared speech · spontaneous speech · wild data": "An important aspect to consider when training an ASR model is the quality and the domain of the data [10, 11, 12]."
        },
        {
          "Keywords\nspeech recognition · portuguese · prepared speech · spontaneous speech · wild data": "While most of the available public datasets are composed of prepared speech [9], mostly read sentences [13, 14], the"
        },
        {
          "Keywords\nspeech recognition · portuguese · prepared speech · spontaneous speech · wild data": "domain of real ASRs are far more complex, mainly because it is formed by spontaneous speech and different speech"
        },
        {
          "Keywords\nspeech recognition · portuguese · prepared speech · spontaneous speech · wild data": "dialects. Quality is another issue: most of ASR use cases involve high noise environments or low recording equipment,"
        },
        {
          "Keywords\nspeech recognition · portuguese · prepared speech · spontaneous speech · wild data": "which is not adressed in most of the public datasets available [9]."
        },
        {
          "Keywords\nspeech recognition · portuguese · prepared speech · spontaneous speech · wild data": "To stimulate research that can advance the present SOTA in ASR in Portuguese, for both prepared and spontaneous"
        },
        {
          "Keywords\nspeech recognition · portuguese · prepared speech · spontaneous speech · wild data": "speech,\nthe shared-task Automatic Speech Recognition for spontaneous and prepared speech & Speech Emotion"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 1: Illustration of the Wav2vec 2.0 framework [6].": "Recognition in Portuguese (SE&R 2022) introduces a new baseline for ASR and a new dataset in Portuguese [9]. The"
        },
        {
          "Figure 1: Illustration of the Wav2vec 2.0 framework [6].": "Corpus of Annotated Audios (CORAA ASR), a large corpus of spontaneous and prepared speech,"
        },
        {
          "Figure 1: Illustration of the Wav2vec 2.0 framework [6].": ""
        },
        {
          "Figure 1: Illustration of the Wav2vec 2.0 framework [6].": "on CORAA ASR test set, a difﬁcult dataset containing samples with low quality, noise, and a variety of domains and"
        },
        {
          "Figure 1: Illustration of the Wav2vec 2.0 framework [6].": "dialects."
        },
        {
          "Figure 1: Illustration of the Wav2vec 2.0 framework [6].": "In this work, we investigate the ﬁne-tuning of the baseline model [9] proposed by the shared task, a ﬁne-tuned model"
        },
        {
          "Figure 1: Illustration of the Wav2vec 2.0 framework [6].": "based on the Wav2vec 2.0 XLSR-53 [15], using only public available Portuguese datasets, including the CORAA ASR"
        },
        {
          "Figure 1: Illustration of the Wav2vec 2.0 framework [6].": "dataset. We conducted several experiments in different domains for the challenge and explored the use of selective noise"
        },
        {
          "Figure 1: Illustration of the Wav2vec 2.0 framework [6].": "insertion and audio normalization during training. This work is organized as follows: Section 2 discuss the proposed"
        },
        {
          "Figure 1: Illustration of the Wav2vec 2.0 framework [6].": "methods and Section 3 shows and discusses the obtained results. Finally, Section 4 presents the conclusions of this"
        },
        {
          "Figure 1: Illustration of the Wav2vec 2.0 framework [6].": "work."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "We used several publicly available datasets in Portuguese. Besides CORAA ASR, most of them is composed by": "prepared speech. In general, we opted to use all the data in the gathered datasets for training, except the dev part of the"
        },
        {
          "We used several publicly available datasets in Portuguese. Besides CORAA ASR, most of them is composed by": "CORAA ASR, as presented in Table 1. The datasets used in this work are:"
        },
        {
          "We used several publicly available datasets in Portuguese. Besides CORAA ASR, most of them is composed by": "• CETUC [13]: contains approximately 145 hours of Brazilian Portuguese speech distributed among 50 male"
        },
        {
          "We used several publicly available datasets in Portuguese. Besides CORAA ASR, most of them is composed by": "and 50 female speakers, each pronouncing approximately 1,000 phonetically balanced sentences selected from"
        },
        {
          "We used several publicly available datasets in Portuguese. Besides CORAA ASR, most of them is composed by": "the CETEN-Folha1 corpus;"
        },
        {
          "We used several publicly available datasets in Portuguese. Besides CORAA ASR, most of them is composed by": "• Common Voice (CV) 7.0 [16]:\nis a project proposed by Mozilla Foundation with the goal to create a wide open"
        },
        {
          "We used several publicly available datasets in Portuguese. Besides CORAA ASR, most of them is composed by": "dataset in different languages. In this project, volunteers donate and validate speech using the ofﬁcial site2;"
        },
        {
          "We used several publicly available datasets in Portuguese. Besides CORAA ASR, most of them is composed by": "• Multilingual LibriSpeech (MLS) [14]: a massive dataset available in many languages. The MLS is based on"
        },
        {
          "We used several publicly available datasets in Portuguese. Besides CORAA ASR, most of them is composed by": "audiobook recordings in public domain like LibriVox3. The dataset contains a total of 6k hours of transcribed"
        },
        {
          "We used several publicly available datasets in Portuguese. Besides CORAA ASR, most of them is composed by": "data in many languages. The set in Portuguese used in this work4 (mostly Brazilian variant) has approximately"
        },
        {
          "We used several publicly available datasets in Portuguese. Besides CORAA ASR, most of them is composed by": "284 hours of speech, obtained from 55 audiobooks read by 62 speakers;"
        },
        {
          "We used several publicly available datasets in Portuguese. Besides CORAA ASR, most of them is composed by": "• Multilingual TEDx [17]:\na collection of audio recordings from TEDx talks in 8 source languages. The"
        },
        {
          "We used several publicly available datasets in Portuguese. Besides CORAA ASR, most of them is composed by": "Portuguese set (mostly Brazilian Portuguese variant) contains 164 hours of transcribed speech."
        },
        {
          "We used several publicly available datasets in Portuguese. Besides CORAA ASR, most of them is composed by": "• Corpus of Annotated Audios (CORAA ASR) v1 [9]:\nis a public available dataset that contains 290.77 hours of"
        },
        {
          "We used several publicly available datasets in Portuguese. Besides CORAA ASR, most of them is composed by": "validated pairs (audio-transcription) in Portuguese (mostly Brazilian Portuguese variant) and is comprised by"
        },
        {
          "We used several publicly available datasets in Portuguese. Besides CORAA ASR, most of them is composed by": "ﬁve other corpora: ALIP [18], C-ORAL Brasil I [19], NURC-Recife [20], SP2010 [21] and TEDx Portuguese"
        },
        {
          "We used several publicly available datasets in Portuguese. Besides CORAA ASR, most of them is composed by": "talks."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset": "Others",
          "Subset": "CETUC",
          "Type": "Prepared Speech",
          "Train": "144.65h",
          "Dev (validation)": "–",
          "Test": "–"
        },
        {
          "Dataset": "",
          "Subset": "Common Voice",
          "Type": "Prepared Speech",
          "Train": "112.08h",
          "Dev (validation)": "–",
          "Test": "–"
        },
        {
          "Dataset": "",
          "Subset": "MLS (Portuguese)",
          "Type": "Prepared Speech",
          "Train": "168.34h",
          "Dev (validation)": "–",
          "Test": "–"
        },
        {
          "Dataset": "",
          "Subset": "Multilingual TEDx (Portuguese)",
          "Type": "Prepared Speech",
          "Train": "152.17h",
          "Dev (validation)": "–",
          "Test": "–"
        },
        {
          "Dataset": "CORAA ASR[9]",
          "Subset": "ALIP",
          "Type": "Spontaneous Speech",
          "Train": "33.40h",
          "Dev (validation)": "0.99h",
          "Test": "1.57h"
        },
        {
          "Dataset": "",
          "Subset": "C-ORAL Brasil I",
          "Type": "Spontaneous Speech",
          "Train": "6.54h",
          "Dev (validation)": "1.13h",
          "Test": "1.97h"
        },
        {
          "Dataset": "",
          "Subset": "NURC-Recife",
          "Type": "Spontaneous Speech",
          "Train": "137.08h",
          "Dev (validation)": "1.29h",
          "Test": "2.94h"
        },
        {
          "Dataset": "",
          "Subset": "SP2010",
          "Type": "Spontaneous Speech",
          "Train": "27.83h",
          "Dev (validation)": "1.13h",
          "Test": "2.18h"
        },
        {
          "Dataset": "",
          "Subset": "TEDx Portuguese",
          "Type": "Prepared Speech",
          "Train": "68.67h",
          "Dev (validation)": "1.37h",
          "Test": "2.70h"
        },
        {
          "Dataset": "Total",
          "Subset": "",
          "Type": "",
          "Train": "850.76h",
          "Dev (validation)": "5.91h",
          "Test": "11.36h"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2.2": "",
          "Experiments": "Our experiments consists on the ﬁne-tuning of the baseline model of [9]. For each experiment, we trained the model for"
        },
        {
          "2.2": "",
          "Experiments": "5 epochs, using a batch size of 192, and using Adam [22] where the learning rate is 3e−05 that is warmed up for the ﬁrst"
        },
        {
          "2.2": "400 updates, then linearly decayed for the remained. For the experiments, we used a NVIDIA TESLA V100 32GB, a",
          "Experiments": ""
        },
        {
          "2.2": "",
          "Experiments": "NVIDIA TESLA Tesla P100 16GB and a NVIDIA A100 80GB, depending on the type of audio pre-processing used. The"
        },
        {
          "2.2": "",
          "Experiments": "code to replicate the results is available at https://github.com/alefiury/SE-R_2022_Challenge_Wav2vec2."
        },
        {
          "2.2": "In total, we conducted ﬁve main experiments to test our methods:",
          "Experiments": ""
        },
        {
          "2.2": "",
          "Experiments": "• Experiment 1: Wav2vec 2.0 XLSR-53 - Base: For this experiment, the model was ﬁne-tuned considering the"
        },
        {
          "2.2": "",
          "Experiments": "whole train set, but did not receive neither normalization nor noise addition;"
        },
        {
          "2.2": "",
          "Experiments": "• Experiment 2: Wav2vec 2.0 XLSR-53 - Norm: The model was ﬁne-tuned with the whole train set with"
        },
        {
          "2.2": "",
          "Experiments": "normalization. For the normalization, the mean gain of all the audios in the train set was considered;"
        },
        {
          "2.2": "",
          "Experiments": "• Experiment 3: Wav2vec 2.0 XLSR-53 - Norm and SNA: The model was ﬁne-tuned with gain normalization"
        },
        {
          "2.2": "",
          "Experiments": "and selective noise addition. The audios were normalized considering the mean gain of all\nthe audios in"
        },
        {
          "2.2": "",
          "Experiments": "the train set, and those audios pertaining to datasets that were considered to have a low presence of noise,"
        },
        {
          "2.2": "",
          "Experiments": "namely MLS and CETUC, received randomly one of the following 5 possible types of noises: additive noise,"
        },
        {
          "2.2": "",
          "Experiments": "being music or nonspeech noises from the MUSAN Corpus [23], Room impulse responses [24], Addition or"
        },
        {
          "2.2": "",
          "Experiments": "reduction of gain, Pitch shift and Gaussian noise;"
        },
        {
          "2.2": "",
          "Experiments": "• Experiment 4: Wav2vec 2.0 XLSR-53 - Norm + Prepared Speech: Model ﬁne-tuned based on the ﬁnal trained"
        },
        {
          "2.2": "",
          "Experiments": "model of Experiment 2, but considering just the prepared speech data from the CORAA ASR dataset, trained"
        },
        {
          "2.2": "",
          "Experiments": "for more 5 epochs;"
        },
        {
          "2.2": "",
          "Experiments": "• Experiment 5: Wav2vec 2.0 XLSR-53 - Norm + Spontaneous Speech: Model ﬁne-tuned based on the ﬁnal"
        },
        {
          "2.2": "",
          "Experiments": "trained model of Experiment 2, but considering just\nthe spontaneous speech data from the CORAA ASR"
        },
        {
          "2.2": "",
          "Experiments": "dataset, trained for more 5 epochs."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "particular domain. Another possible explanation is the low number of training epochs used to train the models.": ""
        },
        {
          "particular domain. Another possible explanation is the low number of training epochs used to train the models.": "Model"
        },
        {
          "particular domain. Another possible explanation is the low number of training epochs used to train the models.": "Wav2vec 2.0 XLSR-53 Baseline"
        },
        {
          "particular domain. Another possible explanation is the low number of training epochs used to train the models.": "Wav2vec 2.0 XLSR-53 - Base"
        },
        {
          "particular domain. Another possible explanation is the low number of training epochs used to train the models.": "Wav2vec 2.0 XLSR-53 - Norm"
        },
        {
          "particular domain. Another possible explanation is the low number of training epochs used to train the models.": "Wav2vec 2.0 XLSR-53 - Norm and SNA"
        },
        {
          "particular domain. Another possible explanation is the low number of training epochs used to train the models.": "Wav2vec 2.0 XLSR-53 - Norm + Prepared Speech"
        },
        {
          "particular domain. Another possible explanation is the low number of training epochs used to train the models.": "Wav2vec 2.0 XLSR-53 - Norm + Spontaneous Speech"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "interesting and promising results when compared to the baseline. These results are shown in Table 3.": ""
        },
        {
          "interesting and promising results when compared to the baseline. These results are shown in Table 3.": "Model"
        },
        {
          "interesting and promising results when compared to the baseline. These results are shown in Table 3.": "Wav2vec 2.0 XLSR-53 Baseline"
        },
        {
          "interesting and promising results when compared to the baseline. These results are shown in Table 3.": "Wav2vec 2.0 XLSR-53 - Base"
        },
        {
          "interesting and promising results when compared to the baseline. These results are shown in Table 3.": "Wav2vec 2.0 XLSR-53 - Norm"
        },
        {
          "interesting and promising results when compared to the baseline. These results are shown in Table 3.": "Wav2vec 2.0 XLSR-53 - Norm and SNA"
        },
        {
          "interesting and promising results when compared to the baseline. These results are shown in Table 3.": "Wav2vec 2.0 XLSR-53 - Norm + Prepared Speech"
        },
        {
          "interesting and promising results when compared to the baseline. These results are shown in Table 3.": "Wav2vec 2.0 XLSR-53 - Norm + Spontaneous Speech"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ASR dataset containing spontaneous speech phrases. And the other one was built considering wikipedia in portuguese": ""
        },
        {
          "ASR dataset containing spontaneous speech phrases. And the other one was built considering wikipedia in portuguese": ""
        },
        {
          "ASR dataset containing spontaneous speech phrases. And the other one was built considering wikipedia in portuguese": ""
        },
        {
          "ASR dataset containing spontaneous speech phrases. And the other one was built considering wikipedia in portuguese": ""
        },
        {
          "ASR dataset containing spontaneous speech phrases. And the other one was built considering wikipedia in portuguese": "with a small amount of text, which might have decreased the performance of the model. However, the results on the"
        },
        {
          "ASR dataset containing spontaneous speech phrases. And the other one was built considering wikipedia in portuguese": ""
        },
        {
          "ASR dataset containing spontaneous speech phrases. And the other one was built considering wikipedia in portuguese": ""
        },
        {
          "ASR dataset containing spontaneous speech phrases. And the other one was built considering wikipedia in portuguese": "data was composed by Brazilian Portuguese audios."
        },
        {
          "ASR dataset containing spontaneous speech phrases. And the other one was built considering wikipedia in portuguese": "Furthermore, as we had suspected earlier,"
        },
        {
          "ASR dataset containing spontaneous speech phrases. And the other one was built considering wikipedia in portuguese": ""
        },
        {
          "ASR dataset containing spontaneous speech phrases. And the other one was built considering wikipedia in portuguese": ""
        },
        {
          "ASR dataset containing spontaneous speech phrases. And the other one was built considering wikipedia in portuguese": "training epochs."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "training epochs.": ""
        },
        {
          "training epochs.": "Model"
        },
        {
          "training epochs.": "Norm + Prepared Speech + Prepared KenLM"
        },
        {
          "training epochs.": "Norm + Prepared Speech + Spontaneous KenLM"
        },
        {
          "training epochs.": "Norm + Spontaneous Speech + Prepared KenLM"
        },
        {
          "training epochs.": "Norm + Spontaneous Speech + Spontaneous KenLM"
        },
        {
          "training epochs.": "Norm + Prepared Speech + Early Stopping"
        },
        {
          "training epochs.": "Norm + Spontaneous Speech + Early Stopping"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "References": "[1]"
        },
        {
          "References": ""
        },
        {
          "References": ""
        },
        {
          "References": ""
        },
        {
          "References": "[3]"
        },
        {
          "References": ""
        },
        {
          "References": ""
        },
        {
          "References": ""
        },
        {
          "References": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[5] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. A": "survey on contrastive self-supervised learning. Technologies, 9(1):2, 2021."
        },
        {
          "[5] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. A": "[6] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: A framework for"
        },
        {
          "[5] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. A": "self-supervised learning of speech representations.\nIn H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and"
        },
        {
          "[5] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. A": "H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 12449–12460. Curran"
        },
        {
          "[5] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. A": "Associates, Inc., 2020."
        },
        {
          "[5] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. A": "[7] Steffen Schneider, Alexei Baevski, Ronan Collobert, and Michael Auli. wav2vec: Unsupervised pre-training for"
        },
        {
          "[5] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. A": "speech recognition.\nIn INTERSPEECH, 2019."
        },
        {
          "[5] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. A": "[8] Lucas Rafael Stefanel Gris, Edresson Casanova, Frederico Santos de Oliveira, Anderson da Silva Soares, and"
        },
        {
          "[5] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. A": "Arnaldo Candido Junior. Brazilian portuguese speech recognition using wav2vec 2.0, 2021."
        },
        {
          "[5] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. A": "[9] Arnaldo Candido Junior, Edresson Casanova, Anderson Soares, Frederico Santos de Oliveira, Lucas Oliveira,"
        },
        {
          "[5] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. A": "Ricardo Corso Fernandes Junior, Daniel Peixoto Pinto da Silva, Fernando Gorgulho Fayet, Bruno Baldissera"
        },
        {
          "[5] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. A": "Carlotto, Lucas Rafael Stefanel Gris, and Sandra Maria Aluísio. Coraa: a large corpus of spontaneous and prepared"
        },
        {
          "[5] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. A": "speech manually validated for speech recognition in brazilian portuguese, 2021."
        },
        {
          "[5] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. A": "[10] Michael L Seltzer, Dong Yu, and Yongqiang Wang. An investigation of deep neural networks for noise robust"
        },
        {
          "[5] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. A": "speech recognition.\nIn 2013 IEEE international conference on acoustics, speech and signal processing, pages"
        },
        {
          "[5] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. A": "7398–7402. IEEE, 2013."
        },
        {
          "[5] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. A": "Jacob Kahn, Gilad Avidov, Ronan\n[11] Tatiana Likhomanenko, Qiantong Xu, Vineel Pratap, Paden Tomasello,"
        },
        {
          "[5] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. A": "Collobert, and Gabriel Synnaeve. Rethinking evaluation in asr: Are our models robust enough? arXiv preprint"
        },
        {
          "[5] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. A": "arXiv:2010.11745, 2020."
        },
        {
          "[5] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. A": "[12] Wei-Ning Hsu, Anuroop Sriram, Alexei Baevski, Tatiana Likhomanenko, Qiantong Xu, Vineel Pratap, Jacob"
        },
        {
          "[5] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. A": "Kahn, Ann Lee, Ronan Collobert, Gabriel Synnaeve, et al. Robust wav2vec 2.0: Analyzing domain shift\nin"
        },
        {
          "[5] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. A": "self-supervised pre-training. arXiv preprint arXiv:2104.01027, 2021."
        },
        {
          "[5] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. A": "[13] VFS Alencar and Abraham Alcaim. Lsf and lpc-derived features for large vocabulary distributed continuous"
        },
        {
          "[5] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. A": "speech recognition in brazilian portuguese.\nIn 2008 42nd Asilomar conference on signals, systems and computers,"
        },
        {
          "[5] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. A": "pages 1237–1241. IEEE, 2008."
        },
        {
          "[5] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. A": "[14] Vineel Pratap, Qiantong Xu, Anuroop Sriram, Gabriel Synnaeve, and Ronan Collobert. Mls: A large-scale"
        },
        {
          "[5] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. A": "multilingual dataset for speech research.\nInterspeech 2020, Oct 2020."
        },
        {
          "[5] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. A": "[15] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán,"
        },
        {
          "[5] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. A": "Édouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual representation"
        },
        {
          "[5] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. A": "learning at scale.\nIn Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,"
        },
        {
          "[5] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. A": "pages 8440–8451, 2020."
        },
        {
          "[5] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. A": "[16] Rosana Ardila, Megan Branson, Kelly Davis, Michael Kohler, Josh Meyer, Michael Henretty, Reuben Morais,"
        },
        {
          "[5] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. A": "Lindsay Saunders, Francis Tyers, and Gregor Weber. Common voice: A massively-multilingual speech corpus.\nIn"
        },
        {
          "[5] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. A": "Proceedings of the 12th Language Resources and Evaluation Conference, pages 4218–4222, 2020."
        },
        {
          "[5] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. A": "[17] Elizabeth Salesky, Matthew Wiesner, Jacob Bremerman, Roldano Cattoni, Matteo Negri, Marco Turchi, Douglas W"
        },
        {
          "[5] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. A": "arXiv preprint\nOard, and Matt Post.\nThe multilingual\ntedx corpus for speech recognition and translation."
        },
        {
          "[5] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. A": "arXiv:2102.01757, 2021."
        },
        {
          "[5] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. A": "[18] Sebastião Carlos Leite Gonçalves. Projeto alip (amostra linguística do interior paulista) e banco de dados iboruna:"
        },
        {
          "[5] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. A": "10 anos de contribuição com a descrição do português brasileiro.\nEstudos Linguísticos (São Paulo. 1978),"
        },
        {
          "[5] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. A": "48(1):276–297, abr. 2019."
        },
        {
          "[5] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. A": "[19] Tommaso Raso and Heliana Mello. C-oral - brasil i: Corpus de referência do portugues brasileiro falado informal."
        },
        {
          "[5] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. A": "pages 362–367, 04 2012."
        },
        {
          "[5] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. A": "[20] Miguel Oliveira, Jr. Nurc digital um protocolo para a digitalização, anotação, arquivamento e disseminação"
        },
        {
          "[5] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. A": "do material do projeto da norma urbana linguística culta (nurc). CHIMERA: Romance Corpora and Linguistic"
        },
        {
          "[5] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. A": "Studies, 3:149–174, 01 2016."
        },
        {
          "[5] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. A": "[21] Mendes R B and Oushiro L. Mapping paulistano portuguese:\nthe sp2010 project. pages 459–463. Firenze, Italy:"
        },
        {
          "[5] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. A": "Fizenze University Press, 01 2012."
        },
        {
          "[5] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. A": "[22] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization.\nIn Yoshua Bengio and Yann"
        },
        {
          "[5] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. A": "LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA,"
        },
        {
          "[5] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. A": "May 7-9, 2015, Conference Track Proceedings, 2015."
        },
        {
          "[5] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. A": "[23] David Snyder, Guoguo Chen, and Daniel Povey. Musan: A music, speech, and noise corpus, 2015."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[24] Tom Ko, Vijayaditya Peddinti, Daniel Povey, Michael L. Seltzer, and Sanjeev Khudanpur. A study on data": "augmentation of reverberant speech for robust speech recognition.\nIn 2017 IEEE International Conference on"
        },
        {
          "[24] Tom Ko, Vijayaditya Peddinti, Daniel Povey, Michael L. Seltzer, and Sanjeev Khudanpur. A study on data": "Acoustics, Speech and Signal Processing (ICASSP), pages 5220–5224, 2017."
        },
        {
          "[24] Tom Ko, Vijayaditya Peddinti, Daniel Povey, Michael L. Seltzer, and Sanjeev Khudanpur. A study on data": "[25] Kenneth Heaﬁeld. KenLM: Faster and smaller language model queries.\nIn Proceedings of the Sixth Workshop on"
        },
        {
          "[24] Tom Ko, Vijayaditya Peddinti, Daniel Povey, Michael L. Seltzer, and Sanjeev Khudanpur. A study on data": "Statistical Machine Translation, pages 187–197, Edinburgh, Scotland, July 2011. Association for Computational"
        },
        {
          "[24] Tom Ko, Vijayaditya Peddinti, Daniel Povey, Michael L. Seltzer, and Sanjeev Khudanpur. A study on data": "Linguistics."
        },
        {
          "[24] Tom Ko, Vijayaditya Peddinti, Daniel Povey, Michael L. Seltzer, and Sanjeev Khudanpur. A study on data": "7"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Recent advances in end-to-end automatic speech recognition",
      "authors": [
        "Jinyu Li"
      ],
      "year": "2021",
      "venue": "Recent advances in end-to-end automatic speech recognition"
    },
    {
      "citation_id": "2",
      "title": "Deep speech 2: End-to-end speech recognition in english and mandarin",
      "authors": [
        "Dario Amodei",
        "Rishita Sundaram Ananthanarayanan",
        "Jingliang Anubhai",
        "Eric Bai",
        "Carl Battenberg",
        "Jared Case",
        "Bryan Casper",
        "Qiang Catanzaro",
        "Guoliang Cheng",
        "Chen"
      ],
      "year": "2016",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "3",
      "title": "An open-source end-to-end asr system for brazilian portuguese using dnns built from newly assembled corpora",
      "authors": [
        "Igor Macedo Quintanilha",
        "Sergio Netto",
        "Luiz Wagner",
        "Pereira Biscainho"
      ],
      "year": "2020",
      "venue": "Journal of Communication and Information Systems"
    },
    {
      "citation_id": "4",
      "title": "vq-wav2vec: Self-supervised learning of discrete speech representations",
      "authors": [
        "Alexei Baevski",
        "Steffen Schneider",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "5",
      "title": "A survey on contrastive self-supervised learning",
      "authors": [
        "Ashish Jaiswal",
        "Ramesh Ashwin",
        "Mohammad Babu",
        "Debapriya Zadeh",
        "Fillia Banerjee",
        "Makedon"
      ],
      "year": "2021",
      "venue": "Technologies"
    },
    {
      "citation_id": "6",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "7",
      "title": "wav2vec: Unsupervised pre-training for speech recognition",
      "authors": [
        "Steffen Schneider",
        "Alexei Baevski",
        "Ronan Collobert",
        "Michael Auli"
      ],
      "year": "2019",
      "venue": "wav2vec: Unsupervised pre-training for speech recognition"
    },
    {
      "citation_id": "8",
      "title": "Brazilian portuguese speech recognition using wav",
      "authors": [
        "Lucas Rafael",
        "Stefanel Gris",
        "Edresson Casanova",
        "Frederico Santos De Oliveira",
        "Anderson Da",
        "Silva Soares",
        "Arnaldo Candido"
      ],
      "year": "2021",
      "venue": "Brazilian portuguese speech recognition using wav"
    },
    {
      "citation_id": "9",
      "title": "Coraa: a large corpus of spontaneous and prepared speech manually validated for speech recognition in brazilian portuguese",
      "authors": [
        "Arnaldo Candido",
        "Edresson Casanova",
        "Anderson Soares",
        "Frederico Santos De Oliveira",
        "Lucas Oliveira",
        "Ricardo Fernandes",
        "Daniel Peixoto Pinto Da Silva",
        "Fernando Fayet",
        "Bruno Baldissera Carlotto",
        "Lucas Rafael Stefanel",
        "Sandra Gris",
        "Aluísio"
      ],
      "year": "2021",
      "venue": "Coraa: a large corpus of spontaneous and prepared speech manually validated for speech recognition in brazilian portuguese"
    },
    {
      "citation_id": "10",
      "title": "An investigation of deep neural networks for noise robust speech recognition",
      "authors": [
        "Dong Michael L Seltzer",
        "Yongqiang Yu",
        "Wang"
      ],
      "year": "2013",
      "venue": "2013 IEEE international conference on acoustics, speech and signal processing"
    },
    {
      "citation_id": "11",
      "title": "Rethinking evaluation in asr: Are our models robust enough? arXiv preprint",
      "authors": [
        "Tatiana Likhomanenko",
        "Qiantong Xu",
        "Vineel Pratap",
        "Paden Tomasello",
        "Jacob Kahn",
        "Gilad Avidov",
        "Ronan Collobert",
        "Gabriel Synnaeve"
      ],
      "year": "2020",
      "venue": "Rethinking evaluation in asr: Are our models robust enough? arXiv preprint",
      "arxiv": "arXiv:2010.11745"
    },
    {
      "citation_id": "12",
      "title": "Robust wav2vec 2.0: Analyzing domain shift in self-supervised pre-training",
      "authors": [
        "Wei-Ning Hsu",
        "Anuroop Sriram",
        "Alexei Baevski",
        "Tatiana Likhomanenko",
        "Qiantong Xu",
        "Vineel Pratap",
        "Jacob Kahn",
        "Ann Lee",
        "Ronan Collobert",
        "Gabriel Synnaeve"
      ],
      "year": "2021",
      "venue": "Robust wav2vec 2.0: Analyzing domain shift in self-supervised pre-training",
      "arxiv": "arXiv:2104.01027"
    },
    {
      "citation_id": "13",
      "title": "Lsf and lpc-derived features for large vocabulary distributed continuous speech recognition in brazilian portuguese",
      "authors": [
        "Abraham Vfs Alencar",
        "Alcaim"
      ],
      "year": "2008",
      "venue": "2008 42nd Asilomar conference on signals, systems and computers"
    },
    {
      "citation_id": "14",
      "title": "Mls: A large-scale multilingual dataset for speech research",
      "authors": [
        "Qiantong Vineel Pratap",
        "Anuroop Xu",
        "Gabriel Sriram",
        "Ronan Synnaeve",
        "Collobert"
      ],
      "year": "2020",
      "venue": "Mls: A large-scale multilingual dataset for speech research"
    },
    {
      "citation_id": "15",
      "title": "Unsupervised cross-lingual representation learning at scale",
      "authors": [
        "Alexis Conneau",
        "Kartikay Khandelwal",
        "Naman Goyal",
        "Vishrav Chaudhary",
        "Guillaume Wenzek",
        "Francisco Guzmán",
        "Édouard Grave",
        "Myle Ott",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "16",
      "title": "Common voice: A massively-multilingual speech corpus",
      "authors": [
        "Rosana Ardila",
        "Megan Branson",
        "Kelly Davis",
        "Michael Kohler",
        "Josh Meyer",
        "Michael Henretty",
        "Reuben Morais",
        "Lindsay Saunders",
        "Francis Tyers",
        "Gregor Weber"
      ],
      "year": "2020",
      "venue": "Proceedings of the 12th Language Resources and Evaluation Conference"
    },
    {
      "citation_id": "17",
      "title": "The multilingual tedx corpus for speech recognition and translation",
      "authors": [
        "Elizabeth Salesky",
        "Matthew Wiesner",
        "Jacob Bremerman",
        "Roldano Cattoni",
        "Matteo Negri",
        "Marco Turchi",
        "Douglas Oard",
        "Matt Post"
      ],
      "year": "2021",
      "venue": "The multilingual tedx corpus for speech recognition and translation",
      "arxiv": "arXiv:2102.01757"
    },
    {
      "citation_id": "18",
      "title": "Projeto alip (amostra linguística do interior paulista) e banco de dados iboruna: 10 anos de contribuição com a descrição do português brasileiro",
      "authors": [
        "Sebastião Carlos",
        "Leite Gonçalves"
      ],
      "year": "1978",
      "venue": "Estudos Linguísticos"
    },
    {
      "citation_id": "19",
      "title": "C-oral -brasil i: Corpus de referência do portugues brasileiro falado informal",
      "authors": [
        "Tommaso Raso",
        "Heliana Mello"
      ],
      "venue": "C-oral -brasil i: Corpus de referência do portugues brasileiro falado informal"
    },
    {
      "citation_id": "20",
      "title": "Nurc digital um protocolo para a digitalização, anotação, arquivamento e disseminação do material do projeto da norma urbana linguística culta (nurc)",
      "authors": [
        "Miguel Oliveira"
      ],
      "venue": "Nurc digital um protocolo para a digitalização, anotação, arquivamento e disseminação do material do projeto da norma urbana linguística culta (nurc)"
    },
    {
      "citation_id": "21",
      "title": "Mapping paulistano portuguese: the sp2010 project",
      "authors": [
        "R Mendes",
        "L Oushiro"
      ],
      "venue": "Mapping paulistano portuguese: the sp2010 project"
    },
    {
      "citation_id": "22",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2015",
      "venue": "3rd International Conference on Learning Representations, ICLR 2015"
    },
    {
      "citation_id": "23",
      "title": "Musan: A music, speech, and noise corpus",
      "authors": [
        "David Snyder",
        "Guoguo Chen",
        "Daniel Povey"
      ],
      "year": "2015",
      "venue": "Musan: A music, speech, and noise corpus"
    },
    {
      "citation_id": "24",
      "title": "A study on data augmentation of reverberant speech for robust speech recognition",
      "authors": [
        "Tom Ko",
        "Vijayaditya Peddinti",
        "Daniel Povey",
        "Michael Seltzer",
        "Sanjeev Khudanpur"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "25",
      "title": "KenLM: Faster and smaller language model queries",
      "authors": [
        "Kenneth Heafield"
      ],
      "year": "2011",
      "venue": "Proceedings of the Sixth Workshop on Statistical Machine Translation"
    }
  ]
}