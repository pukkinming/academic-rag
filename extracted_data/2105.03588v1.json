{
  "paper_id": "2105.03588v1",
  "title": "Facial Emotion Recognition: State Of The Art Performance On Fer2013",
  "published": "2021-05-08T04:20:53Z",
  "authors": [
    "Yousif Khaireddin",
    "Zhuofa Chen"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Facial emotion recognition (FER) is significant for human-computer interaction such as clinical practice and behavioral description. Accurate and robust FER by computer models remains challenging due to the heterogeneity of human faces and variations in images such as different facial pose and lighting. Among all techniques for FER, deep learning models, especially Convolutional Neural Networks (CNNs) have shown great potential due to their powerful automatic feature extraction and computational efficiency. In this work, we achieve the highest singlenetwork classification accuracy on the FER2013 dataset. We adopt the VGGNet architecture, rigorously fine-tune its hyperparameters, and experiment with various optimization methods. To our best knowledge, our model achieves state-of-theart single-network accuracy of 73.28 % on FER2013 without using extra training data.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Facial emotion recognition refers to identifying expressions that convey basic emotions such as fear, happiness, and disgust, etc. It plays an important role in human-computer interactions and can be applied to digital advertisement, online gaming, customer feedback assessment, and healthcare  [1] -  [3] . With advancements in computer vision, high emotion recognition accuracy has been achieved in images captured under controlled conditions and consistent environments, rendering this a solved problem  [4] . Challenges persist in emotion recognition under naturalistic conditions due to high intra-class variation and low inter-class variation, e.g. changes in facial pose and subtle differences between expressions.\n\nDevelopments in computer vision continuously aim to improve classification accuracy on such problems  [5] -  [7] . In image classification, Convolutional Neural Networks (CNNs) have shown great potential due to their computational efficiency and feature extraction capability  [8] . They are the most widely used deep models for FER  [5] -  [7] ,  [9] -  [11] .\n\nOne specific emotion recognition dataset that encompasses the difficult naturalistic conditions and challenges is FER2013. It was introduced at the International Conference on Machine Learning (ICML) in 2013 and became a benchmark in comparing model performance in emotion recognition. Human performance on this dataset is estimated to be 65.5 %  [12] . In comparing different methods and benchmarking our results, we are strictly concerned with previous work trained and evaluated on this dataset.\n\nIn this work, we aim to improve prediction accuracy on FER2013 using CNNs. We adopt the VGG network and construct various experiments to explore different optimization algorithms and learning rate schedulers. We thoroughly tune the model and training hyperparameters to achieve stateof-the-art results at a testing accuracy of 73.28 %. To our best knowledge, this is the highest single-network accuracy achieved on FER2013 without using any extra training data. We then construct several saliency maps to better understand the network's performance and decision-making process.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Since being introduced in the late 1990s, CNNs have shown great potential in image processing  [13] . A typical CNN includes a convolutional layer, a pooling layer, and a fully connected layer. This makes it efficient in static image manipulation. However, at that time, the application of CNNs was limited due to the lack of training data and computing power. After the 2010s, the growth of computing power and the collection of larger datasets made CNNs a much more viable tool in feature extraction and image classification  [8] .\n\nVarious techniques have been proposed to even further improve performance. For instance, the Sigmoid activation function has been replaced by Rectified Linear Unit (ReLU) activation to avoid gradient dispersion problems and speed up training  [14] . Different pooling methods such as average pooling and max pooling are used to downsample the inputs and aid in generalization  [15] ,  [16] . Dropout, regularization, and data augmentation are used to prevent overfitting. Batch normalization has been developed to help prevent gradient vanishing and exploding  [17] ,  [18] .\n\nA great deal of research has also been done in creating different optimization algorithms used in training. Though there is no systematic theoretical guideline on choosing an optimizer, empirical results show that a suitable optimization algorithm can effectively improve a model's performance  [19] . The most commonly used optimizer is Stochastic gradient descent (SGD). It is a simple technique that updates the parameters of a model based on the gradient of a single data point  [19] . Numerous variations of this algorithm have been proposed to speed up training. AdaGrad adaptively scales the learning rate for each dimension in the network  [20] . RMSProp radically diminishes the learning rate  [21] . Adam combines the advantages of AdaGrad and RMSProp by scaling the learning rate and introducing momentum of gradient, etc.  [22] .\n\nAmong many others, one significant factor that could impact performance is the learning rate. A large learning rate could lead to oscillations around the minima or divergence in the loss. A small learning rate would slow down the model's convergence significantly and could trap the model at a nonoptimal local minimum. A commonly used technique is to employ a learning rate scheduler that changes the learning rate during training  [23] . For instance, timebased decay reduces the learning rate either linearly or exponentially as the iteration number increases.\n\nStep decay drops the learning rate by a factor after certain epochs. An adaptive learning rate schedule tries to automatically adjust the learning rate based on the local gradients during training. Cosine annealing resets the learning rate periodically and reuses \"good weights\" during the training process, etc.  [24] ,  [25] .   [34] . However, in order to improve the ensemble performance even further, we aim to first optimize the building blocks of these ensembles, a single network. Other research work has tried to improve their model's performance on FER2013 by including auxiliary training data; however, that is out of the scope of this paper.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Dataset, Preprocessing, And Augmentation",
      "text": "In training on FER2013, we adhere to the official training, validation, and test sets as introduced by the ICML. FER2013 consists of 35888 images of 7 different emotions: anger, neutral, disgust, fear, happiness, sadness, and surprise. A Kaggle forum discussion held by the competition organizers places human accuracy on this dataset in the range of 65 -68 %  [32] .\n\nTo account for the variability in facial expression recognition, we apply a significant amount of data augmentation in training. This augmentation includes rescaling the images up to ± 20 % of its original scale, horizontally and vertically shifting the image by up to ± 20 % of its size, and rotating it up to ± 10 degrees. Each of the techniques is applied randomly and with a probability of 50 %. After this, the image is then tencropped to a size of 40×40, and random portions of each of the crops are erased with a probability of 50 %. Each crop is then normalized by dividing each pixel by 255.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Training And Inference",
      "text": "We run all experiments for 300 epochs optimizing the cross-entropy loss. In the following sections, we vary the optimizer used as well as the learning rate schedulers and maintain other parameters constant. We use a fixed momentum of 0.9 and a weight decay of 0.0001. All experiments are run with gradient scaling to prevent gradient underflow. The models are evaluated using validation accuracy and tested using standard ten-crop averaging.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Vggnet Architecture",
      "text": "VGGNet is a classical convolutional neural network architecture used in large-scale image processing and pattern recognition  [35] . Our variant of VGGNet is shown in Figure  1 . The network consists of 4 convolutional stages and 3 fully connected layers. Each of the convolutional stages contains two convolutional blocks and a max-pooling layer. The convolution block consists of a convolutional layer, a ReLU activation, and a batch normalization layer. Batch normalization is used here to speed up the learning process, reduce the internal covariance shift, and prevent gradient vanishing or explosion  [18] . The first two fully connected layers are followed by a ReLU activation. The third fully connected layer is for classification. The convolutional stages are responsible for feature extraction, dimension reduction, and non-linearity. The fully connected layers are trained to classify the inputs as described by extracted features.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "D. Tuning",
      "text": "Initially, we tune our model architecture to maximize performance. All initial experiments were run using SGD. A grid search is performed to determine the optimal batch size and the best drop-out rate after our fully connected layers. Once the architecture has been optimized, we then explore the effects of different optimizers and learning rate schedulers on our model's performance. We then set up a final experiment to try and fine-tune the trained model's weights and improve its performance.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "D.1 Optimizer",
      "text": "Our first experiment intends to find the best optimizer in training our architecture. For this, we explore 6 different algorithms: SGD, SGD with Nesterov Momentum, Average SGD, Adam, Adam with AMSGrad, Adadelta, and Adagrad. Although many of these algorithms are very closely related, how they perform differently in this optimization will help us understand the importance of their subtle differences.\n\nWe run 2 different variations of this experiment. In the first variation, we run all algorithms with a fixed learning rate of 0.001. This learning rate was determined using a grid search. In the second variation, we set up a simple learning rate scheduler with an initial learning rate of 0.01 and it is reduced by a factor of 0.75 if the validation accuracy plateaus for 5 epochs. The parameters of this scheduler were also determined using a grid search. All other parameters, such as weight decay, momentum, dropout, and batch size are maintained constant after the initial optimization.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "D.2 Lr Schedule",
      "text": "The next experiment we run is to find the optimal learning rate scheduler. In this section, we run the same architecture, using the optimal optimizer decided by the previous section with 5 different schedulers: Reduce Learning Rate on Plateau (RLRP), Cosine Annealing (Cosine), Cosine Annealing with Warm Restarts (CosineWR), One Cycle Learning Rate (OneCycleLR), and Step Learning Rate (StepLR). For a baseline, we also run a model using a constant learning rate that was determined using a grid search. All schedulers have an initial learning rate of 0.01 and their parameters are selected using a grid search. All other parameters are maintained constant.\n\nAlthough these schedulers have their similarities, they perform the learning rate update differently. For example, StepLR and RLRP both have a similar assumption that the longer we train, the smaller our step sizes should be. However, StepLR systematically reduces the learning rate after a set number of epochs while RLRP monitors the model's current performance before making a learning rate update. Similarly, both Cosine Annealing and Cosine Annealing with Warm Restarts have a cosine-based learning rate update function. So, the learning rate gradually oscillates between two values. However, the key difference is that the latter one resets the model's parameters regularly to maintain good model weights before taking update steps. These subtle differences could drastically change the resulting performance.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "D3. Fine Tuning",
      "text": "To further increase our model's accuracy, we then experimented with hyper-tuning the final weights of our model. We reload the parameters and train for a final 50 epochs using an initial learning rate of 0.0001. We set this learning rate to maintain the update steps small; thus, ensuring that our model's weights are not skewed far away, since this is an already trained model. This experiment is run using Cosine Annealing and Cosine Annealing with Warm Restarts since both of these schedulers slowly oscillate the learning rate back and forth thus not allowing for major weight changes. The second schedule is also favorable since its warm restarts would mean that it would regularly reset the model's weights back to some good location during its updates.\n\nWe then ran a second variation of this experiment in which we combined the validation set into training to allow for a larger dataset set when tuning. This larger dataset would allow the model to have more samples to learn from; thus, improving its performance. The test set is left un-altered and all other parameters are kept constant.\n\nBy running two variations of this experiment, we can verify two things. Using the first variation, we can confirm the effectiveness of the tuning. Using the second experiment, we can understand the benefits of added data.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Optimizer",
      "text": "We first study and compare the effect of optimizers on the performance of our model. Figure  2  shows the validation accuracy attained by our model using the different optimizers. The yellow bars show the first variation of this experiment with a constant learning rate and the orange bars show the second variation using a decaying learning rate. Excluding Adadelta, all optimizers show a high validation accuracy, above 70 %. The model using the SGD with Nesterov momentum performs the best in both experiments attaining a validation accuracy of 73.2 % and 73.5 %. We also found that Adam and its AMSGrad variant perform better than Adadelta and Adagrad. This is because Adam optimization combines the advantage of AdaGrad and RMSProp by introducing momentum of the gradient. Finally, on this dataset, all variants of SGD are outperforming all other optimizers.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Lr Schedule",
      "text": "Next, we explore the effects of different learning rate schedulers on our model. Figure  3  shows the validation and testing accuracies attained by our models. All runs here use the best performing optimizer based on the previous section, SGD with Nesterov momentum. The first thing to note is that Reducing Learning Rate on Plateau (RLRP) performs best. It achieves a validation accuracy of 73.59 % and a testing accuracy of 73.06 %. To our best knowledge, this is already surpassing the previous single-network state-of-the-art performance.\n\nFor the next set of comparisons, we will be focused on the validation accuracy, since the testing accuracies we are reporting are strictly for benchmarking publicly. The constant learning rate outperforms some of the other schedulers (OneCycleRL and StepLR). For OneCycleLR, this could be since it is usually intended for fast training with larger learning rates and this may not be applicable on FER2013  [36] . Cosine Annealing and its counterpart Cosine Annealing with Warm Restart perform comparably. Comparing StepLR and RLRP, they both slowly reduce the learning rate to a minimum. RLRP performs better since it monitors the current performance before deciding when to drop the learning rate as opposed to systematically reducing the learning rate.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Fine Tuning",
      "text": "To further increase our final performance, we then run an experiment in which we reload our best performing model thus far and tune it for 50 extra epochs using both cosine annealing schedulers at a small learning rate. We then also combine our training and validation data and perform this same tuning in a separate run. Table  1  shows the final testing accuracies achieved in this experiment benchmarked against the model which they are tuning. Cosine Annealing performs best here and improves the model by 0.05 %. Cosine Annealing with Warm Restarts, on the other hand negatively impacts performance in this tuning stage reducing the accuracy by 0.42 %. As expected both models perform better after training on the combined dataset resulting from the training and validation data. Our final best model achieves an accuracy of 73.28 %. Once again, the testing data is left unaltered at all stages.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "D. Confusion Matrix",
      "text": "Figure  4  shows the final model's confusion matrix on the FER2013 testing set. The model shows the best classification on the \"happiness\" and \"surprise\" emotions. On the other hand, it makes the most mistakes when classifying between \"disgust\" and \"anger\". Next, the low classification accuracy in \"disgust\" and \"fear\" can be attributed to the fact that they have a lower number of samples in the original training set. The misclassification between \"fear\" and \"sadness\" may be due to the inter-class similarities of the dataset",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "E. Performance Comparison",
      "text": "Table  2  summarizes previous reported classification accuracies FER2013. Most reported methods perform better than the estimated human performance (~ 65.5 %). The previous best-reported single-network accuracy is 72.7 %  [34] . In this work, we achieve the state-of-the-art accuracy of 73.28 %.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "F. Saliency Map",
      "text": "The 'black box' nature of deep learning models makes it difficult to understand how the model processes the input image to obtain the final result. Visualizing the information captured inside deep neural networks is important in evaluating the effectiveness of the model and understanding how it computes its prediction. Thus, generating an understandable visualization of our CNN can help describe how it differentiates between and captures different facial emotions. One of the common visualization techniques in deep neural networks is called a saliency map  [37] . By propagating the loss back to the pixel values, a saliency map can highlight the pixels which have the most impact on the loss value. It highlights the visual features the CNN can capture from the input; thus, allowing us to better understand the importance of each feature in the original image on the final classification decision.\n\nWe generate the saliency map using our best performing network to understand how it classifies each emotion in the FER2013 dataset. Figure  5  shows a saliency map for each emotion before and after being superimposed on the image it is generated from. Judging by all the images, our CNN can effectively capture most of the critical regions for each emotion. The model is placing a large importance on almost all facial features of the person in each image. This is most clearly seen in (f) where the saliency map almost perfectly maps the entire face of the woman. Our model is also effectively dropping regions like the background in (a), (d) and (g), the hair in (a), (c), and (g), and the hand in (e), all of which are not very informative when it comes to describing someone's emotion.\n\nThere are some clear mistakes in the saliency maps, this can be seen in (b), (e), and the corner of (g) where the model highlights some of the background pixels. We think that a model that can more effectively identify the facial features in an image and drop all useless information will perform better on this dataset.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "This paper achieves single-network state-of-theart classification accuracy on FER2013 using a VGGNet. We thoroughly tune all hyperparameters towards an optimized model for facial emotion recognition. Different optimizers and learning rate schedulers are explored and the best initial testing classification accuracy achieved is 73.06 %, surpassing all single-network accuracies previously reported. We also carry out extra tuning on our model using Cosine Annealing and combine the training and validation datasets to further improve the classification accuracy to 73.28 %. For future work, we plan to explore different image processing techniques on FER2013 and investigate ensembles of different deep learning architectures to further improve our performance in facial emotion recognition.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The network",
      "page": 3
    },
    {
      "caption": "Figure 1: VGGNet architecture. A face expression image is fed into the model. The four convolutional blocks (Conv) extract high-level features",
      "page": 3
    },
    {
      "caption": "Figure 2: shows the validation accuracy attained by our model",
      "page": 5
    },
    {
      "caption": "Figure 3: shows the",
      "page": 5
    },
    {
      "caption": "Figure 2: VGGNet performance using different optimizers. The",
      "page": 5
    },
    {
      "caption": "Figure 3: VGGNet performance using different LR schedulers. The",
      "page": 5
    },
    {
      "caption": "Figure 4: The confusion matrix of our final VGGNet on the",
      "page": 6
    },
    {
      "caption": "Figure 4: shows the final model’s confusion",
      "page": 6
    },
    {
      "caption": "Figure 5: Original images, Saliency maps, and superimposition for all emotions in FER2013.",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table 1: shows the final testing",
      "page": 6
    },
    {
      "caption": "Table 1: Testing accuracy after extra tuning.",
      "page": 6
    },
    {
      "caption": "Table 2: summarizes",
      "page": 6
    },
    {
      "caption": "Table 2: Fer2013 public testset benchmark.",
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Real Time Face Detection and Facial Expression Recognition: Development and Applications to Human Computer Interaction",
      "authors": [
        "M Bartlett",
        "G Littlewort",
        "I Fasel",
        "J Movellan"
      ],
      "year": "2003",
      "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops",
      "doi": "10.1109/CVPRW.2003.10057"
    },
    {
      "citation_id": "2",
      "title": "Human-computer interaction using emotion recognition from facial expression",
      "authors": [
        "F Abdat",
        "C Maaoui",
        "A Pruski"
      ],
      "year": "2011",
      "venue": "Proceedings -UKSim 5th European Modelling Symposium on Computer Modelling and Simulation",
      "doi": "10.1109/EMS.2011.20"
    },
    {
      "citation_id": "3",
      "title": "Automatic facial expression analysis: A survey",
      "authors": [
        "B Fasel",
        "J Luettin"
      ],
      "year": "2003",
      "venue": "Pattern Recognition",
      "doi": "10.1016/S0031-3203(02)00052-3"
    },
    {
      "citation_id": "4",
      "title": "Automatic analysis of facial affect: A survey of registration, representation, and recognition",
      "authors": [
        "E Sariyanidi",
        "H Gunes",
        "A Cavallaro"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "doi": "10.1109/TPAMI.2014.2366127"
    },
    {
      "citation_id": "5",
      "title": "Facial emotion recognition using convolutional neural networks (FERC)",
      "authors": [
        "N Mehendale"
      ],
      "year": "2020",
      "venue": "SN Appl. Sci",
      "doi": "10.1007/s42452-020-2234-1"
    },
    {
      "citation_id": "6",
      "title": "Facial emotion recognition on a dataset using Convolutional Neural Network",
      "authors": [
        "V Tümen",
        "Ö Söylemez",
        "B Ergen"
      ],
      "year": "2017",
      "venue": "IDAP 2017 -International Artificial Intelligence and Data Processing Symposium",
      "doi": "10.1109/IDAP.2017.8090281"
    },
    {
      "citation_id": "7",
      "title": "Extended deep neural network for facial emotion recognition",
      "authors": [
        "D Jain",
        "P Shamsolmoali",
        "P Sehdev"
      ],
      "year": "2019",
      "venue": "Pattern Recognit. Lett",
      "doi": "10.1016/j.patrec.2019.01.008"
    },
    {
      "citation_id": "8",
      "title": "ImageNet classification with deep convolutional neural networks",
      "authors": [
        "A Krizhevsky",
        "I Sutskever",
        "G Hinton"
      ],
      "year": "2017",
      "venue": "Commun. ACM",
      "doi": "10.1145/3065386"
    },
    {
      "citation_id": "9",
      "title": "Automating facial emotion recognition",
      "authors": [
        "O Gervasi",
        "V Franzoni",
        "M Riganelli",
        "S Tasso"
      ],
      "year": "2019",
      "venue": "Web Intell",
      "doi": "10.3233/WEB-190397"
    },
    {
      "citation_id": "10",
      "title": "Fast Facial emotion recognition Using Convolutional Neural Networks and Gabor Filters",
      "authors": [
        "M Taghi Zadeh",
        "M Imani",
        "B Majidi"
      ],
      "year": "2019",
      "venue": "2019 IEEE 5th Conference on Knowledge Based Engineering and Innovation",
      "doi": "10.1109/KBEI.2019.8734943"
    },
    {
      "citation_id": "11",
      "title": "Facial Emotion Recognition Using Deep Convolutional Neural Network",
      "authors": [
        "E Pranav",
        "S Kamal",
        "C Satheesh Chandran",
        "M Supriya"
      ],
      "year": "2020",
      "venue": "2020 6th International Conference on Advanced Computing and Communication Systems, ICACCS 2020",
      "doi": "10.1109/ICACCS48705.2020.9074302"
    },
    {
      "citation_id": "12",
      "title": "Challenges in representation learning: A report on three machine learning contests",
      "authors": [
        "I Goodfellow"
      ],
      "year": "2015",
      "venue": "Neural Networks",
      "doi": "10.1016/j.neunet.2014.09.005"
    },
    {
      "citation_id": "13",
      "title": "Gradientbased learning applied to document recognition",
      "authors": [
        "Y Lecun",
        "L Bottou",
        "Y Bengio",
        "P Haffner"
      ],
      "year": "1998",
      "venue": "Proc. IEEE",
      "doi": "10.1109/5.726791"
    },
    {
      "citation_id": "14",
      "title": "Improving deep neural networks for LVCSR using rectified linear units and dropout",
      "authors": [
        "G Dahl",
        "T Sainath",
        "G Hinton"
      ],
      "year": "2013",
      "venue": "ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing -Proceedings",
      "doi": "10.1109/ICASSP.2013.6639346"
    },
    {
      "citation_id": "15",
      "title": "Fast image scanning with deep max-pooling convolutional neural networks",
      "authors": [
        "A Giusti",
        "D Cireşan",
        "J Masci",
        "L Gambardella",
        "J Schmidhuber"
      ],
      "year": "2013",
      "venue": "2013 IEEE International Conference on Image Processing",
      "doi": "10.1109/ICIP.2013.6738831"
    },
    {
      "citation_id": "16",
      "title": "Understanding of a convolutional neural network",
      "authors": [
        "S Albawi",
        "T Mohammed",
        "S Al-Zawi"
      ],
      "year": "2017",
      "venue": "Proceedings of 2017 International Conference on Engineering and Technology",
      "doi": "10.1109/ICEngTechnol.2017.8308186"
    },
    {
      "citation_id": "17",
      "title": "BranchOut: Regularization for online ensemble tracking with convolutional neural networks",
      "authors": [
        "B Han",
        "J Sim",
        "H Adam"
      ],
      "year": "2017",
      "venue": "Proceedings -30th IEEE Conference on Computer Vision and Pattern Recognition",
      "doi": "10.1109/CVPR.2017.63"
    },
    {
      "citation_id": "18",
      "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "authors": [
        "S Ioffe",
        "C Szegedy"
      ],
      "year": "2015",
      "venue": "32nd International Conference on Machine Learning, ICML 2015"
    },
    {
      "citation_id": "19",
      "title": "Optimization for deep learning: theory and algorithms",
      "authors": [
        "R Sun"
      ],
      "venue": "Optimization for deep learning: theory and algorithms"
    },
    {
      "citation_id": "20",
      "title": "Adagrad-An Optimizer for Stochastic Gradient Descent",
      "authors": [
        "A Lydia",
        "F Francis"
      ],
      "year": "2019",
      "venue": "Adagrad-An Optimizer for Stochastic Gradient Descent"
    },
    {
      "citation_id": "21",
      "title": "A sufficient condition for convergences of adam and rmsprop",
      "authors": [
        "F Zou",
        "L Shen",
        "Z Jie",
        "W Zhang",
        "W Liu"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition",
      "doi": "10.1109/CVPR.2019.01138"
    },
    {
      "citation_id": "22",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2015",
      "venue": "3rd International Conference on Learning Representations, ICLR 2015 -Conference Track Proceedings"
    },
    {
      "citation_id": "23",
      "title": "A learning-rate schedule for stochastic gradient methods to matrix factorization",
      "authors": [
        "W Chin",
        "Y Zhuang",
        "Y Juan",
        "C Lin"
      ],
      "year": "2015",
      "venue": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics",
      "doi": "10.1007/978-3-319-18038-0_35"
    },
    {
      "citation_id": "24",
      "title": "An Exponential Learning Rate Schedule For Deep Learning",
      "authors": [
        "Z Li",
        "S Arora"
      ],
      "venue": "An Exponential Learning Rate Schedule For Deep Learning"
    },
    {
      "citation_id": "25",
      "title": "A closer look at deep learning heuristics: Learning rate restarts, warmup and distillation",
      "authors": [
        "A Gotmare",
        "N Keskar",
        "C Xiong",
        "R Socher"
      ],
      "venue": "A closer look at deep learning heuristics: Learning rate restarts, warmup and distillation"
    },
    {
      "citation_id": "26",
      "title": "Facial Expression Recognition with CNN Ensemble",
      "authors": [
        "K Liu",
        "M Zhang",
        "Z Pan"
      ],
      "year": "2016",
      "venue": "Proceedings -2016 International Conference on Cyberworlds"
    },
    {
      "citation_id": "27",
      "title": "Deep-emotion: facial expression recognition using attentional convolutional network",
      "authors": [
        "S Minaee",
        "A Abdolrashidi"
      ],
      "venue": "Deep-emotion: facial expression recognition using attentional convolutional network",
      "doi": "10.3390/s21093046"
    },
    {
      "citation_id": "28",
      "title": "Local Learning to Improve Bag of Visual Words Model for Facial Expression Recognition",
      "authors": [
        "R Ionescu",
        "M Popescu",
        "C Grozea"
      ],
      "year": "2013",
      "venue": "Work. challenges Represent. Learn. ICML"
    },
    {
      "citation_id": "29",
      "title": "Local learning with deep and handcrafted features for facial expression recognition",
      "authors": [
        "M Georgescu",
        "R Ionescu",
        "M Popescu"
      ],
      "year": "2019",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2019.2917266"
    },
    {
      "citation_id": "30",
      "title": "Deep learning approaches for facial emotion recognition: A case study on FER-2013",
      "authors": [
        "P Giannopoulos",
        "I Perikos",
        "I Hatzilygeroudis"
      ],
      "year": "2018",
      "venue": "Smart Innovation, Systems and Technologies",
      "doi": "10.1007/978-3-319-66790-4_1"
    },
    {
      "citation_id": "31",
      "title": "Going deeper in facial expression recognition using deep neural networks",
      "authors": [
        "A Mollahosseini",
        "D Chan",
        "M Mahoor"
      ],
      "year": "2016",
      "venue": "2016 IEEE Winter Conference on Applications of Computer Vision, WACV 2016",
      "doi": "10.1109/WACV.2016.7477450"
    },
    {
      "citation_id": "32",
      "title": "Deep learning using linear support vector machines",
      "authors": [
        "Y Tang"
      ],
      "year": "2013",
      "venue": "arXiv"
    },
    {
      "citation_id": "33",
      "title": "Learning to Amend Facial Expression Representation via De-albino and Affinity",
      "authors": [
        "S Jiawei"
      ],
      "year": "2021",
      "venue": "arXiv"
    },
    {
      "citation_id": "34",
      "title": "Facial expression recognition using convolutional neural networks: state of the art",
      "authors": [
        "M Pramerdorfer"
      ],
      "year": "2016",
      "venue": "arXiv"
    },
    {
      "citation_id": "35",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2015",
      "venue": "3rd International Conference on Learning Representations, ICLR 2015 -Conference Track Proceedings"
    },
    {
      "citation_id": "36",
      "title": "Super-convergence: Very fast training of neural networks using large learning rates",
      "authors": [
        "L Smith",
        "N Topin"
      ],
      "venue": "Super-convergence: Very fast training of neural networks using large learning rates",
      "doi": "10.1117/12.2520589"
    },
    {
      "citation_id": "37",
      "title": "Deep inside convolutional networks: Visualising image classification models and saliency maps",
      "authors": [
        "K Simonyan",
        "A Vedaldi",
        "A Zisserman"
      ],
      "year": "2014",
      "venue": "2nd International Conference on Learning Representations, ICLR 2014 -Workshop Track Proceedings"
    }
  ]
}