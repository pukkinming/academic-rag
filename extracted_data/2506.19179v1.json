{
  "paper_id": "2506.19179v1",
  "title": "Situated Haptic Interaction: Exploring The Role Of Context In Affective Perception Of Robotic Touch",
  "published": "2025-06-23T22:49:09Z",
  "authors": [
    "Qiaoqiao Ren",
    "Tony Belpaeme"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Affective interaction is not merely about recognizing emotions; it is an embodied, situated process shaped by context and co-created through interaction. In affective computing, the role of haptic feedback within dynamic emotional exchanges remains underexplored. This study investigates how situational emotional cues influence the perception and interpretation of haptic signals given by a robot. In a controlled experiment, 32 participants watched video scenarios in which a robot experienced either positive actions (such as being kissed), negative actions (such as being slapped) or neutral actions. After each video, the robot conveyed its emotional response through haptic communication, delivered via a wearable vibration sleeve worn by the participant. Participants rated the robot's emotional state-its valence (positive or negative) and arousal (intensity)-based on the video, the haptic feedback, and the combination of the two. The study reveals a dynamic interplay between visual context and touch. Participants' interpretation of haptic feedback was strongly shaped by the emotional context of the video, with visual context often overriding the perceived valence of the haptic signal. Negative haptic cues amplified the perceived valence of the interaction, while positive cues softened it. Furthermore, haptics override the participants' perception of arousal of the video. Together, these results offer insights into how situated haptic feedback can enrich affective humanrobot interaction, pointing toward more nuanced and embodied approaches to emotional communication with machines.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Emotions are central to human experience, shaping how we engage with others and interpret the world around us. In the fields of Human-Computer Interaction (HCI) and Human-Robot Interaction (HRI), there is increasing recognition that emotions are not merely internal cognitive states but are context-dependent, embodied, and dynamically constructed through interactions  [1] ,  [2] . Traditional affective computing has largely focused on recognizing and categorizing emotions based on predefined models, such as Ekman's six basic emotions  [3]  or Russell's circumplex model  [4] . However, these models often oversimplify emotional experiences, failing to capture the nuanced and emergent nature of emotions in realworld interactions. Recent research in affective interaction challenges these reductionist frameworks, emphasizing that emotions emerge through bodily experiences, social context, and multimodal interactions  [5] .\n\nOne underexplored modality in affective interaction is touch  [6] . While vision and auditory cues have been dominant in affective computing research, haptic feedback plays a crucial role in emotional communication  [7] . Humans frequently use touch to express and perceive emotions, from Qiaoqiao Ren and Tony Belpaeme are with AIRO-IDLab, Department of Electronics and Information Systems, Ghent University -imec Qiaoqiao.Ren@ugent.be a comforting pat on the back to a gentle stroke or a reassuring squeeze  [8] . Psychological and neuroscientific research suggests that affective touch is fundamental to social bonding, emotional regulation, and communication  [9] . Studies in human-human interaction have demonstrated that different types of touch (e.g., stroking, pressing, squeezing, tapping) convey distinct emotional meanings  [10] .\n\nIn Human-Robot Interaction (HRI), affective touch has been explored primarily in therapeutic and assistive applications  [11] . For example, socially assistive robots such as PARO (a robotic seal designed for elderly care) provide comfort through tactile interaction  [12] . However, much of the existing research on haptic feedback in robotics has focused on sensory augmentation rather than emotional communication  [13] . Some studies have investigated how robots can convey emotions through haptic modalities, varying touch intensity, duration, and rhythm  [11] . And previous research indicates that human-robot tactile interaction could influence people's behaviour and attitudes  [14] ,  [15] . Furthermore, emotional cues in visual and haptic modalities have been examined in relation to multimodal emotion perception  [16] . These studies suggest that haptic signals can effectively communicate emotions, but they do not fully address the situational and contextual factors that influence the interpretation of affective touch.\n\nWith advancements in haptic technology, researchers have begun exploring mediated touch-where tactile signals are transmitted through digital or robotic interfaces-as a means of conveying emotions. Mediated touch systems enable remote affective communication, such as haptic telepresence, where one user's touch is reproduced through a robotic or vibrotactile interface  [17] . Vibrotactile haptics can encode emotions through variations in frequency, amplitude, and rhythm, simulating natural touch-like sensations  [18] . Previous research has suggested that certain vibrotactile patterns can be associated with positive and negative emotions  [19] ,  [20] . This is particularly relevant in robotic interaction, where direct human-like touch is not always possible, and vibrations can serve as an alternative means of emotional expression.\n\nAffective perception is not solely determined by the physical properties of haptic feedback; it is also shaped by situational cues and contextual meaning  [21] . In humanhuman interaction, the same physical touch can carry vastly different meanings depending on the context  [22] . Moreover, more recent studies indicate that contextual and social factors significantly influence the emotional interpretation of touch  [7] . For instance, a firm handshake in a business setting conveys professionalism and confidence, whereas the same touch in an intimate setting may feel cold and distant  [23] . Similarly, research has shown that most touch behaviours carry symbolic meaning and can effectively communicate complex emotions  [24] . In HRI, similar principles apply-how people interpret a robot's haptic feedback depends on the situational cues surrounding the interaction  [25] . Multimodal signals-such as facial expressions, verbal utterances, and body language-significantly shape how people perceive touchbased robotic interactions  [26] . However, existing studies have not systematically examined the role of context in shaping the perception of robotic haptic feedback.\n\nTo address this gap, this study aimed to investigate how situational context and tactile feedback interact to influence emotional interpretation during interactions with a humanoid robot (Pepper). Specifically, we focus on the following research questions (RQs):\n\n1) RQ1: Can participants decode emotions through vibration by a mediated wearable haptic device? 2) RQ2: Does situational context override or bias the interpretation of a haptic stimulus's valence? 3) RQ3: Does situational context override or bias the interpretation of a haptic stimulus's arousal?",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Methods",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Participants",
      "text": "Thirty-two Chinese participants (10 female, 18 male; M = 27.8, SD = 2.3 years) participated in the experiment. Participants were recruited from a similar cultural background to ensure consistency in emotional interpretation. The study complied with ethical guidelines established by Ghent University, and informed consent was obtained from all.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Experimental Design",
      "text": "The experiment employed a within-subject design, where each participant experienced all conditions, including: (1) two types of haptic feedback (comfort stimulus with positive valence and low arousal, and anger stimulus with negative valence and high arousal  [4] ); (2) six situational context videos (Kiss, Slap, Eye Contact, Stroke, Flick, Cover Eyes), which is available in our GitHub repository 1 ; and (3) the combination of each video with both types of haptic feedback. Participants were asked to decode the emotions conveyed by the Pepper robot (present in the experiment room) through a vibration sleeve  [20] , as shown in Fig.  2b .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Haptic Data Construction",
      "text": "We conducted data collection experiments to collect touch data associated with emotions through a 5 × 5 piezoresistive tactile sensor, as described in  [27] . Participants first expressed ten different emotions by touching the Pepper robot's upper arm, resulting in a dataset of 84 tactile files in CSV format at 45 Hz, each 10 seconds in duration.\n\nTo analyse this data, for each emotion, we applied the k-means clustering algorithm, grouping the audio features and tactile features in  [27]  into three clusters based on 1 https://github.com/qiaoqiao2323/Situational haptic interaction/tree/main their acoustic features. We identified the most populated cluster, representing the largest grouping of participants, as the most characteristic. Within this cluster, we calculated the Euclidean distance of each participant's expression features to the cluster centroid (average feature values) to assess each clip's representativeness. The three participants' IDs with the smallest distances to the centroid were selected as the most representative, and we chose one participant's data among the three to serve as a representative stimulus for all participants. In this experiment, we selected two haptic stimuli based on participant expressions: one representing \"anger\", characterized by high arousal and negative valence, and the other representing \"comfort\", characterized by low arousal and positive valence.\n\nThen we translate the tactile files to vibrations and the system uses pulse-width modulation (PWM) for precise control over the intensity of each motor's vibration, which allows for modulation of the vibration intensity by rapidly switching the transistors on and off at varying duty cycles. Each vibration stimulus lasts for 10 seconds as well.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "D. Situational Context Actions Construction",
      "text": "Inspired by  [28] , we designed six distinct situational context interactions: Kiss, where the Pepper robot received a kiss from the human; Slap, where the human slapped Pepper's face; Eye Contact, where Pepper attempted to establish eye contact by looking at the camera; Stroke, where the human gently stroked Pepper's head; Flick, where the human flicked Pepper's forehead; and Cover Eyes, where the human covered Pepper's eyes.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "E. Procedures",
      "text": "Each participant underwent four phases as shown in Fig.  2a ; in each phase, they were asked to decode emotions for each stimulus by Self Assessment Manikins (SAM) as shown in Fig.  1 .  • Participants observe the robot experiencing an action and responding with an \"Anger\" vibration C Context Anger , as well as a \"Comfort\" vibration C Context Comfort , here the Context includes Slap, Eye Contact, Stroke, Flick, Cover eyes and Kiss.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Results And Analysis",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Tactile Stimulus Only",
      "text": "A Wilcoxon rank-sum test indicated significant differences between C Tactile Anger (8.06 ± 1.50) and C Tactile Comfort (4.16 ± 1.71) haptic stimuli for arousal ratings, (W = 951, p < .001). Similarly, valence ratings significantly differed between C Tactile Anger (2.06 ± 1.41) and C Tactile Comfort",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Comparison Between Haptic-Only And Video With Haptic Feedback",
      "text": "1) Arousal analysis: We analyzed arousal levels in conditions involving action videos with haptic feedback and haptic-only scenarios (served as a no-situational context group). As shown in Fig.  3a , the Kruskal-Wallis test identified a significant main effect of situational context, which included six actions with haptic feedback and hapticonly conditions (where haptic-only conditions are considered as having no prior situational context), on arousal for both C Tactile Anger (H(6) = 20.18, p = 0.003) and C Tactile Comfort (H(6) = 13.89, p = 0.03).     Post-hoc Wilcoxon signed-rank tests with Bonferroni correction revealed no significant differences in arousal levels between C Tactile Comfort and six C Context Comfort . Similarly, no significant differences were found between C Tactile Anger and C Context Anger . This finding suggests that tactile feedback can significantly override the arousal interpretation based on sit-uational context. In addition, a significant difference was observed between the C Slap Anger and the C Stroke Anger (p = 0.047); C Slap Anger and the C Eye Contact Anger (p = 0.041). Moreover, only significant difference was found between C Slap Comfort and the C Eye Contact Comfort (p = 0.041). The number of significant differences observed among situational contexts with haptic feedback is notably lower than in the situational contextonly condition, as illustrated in Fig.  4b . Additionally, the variation in arousal ratings across the six situational contexts is reduced when haptic feedback is introduced, converging toward the arousal level of the haptic stimulus. Furthermore, across all situational contexts with haptic feedback, a significant difference was found between actions featuring anger tactile feedback and those featuring comfort tactile feedback (p < 0.05), suggesting that participants could distinguish between different arousal levels of haptic feedback in various action scenarios.\n\nThe results, summarized in Tab. I, further indicate that tactile stimuli can override the arousal interpretation shaped by situational context, in addition, haptic feedback reduces variability in arousal ratings across different situational contexts, as shown in Fig.  3a .\n\n2) Valence analysis: For valence, the results can be seen in Fig.  3b  and Table . II. The Kruskal-Wallis test identified a significant main effect of situational context on their valence level, which included six actions with haptic feedback and haptic-only conditions (where haptic-only conditions are considered as having no prior situational context), on valence for both C Tactile Anger (H(6) = 142.97, p < 0.001) and C Tactile Comfort (H(6) = 106.90, p < 0.001). Post-hoc Wilcoxon signed-rank tests with Bonferroni correction revealed significant differences in valence levels between C Tactile Comfort and three action videos with C Tactile Comfort , specifically C Flick Comfort (p = 0.03), C Kiss Comfort (p = 0.03), and C Slap Comfort (p < 0.001). Additionally, all six action pairwise comparisons showed significant differences (p < 0.05), with the exception of C Stroke Comfort and C Kiss Comfort , where no significant difference was observed. Similarly, significant differences were found between C Tactile Anger and the three C Tactile Anger , including kiss (p < 0.001), eye contact (p < 0.001), and stroke (p < 0.001). This indicates that tactile stimulus valence does not override the situational context valence.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "D. Comparison Between Video-Only And Video With Haptic Feedback",
      "text": "1) Arousal analysis: For arousal, we found a significant main effect of different haptic arousal levels (including conditions without haptic stimuli) across different actions (p < 0.001). Additionally, there was a significant difference in arousal ratings among all six C Context Comfort (p < 0.001). For pairwise comparison, we found that arousal ratings were significantly higher in all C Context Anger compared to C Context (p < 0.001). However, C Context Comfort , significant differences were observed only in C Stroke Comfort (p = 0.03) and C Slap Comfort (p < 0.001) when compared to their respective video-only conditions.\n\n2) Valence analysis: For valence, no significant differences were found between situational context with anger haptic stimuli and their corresponding situational context video-only conditions (p > 0.05). In addition, no significant differences were found between most situational context with comfort haptic stimuli and their corresponding videoonly conditions, there are only two significant effects were observed, one for C Kiss Comfort compared to C Kiss (p < 0.001), and another for C Slap Comfort compared to C Slap (p = 0.007), which indicates that situational context overrides the valence interpretation derived from tactile feedback.\n\nAs shown in Fig.  3b  and Fig.  4b , the situational context significantly overrides the valence interpretation derived from the tactile feedback. In addition, we observed that negative haptic stimuli (anger) tended to amplify emotions by reinforcing or mirroring the emotional intensity of the action, whereas positive haptic stimuli (comfort) appeared to modulate emotions, reducing the valence intensity of the experience, and tended to converge around the neutral valence rating. To examine how tactile feedback influences emotional responses, we compared situational context with haptic stimulus valence deviation from the neutral valence and used the original situational context valence as a reference. Then, we used a linear mixed-effects model. The model included Emotion Category (C Context as reference level, C Context Comfort and C Context Anger ) as a fixed effect and Participant ID (PN) as a random intercept to account for inter-individual variability:\n\nEmotion deviation was defined as the absolute deviation from neutral valence (5), such that higher values reflect stronger emotional reactions regardless of direction.\n\nCompared to the C Context , C Context Comfort significantly reduced valence deviation (β = -0.41, SE = 0.12, t(542) = -3.26, p = .001), indicating that positive tactile stimulation moderated affective responses and brought them closer to neutrality. In contrast, C Context Anger significantly increased valence deviation (β = 0.58, SE = 0.12, t(542) = 4.68, p < .001), suggesting that negative tactile stimulus amplified situational context valence.\n\nThe regulatory and amplifying effects of tactile stimulation on valence may be understood through Gross's emotion regulation framework  [29] , which emphasizes the role of cognitive strategies in modulating emotional experiences. Positive touch likely engages cognitive reappraisal processes, enabling individuals to reinterpret emotional stimuli in a less extreme manner. Conversely, negative touch may enhance emotional valence through increased emotional perception. This may involve emotion contagion-where tactile input intensifies affective resonance.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Iv. Discussion",
      "text": "This study explored the interaction between tactile feedback and situational context in shaping emotional interpretations during human-robot interaction. Firstly, we can confirm that participants could differentiate the arousal and valence from the solely haptic stimulus sent from the robot by a mediated vibration sleeve.\n\nFor arousal, the findings demonstrate that tactile feedback can override situational context arousal ratings. In addition, Tactile stimulus decreased the variance among all the six situational contexts. One possible explanation is participants could feel the vibration amplitude from the haptic stimulus, which is difficult to be influenced by the situational context.\n\nIn addition, situational context plays a role in shaping valence interpretation under contexts with haptic stimulus, where contextual cues dominate the perception of emotion valence, leaving less room for haptic feedback to alter valence ratings. In addition, negative haptic stimuli (anger) amplify emotional responses, reinforcing the intensity of both positive and negative emotions, whereas positive haptic stimuli (comfort) moderate valence, shifting emotions toward a more neutral state, which could also be because of the high arousal state has interplay with the interpretation of the valence. These findings suggest that tactile feedback does not function in isolation but rather interacts with broader contextual cues to shape emotional responses.\n\nIn addition, visual context overwhelmingly dominated emotional interpretations in scenarios with strongly negative connotations, such as \"slap,\" leaving minimal space for tactile feedback to alter perceptions significantly. However, in scenarios with positive or ambiguous emotional connotations like \"kiss\", \"stroke\", or \"eye contact\", tactile stimuli markedly influenced participants' emotional judgments, highlighting the importance of multimodal congruency in shaping emotional perceptions. In addition, participants decode \"eye contact\" with the robot as positive valence, which might caused by the appearance of the pepper robot or the eye interaction that could raise the positive emotion.\n\nWhile the results underscore the significance of tactilecontext congruency, some methodological limitations exist. The study was conducted in a controlled laboratory setting with video stimuli, potentially limiting ecological validity. Additionally, the homogeneous nature of the participant group restricts generalizability across diverse populations. Future research should examine similar multimodal emotional interactions in real-world settings and involve culturally diverse participant groups.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "V. Conclusion",
      "text": "This study explored the interaction between tactile feedback and situational context in shaping emotional interpretations during human-robot interaction. The results indicate that situational context significantly dominates valence perception, with negative haptic stimuli (anger) amplifying overall valence level and positive haptic stimuli (comfort) moderating valence toward neutrality. Arousal was primarily driven by haptic feedback. These findings underscore the context-dependent nature of haptic feedback, reinforcing the need for adaptive, context-aware haptic systems in affective computing and human-robot interaction.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 2: a; in each phase, they were asked to decode emotions",
      "page": 2
    },
    {
      "caption": "Figure 1: Fig. 1: SAM for valence and arousal, the first row is valence",
      "page": 2
    },
    {
      "caption": "Figure 2: Vibration sleeves.",
      "page": 3
    },
    {
      "caption": "Figure 3: a, the Kruskal-Wallis test",
      "page": 3
    },
    {
      "caption": "Figure 3: Comparison between the tactile feedback only and video with tactile feedback. In subfigure (a), it’s shown that the",
      "page": 4
    },
    {
      "caption": "Figure 4: Comparison between the video only and video with tactile feedback. Subfigue (a) shows that there is a significant",
      "page": 4
    },
    {
      "caption": "Figure 4: b. Additionally, the",
      "page": 5
    },
    {
      "caption": "Figure 3: b and Table. II. The Kruskal-Wallis test identified",
      "page": 5
    },
    {
      "caption": "Figure 3: b and Fig. 4b, the situational con-",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Qiaoqiao Ren and Tony Belpaeme": "Abstract— Affective\ninteraction is not merely\nabout\nrecog-"
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": "nizing\nemotions;\nit\nis\nan embodied,\nsituated process\nshaped"
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": ""
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": "by\ncontext\nand\nco-created\nthrough\ninteraction.\nIn\naffective"
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": ""
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": "computing,\nthe\nrole of haptic\nfeedback within dynamic\nemo-"
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": ""
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": "tional exchanges remains underexplored. This study investigates"
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": "how situational\nemotional\ncues\ninfluence\nthe perception and"
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": "interpretation of haptic signals given by a robot. In a controlled"
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": "experiment, 32 participants watched video scenarios in which a"
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": ""
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": "robot experienced either positive actions (such as being kissed),"
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": ""
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": "negative actions (such as being slapped) or neutral actions. After"
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": ""
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": "each video,\nthe robot conveyed its emotional response through"
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": "haptic\ncommunication,\ndelivered\nvia\na wearable\nvibration"
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": "sleeve worn by the participant. Participants rated the robot’s"
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": "emotional\nstate—its valence (positive or negative) and arousal"
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": ""
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": "(intensity)—based on the video,\nthe haptic\nfeedback, and the"
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": ""
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": "combination of\nthe two. The study reveals a dynamic interplay"
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": ""
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": "between visual context and touch. Participants’\ninterpretation"
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": "of haptic feedback was strongly shaped by the emotional context"
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": "of\nthe video, with visual context often overriding the perceived"
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": "valence of\nthe haptic signal. Negative haptic cues amplified the"
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": ""
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": "perceived valence of the interaction, while positive cues softened"
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": ""
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": "it. Furthermore, haptics override the participants’ perception"
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": ""
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": "of arousal of\nthe video. Together,\nthese\nresults offer\ninsights"
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": "into how situated haptic feedback can enrich affective human-"
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": "robot interaction, pointing toward more nuanced and embodied"
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": "approaches to emotional communication with machines."
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": ""
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": ""
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": "I.\nINTRODUCTION"
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": ""
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": "Emotions are central\nto human experience,\nshaping how"
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": ""
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": "we engage with others and interpret\nthe world around us. In"
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": ""
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": "the fields of Human-Computer Interaction (HCI) and Human-"
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": ""
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": "Robot Interaction (HRI),\nthere is increasing recognition that"
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": ""
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": "emotions\nare\nnot merely\ninternal\ncognitive\nstates\nbut\nare"
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": ""
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": "context-dependent, embodied, and dynamically constructed"
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": ""
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": "through interactions [1], [2]. Traditional affective computing"
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": ""
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": "has largely focused on recognizing and categorizing emotions"
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": ""
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": "based on predefined models, such as Ekman’s six basic emo-"
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": ""
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": "tions [3] or Russell’s circumplex model\n[4]. However,\nthese"
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": ""
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": "models often oversimplify emotional experiences,\nfailing to"
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": ""
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": "capture the nuanced and emergent nature of emotions in real-"
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": ""
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": "world interactions. Recent\nresearch in affective\ninteraction"
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": ""
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": "challenges\nthese reductionist\nframeworks, emphasizing that"
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": ""
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": "emotions emerge through bodily experiences, social context,"
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": ""
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": "and multimodal\ninteractions [5]."
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": ""
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": "One\nunderexplored modality\nin\naffective\ninteraction\nis"
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": "touch [6]. While vision and auditory cues have been domi-"
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": "nant\nin affective computing research, haptic feedback plays"
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": "a\ncrucial\nrole\nin\nemotional\ncommunication\n[7]. Humans"
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": "frequently use touch to express and perceive emotions, from"
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": ""
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": "Qiaoqiao Ren\nand\nTony Belpaeme\nare with AIRO-IDLab, Depart-"
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": ""
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": "ment\nof Electronics\nand\nInformation Systems, Ghent University\n-\nimec"
        },
        {
          "Qiaoqiao Ren and Tony Belpaeme": "Qiaoqiao.Ren@ugent.be"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "in an intimate setting may feel cold and distant\n[23]. Sim-": "ilarly,\nresearch has shown that most\ntouch behaviours carry",
          "their\nacoustic\nfeatures. We\nidentified\nthe most\npopulated": "cluster,\nrepresenting the largest grouping of participants, as"
        },
        {
          "in an intimate setting may feel cold and distant\n[23]. Sim-": "symbolic meaning and can effectively communicate complex",
          "their\nacoustic\nfeatures. We\nidentified\nthe most\npopulated": "the most characteristic. Within this cluster, we calculated the"
        },
        {
          "in an intimate setting may feel cold and distant\n[23]. Sim-": "emotions [24]. In HRI, similar principles apply—how people",
          "their\nacoustic\nfeatures. We\nidentified\nthe most\npopulated": "Euclidean distance of each participant’s expression features"
        },
        {
          "in an intimate setting may feel cold and distant\n[23]. Sim-": "interpret\na\nrobot’s\nhaptic\nfeedback\ndepends\non\nthe\nsitua-",
          "their\nacoustic\nfeatures. We\nidentified\nthe most\npopulated": "to\nthe\ncluster\ncentroid\n(average\nfeature\nvalues)\nto\nassess"
        },
        {
          "in an intimate setting may feel cold and distant\n[23]. Sim-": "tional cues surrounding the interaction [25]. Multimodal sig-",
          "their\nacoustic\nfeatures. We\nidentified\nthe most\npopulated": "each\nclip’s\nrepresentativeness. The\nthree\nparticipants’\nIDs"
        },
        {
          "in an intimate setting may feel cold and distant\n[23]. Sim-": "nals—such as facial expressions, verbal utterances, and body",
          "their\nacoustic\nfeatures. We\nidentified\nthe most\npopulated": "with the smallest distances to the centroid were selected as"
        },
        {
          "in an intimate setting may feel cold and distant\n[23]. Sim-": "language—significantly shape how people perceive\ntouch-",
          "their\nacoustic\nfeatures. We\nidentified\nthe most\npopulated": "the most representative, and we chose one participant’s data"
        },
        {
          "in an intimate setting may feel cold and distant\n[23]. Sim-": "based\nrobotic\ninteractions\n[26]. However,\nexisting\nstudies",
          "their\nacoustic\nfeatures. We\nidentified\nthe most\npopulated": "among the\nthree\nto serve\nas\na\nrepresentative\nstimulus\nfor"
        },
        {
          "in an intimate setting may feel cold and distant\n[23]. Sim-": "have\nnot\nsystematically\nexamined\nthe\nrole\nof\ncontext\nin",
          "their\nacoustic\nfeatures. We\nidentified\nthe most\npopulated": "all participants.\nIn this experiment, we selected two haptic"
        },
        {
          "in an intimate setting may feel cold and distant\n[23]. Sim-": "shaping the perception of\nrobotic haptic feedback.",
          "their\nacoustic\nfeatures. We\nidentified\nthe most\npopulated": "stimuli based on participant\nexpressions: one\nrepresenting"
        },
        {
          "in an intimate setting may feel cold and distant\n[23]. Sim-": "To address\nthis gap,\nthis\nstudy aimed to investigate how",
          "their\nacoustic\nfeatures. We\nidentified\nthe most\npopulated": "“anger”, characterized by high arousal and negative valence,"
        },
        {
          "in an intimate setting may feel cold and distant\n[23]. Sim-": "situational context and tactile feedback interact\nto influence",
          "their\nacoustic\nfeatures. We\nidentified\nthe most\npopulated": "and the other\nrepresenting “comfort”, characterized by low"
        },
        {
          "in an intimate setting may feel cold and distant\n[23]. Sim-": "emotional\ninterpretation during interactions with a humanoid",
          "their\nacoustic\nfeatures. We\nidentified\nthe most\npopulated": "arousal and positive valence."
        },
        {
          "in an intimate setting may feel cold and distant\n[23]. Sim-": "robot\n(Pepper).\nSpecifically, we\nfocus\non\nthe\nfollowing",
          "their\nacoustic\nfeatures. We\nidentified\nthe most\npopulated": "Then we\ntranslate\nthe\ntactile files\nto vibrations\nand the"
        },
        {
          "in an intimate setting may feel cold and distant\n[23]. Sim-": "research questions (RQs):",
          "their\nacoustic\nfeatures. We\nidentified\nthe most\npopulated": "system uses\npulse-width modulation\n(PWM)\nfor\nprecise"
        },
        {
          "in an intimate setting may feel cold and distant\n[23]. Sim-": "",
          "their\nacoustic\nfeatures. We\nidentified\nthe most\npopulated": "control over\nthe intensity of each motor’s vibration, which"
        },
        {
          "in an intimate setting may feel cold and distant\n[23]. Sim-": "1) RQ1: Can participants decode emotions through vibra-",
          "their\nacoustic\nfeatures. We\nidentified\nthe most\npopulated": ""
        },
        {
          "in an intimate setting may feel cold and distant\n[23]. Sim-": "",
          "their\nacoustic\nfeatures. We\nidentified\nthe most\npopulated": "allows\nfor modulation of\nthe vibration intensity by rapidly"
        },
        {
          "in an intimate setting may feel cold and distant\n[23]. Sim-": "tion by a mediated wearable haptic device?",
          "their\nacoustic\nfeatures. We\nidentified\nthe most\npopulated": ""
        },
        {
          "in an intimate setting may feel cold and distant\n[23]. Sim-": "",
          "their\nacoustic\nfeatures. We\nidentified\nthe most\npopulated": "switching the transistors on and off at varying duty cycles."
        },
        {
          "in an intimate setting may feel cold and distant\n[23]. Sim-": "2) RQ2: Does\nsituational\ncontext\noverride\nor\nbias\nthe",
          "their\nacoustic\nfeatures. We\nidentified\nthe most\npopulated": ""
        },
        {
          "in an intimate setting may feel cold and distant\n[23]. Sim-": "",
          "their\nacoustic\nfeatures. We\nidentified\nthe most\npopulated": "Each vibration stimulus lasts for 10 seconds as well."
        },
        {
          "in an intimate setting may feel cold and distant\n[23]. Sim-": "interpretation of a haptic stimulus’s valence?",
          "their\nacoustic\nfeatures. We\nidentified\nthe most\npopulated": ""
        },
        {
          "in an intimate setting may feel cold and distant\n[23]. Sim-": "3) RQ3: Does\nsituational\ncontext\noverride\nor\nbias\nthe",
          "their\nacoustic\nfeatures. We\nidentified\nthe most\npopulated": "D.\nSituational context actions construction"
        },
        {
          "in an intimate setting may feel cold and distant\n[23]. Sim-": "interpretation of a haptic stimulus’s arousal?",
          "their\nacoustic\nfeatures. We\nidentified\nthe most\npopulated": ""
        },
        {
          "in an intimate setting may feel cold and distant\n[23]. Sim-": "",
          "their\nacoustic\nfeatures. We\nidentified\nthe most\npopulated": "Inspired\nby\n[28], we\ndesigned\nsix\ndistinct\nsituational"
        },
        {
          "in an intimate setting may feel cold and distant\n[23]. Sim-": "",
          "their\nacoustic\nfeatures. We\nidentified\nthe most\npopulated": "context\ninteractions: Kiss, where the Pepper\nrobot\nreceived"
        },
        {
          "in an intimate setting may feel cold and distant\n[23]. Sim-": "II. METHODS",
          "their\nacoustic\nfeatures. We\nidentified\nthe most\npopulated": ""
        },
        {
          "in an intimate setting may feel cold and distant\n[23]. Sim-": "",
          "their\nacoustic\nfeatures. We\nidentified\nthe most\npopulated": "a\nkiss\nfrom the\nhuman; Slap, where\nthe\nhuman\nslapped"
        },
        {
          "in an intimate setting may feel cold and distant\n[23]. Sim-": "A. Participants",
          "their\nacoustic\nfeatures. We\nidentified\nthe most\npopulated": ""
        },
        {
          "in an intimate setting may feel cold and distant\n[23]. Sim-": "",
          "their\nacoustic\nfeatures. We\nidentified\nthe most\npopulated": "Pepper’s\nface;\nEye Contact, where\nPepper\nattempted\nto"
        },
        {
          "in an intimate setting may feel cold and distant\n[23]. Sim-": "Thirty-two Chinese participants\n(10 female, 18 male; M",
          "their\nacoustic\nfeatures. We\nidentified\nthe most\npopulated": "establish eye contact by looking at the camera; Stroke, where"
        },
        {
          "in an intimate setting may feel cold and distant\n[23]. Sim-": "= 27.8,\nSD = 2.3\nyears)\nparticipated\nin\nthe\nexperiment.",
          "their\nacoustic\nfeatures. We\nidentified\nthe most\npopulated": "the human gently stroked Pepper’s head; Flick, where\nthe"
        },
        {
          "in an intimate setting may feel cold and distant\n[23]. Sim-": "Participants were\nrecruited\nfrom a\nsimilar\ncultural\nback-",
          "their\nacoustic\nfeatures. We\nidentified\nthe most\npopulated": "human flicked Pepper’s\nforehead;\nand Cover Eyes, where"
        },
        {
          "in an intimate setting may feel cold and distant\n[23]. Sim-": "ground to ensure consistency in emotional interpretation. The",
          "their\nacoustic\nfeatures. We\nidentified\nthe most\npopulated": "the human covered Pepper’s eyes."
        },
        {
          "in an intimate setting may feel cold and distant\n[23]. Sim-": "study complied with ethical guidelines established by Ghent",
          "their\nacoustic\nfeatures. We\nidentified\nthe most\npopulated": ""
        },
        {
          "in an intimate setting may feel cold and distant\n[23]. Sim-": "",
          "their\nacoustic\nfeatures. We\nidentified\nthe most\npopulated": "E. Procedures"
        },
        {
          "in an intimate setting may feel cold and distant\n[23]. Sim-": "University, and informed consent was obtained from all.",
          "their\nacoustic\nfeatures. We\nidentified\nthe most\npopulated": ""
        },
        {
          "in an intimate setting may feel cold and distant\n[23]. Sim-": "",
          "their\nacoustic\nfeatures. We\nidentified\nthe most\npopulated": "Each\nparticipant\nunderwent\nfour\nphases\nas\nshown\nin"
        },
        {
          "in an intimate setting may feel cold and distant\n[23]. Sim-": "B. Experimental design",
          "their\nacoustic\nfeatures. We\nidentified\nthe most\npopulated": "Fig. 2a;\nin each phase,\nthey were asked to decode emotions"
        },
        {
          "in an intimate setting may feel cold and distant\n[23]. Sim-": "",
          "their\nacoustic\nfeatures. We\nidentified\nthe most\npopulated": "for each stimulus by Self Assessment Manikins\n(SAM) as"
        },
        {
          "in an intimate setting may feel cold and distant\n[23]. Sim-": "The experiment employed a within-subject design, where",
          "their\nacoustic\nfeatures. We\nidentified\nthe most\npopulated": ""
        },
        {
          "in an intimate setting may feel cold and distant\n[23]. Sim-": "",
          "their\nacoustic\nfeatures. We\nidentified\nthe most\npopulated": "shown in Fig. 1."
        },
        {
          "in an intimate setting may feel cold and distant\n[23]. Sim-": "each\nparticipant\nexperienced\nall\nconditions,\nincluding:\n(1)",
          "their\nacoustic\nfeatures. We\nidentified\nthe most\npopulated": ""
        },
        {
          "in an intimate setting may feel cold and distant\n[23]. Sim-": "two types of haptic\nfeedback (comfort\nstimulus with pos-",
          "their\nacoustic\nfeatures. We\nidentified\nthe most\npopulated": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4) Combined Tactile and Context Evaluation: Partici-": "pants rated arousal and valence while observing video",
          "no significant difference was found between CFlick and CKiss,": "and CEye Contact\nand CStroke on arousal, which indicate that"
        },
        {
          "4) Combined Tactile and Context Evaluation: Partici-": "scenarios followed by vibration feedback sending from",
          "no significant difference was found between CFlick and CKiss,": "some\nsituational\ncontext\nhas\nsimilar\narousal\nlevel. These"
        },
        {
          "4) Combined Tactile and Context Evaluation: Partici-": "the robot CContextAnger, CContextComfort ).",
          "no significant difference was found between CFlick and CKiss,": ""
        },
        {
          "4) Combined Tactile and Context Evaluation: Partici-": "",
          "no significant difference was found between CFlick and CKiss,": "responses depending on the situational context."
        },
        {
          "4) Combined Tactile and Context Evaluation: Partici-": "•\nParticipants\nobserve\nthe\nrobot\nexperiencing\nan",
          "no significant difference was found between CFlick and CKiss,": ""
        },
        {
          "4) Combined Tactile and Context Evaluation: Partici-": "",
          "no significant difference was found between CFlick and CKiss,": "2) Valence analysis: The Kruskal-Wallis\ntest\nrevealed a"
        },
        {
          "4) Combined Tactile and Context Evaluation: Partici-": "action\nand\nresponding with\nan\n“Anger”\nvibra-",
          "no significant difference was found between CFlick and CKiss,": ""
        },
        {
          "4) Combined Tactile and Context Evaluation: Partici-": "",
          "no significant difference was found between CFlick and CKiss,": "significant main\neffect\nof\nsituational\ncontext\non\nvalence"
        },
        {
          "4) Combined Tactile and Context Evaluation: Partici-": "tion CContextAnger , as well as a “Comfort” vibration",
          "no significant difference was found between CFlick and CKiss,": ""
        },
        {
          "4) Combined Tactile and Context Evaluation: Partici-": "",
          "no significant difference was found between CFlick and CKiss,": "ratings\n(H(5) = 145.81, p < 0.001). Post-hoc Wilcoxon"
        },
        {
          "4) Combined Tactile and Context Evaluation: Partici-": "CContextComfort , here the Context",
          "no significant difference was found between CFlick and CKiss,": ""
        },
        {
          "4) Combined Tactile and Context Evaluation: Partici-": "",
          "no significant difference was found between CFlick and CKiss,": "signed-rank tests with Bonferroni correction further explored"
        },
        {
          "4) Combined Tactile and Context Evaluation: Partici-": "Contact, Stroke, Flick, Cover eyes and Kiss.",
          "no significant difference was found between CFlick and CKiss,": ""
        },
        {
          "4) Combined Tactile and Context Evaluation: Partici-": "",
          "no significant difference was found between CFlick and CKiss,": "these\ndifferences. Significant\ndifferences\nemerged\nnotably"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ratings\n(H(5) = 145.81, p < 0.001). Post-hoc Wilcoxon": ""
        },
        {
          "ratings\n(H(5) = 145.81, p < 0.001). Post-hoc Wilcoxon": "signed-rank tests with Bonferroni correction further explored"
        },
        {
          "ratings\n(H(5) = 145.81, p < 0.001). Post-hoc Wilcoxon": ""
        },
        {
          "ratings\n(H(5) = 145.81, p < 0.001). Post-hoc Wilcoxon": "these\ndifferences. Significant\ndifferences\nemerged\nnotably"
        },
        {
          "ratings\n(H(5) = 145.81, p < 0.001). Post-hoc Wilcoxon": "conditions\n(p < 0.001),\nbetween the CCover eyes\nand CKiss"
        },
        {
          "ratings\n(H(5) = 145.81, p < 0.001). Post-hoc Wilcoxon": "(p < 0.001),\nas well as between CCover eyes and CEye Contact"
        },
        {
          "ratings\n(H(5) = 145.81, p < 0.001). Post-hoc Wilcoxon": ""
        },
        {
          "ratings\n(H(5) = 145.81, p < 0.001). Post-hoc Wilcoxon": "(p < 0.001). Additionally,\nthe\nand CCover eyes versus CStroke"
        },
        {
          "ratings\n(H(5) = 145.81, p < 0.001). Post-hoc Wilcoxon": ""
        },
        {
          "ratings\n(H(5) = 145.81, p < 0.001). Post-hoc Wilcoxon": "(p < 0.001),\nCFlick scenario significantly differed from CKiss"
        },
        {
          "ratings\n(H(5) = 145.81, p < 0.001). Post-hoc Wilcoxon": ""
        },
        {
          "ratings\n(H(5) = 145.81, p < 0.001). Post-hoc Wilcoxon": ""
        },
        {
          "ratings\n(H(5) = 145.81, p < 0.001). Post-hoc Wilcoxon": "(p <\nCEye Contact\n(p < 0.001), CSlap (p < 0.001), and CStroke"
        },
        {
          "ratings\n(H(5) = 145.81, p < 0.001). Post-hoc Wilcoxon": ""
        },
        {
          "ratings\n(H(5) = 145.81, p < 0.001). Post-hoc Wilcoxon": "0.001) conditions. Significant differences were also observed"
        },
        {
          "ratings\n(H(5) = 145.81, p < 0.001). Post-hoc Wilcoxon": ""
        },
        {
          "ratings\n(H(5) = 145.81, p < 0.001). Post-hoc Wilcoxon": "between\nscenario\n(p < 0.001),\nthe CKiss\nand CEye Contact"
        },
        {
          "ratings\n(H(5) = 145.81, p < 0.001). Post-hoc Wilcoxon": ""
        },
        {
          "ratings\n(H(5) = 145.81, p < 0.001). Post-hoc Wilcoxon": "(p < 0.001),\n(p = 0.0007)\nconditions.\nCSlap\nand CStroke"
        },
        {
          "ratings\n(H(5) = 145.81, p < 0.001). Post-hoc Wilcoxon": ""
        },
        {
          "ratings\n(H(5) = 145.81, p < 0.001). Post-hoc Wilcoxon": "Lastly,\nscenario was\nsignificantly different\nfrom\nthe CSlap"
        },
        {
          "ratings\n(H(5) = 145.81, p < 0.001). Post-hoc Wilcoxon": ""
        },
        {
          "ratings\n(H(5) = 145.81, p < 0.001). Post-hoc Wilcoxon": "(p < 0.001)\n(p < 0.001).\nboth CEye Contact\nand CStroke"
        },
        {
          "ratings\n(H(5) = 145.81, p < 0.001). Post-hoc Wilcoxon": ""
        },
        {
          "ratings\n(H(5) = 145.81, p < 0.001). Post-hoc Wilcoxon": "These\nresults\nindicate\na\nsignificant\ndifference\nin\npartici-"
        },
        {
          "ratings\n(H(5) = 145.81, p < 0.001). Post-hoc Wilcoxon": "pants’ valence responses across all\nthe situational contexts,"
        },
        {
          "ratings\n(H(5) = 145.81, p < 0.001). Post-hoc Wilcoxon": ""
        },
        {
          "ratings\n(H(5) = 145.81, p < 0.001). Post-hoc Wilcoxon": "highlighting that\nthe proposed six situational contexts elicit"
        },
        {
          "ratings\n(H(5) = 145.81, p < 0.001). Post-hoc Wilcoxon": ""
        },
        {
          "ratings\n(H(5) = 145.81, p < 0.001). Post-hoc Wilcoxon": "significantly different valence."
        },
        {
          "ratings\n(H(5) = 145.81, p < 0.001). Post-hoc Wilcoxon": ""
        },
        {
          "ratings\n(H(5) = 145.81, p < 0.001). Post-hoc Wilcoxon": "C. Comparison between Haptic-Only and Video with Haptic"
        },
        {
          "ratings\n(H(5) = 145.81, p < 0.001). Post-hoc Wilcoxon": ""
        },
        {
          "ratings\n(H(5) = 145.81, p < 0.001). Post-hoc Wilcoxon": "Feedback"
        },
        {
          "ratings\n(H(5) = 145.81, p < 0.001). Post-hoc Wilcoxon": ""
        },
        {
          "ratings\n(H(5) = 145.81, p < 0.001). Post-hoc Wilcoxon": "1) Arousal analysis: We analyzed arousal\nlevels\nin con-"
        },
        {
          "ratings\n(H(5) = 145.81, p < 0.001). Post-hoc Wilcoxon": "ditions\ninvolving\naction\nvideos with\nhaptic\nfeedback\nand"
        },
        {
          "ratings\n(H(5) = 145.81, p < 0.001). Post-hoc Wilcoxon": "haptic-only\nscenarios\n(served\nas\na\nno-situational\ncontext"
        },
        {
          "ratings\n(H(5) = 145.81, p < 0.001). Post-hoc Wilcoxon": "group). As\nshown\nin\nFig.\n3a,\nthe Kruskal-Wallis\ntest"
        },
        {
          "ratings\n(H(5) = 145.81, p < 0.001). Post-hoc Wilcoxon": "identified\na\nsignificant main\neffect\nof\nsituational\ncontext,"
        },
        {
          "ratings\n(H(5) = 145.81, p < 0.001). Post-hoc Wilcoxon": "which included six actions with haptic feedback and haptic-"
        },
        {
          "ratings\n(H(5) = 145.81, p < 0.001). Post-hoc Wilcoxon": "only conditions\n(where haptic-only conditions\nare\nconsid-"
        },
        {
          "ratings\n(H(5) = 145.81, p < 0.001). Post-hoc Wilcoxon": "ered\nas\nhaving\nno\nprior\nsituational\ncontext),\non\narousal"
        },
        {
          "ratings\n(H(5) = 145.81, p < 0.001). Post-hoc Wilcoxon": "=\n20.18, p\n=\nfor\nboth\n(H(6)\n0.003)\nand"
        },
        {
          "ratings\n(H(5) = 145.81, p < 0.001). Post-hoc Wilcoxon": "CTactileAnger"
        },
        {
          "ratings\n(H(5) = 145.81, p < 0.001). Post-hoc Wilcoxon": ""
        },
        {
          "ratings\n(H(5) = 145.81, p < 0.001). Post-hoc Wilcoxon": "CTactileComfort (H(6) = 13.89, p = 0.03)."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(a) Participants decoding emotions.": "Fig. 2: Vibration sleeves.",
          "(b) Distribution of vibration motors.": ""
        },
        {
          "(a) Participants decoding emotions.": "the robot experiencing an action from a human (e.g.,",
          "(b) Distribution of vibration motors.": "(p = 0.0450)\nconditions. Lastly,\nscenario was\nthe CSlap"
        },
        {
          "(a) Participants decoding emotions.": "being kissed or slapped) without receiving tactile feed-",
          "(b) Distribution of vibration motors.": "significantly different from the CStroke (p < 0.001). However,"
        },
        {
          "(a) Participants decoding emotions.": "back. Specifically,\nincluding CKiss, CSlap, CEye Contact,",
          "(b) Distribution of vibration motors.": "there\nis\nno\nsignificant\ndifference\nand\nbetween CCover eyes"
        },
        {
          "(a) Participants decoding emotions.": "CStroke, CFlick, CCover eyes.",
          "(b) Distribution of vibration motors.": "in addition,\nCFlick, as well as between CCover eyes and CKiss;"
        },
        {
          "(a) Participants decoding emotions.": "4) Combined Tactile and Context Evaluation: Partici-",
          "(b) Distribution of vibration motors.": "no significant difference was found between CFlick and CKiss,"
        },
        {
          "(a) Participants decoding emotions.": "pants rated arousal and valence while observing video",
          "(b) Distribution of vibration motors.": "and CEye Contact\nand CStroke on arousal, which indicate that"
        },
        {
          "(a) Participants decoding emotions.": "scenarios followed by vibration feedback sending from",
          "(b) Distribution of vibration motors.": "some\nsituational\ncontext\nhas\nsimilar\narousal\nlevel. These"
        },
        {
          "(a) Participants decoding emotions.": "the robot CContextAnger, CContextComfort ).",
          "(b) Distribution of vibration motors.": ""
        },
        {
          "(a) Participants decoding emotions.": "",
          "(b) Distribution of vibration motors.": "responses depending on the situational context."
        },
        {
          "(a) Participants decoding emotions.": "•\nParticipants\nobserve\nthe\nrobot\nexperiencing\nan",
          "(b) Distribution of vibration motors.": ""
        },
        {
          "(a) Participants decoding emotions.": "",
          "(b) Distribution of vibration motors.": "2) Valence analysis: The Kruskal-Wallis\ntest\nrevealed a"
        },
        {
          "(a) Participants decoding emotions.": "action\nand\nresponding with\nan\n“Anger”\nvibra-",
          "(b) Distribution of vibration motors.": ""
        },
        {
          "(a) Participants decoding emotions.": "",
          "(b) Distribution of vibration motors.": "significant main\neffect\nof\nsituational\ncontext\non\nvalence"
        },
        {
          "(a) Participants decoding emotions.": "tion CContextAnger , as well as a “Comfort” vibration",
          "(b) Distribution of vibration motors.": ""
        },
        {
          "(a) Participants decoding emotions.": "",
          "(b) Distribution of vibration motors.": "ratings\n(H(5) = 145.81, p < 0.001). Post-hoc Wilcoxon"
        },
        {
          "(a) Participants decoding emotions.": "CContextComfort , here the Context",
          "(b) Distribution of vibration motors.": ""
        },
        {
          "(a) Participants decoding emotions.": "",
          "(b) Distribution of vibration motors.": "signed-rank tests with Bonferroni correction further explored"
        },
        {
          "(a) Participants decoding emotions.": "Contact, Stroke, Flick, Cover eyes and Kiss.",
          "(b) Distribution of vibration motors.": ""
        },
        {
          "(a) Participants decoding emotions.": "",
          "(b) Distribution of vibration motors.": "these\ndifferences. Significant\ndifferences\nemerged\nnotably"
        },
        {
          "(a) Participants decoding emotions.": "III. RESULTS AND ANALYSIS",
          "(b) Distribution of vibration motors.": "conditions\n(p < 0.001),\nbetween the CCover eyes\nand CKiss"
        },
        {
          "(a) Participants decoding emotions.": "",
          "(b) Distribution of vibration motors.": "(p < 0.001),\nas well as between CCover eyes and CEye Contact"
        },
        {
          "(a) Participants decoding emotions.": "A. Tactile stimulus only",
          "(b) Distribution of vibration motors.": ""
        },
        {
          "(a) Participants decoding emotions.": "",
          "(b) Distribution of vibration motors.": "(p < 0.001). Additionally,\nthe\nand CCover eyes versus CStroke"
        },
        {
          "(a) Participants decoding emotions.": "A Wilcoxon rank-sum test indicated significant differences",
          "(b) Distribution of vibration motors.": ""
        },
        {
          "(a) Participants decoding emotions.": "",
          "(b) Distribution of vibration motors.": "(p < 0.001),\nCFlick scenario significantly differed from CKiss"
        },
        {
          "(a) Participants decoding emotions.": "(8.06 ± 1.50)\n(4.16 ±",
          "(b) Distribution of vibration motors.": ""
        },
        {
          "(a) Participants decoding emotions.": "between CTactileAnger\nand CTactileComfort",
          "(b) Distribution of vibration motors.": ""
        },
        {
          "(a) Participants decoding emotions.": "",
          "(b) Distribution of vibration motors.": "(p <\nCEye Contact\n(p < 0.001), CSlap (p < 0.001), and CStroke"
        },
        {
          "(a) Participants decoding emotions.": "1.71)\nhaptic\nstimuli\nfor\narousal\nratings,\n(W = 951, p <",
          "(b) Distribution of vibration motors.": ""
        },
        {
          "(a) Participants decoding emotions.": "",
          "(b) Distribution of vibration motors.": "0.001) conditions. Significant differences were also observed"
        },
        {
          "(a) Participants decoding emotions.": ".001). Similarly,\nvalence\nratings\nsignificantly\ndiffered\nbe-",
          "(b) Distribution of vibration motors.": ""
        },
        {
          "(a) Participants decoding emotions.": "",
          "(b) Distribution of vibration motors.": "between\nscenario\n(p < 0.001),\nthe CKiss\nand CEye Contact"
        },
        {
          "(a) Participants decoding emotions.": "tween CTactileAnger\n(2.06 ± 1.41) and CTactileComfort",
          "(b) Distribution of vibration motors.": ""
        },
        {
          "(a) Participants decoding emotions.": "",
          "(b) Distribution of vibration motors.": "(p < 0.001),\n(p = 0.0007)\nconditions.\nCSlap\nand CStroke"
        },
        {
          "(a) Participants decoding emotions.": "(W = 71, p < .001). The results are shown in Table. I. This",
          "(b) Distribution of vibration motors.": ""
        },
        {
          "(a) Participants decoding emotions.": "",
          "(b) Distribution of vibration motors.": "Lastly,\nscenario was\nsignificantly different\nfrom\nthe CSlap"
        },
        {
          "(a) Participants decoding emotions.": "confirmed\nthat\nparticipants\ncan\ndecode\nemotions\nthrough",
          "(b) Distribution of vibration motors.": ""
        },
        {
          "(a) Participants decoding emotions.": "",
          "(b) Distribution of vibration motors.": "(p < 0.001)\n(p < 0.001).\nboth CEye Contact\nand CStroke"
        },
        {
          "(a) Participants decoding emotions.": "vibration sent by Pepper\nrobots.",
          "(b) Distribution of vibration motors.": ""
        },
        {
          "(a) Participants decoding emotions.": "",
          "(b) Distribution of vibration motors.": "These\nresults\nindicate\na\nsignificant\ndifference\nin\npartici-"
        },
        {
          "(a) Participants decoding emotions.": "",
          "(b) Distribution of vibration motors.": "pants’ valence responses across all\nthe situational contexts,"
        },
        {
          "(a) Participants decoding emotions.": "B. Action videos",
          "(b) Distribution of vibration motors.": ""
        },
        {
          "(a) Participants decoding emotions.": "",
          "(b) Distribution of vibration motors.": "highlighting that\nthe proposed six situational contexts elicit"
        },
        {
          "(a) Participants decoding emotions.": "1) Arousal analysis: The results are shown in the Tab.\nI",
          "(b) Distribution of vibration motors.": ""
        },
        {
          "(a) Participants decoding emotions.": "",
          "(b) Distribution of vibration motors.": "significantly different valence."
        },
        {
          "(a) Participants decoding emotions.": "and Tab.\nII. The Kruskal-Wallis\ntest\nrevealed a\nsignificant",
          "(b) Distribution of vibration motors.": ""
        },
        {
          "(a) Participants decoding emotions.": "",
          "(b) Distribution of vibration motors.": "C. Comparison between Haptic-Only and Video with Haptic"
        },
        {
          "(a) Participants decoding emotions.": "=\nmain\neffect\nof\nsituational\ncontext\non\narousal\n(H(5)",
          "(b) Distribution of vibration motors.": ""
        },
        {
          "(a) Participants decoding emotions.": "",
          "(b) Distribution of vibration motors.": "Feedback"
        },
        {
          "(a) Participants decoding emotions.": "69.68, p < 0.001).\nPost-hoc Wilcoxon\nsigned-rank\ntests",
          "(b) Distribution of vibration motors.": ""
        },
        {
          "(a) Participants decoding emotions.": "with Bonferroni correction further explored these differences.",
          "(b) Distribution of vibration motors.": "1) Arousal analysis: We analyzed arousal\nlevels\nin con-"
        },
        {
          "(a) Participants decoding emotions.": "For arousal\nratings, significant differences emerged notably",
          "(b) Distribution of vibration motors.": "ditions\ninvolving\naction\nvideos with\nhaptic\nfeedback\nand"
        },
        {
          "(a) Participants decoding emotions.": "between\nconditions\n(p <\nthe CCover eyes\nand CEye Contact",
          "(b) Distribution of vibration motors.": "haptic-only\nscenarios\n(served\nas\na\nno-situational\ncontext"
        },
        {
          "(a) Participants decoding emotions.": "con-\n0.001), as well as between the CCover eyes\nand CStroke",
          "(b) Distribution of vibration motors.": "group). As\nshown\nin\nFig.\n3a,\nthe Kruskal-Wallis\ntest"
        },
        {
          "(a) Participants decoding emotions.": "ditions (p = 0.0015), and CCover eyes versus CSlap conditions",
          "(b) Distribution of vibration motors.": "identified\na\nsignificant main\neffect\nof\nsituational\ncontext,"
        },
        {
          "(a) Participants decoding emotions.": "(p = 0.0194). Additionally,\nscenario significantly\nthe CFlick",
          "(b) Distribution of vibration motors.": "which included six actions with haptic feedback and haptic-"
        },
        {
          "(a) Participants decoding emotions.": "(p < 0.001),\ndiffered from CEye Contact\n(p < 0.001), CSlap",
          "(b) Distribution of vibration motors.": "only conditions\n(where haptic-only conditions\nare\nconsid-"
        },
        {
          "(a) Participants decoding emotions.": "conditions\n(p = 0.0111).\nSignificant\ndiffer-\nand CStroke",
          "(b) Distribution of vibration motors.": "ered\nas\nhaving\nno\nprior\nsituational\ncontext),\non\narousal"
        },
        {
          "(a) Participants decoding emotions.": "ences were\nscenario and\nalso observed between the CKiss",
          "(b) Distribution of vibration motors.": "=\n20.18, p\n=\nfor\nboth\n(H(6)\n0.003)\nand"
        },
        {
          "(a) Participants decoding emotions.": "",
          "(b) Distribution of vibration motors.": "CTactileAnger"
        },
        {
          "(a) Participants decoding emotions.": "(p = 0.0067),",
          "(b) Distribution of vibration motors.": ""
        },
        {
          "(a) Participants decoding emotions.": "CEye Contact\n(p < 0.001), CSlap\nand CStroke",
          "(b) Distribution of vibration motors.": "CTactileComfort (H(6) = 13.89, p = 0.03)."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE I: Arousal scores for situational context affective tactile interaction.": "Eye Contact"
        },
        {
          "TABLE I: Arousal scores for situational context affective tactile interaction.": "3.25 ± 1.81"
        },
        {
          "TABLE I: Arousal scores for situational context affective tactile interaction.": "7.09 ± 1.99"
        },
        {
          "TABLE I: Arousal scores for situational context affective tactile interaction.": "4.16 ± 1.71"
        },
        {
          "TABLE I: Arousal scores for situational context affective tactile interaction.": "TABLE II: Valence scores for situational context affective tactile interaction."
        },
        {
          "TABLE I: Arousal scores for situational context affective tactile interaction.": "Eye Contact"
        },
        {
          "TABLE I: Arousal scores for situational context affective tactile interaction.": "5.63 ± 1.29"
        },
        {
          "TABLE I: Arousal scores for situational context affective tactile interaction.": "5.28 ± 2.30"
        },
        {
          "TABLE I: Arousal scores for situational context affective tactile interaction.": "5.16 ± 1.35"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "",
          "(p < 0.001),": "observed, one for CKissComfort"
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "only condition,\nas\nillustrated in Fig. 4b. Additionally,\nthe",
          "(p < 0.001),": "and another\n(p = 0.007),"
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "",
          "(p < 0.001),": "for CSlapComfort"
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "variation in arousal ratings across the six situational contexts",
          "(p < 0.001),": "which indicates that situational context overrides the valence"
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "is\nreduced when haptic feedback is\nintroduced, converging",
          "(p < 0.001),": "interpretation derived from tactile feedback."
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "toward the arousal\nlevel of the haptic stimulus. Furthermore,",
          "(p < 0.001),": "As\nshown\nin Fig.\n3b\nand Fig.\n4b,\nthe\nsituational\ncon-"
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "across all situational contexts with haptic feedback, a signif-",
          "(p < 0.001),": "text significantly overrides the valence interpretation derived"
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "icant difference was\nfound between actions\nfeaturing anger",
          "(p < 0.001),": "from the\ntactile\nfeedback.\nIn\naddition, we\nobserved\nthat"
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "tactile feedback and those featuring comfort\ntactile feedback",
          "(p < 0.001),": "negative haptic stimuli\n(anger)\ntended to amplify emotions"
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "(p < 0.05),\nsuggesting that participants\ncould distinguish",
          "(p < 0.001),": "by reinforcing or mirroring the\nemotional\nintensity of\nthe"
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "between different arousal levels of haptic feedback in various",
          "(p < 0.001),": "action, whereas positive haptic stimuli (comfort) appeared to"
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "action scenarios.",
          "(p < 0.001),": "modulate emotions, reducing the valence intensity of the ex-"
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "The\nresults,\nsummarized in Tab.\nI,\nfurther\nindicate\nthat",
          "(p < 0.001),": "perience, and tended to converge around the neutral valence"
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "tactile stimuli can override the arousal\ninterpretation shaped",
          "(p < 0.001),": "rating. To examine how tactile feedback influences emotional"
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "by situational context,\nin addition, haptic feedback reduces",
          "(p < 0.001),": "responses, we compared situational context with haptic stim-"
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "variability in arousal ratings across different situational con-",
          "(p < 0.001),": "ulus valence deviation from the neutral valence\nand used"
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "texts, as shown in Fig. 3a.",
          "(p < 0.001),": "the original situational context valence as a reference. Then,"
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "2) Valence analysis: For valence,\nthe results can be seen",
          "(p < 0.001),": "we used a linear mixed-effects model. The model\nincluded"
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "in Fig. 3b and Table.\nII. The Kruskal-Wallis\ntest\nidentified",
          "(p < 0.001),": "Emotion Category\nas\nreference"
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "",
          "(p < 0.001),": "level, CContextComfort"
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "a significant main effect of\nsituational context on their va-",
          "(p < 0.001),": "and CContextAnger ) as a fixed effect and Participant ID (PN) as"
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "lence level, which included six actions with haptic feedback",
          "(p < 0.001),": "a random intercept\nto account for inter-individual variability:"
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "and\nhaptic-only\nconditions\n(where\nhaptic-only\nconditions",
          "(p < 0.001),": ""
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "",
          "(p < 0.001),": "Valence deviation ∼ Valence Category + (1 | PN)\n(1)"
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "are\nconsidered as having no prior\nsituational\ncontext), on",
          "(p < 0.001),": ""
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "",
          "(p < 0.001),": "Emotion deviation was defined as\nthe absolute deviation"
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "(H(6) = 142.97, p < 0.001) and",
          "(p < 0.001),": ""
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "valence for both CTactileAnger",
          "(p < 0.001),": ""
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "",
          "(p < 0.001),": "from neutral\nvalence\n(5),\nsuch\nthat\nhigher\nvalues\nreflect"
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "(H(6) = 106.90, p < 0.001). Post-hoc Wilcoxon",
          "(p < 0.001),": ""
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "CTactileComfort",
          "(p < 0.001),": ""
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "",
          "(p < 0.001),": "stronger emotional\nreactions regardless of direction."
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "signed-rank tests with Bonferroni correction revealed signif-",
          "(p < 0.001),": ""
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "icant differences\nand",
          "(p < 0.001),": "Compared to the CContext, CContextComfort significantly reduced"
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "in valence levels between CTactileComfort",
          "(p < 0.001),": ""
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "",
          "(p < 0.001),": "t(542) =\nvalence\ndeviation\n(β = −0.41, SE = 0.12,"
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "action videos with CTactileComfort ,\nspecifically CFlickComfort",
          "(p < 0.001),": ""
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "",
          "(p < 0.001),": "−3.26, p = .001),\nindicating that positive\ntactile\nstimula-"
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "(p = 0.03), CKissComfort\n(p = 0.03), and CSlapComfort",
          "(p < 0.001),": ""
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "Additionally,\nall\nsix\naction\npairwise\ncomparisons\nshowed",
          "(p < 0.001),": "tion moderated affective responses and brought\nthem closer"
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "",
          "(p < 0.001),": "to neutrality.\nsignificantly increased"
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "significant\ndifferences\n(p < 0.05), with\nthe\nexception\nof",
          "(p < 0.001),": "In contrast, CContextAnger"
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "",
          "(p < 0.001),": "valence deviation (β = 0.58, SE = 0.12,\nt(542) = 4.68,"
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "CStrokeComfort\nand CKissComfort , where",
          "(p < 0.001),": ""
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "was observed. Similarly,\nsignificant differences were found",
          "(p < 0.001),": "p < .001), suggesting that negative tactile stimulus amplified"
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "",
          "(p < 0.001),": "situational context valence."
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "between CTactileAnger\nthree CTactileAnger,",
          "(p < 0.001),": ""
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "(p < 0.001),\neye\ncontact\n(p < 0.001),\nand\nstroke\n(p <",
          "(p < 0.001),": "The regulatory and amplifying effects of tactile stimulation"
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "0.001). This indicates that\ntactile stimulus valence does not",
          "(p < 0.001),": "on\nvalence may\nbe\nunderstood\nthrough Gross’s\nemotion"
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "override the situational context valence.",
          "(p < 0.001),": "regulation\nframework\n[29], which\nemphasizes\nthe\nrole\nof"
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "",
          "(p < 0.001),": "cognitive\nstrategies\nin modulating\nemotional\nexperiences."
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "D. Comparison between Video-Only and Video with Haptic",
          "(p < 0.001),": ""
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "",
          "(p < 0.001),": "Positive touch likely engages cognitive reappraisal processes,"
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "Feedback",
          "(p < 0.001),": ""
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "",
          "(p < 0.001),": "enabling individuals to reinterpret emotional stimuli\nin a less"
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "1) Arousal analysis: For arousal, we found a significant",
          "(p < 0.001),": "extreme manner. Conversely, negative\ntouch may enhance"
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "main\neffect\nof\ndifferent\nhaptic\narousal\nlevels\n(including",
          "(p < 0.001),": "emotional valence through increased emotional perception."
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "conditions without\nhaptic\nstimuli)\nacross\ndifferent\nactions",
          "(p < 0.001),": "This may involve\nemotion contagion—where\ntactile\ninput"
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "(p < 0.001). Additionally,\nthere was a significant difference",
          "(p < 0.001),": "intensifies affective resonance."
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "(p < 0.001).",
          "(p < 0.001),": ""
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "six CContextComfort",
          "(p < 0.001),": ""
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "",
          "(p < 0.001),": "IV. DISCUSSION"
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "For pairwise comparison, we found that arousal ratings were",
          "(p < 0.001),": ""
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "significantly higher",
          "(p < 0.001),": "This\nstudy explored the interaction between tactile feed-"
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "in all CContextAnger",
          "(p < 0.001),": ""
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "significant differences\n(p < 0.001). However, CContextComfort ,",
          "(p < 0.001),": "back and situational context\nin shaping emotional\ninterpreta-"
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "were observed only in CStrokeComfort\n(p = 0.03) and CSlapComfort",
          "(p < 0.001),": "tions during human-robot interaction. Firstly, we can confirm"
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "(p < 0.001) when compared to their\nrespective video-only",
          "(p < 0.001),": "that participants could differentiate the arousal and valence"
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "conditions.",
          "(p < 0.001),": "from the\nsolely haptic\nstimulus\nsent\nfrom the\nrobot by a"
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "2) Valence\nanalysis:\nFor\nvalence,\nno\nsignificant\ndiffer-",
          "(p < 0.001),": "mediated vibration sleeve."
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "ences were\nfound\nbetween\nsituational\ncontext with\nanger",
          "(p < 0.001),": "For arousal,\nthe findings demonstrate that\ntactile feedback"
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "haptic\nstimuli\nand\ntheir\ncorresponding\nsituational\ncontext",
          "(p < 0.001),": "can override situational context arousal\nratings.\nIn addition,"
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "video-only conditions (p > 0.05).\nIn addition, no significant",
          "(p < 0.001),": "Tactile\nstimulus decreased the variance\namong all\nthe\nsix"
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "differences were\nfound\nbetween most\nsituational\ncontext",
          "(p < 0.001),": "situational contexts. One possible explanation is participants"
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "with comfort haptic stimuli and their corresponding video-",
          "(p < 0.001),": "could feel\nthe vibration amplitude from the haptic stimulus,"
        },
        {
          "feedback is notably lower\nthan in the\nsituational\ncontext-": "only conditions,\nthere are only two significant effects were",
          "(p < 0.001),": "which is difficult\nto be influenced by the situational context."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "future directions,” IEEE Access, vol. 4, pp. 26–40, 2015."
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "valence\ninterpretation under\ncontexts with haptic\nstimulus,",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": ""
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "[7]\nJ. B. Van Erp and A. Toet, “Social\ntouch in human–computer interac-"
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "where contextual cues dominate the perception of emotion",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": ""
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "tion,” Frontiers in digital humanities, vol. 2, p. 2, 2015."
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "valence,\nleaving\nless\nroom for\nhaptic\nfeedback\nto\nalter",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "[8]\nJ. T. Suvilehto, A. Cekaite, and I. Morrison, “The why, who and how"
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "of social\ntouch,” Nature Reviews Psychology, vol. 2, no. 10, pp. 606–"
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "valence ratings.\nIn addition, negative haptic stimuli\n(anger)",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": ""
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "621, 2023."
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "amplify\nemotional\nresponses,\nreinforcing\nthe\nintensity\nof",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": ""
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "[9] A. Gallace and C. Spence, In touch with the future: The sense of touch"
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "both positive and negative emotions, whereas positive haptic",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "from cognitive neuroscience to virtual reality. OUP Oxford, 2014."
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "[10] B. App, D. N. McIntosh, C. L. Reed, and M. J. Hertenstein, “Nonver-"
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "stimuli (comfort) moderate valence, shifting emotions toward",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": ""
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "bal channel use in communication of emotion: how may depend on"
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "a more neutral\nstate, which could also be because of\nthe",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": ""
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "why.,” Emotion, vol. 11, no. 3, p. 603, 2011."
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "high arousal state has interplay with the interpretation of the",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "[11]\nS. Yohanan and K. E. MacLean, “The role of affective touch in human-"
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "robot interaction: Human intent and expectations in touching the haptic"
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "valence. These findings\nsuggest\nthat\ntactile\nfeedback does",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": ""
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "creature,” International Journal of Social Robotics, vol. 4, pp. 163–"
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "not\nfunction in isolation but\nrather\ninteracts with broader",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": ""
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "180, 2012."
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "contextual cues to shape emotional\nresponses.",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "[12] K. Wada and T. Shibata, “Living with seal\nrobots—its\nsociopsycho-"
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "logical and physiological\ninfluences on the elderly at a care house,”"
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "In\naddition,\nvisual\ncontext\noverwhelmingly\ndominated",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": ""
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "IEEE transactions on robotics, vol. 23, no. 5, pp. 972–980, 2007."
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "emotional\ninterpretations in scenarios with strongly negative",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": ""
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "[13]\nP. B. Shull\nand D. D. Damian,\n“Haptic wearables\nas\nsensory\nre-"
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "connotations,\nsuch\nas\n“slap,”\nleaving minimal\nspace\nfor",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "Journal of\nplacement,\nsensory augmentation and trainer–a\nreview,”"
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "neuroengineering and rehabilitation, vol. 12, pp. 1–13, 2015."
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "tactile feedback to alter perceptions\nsignificantly. However,",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": ""
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "[14] Q. Ren\nand T. Belpaeme,\n“Tactile\ninteraction with\nsocial\nrobots"
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "in scenarios with positive or\nambiguous\nemotional\nconno-",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": ""
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "International\nJournal of Social\ninfluences\nattitudes\nand behaviour,”"
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "tations\nlike “kiss”, “stroke”, or “eye contact”,\ntactile stim-",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "Robotics, vol. 16, no. 11, pp. 2297–2317, 2024."
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "[15] Q. Ren, Y. Hou, D. Botteldooren,\nand T. Belpaeme,\n“Behavioural"
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "uli markedly influenced participants’ emotional\njudgments,",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": ""
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "models of\nrisk-taking in human–robot\ntactile\ninteractions,” Sensors,"
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "highlighting\nthe\nimportance\nof multimodal\ncongruency\nin",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": ""
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "vol. 23, no. 10, p. 4786, 2023."
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "shaping emotional perceptions.\nIn addition, participants de-",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "[16] Akshita, H. Alagarai Sampath, B.\nIndurkhya, E. Lee,\nand Y. Bae,"
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "“Towards multimodal affective feedback:\nInteraction between visual"
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "code “eye contact” with the robot as positive valence, which",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": ""
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "of\nthe\n33rd Annual ACM\nand\nhaptic modalities,”\nin Proceedings"
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "might caused by the appearance of\nthe pepper\nrobot or\nthe",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": ""
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "Conference on Human Factors in Computing Systems, pp. 2043–2052,"
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "eye interaction that could raise the positive emotion.",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "2015."
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "[17]\nJ. Smith and K. MacLean, “Communicating emotion through a hap-"
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "While\nthe\nresults underscore\nthe\nsignificance of\ntactile-",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": ""
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "International\nJournal\nof\ntic\nlink: Design\nspace\nand methodology,”"
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "context congruency,\nsome methodological\nlimitations exist.",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": ""
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "Human-Computer Studies, vol. 65, no. 4, pp. 376–387, 2007."
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "The study was conducted in a controlled laboratory setting",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "[18]\nJ. Rantala, K. Salminen, R. Raisamo, and V. Surakka, “Touch gestures"
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "in communicating emotional\nintention via vibrotactile\nstimulation,”"
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "with video stimuli, potentially limiting ecological validity.",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": ""
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "International\nJournal\nof Human-Computer\nStudies,\nvol.\n71,\nno.\n6,"
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "Additionally,\nthe\nhomogeneous\nnature\nof\nthe\nparticipant",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": ""
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "pp. 679–690, 2013."
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "group\nrestricts\ngeneralizability\nacross\ndiverse\npopulations.",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "[19] Y.\nJu, D. Zheng, D. Hynds, G. Chernyshov, K. Kunze, and K. Mi-"
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "namizawa, “Haptic empathy: Conveying emotional meaning through"
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "Future\nresearch\nshould\nexamine\nsimilar multimodal\nemo-",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": ""
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "the 2021 CHI Con-\nvibrotactile feedback,” in Extended Abstracts of"
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "tional\ninteractions in real-world settings and involve cultur-",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": ""
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "ference on Human Factors in Computing Systems, pp. 1–7, 2021."
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "ally diverse participant groups.",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "[20] Q. Ren and T. Belpaeme, “Touched by chatgpt: Using an llm to drive"
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "affective tactile interaction,” arXiv preprint arXiv:2501.07224, 2025."
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "V. CONCLUSION",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "[21] K. Hoemann, M. Gendron, and L. F. Barrett, “Mixed emotions in the"
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "predictive brain,” Current Opinion in Behavioral Sciences, vol. 15,"
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "This\nstudy explored the interaction between tactile feed-",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": ""
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "pp. 51–57, 2017."
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "back\nand\nsituational\ncontext\nin\nshaping\nemotional\ninter-",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "[22] A. Gallace\nand C. Spence,\n“The\nscience of\ninterpersonal\ntouch:\nan"
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "overview,” Neuroscience & Biobehavioral Reviews,\nvol.\n34,\nno.\n2,"
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "pretations during human-robot\ninteraction. The results indi-",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": ""
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "pp. 246–259, 2010."
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "cate that situational context significantly dominates valence",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": ""
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "inter-\n[23]\nJ. Huwer, Understanding handshaking: the result of contextual,"
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "perception, with negative haptic stimuli\n(anger) amplifying",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "personal and social demands. PhD thesis, 2003."
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "[24]\nS. E. Jones and A. E. Yarbrough, “A naturalistic study of the meanings"
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "overall valence\nlevel\nand positive haptic\nstimuli\n(comfort)",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": ""
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "of\ntouch,” Communications Monographs, vol. 52, no. 1, pp. 19–56,"
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "moderating valence toward neutrality. Arousal was primarily",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": ""
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "1985."
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "driven\nby\nhaptic\nfeedback. These findings\nunderscore\nthe",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "[25]\nJ. Urakami and K. Seaborn, “Nonverbal cues\nin human–robot\ninter-"
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "action: A communication studies perspective,” ACM Transactions on"
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "context-dependent nature of haptic feedback, reinforcing the",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": ""
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "Human-Robot\nInteraction, vol. 12, no. 2, pp. 1–21, 2023."
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "need for adaptive, context-aware haptic systems in affective",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": ""
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "[26] M. Tielman, M. Neerincx,\nJ.-J. Meyer,\nand R. Looije,\n“Adaptive"
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "computing and human-robot\ninteraction.",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "the\nemotional expression in robot-child interaction,” in Proceedings of"
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "2014 ACM/IEEE international\nconference on Human-robot\ninterac-"
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "REFERENCES",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "tion, pp. 407–414, 2014."
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "[27] Q. Ren, R. Proesmans, F. Bossuyt,\nJ. Vanfleteren, F. Wyffels,\nand"
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "[1]\nP. Dourish, Where the action is: the foundations of embodied interac-",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": ""
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "T. Belpaeme,\n“Conveying\nemotions\nto\nrobots\nthrough\ntouch\nand"
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "tion. MIT press, 2001.",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": ""
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "sound,” arXiv preprint arXiv:2412.03300, 2024."
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "[2] K. Battarbee, Co-experience: understanding user experiences in inter-",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": ""
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "[28] R. Read and T. Belpaeme,\n“Situational\ncontext directs how people"
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "action. Aalto University, 2004.",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": ""
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "affectively interpret\nrobotic non-linguistic utterances,” in Proceedings"
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "[3]\nP. Ekman, T. Dalgleish, and M. Power, “Basic emotions,” San Fran-",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": ""
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "of\nthe\n2014 ACM/IEEE international\nconference\non Human-robot"
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "cisco, USA, 1999.",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": ""
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "interaction, pp. 41–48, 2014."
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "[4]\nJ. A. Russell, “A circumplex model of affect.,” Journal of personality",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": ""
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "[29]\nJ. J. Gross, “The emerging field of emotion regulation: An integrative"
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "and social psychology, vol. 39, no. 6, p. 1161, 1980.",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": ""
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "review,” Review of general psychology, vol. 2, no. 3, pp. 271–299,"
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "[5] N. Ahmadpour, D. Lottridge,\nJ. Fritsch, C. Sas, M. E. Cecchinato,",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": ""
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": "1998."
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "D. Harrison, K. H¨o¨ok, P. FOONG, K. Ijaz, P. Gough, et al., “Affective",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": ""
        },
        {
          "In\naddition,\nsituational\ncontext\nplays\na\nrole\nin\nshaping": "interaction and affective computing-past, present and future,” 2025.",
          "[6] M. A. Eid and H. Al Osman, “Affective haptics: Current research and": ""
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Where the action is: the foundations of embodied interaction",
      "authors": [
        "P Dourish"
      ],
      "year": "2001",
      "venue": "Where the action is: the foundations of embodied interaction"
    },
    {
      "citation_id": "2",
      "title": "Co-experience: understanding user experiences in interaction",
      "authors": [
        "K Battarbee"
      ],
      "year": "2004",
      "venue": "Co-experience: understanding user experiences in interaction"
    },
    {
      "citation_id": "3",
      "title": "Basic emotions",
      "authors": [
        "P Ekman",
        "T Dalgleish",
        "M Power"
      ],
      "year": "1999",
      "venue": "Basic emotions"
    },
    {
      "citation_id": "4",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "5",
      "title": "Affective interaction and affective computing-past, present and future",
      "authors": [
        "N Ahmadpour",
        "D Lottridge",
        "J Fritsch",
        "C Sas",
        "M Cecchinato",
        "D Harrison",
        "K Höök",
        "P Foong",
        "K Ijaz",
        "P Gough"
      ],
      "year": "2025",
      "venue": "Affective interaction and affective computing-past, present and future"
    },
    {
      "citation_id": "6",
      "title": "Affective haptics: Current research and future directions",
      "authors": [
        "M Eid",
        "H Osman"
      ],
      "year": "2015",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "7",
      "title": "Social touch in human-computer interaction",
      "authors": [
        "J Van Erp",
        "A Toet"
      ],
      "year": "2015",
      "venue": "Frontiers in digital humanities"
    },
    {
      "citation_id": "8",
      "title": "The why, who and how of social touch",
      "authors": [
        "J Suvilehto",
        "A Cekaite",
        "I Morrison"
      ],
      "year": "2023",
      "venue": "Nature Reviews Psychology"
    },
    {
      "citation_id": "9",
      "title": "In touch with the future: The sense of touch from cognitive neuroscience to virtual reality",
      "authors": [
        "A Gallace",
        "C Spence"
      ],
      "year": "2014",
      "venue": "In touch with the future: The sense of touch from cognitive neuroscience to virtual reality"
    },
    {
      "citation_id": "10",
      "title": "Nonverbal channel use in communication of emotion: how may depend on why",
      "authors": [
        "B App",
        "D Mcintosh",
        "C Reed",
        "M Hertenstein"
      ],
      "year": "2011",
      "venue": "Emotion"
    },
    {
      "citation_id": "11",
      "title": "The role of affective touch in humanrobot interaction: Human intent and expectations in touching the haptic creature",
      "authors": [
        "S Yohanan",
        "K Maclean"
      ],
      "year": "2012",
      "venue": "International Journal of Social Robotics"
    },
    {
      "citation_id": "12",
      "title": "Living with seal robots-its sociopsychological and physiological influences on the elderly at a care house",
      "authors": [
        "K Wada",
        "T Shibata"
      ],
      "year": "2007",
      "venue": "IEEE transactions on robotics"
    },
    {
      "citation_id": "13",
      "title": "Haptic wearables as sensory replacement, sensory augmentation and trainer-a review",
      "authors": [
        "P Shull",
        "D Damian"
      ],
      "year": "2015",
      "venue": "Journal of neuroengineering and rehabilitation"
    },
    {
      "citation_id": "14",
      "title": "Tactile interaction with social robots influences attitudes and behaviour",
      "authors": [
        "Q Ren",
        "T Belpaeme"
      ],
      "year": "2024",
      "venue": "International Journal of Social Robotics"
    },
    {
      "citation_id": "15",
      "title": "Behavioural models of risk-taking in human-robot tactile interactions",
      "authors": [
        "Q Ren",
        "Y Hou",
        "D Botteldooren",
        "T Belpaeme"
      ],
      "year": "2023",
      "venue": "Sensors"
    },
    {
      "citation_id": "16",
      "title": "Towards multimodal affective feedback: Interaction between visual and haptic modalities",
      "authors": [
        "H Akshita",
        "B Sampath",
        "E Indurkhya",
        "Y Lee",
        "Bae"
      ],
      "year": "2015",
      "venue": "Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems"
    },
    {
      "citation_id": "17",
      "title": "Communicating emotion through a haptic link: Design space and methodology",
      "authors": [
        "J Smith",
        "K Maclean"
      ],
      "year": "2007",
      "venue": "International Journal of Human-Computer Studies"
    },
    {
      "citation_id": "18",
      "title": "Touch gestures in communicating emotional intention via vibrotactile stimulation",
      "authors": [
        "J Rantala",
        "K Salminen",
        "R Raisamo",
        "V Surakka"
      ],
      "year": "2013",
      "venue": "International Journal of Human-Computer Studies"
    },
    {
      "citation_id": "19",
      "title": "Haptic empathy: Conveying emotional meaning through vibrotactile feedback",
      "authors": [
        "Y Ju",
        "D Zheng",
        "D Hynds",
        "G Chernyshov",
        "K Kunze",
        "K Minamizawa"
      ],
      "year": "2021",
      "venue": "Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems"
    },
    {
      "citation_id": "20",
      "title": "Touched by chatgpt: Using an llm to drive affective tactile interaction",
      "authors": [
        "Q Ren",
        "T Belpaeme"
      ],
      "year": "2025",
      "venue": "Touched by chatgpt: Using an llm to drive affective tactile interaction",
      "arxiv": "arXiv:2501.07224"
    },
    {
      "citation_id": "21",
      "title": "Mixed emotions in the predictive brain",
      "authors": [
        "K Hoemann",
        "M Gendron",
        "L Barrett"
      ],
      "year": "2017",
      "venue": "Current Opinion in Behavioral Sciences"
    },
    {
      "citation_id": "22",
      "title": "The science of interpersonal touch: an overview",
      "authors": [
        "A Gallace",
        "C Spence"
      ],
      "year": "2010",
      "venue": "Neuroscience & Biobehavioral Reviews"
    },
    {
      "citation_id": "23",
      "title": "Understanding handshaking: the result of contextual, interpersonal and social demands",
      "authors": [
        "J Huwer"
      ],
      "year": "2003",
      "venue": "Understanding handshaking: the result of contextual, interpersonal and social demands"
    },
    {
      "citation_id": "24",
      "title": "A naturalistic study of the meanings of touch",
      "authors": [
        "S Jones",
        "A Yarbrough"
      ],
      "year": "1985",
      "venue": "Communications Monographs"
    },
    {
      "citation_id": "25",
      "title": "Nonverbal cues in human-robot interaction: A communication studies perspective",
      "authors": [
        "J Urakami",
        "K Seaborn"
      ],
      "year": "2023",
      "venue": "ACM Transactions on Human-Robot Interaction"
    },
    {
      "citation_id": "26",
      "title": "Adaptive emotional expression in robot-child interaction",
      "authors": [
        "M Tielman",
        "M Neerincx",
        "J.-J Meyer",
        "R Looije"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 ACM/IEEE international conference on Human-robot interaction"
    },
    {
      "citation_id": "27",
      "title": "Conveying emotions to robots through touch and sound",
      "authors": [
        "Q Ren",
        "R Proesmans",
        "F Bossuyt",
        "J Vanfleteren",
        "F Wyffels",
        "T Belpaeme"
      ],
      "year": "2024",
      "venue": "Conveying emotions to robots through touch and sound",
      "arxiv": "arXiv:2412.03300"
    },
    {
      "citation_id": "28",
      "title": "Situational context directs how people affectively interpret robotic non-linguistic utterances",
      "authors": [
        "R Read",
        "T Belpaeme"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 ACM/IEEE international conference on Human-robot interaction"
    },
    {
      "citation_id": "29",
      "title": "The emerging field of emotion regulation: An integrative review",
      "authors": [
        "J Gross"
      ],
      "year": "1998",
      "venue": "Review of general psychology"
    }
  ]
}