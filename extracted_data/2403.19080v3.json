{
  "paper_id": "2403.19080v3",
  "title": "Mmcert: Provable Defense Against Adversarial Attacks To Multi-Modal Models",
  "published": "2024-03-28T01:05:06Z",
  "authors": [
    "Yanting Wang",
    "Hongye Fu",
    "Wei Zou",
    "Jinyuan Jia"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Different from a unimodal model whose input is from a single modality, the input (called multi-modal input) of a multi-modal model is from multiple modalities such as image, 3D points, audio, text, etc. Similar to unimodal models, many existing studies show that a multi-modal model is also vulnerable to adversarial perturbation, where an attacker could add small perturbation to all modalities of a multi-modal input such that the multi-modal model makes incorrect predictions for it. Existing certified defenses are mostly designed for unimodal models, which achieve suboptimal certified robustness guarantees when extended to multi-modal models as shown in our experimental results. In our work, we propose MMCert, the first certified defense against adversarial attacks to a multi-modal model. We derive a lower bound on the performance of our MMCert under arbitrary adversarial attacks with bounded perturbations to both modalities (e.g., in the context of auto-driving, we bound the number of changed pixels in both RGB image and depth image). We evaluate our MMCert using two benchmark datasets: one for the multi-modal road segmentation task and the other for the multi-modal emotion recognition task. Moreover, we compare our MMCert with a state-of-the-art certified defense extended from unimodal models. Our experimental results show that our MMCert outperforms the baseline.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "With the rapid advancement of machine learning, multimodal models have emerged as a powerful paradigm. Differing from their unimodal counterpart whose input is from a singular modality, these multi-modal models leverage input (called multi-modal input) from diverse modalities such as images, 3D data points, audio, and text  [8, 11, 24, 39, 50] . Those multi-modal models have been widely used in many security and safety critical applications such as autonomous driving  [11, 30, 35, 44, 48]  and medical imaging  [15] . As shown in many existing studies  [13, 37, 42] , unimodal models are susceptible to adversarial attacks. There is no exception for multi-modal models. In particular, many recent studies  [6, 41, 43, 45, 54, 61]  showed that multimodal models are also vulnerable to adversarial perturbations. In particular, an attacker could simultaneously manipulate all modalities of a multi-modal input such that a multi-modal model makes incorrect predictions. For instance, in the scenario of road segmentation for autodriving, the attacker can add small perturbations to both the RGB image (captured by a camera) and the depth image (captured by a LiDAR depth sensor) to degrade the segmentation quality. Similarly, in the scenario of video emotion recognition, the attacker can apply subtle disruptions to both visual and audio data to reduce prediction accuracy.\n\nMany defenses were proposed to defend against adversarial attacks, In particular, they can be categorized into empirical defenses  [25, 34, 40, 45, 49, 52, 56]  and certified defenses  [7, 10, 14, 19, 27, 28, 51, 53, 57, 60] . Many existing studies  [4, 5, 46]  showed that most empirical defenses could be broken by strong, adaptive attacks (one exception is adversarial training  [34] ). Therefore, we focus on certified defense in this work. Existing certified defenses are mainly designed for unimodal models (its input is from a single modality). Our experimental results show that they achieve sub-optimal performance when extended to defend against adversarial attacks for multi-modal models. The key reason is that when the attacker adds l p bounded perturbations to all modalities, the space of perturbed multi-modal inputs cannot be simply formulated as a l p ball. In this work, we focus on l 0 -like adversarial attacks applied to each modality (i.e., manipulate a certain number of features for each modality) due to their straightforward applicability across various modalities. The investigation of alternative forms of attacks is reserved for future research.\n\nOur work. We propose MMCert, the first certified defense against adversarial attacks to multi-modal models. Suppose we have a multi-modal input M = (m 1 , m 2 , • • • , m T ) with T modalities, where m i contains a set/sequence of basic elements from the i-th modality. We consider a general scenario, where each element could be arbitrary. For in-stance, each element could be a pixel value, a 3D point, an image frame, an audio frame, etc.. Given a multi-modal input M and a multi-modal model g (called base multi-modal model), we first create multiple sub-sampled multi-modal inputs. In particular, each sub-sampled multi-modal input is obtained by randomly sub-sampling k 1 , k 2 , • • • , k T basic elements from m 1 , m 2 , • • • , m T , respectively. Then, we use the base multi-modal model g to make a prediction for each sub-sampled multi-modal input. Finally, we build an ensemble multi-modal model by aggregating those predictions as the final prediction made by our ensemble multimodal classifier for the given multi-modal input M.\n\nWe derive the provable robustness guarantee of our ensemble multi-modal model. In particular, we show that our ensemble multi-modal model provably makes the same prediction for a multi-modal input when the number of added (or deleted or modified) basic elements to m 1 , m 2 , • • • , m T is no larger than r 1 , r 2 , • • • , r T . Intuitively, there is a considerable overlap between the space of randomly sub-sampled multi-modal inputs before the attack and those sub-sampled after the attack. This suggests that the alterations in the output prediction probabilities are constrained. Following  [10, 20] , the robustness guarantee is achieved by utilizing Neyman-Pearson Lemma  [36] .\n\nWe conduct a systematic evaluation for our MMCert on two benchmark datasets for multi-modal road segmentation and multi-modal emotion recognition tasks, respectively. We measure the performance lower bounds of our defense under adversarial attacks, with the constraint that the number of modified (or deleted or added) basic elements to each modality is bounded. We compare our MMCert with randomized ablation  [28] , which is a state-of-the-art certified defense for unimodal models. Our experimental results show that our MMCert significantly outperforms randomized ablation when extending it to multi-modal models.\n\nIn summary, we make the following major contributions:\n\n• We propose MMCert, the first certified defense against adversarial attacks to multi-modal models.\n\n• We derive the provable robustness guarantees of our MMCert.\n\n• We conduct a systematic evaluation for our MMCert and compare it with state-of-the-art certified defense for unimodal models.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Background And Related Work",
      "text": "Multi-modal models  [8, 11, 24, 39]  are designed to process information across multiple types of data, such as text, images, 3D point clouds, and audio, simultaneously. Multimodal models have shown impressive results across a variety of applications, such as scene understanding  [24] , object detection  [16, 39, 47] , sentiment analysis  [8, 26, 58, 59] ,\n\nvisual question answering  [3, 18] , and semantic segmentation  [11, 31] .\n\nFor simplicity, we use M = (m 1 , m 2 , • • • , m T ) to denote a multi-modal input with T modalities, where m i represents the group of basic elements (pixels, images, audio) from the ith (i = 1, 2, • • • , T ) modality.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Adversarial Attacks To Multi-Modal Models",
      "text": "Many existing studies  [6, 41, 43, 45, 54, 61]  showed that multi-modal models are vulnerable to adversarial attacks  [13] . For instance, Cheng et al.  [6]  showed that the multi-modal auto-driving system can be undermined by a single-modal attack that only aims at the camera modality, which is considered less expensive to compromise. Those attacks cause severe security and safety concerns for the deployment of multi-modal models in various real-world applications such as autonomous driving  [11, 30, 35, 44, 48] . In our work, we consider a general attack, where an attacker could arbitrarily add (or delete or modify) a certain number of basic elements to each modality. For instance, when each basic element of a modality represents a pixel, an attacker could arbitrarily manipulate (e.g., modify) some pixel values for that modality.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Existing Defenses",
      "text": "Defenses against adversarial attacks can be categorized into empirical defenses and certified defenses. Empirical defenses  [25, 34, 40, 45, 49, 52, 56]  cannot provide formal robustness guarantees under arbitrary attacks. Multiple works  [4, 5, 46]  have shown that they can be bypassed by more advanced attacks. Existing certified defenses  [7, 10, 19, 22, 27, 28, 32, 38, 53, 55, 57, 60, 62]  against adversarial attacks all focus on unimodal model whose input is only from a single modality. Among those defenses, randomized ablation  [21, 28]  achieves state-of-the-art certified robustness guarantee when an attacker could arbitrarily modify a certain number of basic elements to the input. Our experimental results show that randomized ablation achieves suboptimal provable robustness guarantees when extended to multi-modal models. This is because when the attacker introduces perturbations with l 0 bounds across all modalities, the space of possible perturbed multi-modal inputs cannot be straightforwardly formulated as a l 0 ball.\n\nWe note that all the previously discussed certified defenses  [7, 10, 19, 27, 28, 53, 57, 60]  are model-agnostic and scalable to large models. Another family of certified defenses  [14, 23, 51]  proposed to derive the certified robustness guarantee of an unimodal model by conducting a layer-by-layer analysis. In general, those methods cannot be applied to general models and are not scalable to large neural networks.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Problem Formulation",
      "text": "We first introduce the threat model and then formally define certified defense against adversarial attacks to classification and segmentation tasks.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Threat Model",
      "text": "We discuss the threat model from the perspective of the attacker's goals, background knowledge, and capabilities. Attacker's goals. Given a multi-modal input and a multimodal model, an attacker aims to adversarially perturb the multi-modal input such that the multi-modal model makes incorrect predictions for the perturbed multi-modal input. Attacker's background knowledge and capabilities. As we focus on the certified defense, we assume the attacker has full knowledge about about the multi-modal model, including its architecture and parameters. We consider a strong attack to multi-modal models. In particular, given a multi-modal input, an attacker could simultaneously manipulate all modalities of the input  [45, 61] . As a result, a multi-modal makes incorrect predictions for the perturbed multi-modal input. For example, to attack an auto-driving system, the attacker can add adversarial perturbation to both the depth image (captured by a LiDAR depth sensor) and the RGB image (captured by a camera) to lower the prediction quality.\n\nFormally, we denote a multi-modal input as\n\n, where m i represents a group of elements of the i-th modality, the attacker could arbitrarily add (or delete or modify) at most r i elements to m i . For instance, when m i represents an image, an attacker could arbitrarily change r i pixel values.\n\nWe use\n\nto denote the adversarial input. Without loss of generality, every modality can be rewritten as a list of it's basic elements. For example, an image (e.g., RGB image) can be written as a list of pixels, and an audio can be written as a list of audio frames. Therefore, we can denote m i as a composition of basic elements denoted by\n\n, where m j i represents the j-th basic element in the i-th modality, and n i represents the total number of basic elements in the i-th modality. We denote the number of basic elements in each modality after the attack as n ′ 1 , n ′ 2 , . . . , n ′ T , respectively. For the image modality, we know the number of basic elements (pixels) is fixed. However, for some other modalities like audio, the attacker is able to change the number of basic elements (e.g., audio frames) via addition or deletion.\n\nHence, we define three kinds of attacks for each modality: modification attack, addition attack, and deletion attack. We use S(m i , r i ) to denote the set of all possible m ′ i when an attacker could add (or delete or modify) at most r i basic elements in m i . For simplicity, we use R = (r 1 , r 2 , . . . , r T ) to denote the added (or deleted or modified) basic elements to all modalities. Then we use S(M, R) = S(m 1 , r 1 )×S(m 2 , r 2 ) . . .×S(m T , r T ) to denote the set of all possible adversarial inputs",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Certifiably Robust Multi-Modal Prediction",
      "text": "For classification tasks, suppose we have a multi-modal classifier G. Given a test sample (M, y), where y is the ground truth label, we say G is certifiably stable for M if the predicted label remains unchanged under attack:\n\nIf this unchanged label is the ground-truth label of M, i.e., G(M) = y, then we say the classifier G is certifiably robust for this test sample. For segmentation tasks, without loss of generality, we assume the multi-modal model outputs the segmentation result for one of the input modalities (denoted by m o ) with n o basic elements (e.g., pixels). Then the output contains n o labels. For example, if RGB image is one of the input modalities, the output can be a segmentation of this RGB image, which contains a label for each pixel in the RGB image. Unless otherwise mentioned, we assume that the attacker performs modification attacks on m o (please refer to Appendix C for deletion and addition attacks on m o ).\n\nWe can think of the multi-modal segmentation model G as composed of multiple classifiers denoted by\n\nThe ground truth y also includes n o labels, denoted by y 1 , y 2 , . . . , y no . We use G j (M ′ ) to denote the predicted label for m j o after the attack. We say G j is certifiably stable for a basic element (e.g., a pixel) m j o if:\n\nwhich means the predicted label for the the j-th basic element of m o remains unchanged under attack. If G j (M) = y j , then we term G j as certifiably robust for m j o . By deriving a lower bound on the number of basic elements whose predictions are certifiably robust, we can guarantee the segmentation quality for a test sample, measured via metrics such as Certified Pixel Accuracy, Certified Fscore, or Certified IoU.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Our Design",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Independent Sub-Sampling",
      "text": "In this section, we will first outline a universal sub-sampling method  [21, 28, 32] , and then demonstrate its application across various multi-modal tasks. Sub-sampling Strategy. We repeatedly randomly subsample k i basic elements (e.g., pixels) from the i-th modality\n\nFor simplicity, we use Z = (z 1 , z 2 , . . . , z T ) to denote the randomly sampled multi-modal input. Thus, we have |z i | = k i for all i = 1, 2, . . . , T . This sampling strategy exhibits versatility by being applicable across various modalities and tasks. It can be applied for classification tasks, e.g., emotion recognition. And it can also be employed for segmentation tasks, e.g., road segmentation. Figure  9  in Appendix provides a visualization of this sub-sampling method.\n\nNext, we first apply this sampling strategy to build an ensemble classifier for classification tasks.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Certify Multi-Modal Classification",
      "text": "Ensemble Classifier.\n\nGiven a testing input M = (m 1 , m 2 , . . . , m T ), we use Z = (z 1 , z 2 , . . . , z T ) to denote the randomly sub-sampled multi-modal input. We denote the multi-modal model by g. For simplicity, we use g(Z) and y to denote the predicted label and the true label. As Z is randomly sub-sampled, g(Z) is also random. Given an arbitrary label l ∈ {1, 2, • • • , C} (C is the total number of classes), we use p l to denote the probability that the predicted label g(Z) is l. Formally, we have p l = Pr(l = g(Z)). We call p l label probability. In practice, it is computationally expensive to calculate the exact label probabilities. Following  [10, 19, 28] , we use Monte Carlo sampling to estimate a lower bound or upper bound of p l , denoted as p l and p l respectively. This is achieved by randomly sample N ablated inputs from the distribution Z, represented as Z 1 , Z 2 , • • • , Z N , and then count the label frequency N l = N i=1 I(g(Z i ) = l) for each label l. Our ensemble classifier G then predicts the label with the largest frequency N l . For simplicity, we denote this label by A and use p A to represent A's label probability lower bound. We define the runner-up label B as the label with the second highest label frequency, i.e., B = argmax l̸ =A N l . We present our certification result below:\n\nTheorem 1 (Certification for classification). Suppose we have a multi-modal test input M and a base multi-modal classifier g. Our ensemble classifier G is as defined as above. We denote A = G(M) and use p A to denote the label probability lower bound for the label A. We use B to denote the runner-up class and use p B to denote the label probability upper bound for the label B. We define\n\nGiven a perturbation size R = (r 1 , r 2 , . . . , r T ), we have the following:\n\nif:\n\nwhere e i = n i -r i and n ′ i = n i for modification attack; e i = n i and n ′ i = n i + r i for addition attack; e i = n i -r i and n ′ i = n i -r i for deletion attack, where i = 1, 2, . . . , T is the modality index.\n\nProof. Please refer to Appendix A.\n\nComputing p B and p A .\n\nFollowing  [10, 19, 20] , we apply Monte Carlo sampling to approximate p B and p A . We first randomly sub-sample N multi-modal inputs from the test input M, and we denote these ablated inputs as\n\nWe denote the number of sub-sampled inputs that predicts for the label l as N l , i.e., N l = N i=1 I(g(Z i ) = l). Then, the frequency N l of any label l follows a binomial distribution. Therefore, we can apply Clopper-Pearson  [9]  based method to estimate p B and p A with predefined confidence level 1 -α:\n\nwhere A represents the predicted label, i.e., A = argmax l N l , and B represents the runner-up label, i.e., argmax l̸ =A N l . Beta(β; λ, θ) calculates the β-th quantile of the Beta distribution given shape parameters λ and θ.\n\nWe divide α by the number of classes because we estimate bounds for C classes simultaneously  [20] . By Bonferroni correction, if we use 1 -α/C as the confidence level to estimate each bound, then the overall confidence level for the C classes is at least 1 -α.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Certify Multi-Modal Segmentation",
      "text": "In this section, we extend our certification method for classification tasks to certify multi-model segmentation tasks. Segmentation tasks are essentially a variant of classification since each basic element (e.g., a pixel) in one of the input modalities (e.g., an image) is assigned a label. We denote this input modality as m o , and denote the j-th basic element of m o as m j o . Suppose m o has n o basic elements. If we naively apply union bound, certifying the test input with overall confidence level 1 -α requires certifying each basic element with confidence level 1 -α no , which becomes hard when n o grows large. To maximize the number of certified basic elements, Fischer et al.  [12]  utilized the Holm-Bonferroni method  [17] , originally designed for Multiple Hypothesis Testing. Specifically, this method tends to certify basic elements with confident predictions, while abstaining ambiguous basic elements. Furthermore, this method guarantees that the probability of mistakenly reporting at least one non-certifiably-stable basic element as certified is limited at α. In this work, we adapt the approach from Fischer et al.  [12]  to multi-modal scenarios. Ensemble Classifiers for Segmentation. Given a testing input M, we use Z = (z 1 , z 2 , . . . , z T ) to denote the randomly sub-sampled input. The base multi-modal segmentation model can be seen as a composition of multimodal classifiers g 1 , g 2 , . . . , g no , where g j predicts a label for the basic element m j o . We use g j (Z) to denote the predicted label for m j o . We randomly sample N ablated inputs from the distribution Z, and represent them as\n\nFor each basic element m j o and each label l, we count the label frequency N j l = N i=1 I(g j (Z i ) = l). The ensemble classifier for m j o (denoted by G j ) then predicts the the label l with the highest label frequency N j l , i.e., G j (M) = argmax l N j l . We say G j is certifiably stable for m j o if the predicted label of G j for m j o remains unchanged under attack, i.e., G j (M) = G j (M ′ ), ∀M ′ ∈ S(M, R). Next, we discuss how to certify as many basic elements as possible given that the possibility of mistakenly certifying a non-certifiably-stable basic element is at most α. Calculate a Confidence Level for Each Basic Element. For each basic element m j o , we denote the number of ablated inputs that predicts the label l for this component as N j l . We denote the total number of ablated inputs as N . We define:\n\nwhere A represents the predicted label for this basic element, i.e., argmax l N j l , and B represents the runner-up label for this component, i.e., argmax l̸ =A N j l . Then we define:\n\nwhere n i , n ′ i , e i , k i , δ l and δ u are defined as in Theorem 1. Then with probability at least 1 -α * j , the basic element m j o is certifiably stable (the output label of this basic element cannot be changed by the attacker) according to Theorem 1.\n\nIn practice, we calculate α * j by binary search. If such an α * j does not exist, the binary search algorithm returns 1 instead. Apply Holm-Bonferroni method. Using the computed values of α * j , we employ the Holm-Bonferroni method  [17]  to determine the basic elements eligible for certification. This method maximizes the number of certified basic elements, while at the same time ensures that the possibility of mistakenly certifying a non-certifiably-stable basic element remains within the limit of α. Specifically, we have two steps:\n\nWe report all basic elements m j o for which α * j < α * (L) as certifiably stable (the output labels of these basic elements cannot be changed by the attacker), and the predictions for other basic elements are abstained. In Section 5, we derive certified metrics, e.g., Certified Pixel Accuracy, from these certifiably stable basic elements.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Evaluation",
      "text": "In this section, we demonstrate the effectiveness of our method on multi-modal emotion recognition task and multimodal road segmentation task.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experimental Setup",
      "text": "Datasets. We use the following benchmark datasets in our evaluation: RAVDESS  [33]  for the multi-modal emotion recognition task and KITTI Road  [1]  for the multi-modal road segmentation task. Details of the datasets can be found in Appendix B. Models. For the multi-modal emotion recognition task, we follow the pipeline proposed by  [8] . Specifically, we utilize EfficientFace  [63]  (a recently proposed facial expression recognition architecture) to extract features from image frames, and use 1D convolutional layers to extract features from audio frames. Then we use intermediate attentionbased fusion  [8]  to combine features extracted from these two modalities. This fusion method ensures that features that are consistent between both modalities have the most significant impact on the final prediction.\n\nAs for the multi-modal road segmentation task, we apply SNE-RoadSeg  [11] , which is capable of merging features from both RGB images and depth images for road segmentation. Specifically, this method first computes surface normal information from depth images, and then employs a data-fusion CNN architecture to fuse features from both RGB images and the inferred surface normal information for accurate prediction.\n\nWe note that if we directly use the original training recipes for these models, we get low prediction accuracy for randomly sub-sampled testing inputs, and the certified robustness of MMCert and randomized ablation  [29]  would be low as both rely on predicting ablated inputs. In response, we perform data augmentation by randomly ablate training inputs, such that the distribution of training data can match that of testing data. We note that this is standard practice for randomized smoothing-based certification methods  [10] . For MMCert, we independently sub-sample between 0% and 5% of basic elements from each modality, ablating the rest. For randomized ablation  [29] , we randomly sample between 0% and 5% of basic elements collectively from the two modalities to keep and ablate the remaining elements. Specifically, we initially merge the basic elements of both modalities into one list. After sampling and ablating, we then split the modified list back into two separate modalities. Compared Method. We compare our method with randomized ablation  [28] , which is the state-of-the-art certification method for l 0 attacks on a single image. We adapt randomized ablation to multi-modal models by combining the sets of basic elements from each modality. Given the original input (m 1 , m 2 , . . . , m T ), where\n\nWe denote the size of m as n = T i=1 n i . Then we randomly sample k elements from m without replacement to get a subset z ⊆ m. Finally, we divide z back to T modalities, i.e., z i = z ∩ m i , and (z 1 , z 2 , . . . , z T ) is the randomly ablated input. We make the final prediction by taking the majority vote of all ablated multi-modal inputs.\n\nFor multi-modal classification tasks, we use the same certification process for randomized ablation as in the original paper  [28] . For multi-modal segmentation tasks, we follow the same certification process as described in Section 4.3 as the original work  [28]  only considered classifi-cation tasks. The only difference is that we define α * j as:\n\nIt is worth noting that in this context, r i represent the maximum number of modified basic elements in the i-th modality. The original work did not take into account addition and deletion attacks, as  [28]  focuses on image domain.\n\nParameter Settings. By default, we focus on modification attacks, where r i denote the maximum basic elements that can be modified by the attacker in ith modality.\n\nFor the multi-modal emotion recognition task, the visual modality contains 108 image frames, while the audio modality contains 79,380 audio frames. Without loss of generality, we denote the maximum number of modified image frames as r 1 and the maximum number of modified audio frames as r 2 . The default setting is that the attacker can modify equal or more audio frames than image frames. That is, we let r 2 = ĉ • r 1 , for ĉ = 1, 2, 3, 4. We set k 1 = 5 and k 2 = 1, 000. For randomized ablation, k is set to 3,000 such that, when there is no attack (r 1 = r 2 = 0), the accuracy of randomized ablation is similar to our MMCert. In Appendix D, we show the case where an attacker can modify more image frames than audio frames (r 1 > r 2 ).\n\nFor the multi-modal road segmentation task, the first modality is a RGB image that consists of 375 × 1, 242 pixels, where each pixel has three channels (representing the three primary colors), while the second modality is a depth image that has the same number of pixels, but each pixel has a single channel for depth. The default setting is that the attacker can modify equal or more pixels from the depth image than pixels from the RGB image. Specifically, we test for r 2 = ĉ • r 1 where ĉ = 1, 2, 3, 4. For our MMCert, we set k 1 = 9, 000 and k 2 = 1, 000. Regarding randomized ablation, we set the total number of retained pixels k to 10,000 such when there is no attack (r 1 = r 2 = 0), the accuracy of randomized ablation is similar to our MMCert. In Appendix D, we show the case where the attacker can change more pixels from the RGB image than pixels from the depth image (r 1 > r 2 ).\n\nFor Monte Carlo sampling, we set N = 100 and α = 0.001 for all experiments.\n\nEvaluation Metrics. We use Certified Accuracy as the evaluation metric for the multi-modal emotion recognition task, and use Certified Pixel Accuracy, Certified F-score, and Certified IoU as the evaluation metrics for the multimodal road segmentation task.\n\n• Certified Accuracy. Certified Accuracy is defined as the fraction of testing inputs whose predicted labels are not only correct but also verified to be unchanged by an attacker, i.e., certifiably stable. A testing sample for a multi-modal model can be represented as (M, y) ∈ D test , where D test is the testing dataset. M is the multimodal test input and y is the ground truth label. We use G to denote the multi-modal classifier. Then we can define Certified Accuracy as:\n\nIsStable(M) is true if and only if for all M ′ ∈ S(M, R), we have G(M) = G(M ′ ). I is the indicator function, and |D test | is the total number of testing inputs in D test .\n\n• Certified Pixel Accuracy (or F-score or IoU). Here we consider these certified metrics for the purpose of freespace detection  [11] . For general purposed segmentation tasks, mean values over different classes should be considered. The Certified Pixel Accuracy (or F-score or IoU) is defined as the average Pixel Accuracy (or F-score or IoU) lower bound of testing inputs under a given adversarial perturbation space R. We use j ∈ [n o ] to denote the index of a basic element of the segmented input modality m o . We define:\n\nwhere label 1 represents freespace and label 0 represents non-freespace. IsStable(M, j) is true if and only if the predicted label of the jth basic element of m o cannot be changed by the attacker, i.e., G j (M) = G j (M ′ ), ∀M ′ ∈ S(M, R). Then for an individual test sample, we have Certified Pixel Accuracy =\n\n, and Certified IoU = T P T P +F P +F N . To obtain the final metrics, we compute the average of these values across all test samples.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experimental Results",
      "text": "In this section, we first compare our method with an existing state-of-the-art method, followed by an analysis of the impact of hyper-parameters on MMCert. Then, we show the performance of our method on attack types other than modification attack, i.e., addition and deletion attacks. Our MMCert Outperforms Existing State-of-the-Art Method. Figure  1  and Figure  2  show the comparison result between our MMCert and randomized ablation  [28] , which is the state-of-the-art certified defense against l 0 attacks. We can see that our MMCert consistently outperforms randomized ablation on both tasks, for all combinations of r 1 and r 2 . For example, Figure  1  shows that on the RAVDESS dataset, when r 1 = r 2 = 8 (the attacker can modify 8 frames in both visual and audio modalities), our MMCert can guarantee correct predictions for more than 40% of the test samples, while randomized ablation can guarantee 0% of the test samples.\n\nOur method is more effective than randomized ablation because of two reasons. First, our method provides an adaptive selection of k 1 and k 2 to control the fraction of subsampled basic elements, i.e., k1 n1 and k2 n2 , of the two modalities. In contrast, for randomized ablation, the sub-sampled fractions for both modalities are the identical on average, i.e., k n . This means that randomized ablation is essentially a special case of our MMCert. Secondly, our MMCert is more stable than randomized ablation during both training and testing phases. In our method, the count of sub-sampled basic elements remains constant at k 1 and k 2 for each modality. Meanwhile, in randomized ablation, this count, adding up to k, fluctuates. As a result, our method's sub-sampled input space is smaller than that of randomized ablation, enhancing stability. Impact of k 1 and k 2 . Here we study the impact of k 1 and k 2 on the performance of our MMCert. To simplify the analysis, we perform the experiment on KITTI Road Dataset such that we have n 1 = n 2 . This setup allows a direct comparison of the attacker's capability across two modalities using r 1 and r 2 . We keep the sum of k 1 and k 2 constant at 10,000 but vary their ratio. Three specific ratios were tested:\n\nThe results are presented in Figure  4 . We observe that with r 2 = r 1 (indicating similar attack capabilities on both modalities), different ratios of k 1 and k 2 have similar performance outcomes. However, for r 2 > r 1 (where the attacker has more attack capability on the second modality), strategies with a larger k 1 /k 2 ratio demonstrated better robustness. For example, if r 2 > r 1 , the k 2 = 9k 1 sub-sampling strategy consistently outperforms k 2 = 3k 1 , with this advantage magnifying as r 2 /r 1 increased.. Therefore, in practice, it is advantageous to sub-sample fewer basic elements from the modality with higher attack capability and sub-sample more basic elements from the modality with lower attack capability, provided this doesn't compromise the utility (accuracy when there is no attack). Different Attack Types. We previously focused on modification attacks, where the attacker modifies at most r 1 and r 2 basic elements for the two respective modalities. Our method also allows the attacker to add or delete basic elements from each modality. Here we do experiments in scenarios where the attacker can add (or delete) at most r 1 and r 2 basic elements respectively for the two modalities, and compare with the modification attack scenario. The results are shown in Figure  3 . We can see that modification attack is the strongest attack type. For example, when r 1 and r 2 are both 10, modification attack brings the certified accuracy down to 0. In contrast, the addition attack maintains a certified accuracy greater than 0.4, and the deletion attack maintains a certified accuracy greater than 0.6.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we propose MMCert, the first certified defense against adversarial attacks for multi-modal models.\n\nOur experimental results show that MMCert significantly improves the certified robustness guarantees by leveraging a modality-independent sub-sampling strategy.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "A. Proof Of Theorem 1",
      "text": "Our proof is extended from previous studies  [20] . We first specify notations and then show our proof. Given original multimodal input pair M = (m 1 , m 2 , . . . , m T ) and attacked input pair (m ′ 1 , m ′ 2 , . . . , m ′ T ), we respectively use X and Y to denote the ablated multi-modal input sampled from them without replacement. We use e i to denote the number of basic elements (e.g., pixels) that are in both m i and m ′ i , i.e., e i = |m i ∩ m ′ i |. Moreover, we use Υ to denote the joint space between X and Y. We use E = (E 1 , E 2 , . . . , E T ) to denote a variable in the space Υ.\n\nWe divide the space Υ into the following subspace:\n\nWe present Neyman Pearson Lemma  [10, 20, 36]  for later use. (\n\nProof. Let's start by proving part  (1) . For convenience, we denote the complement of S as S c . With this notation, we have the following:\n\nEquation 23 is derived from 22 due to the fact that Pr\n\n, and 1 -Z(1|E) ≥ 0. Similarly, we can establish the proof for part (2), but we have omitted the detailed steps for the sake of conciseness.\n\nFor simplicity, we use n i and n ′ i to denote the number of basic elements (e.g., pixels) in m i and m ′ i respectively, i.e.,\n\nThen, we have the following probability mass function:\n\nPr\n\nRecall that we have e i = |m ′ i ∩ m i | for i = 1, 2, . . . , T , so the probability of X and Y in Ã, B and C can be computed as follows:\n\nWe first define δ l = Pr(g(X ) = A) -\n\nto help rounding Pr(g(X ) = A). Then we can construct a set S = Ã + B′ , where B′ ⊆ B and Pr(X ∈ B′ ) = Pr(g(X ) = A) -δ l -Pr(X ∈ Ã). We can assume Pr(g(X ) = A) > Pr(X ∈ Ã) because otherwise Pr(g(Y) = A) is bounded by 0. Then we have Pr(g(X ) = A) ≥ Pr(X ∈ S). So we have the following lower bound on Pr(g(Y) = A):\n\n≥ Pr(Y ∈ B′ )\n\nSimilarly we define\n\n-Pr(g(X ) = B), so we can construct a set S = B′ + C, where B′ ⊆ B and Pr(X ∈ B′ ) = Pr(g(X ) = B) + δ u -Pr(X ∈ C). Then we have Pr(g(X ) = B) ≤ Pr(X ∈ S). So we have the following upper bound on Pr(g(Y) = B):\n\nTo certify a test sample, we just need to enforce Pr(g(Y) = A) > Pr(g(Y) = B). So we get Theorem 1.",
      "page_start": 9,
      "page_end": 13
    },
    {
      "section_name": "B. Details About The Datasets",
      "text": "We use two benchmark datasets for evaluation.\n\n• RAVDESS. We use RAVDESS dataset  [33]  for the multi-modal emotion recognition task. This dataset contains video recordings of 24 participants, each speaking with a variety of emotions. The goal is to classify these emotions into one of seven categories: calm, happy, sad, angry, fearful, surprise, and disgust. For each participant, there are 60 distinct video sequences. For data pre-processing, we follow previous work  [2, 8]  and crop or zero-pad these videos to 3.6 seconds, which is the average video length. After pre-processing, each data sample contains 108 image frames and 79380 audio frames. We assume that the attacker can arbitrarily modify r 1 image frames (from 108 image frames of visual input) and r 2 audio frames (from 79380 audio frames of audio input). We divide the data into training, validation and test sets ensuring that the identities of actors are not repeated across sets. Particularly, we used four actors for testing, four for validation, and the remaining 16 for training.\n\n• KITTI Road. For the multi-modal road segmentation task, we use KITTI Road Dataset  [1] , which contains 289 training and 290 test samples across three distinct road scene categories. Notably, the initial release  [1]  lacks ground-truth labels for its test samples. As a result, we divided the original training dataset into 231 data samples (80% of the data samples) for training and 58 data samples (20% of the data samples) for testing. Each data sample consists of a RGB image, a depth image, and the ground truth segmentation. We assume that the attacker can arbitrarily modify r 1 pixels from the RGB image and r 2 pixels from the depth image for each testing input. First, we think of the multi-modal segmentation model before the attack (denoted by G) as composed of multiple classifiers denoted by G 1 , G 2 , . . . , G no . Each classifier G j predicts a label G j (M) for m j o (the jth basic element of m o ). The ground truth y also includes n o labels, denoted by y 1 , y 2 , . . . , y no . We use G j (M) to denote the predicted label for m j o before the attack and use G j (M ′ ) to denote the predicted label for m j o after the attack. We say a basic element (e.g., a pixel)",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "C. Special Cases In Multi-Modal Segmentation",
      "text": "which means jth basic element of m o is also in m ′ o and the predicted label for it is unchanged by the attack. If it also holds that G j (M) = y j , then we term m j o as certifiably robust. Then we derive Certified Pixel Accuracy (or F-score or IoU) for deletion and addition attacks on m o . We use j ∈ where 1 indicates that this basic element has been identified as belonging to this label, while label 0 signifies the opposite. IsStable(M, j) is true if and only if the jth basic element of m o is certifiably stable as defined above. We use r o denote the added (or deleted) basic elements for m o . Then for addition attacks to m o , the worst case is that all added basic elements are not certifiably robust, so we have Certified Pixel Accuracy = Here, we compare our method with randomized ablation for the case r 1 > r 2 . For KITTI Road dataset, we set k 1 to 4,000 and k 2 to 6,000 for our MMCert and set k to 10,000 for randomized ablation. For RAVEDESS, we let k 1 = 5 and k 2 = 1,000 for our MMCert and let k = 3,000 for randomized ablation. The results of these experiments are illustrated in Figures  5  and 6 , corresponding to RAVNESS and KITTI Road datasets, respectively. Our findings reveal that our method consistently",
      "page_start": 15,
      "page_end": 15
    }
  ],
  "figures": [
    {
      "caption": "Figure 9: in Appendix",
      "page": 4
    },
    {
      "caption": "Figure 1: Compare our MMCert with randomized ablation on RAVDESS Dataset.",
      "page": 6
    },
    {
      "caption": "Figure 2: Compare our MMCert with randomized ablation on KITTI Road Dataset. Certified Pixel Accuracy (first row), Certified F-score",
      "page": 6
    },
    {
      "caption": "Figure 3: Compare different attack types on RAVDESS Dataset.",
      "page": 7
    },
    {
      "caption": "Figure 4: Impact of the ratio between k1 and k2. Certified Pixel Accuracy (first row), Certified F-score (second row) and Certified IoU",
      "page": 7
    },
    {
      "caption": "Figure 1: and Figure 2 show the comparison result",
      "page": 8
    },
    {
      "caption": "Figure 1: shows that on the RAVDESS",
      "page": 8
    },
    {
      "caption": "Figure 4: We observe that with r2 = r1",
      "page": 8
    },
    {
      "caption": "Figure 3: We can see that modification attack",
      "page": 8
    },
    {
      "caption": "Figure 5: Compare our MMCert with randomized ablation on RAVDESS Dataset.",
      "page": 15
    },
    {
      "caption": "Figure 6: Compare our MMCert with randomized ablation on KITTI Road Dataset. Certified Pixel Accuracy (first row), Certified F-score",
      "page": 15
    },
    {
      "caption": "Figure 7: Impact of N on RAVDESS dataset.",
      "page": 15
    },
    {
      "caption": "Figure 8: Impact of α on RAVDESS dataset.",
      "page": 16
    },
    {
      "caption": "Figure 9: Illustration of independent sub-sampling on KITTI Road dataset. Our method repeatedly generate predictions for subsampled",
      "page": 16
    },
    {
      "caption": "Figure 7: in Appendix shows the impact of N. We discover that the",
      "page": 16
    },
    {
      "caption": "Figure 8: in Appendix",
      "page": 16
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "RA": "MMCert"
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "RA": "MMCert"
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "RA": "MMCert"
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "RA": "MMCert"
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "RA": "MMCert"
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "RA": "MMCert"
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "RA": "MMCert"
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "RA": "MMCert"
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "RA": "MMCert"
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "RA": "MMCert"
        }
      ],
      "page": 15
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "Dataset Kitti Road"
      ],
      "year": "2023",
      "venue": ""
    },
    {
      "citation_id": "3",
      "title": "Vqa: Visual question answering",
      "authors": [
        "Stanislaw Antol",
        "Aishwarya Agrawal",
        "Jiasen Lu",
        "Margaret Mitchell",
        "Dhruv Batra",
        "C Lawrence Zitnick",
        "Devi Parikh"
      ],
      "year": "2015",
      "venue": "ICCV"
    },
    {
      "citation_id": "4",
      "title": "Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples",
      "authors": [
        "Anish Athalye",
        "Nicholas Carlini",
        "David Wagner"
      ],
      "year": "2018",
      "venue": "ICML"
    },
    {
      "citation_id": "5",
      "title": "Adversarial examples are not easily detected: Bypassing ten detection methods",
      "authors": [
        "Nicholas Carlini",
        "David Wagner"
      ],
      "year": "2017",
      "venue": "Proceedings of the 10th ACM workshop on artificial intelligence and security"
    },
    {
      "citation_id": "6",
      "title": "Fusion is not enough: Single-modal attacks to compromise fusion models in autonomous driving",
      "authors": [
        "Zhiyuan Cheng",
        "Hongjun Choi",
        "James Liang",
        "Shiwei Feng",
        "Guanhong Tao",
        "Dongfang Liu",
        "Michael Zuzak",
        "Xiangyu Zhang"
      ],
      "year": "2023",
      "venue": "Fusion is not enough: Single-modal attacks to compromise fusion models in autonomous driving",
      "arxiv": "arXiv:2304.14614"
    },
    {
      "citation_id": "7",
      "title": "Certified defenses for adversarial patches",
      "authors": [
        "Ping-Yeh Chiang",
        "Renkun Ni",
        "Ahmed Abdelkader",
        "Chen Zhu",
        "Christoph Studer",
        "Tom Goldstein"
      ],
      "year": "2020",
      "venue": "Certified defenses for adversarial patches",
      "arxiv": "arXiv:2003.06693"
    },
    {
      "citation_id": "8",
      "title": "Alexandros Iosifidis, and Moncef Gabbouj. Self-attention fusion for audiovisual emotion recognition with incomplete data",
      "authors": [
        "Kateryna Chumachenko"
      ],
      "year": "2022",
      "venue": "ICPR"
    },
    {
      "citation_id": "9",
      "title": "The use of confidence or fiducial limits illustrated in the case of the binomial",
      "authors": [
        "J Charles",
        "Egon S Clopper",
        "Pearson"
      ],
      "year": "1934",
      "venue": "Biometrika"
    },
    {
      "citation_id": "10",
      "title": "Certified adversarial robustness via randomized smoothing",
      "authors": [
        "Elan Jeremy M Cohen",
        "J Zico Rosenfeld",
        "Kolter"
      ],
      "year": "2019",
      "venue": "Certified adversarial robustness via randomized smoothing",
      "arxiv": "arXiv:1902.02918"
    },
    {
      "citation_id": "11",
      "title": "Sneroadseg: Incorporating surface normal information into semantic segmentation for accurate freespace detection",
      "authors": [
        "Rui Fan",
        "Hengli Wang",
        "Peide Cai",
        "Ming Liu"
      ],
      "year": "2008",
      "venue": "ECCV"
    },
    {
      "citation_id": "12",
      "title": "Scalable certified segmentation via randomized smoothing",
      "authors": [
        "Marc Fischer",
        "Maximilian Baader",
        "Martin Vechev"
      ],
      "year": "2021",
      "venue": "ICML"
    },
    {
      "citation_id": "13",
      "title": "Explaining and harnessing adversarial examples",
      "authors": [
        "Ian Goodfellow",
        "Jonathon Shlens",
        "Christian Szegedy"
      ],
      "year": "2014",
      "venue": "Explaining and harnessing adversarial examples",
      "arxiv": "arXiv:1412.6572"
    },
    {
      "citation_id": "14",
      "title": "On the effectiveness of interval bound propagation for training verifiably robust models",
      "authors": [
        "Sven Gowal",
        "Krishnamurthy Dvijotham",
        "Robert Stanforth",
        "Rudy Bunel",
        "Chongli Qin",
        "Jonathan Uesato",
        "Relja Arandjelovic",
        "Timothy Mann",
        "Pushmeet Kohli"
      ],
      "year": "2018",
      "venue": "On the effectiveness of interval bound propagation for training verifiably robust models",
      "arxiv": "arXiv:1810.12715"
    },
    {
      "citation_id": "15",
      "title": "Deep learning-based image segmentation on multimodal medical imaging",
      "authors": [
        "Zhe Guo",
        "Xiang Li",
        "Heng Huang",
        "Ning Guo",
        "Quanzheng Li"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Radiation and Plasma Medical Sciences"
    },
    {
      "citation_id": "16",
      "title": "Learning feature fusion in deep learning-based object detector",
      "authors": [
        "Ehtesham Hassan",
        "Yasser Khalil",
        "Imtiaz Ahmad"
      ],
      "year": "2020",
      "venue": "Journal of Engineering"
    },
    {
      "citation_id": "17",
      "title": "A simple sequentially rejective multiple test procedure",
      "authors": [
        "Sture Holm"
      ],
      "year": "1979",
      "venue": "Scandinavian journal of statistics"
    },
    {
      "citation_id": "18",
      "title": "Trevor Darrell, and Marcus Rohrbach. Iterative answer prediction with pointeraugmented multimodal transformers for textvqa",
      "authors": [
        "Ronghang Hu",
        "Amanpreet Singh"
      ],
      "year": "2020",
      "venue": "CVPR"
    },
    {
      "citation_id": "19",
      "title": "Certified robustness for top-k predictions against adversarial perturbations via randomized smoothing",
      "authors": [
        "Jinyuan Jia",
        "Xiaoyu Cao",
        "Binghui Wang",
        "Neil Zhenqiang"
      ],
      "year": "2004",
      "venue": "ICLR"
    },
    {
      "citation_id": "20",
      "title": "Intrinsic certified robustness of bagging against data poisoning attacks",
      "authors": [
        "Jinyuan Jia",
        "Xiaoyu Cao",
        "Neil Zhenqiang"
      ],
      "year": "2004",
      "venue": "AAAI"
    },
    {
      "citation_id": "21",
      "title": "Almost tight l0-norm certified robustness of top-k predictions against adversarial perturbations",
      "authors": [
        "Jinyuan Jia",
        "Binghui Wang",
        "Xiaoyu Cao",
        "Hongbin Liu",
        "Neil Zhenqiang"
      ],
      "year": "2021",
      "venue": "ICLR"
    },
    {
      "citation_id": "22",
      "title": "Multiguard: Provably robust multi-label classification against adversarial examples",
      "authors": [
        "Jinyuan Jia",
        "Wenjie Qu",
        "Neil Gong"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "23",
      "title": "Reluplex: An efficient smt solver for verifying deep neural networks",
      "authors": [
        "Guy Katz",
        "Clark Barrett",
        "David Dill",
        "Kyle Julian",
        "Mykel",
        "Kochenderfer"
      ],
      "year": "2017",
      "venue": "Computer Aided Verification: 29th International Conference"
    },
    {
      "citation_id": "24",
      "title": "Epic-fusion: Audio-visual temporal binding for egocentric action recognition",
      "authors": [
        "Evangelos Kazakos",
        "Arsha Nagrani",
        "Andrew Zisserman",
        "Dima Damen"
      ],
      "year": "2019",
      "venue": "ICCV"
    },
    {
      "citation_id": "25",
      "title": "On single source robustness in deep fusion models",
      "authors": [
        "Taewan Kim",
        "Joydeep Ghosh"
      ],
      "year": "2019",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "26",
      "title": "Gated mechanism for attention based multi modal sentiment analysis. In ICASSP",
      "authors": [
        "Ayush Kumar",
        "Jithendra Vepa"
      ],
      "year": "2020",
      "venue": "IEEE"
    },
    {
      "citation_id": "27",
      "title": "Certified robustness to adversarial examples with differential privacy",
      "authors": [
        "Mathias Lecuyer",
        "Vaggelis Atlidakis",
        "Roxana Geambasu",
        "Daniel Hsu",
        "Suman Jana"
      ],
      "year": "2019",
      "venue": "2019 IEEE Symposium on Security and Privacy (SP)"
    },
    {
      "citation_id": "28",
      "title": "Robustness certificates for sparse adversarial attacks by randomized ablation",
      "authors": [
        "Alexander Levine",
        "Soheil Feizi"
      ],
      "year": "2008",
      "venue": "Robustness certificates for sparse adversarial attacks by randomized ablation"
    },
    {
      "citation_id": "29",
      "title": "Robustness certificates for sparse adversarial attacks by randomized ablation",
      "authors": [
        "Alexander Levine",
        "Soheil Feizi"
      ],
      "year": "2020",
      "venue": "AAAI"
    },
    {
      "citation_id": "30",
      "title": "Deepfusion: Lidar-camera deep fusion for multi-modal 3d object detection",
      "authors": [
        "Yingwei Li",
        "Adams Yu",
        "Tianjian Meng",
        "Ben Caine",
        "Jiquan Ngiam",
        "Daiyi Peng",
        "Junyang Shen",
        "Yifeng Lu",
        "Denny Zhou",
        "Quoc V Le"
      ],
      "year": "2022",
      "venue": "CVPR"
    },
    {
      "citation_id": "31",
      "title": "Multimodal material segmentation",
      "authors": [
        "Yupeng Liang",
        "Ryosuke Wakaki",
        "Shohei Nobuhara",
        "Ko Nishino"
      ],
      "year": "2022",
      "venue": "CVPR"
    },
    {
      "citation_id": "32",
      "title": "Pointguard: Provably robust 3d point cloud classification",
      "authors": [
        "Hongbin Liu",
        "Jinyuan Jia",
        "Neil Zhenqiang"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "33",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "R Steven",
        "Frank Livingstone",
        "Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "34",
      "title": "Towards deep learning models resistant to adversarial attacks",
      "authors": [
        "Aleksander Madry",
        "Aleksandar Makelov",
        "Ludwig Schmidt",
        "Dimitris Tsipras",
        "Adrian Vladu"
      ],
      "year": "2017",
      "venue": "Towards deep learning models resistant to adversarial attacks"
    },
    {
      "citation_id": "35",
      "title": "End-to-end autonomous driving with semantic depth cloud mapping and multi-agent",
      "authors": [
        "Oskar Natan",
        "Jun Miura"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Intelligent Vehicles"
    },
    {
      "citation_id": "36",
      "title": "on the problem of the most efficient tests of statistical hypotheses",
      "authors": [
        "Jerzy Neyman",
        "Egon Sharpe Pearson",
        "Ix"
      ],
      "year": "1933",
      "venue": "Philosophical Transactions of the Royal Society of London"
    },
    {
      "citation_id": "37",
      "title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images",
      "authors": [
        "Anh Nguyen",
        "Jason Yosinski",
        "Jeff Clune"
      ],
      "year": "2015",
      "venue": "CVPR"
    },
    {
      "citation_id": "38",
      "title": "Textguard: Provable defense against backdoor attacks on text classification",
      "authors": [
        "Hengzhi Pei",
        "Jinyuan Jia",
        "Wenbo Guo",
        "Bo Li",
        "Dawn Song"
      ],
      "year": "2024",
      "venue": "NDSS"
    },
    {
      "citation_id": "39",
      "title": "Multimodal fusion transformer for end-to-end autonomous driving",
      "authors": [
        "Aditya Prakash",
        "Kashyap Chitta",
        "Andreas Geiger"
      ],
      "venue": "CVPR, 2021"
    },
    {
      "citation_id": "40",
      "title": "Adversarial training for free! NeurIPS",
      "authors": [
        "Ali Shafahi",
        "Mahyar Najibi",
        "Mohammad Amin Ghiasi",
        "Zheng Xu",
        "John Dickerson",
        "Christoph Studer",
        "Larry Davis",
        "Gavin Taylor",
        "Tom Goldstein"
      ],
      "year": "2019",
      "venue": "Adversarial training for free! NeurIPS"
    },
    {
      "citation_id": "41",
      "title": "Cycle-consistency for robust visual question answering",
      "authors": [
        "Meet Shah",
        "Xinlei Chen",
        "Marcus Rohrbach",
        "Devi Parikh"
      ],
      "year": "2019",
      "venue": "CVPR"
    },
    {
      "citation_id": "42",
      "title": "Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition",
      "authors": [
        "Mahmood Sharif",
        "Sruti Bhagavatula",
        "Lujo Bauer",
        "Michael Reiter"
      ],
      "year": "2016",
      "venue": "Proceedings of the 2016 acm sigsac conference on computer and communications security"
    },
    {
      "citation_id": "43",
      "title": "Drift with devil: Security of {Multi-Sensor} fusion based localization in {High-Level} autonomous driving under {GPS} spoofing",
      "authors": [
        "Junjie Shen",
        "Jun Yeon Won",
        "Zeyuan Chen",
        "Qi Alfred"
      ],
      "year": "2020",
      "venue": "USENIX Security"
    },
    {
      "citation_id": "44",
      "title": "Multinet: Real-time joint semantic reasoning for autonomous driving",
      "authors": [
        "Marvin Teichmann",
        "Michael Weber",
        "Marius Zoellner",
        "Roberto Cipolla",
        "Raquel Urtasun"
      ],
      "year": "2018",
      "venue": "2018 IEEE intelligent vehicles symposium (IV)"
    },
    {
      "citation_id": "45",
      "title": "Can audio-visual integration strengthen robustness under multimodal attacks?",
      "authors": [
        "Yapeng Tian",
        "Chenliang Xu"
      ],
      "year": "2021",
      "venue": "CVPR"
    },
    {
      "citation_id": "46",
      "title": "Adversarial risk and the dangers of evaluating against weak attacks",
      "authors": [
        "Jonathan Uesato",
        "O' Brendan",
        "Pushmeet Donoghue",
        "Aaron Kohli",
        "Oord"
      ],
      "year": "2018",
      "venue": "ICML"
    },
    {
      "citation_id": "47",
      "title": "Multispectral pedestrian detection using deep fusion convolutional neural networks",
      "authors": [
        "Jörg Wagner",
        "Volker Fischer",
        "Michael Herman",
        "Sven Behnke"
      ],
      "year": "2016",
      "venue": "In ESANN"
    },
    {
      "citation_id": "48",
      "title": "Pointaugmenting: Cross-modal augmentation for 3d object detection",
      "authors": [
        "Chunwei Wang",
        "Chao Ma",
        "Ming Zhu",
        "Xiaokang Yang"
      ],
      "venue": "CVPR, 2021"
    },
    {
      "citation_id": "49",
      "title": "Certified robustness to word substitution attack with differential privacy",
      "authors": [
        "Wenjie Wang",
        "Pengfei Tang",
        "Jian Lou",
        "Li Xiong"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "50",
      "title": "Vqa-gnn: Reasoning with multimodal knowledge via graph neural networks for visual question answering",
      "authors": [
        "Yanan Wang",
        "Michihiro Yasunaga",
        "Hongyu Ren",
        "Shinya Wada",
        "Jure Leskovec"
      ],
      "year": "2023",
      "venue": "ICCV"
    },
    {
      "citation_id": "51",
      "title": "Provable defenses against adversarial examples via the convex outer adversarial polytope",
      "authors": [
        "Eric Wong",
        "Zico Kolter"
      ],
      "year": "2018",
      "venue": "ICML"
    },
    {
      "citation_id": "52",
      "title": "Fast is better than free: Revisiting adversarial training",
      "authors": [
        "Eric Wong",
        "Leslie Rice",
        "J Zico Kolter"
      ],
      "year": "2020",
      "venue": "Fast is better than free: Revisiting adversarial training",
      "arxiv": "arXiv:2001.03994"
    },
    {
      "citation_id": "53",
      "title": "{PatchCleanser}: Certifiably robust defense against adversarial patches for any image classifier",
      "authors": [
        "Chong Xiang",
        "Saeed Mahloujifar",
        "Prateek Mittal"
      ],
      "year": "2022",
      "venue": "USENIX Security"
    },
    {
      "citation_id": "54",
      "title": "Fooling vision and language models despite localization and attention mechanism",
      "authors": [
        "Xiaojun Xu",
        "Xinyun Chen",
        "Chang Liu",
        "Anna Rohrbach",
        "Trevor Darrell",
        "Dawn Song"
      ],
      "year": "2018",
      "venue": "CVPR"
    },
    {
      "citation_id": "55",
      "title": "Graphguard: Provably robust graph classification against adversarial attacks",
      "authors": [
        "Han Yang",
        "Binghui Wang",
        "Jinyuan Jia"
      ],
      "year": "2023",
      "venue": "The Twelfth International Conference on Learning Representations"
    },
    {
      "citation_id": "56",
      "title": "Defending multimodal fusion models against single-source adversaries",
      "authors": [
        "Karren Yang",
        "Wan-Yi Lin",
        "Manash Barman",
        "Filipe Condessa",
        "Zico Kolter"
      ],
      "venue": "CVPR, 2021"
    },
    {
      "citation_id": "57",
      "title": "Safer: A structurefree approach for certified robustness to adversarial word substitutions. arXiv",
      "authors": [
        "Mao Ye",
        "Chengyue Gong",
        "Qiang Liu"
      ],
      "year": "2020",
      "venue": "Safer: A structurefree approach for certified robustness to adversarial word substitutions. arXiv"
    },
    {
      "citation_id": "58",
      "title": "Mosi: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos",
      "authors": [
        "Amir Zadeh",
        "Rowan Zellers",
        "Eli Pincus",
        "Louis-Philippe Morency"
      ],
      "year": "2016",
      "venue": "Mosi: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos",
      "arxiv": "arXiv:1606.06259"
    },
    {
      "citation_id": "59",
      "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "Amirali Bagher Zadeh",
        "Paul Liang",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "60",
      "title": "Certified robustness to text adversarial attacks by randomized [mask]. Computational Linguistics",
      "authors": [
        "Jiehang Zeng",
        "Jianhan Xu",
        "Xiaoqing Zheng",
        "Xuanjing Huang"
      ],
      "year": "2023",
      "venue": "Certified robustness to text adversarial attacks by randomized [mask]. Computational Linguistics"
    },
    {
      "citation_id": "61",
      "title": "Towards adversarial attack on vision-language pre-training models",
      "authors": [
        "Jiaming Zhang",
        "Qi Yi",
        "Jitao Sang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "62",
      "title": "Pointcert: Point cloud classification with deterministic certified robustness guarantees",
      "authors": [
        "Jinghuai Zhang",
        "Jinyuan Jia",
        "Hongbin Liu",
        "Neil Zhenqiang"
      ],
      "year": "2023",
      "venue": "CVPR"
    },
    {
      "citation_id": "63",
      "title": "Robust lightweight facial expression recognition network with label distribution training",
      "authors": [
        "Zengqun Zhao",
        "Qingshan Liu",
        "Feng Zhou"
      ],
      "year": "2021",
      "venue": "AAAI"
    }
  ]
}