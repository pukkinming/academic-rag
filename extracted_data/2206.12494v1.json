{
  "paper_id": "2206.12494v1",
  "title": "Multitask Vocal Burst Modeling With Resnets And Pre-Trained Paralinguistic Conformers",
  "published": "2022-06-24T21:42:16Z",
  "authors": [
    "Josh Belanich",
    "Krishna Somandepalli",
    "Brian Eoff",
    "Brendan Jou"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This technical report presents the modeling approaches used in our submission to the ICML Expressive Vocalizations Workshop & Competition multitask track (EXVO-MULTITASK). We first applied image classification models of various sizes on mel-spectrogram representations of the vocal bursts, as is standard in sound event detection literature. Results from these models show an increase of 21.24% over the baseline system with respect to the harmonic mean of the task metrics, and comprise our team's main submission to the MULTITASK track. We then sought to characterize the headroom in the MULTITASK track by applying a large pre-trained Conformer model that previously achieved state-of-the-art results on paralinguistic tasks like speech emotion recognition and mask detection. We additionally investigated the relationship between the sub-tasks of emotional expression, country of origin, and age prediction, and discovered that the best performing models are trained as single-task models, questioning whether the problem truly benefits from a multitask setting.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Vocal bursts are an important but often overlooked component of paralanguage in affective computing applications. They are ubiquitous in human communication, and have been studied as channels for emotional expression  (Hawk et al., 2009; Cowen et al., 2019) . Due to their prevalence in natural speech, predicting emotion perception of vocal bursts is an important complement to in-thewild speech emotion recognition. The Hume Vocal Bursts dataset (HUME-VB) is a recent dataset for studying these paralinguistic phenomena covering four globally diverse 1 Google Research. Correspondence to: Josh Belanich <joshbelanich@google.com>.\n\nProceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copyright 2022 by the author(s).\n\nregions across 3 languages: USA (English), China (Mandarin), South Africa (English), and Venezuela (Spanish).\n\nWe explore the suitability of ResNet  (He et al., 2016)  image classifiers of various sizes in predicting the perceived emotional expression of vocal bursts and subject attributes in HUME-VB. We turn to ResNet architectures given their competitive history in sound event detection tasks  (Hershey et al., 2017) . Such models are typically trained over a mel-spectrogram representation of a waveform that is then treated as an image. Recognizing vocal burst categories (e.g., laughter, sighs) could be seen as a special-case of the sound event detection problem, and a small set of vocal burst categories are contained in AudioSet's \"human voice\" ontology branch  (Gemmeke et al., 2017) . While these models may be suitable for vocal burst recognition, it is unclear whether they are suitable for recognizing the emotional nuance in vocal bursts.\n\nWe also explore features extracted using large pre-trained self-supervised Conformers  (Gulati et al., 2020)  to better understand the performance headroom available on this dataset. In particular, we use a 600M+ parameter Conformer model trained on a 900K hour dataset of \"speech-heavy\" YouTube videos (YT-U)  (Zhang et al., 2021) . This Conformer has been shown to achieve SoTA on a number of speech paralinguistic tasks including speech emotion recognition and mask detection  (Shor et al., 2022) .\n\nFinally, we explore the relationship between the three tasks of perceived emotion, age, and country of origin prediction. In the literature there is evidence of cultural modulation and in-group advantage for emotion recognition across multiple channels  (Elfenbein & Ambady, 2002) . However, on HUME-VB, we observed little evidence for the relationship between the three tasks in the context of the two model families we studied. Our best competition models predict each task individually with no shared parameters.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Model Architectures",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Resnet Models",
      "text": "We downsample audio waveforms to 16kHz and normalize the signal between -1 and 1 by re-scaling using the maximum and minimum values. On these normalized waveforms, we compute log mel-spectrograms using an FFT frame window of 64 ms and a frame step of 24 ms. We use 128 mel filters spaced between 0 kHz and 8 kHz.\n\nWe use each of ResNet18, ResNet34, and ResNet50 architectures as our shared cross-task model trunk with hard parameter sharing. We average pool the final convolutional layer of the ResNet, on top of which we apply dropout and dense layers projecting to each individual task. For the emotional intensity prediction task, we use a linear activation with a negative mean concordance correlation coefficient (meanCCC) loss. For the age classification task, we use a linear activation with a Mean-Squared-Error loss. Finally, for the country of origin prediction task, we use a softmax activation and a categorical cross-entropy loss.\n\nWhen training all three tasks simultaneously, the emotion task under-trained in comparison to the other two tasks. So, we experimented with a task-specific weighted loss. We found the most success with a weight of 5.0 for the emotion task, and a weight of 0.05 for both the age and country of origin tasks. We trained using Adam  (Kingma & Ba, 2014)  with a learning rate of 3e-4 and epsilon of 1e-08 with batch sizes of 64. We found that when training using the meanCCC objective, larger batch sizes stabilized the loss.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Conformer-Based Models",
      "text": "We preprocess the audio by downsampling to 16kHz and normalizing between ±32768.0. The Conformer model takes as input 80-bin log mel spectrogram features that are first processed by a 3-layer 1D convolutional feature encoder, and subsequently processed by two convolutional strides of two that creates a vector time series that has been downsampled by a factor of 4. On this vector time series a sequence of 24 Conformer layers are stacked to produce an encoding for each time point. The Conformer architecture is pre-trained using the Wav2Vec 2.0 contrastive loss  (Baevski et al., 2020)  on the \"speech-heavy\" YT-U dataset  (Zhang et al., 2021) .\n\nIn all our experiments, we freeze the base Conformer model developed in  (Shor et al., 2022)  and use the 1024dimensional embeddings from the 12-th layer of the Conformer as input features. The 12-th layer of the model was shown in previous experiments to be a powerful \"universal embedding\" for a variety of downstream paralinguistic tasks  (Shor et al., 2022) . We extract one embedding for approximately 2.5s non-overlapping window of the audio.\n\nWe trained five multitask learning (MTL) models, both simple fully connected models on the aggregated 1024dimensional (Conformer) features as well as sequence models: (1) Conformer: the features are averaged across the entire audio clip and three dense layers projecting to individual classification/regression tasks (i.e., task layers), (2) Conformer+FC: A single shared fully connected (FC) layer of 128 units on top of the averaged feature, followed by the task layers.\n\n(2) Conformer+LSTM: A single LSTM layer of 128 units on top of the features, followed by the task layers, (3) Conformer+NetVlad: A NetVlad layer  (Arandjelovic et al., 2016)  of 5 clusters followed by the task layers, and (4) Conformer+Autopool: An adaptive pooling layer  (McFee et al., 2018)  which learns a set of weights before aggregating the features, followed by the task layers. Besides LSTM, we chose NetVlad and Autopool as they have shown competitive performance, and fewer trainable parameters than that of LSTMs. We follow the same activation functions and losses as the ResNet models. We trained using Adam  (Kingma & Ba, 2014)  with a learning rate of 3e-4, epsilon of 1e-08 and a batch size of 128.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Results",
      "text": "The results from various MTL models are summarized in Table  1 . All results are reported on the competition defined validation split. Per  (Baird et al., 2022) , we use meanCCC, unweighted average recall (UAR) and mean absolute error (MAE) for evaluating the performance of emotion intensity, country and age prediction respectively. We also report the harmonic mean of the three scores. We refer the reader to  (Baird et al., 2022)  for details on the performance metrics.\n\nThe first row reports the naive baseline for each task, computed as follows: For country prediction, the majority class, \"United States\" was used. The median age of speakers in training set (26.0 years) was used for age prediction. Notably, for the age classification task, using the median age of the speakers in training split outperformed the EXVO competition baseline  (Baird et al., 2022) , as well as all but one of our proposed MTL models. This is likely due to the narrow range of the age values in the validation set. That is, about two-thirds of the speakers in the validation split were 26 ± 4 years old. In addition to MTL, for many models we explored training each task individually (MTL \"No\" in Table  1 ). We did not observe a boost in performance with MTL. In fact, when predicting country of origin, the MTL models performed poorer than their single-task counterparts (e.g., 0.596 vs 0.583 for Conformer-based, 0.528 vs 0.483 for ResNet34). This suggests that the traditional benefits of MTL, like generalization via regularization and feature sharing for representational efficiency, do not seem to be present for the three tasks considered  (Ruder, 2017)  within the context of HUME-VB.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Exploring The Relationship Between Tasks",
      "text": "In order to better understand the relationship between the three tasks, we experiment with using the labels from two of the three tasks (country of origin and age) as inputs to our models. We use permutation testing to compare the performance of a model when given random task labels as input versus when given true task labels as input. Intuitively, if the country and age tasks were useful for perceived emotional intensity prediction, we would expect to see a higher meanCCC score for the true-label model vs the randomlabel model.\n\nConcretely, first, we shuffled the country and age values (with different random seeds for each variable) across all samples in the training set while leaving the emotion intensity values unaltered. We then trained a Conformer-based model on this shuffled data with the country and age variables concatenated to the input Conformer features (i.e., total 1049 dimensional input). We repeat this process 50 times to generate a \"random distribution\" of emotional intensity meanCCC scores from the unaltered validation set. We refer to these models as \"randomly shuffled\" (blue dotted line in Fig.  1 ).\n\nAlthough shuffling the country and age variables across the entire train split maintains the overall distribution, on average 28% and 6% of the samples receive a correct label assignment for country and age respectively after shuffling.\n\nIn order to further probe to what extent country and age information help improve emotion classification prediction, we randomize the country and age assignment such that they do not follow the original distributions in the train split by assigning incorrect country and age labels for all samples. Like before, we train Conformer-based models on the concatenated inputs, and we repeat this 50 times to generate a distribution of meanCCC scores which we refer to \"incorrectly assigned\" (orange line in Fig.  1 )\n\nIn order to generate a distribution of the meanCCC scores when using the true labels, we repeat the experiment in Table 1 50 times without altering the corresponding country, age, or emotion labels. Due to SGD and the mini-batch operations, the multiple runs are not deterministic, and thus only produce an approximate \"true distribution\" of meanCCC scores, shown in green dashed line in Fig.  1 .\n\nWe conducted two-sample t-tests to assess if the performance in the three settings described here was different. We did not observe a significant difference in emotion classification performance for the true distribution vs. randomly shuffled country and age (t = 0.14, p = 0.89). However we did observe that when the distribution of country and age were changed through incorrect assignment, the unaltered models performed better (t = 4.7, p < 0.01). This analysis suggests that the influence of country and age variables on emotion classification is not substantial. This may explain why we see minimal improvement in performance in the MTL models.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Conformer-Based Results",
      "text": "Our Conformer-based models seem to outperform our ResNet models on all tasks, most notably on the country of origin task (0.5955 vs 0.5394 when trained single-task). However, given the success of large self-supervised representations for downstream acoustic tasks  (Hsu et al., 2021; Shor et al., 2022; Baevski et al., 2020) , we were surprised to discover that, on the emotional intensity task, our best pre-trained Conformer-based models only marginally outperformed (0.648 vs 0.645) relatively smaller models like ResNet34 that were trained directly on HUME-VB without any pre-training. While this highlights the suitability of the pre-trained Conformer representations for vocal burst modeling, we expected a larger performance gap; especially considering the amount of data used for pre-training Conformers. We have a few hypotheses for this discrepancy.\n\nFirst, the downstream speech emotion recognition tasks on which the Conformer representations demonstrated SoTA performance-like IEMOCAP  (Busso et al., 2008)  and CREMA-D  (Cao et al., 2014) -were largely speech-centric tasks and do not contain many vocal bursts. It is possible that speech emotion recognition based on prosody may not generalize well to the types of emotional expression found in vocal bursts.\n\nSecond, the Conformer model was pre-trained on a \"speechheavy\" dataset that may not contain vocal bursts like those in HUME-VB. In addition, the manner in which these Conformers were pre-trained may not be optimized for modeling the emotional nuances of vocal bursts. If this is the case, the use of pre-trained Conformers to model vocal bursts remains an open area for study.\n\nFinally, it is possible that the Conformer model representations are significantly more powerful at modeling vocal bursts, but HUME-VB is poor at differentiating the performance of these models. For instance, maybe the dataset is noisy in that the emotion intensity labels are not sufficiently explained by the vocalizations themselves, leading to a noise ceiling (e.g.,  Lage-Castellanos et al. 2019)  in possible model performance.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Competition Submission",
      "text": "Our EXVO-MULTITASK submission consists of an ensemble of two single-task ResNet34 models, one trained directly on the emotion task and the other, on the country of origin task. We found in our experiments that ResNet34 was the best performing ResNet model on both tasks. Surprisingly, for age, we opted to use the median of 26.0, as we were unable to reliably outperform with our trained models. We created our own train-validation split by moving vocalizations from 250 random speakers in the HUME-VB validation set to the train split. Our analysis showed that the overall country, age, and emotional intensity distributions remained largely the same in both train and validation after this change. Our submission achieved a harmonic mean of 0.406 per the MTL competition metric: a 21.24% increase over the baseline.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Caveats",
      "text": "Machine perception models of apparent emotional expres-",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we demonstrated that ResNet models are competitive in predicting the perceived emotional expression of vocal bursts and other subject attributes. An ensemble of ResNet34 models trained on emotion intensity and country of origin comprised our main submission to EXVO-MULTITASK.\n\nWe further demonstrated that pre-trained Conformers that have been explored previously for detecting paralinguistic phenomena also perform quite well on all three tasks. However, when predicting perceived emotional intensity, they performed only marginally better than significantly smaller models trained directly on the competition data.\n\nFinally, we explored the relationship between the three competition tasks, and found that models trained on the emotion task with randomly permuted country and age as input features appear to perform approximately the same as models trained with the true country and age as input features. This suggests that there is surprisingly little interaction between the three tasks in HUME-VB. Understanding who the raters were for labeling perceived emotion and their relationship with the stimuli may help disentangle these findings further.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Acknowledgements",
      "text": "We would like to thank Joel Shor for his guidance in using the conformer model explored in this work.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Gaussian kernel density estimates of emotion classiﬁ-",
      "page": 3
    },
    {
      "caption": "Figure 1: We conducted two-sample t-tests to assess if the perfor-",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": "Abstract"
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": ""
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": "This technical report presents the modeling ap-"
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": "proaches used in our submission to the ICML"
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": "Expressive Vocalizations Workshop & Competi-"
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": "tion multitask track (EXVO-MULTITASK). We"
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": "ﬁrst applied image classiﬁcation models of vari-"
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": "ous sizes on mel-spectrogram representations of"
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": "the vocal bursts, as is standard in sound event de-"
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": "tection literature. Results from these models show"
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": "an increase of 21.24% over the baseline system"
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": "with respect to the harmonic mean of the task met-"
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": "rics, and comprise our team’s main submission to"
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": "the MULTITASK track. We then sought\nto char-"
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": "acterize the headroom in the MULTITASK track"
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": "by applying a large pre-trained Conformer model"
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": "that previously achieved state-of-the-art\nresults"
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": "on paralinguistic tasks like speech emotion recog-"
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": "nition and mask detection. We additionally in-"
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": ""
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": "vestigated the relationship between the sub-tasks"
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": ""
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": "of emotional expression, country of origin, and"
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": ""
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": "age prediction, and discovered that the best per-"
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": ""
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": "forming models are trained as single-task models,"
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": ""
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": "questioning whether\nthe problem truly beneﬁts"
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": ""
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": "from a multitask setting."
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": ""
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": ""
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": ""
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": "1. Introduction"
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": ""
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": "Vocal bursts are an important but often overlooked com-"
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": ""
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": "ponent of paralanguage in affective computing applica-"
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": ""
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": "tions. They are ubiquitous in human communication, and"
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": ""
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": "have been studied as channels for emotional expression"
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": ""
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": "(Hawk et al., 2009; Cowen et al., 2019).\nDue to their"
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": ""
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": "prevalence in natural speech, predicting emotion percep-"
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": ""
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": "tion of vocal bursts is an important complement to in-the-"
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": ""
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": "wild speech emotion recognition. The Hume Vocal Bursts"
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": ""
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": "dataset (HUME-VB) is a recent dataset for studying these"
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": "paralinguistic phenomena covering four globally diverse"
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": ""
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": "1Google Research. Correspondence to: Josh Belanich <josh-"
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": ""
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": "belanich@google.com>."
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": ""
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": "Proceedings of\nthe 39 th International Conference on Machine"
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": "Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy-"
        },
        {
          "Josh Belanich 1 Krishna Somandepalli 1 Brian Eoff 1 Brendan Jou 1": "right 2022 by the author(s)."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 1. Performance comparison of models trained on HUME-VB.": "MTL?"
        },
        {
          "Table 1. Performance comparison of models trained on HUME-VB.": "—"
        },
        {
          "Table 1. Performance comparison of models trained on HUME-VB.": "YES"
        },
        {
          "Table 1. Performance comparison of models trained on HUME-VB.": "YES"
        },
        {
          "Table 1. Performance comparison of models trained on HUME-VB.": "NO"
        },
        {
          "Table 1. Performance comparison of models trained on HUME-VB.": "YES"
        },
        {
          "Table 1. Performance comparison of models trained on HUME-VB.": "NO"
        },
        {
          "Table 1. Performance comparison of models trained on HUME-VB.": "YES"
        },
        {
          "Table 1. Performance comparison of models trained on HUME-VB.": "NO"
        },
        {
          "Table 1. Performance comparison of models trained on HUME-VB.": "YES"
        },
        {
          "Table 1. Performance comparison of models trained on HUME-VB.": "NO"
        },
        {
          "Table 1. Performance comparison of models trained on HUME-VB.": "YES"
        },
        {
          "Table 1. Performance comparison of models trained on HUME-VB.": "YES"
        },
        {
          "Table 1. Performance comparison of models trained on HUME-VB.": "YES"
        },
        {
          "Table 1. Performance comparison of models trained on HUME-VB.": "YES"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "CONFORMER + LSTM\nYES\n0.601": "CONFORMER + NETVLAD\nYES\n0.640",
          "0.536\n4.121\n0.392": "0.594\n3.910\n0.419"
        },
        {
          "CONFORMER + LSTM\nYES\n0.601": "CONFORMER + AUTOPOOL\nYES\n0.652",
          "0.536\n4.121\n0.392": "0.587\n3.954\n0.417"
        },
        {
          "CONFORMER + LSTM\nYES\n0.601": "maximum and minimum values. On these normalized wave-",
          "0.536\n4.121\n0.392": "sequence of 24 Conformer layers are stacked to produce an"
        },
        {
          "CONFORMER + LSTM\nYES\n0.601": "forms, we compute log mel-spectrograms using an FFT",
          "0.536\n4.121\n0.392": "encoding for each time point. The Conformer architecture is"
        },
        {
          "CONFORMER + LSTM\nYES\n0.601": "frame window of 64 ms and a frame step of 24 ms. We use",
          "0.536\n4.121\n0.392": "pre-trained using the Wav2Vec 2.0 contrastive loss (Baevski"
        },
        {
          "CONFORMER + LSTM\nYES\n0.601": "128 mel ﬁlters spaced between 0 kHz and 8 kHz.",
          "0.536\n4.121\n0.392": "et al., 2020) on the “speech-heavy” YT-U dataset (Zhang"
        },
        {
          "CONFORMER + LSTM\nYES\n0.601": "",
          "0.536\n4.121\n0.392": "et al., 2021)."
        },
        {
          "CONFORMER + LSTM\nYES\n0.601": "We use each of ResNet18, ResNet34, and ResNet50 archi-",
          "0.536\n4.121\n0.392": ""
        },
        {
          "CONFORMER + LSTM\nYES\n0.601": "tectures as our shared cross-task model\ntrunk with hard",
          "0.536\n4.121\n0.392": "In all our\nexperiments, we\nfreeze\nthe base Conformer"
        },
        {
          "CONFORMER + LSTM\nYES\n0.601": "parameter sharing. We average pool the ﬁnal convolutional",
          "0.536\n4.121\n0.392": "model developed in (Shor et al., 2022) and use the 1024-"
        },
        {
          "CONFORMER + LSTM\nYES\n0.601": "layer of the ResNet, on top of which we apply dropout and",
          "0.536\n4.121\n0.392": "dimensional embeddings from the 12-th layer of the Con-"
        },
        {
          "CONFORMER + LSTM\nYES\n0.601": "dense layers projecting to each individual task. For the emo-",
          "0.536\n4.121\n0.392": "former as input features. The 12-th layer of the model was"
        },
        {
          "CONFORMER + LSTM\nYES\n0.601": "tional\nintensity prediction task, we use a linear activation",
          "0.536\n4.121\n0.392": "shown in previous experiments to be a powerful “univer-"
        },
        {
          "CONFORMER + LSTM\nYES\n0.601": "with a negative mean concordance correlation coefﬁcient",
          "0.536\n4.121\n0.392": "sal embedding” for a variety of downstream paralinguistic"
        },
        {
          "CONFORMER + LSTM\nYES\n0.601": "(meanCCC) loss. For the age classiﬁcation task, we use a",
          "0.536\n4.121\n0.392": "tasks (Shor et al., 2022). We extract one embedding for"
        },
        {
          "CONFORMER + LSTM\nYES\n0.601": "linear activation with a Mean-Squared-Error loss. Finally,",
          "0.536\n4.121\n0.392": "approximately 2.5s non-overlapping window of the audio."
        },
        {
          "CONFORMER + LSTM\nYES\n0.601": "for the country of origin prediction task, we use a softmax",
          "0.536\n4.121\n0.392": ""
        },
        {
          "CONFORMER + LSTM\nYES\n0.601": "",
          "0.536\n4.121\n0.392": "We trained ﬁve multitask learning (MTL) models, both"
        },
        {
          "CONFORMER + LSTM\nYES\n0.601": "activation and a categorical cross-entropy loss.",
          "0.536\n4.121\n0.392": ""
        },
        {
          "CONFORMER + LSTM\nYES\n0.601": "",
          "0.536\n4.121\n0.392": "simple fully connected models on the aggregated 1024-"
        },
        {
          "CONFORMER + LSTM\nYES\n0.601": "When training all three tasks simultaneously, the emotion",
          "0.536\n4.121\n0.392": "dimensional (Conformer) features as well as sequence mod-"
        },
        {
          "CONFORMER + LSTM\nYES\n0.601": "task under-trained in comparison to the other two tasks. So,",
          "0.536\n4.121\n0.392": "els: (1) Conformer:\nthe features are averaged across the"
        },
        {
          "CONFORMER + LSTM\nYES\n0.601": "we experimented with a task-speciﬁc weighted loss. We",
          "0.536\n4.121\n0.392": "entire audio clip and three dense layers projecting to indi-"
        },
        {
          "CONFORMER + LSTM\nYES\n0.601": "found the most success with a weight of 5.0 for the emotion",
          "0.536\n4.121\n0.392": "vidual classiﬁcation/regression tasks (i.e., task layers), (2)"
        },
        {
          "CONFORMER + LSTM\nYES\n0.601": "task, and a weight of 0.05 for both the age and country of",
          "0.536\n4.121\n0.392": "Conformer+FC: A single shared fully connected (FC)"
        },
        {
          "CONFORMER + LSTM\nYES\n0.601": "origin tasks. We trained using Adam (Kingma & Ba, 2014)",
          "0.536\n4.121\n0.392": "layer of 128 units on top of\nthe\naveraged feature,\nfol-"
        },
        {
          "CONFORMER + LSTM\nYES\n0.601": "with a learning rate of 3e−4 and epsilon of 1e−08 with",
          "0.536\n4.121\n0.392": "lowed by the task layers. (2) Conformer+LSTM: A single"
        },
        {
          "CONFORMER + LSTM\nYES\n0.601": "batch sizes of 64. We found that when training using the",
          "0.536\n4.121\n0.392": "LSTM layer of 128 units on top of the features, followed by"
        },
        {
          "CONFORMER + LSTM\nYES\n0.601": "meanCCC objective, larger batch sizes stabilized the loss.",
          "0.536\n4.121\n0.392": "the task layers, (3) Conformer+NetVlad: A NetVlad"
        },
        {
          "CONFORMER + LSTM\nYES\n0.601": "",
          "0.536\n4.121\n0.392": "layer (Arandjelovic et al., 2016) of 5 clusters followed by the"
        },
        {
          "CONFORMER + LSTM\nYES\n0.601": "2.2. Conformer-based models",
          "0.536\n4.121\n0.392": "task layers, and (4) Conformer+Autopool: An adap-"
        },
        {
          "CONFORMER + LSTM\nYES\n0.601": "",
          "0.536\n4.121\n0.392": "tive pooling layer (McFee et al., 2018) which learns a set of"
        },
        {
          "CONFORMER + LSTM\nYES\n0.601": "We preprocess the audio by downsampling to 16kHz and",
          "0.536\n4.121\n0.392": ""
        },
        {
          "CONFORMER + LSTM\nYES\n0.601": "",
          "0.536\n4.121\n0.392": "weights before aggregating the features, followed by the task"
        },
        {
          "CONFORMER + LSTM\nYES\n0.601": "normalizing between ±32768.0.\nThe Conformer model",
          "0.536\n4.121\n0.392": ""
        },
        {
          "CONFORMER + LSTM\nYES\n0.601": "",
          "0.536\n4.121\n0.392": "layers. Besides LSTM, we chose NetVlad and Autopool as"
        },
        {
          "CONFORMER + LSTM\nYES\n0.601": "takes as input 80-bin log mel spectrogram features that",
          "0.536\n4.121\n0.392": ""
        },
        {
          "CONFORMER + LSTM\nYES\n0.601": "",
          "0.536\n4.121\n0.392": "they have shown competitive performance, and fewer train-"
        },
        {
          "CONFORMER + LSTM\nYES\n0.601": "are ﬁrst processed by a 3-layer 1D convolutional feature",
          "0.536\n4.121\n0.392": ""
        },
        {
          "CONFORMER + LSTM\nYES\n0.601": "",
          "0.536\n4.121\n0.392": "able parameters than that of LSTMs. We follow the same"
        },
        {
          "CONFORMER + LSTM\nYES\n0.601": "encoder, and subsequently processed by two convolutional",
          "0.536\n4.121\n0.392": ""
        },
        {
          "CONFORMER + LSTM\nYES\n0.601": "",
          "0.536\n4.121\n0.392": "activation functions and losses as the ResNet models. We"
        },
        {
          "CONFORMER + LSTM\nYES\n0.601": "strides of two that creates a vector time series that has been",
          "0.536\n4.121\n0.392": ""
        },
        {
          "CONFORMER + LSTM\nYES\n0.601": "",
          "0.536\n4.121\n0.392": "trained using Adam (Kingma & Ba, 2014) with a learning"
        },
        {
          "CONFORMER + LSTM\nYES\n0.601": "downsampled by a factor of 4. On this vector time series a",
          "0.536\n4.121\n0.392": ""
        },
        {
          "CONFORMER + LSTM\nYES\n0.601": "",
          "0.536\n4.121\n0.392": "rate of 3e−4, epsilon of 1e−08 and a batch size of 128."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "3. Results"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "The results from various MTL models are summarized in"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "Table 1. All results are reported on the competition deﬁned"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "validation split. Per (Baird et al., 2022), we use meanCCC,"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "unweighted average recall (UAR) and mean absolute error"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "(MAE) for evaluating the performance of emotion intensity,"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "country and age prediction respectively. We also report"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "the harmonic mean of the three scores. We refer the reader"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "to (Baird et al., 2022) for details on the performance metrics."
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "The ﬁrst row reports the naive baseline for each task, com-"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "puted as follows: For country prediction, the majority class,"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "“United States” was used. The median age of speakers in"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "training set (26.0 years) was used for age prediction. No-"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "tably, for the age classiﬁcation task, using the median age"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "of the speakers in training split outperformed the EXVO"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "competition baseline (Baird et al., 2022), as well as all but"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "one of our proposed MTL models. This is likely due to the"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "narrow range of the age values in the validation set. That is,"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "about two-thirds of the speakers in the validation split were"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "26 ± 4 years old."
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "3.1. Exploring the relationship between tasks"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "Figure 1. Gaussian kernel density estimates of emotion classiﬁ-"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "cation meanCCC scores across 50 trials for permutation testing."
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "Country and age were concatenated to the Conformer features."
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "In addition to MTL, for many models we explored training"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "each task individually (MTL “No” in Table 1). We did not"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "observe a boost\nin performance with MTL. In fact, when"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "predicting country of origin,\nthe MTL models performed"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "poorer\nthan their single-task counterparts (e.g., 0.596 vs"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "0.583 for Conformer-based, 0.528 vs 0.483 for ResNet34)."
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "This suggests that the traditional beneﬁts of MTL, like gen-"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "eralization via regularization and feature sharing for rep-"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "resentational efﬁciency, do not seem to be present for the"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "three tasks considered (Ruder, 2017) within the context of"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "HUME-VB."
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "In order to better understand the relationship between the"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "3.2. Conformer-based results"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "Our Conformer-based models\nseem to outperform our"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "ResNet models on all\ntasks, most notably on the country"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "of origin task (0.5955 vs 0.5394 when trained single-task)."
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "However, given the success of large self-supervised repre-"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "sentations for downstream acoustic tasks (Hsu et al., 2021;"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "Shor et al., 2022; Baevski et al., 2020), we were surprised"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "to discover that, on the emotional\nintensity task, our best"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "pre-trained Conformer-based models only marginally out-"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "performed (0.648 vs 0.645) relatively smaller models like"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "ResNet34 that were trained directly on HUME-VB without"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "any pre-training. While this highlights the suitability of"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "the pre-trained Conformer representations for vocal burst"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "modeling, we expected a larger performance gap; especially"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "considering the amount of data used for pre-training Con-"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "formers. We have a few hypotheses for this discrepancy."
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "First, the downstream speech emotion recognition tasks on"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "which the Conformer representations demonstrated SoTA"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "performance—like IEMOCAP (Busso et al., 2008) and"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "CREMA-D (Cao et al., 2014)—were largely speech-centric"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "tasks and do not contain many vocal bursts.\nIt is possible"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "that speech emotion recognition based on prosody may not"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "generalize well to the types of emotional expression found"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "in vocal bursts."
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "Second, the Conformer model was pre-trained on a “speech-"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "heavy” dataset that may not contain vocal bursts like those"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "in HUME-VB. In addition, the manner in which these Con-"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "formers were pre-trained may not be optimized for modeling"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "the emotional nuances of vocal bursts.\nIf this is the case,"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "the use of pre-trained Conformers to model vocal bursts"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "remains an open area for study."
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "Finally,\nit\nis possible that\nthe Conformer model represen-"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "tations are signiﬁcantly more powerful at modeling vocal"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "bursts, but HUME-VB is poor at differentiating the per-"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "formance of these models. For instance, maybe the dataset"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "is noisy in that\nthe emotion intensity labels are not sufﬁ-"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "ciently explained by the vocalizations themselves, leading"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "to a noise ceiling (e.g., Lage-Castellanos et al. 2019)\nin"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "possible model performance."
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "3.3. Competition Submission"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "Our EXVO-MULTITASK submission consists of an ensem-"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "ble of two single-task ResNet34 models, one trained directly"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "on the emotion task and the other, on the country of origin"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "task. We found in our experiments that ResNet34 was the"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "best performing ResNet model on both tasks. Surprisingly,"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "for age, we opted to use the median of 26.0, as we were"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "unable to reliably outperform with our trained models. We"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "created our own train-validation split by moving vocaliza-"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "tions from 250 random speakers in the HUME-VB vali-"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "dation set\nto the train split. Our analysis showed that\nthe"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "References"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "Arandjelovic, R., Gronat, P., Torii, A., Pajdla, T., and Sivic,"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "J. NetVLAD: CNN architecture for weakly supervised"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "place recognition.\nIn IEEE Conference on Computer"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "Vision and Pattern Recognition (CVPR), pp. 5297–5307,"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "2016."
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "Baevski, A., Zhou, Y., Mohamed, A., and Auli, M. wav2vec"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "2.0: A framework for self-supervised learning of speech"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "representations. Advances in Neural Information Process-"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "ing Systems (NeurIPS), 33:12449–12460, 2020."
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "Baird, A., Tzirakis, P., Gidel, G., Jiralerspong, M., Muller,"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "E. B., Mathewson, K., Schuller, B., Cambria, E., Kelt-"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "ner, D., and Cowen, A.\nThe ICML 2022 Expressive"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "Vocalizations Workshop and Competition: Recognizing,"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "generating, and personalizing vocal bursts. arXiv preprint"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "arXiv:2205.01780, 2022."
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "Busso, C., Bulut, M., Lee, C.-C., Kazemzadeh, A., Mower,"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "E., Kim, S., Chang, J. N., Lee, S., and Narayanan, S. S."
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "IEMOCAP: Interactive emotional dyadic motion capture"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "database.\nLanguage resources and evaluation, 42(4):"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "335–359, 2008."
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "Cao, H., Cooper, D. G., Keutmann, M. K., Gur, R. C.,"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "Nenkova, A., and Verma, R. CREMA-D: Crowd-sourced"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "IEEE Transactions\nemotional multimodal actors dataset."
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "on Affective Computing, 5(4):377–390, 2014."
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "Cowen, A. S., Elfenbein, H. A., Laukka, P., and Keltner, D."
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "Mapping 24 emotions conveyed by brief human vocaliza-"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "tion. American Psychologist, 74(6):698, 2019."
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": ""
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "Elfenbein, H. A. and Ambady, N.\nOn the universality"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "and cultural speciﬁcity of emotion recognition: A meta-"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "analysis. Psychological bulletin, 128(2):203, 2002."
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "Gemmeke,\nJ. F., Ellis, D. P., Freedman, D.,\nJansen, A.,"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "Lawrence, W., Moore, R. C., Plakal, M., and Ritter, M."
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "Audio Set: An ontology and human-labeled dataset for"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "audio events. In IEEE International Conference on Acous-"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "tics, Speech and Signal Processing (ICASSP), pp. 776–"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "780. IEEE, 2017."
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "Gulati, A., Qin, J., Chiu, C.-C., Parmar, N., Zhang, Y., Yu,"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "J., Han, W., Wang, S., Zhang, Z., Wu, Y., et al. Con-"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "former: Convolution-augmented transformer for speech"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "recognition. arXiv preprint arXiv:2005.08100, 2020."
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "Hawk, S. T., Van Kleef, G. A., Fischer, A. H., and Van"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "Der Schalk, J. “Worth a thousand words”: Absolute and"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "relative decoding of nonlinguistic affect vocalizations."
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "Emotion, 9(3):293, 2009."
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "He, K., Zhang, X., Ren, S., and Sun,\nJ.\nDeep residual"
        },
        {
          "Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers": "learning for image recognition.\nIn IEEE Conference on"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "NetVLAD: CNN architecture for weakly supervised place recognition",
      "authors": [
        "R Arandjelovic",
        "P Gronat",
        "A Torii",
        "T Pajdla",
        "J Sivic"
      ],
      "year": "2016",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "2",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems (NeurIPS)"
    },
    {
      "citation_id": "3",
      "title": "The ICML 2022 Expressive Vocalizations Workshop and Competition: Recognizing, generating, and personalizing vocal bursts",
      "authors": [
        "A Baird",
        "P Tzirakis",
        "G Gidel",
        "M Jiralerspong",
        "E Muller",
        "K Mathewson",
        "B Schuller",
        "E Cambria",
        "D Keltner",
        "A Cowen"
      ],
      "year": "2022",
      "venue": "The ICML 2022 Expressive Vocalizations Workshop and Competition: Recognizing, generating, and personalizing vocal bursts",
      "arxiv": "arXiv:2205.01780"
    },
    {
      "citation_id": "4",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "5",
      "title": "CREMA-D: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "6",
      "title": "Mapping 24 emotions conveyed by brief human vocalization",
      "authors": [
        "A Cowen",
        "H Elfenbein",
        "P Laukka",
        "D Keltner"
      ],
      "year": "2019",
      "venue": "American Psychologist"
    },
    {
      "citation_id": "7",
      "title": "On the universality and cultural specificity of emotion recognition: A metaanalysis",
      "authors": [
        "H Elfenbein",
        "N Ambady"
      ],
      "year": "2002",
      "venue": "Psychological bulletin"
    },
    {
      "citation_id": "8",
      "title": "Audio Set: An ontology and human-labeled dataset for audio events",
      "authors": [
        "J Gemmeke",
        "D Ellis",
        "D Freedman",
        "A Jansen",
        "W Lawrence",
        "R Moore",
        "M Plakal",
        "M Ritter"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "9",
      "title": "Conformer: Convolution-augmented transformer for speech recognition",
      "authors": [
        "A Gulati",
        "J Qin",
        "C.-C Chiu",
        "N Parmar",
        "Y Zhang",
        "J Yu",
        "W Han",
        "S Wang",
        "Z Zhang",
        "Y Wu"
      ],
      "year": "2020",
      "venue": "Conformer: Convolution-augmented transformer for speech recognition",
      "arxiv": "arXiv:2005.08100"
    },
    {
      "citation_id": "10",
      "title": "Worth a thousand words\": Absolute and relative decoding of nonlinguistic affect vocalizations",
      "authors": [
        "S Hawk",
        "G Van Kleef",
        "A Fischer",
        "Van Der",
        "J Schalk"
      ],
      "year": "2009",
      "venue": "Worth a thousand words\": Absolute and relative decoding of nonlinguistic affect vocalizations"
    },
    {
      "citation_id": "11",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "12",
      "title": "CNN architectures for largescale audio classification",
      "authors": [
        "S Hershey",
        "S Chaudhuri",
        "D Ellis",
        "J Gemmeke",
        "A Jansen",
        "R Moore",
        "M Plakal",
        "D Platt",
        "R Saurous",
        "B Seybold"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "13",
      "title": "Selfsupervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Hubert"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "14",
      "title": "A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba",
        "Adam"
      ],
      "year": "2014",
      "venue": "A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "15",
      "title": "Methods for computing the maximum performance of computational models of fmri responses",
      "authors": [
        "A Lage-Castellanos",
        "G Valente",
        "E Formisano",
        "F De Martino"
      ],
      "year": "2019",
      "venue": "PLoS computational biology"
    },
    {
      "citation_id": "16",
      "title": "Adaptive pooling operators for weakly labeled sound event detection",
      "authors": [
        "B Mcfee",
        "J Salamon",
        "J Bello"
      ],
      "year": "2018",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "17",
      "title": "An overview of multi-task learning in deep neural networks",
      "authors": [
        "S Ruder"
      ],
      "year": "2017",
      "venue": "An overview of multi-task learning in deep neural networks",
      "arxiv": "arXiv:1706.05098"
    },
    {
      "citation_id": "18",
      "title": "Universal paralinguistic speech representations using selfsupervised conformers",
      "authors": [
        "J Shor",
        "A Jansen",
        "W Han",
        "D Park",
        "Y Zhang"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "19",
      "title": "Exploring the frontier of large-scale semi-supervised learning for automatic speech recognition",
      "authors": [
        "Y Zhang",
        "D Park",
        "W Han",
        "J Qin",
        "A Gulati",
        "J Shor",
        "A Jansen",
        "Y Xu",
        "Y Huang",
        "S Wang"
      ],
      "year": "2021",
      "venue": "Exploring the frontier of large-scale semi-supervised learning for automatic speech recognition",
      "arxiv": "arXiv:2109.13226"
    }
  ]
}