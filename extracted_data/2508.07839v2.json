{
  "paper_id": "2508.07839v2",
  "title": "Touch Speaks, Sound Feels: A Multimodal Approach To Affective And Social Touch From Robots To Humans",
  "published": "2025-08-11T10:45:43Z",
  "authors": [
    "Qiaoqiao Ren",
    "Tony Belpaeme"
  ],
  "keywords": [
    "Tactile interaction",
    "affective computing",
    "sound communication",
    "multi-modal",
    "human-robot interaction",
    "emotion classification",
    "gesture classification"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Affective tactile interaction constitutes a fundamental component of human communication. In natural human-human encounters, touch is seldom experienced in isolation; rather, it is inherently multisensory. Individuals not only perceive the physical sensation of touch but also register the accompanying auditory cues generated through contact. The integration of haptic and auditory information forms a rich and nuanced channel for emotional expression. While extensive research has examined how robots convey emotions through facial expressions and speech, their capacity to communicate social gestures and emotions via touch remains largely underexplored. To address this gap, we developed a multimodal interaction system incorporating a 5×5 grid of 25 vibration motors synchronized with audio playback of touch sound, enabling robots to deliver combined haptic-audio stimuli. In an experiment involving 32 Chinese participants, ten emotions and six social gestures were presented through vibration, sound, or their combination. Participants rated each stimulus on arousal and valence scales. The results revealed that (1) the combined haptic-audio modality significantly enhanced decoding accuracy compared to single modalities; (2) each individual channel-vibration or sound-effectively supported certain emotions recognition, with distinct advantages depending on the emotional expression; and (3) gestures alone were generally insufficient for conveying clearly distinguishable emotions. These findings underscore the importance of multisensory integration in affective human-robot interaction and highlight the complementary roles of haptic and auditory cues in enhancing emotional communication.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "T Ouch is a fundamental aspect of human interaction, serving as a primary channel for communication, emotional expression, and social bonding  [1] . Unlike other sensory modalities such as vision or hearing, touch provides direct and immediate feedback, reinforcing trust, intimacy, and empathy in interpersonal relationships  [2] . Studies in psychology and neuroscience have shown that affective touch, such as a gentle pat or a reassuring squeeze, can reduce stress, enhance wellbeing, and strengthen social bonds  [1] ,  [3] . Early research on human-human affective tactile communication suggests that touch primarily conveys emotional valence and intensity, with Faculty of Engineering and Architecture, IDLab-AIRO, Ghent University -imec, Technologiepark 126, 9052 Gent, Belgium\n\nThe authors acknowledge the use of generative AI in preparing this manuscript. Specifically, Grammarly and GPT-4o were used to assist with grammar checking and enhancing the overall readability. All content was subsequently reviewed and edited by the authors, who accept full responsibility for the final version of the manuscript.\n\ncontext playing a critical role in interpretation  [4] . Hertenstein et al. demonstrated that emotions such as anger, disgust, fear, gratitude, happiness, love, sadness, and sympathy could be recognized solely through touch, with recognition rates between 48% and 83%, whereas self-focused emotions (e.g., embarrassment, envy, pride) were less accurately conveyed  [5] . Studies have shown that haptic behaviour alone is insufficient for distinct emotion recognition, with research identifying 23 different types of tactile behaviours (e.g., hugging, squeezing, shaking) that do not exclusively correspond to a single emotion. For instance, stroking was observed in expressions of sadness, love, and sympathy, suggesting that tactile behaviours must be interpreted alongside other features such as duration and intensity to enhance emotion decoding.  [6] .\n\nAs robots become increasingly integrated into daily life-from caregiving and therapy to collaborative workspaces, establishing effective communication between humans and robots is essential  [7] . While current research in human-robot affective interaction has predominantly emphasized modalities such as speech  [8] , facial expressions  [9] , and body movement  [10] , the tactile channel remains relatively underexplored  [11] . This neglect is partly due to the interdisciplinary challenges inherent in studying affective touch  [4] . However, prior studies have shown that tactile interaction with robots can significantly influence human behaviours, such as risk-taking, stress responses, and attitudes toward the robot  [12] .\n\nRecent research has increasingly explored the role of vibration and sound in emotion expression, demonstrating that vibrotactile signals can effectively convey emotions such as happiness, sadness, and anger  [13] . Specific vibration patterns, defined by variations in frequency, amplitude, and rhythm, have been mapped to distinct emotional states, contributing to affective perception  [14] . Notably, touch is rarely experienced in isolation; it is inherently multisensory  [15] . When interacting with vibrotactile systems, individuals often experience not only the tactile sensations but also the accompanying auditory cues produced by touch. Such as snatching or tapping. Recent research, for instance, has investigated how people decode emotional intent and specific touch gestures from these tactile sounds. This opens new avenues for capturing the quality of social touch  [16] .\n\nThis co-occurrence of tactile and auditory feedback has important implications for affective human-robot interaction (HRI). While computer vision has been widely adopted for emotion recognition, alternative modalities become essential in contexts where visual input is limited or unreliable  [17] . Au-ditory signals, based on their temporal and spectral properties, can independently convey affective qualities such as urgency, warmth, or discomfort  [18] . Moreover, previous studies further indicate that tactile gestures and emotional expressions can be recognized through their auditory counterparts alone  [19] . Despite these insights, the potential of integrating tactile and auditory modalities during human-robot tactile interaction remains underexplored. Most existing studies focus solely on vision and sound  [20]  for emotion expression  [21] , overlooking the communicative richness of multimodal tactile-auditory signals. There is limited understanding of how humans perceive and decode both the combination of touch and contactgenerated sound, as well as each modality's contribution in affective and social contexts in HRI. Addressing this gap is crucial for developing emotionally intelligent robots capable of effective communication when visual cues are limited, and enabling remote interactions with both human partners and virtual agents.\n\nAnother key challenge in HRI is the physical limitation of current humanoid platforms. For example, robots such as Pepper and Nao offer only limited tactile expressivity, as their grippers are typically restricted to basic open-close motions. In contrast, advances in mediated touch technologies-such as haptic gloves, vibration devices, and smart bands-have demonstrated the effectiveness of tactile feedback in virtual and augmented reality, highlighting the potential for richer multisensory integration in HRI  [22] ,  [23] . However, these devices are rarely designed for emotional communication in HRI, and their integration with other sensory modalities remains limited. Furthermore, the perception and interpretation of affective touch can be influenced by the embodiment of the humanoid robot, making it valuable to investigate how such technologies might be adapted for human-robot interaction.\n\nTo address this, we employ mediated tactile technologies, specifically, wearable vibrotactile devices that enable spatially distributed and dynamic feedback. In this context, we developed the VibroSleeve, a wearable haptic interface that delivers spatially distributed vibration to convey different emotions. By integrating vibrotactile and auditory modalities, this study proposes a multimodal framework for conveying affective and touch gestures from a humanoid robot to humans and investigates the following research questions:\n\n1) Does the combination of haptic and auditory cues generated by touch enhance the accuracy of emotion and touch gesture recognition compared to either modality alone? 2) Do participants associate certain touch gestures with specific emotional meanings, even when those gestures were not explicitly designed to convey particular affective responses? 3) What are the individual contributions of the haptic and auditory channels to emotion and touch gestures decoding, and how does their effectiveness vary across different emotional expressions? 4) Which emotions or touch gestures are most frequently misinterpreted, and what patterns of confusion emerge across modalities?",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Relevant Work",
      "text": "Affective communication via mediated touch has gained increasing attention as a means of transmitting emotional information through haptic interfaces, either between humans or between humans and machines  [24] ,  [25] . A central question in this domain concerns how effectively users can recognize and interpret robotic emotions conveyed through mediated touch, particularly in comparison to human-human tactile interaction  [26] .\n\nResearch into tactile interfaces spans a range of technologies, including wearable systems, robotic actuators, and mid-air haptic devices. For instance, Yohanan and MacLean developed the Haptic Creature, a social robot that conveys emotional states through multimodal tactile behaviours such as ear stiffness, breathing, and purring  [27] . In parallel, although force-feedback-based emotion communication remains relatively underexplored, foundational work by Smith and MacLean demonstrated that affective states can be transmitted solely through haptic means  [28] . Similarly, Bailenson et al.\n\nshowed that emotions such as anger, disgust, joy, fear, sadness, and surprise could be communicated using a haptic device, though with modest accuracy, notably lower than that achieved by people communicating emotions through handshakes  [29] . Bonnet et al. extended these findings by integrating haptic feedback with visual avatars, which improved recognition rates for emotions like anger and disgust  [30] .\n\nEmpirical studies have shed light on the physical characteristics of affective touch. Huisman et al. used a pressuresensitive wearable sleeve to analyze how users express emotions through touch. They found that fear, happiness, and anger involved larger contact areas, sympathy was associated with greater force, and temporal patterns varied by emotion-e.g., anger had shorter intervals between touches than happiness or love  [31] . However, the study focused on expression rather than recognition of emotions.\n\nA more targeted subset of wearable haptic devices has emerged to support interpersonal emotional communication. Examples include \"hug shirts\" that use distributed air actuators to simulate affective touch remotely  [32] . Ju et al. explored this domain by mapping touch-generated sounds to vibration amplitude, asking participants to express four emotions through the same gestures. Their system achieved a recognition rate of 57.74% against a 25% chance level  [13] . Bailenson's earlier work examined mediated touch between humans using a 2-DOF force-feedback joystick to express seven emotions, achieving 33.04% recognition accuracy compared to a 14.29% chance level  [29] .\n\nThese studies collectively suggest that vibration, when carefully modulated in terms of frequency, amplitude, spatial location, and rhythm, offers a powerful channel for emotional expression. Dynamic vibrotactile signals enable fine-grained affective communication, making them highly relevant for social human-robot interaction. Moreover, combining vibrotactile feedback with auditory or visual cues can further enhance emotional clarity and social engagement. Emotions are positioned on Russell's circumplex model according to their valence and arousal levels. The intention \"grab attention\" is not an emotion and is therefore placed at the origin (Quadrant 0), representing its status as a communicative act outside the affective dimensions.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iii. Materials & Methods",
      "text": "In this experiment, we propose to collect data on how people interpret tactile stimuli. Participants will be tasked with decoding 10 emotions (anger, fear, disgust, happiness, surprise, sadness, confusion, comfort, calm, and attention) and 6 gestures (hold, pat, tickle, rub, tap, and poke). Before data collection, participants are provided with definitions of all emotions (to ensure consistent interpretations for nonnative English-speaking participants) and touch gestures to ensure they understood and agreed with the given definitions. Participants are asked to rate the arousal, valence, and dominance for each stimulus drawn from our previous study  [33] . Participants receive definitions of arousal and valence, along with a detailed explanation of each session's procedure. The experiment begins only after they fully understand all terms and steps, with the option to ask for further clarification at any time. Afterwards, they are asked to choose a specific emotion or touch gesture for the stimulus. The following sections will detail the equipment used, the setup process, and the specific configurations implemented for data acquisition. According to previous research, we classified the 10 emotions into different arousal and valence quadrants based on Russell's circumplex model as shown in the 1. In addition, some researchers state that surprise is valence-free, while other researchers argue that surprise should be either positive or negative, rather than being described as neither positive nor negative. Here, we put surprise emotions in the negative valence zones.\n\nA. Equipment 1) Vibration sleeve: Many previous vibration-based haptic devices treat each motor as an independent stimulation point, focusing primarily on conveying discrete spatial locations or symbolic information. For example, large-spacing air actuator arrays and garments such as the \"Hug Shirt\"  [32]  have been designed to activate individual actuators to represent specific points or patterns on the body. Other designs use a single vibration motor to examine how vibration characteristics-such as frequency, amplitude, and duration-affect perceived arousal and valence  [13] . While these approaches are effective for delivering spatial cues or studying isolated perceptual parameters, they often lack the continuity and nuanced spatiotemporal dynamics that are characteristic of natural human touch.\n\nIn contrast, our design aims to move towards the relative and continuous qualities of human touch, producing tactile patterns that feel more organic and socially expressive rather than isolated point stimulations. To achieve this, we implemented a vibration device that consists of a 5x5 grid of vibration motors (6.5 centimetres(cm) × 6.5cm in total) embedded in an upper arm sleeve, powered by a 3.3V power supply. Each motor is controlled by a Raspberry Pi using BC557B PNP transistors, which act as switches to regulate current flow to the motors, as shown in Fig.  4b  and Fig.  4a . The system uses pulse-width modulation (PWM) for precise control over the intensity of each motor's vibration, which allows for modulation of the vibration intensity by rapidly switching the transistors on and off at varying duty cycles. By adjusting the duty cycle of the PWM signal, the perceived intensity of each motor's vibration can be finely controlled, allowing for a dynamic range of vibration patterns.\n\n2) Headphone: To prevent participants from hearing the sound of the vibrations during the experiment, we required them to wear active noise-cancelling headphones throughout the session to block out noise or any auditory cues generated by the motor vibrations.\n\n3) Questionnaire: We collected subjective feedback through a questionnaire with four questions aimed at understanding the challenge of interpreting affective touch expression and the different modalities' contribution to emotion decoding. As shown below:\n\n1) Difficulty in Decoding Emotions: We asked participants which emotions or intentions they find most challenging to interpret through a single multiple-choice question: \"Which emotion was the most difficult to decode? 2) Confidence in Decoding Emotions Across Modalities:\n\nParticipants rated their confidence in decoding emotions through different sensory channels: \"How confident were you in decoding emotions using vibrations?\"; \"How confident were you in decoding emotions using sound?\"; and \"How confident were you in decoding emotions using both sound and vibration?\" (Likert scale: 1 = Not confident at all, 5 = Very confident). 3) Primary Sensory Channel Preference: Participants were asked to identify the sensory channel they primarily relied on to decode emotions. Options included the sound modality, the touch modality, or a combination of both (multiple choice). 4) Emotion Decoding Strategy: Participants were asked to indicate the specific strategies they used to decode emotions, with the option to select multiple strategies. These included: relying on vibration characteristics (e.g., intensity, magnitude, frequency, speed), interpreting sound cues such as volume or pitch, assessing the gesture's speed or rhythm, using a combination of these features, recognizing the type of touch gesture experienced, selecting all of the above, or relying purely on intuitive or gut feeling. 5) Confusing Emotions or Gestures: An optional question asked participants if there were any particular emotions or gestures that were especially confusing.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Dataset",
      "text": "We conducted data collection experiments to separately record touch sounds associated with gestures and those associated with emotions, as described in  [33] . Participants first expressed six distinct gestures on the left arm of the Pepper robot and subsequently expressed ten different emotions through similar touch interactions.The sounds generated by these interactions were recorded, resulting in a dataset of 84 audio clips (28 participants × 3 rounds), sampled at 44.1 kHz. Examples of the recordings for \"Anger\" and \"Comfort\" are shown in Fig.  5  and Fig.  6 , respectively. In addition, 84 tactile files were collected in CSV format at 45 Hz, each with a duration of 10 s, covering both gestures and emotions. Examples of combined auditory and haptic cue visualizations, along with the recordings, are available at the GitHub repository: 1  .\n\nTo analyse this data, we applied the k-means clustering algorithm, grouping the audio features and tactile features in  [33]  into three clusters based on their acoustic features. We identified the most populated cluster, representing the largest grouping of participants, as the most characteristic. Within this cluster, we calculated the Euclidean distance of each participant's expression features to the cluster centroid (average feature values) to assess each clip's representativeness. The three participants' IDs with the smallest distances to the centroid were selected as the most representative, and from these, we chose one participant's data among the three to serve as a representative stimulus for all participants for each emotion. Then we translate the tactile files to vibrations, each vibration stimulus lasts for 10 seconds as well. By adjusting the duty cycle of the PWM signal, the perceived intensity of each motor's vibration can be finely controlled, allowing the replication of tactile dynamic patterns and converting them to vibration patterns.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Iv. Methods",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Participants",
      "text": "Thirty-two Chinese participants (17 female, 15 male; M = 27.8, SD = 2.3 years) participated in the experiment. Participants were recruited from a similar cultural background to ensure consistency in emotional interpretation. The study complied with ethical guidelines established by Ghent University, and informed consent was obtained from all. The entire experiment lasts for one hour, and each participant receives 10 euros as compensation for their participation. In addition, to maintain engagement, the participant with the highest score receives a 30 euro reward. The ranking is presented anonymously to all participants.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Procedures",
      "text": "Each participant underwent four phases as shown in Fig.  3  and Fig.  2 ; in each phase, they were asked to decode emotions  for each stimulus by Self Assessment Manikins (SAM) as shown in Fig.  7 .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Experimental Design",
      "text": "To explore emotion decoding via touch-based expression and reveal how consistently emotions and gestures are expressed across different individuals, we set up a data collection experiment, which can be seen in Fig.  4 . The experiment followed a within-subject design using three sessions, including a pre-session, a touch gesture decoding session, and an emotion decoding session. During each session, participants are allowed to replay the stimulus multiple times.\n\n1) Pre-Session: In this preliminary session, participants are asked if they can feel the vibration to ensure it meets • Touch Modality: Participants decode emotions conveyed through vibration, rating arousal and valence for each stimulus, and forced-choice selecting the emotion from a list of ten options. • Sound Modality: Participants decode emotions conveyed through sound, rate the arousal and valence for each stimulus, and identify the emotion from the provided list. During this session, the sound level for each stimulus is played at the same level. • Combined Touch and Sound Modality: Participants Fig.  7 : SAM for valence and arousal, the first row is valence and the second is arousal. decode emotions using synchronized sound and vibration, rate arousal and valence, and identify the emotion from the list of ten options.\n\n3) Study 2: Touch Gestures Decoding Session:\n\n• Touch Modality: Participants decode touch gestures using vibration alone, rating the arousal and valence for each stimulus, and identifying the gesture from a list of provided options. • Sound Modality: Participants decode touch gestures using sound, rate arousal and valence for each stimulus, and identify the gesture from the provided options. During this session, the sound level for each stimulus is at the same level. • Combined Touch and Sound Modality: Participants decode touch gestures using synchronized sound and vibration, rate the arousal and valence, and select the corresponding gesture from the provided list.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "V. Results And Analysis",
      "text": "A. Sound modality 1) Emotion decoding: We calculated the average arousal and valence ratings provided by participants for various emotional stimuli. The results for the 10 emotions are summarized in Table . I, and their distribution within Russell's circumplex model is depicted in Figure  1 . Participants placed \"Happiness\" in the high-arousal, positive-valence quadrant. Emotions such as \"Surprise\" and \"Anger\" were placed in the higharousal, negative-valence quadrant. \"Confusion\", \"Sadness\", \"Calming\", \"Fear\", and \"Attention\" were associated with the low-arousal, negative-valence quadrant; while \"Comfort\" and \"Disgust\" were classified in the low-arousal, positive-valence quadrant.\n\nThis classification generally aligns with the emotion categorization described in Russell's circumplex model and findings from previous studies  [34] ,  [35] . However, discrepancies were observed. For instance, \"Calming\" is typically expected to fall in the low-arousal, positive-valence quadrant but was misclassified into the low-arousal, negative-valence quadrant. Participants mentioned they were confused \"Comfort\", \"Calming\", and \"Sadness\" when they tried to decode emotions. Similarly, \"Fear\" was misclassified into the low-arousal, negative-valence quadrant. A possible explanation is that participants expressed fear by tightly grasping the robot's arm and shaking it rapidly, which produced minimal sound but a large force, which might lead to the wrong classification only based on the sound channel.\n\nAdditionally, \"Disgust\" was misclassified into the lowarousal, positive-valence quadrant. Participants expressed difficulty decoding \"Disgust\". Observational data indicate that participants often used a single finger to push the robot away multiple times. Since this action was performed slowly, the resulting sound was perceived as gentle and deliberate, despite the actual force being significant. These findings highlight potential challenges in accurately categorizing emotions based on arousal and valence, particularly when relying on multisensory input.\n\nAs our findings showed, participants often misclassified emotions that fell within the same emotional quadrant. To better understand this pattern, we analyzed decoding accuracy across four quadrants plus origin based on arousal and valence dimensions. The results indicated that participants were generally able to distinguish emotions across different quadrants, suggesting that the sound modality effectively conveys differences in arousal and valence. However, decoding became more difficult when emotions were closer in affective space-particularly those sharing the same arousal and valence levels. Interestingly, decoding accuracy for emotions in Quadrant 2 (high arousal, negative valence) was higher than in other quadrants. One possible explanation is that high-arousal negative stimuli tend to have distinctive acoustic features, such as large sound amplitude or high frequency, which participants may associate more consistently with this emotional zone.\n\nThe pair decoding results in Fig.  11  showed that 26 out of 32 participants correctly identified \"Anger\", making it the most accurately decoded emotion. This was followed by \"Attention\", which 14 participants decoded correctly. We tested whether participants' emotion decoding performance was significantly above chance (10%) for sound modality using binomial tests. The decoding accuracy was 31.6% across 10 emotions, above chance, p < 0.01, 95% CI [0.273, 1].\n\n2) Touch gesture decoding: As shown in the Table. III and Fig.  12 , the participants' touch gesture decoding accuracy is 66.1%; specifically, the gesture that participants found easiest to decode was \"Tickle\", followed by \"Rub\". In contrast, \"Poke\", \"Pat\" and \"Tap\" were more frequently misclassified and often confused with each other. A likely reason is that these gestures produce acoustically similar, short percussive TABLE I: Arousal and valence of different emotions and decoding accuracy(%) based on sound modality, decoding accuracy for each emotion was compared against the chance level (10%, corresponding to random guessing among ten classes). A one-sample binomial test was used to determine whether the observed decoding accuracy was significantly higher than chance.   events with overlapping temporal rhythms and spectral envelopes, which are difficult to differentiate using audio alone. By comparison, \"Tickle\" (irregular, high-frequency friction) and \"Rub\" (sustained friction) provide more distinctive acoustic cues. Overall, all gestures were decodable from sound, and gesture decoding outperformed emotion decoding.\n\nRegarding the affective responses to touch gestures based on the sound modality (as shown in the Figure . 10 and Table. III), the average valence ratings across all gestures were generally close to neutral (around 5). However, arousal levels varied more distinctly between different gestures, suggesting that arousal may play a more discriminative role. Participants tended to perceive \"Pat\" as slightly positive and moderately arousing, while \"Hold\" and \"Poke\" were associated with low arousal and slightly positive valence. In contrast, \"Rub\", \"Tap\" and \"Tickle\" were rated as higher in arousal and also slightly more positive in valence.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "B. Touch Modality",
      "text": "We averaged the participants' arousal and valence ratings for different emotional stimuli. The results for the 10 emotions  Participants placed \"Happiness\" and \"Surprise\" in the higharousal, positive-valence quadrant. \"Disgust, \"Fear\", \"Calming\", \"Confusion\" and \"Anger\" were categorized in the higharousal, negative-valence quadrant. \"Sadness\", \"Attention\" and \"Comfort\" were assigned to the low-arousal, negative-valence quadrant, while no emotion was classified in the low-arousal,   positive-valence quadrant. Some decoding classifications are consistent with the emotion categorization described in Russell's circumplex model and previous research, while some emotions like \"Calming\" and \"Confusion\" are misclassified to high arousal and negative quadrant, plus \"'Attention\" is misclassified to low arousal, and negative valence quadrant, which might be some information missed when transferring the information, for example, participants tend to hold the robot's arm to convey \"Calming\", however, it leads to strong vibrations, which might mistakenly convey conflicting cues. Additionally, \"Anger\" was frequently placed near the neutral zone, which might be due to hardware limitations, such as the size of the vibrosleeves and the motor updating frequency. Participants used fast, high-frequency pouching gestures to express anger, but the robot's vibration motors require a certain time to start and stop. This latency likely resulted in short and weak vibration bursts, reducing the perceived intensity of the emotion and potentially leading to its misclassification.\n\nThe overall decoding accuracy across the 10 emotions is 25%, and the pair decoding results in Fig.  8  showed that 13 out of 32 participants correctly identified \"Fear\", making it the most accurately decoded emotion. This was followed by \"Happiness\" and \"Attention\", which both were correctly decoded by 11 participants. The decoding performance was notably lower than that of the sound modality, which may be attributed to individual differences in tactile sensitivity. While the human auditory system is finely tuned to detect a wide range of sound frequencies, the skin's sensitivity to vibrations is more limited and varies across individuals and body regions [?]. These sensory differences likely contribute to the lower accuracy observed in the touch-based condition. Despite this, participants were still able to successfully recognize emotions across different arousal-valence quadrants using the touch modality, as shown in the  touch modality, particularly when the emotions differ in their overall arousal.\n\nAs shown in Table . V and Fig.  12 , the gesture most easily decoded by participants was \"Rub\", followed by \"Tap\" and \"Poke\". Additionally, \"Poke\", \"Pat\", and \"Tap\" were often misclassified and confused with one another. This is understandable, as \"Poke\" and \"Tap\" may share a similar contact area, while \"Pat\", \"Poke\", and \"Tap\" exhibit comparable movement rhythms. Similarly, \"Hold\" and \"Rub\" could be mistaken for each other due to their overlapping contact areas. The overall decoding accuracy is 42.2%.\n\nOne possible explanation for this confusion is the limited size of the designed sleeve's touch-sensitive area. Although the 25-motor setup aims to facilitate relative haptic perception, the small touch area may hinder participants from distinguishing significant differences between certain gestures. For instance, while \"Poke\" involves a smaller contact area compared to \"Pat\", their rhythmic patterns can be similar, as is the case with \"Tap\". This overlap in tactile cues may lead to confusion and reduce the fidelity of gesture recognition when conveying gestures to participants. Overall, participants successfully decoded all the touch gestures, and touch gesture decoding performed better than emotion decoding.\n\nIn terms of affective responses to touch gestures in the touch modality, as shown in Fig.  13  and Table.  V, gestures such as \"Hold\", \"Poke\", \"Rub\", and \"Tap\" were predominantly associated with high arousal and positive valence. Interestingly, \"Pat\" was classified as high arousal but neutral in valence, while \"Tickle\" contrary to expectations, was placed in the low arousal, positive valence quadrant. This classification differs notably from the sound modality, where the same gestures tended to elicit higher arousal ratings. One possible explanation for this discrepancy is that tactile perception focuses more on contact dynamics and pressure, while auditory cues (such as rhythm and intensity of impact sounds) may amplify the  perceived energy or emotional expressiveness of the gesture, leading to higher arousal interpretations. Additionally, the lack of accompanying sound in the touch modality may soften the perceived intensity of certain gestures, such as \"Tickle\", making them feel more soothing than stimulating.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "C. Combined Sound And Touch Modality",
      "text": "We averaged participants' arousal and valence ratings for different emotional stimuli during the Combined Touch and Sound Modality session. The results for the 10 emotions are shown in Table . VI. Figure . 14 illustrates the distribution of the 10 emotions within Russell's circumplex model.\n\nParticipants placed \"Happiness\" and \"Surprise\" in the higharousal, positive-valence quadrant. \"Disgust\", \"Fear\", \"Surprise\", and \"Anger\" were categorized in the high-arousal, negative-valence quadrant. \"Confusion\" and \"Sadness\" were assigned to the low-arousal, negative-valence quadrant, while \"Comfort\" and \"Calming\" were grouped in the low-arousal, positive-valence quadrant. This classification is consistent with the emotion categorization described in Russell's circumplex model and previous research  [34] ,  [35] .\n\nThe overall decoding accuracy across the 10 emotions is 44.1%, and the pair decoding results in Fig.  14  showed that 27 out of 32 participants correctly identified \"Anger\", making it the most accurately decoded emotion. This was followed by \"Fear\", which 21 participants decoded correctly, and \"Attention\", which was decoded correctly by 17 participants.\n\nAs shown in Table . V and Fig.  15 , the gesture that participants found easiest to decode was \"Tap\", followed by  \"Rub\" and \"Pat\". In general, the gesture decoding accuracy is quite good, except \"Poke\" is sometimes misclassified as \"Tickle\". The participants' touch gesture decoding accuracy is 66.1%. Overall, participants decoded all the touch gestures significantly higher than the chance level, and touch gesture decoding performed better than emotion decoding. For the multimodal condition, the affective responses to touch gestures-presented in Fig.  16  and Table . VII-revealed distinct patterns across the arousal-valence space. Specifically, \"Hold\" and \"Poke\" were classified in the high arousal, negative valence quadrant, suggesting that these gestures were perceived as intense but emotionally aversive or uncomfortable when accompanied by both tactile and auditory cues. In contrast, \"Pat\", \"Rub\", \"Tap\", and \"Tickle\" were positioned in the high arousal, positive valence quadrant, indicating that these gestures were interpreted as pleasant.\n\nThis distribution highlights the modulating effect of multimodal feedback. The addition of sound appears to sharpen emotional interpretation, possibly by reinforcing or clarifying the intent behind each gesture. For example, when we try to convey \"Hold\" gesture through vibration, as this gesture trigger more vibration motors than other gesture, like \"Poke\" or \"Tap\", and the amplitude is high, people might interpret it as negative valence and high arousal; however, the generated sound might be very little, so it might cause some conflict. On the contrary, \"Poke\", typically brief and neutral, may be interpreted as more abrupt or even aggressive when its associated sound is amplified. On the other hand, rhythmic and familiar gestures like \"Pat\" or \"Tap\" benefit from congruent  audio-tactile feedback, enhancing their positive affective interpretation. Overall, these findings underscore the importance of sensory synergy in shaping emotional responses and emphasize the need to carefully calibrate multimodal cues when designing robot touch behaviours that are intended to convey specific social or emotional meanings.\n\nD. Qualitative results and analysis 1) Questionnaire: 50% of participants reported that \"Disgust\" was the most difficult emotion to decode, followed by \"Confusion\" (21.98%), and \"Happiness\" (9.4%). In contrast, none of the participants selected \"Grab attention\" or \"Anger\" as difficult to interpret, which aligns with their decoding results and suggests that participants were able to identify consistent cues associated with these two emotions.\n\nAs for emotion decoding from the touch modality, none of the participants reported being very confident. Approximately 53.1% of participants rated their confidence as 2 out of 5, while 18.8% indicated no confidence at all (1 out of 5). In comparison, participants expressed greater confidence in decoding emotions from the sound modality, with the average confidence rating being notably higher. Specifically, 72.7% of participants rated their confidence as 4 out of 5.\n\nRegarding the strategies used for emotion decoding, 40.6% of participants reported relying on a combination of vibration features (e.g., intensity, magnitude, frequency, speed), sound characteristics (e.g., intensity or pitch), and the speed or rhythm of the gesture. Additionally, 37.5% of participants indicated using a broader set of cues, including vibration and sound features, gesture dynamics, and the specific touch gestures they perceived.\n\nParticipants' responses revealed notable subjective confusion when decoding the emotions conveyed through touch and sound modality. Rather than offering consistent interpretations, many participants explicitly reported difficulty distinguishing between specific emotions or gestures, indicating that their perception of affective cues was often ambiguous or overlapping. Some participants mentioned being confused between emotions such as \"Comfort\", \"Sadness\", \"Calmness\", and \"Confusion\". These emotions all belong to the low-arousal emotions. One participant reported emotions like \"Anger\" and \"Disgust\" are also easily misclassified, and those two emotions are high-arousal emotions. This suggests that participants experienced difficulty in differentiating between emotional expressions that share similar valence or arousal levels, particularly those with low arousal and soft tactile features. For example, a slow, gentle vibration might be interpreted as calming by one participant and melancholic by another, depending on their personal emotional framework. Some participants explicitly noted confusions like \"Disgust\" with \"Anger\" or \"Surprise\" with \"Calming\". These confusions highlight how similar tactile characteristics across certain emotions affected participants' ability to decode them accurately. Although participants recognized these emotions as distinct, pinpointing explicit tactile differences between them remained challenging.\n\nIn addition to confusion on emotion decoding, some participants reported uncertainty between similar gestures, such as tap and pat. These gestures may share similar tactile characteristics, such as brief contact or repetitive motion, leading to inconsistent emotional interpretations. As a result, the perceived emotion may be influenced more by gesture familiarity or personal associations than by any clear expressive intent. The participant feedback underscores that emotion decoding in this context is not a purely objective or consistent process. Individual experiences, cultural backgrounds, and expectations all shape how people interpret affective touch and sound. These reports of confusion highlight that some tactile signals may lack the distinctiveness or clarity needed to reliably convey specific emotional states across different individuals.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Vi. Discussion",
      "text": "This study investigated how people interpret emotions and touch gestures in human-robot interaction, and whether combining auditory and haptic cues improves recognition accuracy compared to using each modality alone. Our results clearly show that participants were able to recognize both emotions and gestures using all three conditions-haptic, auditory, and their combination-though with varying levels of accuracy and modality-specific strengths.\n\nEmotion recognition was most effective in the multimodal condition, where participants achieved an accuracy of 44.1%, significantly outperforming the sound-only (31.6%) and touchonly (25%) conditions. The results highlight the complementary nature of auditory and haptic channels. Auditory cues provide high-resolution temporal and spectral features, such as pitch, loudness, and rhythm, which humans are evolutionarily tuned to interpret for emotional content  [36] . Haptic cues, in contrast, deliver spatial, force-based, and rhythmic feedback directly to the body, offering a more embodied and visceral experience of affect  [33] .\n\nImportantly, each single modality also showed unique advantages, which can be seen also in  [37] . The auditory channel enabled higher confidence ratings from participants and generally higher emotion recognition accuracy. It was particularly effective for high-arousal emotions such as Anger, Fear, and Surprise, likely due to their characteristic sound dynamics (e.g., sharp, loud, or abrupt auditory features). On the other hand, the haptic channel showed greater success in conveying affective nuances for gestures and for emotions involving bodily proximity, such as \"Attention\" or \"Fear\". While decoding accuracy was lower than with sound, haptic feedback was still effective-especially when spatial vibration patterns or rhythms were clearly defined.\n\nGesture recognition benefited even more strongly from multimodal input. In the combined condition, gestures such as \"Tap\", \"Rub\", and \"Hold\" were identified with accuracies up to 84.4%, whereas even in the touch-only condition, recognition remained above chance. This suggests that motor-based pattern recognition-especially with consistent rhythm and spatial placement-is intuitively accessible through the tactile sense. However, certain emotional categories-especially those within similar valence-arousal zones (e.g., \"Calming\", \"Comfort\", \"Sadness\", \"Confusion\") -remained challenging across all modalities. These results confirm that affective touch decoding is subjective, context-sensitive, and vulnerable to overlap when relying on a single sensory channel. Multimodal feedback, by reinforcing emotional signals through redundancy and cross-modal convergence, helps disambiguate such cases.\n\nThese results reinforce a key insight: emotional communication through touch is deeply subjective, especially in the absence of contextual or visual cues. Participants frequently reported difficulty distinguishing between low-arousal emotions such as \"Comfort\", \"Calming\", \"Sadness\", and \"Confusion\", which tend to share overlapping tactile qualities (e.g., gentle pressure, slow rhythm). Such perceptual overlap highlights a fundamental challenge in affective robotics: while human touch conveys rich emotional meaning in interpersonal interaction, it is highly context-dependent, influenced by cultural background, personal associations, and situational interpretation  [38] .\n\nOur analysis of the individual contributions of haptic and auditory channels to emotion and gesture decoding, and how their effectiveness varies by expression, reveals notable differences in user experience and modality performance. Participants reported significantly higher confidence in decoding emotions via the sound modality, with 72.7% rating their confidence at 4 out of 5. In contrast, tactile decoding was met with much more uncertainty: 53.1% of participants rated their confidence as only 2 out of 5, and 18.8% reported no confidence at all. This discrepancy likely reflects the human auditory system's superior temporal and spectral resolution, which makes it easier to detect emotional nuances through pitch, volume, and rhythm. Tactile sensitivity, on the other hand, varies widely between individuals and across differ-ent body areas, making touch-based emotional interpretation more subjective and inconsistent  [39] , which might extend to human-robot interaction as well.\n\nDespite this, the haptic channel still showed unique strengths for certain emotions. Fear and Happiness, for instance, were recognized with relatively high accuracy in the touch-only condition (40.6% and 34.4%, respectively). Participants reported relying on tactile features such as vibration frequency, intensity, and gesture speed to make judgments, indicating that touch, when well-calibrated, can effectively convey emotional urgency or positivity. However, some higharousal emotions like \"Anger\" and \"Disgust\" were frequently misinterpreted across all modalities. This may be due to overlapping tactile features, such as strong or fast vibrations, which fail to distinguish between different negative affective states.\n\nAs for the confusion between different emotions. Emotions like \"Disgust\", \"Confusion\", and \"Sadness\" were among the most difficult for participants to interpret. Over half reported that More than half of the participants reported that \"Disgust\" was the most difficult emotion to decode. This difficulty likely stems from the subtle or ambiguous tactile and auditory cues generated by the chosen gesture (e.g., a slow pushing-away movement). Moreover, participants held different interpretations of how \"Disgust\" should be expressed through touch. Some felt that the robot should not touch at all when conveying this emotion, while others expected a clear pushing-away gesture. Still others considered it similar to \"Anger\", anticipating strong and intense vibrations. These divergent expectations highlight a lack of consistency among participants in how they conceptualize tactile expressions of Disgust, which may explain the confusion. \"Confusion\" also frequently overlapped with \"Comfort\", \"Calming\", and \"Sadness\", highlighting how emotions within the same low-arousal, similar-valence space are particularly prone to perceptual blending.\n\nIn terms of gestures, \"Tap\", \"Pat\" and \"Poke\" were often confused, as they all involved brief, rhythmic contact with overlapping spatial patterns. The limited surface area of the vibration sleeve may have further contributed to the difficulty in distinguishing between these gestures. These findings suggest that fine distinctions in gesture dynamics are hard to perceive unless the haptic feedback system has high spatial resolution and a broader range of expression. Improving the granularity and richness of tactile feedback-such as by varying motor location, amplitude contrast, or rhythm complexity-may help reduce confusion and improve decoding accuracy across both emotional and gestural domains.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "A. Limitations",
      "text": "While this study provides valuable insights into multimodal emotion and gesture recognition in human-robot interaction, several limitations should be acknowledged:\n\nAll participants were recruited from a single cultural background (Chinese), which may limit the generalizability of the findings. Cultural norms and expectations around touch and emotional expression vary significantly, and future studies should include more diverse populations to explore crosscultural differences in affective interpretation.\n\nThe study focused on ten emotions and six predefined social touch gestures. While this set captures a meaningful range of affective expressions, it does not cover the full complexity of human emotional and social communication. Emotions like pride, embarrassment, or empathy, which may involve more subtle cues, were not included.\n\nThe tactile feedback was delivered using a 5×5 vibration motor sleeve with limited surface area and resolution. As participants noted, the small contact area may have made it difficult to distinguish between similar gestures (e.g., tap vs. pat), potentially lowering decoding accuracy. Additionally, the latency and limited frequency range of the vibration motors may have dampened the expressivity of certain emotional signals, such as anger or urgency.\n\nIn real-life interactions, emotional expressions are often interpreted within rich social, visual, and contextual frameworks. Our study isolated sound and touch stimuli from contextual cues to focus on modality-specific decoding, but this may have made some emotions-particularly low-arousal or ambiguous ones-more difficult to interpret. Integrating situational or embodied context in future experiments could provide a more ecologically valid understanding of multimodal affect decoding.\n\nWhile participants were provided with standardized emotion and gesture definitions, their interpretations and decoding strategies were inevitably influenced by individual experiences, expectations, and preferences. As our qualitative results suggest, emotional perception is inherently subjective, and some participants relied on intuition or gut feeling rather than systematic decoding strategies.\n\nAll participants received the same stimuli derived from selected representative samples. Although this ensured consistency in presentation, it may have limited the variability and richness of expression seen in more naturalistic or spontaneous interactions. Future studies could explore participant-generated stimuli or real-time robot expression to increase ecological relevance.\n\nDuring the experiment, participants rarely looked at the robot, even though it was present in the room. One possible explanation is that the study focused on the sound and touch modalities, and participants did not have direct communication with the robot. However, the influence of the robot's appearance remains unexplored. Similarly, the potential effect of the robot's facial expressions has not been addressed; in this experiment, we used a neutral facial expression. Since this research focused exclusively on sound and touch modalities, the interpretation of results might differ if a visual modality were also involved.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Vii. Conclusion",
      "text": "This study explored how humans decode emotions and social touch gestures conveyed by a robot through auditory, haptic, and combined modalities. Our results show that while each modality contributes uniquely to affective interpretation, multimodal integration significantly enhances decoding accuracy, especially for complex emotional expressions. Participants could reliably distinguish emotions across different arousal-valence zones, but decoding became more difficult when emotions shared similar affective profiles. Gesture recognition was generally more accurate than emotion recognition, particularly in the combined modality. These findings highlight the value of multisensory feedback in human-robot interaction and support the design of socially expressive robots that leverage both touch and sound for more intuitive and emotionally resonant communication.",
      "page_start": 12,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Emotions distribution based on Russell’s circumplex",
      "page": 3
    },
    {
      "caption": "Figure 4: b and Fig. 4a. The system uses pulse-width",
      "page": 3
    },
    {
      "caption": "Figure 2: Decoding performance across different modalities. The figure compares unimodal (tactile or auditory) and multimodal",
      "page": 4
    },
    {
      "caption": "Figure 3: Mediate touch from the robot.",
      "page": 4
    },
    {
      "caption": "Figure 5: and Fig. 6, respectively. In addition, 84 tactile",
      "page": 4
    },
    {
      "caption": "Figure 3: and Fig. 2; in each phase, they were asked to decode emotions",
      "page": 4
    },
    {
      "caption": "Figure 4: Vibration sleeves.",
      "page": 5
    },
    {
      "caption": "Figure 5: Top: Waveform of “Anger” audio, showing the raw",
      "page": 5
    },
    {
      "caption": "Figure 7: C. Experimental design",
      "page": 5
    },
    {
      "caption": "Figure 4: The experiment",
      "page": 5
    },
    {
      "caption": "Figure 6: Top: Waveform of the “Comfort” audio excerpt in",
      "page": 5
    },
    {
      "caption": "Figure 7: SAM for valence and arousal, the first row is valence",
      "page": 6
    },
    {
      "caption": "Figure 1: Participants placed “Happi-",
      "page": 6
    },
    {
      "caption": "Figure 8: Confusion matrix for emotions (sound modality).",
      "page": 6
    },
    {
      "caption": "Figure 11: showed that 26 out",
      "page": 6
    },
    {
      "caption": "Figure 12: , the participants’ touch gesture decoding accuracy is",
      "page": 6
    },
    {
      "caption": "Figure 9: Confusion matrix for touch gestures (sound modality).",
      "page": 7
    },
    {
      "caption": "Figure 10: Scatter plots for affective response of touch gestures",
      "page": 7
    },
    {
      "caption": "Figure 11: Confusion matrix for emotions (touch modality).",
      "page": 7
    },
    {
      "caption": "Figure 12: Confusion matrix for touch gestures (touch modality).",
      "page": 8
    },
    {
      "caption": "Figure 8: showed that",
      "page": 8
    },
    {
      "caption": "Figure 12: , the gesture most easily",
      "page": 8
    },
    {
      "caption": "Figure 13: and Table. V, gestures such as",
      "page": 8
    },
    {
      "caption": "Figure 13: Scatter plots for affective response of touch gestures",
      "page": 9
    },
    {
      "caption": "Figure 14: Confusion matrix for emotions (sound and touch",
      "page": 9
    },
    {
      "caption": "Figure 14: showed that",
      "page": 9
    },
    {
      "caption": "Figure 15: , the gesture that par-",
      "page": 9
    },
    {
      "caption": "Figure 15: Confusion matrix for gestures (combined sound and",
      "page": 9
    },
    {
      "caption": "Figure 16: Scatter plots for affective response of touch gestures",
      "page": 9
    },
    {
      "caption": "Figure 16: and Table. VII—revealed",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2": "522173001\n9701920301\n1130001\n00\n325231\n311\n03217013\n3321\n11142\n3303317624\n2303118617\n0604313546\n3302212478\n13\n14\n11\n26\n11"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "T Field"
      ],
      "year": "2014",
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Interpersonal affective touch in a virtual world: feeling the social presence of others to overcome loneliness",
      "authors": [
        "L Della Longa",
        "I Valori",
        "T Farroni"
      ],
      "year": "2022",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "3",
      "title": "Touch Matters: Handshakes, Hugs, and the New Science on How Touch Can Enhance Your Well-Being",
      "authors": [
        "M Banissy"
      ],
      "year": "2023",
      "venue": "Touch Matters: Handshakes, Hugs, and the New Science on How Touch Can Enhance Your Well-Being"
    },
    {
      "citation_id": "4",
      "title": "Affective haptics: Current research and future directions",
      "authors": [
        "M Eid",
        "H Osman"
      ],
      "year": "2015",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "5",
      "title": "Touch communicates distinct emotions",
      "authors": [
        "M Hertenstein",
        "D Keltner",
        "B App",
        "B Bulleit",
        "A Jaskolka"
      ],
      "year": "2006",
      "venue": "Emotion"
    },
    {
      "citation_id": "6",
      "title": "The communication of emotion via touch",
      "authors": [
        "M Hertenstein",
        "R Holmes",
        "M Mccullough",
        "D Keltner"
      ],
      "year": "2009",
      "venue": "Emotion"
    },
    {
      "citation_id": "7",
      "title": "Robots in elderly care",
      "authors": [
        "A Vercelli",
        "I Rainero",
        "L Ciferri",
        "M Boido",
        "F Pirri"
      ],
      "year": "2018",
      "venue": "DigitCult-Scientific Journal on Digital Cultures"
    },
    {
      "citation_id": "8",
      "title": "Emotion recognition and synthesis system on speech",
      "authors": [
        "T Moriyama",
        "S Ozawa"
      ],
      "year": "1999",
      "venue": "Proceedings IEEE International Conference on Multimedia Computing and Systems"
    },
    {
      "citation_id": "9",
      "title": "Emotion recognition using facial expressions",
      "authors": [
        "P Tarnowski",
        "M Kołodziej",
        "A Majkowski",
        "R Rak"
      ],
      "year": "2017",
      "venue": "Procedia Computer Science"
    },
    {
      "citation_id": "10",
      "title": "Emotion recognition from body movement",
      "authors": [
        "F Ahmed",
        "A Bari",
        "M Gavrilova"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "11",
      "title": "Multimodal interaction of contextual and non-contextual sound and haptics in virtual simulations",
      "authors": [
        "M Melaisi",
        "D Rojas",
        "B Kapralos",
        "A Uribe-Quevedo",
        "K Collins"
      ],
      "year": "2018",
      "venue": "Informatics"
    },
    {
      "citation_id": "12",
      "title": "Tactile interaction with social robots influences attitudes and behaviour",
      "authors": [
        "Q Ren",
        "T Belpaeme"
      ],
      "year": "2024",
      "venue": "International Journal of Social Robotics"
    },
    {
      "citation_id": "13",
      "title": "Haptic empathy: Conveying emotional meaning through vibrotactile feedback",
      "authors": [
        "Y Ju",
        "D Zheng",
        "D Hynds",
        "G Chernyshov",
        "K Kunze",
        "K Minamizawa"
      ],
      "year": "2021",
      "venue": "Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems"
    },
    {
      "citation_id": "14",
      "title": "Toward affective handles for tuning vibrations",
      "authors": [
        "H Seifi",
        "M Chun",
        "K Maclean"
      ],
      "year": "2018",
      "venue": "ACM Transactions on Applied Perception (TAP)"
    },
    {
      "citation_id": "15",
      "title": "Rethinking the senses and their interactions: the case for sensory pluralism",
      "authors": [
        "M Fulkerson"
      ],
      "year": "2014",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "16",
      "title": "Paving the way for social touch at a distance: Sonifying tactile interactions and their underlying emotions",
      "authors": [
        "A De Lagarde",
        "C Pelachaud",
        "L Kirsch",
        "M Auvray"
      ],
      "year": "2025",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "17",
      "title": "10 years of human-nao interaction research: A scoping review",
      "authors": [
        "A Amirova",
        "N Rakhymbayeva",
        "E Yadollahi",
        "A Sandygulova"
      ],
      "year": "2021",
      "venue": "Frontiers in Robotics and AI"
    },
    {
      "citation_id": "18",
      "title": "Embodied psychoacoustics: Spatial and multisensory determinants of auditory-induced emotion",
      "authors": [
        "A Tajadura-Jiménez"
      ],
      "year": "2008",
      "venue": "Embodied psychoacoustics: Spatial and multisensory determinants of auditory-induced emotion"
    },
    {
      "citation_id": "19",
      "title": "Audiotouch: tactile gestures and emotions can be recognised through their auditory counterparts",
      "authors": [
        "A De Lagarde",
        "C Pelachaud",
        "L Kirsch",
        "M Auvray"
      ],
      "year": "2023",
      "venue": "Audiotouch: tactile gestures and emotions can be recognised through their auditory counterparts"
    },
    {
      "citation_id": "20",
      "title": "No more mumbles: Enhancing robot intelligibility through speech adaptation",
      "authors": [
        "Q Ren",
        "Y Hou",
        "D Botteldooren",
        "T Belpaeme"
      ],
      "year": "2024",
      "venue": "IEEE Robotics and Automation Letters"
    },
    {
      "citation_id": "21",
      "title": "Facial emotion expressions in human-robot interaction: A survey",
      "authors": [
        "R Stock-Homburg"
      ],
      "year": "2022",
      "venue": "International Journal of Social Robotics"
    },
    {
      "citation_id": "22",
      "title": "A survey on haptics: Communication, sensing and feedback",
      "authors": [
        "M Emami",
        "A Bayat",
        "R Tafazolli",
        "A Quddus"
      ],
      "year": "2024",
      "venue": "IEEE Communications Surveys & Tutorials"
    },
    {
      "citation_id": "23",
      "title": "Advances, Applications and the Future of Haptic Technology",
      "authors": [
        "M Kuhail",
        "J Berengueres",
        "F Taher",
        "M Kuwaiti"
      ],
      "year": "2024",
      "venue": "Advances, Applications and the Future of Haptic Technology"
    },
    {
      "citation_id": "24",
      "title": "Touch technology in affective human-, robot-, and virtual-human interactions: A survey",
      "authors": [
        "T Olugbade",
        "L He",
        "P Maiolino",
        "D Heylen",
        "N Bianchi-Berthouze"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "25",
      "title": "Touched by chatgpt: Using an llm to drive affective tactile interaction",
      "authors": [
        "Q Ren",
        "T Belpaeme"
      ],
      "year": "2025",
      "venue": "Touched by chatgpt: Using an llm to drive affective tactile interaction",
      "arxiv": "arXiv:2501.07224"
    },
    {
      "citation_id": "26",
      "title": "Social touch in human-computer interaction",
      "authors": [
        "J Van Erp",
        "A Toet"
      ],
      "year": "2015",
      "venue": "Frontiers in digital humanities"
    },
    {
      "citation_id": "27",
      "title": "Design and assessment of the haptic creature's affect display",
      "authors": [
        "S Yohanan",
        "K Maclean"
      ],
      "year": "2011",
      "venue": "Proceedings of the 6th international conference on Human-robot interaction"
    },
    {
      "citation_id": "28",
      "title": "Communicating emotion through a haptic link: Design space and methodology",
      "authors": [
        "J Smith",
        "K Maclean"
      ],
      "year": "2007",
      "venue": "International Journal of Human-Computer Studies"
    },
    {
      "citation_id": "29",
      "title": "Virtual interpersonal touch: Expressing and recognizing emotions through haptic devices",
      "authors": [
        "J Bailenson",
        "N Yee",
        "S Brave",
        "D Merget",
        "D Koslow"
      ],
      "year": "2007",
      "venue": "Human-Computer Interaction"
    },
    {
      "citation_id": "30",
      "title": "Improvement of the recognition of facial expressions with haptic feedback",
      "authors": [
        "D Bonnet",
        "M Ammi",
        "J.-C Martin"
      ],
      "year": "2011",
      "venue": "2011 IEEE International Workshop on Haptic Audio Visual Environments and Games"
    },
    {
      "citation_id": "31",
      "title": "Towards tactile expressions of emotion through mediated touch",
      "authors": [
        "G Huisman",
        "A Frederiks"
      ],
      "year": "2013",
      "venue": "CHI'13 Extended Abstracts on Human Factors in Computing Systems"
    },
    {
      "citation_id": "32",
      "title": "Huggy pajama: remote hug system for family communication",
      "authors": [
        "A Cheok",
        "E Zhang",
        "A Cheok",
        "E Zhang"
      ],
      "year": "2019",
      "venue": "Huggy pajama: remote hug system for family communication"
    },
    {
      "citation_id": "33",
      "title": "Conveying emotions to robots through touch and sound",
      "authors": [
        "Q Ren",
        "R Proesmans",
        "F Bossuyt",
        "J Vanfleteren",
        "F Wyffels",
        "T Belpaeme"
      ],
      "year": "2024",
      "venue": "Conveying emotions to robots through touch and sound",
      "arxiv": "arXiv:2412.03300"
    },
    {
      "citation_id": "34",
      "title": "On the valence of surprise",
      "authors": [
        "M Noordewier",
        "S Breugelmans"
      ],
      "year": "2013",
      "venue": "Cognition & emotion"
    },
    {
      "citation_id": "35",
      "title": "Emotion and motivation i: defensive and appetitive reactions in picture processing",
      "authors": [
        "M Bradley",
        "M Codispoti",
        "B Cuthbert",
        "P Lang"
      ],
      "year": "2001",
      "venue": "Emotion"
    },
    {
      "citation_id": "36",
      "title": "Animal signals and emotion in music: Coordinating affect across groups",
      "authors": [
        "G Bryant"
      ],
      "year": "2013",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "37",
      "title": "Emotion recognition from unimodal to multimodal analysis: A review",
      "authors": [
        "K Ezzameli",
        "H Mahersia"
      ],
      "year": "2023",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "38",
      "title": "Situated haptic interaction: Exploring the role of context in affective perception of robotic touch",
      "authors": [
        "Q Ren",
        "T Belpaeme"
      ],
      "year": "2025",
      "venue": "Situated haptic interaction: Exploring the role of context in affective perception of robotic touch",
      "arxiv": "arXiv:2506.19179"
    },
    {
      "citation_id": "39",
      "title": "Touch and the body",
      "authors": [
        "A Serino",
        "P Haggard"
      ],
      "year": "2010",
      "venue": "Neuroscience & Biobehavioral Reviews"
    }
  ]
}