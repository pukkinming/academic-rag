{
  "paper_id": "2403.17562v1",
  "title": "Deep Functional Multiple Index Models With An Application To Ser",
  "published": "2024-03-26T10:10:56Z",
  "authors": [
    "Matthieu Saumard",
    "Abir El Haj",
    "Thibault Napoleon"
  ],
  "keywords": [
    "speech emotion recognition",
    "human-computer interaction",
    "functional data analysis",
    "deep learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech Emotion Recognition (SER) plays a crucial role in advancing human-computer interaction and speech processing capabilities. We introduce a novel deep-learning architecture designed specifically for the functional data model known as the multiple-index functional model. Our key innovation lies in integrating adaptive basis layers and an automated data transformation search within the deep learning framework. Simulations for this new model show good performances. This allows us to extract features tailored for chunk-level SER, based on Mel Frequency Cepstral Coefficients (MFCCs). We demonstrate the effectiveness of our approach on the benchmark IEMOCAP database, achieving good performance compared to existing methods.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Emotion recognition is an essential aspect of human-robot interaction, as it allows for a more natural and effective means of communication. Humans communicate their emotions through various modalities, including facial expressions, body language, and speech. However, among these modalities, speech plays a crucial role in emotion recognition, as it is one of the most reliable and informative ways to convey emotional information. Through speech, individuals can convey not only the content of their message but also the underlying emotional state, including tone, pitch, and intonation. Moreover, speech can also provide insights into the speaker's personality, cognitive state, and overall well-being. As such, developing accurate and efficient methods for speech emotion recognition is critical for creating more effective and responsive humanrobot interfaces.\n\nSupport Vector Machines have been shown to be effective for speech emotion recognition. They are able to learn complex decision boundaries and can handle high-dimensional data, such as speech. Other types of classifiers that have been used for speech emotion recognition include decision trees, k-nearest neighbors, and neural networks. Speech emotion recognition has made rapid progress in recent years with the use of deep learning and convolutional neural networks (CNN)  [1] , transformer, attention and self-supervised learning  [2] -  [5]  methods.\n\nSince the pioneer monographs  [6]  and  [7] , functional data analysis (FDA) has become a vibrant field of research in the statistical community due to it vast applications in various sciences, including astronomy, chemo-metrics, health and finance  [8] -  [10] . The core objects of FDA are curves or functions of a separable Hilbert space like L 2 [0, 1]. However, statistical study of random variables of more general functions space like Banach space  [11]  or curves on manifold  [12]  falls within the spectrum of FDA. The fundamental frequency curve of an utterance has already been considered as a functional object  [13] ,  [14] . Recently,  [15]  analysed dialect sound variations across Great Britain using a spatial modeling approach that employs MFCC. In this article, we extend this point of view to speech emotion recognition. Each coefficient of MFCC can be interpreted as a functional data variable. With a collection of coefficients, it is therefore natural to establish a correspondence between a speech recording and a multivariate functional data object. The number of covariates of the multivariate functional data object is the number of coefficients of the MFCC used. A transformation of the multivariate functional object is employed to serve as the final multivariate object, which is considered in a functional multiple-index model.\n\nIn our work, we propose a novel approach for speech emotion recognition that involves treating Mel Frequency Cepstral Coefficients (MFCCs) as a functional data object. By doing so, we can represent each coefficient as a function of time, and thus extract information from the speech signal. However, to compare functional data objects between samples with different duration, we need to preprocess the MFCC. This is achieved by splitting the MFCC in chunks, which allows us to represent each sample as a multivariate functional object. An abundant literature on chunk-level SER exists, see  [16] ,  [17]  and  [18]  for example. This multivariate object can then be transformed into another multivariate object using a suitable transformation, enabling us to use the functional multipleindex model for multivariate functional covariate to classify the emotions of the speaker.\n\nThis novel approach is particularly advantageous as it allows us to consider each MFCC as a functional variable, which captures the dynamic nature of speech and its relationship to emotions. By using a multivariate functional object, we can compare it across samples with different duration. Moreover, the functional multiple-index model allows us to consider the interdependence between the different coefficients of the MFCC, providing a more accurate and comprehensive representation of the speech signal. Overall, our approach shows interesting perspectives for improving speech emotion recognition.\n\nThe paper is organized as follows. We highlight the previous work in the following section. Next, we describe the method in details with our contributions. We propose to evaluate our method on IEMOCAP database described in the following fourth section along with the results and comparison with other methods. We conclude and discuss the results in the final section.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. On Functional Data Models",
      "text": "The functional single index models have been studied both from a theoretical and practical point of view in  [19] -  [21] . Some authors  [22]  and  [23]  use functional additive models that can be more stable than functional multiple-index models. The authors of  [24]  use deep neural network strategy to learn the parameter of the functional linear model by using a new functional neuron that can learn the functional representation with functional inputs. The article of  [25]  proposes a novel neural network that learns the best basis functions for supervised learning tasks with functional inputs. They called it AdaFNN. The AdaFNN network parameterizes each basis node with a micro neural network that outputs a score of the input function X(t), which is the inner product between the basis function θ(t) and the input function:\n\nThey introduce neural networks that employ a new Basis Layer whose hidden units are each basis functions themselves implemented as a micro neural network. On the link between deep learning and FDA, there exist other approach based on a functional layer developed by  [26]  investigated in details by  [27]  and  [24] . We present in the next section our model and a simulation comparison with  [24] . The advantage of the approach in  [25]  is that we can use all the deep learning tools without rewriting the back-propagation algorithm which is necessary in  [24] .\n\nLet us introduce the functional single index model in the regression context. Let (Y i , X i ) i=1,...,n be the set of data where X i represent the i-th functional data variable in L 2 [0, 1], the space of square integrable function on [0, 1], and Y i ∈ R.\n\nwhere ε i are the error terms, ⟨., .⟩ is the inner product in L 2 [0, 1]. g and θ are unknown function to be estimated. Generally, g is estimated from a nonparametric framework with a kernel and θ is estimated from a basis decomposition or a functional principal component analysis. We propose to extend this definition to a functional multiple-index model for multivariate functional data covariate, see section 3.3.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. On Speech Emotion Recognition",
      "text": "There is a rich literature on speech emotion recognition using various deep learning architectures. We can cite  [28] -  [31]  which use self-attention, Bayesian neural networks and positional encoding. The article of  [32]  proposes to make silence representations of speech. The article of  [33]  makes progress by creating self-supervised learning for SER tasks.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Method",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Mfcc",
      "text": "Let us recall the method to calculate the MFCC. With at hand a raw audio signal data representing by a time series s(t) for t = 1, . . . , T. We can consider that s is defined for t ∈ Z by adding 0 to non-value. Let w M (t) for t ∈ Z be a window function of width M , we can define the spectrogram of the audio signal s(t) by\n\nHence, we can define the Mel spectrogram, which is a filtered version of the spectrogram to represent the human ear auditory system:\n\nfor f = 0, . . . , F , with b f,k representing the set of the Mel-scale filter bank. Recall that the Mel scale is m = 2595 log 10 (1 + f 700 ). The MFCC are then:\n\nfor m = 1, . . . , n M F CC .\n\nNote that there exist variations of the definition of MFCC in the literature. For example, we can use a Discrete Cosinus Transform (DCT) to return to the time scale.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Functional Multiple Index Models",
      "text": "For extracting new features based on MFCC, we employ a deep functional multiple index model. Let (Y i , X i ) i=1,...,n be the set of data where X i represent the i-th multivariate functional object and Y i is its associated label. Let us introduce the model.\n\nwhere T : (L 2 [0, 1]) p → (L 2 [0, 1]) p is a transformation of the multivariate functional data, θ j are the indexes (functions of L 2 [0, 1]), g : R p×K → {0, . . . , C -1} is the link function, C is the number of classes, ⟨., .⟩ is the inner product of L 2 [0, 1] functions and the power j of the variable Z i is the jth component function of Z i as Z i is a multivariate functional data.\n\nThe unknown functions are the transformation T , the link function g and the indexes θ j , j = 1, . . . , p × K. The transformation T is inferred by a transformer encoder architecture. Next, we apply an AdaFNN network to each functional variable, the output represents new features extracted from the MFCCs. Thus, the θ j are estimated by a deep functional network (DFN) based on a concatenation of the AdaFNN module. Then, we apply a fully connected layer to estimate the link function g.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Deep Neural Network Of The Model",
      "text": "With a speech recording, we calculate the MFCC associated and cut it with an overlapping method. Then, we can feed the Deep Neural Network with this chunk of MFFC seen as a multivariate functional object. First, we make a transformation of the MFCC chunk. There is undoubtedly a link between the size of MFCC and the information about the emotion of the speech. But we do not know how is this link. So we apply a transformation T on the resized MFCC to accurately predict the emotion contained in the speech. The transformation module is made of a stack of N transformer encoders with self-attention and Feed-Forward network. The second module is the neural network of the paper  [25]  which is generalized to adapt to the multivariate context. In few words, the paper of  [25]  proposes to integrate the data by an adaptive function by a numerical scheme . So, the outputs of this second module are simply the L 2 product between each component of the transformed MFCC and an adaptive function. The third module of our network is a classical fully connected layer. The proposed method is represented globally in Figure  1 .\n\nThe three modules of our new network architecture are:\n\n1) The transformation module made of N transformer encoders.\n\n2) The Deep Functional Network that outputs the scores of multivariate functional variable. 3) A Fully Connected Layers to classify the emotions.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "D. Simulations",
      "text": "In order to show the ability of our proposed method, we study three different scenarios. For completing the study on other potential models using adaptive layer, we refer to the simulation section of  [25] . We simulate four functional covariates X (j) j = 1, 2, 3, 4 coming from four different processes: exponential variogram (j = 1), Brownian (j = 2), Fractional Brownian (j = 3) and Gaussian process with Matérn covariance function. These curves are evaluated at 30 equally-spaced time points from [0; 1]. The unknown parameter functions are β 1 (t) = 5 sin(2πt), β 2 (t) = 5 sin(3πt), β 3 (t) = 3 cos(2πt) and β 4 (t) = 3 cos(3πt). A Gaussian error of variance 0.04 has been added to the regression term. Let us introduce the three different scenarios:\n\ni ⟩×⟨β 4 , X\n\ni ⟩) 2 + (⟨β 1 , Xi (1) ⟩×⟨β 2 , Xi (2) + ⟨β 3 , X\n\ni ⟩) 2 + ε i To make an evaluation of the transformation part, we change X (1) by X(1) = (X (1) ) 2 and take X(2) = |X (2) | in the last scenario. We do not have the code of the paper  [24] , so we could not compare to their method. In table I, the simulations reflect a good behaviour of our approach, even with complex behaviour and transformations in variables.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iv. Application To Ser",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Database",
      "text": "The dataset IEMOCAP  [34]  contains approximately 12 hours of speech from 10 speakers. The literature selects a total of 5531 utterances that are labeled with one of five categories: Happy, Angry, Neutral, Sad, Exited. This set is reduced to four categories by merging Exited and Happy into a single category.\n\nFor evaluating our method, we use the protocol designed in  [35] . Namely, we perform a 10-fold Speaker Independent cross-validation with one speaker as test, eight as training and one as validation set.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Implementation And Hyperparameters",
      "text": "The code is written in python and use the pytorch library including its modules. We use the Adam optimizer with a learning rate of 3 × 10 -4 and a focal loss with the L 2 penalty on the basis parameter. We set the batch size to 32 with 15 epoch and return the best model on the validation set. We do not tune the number of basis K, we use the best result of  [25] , namely 4 basis functions. We set the number of MFCC p to 40. And, we make chunks of the MFCC with a duration of 64. So, finally, the size of the final MFCC is (64, 40). We use N = 2 transformer encoder layers. We use in the DFN of the basis layers an hidden FF of three connected layers of 128 each. The subsequent FCL network is two dense layer with an tanh activation function, dropout of 0.2, follow up by a projection to the 4 classes.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Results",
      "text": "We calculate two metrics namely WA for weighted accuracy (overall accuracy) and UA for unweigthed accuracy (average of the recall). We compare with  [2] ,  [36]  which are the best results on IEMOCAP with four emotions and speech only, see Table  II . It is worth mentioning that  [36]  do not only use MFCC but also Zero-crossing rate (ZCR), root mean square (RMS), Mel vector, chroma. And  [2]  use MFCC and first and second order frame-to-frame difference. The results cannot be compared directly because we make our analysis on chunk-level. We think that the performance of our approach with the protocol of  [35]  can be improve by considering all the chunks of the audio record considered.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "V. Discussion",
      "text": "In this method, we choose to make chunks with an overlap of 25% of the duration of the chunk. Recently,  [16]  and  [18]  choose dynamically the percentage of overlapping with a significant improvement in accuracy. We can enhance our method by dynamically choosing the chunk overlapping by using the method in  [16]  and  [18] . Moreover, the features can be passed to a recurrent neural network (RNN) (like LSTM, Bi-LSTM, GRU) to make a global decision on the whole audio input. Adding a RNN on top of the model may enhance the results, and that can be done in a two-stage or end-to-end learning.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "In conclusion, the article presents a promising advancement in SER using a novel model. The results, validated through simulations and tested on the IEMOCAP dataset at the chunk-level, demonstrate satisfactory performance. The model leverages new features extracted from MFCC and relies on functional data, showcasing an innovative approach to emotion detection in speech. This development not only contributes to the ongoing evolution of SER methodologies but also highlights the potential for further exploration of functional data in enhancing emotion recognition systems.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Proposed method. SA: Self-Attention, FF: Feed-Forward network, DFN: Deep Functional Network, FCL: Fully Connected Layers.",
      "page": 3
    },
    {
      "caption": "Figure 1: The three modules of our new network architecture are:",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "matthieu.saumard@isen-ouest.yncrea.fr": "Abstract—Speech Emotion Recognition (SER) plays a crucial",
          "thibault.napoleon@isen-ouest.yncrea.fr\nabir.el-haj@isen-ouest.yncrea.fr": "statistical community due to it vast applications in various sci-"
        },
        {
          "matthieu.saumard@isen-ouest.yncrea.fr": "role in advancing human-computer interaction and speech pro-",
          "thibault.napoleon@isen-ouest.yncrea.fr\nabir.el-haj@isen-ouest.yncrea.fr": ""
        },
        {
          "matthieu.saumard@isen-ouest.yncrea.fr": "",
          "thibault.napoleon@isen-ouest.yncrea.fr\nabir.el-haj@isen-ouest.yncrea.fr": "ences, including astronomy, chemo-metrics, health and finance"
        },
        {
          "matthieu.saumard@isen-ouest.yncrea.fr": "cessing capabilities. We introduce a novel deep-learning architec-",
          "thibault.napoleon@isen-ouest.yncrea.fr\nabir.el-haj@isen-ouest.yncrea.fr": ""
        },
        {
          "matthieu.saumard@isen-ouest.yncrea.fr": "",
          "thibault.napoleon@isen-ouest.yncrea.fr\nabir.el-haj@isen-ouest.yncrea.fr": "[8]–[10]. The core objects of FDA are curves or\nfunctions of"
        },
        {
          "matthieu.saumard@isen-ouest.yncrea.fr": "ture designed specifically for the\nfunctional data model known",
          "thibault.napoleon@isen-ouest.yncrea.fr\nabir.el-haj@isen-ouest.yncrea.fr": ""
        },
        {
          "matthieu.saumard@isen-ouest.yncrea.fr": "",
          "thibault.napoleon@isen-ouest.yncrea.fr\nabir.el-haj@isen-ouest.yncrea.fr": "a\nseparable Hilbert\nspace\nlike L2[0, 1]. However,\nstatistical"
        },
        {
          "matthieu.saumard@isen-ouest.yncrea.fr": "as\nthe multiple-index functional model. Our key innovation lies",
          "thibault.napoleon@isen-ouest.yncrea.fr\nabir.el-haj@isen-ouest.yncrea.fr": ""
        },
        {
          "matthieu.saumard@isen-ouest.yncrea.fr": "in\nintegrating\nadaptive\nbasis\nlayers\nand\nan\nautomated\ndata",
          "thibault.napoleon@isen-ouest.yncrea.fr\nabir.el-haj@isen-ouest.yncrea.fr": "study of\nrandom variables of more general\nfunctions\nspace"
        },
        {
          "matthieu.saumard@isen-ouest.yncrea.fr": "transformation\nsearch within\nthe\ndeep\nlearning\nframework.",
          "thibault.napoleon@isen-ouest.yncrea.fr\nabir.el-haj@isen-ouest.yncrea.fr": "like Banach space [11] or curves on manifold [12] falls within"
        },
        {
          "matthieu.saumard@isen-ouest.yncrea.fr": "Simulations\nfor\nthis new model\nshow good performances. This",
          "thibault.napoleon@isen-ouest.yncrea.fr\nabir.el-haj@isen-ouest.yncrea.fr": ""
        },
        {
          "matthieu.saumard@isen-ouest.yncrea.fr": "",
          "thibault.napoleon@isen-ouest.yncrea.fr\nabir.el-haj@isen-ouest.yncrea.fr": "the spectrum of FDA. The fundamental frequency curve of an"
        },
        {
          "matthieu.saumard@isen-ouest.yncrea.fr": "allows\nus\nto\nextract\nfeatures\ntailored\nfor\nchunk-level\nSER,",
          "thibault.napoleon@isen-ouest.yncrea.fr\nabir.el-haj@isen-ouest.yncrea.fr": ""
        },
        {
          "matthieu.saumard@isen-ouest.yncrea.fr": "",
          "thibault.napoleon@isen-ouest.yncrea.fr\nabir.el-haj@isen-ouest.yncrea.fr": "utterance has already been considered as a functional object"
        },
        {
          "matthieu.saumard@isen-ouest.yncrea.fr": "based\non Mel Frequency Cepstral Coefficients\n(MFCCs). We",
          "thibault.napoleon@isen-ouest.yncrea.fr\nabir.el-haj@isen-ouest.yncrea.fr": ""
        },
        {
          "matthieu.saumard@isen-ouest.yncrea.fr": "",
          "thibault.napoleon@isen-ouest.yncrea.fr\nabir.el-haj@isen-ouest.yncrea.fr": "[13],\n[14]. Recently,\n[15]\nanalysed dialect\nsound variations"
        },
        {
          "matthieu.saumard@isen-ouest.yncrea.fr": "demonstrate the effectiveness of our approach on the benchmark",
          "thibault.napoleon@isen-ouest.yncrea.fr\nabir.el-haj@isen-ouest.yncrea.fr": ""
        },
        {
          "matthieu.saumard@isen-ouest.yncrea.fr": "IEMOCAP database, achieving good performance compared to",
          "thibault.napoleon@isen-ouest.yncrea.fr\nabir.el-haj@isen-ouest.yncrea.fr": "across Great Britain using a spatial modeling approach that"
        },
        {
          "matthieu.saumard@isen-ouest.yncrea.fr": "existing methods.",
          "thibault.napoleon@isen-ouest.yncrea.fr\nabir.el-haj@isen-ouest.yncrea.fr": "employs MFCC.\nIn this article, we extend this point of view"
        },
        {
          "matthieu.saumard@isen-ouest.yncrea.fr": "Index Terms—speech emotion recognition, human-computer",
          "thibault.napoleon@isen-ouest.yncrea.fr\nabir.el-haj@isen-ouest.yncrea.fr": ""
        },
        {
          "matthieu.saumard@isen-ouest.yncrea.fr": "",
          "thibault.napoleon@isen-ouest.yncrea.fr\nabir.el-haj@isen-ouest.yncrea.fr": "to speech emotion recognition. Each coefficient of MFCC can"
        },
        {
          "matthieu.saumard@isen-ouest.yncrea.fr": "interaction,\nfunctional data analysis, deep learning",
          "thibault.napoleon@isen-ouest.yncrea.fr\nabir.el-haj@isen-ouest.yncrea.fr": ""
        },
        {
          "matthieu.saumard@isen-ouest.yncrea.fr": "",
          "thibault.napoleon@isen-ouest.yncrea.fr\nabir.el-haj@isen-ouest.yncrea.fr": "be interpreted as a functional data variable. With a collection"
        },
        {
          "matthieu.saumard@isen-ouest.yncrea.fr": "",
          "thibault.napoleon@isen-ouest.yncrea.fr\nabir.el-haj@isen-ouest.yncrea.fr": "of coefficients,\nit\nis therefore natural\nto establish a correspon-"
        },
        {
          "matthieu.saumard@isen-ouest.yncrea.fr": "I.\nINTRODUCTION",
          "thibault.napoleon@isen-ouest.yncrea.fr\nabir.el-haj@isen-ouest.yncrea.fr": ""
        },
        {
          "matthieu.saumard@isen-ouest.yncrea.fr": "",
          "thibault.napoleon@isen-ouest.yncrea.fr\nabir.el-haj@isen-ouest.yncrea.fr": "dence between a\nspeech recording and a multivariate\nfunc-"
        },
        {
          "matthieu.saumard@isen-ouest.yncrea.fr": "Emotion recognition is an essential aspect of human-robot",
          "thibault.napoleon@isen-ouest.yncrea.fr\nabir.el-haj@isen-ouest.yncrea.fr": "tional data object. The number of covariates of the multivariate"
        },
        {
          "matthieu.saumard@isen-ouest.yncrea.fr": "interaction,\nas\nit\nallows\nfor\na more\nnatural\nand\neffective",
          "thibault.napoleon@isen-ouest.yncrea.fr\nabir.el-haj@isen-ouest.yncrea.fr": "functional\ndata\nobject\nis\nthe\nnumber\nof\ncoefficients\nof\nthe"
        },
        {
          "matthieu.saumard@isen-ouest.yncrea.fr": "means of\ncommunication. Humans\ncommunicate\ntheir\nemo-",
          "thibault.napoleon@isen-ouest.yncrea.fr\nabir.el-haj@isen-ouest.yncrea.fr": "MFCC used. A transformation of\nthe multivariate functional"
        },
        {
          "matthieu.saumard@isen-ouest.yncrea.fr": "tions through various modalities,\nincluding facial expressions,",
          "thibault.napoleon@isen-ouest.yncrea.fr\nabir.el-haj@isen-ouest.yncrea.fr": "object\nis employed to serve as\nthe final multivariate object,"
        },
        {
          "matthieu.saumard@isen-ouest.yncrea.fr": "body language, and speech. However, among these modalities,",
          "thibault.napoleon@isen-ouest.yncrea.fr\nabir.el-haj@isen-ouest.yncrea.fr": "which is considered in a functional multiple-index model."
        },
        {
          "matthieu.saumard@isen-ouest.yncrea.fr": "speech plays a crucial role in emotion recognition, as it\nis one",
          "thibault.napoleon@isen-ouest.yncrea.fr\nabir.el-haj@isen-ouest.yncrea.fr": "In\nour work, we\npropose\na\nnovel\napproach\nfor\nspeech"
        },
        {
          "matthieu.saumard@isen-ouest.yncrea.fr": "of the most reliable and informative ways to convey emotional",
          "thibault.napoleon@isen-ouest.yncrea.fr\nabir.el-haj@isen-ouest.yncrea.fr": "emotion\nrecognition\nthat\ninvolves\ntreating Mel\nFrequency"
        },
        {
          "matthieu.saumard@isen-ouest.yncrea.fr": "information. Through speech,\nindividuals can convey not only",
          "thibault.napoleon@isen-ouest.yncrea.fr\nabir.el-haj@isen-ouest.yncrea.fr": "Cepstral Coefficients\n(MFCCs)\nas\na\nfunctional\ndata\nobject."
        },
        {
          "matthieu.saumard@isen-ouest.yncrea.fr": "the\ncontent of\ntheir message\nbut\nalso\nthe\nunderlying\nemo-",
          "thibault.napoleon@isen-ouest.yncrea.fr\nabir.el-haj@isen-ouest.yncrea.fr": "By doing so, we can represent each coefficient as a function"
        },
        {
          "matthieu.saumard@isen-ouest.yncrea.fr": "tional\nstate,\nincluding tone, pitch, and intonation. Moreover,",
          "thibault.napoleon@isen-ouest.yncrea.fr\nabir.el-haj@isen-ouest.yncrea.fr": "of\ntime, and thus extract\ninformation from the speech signal."
        },
        {
          "matthieu.saumard@isen-ouest.yncrea.fr": "speech can also provide insights into the speaker’s personality,",
          "thibault.napoleon@isen-ouest.yncrea.fr\nabir.el-haj@isen-ouest.yncrea.fr": "However,\nto compare functional data objects between samples"
        },
        {
          "matthieu.saumard@isen-ouest.yncrea.fr": "cognitive\nstate,\nand overall well-being. As\nsuch, developing",
          "thibault.napoleon@isen-ouest.yncrea.fr\nabir.el-haj@isen-ouest.yncrea.fr": "with different duration, we need to preprocess the MFCC. This"
        },
        {
          "matthieu.saumard@isen-ouest.yncrea.fr": "accurate and efficient methods for speech emotion recognition",
          "thibault.napoleon@isen-ouest.yncrea.fr\nabir.el-haj@isen-ouest.yncrea.fr": "is achieved by splitting the MFCC in chunks, which allows us"
        },
        {
          "matthieu.saumard@isen-ouest.yncrea.fr": "is critical\nfor creating more effective and responsive human-",
          "thibault.napoleon@isen-ouest.yncrea.fr\nabir.el-haj@isen-ouest.yncrea.fr": "to represent each sample as a multivariate functional object."
        },
        {
          "matthieu.saumard@isen-ouest.yncrea.fr": "robot\ninterfaces.",
          "thibault.napoleon@isen-ouest.yncrea.fr\nabir.el-haj@isen-ouest.yncrea.fr": "An abundant\nliterature on chunk-level SER exists,\nsee [16],"
        },
        {
          "matthieu.saumard@isen-ouest.yncrea.fr": "Support Vector Machines have been shown to be effective",
          "thibault.napoleon@isen-ouest.yncrea.fr\nabir.el-haj@isen-ouest.yncrea.fr": "[17] and [18]\nfor example. This multivariate object can then"
        },
        {
          "matthieu.saumard@isen-ouest.yncrea.fr": "for speech emotion recognition. They are able to learn complex",
          "thibault.napoleon@isen-ouest.yncrea.fr\nabir.el-haj@isen-ouest.yncrea.fr": "be transformed into another multivariate object using a suitable"
        },
        {
          "matthieu.saumard@isen-ouest.yncrea.fr": "decision\nboundaries\nand\ncan\nhandle\nhigh-dimensional\ndata,",
          "thibault.napoleon@isen-ouest.yncrea.fr\nabir.el-haj@isen-ouest.yncrea.fr": "transformation,\nenabling\nus\nto\nuse\nthe\nfunctional multiple-"
        },
        {
          "matthieu.saumard@isen-ouest.yncrea.fr": "such\nas\nspeech. Other\ntypes\nof\nclassifiers\nthat\nhave\nbeen",
          "thibault.napoleon@isen-ouest.yncrea.fr\nabir.el-haj@isen-ouest.yncrea.fr": "index model\nfor multivariate functional covariate to classify"
        },
        {
          "matthieu.saumard@isen-ouest.yncrea.fr": "used for\nspeech emotion recognition include decision trees,",
          "thibault.napoleon@isen-ouest.yncrea.fr\nabir.el-haj@isen-ouest.yncrea.fr": "the emotions of\nthe speaker."
        },
        {
          "matthieu.saumard@isen-ouest.yncrea.fr": "k-nearest\nneighbors,\nand\nneural\nnetworks. Speech\nemotion",
          "thibault.napoleon@isen-ouest.yncrea.fr\nabir.el-haj@isen-ouest.yncrea.fr": "This novel approach is particularly advantageous as it allows"
        },
        {
          "matthieu.saumard@isen-ouest.yncrea.fr": "recognition has made rapid progress in recent years with the",
          "thibault.napoleon@isen-ouest.yncrea.fr\nabir.el-haj@isen-ouest.yncrea.fr": "us\nto consider\neach MFCC as\na\nfunctional variable, which"
        },
        {
          "matthieu.saumard@isen-ouest.yncrea.fr": "use of deep learning and convolutional neural networks (CNN)",
          "thibault.napoleon@isen-ouest.yncrea.fr\nabir.el-haj@isen-ouest.yncrea.fr": "captures the dynamic nature of speech and its relationship to"
        },
        {
          "matthieu.saumard@isen-ouest.yncrea.fr": "[1],\ntransformer, attention and self-supervised learning [2]–[5]",
          "thibault.napoleon@isen-ouest.yncrea.fr\nabir.el-haj@isen-ouest.yncrea.fr": "emotions. By using a multivariate functional object, we can"
        },
        {
          "matthieu.saumard@isen-ouest.yncrea.fr": "methods.",
          "thibault.napoleon@isen-ouest.yncrea.fr\nabir.el-haj@isen-ouest.yncrea.fr": "compare it across samples with different duration. Moreover,"
        },
        {
          "matthieu.saumard@isen-ouest.yncrea.fr": "Since the pioneer monographs [6] and [7],\nfunctional data",
          "thibault.napoleon@isen-ouest.yncrea.fr\nabir.el-haj@isen-ouest.yncrea.fr": "the\nfunctional multiple-index model\nallows\nus\nto\nconsider"
        },
        {
          "matthieu.saumard@isen-ouest.yncrea.fr": "analysis (FDA) has become a vibrant field of\nresearch in the",
          "thibault.napoleon@isen-ouest.yncrea.fr\nabir.el-haj@isen-ouest.yncrea.fr": "the\ninterdependence\nbetween\nthe\ndifferent\ncoefficients\nof"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "the MFCC,\nproviding\na more\naccurate\nand\ncomprehensive": "representation\nof\nthe\nspeech\nsignal. Overall,\nour\napproach",
          "B. On speech emotion recognition": ""
        },
        {
          "the MFCC,\nproviding\na more\naccurate\nand\ncomprehensive": "",
          "B. On speech emotion recognition": "There\nis\na\nrich\nliterature\non\nspeech\nemotion\nrecognition"
        },
        {
          "the MFCC,\nproviding\na more\naccurate\nand\ncomprehensive": "shows interesting perspectives for\nimproving speech emotion",
          "B. On speech emotion recognition": ""
        },
        {
          "the MFCC,\nproviding\na more\naccurate\nand\ncomprehensive": "",
          "B. On speech emotion recognition": "using various deep learning architectures. We can cite [28]–"
        },
        {
          "the MFCC,\nproviding\na more\naccurate\nand\ncomprehensive": "recognition.",
          "B. On speech emotion recognition": ""
        },
        {
          "the MFCC,\nproviding\na more\naccurate\nand\ncomprehensive": "",
          "B. On speech emotion recognition": "[31] which use self-attention, Bayesian neural networks and"
        },
        {
          "the MFCC,\nproviding\na more\naccurate\nand\ncomprehensive": "The paper is organized as follows. We highlight the previous",
          "B. On speech emotion recognition": ""
        },
        {
          "the MFCC,\nproviding\na more\naccurate\nand\ncomprehensive": "",
          "B. On speech emotion recognition": "positional\nencoding. The\narticle\nof\n[32]\nproposes\nto make"
        },
        {
          "the MFCC,\nproviding\na more\naccurate\nand\ncomprehensive": "work in the following section. Next, we describe the method",
          "B. On speech emotion recognition": ""
        },
        {
          "the MFCC,\nproviding\na more\naccurate\nand\ncomprehensive": "",
          "B. On speech emotion recognition": "silence representations of\nspeech. The article of\n[33] makes"
        },
        {
          "the MFCC,\nproviding\na more\naccurate\nand\ncomprehensive": "in details with our contributions. We propose to evaluate our",
          "B. On speech emotion recognition": ""
        },
        {
          "the MFCC,\nproviding\na more\naccurate\nand\ncomprehensive": "",
          "B. On speech emotion recognition": "progress by creating self-supervised learning for SER tasks."
        },
        {
          "the MFCC,\nproviding\na more\naccurate\nand\ncomprehensive": "method\non\nIEMOCAP database\ndescribed\nin\nthe\nfollowing",
          "B. On speech emotion recognition": ""
        },
        {
          "the MFCC,\nproviding\na more\naccurate\nand\ncomprehensive": "fourth\nsection\nalong with\nthe\nresults\nand\ncomparison with",
          "B. On speech emotion recognition": "III. METHOD"
        },
        {
          "the MFCC,\nproviding\na more\naccurate\nand\ncomprehensive": "other methods. We\nconclude\nand discuss\nthe\nresults\nin the",
          "B. On speech emotion recognition": ""
        },
        {
          "the MFCC,\nproviding\na more\naccurate\nand\ncomprehensive": "",
          "B. On speech emotion recognition": "A. MFCC"
        },
        {
          "the MFCC,\nproviding\na more\naccurate\nand\ncomprehensive": "final section.",
          "B. On speech emotion recognition": ""
        },
        {
          "the MFCC,\nproviding\na more\naccurate\nand\ncomprehensive": "",
          "B. On speech emotion recognition": "Let us\nrecall\nthe method to calculate the MFCC. With at"
        },
        {
          "the MFCC,\nproviding\na more\naccurate\nand\ncomprehensive": "II. RELATED WORK",
          "B. On speech emotion recognition": "hand a\nraw audio signal data\nrepresenting by a\ntime\nseries"
        },
        {
          "the MFCC,\nproviding\na more\naccurate\nand\ncomprehensive": "",
          "B. On speech emotion recognition": "s(t)\nfor\nt = 1, . . . , T. We can consider\nthat s is defined for"
        },
        {
          "the MFCC,\nproviding\na more\naccurate\nand\ncomprehensive": "A. On functional data models",
          "B. On speech emotion recognition": ""
        },
        {
          "the MFCC,\nproviding\na more\naccurate\nand\ncomprehensive": "",
          "B. On speech emotion recognition": "for\nt ∈ Z be a\nt ∈ Z by adding 0 to non-value. Let wM (t)"
        },
        {
          "the MFCC,\nproviding\na more\naccurate\nand\ncomprehensive": "The functional single index models have been studied both",
          "B. On speech emotion recognition": "window function of width M , we can define the spectrogram"
        },
        {
          "the MFCC,\nproviding\na more\naccurate\nand\ncomprehensive": "from a\ntheoretical\nand practical point of view in [19]–[21].",
          "B. On speech emotion recognition": "of\nthe audio signal s(t) by"
        },
        {
          "the MFCC,\nproviding\na more\naccurate\nand\ncomprehensive": "Some authors [22] and [23] use functional additive models that",
          "B. On speech emotion recognition": ""
        },
        {
          "the MFCC,\nproviding\na more\naccurate\nand\ncomprehensive": "can be more stable than functional multiple-index models. The",
          "B. On speech emotion recognition": ""
        },
        {
          "the MFCC,\nproviding\na more\naccurate\nand\ncomprehensive": "authors of\n[24] use deep neural network strategy to learn the",
          "B. On speech emotion recognition": ""
        },
        {
          "the MFCC,\nproviding\na more\naccurate\nand\ncomprehensive": "",
          "B. On speech emotion recognition": "T(cid:88) u\nSpec(t, ω) = |\n(2)\ns(t − u)wM (u) exp(−iωu)|,"
        },
        {
          "the MFCC,\nproviding\na more\naccurate\nand\ncomprehensive": "parameter of the functional\nlinear model by using a new func-",
          "B. On speech emotion recognition": "=1"
        },
        {
          "the MFCC,\nproviding\na more\naccurate\nand\ncomprehensive": "tional neuron that can learn the functional representation with",
          "B. On speech emotion recognition": ""
        },
        {
          "the MFCC,\nproviding\na more\naccurate\nand\ncomprehensive": "",
          "B. On speech emotion recognition": "for t = 1, . . . , T, ω ∈ [0, 2π]."
        },
        {
          "the MFCC,\nproviding\na more\naccurate\nand\ncomprehensive": "functional\ninputs. The article of\n[25] proposes a novel neural",
          "B. On speech emotion recognition": ""
        },
        {
          "the MFCC,\nproviding\na more\naccurate\nand\ncomprehensive": "",
          "B. On speech emotion recognition": "Hence, we\ncan\ndefine\nthe Mel\nspectrogram, which\nis\na"
        },
        {
          "the MFCC,\nproviding\na more\naccurate\nand\ncomprehensive": "network\nthat\nlearns\nthe\nbest\nbasis\nfunctions\nfor\nsupervised",
          "B. On speech emotion recognition": ""
        },
        {
          "the MFCC,\nproviding\na more\naccurate\nand\ncomprehensive": "",
          "B. On speech emotion recognition": "filtered version of the spectrogram to represent\nthe human ear"
        },
        {
          "the MFCC,\nproviding\na more\naccurate\nand\ncomprehensive": "learning tasks with functional\ninputs. They called it AdaFNN.",
          "B. On speech emotion recognition": ""
        },
        {
          "the MFCC,\nproviding\na more\naccurate\nand\ncomprehensive": "",
          "B. On speech emotion recognition": "auditory system:"
        },
        {
          "the MFCC,\nproviding\na more\naccurate\nand\ncomprehensive": "The AdaFNN network parameterizes each basis node with a",
          "B. On speech emotion recognition": ""
        },
        {
          "the MFCC,\nproviding\na more\naccurate\nand\ncomprehensive": "micro neural network that outputs a score of the input function",
          "B. On speech emotion recognition": ""
        },
        {
          "the MFCC,\nproviding\na more\naccurate\nand\ncomprehensive": "",
          "B. On speech emotion recognition": "N −1"
        },
        {
          "the MFCC,\nproviding\na more\naccurate\nand\ncomprehensive": "",
          "B. On speech emotion recognition": "(cid:19)\n(cid:18)"
        },
        {
          "the MFCC,\nproviding\na more\naccurate\nand\ncomprehensive": "",
          "B. On speech emotion recognition": "2kπ"
        },
        {
          "the MFCC,\nproviding\na more\naccurate\nand\ncomprehensive": "X(t), which is\nthe inner product between the basis\nfunction",
          "B. On speech emotion recognition": ""
        },
        {
          "the MFCC,\nproviding\na more\naccurate\nand\ncomprehensive": "",
          "B. On speech emotion recognition": "(cid:88) k\nt,\n(3)\nSpec\nMelSpec(t, f ) =\nbf,k"
        },
        {
          "the MFCC,\nproviding\na more\naccurate\nand\ncomprehensive": "θ(t) and the input\nfunction:",
          "B. On speech emotion recognition": "N"
        },
        {
          "the MFCC,\nproviding\na more\naccurate\nand\ncomprehensive": "",
          "B. On speech emotion recognition": "=0"
        },
        {
          "the MFCC,\nproviding\na more\naccurate\nand\ncomprehensive": "(cid:90)",
          "B. On speech emotion recognition": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "The unknown functions are the transformation T ,\nthe link": "g\nj\nfunction\nand\nthe\nindexes\n= 1, . . . , p × K. The\nθj,",
          "module are simply the L2 product between each component": "of the transformed MFCC and an adaptive function. The third"
        },
        {
          "The unknown functions are the transformation T ,\nthe link": "transformation T is inferred by a transformer encoder architec-",
          "module are simply the L2 product between each component": "module of our network is\na\nclassical\nfully connected layer."
        },
        {
          "The unknown functions are the transformation T ,\nthe link": "ture. Next, we apply an AdaFNN network to each functional",
          "module are simply the L2 product between each component": "The proposed method is represented globally in Figure 1."
        },
        {
          "The unknown functions are the transformation T ,\nthe link": "variable,\nthe\noutput\nrepresents\nnew features\nextracted\nfrom",
          "module are simply the L2 product between each component": "The three modules of our new network architecture are:"
        },
        {
          "The unknown functions are the transformation T ,\nthe link": "the MFCCs. Thus,\nare estimated by a deep functional\nthe θj",
          "module are simply the L2 product between each component": "1) The\ntransformation module made\nof N transformer"
        },
        {
          "The unknown functions are the transformation T ,\nthe link": "network\n(DFN)\nbased\non\na\nconcatenation\nof\nthe AdaFNN",
          "module are simply the L2 product between each component": "encoders."
        },
        {
          "The unknown functions are the transformation T ,\nthe link": "module. Then, we apply a fully connected layer\nto estimate",
          "module are simply the L2 product between each component": "2) The Deep Functional Network that outputs the scores of"
        },
        {
          "The unknown functions are the transformation T ,\nthe link": "the link function g.",
          "module are simply the L2 product between each component": "multivariate functional variable."
        },
        {
          "The unknown functions are the transformation T ,\nthe link": "",
          "module are simply the L2 product between each component": "3) A Fully Connected Layers to classify the emotions."
        },
        {
          "The unknown functions are the transformation T ,\nthe link": "C. Deep neural network of\nthe model",
          "module are simply the L2 product between each component": ""
        },
        {
          "The unknown functions are the transformation T ,\nthe link": "",
          "module are simply the L2 product between each component": "D.\nSimulations"
        },
        {
          "The unknown functions are the transformation T ,\nthe link": "With a speech recording, we calculate the MFCC associ-",
          "module are simply the L2 product between each component": ""
        },
        {
          "The unknown functions are the transformation T ,\nthe link": "ated and cut\nit with an overlapping method. Then, we\ncan",
          "module are simply the L2 product between each component": "In order\nto show the ability of our proposed method, we"
        },
        {
          "The unknown functions are the transformation T ,\nthe link": "feed\nthe Deep Neural Network with\nthis\nchunk\nof MFFC",
          "module are simply the L2 product between each component": "study three different\nscenarios. For completing the study on"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 1.\nProposed method. SA: Self-Attention, FF: Feed-Forward network, DFN: Deep Functional Network, FCL: Fully Connected Layers.": "generalized to adapt\nto the multivariate context. In few words,\nis the j-\nL2[0, 1] functions and the power j of the variable Zi"
        },
        {
          "Fig. 1.\nProposed method. SA: Self-Attention, FF: Feed-Forward network, DFN: Deep Functional Network, FCL: Fully Connected Layers.": "the paper of [25] proposes to integrate the data by an adaptive\nis a multivariate functional\nth component function of Zi as Zi"
        },
        {
          "Fig. 1.\nProposed method. SA: Self-Attention, FF: Feed-Forward network, DFN: Deep Functional Network, FCL: Fully Connected Layers.": "function by a numerical scheme . So, the outputs of this second\ndata."
        },
        {
          "Fig. 1.\nProposed method. SA: Self-Attention, FF: Feed-Forward network, DFN: Deep Functional Network, FCL: Fully Connected Layers.": "module are simply the L2 product between each component\nThe unknown functions are the transformation T ,\nthe link"
        },
        {
          "Fig. 1.\nProposed method. SA: Self-Attention, FF: Feed-Forward network, DFN: Deep Functional Network, FCL: Fully Connected Layers.": "g\nj\nof the transformed MFCC and an adaptive function. The third\nfunction\nand\nthe\nindexes\n= 1, . . . , p × K. The\nθj,"
        },
        {
          "Fig. 1.\nProposed method. SA: Self-Attention, FF: Feed-Forward network, DFN: Deep Functional Network, FCL: Fully Connected Layers.": "module of our network is\na\nclassical\nfully connected layer.\ntransformation T is inferred by a transformer encoder architec-"
        },
        {
          "Fig. 1.\nProposed method. SA: Self-Attention, FF: Feed-Forward network, DFN: Deep Functional Network, FCL: Fully Connected Layers.": "The proposed method is represented globally in Figure 1.\nture. Next, we apply an AdaFNN network to each functional"
        },
        {
          "Fig. 1.\nProposed method. SA: Self-Attention, FF: Feed-Forward network, DFN: Deep Functional Network, FCL: Fully Connected Layers.": "The three modules of our new network architecture are:\nvariable,\nthe\noutput\nrepresents\nnew features\nextracted\nfrom"
        },
        {
          "Fig. 1.\nProposed method. SA: Self-Attention, FF: Feed-Forward network, DFN: Deep Functional Network, FCL: Fully Connected Layers.": "the MFCCs. Thus,\nare estimated by a deep functional\nthe θj\n1) The\ntransformation module made\nof N transformer"
        },
        {
          "Fig. 1.\nProposed method. SA: Self-Attention, FF: Feed-Forward network, DFN: Deep Functional Network, FCL: Fully Connected Layers.": "network\n(DFN)\nbased\non\na\nconcatenation\nof\nthe AdaFNN\nencoders."
        },
        {
          "Fig. 1.\nProposed method. SA: Self-Attention, FF: Feed-Forward network, DFN: Deep Functional Network, FCL: Fully Connected Layers.": "module. Then, we apply a fully connected layer\nto estimate\n2) The Deep Functional Network that outputs the scores of"
        },
        {
          "Fig. 1.\nProposed method. SA: Self-Attention, FF: Feed-Forward network, DFN: Deep Functional Network, FCL: Fully Connected Layers.": "the link function g.\nmultivariate functional variable."
        },
        {
          "Fig. 1.\nProposed method. SA: Self-Attention, FF: Feed-Forward network, DFN: Deep Functional Network, FCL: Fully Connected Layers.": "3) A Fully Connected Layers to classify the emotions."
        },
        {
          "Fig. 1.\nProposed method. SA: Self-Attention, FF: Feed-Forward network, DFN: Deep Functional Network, FCL: Fully Connected Layers.": "C. Deep neural network of\nthe model"
        },
        {
          "Fig. 1.\nProposed method. SA: Self-Attention, FF: Feed-Forward network, DFN: Deep Functional Network, FCL: Fully Connected Layers.": "D.\nSimulations"
        },
        {
          "Fig. 1.\nProposed method. SA: Self-Attention, FF: Feed-Forward network, DFN: Deep Functional Network, FCL: Fully Connected Layers.": "With a speech recording, we calculate the MFCC associ-"
        },
        {
          "Fig. 1.\nProposed method. SA: Self-Attention, FF: Feed-Forward network, DFN: Deep Functional Network, FCL: Fully Connected Layers.": "In order\nto show the ability of our proposed method, we\nated and cut\nit with an overlapping method. Then, we\ncan"
        },
        {
          "Fig. 1.\nProposed method. SA: Self-Attention, FF: Feed-Forward network, DFN: Deep Functional Network, FCL: Fully Connected Layers.": "study three different\nscenarios. For completing the study on\nfeed\nthe Deep Neural Network with\nthis\nchunk\nof MFFC"
        },
        {
          "Fig. 1.\nProposed method. SA: Self-Attention, FF: Feed-Forward network, DFN: Deep Functional Network, FCL: Fully Connected Layers.": "other potential models using adaptive layer, we refer\nto the\nseen\nas\na multivariate\nfunctional\nobject. First, we make\na"
        },
        {
          "Fig. 1.\nProposed method. SA: Self-Attention, FF: Feed-Forward network, DFN: Deep Functional Network, FCL: Fully Connected Layers.": "simulation section of [25]. We simulate four functional covari-\ntransformation of\nthe MFCC chunk. There is undoubtedly a"
        },
        {
          "Fig. 1.\nProposed method. SA: Self-Attention, FF: Feed-Forward network, DFN: Deep Functional Network, FCL: Fully Connected Layers.": "ates X (j) j = 1, 2, 3, 4 coming from four different processes:\nlink between the\nsize of MFCC and the\ninformation about"
        },
        {
          "Fig. 1.\nProposed method. SA: Self-Attention, FF: Feed-Forward network, DFN: Deep Functional Network, FCL: Fully Connected Layers.": "exponential variogram (j = 1), Brownian (j = 2), Fractional\nthe emotion of\nthe speech. But we do not know how is\nthis"
        },
        {
          "Fig. 1.\nProposed method. SA: Self-Attention, FF: Feed-Forward network, DFN: Deep Functional Network, FCL: Fully Connected Layers.": "Brownian (j = 3) and Gaussian process with Mat´ern covari-\nlink. So we apply a transformation T on the resized MFCC"
        },
        {
          "Fig. 1.\nProposed method. SA: Self-Attention, FF: Feed-Forward network, DFN: Deep Functional Network, FCL: Fully Connected Layers.": "ance function. These curves are evaluated at 30 equally-spaced\nto accurately predict\nthe emotion contained in the speech. The"
        },
        {
          "Fig. 1.\nProposed method. SA: Self-Attention, FF: Feed-Forward network, DFN: Deep Functional Network, FCL: Fully Connected Layers.": "time points from [0; 1]. The unknown parameter functions are\ntransformation module is made of a stack of N transformer"
        },
        {
          "Fig. 1.\nProposed method. SA: Self-Attention, FF: Feed-Forward network, DFN: Deep Functional Network, FCL: Fully Connected Layers.": "encoders with self-attention and Feed-Forward network. The\nβ1(t) = 5 sin(2πt), β2(t) = 5 sin(3πt), β3(t) = 3 cos(2πt)"
        },
        {
          "Fig. 1.\nProposed method. SA: Self-Attention, FF: Feed-Forward network, DFN: Deep Functional Network, FCL: Fully Connected Layers.": "second module is the neural network of the paper [25] which is\nand β4(t) = 3 cos(3πt). A Gaussian error of variance 0.04"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "has been added to the regression term. Let us": "",
          "introduce the": "",
          "B.\nImplementation and hyperparameters": ""
        },
        {
          "has been added to the regression term. Let us": "",
          "introduce the": "",
          "B.\nImplementation and hyperparameters": "The code is written in python and use the pytorch library"
        },
        {
          "has been added to the regression term. Let us": "",
          "introduce the": "",
          "B.\nImplementation and hyperparameters": "including its modules. We use\nthe Adam optimizer with a"
        },
        {
          "has been added to the regression term. Let us": " ",
          "introduce the": "",
          "B.\nImplementation and hyperparameters": "learning rate of 3 × 10−4 and a focal\nloss with the L2 penalty"
        },
        {
          "has been added to the regression term. Let us": "4(cid:88) j\n4(cid:88) j\n⟩,\n⟨β1, X (j)",
          "introduce the": "+ εi",
          "B.\nImplementation and hyperparameters": ""
        },
        {
          "has been added to the regression term. Let us": "",
          "introduce the": "",
          "B.\nImplementation and hyperparameters": "on the basis parameter. We set\nthe batch size to 32 with 15"
        },
        {
          "has been added to the regression term. Let us": "=1\n=1",
          "introduce the": "",
          "B.\nImplementation and hyperparameters": ""
        },
        {
          "has been added to the regression term. Let us": "",
          "introduce the": "",
          "B.\nImplementation and hyperparameters": "epoch and return the best model on the validation set. We do"
        },
        {
          "has been added to the regression term. Let us": "",
          "introduce the": "",
          "B.\nImplementation and hyperparameters": "not\ntune the number of basis K, we use the best result of [25],"
        },
        {
          "has been added to the regression term. Let us": "",
          "introduce the": "",
          "B.\nImplementation and hyperparameters": ""
        },
        {
          "has been added to the regression term. Let us": "",
          "introduce the": "",
          "B.\nImplementation and hyperparameters": "namely 4 basis functions. We set\nthe number of MFCC p to"
        },
        {
          "has been added to the regression term. Let us": "",
          "introduce the": "",
          "B.\nImplementation and hyperparameters": "40. And, we make chunks of\nthe MFCC with a duration of"
        },
        {
          "has been added to the regression term. Let us": "g(a, b) = a2 + b2.",
          "introduce the": "",
          "B.\nImplementation and hyperparameters": ""
        },
        {
          "has been added to the regression term. Let us": "",
          "introduce the": "",
          "B.\nImplementation and hyperparameters": "64. So, finally,\nthe size of the final MFCC is (64, 40). We use"
        },
        {
          "has been added to the regression term. Let us": "",
          "introduce the": "",
          "B.\nImplementation and hyperparameters": "N = 2 transformer encoder layers. We use in the DFN of the"
        },
        {
          "has been added to the regression term. Let us": "",
          "introduce the": "",
          "B.\nImplementation and hyperparameters": "basis\nlayers an hidden FF of\nthree connected layers of 128"
        },
        {
          "has been added to the regression term. Let us": "",
          "introduce the": "",
          "B.\nImplementation and hyperparameters": "each. The subsequent FCL network is\ntwo dense layer with"
        },
        {
          "has been added to the regression term. Let us": " ",
          "introduce the": " ",
          "B.\nImplementation and hyperparameters": ""
        },
        {
          "has been added to the regression term. Let us": "4(cid:88) j\n4(cid:88) j\n⟩, · · ·\n,\n⟨β1, X (j)",
          "introduce the": "+ εi",
          "B.\nImplementation and hyperparameters": "an tanh activation function, dropout of 0.2,\nfollow up by a"
        },
        {
          "has been added to the regression term. Let us": "=1\n=1",
          "introduce the": "",
          "B.\nImplementation and hyperparameters": "projection to the 4 classes."
        },
        {
          "has been added to the regression term. Let us": "",
          "introduce the": "",
          "B.\nImplementation and hyperparameters": "C. Results"
        },
        {
          "has been added to the regression term. Let us": "",
          "introduce the": "",
          "B.\nImplementation and hyperparameters": ""
        },
        {
          "has been added to the regression term. Let us": "",
          "introduce the": "",
          "B.\nImplementation and hyperparameters": "We calculate two metrics namely WA for weighted accuracy"
        },
        {
          "has been added to the regression term. Let us": "g(a, b) = a2 + b2 + c2 + d2.",
          "introduce the": "",
          "B.\nImplementation and hyperparameters": "(overall accuracy) and UA for unweigthed accuracy (average"
        },
        {
          "has been added to the regression term. Let us": "",
          "introduce the": "",
          "B.\nImplementation and hyperparameters": "of\nthe recall). We compare with [2],\n[36] which are the best"
        },
        {
          "has been added to the regression term. Let us": "",
          "introduce the": "",
          "B.\nImplementation and hyperparameters": "results on IEMOCAP with four\nemotions\nand speech only,"
        },
        {
          "has been added to the regression term. Let us": "",
          "introduce the": "",
          "B.\nImplementation and hyperparameters": "see Table II.\nIt\nis worth mentioning that\n[36] do not only use"
        },
        {
          "has been added to the regression term. Let us": "(1)\n(2)",
          "introduce the": "",
          "B.\nImplementation and hyperparameters": ""
        },
        {
          "has been added to the regression term. Let us": "⟩+\n⟩ + ⟨β2, ˜Xi",
          "introduce the": "",
          "B.\nImplementation and hyperparameters": "MFCC but also Zero-crossing rate (ZCR),\nroot mean square"
        },
        {
          "has been added to the regression term. Let us": "",
          "introduce the": "",
          "B.\nImplementation and hyperparameters": "(RMS), Mel vector, chroma. And [2] use MFCC and first and"
        },
        {
          "has been added to the regression term. Let us": "⟩)2+\n⟩×⟨β4, X (4)",
          "introduce the": "",
          "B.\nImplementation and hyperparameters": ""
        },
        {
          "has been added to the regression term. Let us": "",
          "introduce the": "",
          "B.\nImplementation and hyperparameters": "second order\nframe-to-frame difference."
        },
        {
          "has been added to the regression term. Let us": "(2)",
          "introduce the": "",
          "B.\nImplementation and hyperparameters": ""
        },
        {
          "has been added to the regression term. Let us": "⟩×⟨β2, ˜Xi\n+ ⟨β3, X (3)",
          "introduce the": "⟩)2 + εi",
          "B.\nImplementation and hyperparameters": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "chunk-level, demonstrate satisfactory performance. The model": "",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": "advances\nin\nfunctional\ndata\nanalysis\nand\nrelated\nmodel,”\nin Recent"
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "leverages new features\nextracted from MFCC and relies on",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": "topics.\nSpringer, 2011, pp. 111–116."
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "functional data, showcasing an innovative approach to emotion",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": "the\n[22] H.-G. M¨uller and F. Yao, “Functional additive models,” Journal of"
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "detection in speech. This development not only contributes",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": "American\nStatistical Association,\nvol.\n103,\nno.\n484,\npp.\n1534–1544,"
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": "2008."
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "to\nthe\nongoing\nevolution\nof\nSER methodologies\nbut\nalso",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": "[23] R. K. Wong, Y. Li,\nand Z. Zhu,\n“Partially linear\nfunctional\nadditive"
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "highlights\nthe potential\nfor\nfurther\nexploration of\nfunctional",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": "Journal\nof\nthe\nAmerican\nmodels\nfor multivariate\nfunctional\ndata,”"
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "data in enhancing emotion recognition systems.",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": "Statistical Association, vol. 114, no. 525, pp. 406–418, 2019."
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": "[24] A. R. Rao\nand M. Reimherr,\n“Nonlinear\nfunctional modeling\nusing"
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "REFERENCES",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": "neural networks,” Journal of Computational and Graphical Statistics,"
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": "pp. 1–10, 2023."
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "[1] B.\nF.\nP. Dossou\nand Y. K.\nS. Gbenou,\n“Fser: Deep\nconvolutional",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": "[25]\nJ. Yao,\nJ. Mueller,\nand\nJ.-L. Wang,\n“Deep\nlearning\nfor\nfunctional"
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "of\nneural\nnetworks\nfor\nspeech\nemotion\nrecognition,”\nin Proceedings",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": "the 38th\ndata\nanalysis with adaptive basis\nlayers,”\nin Proceedings of"
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "the\nIEEE/CVF International Conference on Computer Vision (ICCV)",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": "International Conference\non Machine Learning,\nser. Proceedings\nof"
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "Workshops, October 2021, pp. 3533–3538.",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": "Machine\nLearning\nResearch, M. Meila\nand\nT.\nZhang,\nEds.,\nvol."
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "[2]\nZ. Peng, Y. Lu, S. Pan, and Y. Liu, “Efficient speech emotion recognition",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": "139.\nPMLR, 18–24 Jul 2021, pp. 11 898–11 908.\n[Online]. Available:"
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "using multi-scale CNN and attention,” in ICASSP 2021 - 2021 IEEE",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": "https://proceedings.mlr.press/v139/yao21c.html"
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "International Conference on Acoustics, Speech and Signal Processing",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": "[26]\nF. Rossi, B. Conan-Guez, and F. Fleuret, “Functional data analysis with"
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "(ICASSP).\nIEEE,\njun 2021.",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": "the 2002 International Joint\nmulti\nlayer perceptrons,” in Proceedings of"
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "[3] W. Zhu and X. Li, “Speech emotion recognition with global-aware fusion",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": "Conference\non Neural Networks.\nIJCNN’02\n(Cat. No.\n02CH37290),"
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "on multi-scale\nfeature\nrepresentation,”\nin ICASSP 2022 - 2022 IEEE",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": "vol. 3.\nIEEE, 2002, pp. 2843–2848."
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "International Conference on Acoustics, Speech and Signal Processing",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": "[27] Q. Wang, S. Zheng, A. Farahat, S. Serita, T. Saeki,\nand C. Gupta,"
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "(ICASSP).\nIEEE, may 2022.",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": "“Multilayer perceptron for sparse functional data,” in 2019 International"
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "[4]\nL. Goncalves\nand C. Busso,\n“Improving speech emotion recognition",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": "Joint Conference on Neural Networks (IJCNN).\nIEEE, 2019, pp. 1–10."
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "using self-supervised learning with domain-specific audiovisual\ntasks,”",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": "[28]\nJ. Kim, Y. An,\nand J. Kim,\n“Improving speech emotion recognition"
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "Proc.\nInterspeech 2022, pp. 1168–1172, 2022.",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": "Inter-\nthrough focus\nand calibration attention mechanisms,”\nin Proc."
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "[5]\nE. Morais, R. Hoory, W. Zhu, I. Gat, M. Damasceno, and H. Aronowitz,",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": "speech 2022, 2022, pp. 136–140."
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "“Speech emotion recognition using self-supervised features,” in ICASSP",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": "[29] N. R. Prabhu, G. Carbajal, N. Lehmann-Willenbrock, and T. Gerkmann,"
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "2022 - 2022 IEEE International Conference on Acoustics, Speech and",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": "“End-to-end label uncertainty modeling for speech-based arousal recog-"
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "Signal Processing (ICASSP).\nIEEE, may 2022.",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": "nition using bayesian neural networks,” in Proc. Interspeech 2022, 2022,"
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "[6]\nJ. O. Ramsay and B. W. Silverman, Functional Data Analysis.\nSpringer",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": "pp. 151–155."
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "New York, 2005.",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": "[30] H. Dhamyal, B. Raj, and R. Singh, “Positional encoding for capturing"
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "theory\n[7]\nF. Ferraty and P. Vieu, Nonparametric functional data analysis:",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": "Interspeech\nmodality specific cadence for emotion detection,” in Proc."
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "and practice.\nSpringer Science; Business Media, 2006.",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": "2022, 2022, pp. 166–170."
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "[8]\nS. Robbiano, M. Saumard, and M. Cur´e, “Improving prediction perfor-",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": "[31] M. Perez, M.\nJaiswal, M. Niu, C. Gorrostieta, M. Roddy, K. Taylor,"
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "mance of stellar parameters using functional models,” Journal of Applied",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": "R. Lotfian, J. Kane, and E. M. Provost, “Mind the gap: On the value of"
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "Statistics, vol. 43, no. 8, pp. 1465–1476, 2016.",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": "silence representations to lexical-based speech emotion recognition,” in"
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "[9] W. Saeys, B. De Ketelaere,\nand P. Darius,\n“Potential\napplications of",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": "Proc.\nInterspeech 2022, 2022, pp. 156–160."
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "functional data analysis in chemometrics,” Journal of Chemometrics: A",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": "[32]\nE. Vaaras, M. Airaksinen, and O. R¨as¨anen, “Analysis of self-supervised"
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "Journal of\nthe Chemometrics Society, vol. 22, no. 5, pp. 335–344, 2008.",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": "learning and dimensionality reduction methods in clustering-based active"
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "[10] R. Cao, L. Horv´ath, Z. Liu, and Y. Zhao, “A study of data-driven mo-",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": "learning for\nspeech emotion recognition,”\nin Proc.\nInterspeech 2022,"
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "mentum and disposition effects in the chinese stock market by functional",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": "2022, pp. 1143–1147."
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "data analysis,” Review of Quantitative Finance and Accounting, vol. 54,",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": "[33] M. Baruah and B. Banerjee, “Speech emotion recognition via generation"
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "no. 1, pp. 335–358, 2020.",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": "using an attention-based variational\nrecurrent neural network,” in Proc."
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "[11] D. Bosq, “Estimation of mean and covariance operator of autoregres-",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": "Interspeech 2022, 2022, pp. 4710–4714."
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "Inference\nfor Stochastic\nsive processes\nin banach spaces,” Statistical",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": "[34] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower, S. Kim, J. N."
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "Processes, vol. 5, no. 3, pp. 287–306, 2002.",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": "Chang, S. Lee, and S. S. Narayanan, “Iemocap:\nInteractive emotional"
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "[12] D. Chen\nand H.-G. M¨uller,\n“Nonlinear manifold\nrepresentations\nfor",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": "dyadic motion capture database,” Language resources and evaluation,"
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "functional data,” The Annals of Statistics, vol. 40, no. 1, pp. 1–29, 2012.",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": "vol. 42, no. 4, pp. 335–359, 2008."
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "[13]\nJ. P. Arias, C. Busso, and N. B. Yoma, “Energy and F0 contour modeling",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": "[35] N. Antoniou, A. Katsamanis, T. Giannakopoulos,\nand S. Narayanan,"
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "with functional data analysis for emotional speech detection,” in Proc.",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": "“Designing\nand\nevaluating\nspeech\nemotion\nrecognition\nsystems: A"
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "Interspeech 2013, 2013, pp. 2871–2875.",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": "reality check case\nstudy with iemocap,”\nin ICASSP 2023-2023 IEEE"
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "[14] ——, “Shape-based modeling of the fundamental frequency contour for",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": "International Conference on Acoustics, Speech and Signal Processing"
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "emotion detection in speech,” Computer Speech & Language, vol. 28,",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": "(ICASSP).\nIEEE, 2023, pp. 1–5."
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "no. 1, pp. 278–294,\njan 2014.",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": "[36]\nJ.-K. H. Hyun-Sam Shin,\n“Performance\nanalysis\nof\na\nchunk-based"
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "[15]\nS. Tavakoli, D. Pigoli,\nJ. A. D. Aston, and J. S. Coleman, “A spatial",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": "speech emotion recognition model using rnn,” Intelligent Automation &"
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "modeling approach for\nlinguistic object data: Analyzing dialect\nsound",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": "Soft Computing, vol. 36, no. 1, pp. 235–248, 2023. [Online]. Available:"
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "Journal\nof\nthe American\nStatistical\nvariations\nacross\ngreat\nbritain,”",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": "http://www.techscience.com/iasc/v36n1/50034"
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "Association, vol. 114, no. 527, pp. 1081–1096,\njul 2019.",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "[16] W.-C. Lin and C. Busso, “An efficient\ntemporal modeling approach for",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "speech emotion recognition by mapping varied duration sentences into",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "fixed number of chunks,” Interspeech 2020, 2020.",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "[17]\nP. Kumawat and A. Routray, “Applying tdnn architectures for analyzing",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "duration dependencies on speech emotion recognition.” in Interspeech,",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "2021, pp. 3410–3414.",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "[18] W.-C. Lin and C. Busso, “Chunk-level\nspeech emotion recognition: A",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "general\nframework\nof\nsequence-to-one\ndynamic\ntemporal modeling,”",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "IEEE Transactions on Affective Computing, vol. 14, no. 2, pp. 1215–",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "1227, 2023.",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "[19]\nF. Jiang, S. Baek, J. Cao, and Y. Ma, “A functional single-index model,”",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "Statistica sinica, vol. 30, no. 1, pp. 303–324, 2020.",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "[20] C.-R.\nJiang\nand\nJ.-L. Wang,\n“Functional\nsingle\nindex models\nfor",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "longitudinal data,” The Annals of Statistics, vol. 39, no. 1, pp. 362–388,",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        },
        {
          "chunk-level, demonstrate satisfactory performance. The model": "2011.\n[Online]. Available: http://www.jstor.org/stable/29783641",
          "[21]\nF. Ferraty, J. Park, and P. Vieu, “Estimation of a functional single index": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Fser: Deep convolutional neural networks for speech emotion recognition",
      "authors": [
        "B Dossou",
        "Y Gbenou"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops"
    },
    {
      "citation_id": "2",
      "title": "Efficient speech emotion recognition using multi-scale CNN and attention",
      "authors": [
        "Z Peng",
        "Y Lu",
        "S Pan",
        "Y Liu"
      ],
      "year": "2021",
      "venue": "ICASSP 2021 -2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "3",
      "title": "Speech emotion recognition with global-aware fusion on multi-scale feature representation",
      "authors": [
        "W Zhu",
        "X Li"
      ],
      "year": "2022",
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "4",
      "title": "Improving speech emotion recognition using self-supervised learning with domain-specific audiovisual tasks",
      "authors": [
        "L Goncalves",
        "C Busso"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech 2022"
    },
    {
      "citation_id": "5",
      "title": "Speech emotion recognition using self-supervised features",
      "authors": [
        "E Morais",
        "R Hoory",
        "W Zhu",
        "I Gat",
        "M Damasceno",
        "H Aronowitz"
      ],
      "year": "2022",
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "6",
      "title": "Functional Data Analysis",
      "authors": [
        "J Ramsay",
        "B Silverman"
      ],
      "year": "2005",
      "venue": "Functional Data Analysis"
    },
    {
      "citation_id": "7",
      "title": "Nonparametric functional data analysis: theory and practice",
      "authors": [
        "F Ferraty",
        "P Vieu"
      ],
      "year": "2006",
      "venue": "Nonparametric functional data analysis: theory and practice"
    },
    {
      "citation_id": "8",
      "title": "Improving prediction performance of stellar parameters using functional models",
      "authors": [
        "S Robbiano",
        "M Saumard",
        "M Curé"
      ],
      "year": "2016",
      "venue": "Journal of Applied Statistics"
    },
    {
      "citation_id": "9",
      "title": "Potential applications of functional data analysis in chemometrics",
      "authors": [
        "W Saeys",
        "B De Ketelaere",
        "P Darius"
      ],
      "year": "2008",
      "venue": "Journal of Chemometrics: A Journal of the Chemometrics Society"
    },
    {
      "citation_id": "10",
      "title": "A study of data-driven momentum and disposition effects in the chinese stock market by functional data analysis",
      "authors": [
        "R Cao",
        "L Horváth",
        "Z Liu",
        "Y Zhao"
      ],
      "year": "2020",
      "venue": "Review of Quantitative Finance and Accounting"
    },
    {
      "citation_id": "11",
      "title": "Estimation of mean and covariance operator of autoregressive processes in banach spaces",
      "authors": [
        "D Bosq"
      ],
      "year": "2002",
      "venue": "Statistical Inference for Stochastic Processes"
    },
    {
      "citation_id": "12",
      "title": "Nonlinear manifold representations for functional data",
      "authors": [
        "D Chen",
        "H.-G Müller"
      ],
      "year": "2012",
      "venue": "The Annals of Statistics"
    },
    {
      "citation_id": "13",
      "title": "Energy and F0 contour modeling with functional data analysis for emotional speech detection",
      "authors": [
        "J Arias",
        "C Busso",
        "N Yoma"
      ],
      "year": "2013",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "14",
      "title": "Shape-based modeling of the fundamental frequency contour for emotion detection in speech",
      "year": "2014",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "15",
      "title": "A spatial modeling approach for linguistic object data: Analyzing dialect sound variations across great britain",
      "authors": [
        "S Tavakoli",
        "D Pigoli",
        "J Aston",
        "J Coleman"
      ],
      "year": "2019",
      "venue": "Journal of the American Statistical Association"
    },
    {
      "citation_id": "16",
      "title": "An efficient temporal modeling approach for speech emotion recognition by mapping varied duration sentences into fixed number of chunks",
      "authors": [
        "W.-C Lin",
        "C Busso"
      ],
      "year": "2020",
      "venue": "An efficient temporal modeling approach for speech emotion recognition by mapping varied duration sentences into fixed number of chunks"
    },
    {
      "citation_id": "17",
      "title": "Applying tdnn architectures for analyzing duration dependencies on speech emotion recognition",
      "authors": [
        "P Kumawat",
        "A Routray"
      ],
      "year": "2021",
      "venue": "Applying tdnn architectures for analyzing duration dependencies on speech emotion recognition"
    },
    {
      "citation_id": "18",
      "title": "Chunk-level speech emotion recognition: A general framework of sequence-to-one dynamic temporal modeling",
      "authors": [
        "W.-C Lin",
        "C Busso"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "19",
      "title": "A functional single-index model",
      "authors": [
        "F Jiang",
        "S Baek",
        "J Cao",
        "Y Ma"
      ],
      "year": "2020",
      "venue": "Statistica sinica"
    },
    {
      "citation_id": "20",
      "title": "Functional single index models for longitudinal data",
      "authors": [
        "C.-R Jiang",
        "J.-L Wang"
      ],
      "year": "2011",
      "venue": "The Annals of Statistics"
    },
    {
      "citation_id": "21",
      "title": "Estimation of a functional single index model",
      "authors": [
        "F Ferraty",
        "J Park",
        "P Vieu"
      ],
      "year": "2011",
      "venue": "Recent advances in functional data analysis and related topics"
    },
    {
      "citation_id": "22",
      "title": "Functional additive models",
      "authors": [
        "H.-G Müller",
        "F Yao"
      ],
      "year": "2008",
      "venue": "Journal of the American Statistical Association"
    },
    {
      "citation_id": "23",
      "title": "Partially linear functional additive models for multivariate functional data",
      "authors": [
        "R Wong",
        "Y Li",
        "Z Zhu"
      ],
      "year": "2019",
      "venue": "Journal of the American Statistical Association"
    },
    {
      "citation_id": "24",
      "title": "Nonlinear functional modeling using neural networks",
      "authors": [
        "A Rao",
        "M Reimherr"
      ],
      "year": "2023",
      "venue": "Journal of Computational and Graphical Statistics"
    },
    {
      "citation_id": "25",
      "title": "Deep learning for functional data analysis with adaptive basis layers",
      "authors": [
        "J Yao",
        "J Mueller",
        "J.-L Wang"
      ],
      "venue": "Proceedings of the 38th International Conference on Machine Learning, ser. Proceedings of Machine Learning"
    },
    {
      "citation_id": "26",
      "title": "",
      "authors": [
        "Pmlr"
      ],
      "year": "2021",
      "venue": ""
    },
    {
      "citation_id": "27",
      "title": "Functional data analysis with multi layer perceptrons",
      "authors": [
        "F Rossi",
        "B Conan-Guez",
        "F Fleuret"
      ],
      "year": "2002",
      "venue": "Proceedings of the 2002 International Joint Conference on Neural Networks. IJCNN'02"
    },
    {
      "citation_id": "28",
      "title": "Multilayer perceptron for sparse functional data",
      "authors": [
        "Q Wang",
        "S Zheng",
        "A Farahat",
        "S Serita",
        "T Saeki",
        "C Gupta"
      ],
      "year": "2019",
      "venue": "2019 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "29",
      "title": "Improving speech emotion recognition through focus and calibration attention mechanisms",
      "authors": [
        "J Kim",
        "Y An",
        "J Kim"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "30",
      "title": "End-to-end label uncertainty modeling for speech-based arousal recognition using bayesian neural networks",
      "authors": [
        "N Prabhu",
        "G Carbajal",
        "N Lehmann-Willenbrock",
        "T Gerkmann"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "31",
      "title": "Positional encoding for capturing modality specific cadence for emotion detection",
      "authors": [
        "H Dhamyal",
        "B Raj",
        "R Singh"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "32",
      "title": "Mind the gap: On the value of silence representations to lexical-based speech emotion recognition",
      "authors": [
        "M Perez",
        "M Jaiswal",
        "M Niu",
        "C Gorrostieta",
        "M Roddy",
        "K Taylor",
        "R Lotfian",
        "J Kane",
        "E Provost"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "33",
      "title": "Analysis of self-supervised learning and dimensionality reduction methods in clustering-based active learning for speech emotion recognition",
      "authors": [
        "E Vaaras",
        "M Airaksinen",
        "O Räsänen"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "34",
      "title": "Speech emotion recognition via generation using an attention-based variational recurrent neural network",
      "authors": [
        "M Baruah",
        "B Banerjee"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "35",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "36",
      "title": "Designing and evaluating speech emotion recognition systems: A reality check case study with iemocap",
      "authors": [
        "N Antoniou",
        "A Katsamanis",
        "T Giannakopoulos",
        "S Narayanan"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "37",
      "title": "Performance analysis of a chunk-based speech emotion recognition model using rnn",
      "authors": [
        "J.-K Hyun",
        "-Sam Shin"
      ],
      "year": "2023",
      "venue": "Intelligent Automation & Soft Computing"
    }
  ]
}