{
  "paper_id": "2010.14607v1",
  "title": "Deformable Convolutional Lstm For Human Body Emotion Recognition",
  "published": "2020-10-27T21:01:09Z",
  "authors": [
    "Peyman Tahghighi",
    "Abbas Koochari",
    "Masoume Jalali"
  ],
  "keywords": [
    "Human emotion recognition",
    "Deformable convolutions",
    "Convolutional long short-term memory",
    "Recurrent neural networks",
    "Long short-term memory"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "People represent their emotions in a myriad of ways. Among the most important ones is whole body expressions which have many applications in different fields such as human-computer interaction (HCI). One of the most important challenges in human emotion recognition is that people express the same feeling in various ways using their face and their body. Recently many methods have tried to overcome these challenges using Deep Neural Networks (DNNs). However, most of these methods were based on images or on facial expressions only and did not consider deformation that may happen in the images such as scaling and rotation which can adversely affect the recognition accuracy. In this work, motivated by recent researches on deformable convolutions, we incorporate the deformable behavior into the core of convolutional long short-term memory (ConvLSTM) to improve robustness to these deformations in the image and, consequently, improve its accuracy on the emotion recognition task from videos of arbitrary length. We did experiments on the GEMEP dataset and achieved state-of-the-art accuracy of 98.8% on the task of whole human body emotion recognition on the validation set.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Understanding and interpreting human emotions from videos have many different applications especially in the field of Human Computer/Robotic Interaction (HCI/HRI)  [14] . Humans can represent different emotions by their behavior using verbal and non-verbal signs during a conversation. Generally, human communication can be classified as verbal and nonverbal. The verbal communication includes voice and its tune, while nonverbal includes body movement, facial expression and gestures  [12, 6] . So far, many researchers have tried to analyze and classify emotion from a single image or a stream of frames. For instance, Jain et al.  [8] , used images of the face and a deep convolutional neural network (CNN) based on ResNet  [7]  for classification of emotions based on human faces, while  [10]  used a similar idea to classify whole body emotion. Nevertheless, both of these methods were based on fixed images, while we are trying to classify emotion from videos. Moreover, Jeong et al.  [9]  used 3D CNNs on a stream of facial videos for classification, but their method was limited to facial videos. Considering whole human body emotion recognition, Ahmed et al.  [1]  used features extracted from human body movement to classify different emotions, while Santhoshkumar et al.  [13]  used features extracted from two consecutive frames using their difference. Some other methods relied on multi modalities such as video and audio. For instance, Chen et al.  [3]  leveraged both audio and video in a multiple feature fusion method to classify videos. However, in this paper, we try to use video frames only and the whole human body to classify emotions.\n\nMotivated by the recent research on deformable 2D convolutions  [4, 17]  which showed its effectiveness in classification and object recognition, in this paper, we tried to incorporate deformable convolutions into the core of ConvLSTMs  [15]  in order to improve its flexibility to detect and extract features from a given frame. That is to say, since the offsets in the deformable convolutional layer allow free deformation of the sampling grid (receptive field) in comparison to fix grid in regular convolution layers, they can perform better at capturing and focusing on the salient part of frames. Furthermore, we combined deformable ConvLSTMs with 3D convolutions to extract both long-term and short-term spatio-temporal features from videos. Afterward, we used a shallow 2D CNN architecture to extract features from each 2D spatio-temporal feature map individually and finally, we used feature fusion for final classification. We experimented with GEMEP  [2]  dataset which contains 145 videos representing 17 different emotions and achieved state-of-the-art accuracy of 98.8% on the validation set.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Method",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Input Preprocessing",
      "text": "Different individuals may represent different emotions at various speeds. Hence, videos in our dataset contain an arbitrary number of frames and we had to set all videos to a fixed number of frames. One of the ways was to split each video into an array of videos with fixed frames, but one clip sometimes cannot represent the whole emotion. Consequently, we used the idea of Uniform sampling with temporal jitter  [16]  to fix the length of all videos to 32 frames. If the length of one video was less than 32, we repeated the last frame. Moreover, we reduced the original frame size of videos from 720 × 576 to 112 × 112 to reduce the parameter size of our network.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Network Architecture",
      "text": "As can be seen in Figure  1 , our network is consists of three major components which in this section we elaborate on each of them thoroughly.\n\n3D CNN component Inspired by  [16] , since 3D convolutions can perform better at learning local and short-term spatio-temporal features, in the first layer we used a 3D CNN component. As can be seen in Figure  2a , we only used two max pooling operations which only one of them shrinks the video length. Consequently, this component only focuses on the short-term spatio-temporal features and we leave the learning of long-term spatio-temporal features to the deformable convolutional LSTMs. Deformable convolutional LSTMs Recurrent Neural Networks (RNNs) and LSTMs are more suitable for learning long-term spatio-temporal features  [16] . Nevertheless, ConvLSTMs are inherently limited to model large, unknown transformations since we use regular CNNs with a fixed receptive field at the core of them. Hence, so as to improve the robustness of traditional ConvLSTMs to these transformations, we used deformable 2D convolutions in the core of Con-vLSTMs. Additionally, since deformable convolutions require a large number of parameters, we could not use them instead of every normal ConvLSTM layer.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "3D Cnn Component",
      "text": "Consequently, we decided to use deformable ConvLSTMs on certain frames only. Since the characters usually start to represent the emotion after some frames and salient parts for classification usually happen in the middle parts of a video, we decided to choose frames after 25% of video have passed, after half of the video has passed and after 75% of video have passed. In each of these parts, we chose three different frames. Moreover, as stated in  [16] , we removed the convolutional structure of all gates except for the input-to-state gate for spatio-temporal feature fusion which we used deformable 2D convolutions.\n\n2D CNN component So far, we have reduced the spatial dimension of our input to 28 × 28, but already, we have learned short-term and long-term spatiotemporal features. Therefore, in this part, we try to learn and focus on spatial features only using a 2D CNN component. As can be seen in Figure  2b , we have chosen a shallow network with three convolutional and average pooling layers. After this stage, we did a final global average pooling for final feature fusion among all frames before using a fully connected layer for final classification.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Dataset",
      "text": "For experimental studies, we used the GEMEP dataset which contains 145 videos representing 17 different emotions which are called compound emotions in the literature  [5] . The different emotion classes are admiration, amusement, tenderness, anger, disgust, despair, pride, anxiety, interest, irritation, joy, contempt, fear, pleasure, relief, surprise and sadness. Since the number of videos for training our network was insufficient for training a deep neural network, we did data augmentation. For augmentation, we translated each frame to four different corners by 25 pixels, rotated each video between -30 • to 30 • , used gaussian filter with different intensities and changed the brightness. Finally, we had 9052 videos which since there were not any standard division into training and validation set for this dataset, we decided to use 80% for training and 20% for validation. Some frames of the dataset utilized in this work are depicted in Figure  3 . Table  1 : Comparison between using deformable ConvLSTM and normal ConvL-STM. Note that we only applied deformable ConvLSTM to 9 different frames.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Comparison Between Deformable Convlstm And Convlstm",
      "text": "In this section, we compare the result of using normal ConvLSTM and deformable ConvLSTM. Since deformation may happen in any given image, it makes it difficult for a fixed size receptive field to focus on the salient parts of the image. On the other hand, deformable convolutions due to their flexible nature can easily focus on these deformed parts and extract features using their arbitrary shaped receptive field. As can be seen in Table  1 , even though we used deformable ConvLSTM only on a limited number of frames (9 frames) we were able to achieve a higher classification accuracy in comparison to normal ConvLSTM which prove their effectiveness.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Comparison Between Other Methods",
      "text": "As can be seen in Table  2 , our method based on deformable ConvLSTMs outperformed other methods in terms of accuracy. For comparison, we selected methods that used the whole human body for classification. The method in  [10]  used a deep architecture for classification using images extracted from videos. The superior result of our approach indicates that in some frames the emotion may become ambiguous and by using spatio-temporal features we can clarify this ambiguity and achieve higher accuracy. Additionally, both MBMIC  [13]  and HOG-KLT  [11]  used frame difference to extract temporal features. However, results show that using 3D convolutions and ConvLSTMs for learning and extracting spatio-temporal features is the superior choice.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Accuracy Ours",
      "text": "98.8 FDCNN (Image)  [10]  95.4 MBMIC  [13]  94.6 HOG-KLT  [11]  95.9\n\nTable  2 : Comparison between our proposed method and other methods on the GEMEP dataset. Note that we only chose approaches which considered whole human body and the results of our method is based on validation set.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we proposed a method for the classification of human wholebody emotion from videos. In the first layer, we used 3D convolutions to extract short-term spatio-temporal features. Afterward, motivated by the recent success of deformable 2D convolutions we incorporated these flexible convolutions in the core of normal ConvLSTMs to form deformable ConvLSTMs. Because the receptive in these forms of convolutions are adjustable, it gives the network more freedom to extract features from different regions of a given frame. Finally, we used a shallow 2D convolutional network to further extract features from the 2D features map and used a final feature fusion. Results using GEMEP dataset show a state-of-the-art result of 98.8% accuracy on the validation set.\n\nIn future works, we will try to study the effectiveness of our deformable ConvLSTMs on other classification tasks which involve videos such as human gesture recognition or facial emotion recognition. Additionally, we will explore with different frame selection strategies for using deformable ConvLSTMs so as to find the optimal choice.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , our network is consists of three major components",
      "page": 3
    },
    {
      "caption": "Figure 2: a, we only used",
      "page": 3
    },
    {
      "caption": "Figure 1: Network architecture for emotion classiﬁcation.",
      "page": 3
    },
    {
      "caption": "Figure 2: b, we have",
      "page": 4
    },
    {
      "caption": "Figure 2: (a) 3D CNN component that is used to extract short-term spatio-temporal",
      "page": 4
    },
    {
      "caption": "Figure 3: Fig. 3: Some examples of the utilized GEMEP dataset in our experiments.",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Tehran, Iran": "Abstract. People represent their emotions in a myriad of ways. Among"
        },
        {
          "Tehran, Iran": "the most important ones is whole body expressions which have many ap-"
        },
        {
          "Tehran, Iran": "plications in diﬀerent ﬁelds such as human-computer interaction (HCI)."
        },
        {
          "Tehran, Iran": "One of the most important challenges in human emotion recognition is"
        },
        {
          "Tehran, Iran": "that people\nexpress\nthe\nsame\nfeeling in various ways using their\nface"
        },
        {
          "Tehran, Iran": "and their body. Recently many methods have tried to overcome these"
        },
        {
          "Tehran, Iran": "challenges using Deep Neural Networks (DNNs). However, most of these"
        },
        {
          "Tehran, Iran": "methods were based on images or on facial\nexpressions only and did"
        },
        {
          "Tehran, Iran": "not consider deformation that may happen in the images such as scal-"
        },
        {
          "Tehran, Iran": "ing and rotation which can adversely aﬀect the recognition accuracy. In"
        },
        {
          "Tehran, Iran": "this work, motivated by recent\nresearches on deformable convolutions,"
        },
        {
          "Tehran, Iran": "we incorporate the deformable behavior\ninto the core of convolutional"
        },
        {
          "Tehran, Iran": "long short-term memory (ConvLSTM)\nto improve robustness\nto these"
        },
        {
          "Tehran, Iran": "deformations\nin the image and, consequently,\nimprove its accuracy on"
        },
        {
          "Tehran, Iran": "the emotion recognition task from videos of arbitrary length. We did ex-"
        },
        {
          "Tehran, Iran": "periments on the GEMEP dataset and achieved state-of-the-art accuracy"
        },
        {
          "Tehran, Iran": "of 98.8% on the task of whole human body emotion recognition on the"
        },
        {
          "Tehran, Iran": "validation set."
        },
        {
          "Tehran, Iran": "·\nKeywords: Human emotion recognition · Deformable\nconvolutions"
        },
        {
          "Tehran, Iran": "·\nConvolutional\nlong short-term memory · Recurrent neural networks"
        },
        {
          "Tehran, Iran": "Long short-term memory."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1": "Understanding and interpreting human emotions from videos have many diﬀer-",
          "Introduction": ""
        },
        {
          "1": "ent applications especially in the ﬁeld of Human Computer/Robotic Interaction",
          "Introduction": ""
        },
        {
          "1": "(HCI/HRI)",
          "Introduction": "[14]. Humans"
        },
        {
          "1": "using verbal and non-verbal signs during a conversation. Generally, human com-",
          "Introduction": ""
        },
        {
          "1": "munication can be classiﬁed as verbal and nonverbal. The verbal communication",
          "Introduction": ""
        },
        {
          "1": "includes voice and its tune, while nonverbal",
          "Introduction": ""
        },
        {
          "1": "pression and gestures [12,6].",
          "Introduction": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2\nP.Tahghighi et al.": "So far, many researchers have tried to analyze and classify emotion from a"
        },
        {
          "2\nP.Tahghighi et al.": "single image or a stream of\nframes. For instance, Jain et al.\n[8], used images of"
        },
        {
          "2\nP.Tahghighi et al.": "the face and a deep convolutional neural network (CNN) based on ResNet [7] for"
        },
        {
          "2\nP.Tahghighi et al.": "classiﬁcation of emotions based on human faces, while [10] used a similar idea"
        },
        {
          "2\nP.Tahghighi et al.": "to classify whole body emotion. Nevertheless, both of these methods were based"
        },
        {
          "2\nP.Tahghighi et al.": "on ﬁxed images, while we are trying to classify emotion from videos. Moreover,"
        },
        {
          "2\nP.Tahghighi et al.": "Jeong et al.\n[9] used 3D CNNs on a stream of\nfacial videos\nfor classiﬁcation,"
        },
        {
          "2\nP.Tahghighi et al.": "but their method was limited to facial videos. Considering whole human body"
        },
        {
          "2\nP.Tahghighi et al.": "emotion recognition, Ahmed et al. [1] used features extracted from human body"
        },
        {
          "2\nP.Tahghighi et al.": "movement\nto classify diﬀerent emotions, while Santhoshkumar et al.\n[13] used"
        },
        {
          "2\nP.Tahghighi et al.": "features extracted from two consecutive frames using their diﬀerence. Some other"
        },
        {
          "2\nP.Tahghighi et al.": "methods relied on multi modalities such as video and audio. For instance, Chen"
        },
        {
          "2\nP.Tahghighi et al.": "et al.\n[3]\nleveraged both audio and video in a multiple feature fusion method to"
        },
        {
          "2\nP.Tahghighi et al.": "classify videos. However,\nin this paper, we try to use video frames only and the"
        },
        {
          "2\nP.Tahghighi et al.": "whole human body to classify emotions."
        },
        {
          "2\nP.Tahghighi et al.": "Motivated by the recent research on deformable 2D convolutions [4,17] which"
        },
        {
          "2\nP.Tahghighi et al.": "showed its eﬀectiveness in classiﬁcation and object recognition,\nin this paper, we"
        },
        {
          "2\nP.Tahghighi et al.": "tried to incorporate deformable convolutions into the core of ConvLSTMs [15] in"
        },
        {
          "2\nP.Tahghighi et al.": "order to improve its ﬂexibility to detect and extract features from a given frame."
        },
        {
          "2\nP.Tahghighi et al.": "That is to say, since the oﬀsets in the deformable convolutional\nlayer allow free"
        },
        {
          "2\nP.Tahghighi et al.": "deformation of\nthe sampling grid (receptive ﬁeld)\nin comparison to ﬁx grid in"
        },
        {
          "2\nP.Tahghighi et al.": "regular convolution layers, they can perform better at capturing and focusing on"
        },
        {
          "2\nP.Tahghighi et al.": "the salient part of\nframes. Furthermore, we combined deformable ConvLSTMs"
        },
        {
          "2\nP.Tahghighi et al.": "with 3D convolutions to extract both long-term and short-term spatio-temporal"
        },
        {
          "2\nP.Tahghighi et al.": "features from videos. Afterward, we used a shallow 2D CNN architecture to ex-"
        },
        {
          "2\nP.Tahghighi et al.": "tract features from each 2D spatio-temporal feature map individually and ﬁnally,"
        },
        {
          "2\nP.Tahghighi et al.": "we used feature fusion for ﬁnal classiﬁcation. We experimented with GEMEP"
        },
        {
          "2\nP.Tahghighi et al.": "[2] dataset which contains 145 videos\nrepresenting 17 diﬀerent\nemotions and"
        },
        {
          "2\nP.Tahghighi et al.": "achieved state-of-the-art accuracy of 98.8% on the validation set."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition\n3": "2.2\nNetwork architecture"
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition\n3": "As can be seen in Figure 1, our network is consists of three major components"
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition\n3": "which in this section we elaborate on each of them thoroughly."
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition\n3": "3D CNN component\nInspired by [16],\nsince 3D convolutions\ncan perform"
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition\n3": "better at\nlearning local and short-term spatio-temporal\nfeatures,\nin the ﬁrst"
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition\n3": "layer we used a 3D CNN component. As can be seen in Figure 2a, we only used"
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition\n3": "two max pooling operations which only one of\nthem shrinks\nthe video length."
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition\n3": "Consequently,\nthis component only focuses on the short-term spatio-temporal"
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition\n3": "features and we leave the learning of\nlong-term spatio-temporal\nfeatures to the"
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition\n3": "deformable convolutional LSTMs."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4\nP.Tahghighi et al.": "Consequently, we decided to use deformable ConvLSTMs on certain frames only."
        },
        {
          "4\nP.Tahghighi et al.": "Since the characters usually start to represent the emotion after some frames and"
        },
        {
          "4\nP.Tahghighi et al.": "salient parts for classiﬁcation usually happen in the middle parts of a video, we"
        },
        {
          "4\nP.Tahghighi et al.": "decided to choose frames after 25% of video have passed, after half of the video"
        },
        {
          "4\nP.Tahghighi et al.": "has passed and after 75% of video have passed. In each of these parts, we chose"
        },
        {
          "4\nP.Tahghighi et al.": "three diﬀerent frames. Moreover, as stated in [16], we removed the convolutional"
        },
        {
          "4\nP.Tahghighi et al.": "structure of all gates except for the input-to-state gate for spatio-temporal"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Leaky Relu": "Batch Normalization",
          "Avg Pooling 2x2": "Leaky Relu"
        },
        {
          "Leaky Relu": "3D Conv 3x3x3*256",
          "Avg Pooling 2x2": "Batch Normalization"
        },
        {
          "Leaky Relu": "Avg Pooling 2x2x2",
          "Avg Pooling 2x2": "2D Conv 3x3*256"
        },
        {
          "Leaky Relu": "Leaky Relu",
          "Avg Pooling 2x2": "Avg Pooling 2x2"
        },
        {
          "Leaky Relu": "Batch Normalization",
          "Avg Pooling 2x2": "Leaky Relu"
        },
        {
          "Leaky Relu": "3D Conv 3x3x3*128",
          "Avg Pooling 2x2": "Batch Normalization"
        },
        {
          "Leaky Relu": "Avg Pooling 1x2x2",
          "Avg Pooling 2x2": "2D Conv 3x3*256"
        },
        {
          "Leaky Relu": "Leaky Relu",
          "Avg Pooling 2x2": "Avg Pooling 2x2"
        },
        {
          "Leaky Relu": "Batch Normalization",
          "Avg Pooling 2x2": "Leaky Relu"
        },
        {
          "Leaky Relu": "3D Conv 3x3x3*64",
          "Avg Pooling 2x2": "Batch Normalization"
        },
        {
          "Leaky Relu": "",
          "Avg Pooling 2x2": "2D Conv 3x3*128"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 1: , even though we",
      "data": [
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition": "3\nExperiments",
          "5": ""
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition": "3.1\nDataset",
          "5": ""
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition": "For experimental studies, we used the GEMEP dataset which contains 145 videos",
          "5": ""
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition": "representing 17 diﬀerent emotions which are called compound emotions",
          "5": "in the"
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition": "literature [5]. The diﬀerent emotion classes are admiration, amusement, tender-",
          "5": ""
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition": "ness, anger, disgust, despair, pride, anxiety,\ninterest,\nirritation,",
          "5": "joy, contempt,"
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition": "fear, pleasure, relief, surprise and sadness. Since the number of videos for train-",
          "5": ""
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition": "ing our network was insuﬃcient for training a deep neural network, we did data",
          "5": ""
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition": "augmentation. For augmentation, we translated each frame to four diﬀerent cor-",
          "5": ""
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition": "ners by 25 pixels, rotated each video between −30◦ to 30◦, used gaussian ﬁlter",
          "5": ""
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition": "with diﬀerent intensities and changed the brightness. Finally, we had 9052 videos",
          "5": ""
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition": "which since there were not any standard division into training and validation set",
          "5": ""
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition": "for this dataset, we decided to use 80% for training and 20% for validation. Some",
          "5": ""
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition": "frames of the dataset utilized in this work are depicted in Figure 3.",
          "5": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 2: , our method based on deformable ConvLSTMs out-",
      "data": [
        {
          "6\nP.Tahghighi et al.": "used deformable ConvLSTM only on a limited number of\nframes (9 frames) we"
        },
        {
          "6\nP.Tahghighi et al.": "were able to achieve a higher classiﬁcation accuracy in comparison to normal"
        },
        {
          "6\nP.Tahghighi et al.": "ConvLSTM which prove their eﬀectiveness."
        },
        {
          "6\nP.Tahghighi et al.": "3.3\nComparison between other methods"
        },
        {
          "6\nP.Tahghighi et al.": "As can be seen in Table 2, our method based on deformable ConvLSTMs out-"
        },
        {
          "6\nP.Tahghighi et al.": "performed other methods\nin terms of accuracy. For\ncomparison, we\nselected"
        },
        {
          "6\nP.Tahghighi et al.": "methods that used the whole human body for classiﬁcation. The method in [10]"
        },
        {
          "6\nP.Tahghighi et al.": "used a deep architecture for classiﬁcation using images extracted from videos."
        },
        {
          "6\nP.Tahghighi et al.": "The superior result of our approach indicates that in some frames the emotion"
        },
        {
          "6\nP.Tahghighi et al.": "may become ambiguous and by using spatio-temporal\nfeatures we can clarify"
        },
        {
          "6\nP.Tahghighi et al.": "this ambiguity and achieve higher accuracy. Additionally, both MBMIC [13]"
        },
        {
          "6\nP.Tahghighi et al.": "and HOG-KLT [11] used frame diﬀerence to extract\ntemporal\nfeatures. How-"
        },
        {
          "6\nP.Tahghighi et al.": "ever, results show that using 3D convolutions and ConvLSTMs for learning and"
        },
        {
          "6\nP.Tahghighi et al.": "extracting spatio-temporal\nfeatures is the superior choice."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition\n7": "References"
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition\n7": "1. Ahmed, F., Bari, A., Gavrilova, M.: Emotion recognition from body movement."
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition\n7": "IEEE Access PP,\n1–1 (12 2019). https://doi.org/10.1109/ACCESS.2019.2963113"
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition\n7": "2. B¨anziger, T., Scherer, K.:\nIntroducing the geneva multimodal emotion portrayal"
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition\n7": "(gemep) corpus. Blueprint for Aﬀective Computing: A Sourcebook (01 2010)"
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition\n7": "3. Chen, J., Chen, Z., Chi, Z., Fu, H.: Facial expression recognition in video with"
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition\n7": "multiple feature fusion. IEEE Transactions on Aﬀective Computing PP,\n1–1 (07"
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition\n7": "2016). https://doi.org/10.1109/TAFFC.2016.2593719"
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition\n7": "4. Dai, J., Qi, H., Xiong, Y., Li, Y., Zhang, G., Hu, H., Wei, Y.: Deformable convo-"
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition\n7": "lutional networks (2017)"
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition\n7": "5. Du, S., Martinez, A.: Compound facial expressions of emotion: From basic research"
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition\n7": "to clinical applications. Dialogues in Clinical Neuroscience 17, 443–455 (12 2015)"
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition\n7": "6. Glowinski, D., Dael, N., Camurri, A., Volpe, G., Mortillaro, M., Scherer, K.: Toward"
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition\n7": "a minimal representation of aﬀective gestures. T. Aﬀective Computing 2, 106–118"
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition\n7": "(04 2011). https://doi.org/10.1109/T-AFFC.2011.7"
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition\n7": "7. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual\nlearning for image recognition"
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition\n7": "(2015)"
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition\n7": "8.\nJain,\nD.,\nShamsolmoali,\nP.,\nSehdev,\nP.:\nExtended\ndeep\nneural\nnetwork"
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition\n7": "for\nfacial\nemotion\nrecognition.\nPattern Recognition\nLetters\n120\n(04\n2019)."
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition\n7": "https://doi.org/10.1016/j.patrec.2019.01.008"
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition\n7": "9.\nJeong,\nD.,\nKim,\nB.G.,\nDong,\nS.Y.:\nDeep\njoint\nspatiotemporal\nnetwork"
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition\n7": "(djstn)\nfor\neﬃcient\nfacial\nexpression\nrecognition.\nSensors\n20(7)\n(2020)."
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition\n7": "https://doi.org/10.3390/s20071936,\nhttps://www.mdpi.com/1424-8220/20/"
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition\n7": "7/1936"
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition\n7": "10. Rajaram,\nS.,\nGeetha,\nM.:\nDeep\nlearning\napproach\nfor\nemotion\nrecog-"
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition\n7": "nition\nfrom\nhuman\nbody\nmovements\nwith\nfeedforward\ndeep\nconvolution"
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition\n7": "neural\nnetworks.\nProcedia\nComputer\nScience\n152,\n158–165\n(01\n2019)."
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition\n7": "https://doi.org/10.1016/j.procs.2019.05.038"
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition\n7": "11. Rajaram, S., Geetha, M.: Vision-Based Human Emotion Recognition Using HOG-"
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition\n7": "KLT Feature, pp. 261–272 (01 2020). https://doi.org/10.1007/978-981-15-3369-3-"
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition\n7": "20"
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition\n7": "12. Rajaram, S., Geetha, M., J, A.: Svm-knn based emotion recognition of human in"
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition\n7": "video using hog feature and klt tracking algorithm. International Journal of Pure"
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition\n7": "and Applied Mathematics 117, 621–634 (01 2017)"
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition\n7": "13. Santhoshkumar,\nR.,\nGeetha, M.K.:\nEmotion\nRecognition\non Multi\nView"
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition\n7": "Static Action Videos Using Multi\nBlocks Maximum Intensity\nCode\n(MB-"
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition\n7": "MIC),\npp.\n1143–1151.\nSpringer\nInternational\nPublishing,\nCham\n(2020)."
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition\n7": "https://doi.org/10.1007/978-3-030-41862-5-116,\nhttps://doi.org/10.1007/"
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition\n7": "978-3-030-41862-5-116"
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition\n7": "14. Sharma, G., Dhall, A.: A Survey\non Automatic Multimodal Emotion Recog-"
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition\n7": "nition\nin\nthe Wild,\npp.\n35–64.\nSpringer\nInternational\nPublishing,\nCham"
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition\n7": "(2021). https://doi.org/10.1007/978-3-030-51870-7-3, https://doi.org/10.1007/"
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition\n7": "978-3-030-51870-7-3"
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition\n7": "15. Shi, X., Chen, Z., Wang, H., Yeung, D.Y., kin Wong, W., chun Woo, W.: Convo-"
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition\n7": "lutional\nlstm network: A machine learning approach for precipitation nowcasting"
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition\n7": "(2015)"
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition\n7": "16. Zhang, L., Zhu, G., Shen, P., Song, J.: Learning spatiotemporal\nfeatures using"
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition\n7": "3dcnn and convolutional\nlstm for gesture recognition. pp. 3120–3128 (10 2017)."
        },
        {
          "Deformable Convolutional LSTM for Human Body Emotion Recognition\n7": "https://doi.org/10.1109/ICCVW.2017.369"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "8": "17. Zhu, X., Hu, H., Lin, S., Dai, J.: Deformable convnets v2: More deformable, better",
          "P.Tahghighi et al.": ""
        },
        {
          "8": "",
          "P.Tahghighi et al.": "results (2018)"
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion recognition from body movement",
      "authors": [
        "F Ahmed",
        "A Bari",
        "M Gavrilova"
      ],
      "year": "2019",
      "venue": "IEEE Access PP",
      "doi": "10.1109/ACCESS.2019.2963113"
    },
    {
      "citation_id": "2",
      "title": "Introducing the geneva multimodal emotion portrayal (gemep) corpus. Blueprint for Affective Computing: A Sourcebook",
      "authors": [
        "T Bänziger",
        "K Scherer"
      ],
      "year": "2010",
      "venue": "Introducing the geneva multimodal emotion portrayal (gemep) corpus. Blueprint for Affective Computing: A Sourcebook"
    },
    {
      "citation_id": "3",
      "title": "Facial expression recognition in video with multiple feature fusion",
      "authors": [
        "J Chen",
        "Z Chen",
        "Z Chi",
        "H Fu"
      ],
      "venue": "IEEE Transactions on Affective Computing PP",
      "doi": "10.1109/TAFFC.2016.2593719"
    },
    {
      "citation_id": "4",
      "title": "",
      "authors": [
        "J Dai",
        "H Qi",
        "Y Xiong",
        "Y Li",
        "G Zhang",
        "H Hu",
        "Y Wei"
      ],
      "year": "2017",
      "venue": ""
    },
    {
      "citation_id": "5",
      "title": "Compound facial expressions of emotion: From basic research to clinical applications",
      "authors": [
        "S Du",
        "A Martinez"
      ],
      "year": "2015",
      "venue": "Dialogues in Clinical Neuroscience"
    },
    {
      "citation_id": "6",
      "title": "Toward a minimal representation of affective gestures",
      "authors": [
        "D Glowinski",
        "N Dael",
        "A Camurri",
        "G Volpe",
        "M Mortillaro",
        "K Scherer"
      ],
      "year": "2011",
      "venue": "T. Affective Computing",
      "doi": "10.1109/T-AFFC.2011.7"
    },
    {
      "citation_id": "7",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2015",
      "venue": "Deep residual learning for image recognition"
    },
    {
      "citation_id": "8",
      "title": "Extended deep neural network for facial emotion recognition",
      "authors": [
        "D Jain",
        "P Shamsolmoali",
        "P Sehdev"
      ],
      "year": "2019",
      "venue": "Pattern Recognition Letters",
      "doi": "10.1016/j.patrec.2019.01.008"
    },
    {
      "citation_id": "9",
      "title": "Deep joint spatiotemporal network (djstn) for efficient facial expression recognition",
      "authors": [
        "D Jeong",
        "B Kim",
        "S Dong"
      ],
      "year": "2020",
      "venue": "Sensors",
      "doi": "10.3390/s20071936"
    },
    {
      "citation_id": "10",
      "title": "Deep learning approach for emotion recognition from human body movements with feedforward deep convolution neural networks",
      "authors": [
        "S Rajaram",
        "M Geetha"
      ],
      "year": "2019",
      "venue": "Procedia Computer Science",
      "doi": "10.1016/j.procs.2019.05.038"
    },
    {
      "citation_id": "11",
      "title": "Vision-Based Human Emotion Recognition Using HOG-KLT Feature",
      "authors": [
        "S Rajaram",
        "M Geetha"
      ],
      "year": "2020",
      "venue": "Vision-Based Human Emotion Recognition Using HOG-KLT Feature",
      "doi": "10.1007/978-981-15-3369-3-20"
    },
    {
      "citation_id": "12",
      "title": "Svm-knn based emotion recognition of human in video using hog feature and klt tracking algorithm",
      "authors": [
        "S Rajaram",
        "M Geetha"
      ],
      "year": "2017",
      "venue": "International Journal of Pure and Applied Mathematics"
    },
    {
      "citation_id": "13",
      "title": "Emotion Recognition on Multi View Static Action Videos Using Multi Blocks Maximum Intensity Code (MB-MIC)",
      "authors": [
        "R Santhoshkumar",
        "M Geetha"
      ],
      "year": "2020",
      "venue": "Emotion Recognition on Multi View Static Action Videos Using Multi Blocks Maximum Intensity Code (MB-MIC)",
      "doi": "10.1007/978-3-030-41862-5-116"
    },
    {
      "citation_id": "14",
      "title": "A Survey on Automatic Multimodal Emotion Recognition in the Wild",
      "authors": [
        "G Sharma",
        "A Dhall"
      ],
      "year": "2021",
      "venue": "A Survey on Automatic Multimodal Emotion Recognition in the Wild",
      "doi": "10.1007/978-3-030-51870-7-3"
    },
    {
      "citation_id": "15",
      "title": "Convolutional lstm network: A machine learning approach for precipitation nowcasting",
      "authors": [
        "X Shi",
        "Z Chen",
        "H Wang",
        "D Yeung",
        "W Wong",
        "W Chun Woo"
      ],
      "year": "2015",
      "venue": "Convolutional lstm network: A machine learning approach for precipitation nowcasting"
    },
    {
      "citation_id": "16",
      "title": "Learning spatiotemporal features using 3dcnn and convolutional lstm for gesture recognition",
      "authors": [
        "L Zhang",
        "G Zhu",
        "P Shen",
        "J Song"
      ],
      "year": "2017",
      "venue": "Learning spatiotemporal features using 3dcnn and convolutional lstm for gesture recognition",
      "doi": "10.1109/ICCVW.2017.369"
    },
    {
      "citation_id": "17",
      "title": "Deformable convnets v2: More deformable",
      "authors": [
        "X Zhu",
        "H Hu",
        "S Lin",
        "J Dai"
      ],
      "year": "2018",
      "venue": "Deformable convnets v2: More deformable"
    }
  ]
}