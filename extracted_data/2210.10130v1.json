{
  "paper_id": "2210.10130v1",
  "title": "Peri: Part Aware Emotion Recognition In The Wild",
  "published": "2022-10-18T20:01:40Z",
  "authors": [
    "Akshita Mittel",
    "Shashank Tripathi"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition aims to interpret the emotional states of a person based on various inputs including audio, visual, and textual cues. This paper focuses on emotion recognition using visual features. To leverage the correlation between facial expression and the emotional state of a person, pioneering methods rely primarily on facial features. However, facial features are often unreliable in natural unconstrained scenarios, such as in crowded scenes, as the face lacks pixel resolution and contains artifacts due to occlusion and blur. To address this, methods focusing on in the wild emotion recognition exploit full-body person crops as well as the surrounding scene context. While effective, in a bid to use body pose for emotion recognition, such methods fail to realize the potential that facial expressions, when available, offer. Thus, the aim of this paper is two-fold. First, we demonstrate a method, PERI, to leverage both body pose and facial landmarks. We create part aware spatial (PAS) images by extracting key regions from the input image using a mask generated from both body pose and facial landmarks. This allows us to exploit body pose in addition to facial context whenever available. Second, to reason from the PAS images, we introduce context infusion (Cont-In) blocks. These blocks attend to part-specific information, and pass them onto the intermediate features of an emotion recognition network. Our approach is conceptually simple and can be applied to any existing emotion recognition method. We provide our results on the publicly available in the wild EMOTIC dataset. Compared to existing methods, PERI achieves superior performance and leads to significant improvements in the mAP of emotion categories, while decreasing Valence, Arousal and Dominance errors. Importantly, we observe that our method improves performance in both images with fully visible faces as well as in images with occluded or blurred faces.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "The objective of emotion recognition is to recognise how people feel. Humans function on a daily basis by interpreting social cues from around them. Lecturers can sense confusion in the class, comedians can sense engagement in their audience, and psychiatrists can sense complex emotional states in their patients.\n\nAs machines become an integral part of our lives, it is imperative that they understand social cues in order to assist us better. By making machines more aware of context, body language, and facial expressions, we enable them to play a key in role in numerous situations. This includes monitoring critical patients in hospitals, helping psychologists monitor patients they are consulting, detecting engagement in students, analysing fatigue in truck drivers, to name a few. Thus, emotion recognition and social AI have the potential to drive key technological advancements in the future.\n\nFacial expressions are one of the biggest indicators of how a person feels. Therefore, early work in recognizing emotions focused on detecting and analyzing faces  [2, 12, 33, 37] . Although rapid strides have been made in this direction, such methods assume availability of well aligned, fully visible and high-resolution face crops  [8, 18, 21, 29, 31, 35, 39] . Unfortunately, this assumption does not hold in realistic and unconstrained scenarios such as internet images, crowded scenes, and autonomous driving. In the wild emotion recognition, thus, presents a significant challenge for these methods as face crops tend to be low-resolution, blurred or partially visible due to factors such as subject's distance from the camera, person and camera motion, crowding, person-object occlusion, frame occlusion etc. In this paper, we address in-the-wild emotion recognition by leveraging face, body and scene context in a robust and efficient framework called Part-aware Emotion Recognition In the wild, or PERI.\n\nResearch in psychology and affective computing has shown that body pose offers significant cues on how a person feels  [5, 7, 20] . For example, when people are interested in something, they tilt their head forward. When someone is confident, they tend to square their shoulders. Recent methods recognize the importance of body pose for emotion recognition and tackle issues such as facial occlusion and blurring by processing image crops of the entire body  [1, 19, 22, 24, 38, 41] . Kosti et al.  [22, 24]  expand upon previous work by adding scene context in the mix, noting that the surrounding scene plays a key role in deciphering the emotional state of an individual. An illustrative example of this could be of a person crying at a celebration such as graduation as opposed to a person at a funeral. Both individuals can have identical posture but may feel vastly different set of emotions. Huang et al.  [19]  expanded on this by improving emotion recognition using body pose estimations.\n\nIn a bid to exploit full body crops, body keypoints and scene context, such methods tend to ignore part-specific information such as shoulder position, head tilt, facial expressions, etc. which, when available, serve as powerful indicators of the emotional state. While previous approaches focus on either body pose or facial expression, we hypothesize that a flexible architecture capable of leveraging both body and facial features is needed. Such an architecture should be robust to lack of reliable features on both occluded/blurred body or face, attend to relevant body parts and be extensible enough to include context from the scene. To this end, we present a novel representation, called part-aware spatial (PAS) images that encodes both facial and part specific features and retains pixel-alignment relative to the input image. Given a person crop, we generate a part-aware mask by fitting Gaussian functions to the detected face and body landmarks. Each Gaussian in the part-aware mask represents the spatial context around body and face regions and specifies key regions in the image the network should attend to. We apply the part-aware mask on the input image which gives us the final PAS image (see Fig.  2 ). The PAS images are agnostic to occlusion and blur and take into account both body and face features.\n\nTo reason from PAS images, we propose novel context-infusion (Cont-In) blocks to inject part-aware features at multiple depths in a deep feature backbone network. Since the PAS images are pixel-aligned, each Cont-In block implements explicit attention on part-specific features from the input image. We show that as opposed to early fusion (e.g. channel-wise concatenation) of PAS image with input image I, or late fusion (concatenating the features extracted from PAS images just before the final classification), Cont-In blocks effectively utilize partaware features from the image. Cont-In blocks do not alter the architecture of the base network, thereby allowing Imagenet pretraining on all layers. The Cont-In blocks are designed to be easy to implement, efficient to compute and can be easily integrated with any emotion recognition network with minimal effort.\n\nClosest to our work is the approach of Gunes et al.  [15]  which combines visual channels from face and upper body gestures for emotion recognition. However, unlike PERI, which takes unconstrained in the wild monocular images as input, their approach takes two high-resolution camera streams, one focusing only on the face and other focusing only on the upper body gestures from the waist up. All of the training data in  [15]  is recorded in an indoor setting with a uniform background, single subject, consistent lighting, front-facing camera and fully visible face and body; a setting considerably simpler than our goal of emotion recognition in real-world scenarios. Further, our architecture and training scheme is fundamentally different and efficiently captures part-aware features from monocular images.\n\nIn summary, we make the following contributions:\n\n1. Our approach, PERI, advances in the wild emotion recognition by introducing a novel representation (called PAS images) which efficiently combines body pose and facial landmarks such that they can supplement one another. 2. We propose context infusion (Cont-In) blocks which modulate intermediate features of a base emotion recognition network, helping in reasoning from both body poses and facial landmarks. Notably, Cont-In blocks are compatible with any exiting emotion recognition network with minimal effort. 3. Our approach results in significant improvements compared to existing approaches in the publicly-available in the wild EMOTIC dataset  [23] . We show that PERI adds robustness under occlusion, blur and low-resolution input crops.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Emotion recognition is a field of research with the objective of interpreting a person's emotions using various cues such as audio, visual, and textual inputs.\n\nPreliminary methods focused on recognising six basic discrete emotions defined by the psychologists Ekman and Friesen  [9] . These include anger, surprise, disgust, enjoyment, fear, and sadness. As research progressed, datasets, such as the EMOTIC dataset  [22, 23, 24] , have expanded on these to provide a wider label set. A second class of emotion recognition methods focus not on the discrete classes but on a continuous set of labels described by Mehrabian  [30]  including Valence (V), Arousal (A), and Dominance (D). We evaluate the performance of our model using both the 26 discrete classes in the EMOTIC dataset  [23] , as well as valence, arousal, and dominance errors. Our method works on visual cues, more specifically on images and body crops.\n\nEmotion recognition using facial features. Most existing methods in Computer Vision for emotion recognition focus on facial expression analysis  [2, 12, 33, 37] . Initial work in this field was based on using the Facial Action Coding System (FACS)  [4, 10, 11, 26, 34]  to recognise the basic set of emotions. FACS refers to a set of facial muscle movements that correspond to a displayed emotion, for instance raising the inner eyebrow can be considered as a unit of FACS. These methods first extract facial landmarks from a face, which are then used to create facial action units, a combination of which are used to recognise the emotion. Another class of methods use CNNs to recognize the emotions  [2, 19, 22, 23, 32, 42] . For instance, Emotionnet  [2]  uses face detector to obtain face crops which are then passed into a CNN to get the emotion category. Similar to these methods, we use facial landmarks in our work. However, uniquely, the landmarks are used to create the PAS contextual images, which in turn modulate the main network through a series of convolutions layers in the Cont-In blocks.\n\nEmotion recognition using body poses. Unlike facial emotion recognition, the work on emotion recognition using body poses is relatively new. Research in psychology  [3, 13, 14]  suggests that cues from body pose, including features such as hip, shoulder, elbow, pelvis, neck, and trunk can provide significant insight into the emotional state of a person. Based on this hypothesis, Crenn et al.  [6]  sought to classify body expressions by obtaining low-level features from 3D skeleton sequences. They separate the features into three categories: geometric features, motion features, and fourier features. Based on these low-level features, they calculate meta features (mean and variance), which are sent to the classifier to obtain the final expression labels. Huang et al.  [40]  use a body pose extractor built on Actional-Structural GCN blocks as an input stream to their model. The other streams in their model extract information from images and body crops based on the architecture of Kosti et al.  [22, 24] . The output of all the streams are concatenated using a fusion layer before the final classification. Gunes et al.  [15]  also uses body gestures. Similar to PERI, they use facial features by combining visual channels from face and upper body gestures. However, their approach takes two high-resolution camera streams, one focusing only on the face and other focusing only on the upper body gestures, making them unsuitable for unconstrained settings. For our method, we use two forms of body posture information, body crops and body pose detections. Body crops taken from the original input image are passed into one stream of our architecture. The intermediate features of the body stream are then modulated at regular intervals using Cont-In blocks, which derive information from the PAS image based on body pose and facial landmarks.\n\nAdding visual context from the entire image. The most successful methods for emotion recognition in the wild use context from the entire image as opposed to just the body or facial crop. Kosti et al.  [22, 24]  were among the first to explore emotion recognition in the wild using the entire image as context. They introduced the EMOTIC dataset  [23]  on which they demonstrated the efficacy of a two-stream architecture where one of the streams is supplied with the entire image while the other is supplied with body crops. Gupta et al.  [16]  also utilise context from the entire image using an image feature extraction stream. Additionally, the facial crops from the original image are passed through three modules; a facial feature extraction stream, an attention block and finally a fusion network. The attention block utilizes the features extracted from the full image to additionally modulate the features of the facial feature extraction stream. However, unlike Kosti et al. they focus on recognising just the valence of the entire scene. Zhang et al.  [42]  also use context from an image. Their approach uses a Region Proposal Network (RPN) to detect nodes which then form an affective graph. This graph is fed into a Graph Convolutional Network (GCN) similar to Mittal et al.  [32] . Similar to Kosti et al. the second CNN stream in their network extracts the body features. Lee et al.  [25]  present CAERNet, which consists of two subnetworks. CAERNet is two-stream network where one stream works with facial crops and the other in which both facial expression and context (background) are extracted. They use an adaptive fusion network in order to fuse the two streams. Mittal et al.  [32]  take context a step further. Similar to our approach, they use both body crops and facial landmarks. However, akin to Huang et al.  [40]  they pass body crops and facial landmarks as a separate stream. Their architecture consists of three streams. In addition to the body pose and facial landmark stream, the second stream extracts information from the entire image where the body crop of the person has been masked out. The third stream adds modality in one of two ways. They first encode spatio-temporal relationships using a GCN network similar to  [42] , these are then passed through the third stream. The other method uses a CNN which parses depth images in the third stream. Similar to these methods, PERI maintains two streams, where one of the stream extracts meaningful context from the entire image while the other focuses on the individual person.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Method",
      "text": "The following section describes our framework to effectively recognize emotions from images in the wild. Facial expressions, where available, provide key insight to the emotional state of a person. We find a way to represent body pose and facial landmarks such that we can utilise both set of features subject to their availability in the input image. Concretely, we use MediaPipe's Holistic model  [28]",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Mediapipe Holistic Model",
      "text": "In order to obtain the body poses and facial landmarks, we use the MediaPipe Holistic pipeline  [28] . It is a multi-stage pipeline which includes separate models for body pose and facial landmark detection. The body pose estimation model is trained on 224 × 224 input resolution. However, detecting face and fine-grained facial landmarks requires high resolution inputs. Therefore, the MediaPipe Holistic pipeline first estimates the human pose and then finds the region of interest for the face keypoints detected in the pose output. The region of interest is upsampled and the facial crop is extracted from the original resolution input image and is sent to a separate model for fine-grained facial landmark detection.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "The Emotic Model",
      "text": "The baseline of our paper is the the two-stream CNN architecture from Kosti et. al  [22, 24] . The paper defines the task of emotion recognition in context, which considers both body pose and scene context for emotion detection. The architecture takes as input the body crop image, which is sent to the body feature extraction stream, and the entire image, which is sent to the image feature extraction stream. The outputs from the two streams are concatenated and combined through linear classification layers. The model outputs classification labels from 26 discrete emotion categories and 3 continuous emotion dimensions, Valence, Arousal and Dominance  [30] . The 2 stream architecture is visualized in our pipeline in Fig.  1 . In order to demonstrate our idea, we stick with the basic Resnet-18  [17]  backbone for both the streams.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Part Aware Spatial Image",
      "text": "One contribution of our framework is how we combine body pose information along with facial landmarks such that we can leverage both sets of features and allow them to complement each other subject to their availability. In order to do so we have three main stages to our pipeline. First, we use the MediaPipe Holistic model to extract the keypoints as described in Sec. 3.1. Here we get two sets of keypoint coordinates for each body crop image I. The first set of N coordinates describe the body landmarks b i where i ∈ (0, N ). The second set of M coordinates describe the location of the facial landmarks f j where j ∈ (0, M ). For simplicity, we combine all detected landmarks and denote then as b k where k ∈ (0, M + N ). We take an all black mask B ∈ R 1×H×W the same size as the body crop, and fit a Gaussian kernel to every landmark in the original image as\n\nThe part-aware mask B ′ ∈ R (1×H×W ) is created by binarizing b ′ k using a constant threshold ρ, such that\n\nwhere x is the coordinates of all pixels in B. The distance threshold ρ is determined empirically. Finally, to obtain the part aware spatial (PAS) image P ∈ R 3×H×W , the part-aware mask is applied to the input body crop I using channel-wise hadamard product,\n\nThis process can be visualized in Fig.  2  (left).",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Context Infusion Blocks",
      "text": "To extract information from PAS images, we explore early fusion, which simply concatenates PAS with the body crop image I in the body feature extraction stream of our network. We also explore late fusion, concatenating feature maps derived from PAS images before the fusion network. However, both of these approaches failed to improve performance. Motivated by the above, we present our second contribution, the Context Infusion Block (Cont-In) which is an architectural block that utilizes the PAS contextual image to condition the base network. We design Cont-In blocks such that they can be easily introduced in any existing emotion recognition network. Fig.  2  shows the architecture of a Cont-In block in detail. In PERI, the body feature extraction stream uses the Cont-In blocks to attend to part-aware context in the input image. Our intuition is that the pixel-aligned PAS images and the Cont-In block enables the network to determine the body part regions most salient for detecting emotion. Cont-In learns to modulate the network features by fusing the features of the intermediate layer with feature maps derived from PAS. Let X ∈ R H×W ×C be the intermediate features from the n -1 th block of the base network. The PAS image P is first passed through a series of convolutions and activation operations, denoted by g(.), to get an intermediate representation G = g(P) where G ∈ R H×W ×C . These feature maps are then concatenated with X to get a fused representation F = G ⊕ X. F is then passed through a second series of convolutions, activations, and finally batchnorm to get the feature map X ′ ∈ R H×W ×C ′ which is then passed through to the n th block of the base network (see Fig.  1 ).",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Experiment Setup",
      "text": "Dataset and metrics. For the purpose of our experiments, we use the twostream architecture of  [22, 24]  as our base implementation. We use their EMOTIC database  [23] , which is composed of images from MS-COCO  [27] , ADE20K  [43]  along with images downloaded from the web. The database offers two emotion representation labels; a set of 26 discrete emotional categories (Cat), and a set of three emotional dimensions, Valence, Arousal and Dominance from the VAD Emotional State Model  [30] . Valence (V), is a measure of how positive or pleasant an emotion is (negative to positive); Arousal (A), is a measure of the agitation level of the person (non-active, calm, agitated, to ready to act); and Dominance (D) is a measure of the control level of the situation by the person (submissive, non-control, dominant, to in-control). The continuous dimensions (Cont) annotations of VAD are in a 1-10 scale.\n\nLoss function. A dynamic weighted MSE loss L cat is used on the Category classification output layer (Cat) of the model.\n\nwhere i corresponds to 26 discrete categories shown in Tab. 1. ŷcat i and y cat i are the prediction and ground-truth for the i th category. The dynamic weight w i are computed per batch and is based on the number of occurrences of each class in the batch. Since the occurrence of a particular class can be 0,  [22, 24]  defined an additional hyper-parameter c. The constant c is added to the dynamic weight w i along with p i , which is the probability of the i th category. The final weight is defined as w i = 1 ln(pi+c) . For the continuous (Cont) output layer, an L1 loss L cont is employed.\n\nHere i represents one of valence, arousal, and dominance (C). ŷcont i and y cont i are the prediction and ground-truth for the i th metric (VAD).\n\nBaselines. We compare PERI to the SOTA baselines including Emotic Kosti et al.  [22, 24] , Huang et al.  [19] , Zhang et al.  [42] , Lei et al.  [25] , and Mittal et al.  [32] . We reproduce the three stream architecture in  [19]  based on their proposed method. For a fair comparison, we compare PERI's image-based model results with EmotiCon's  [32]  image-based GCN implementation.\n\nImplementation details. We use the two-stream architecture from Kosti et al.  [22, 24] . Here both the image and body feature extraction streams are Resnet-18  [17]  networks which are pre-trained on ImageNet  [36] . All PAS images are re-sized to 128X128 similar to the input of the body feature extraction stream. The PAS image is created by plotting 501 landmarks N + M on the base mask and passing it through a Gaussian filter of size σ = 3. We consider the same train, validation, and test splits provided by the EMOTIC  [23]  open repository.\n\nTable  1 : The average precision (AP) results on state-of-the-art methods and PERI. We see a consistent increase across a majority of the discrete class APs as well as the mAP using PERI.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Category",
      "text": "Kosti  [22, 24]  Huang  [19]  Zhang  [42]  Lee  [25]",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Quantitative Results",
      "text": "Tab. 1 and Tab. 2 show quantitative comparisons between PERI and state-ofthe-art approaches. Tab. 1 compares the average precision (AP) for each discrete Table  2 : The VAD and mean errors for continuous labels. The models include the state-of-the-art methods and PERI. We see a consistent decrease across each VAD L1 error along with the mean L1 error using PERI.\n\nValence↓ Arousal↓ Dominance↓ VAD Error↓ Kosti et al.  [22, 24]  0.71 0.91 0.89 0.84 Huang et al.  [19]  0.72 0.90 0.88 0.83 Zhang et al.  [42]  0.70 1.00 1.00 0.90 PERI 0.70 0.85 0.87 0.80 emotion category in the EMOTIC dataset  [23] . Tab. 2 compares the valence, dominance and arousal L1 errors. Our model consistently outperforms existing approaches in both metrics. We achieve a significant 6.3% increase in mean AP (mAP) over our base network  [22, 24]  and a 1.8% improvement in mAP over the closest competing method  [32] . Compared to methods that report VAD errors, PERI achieves lower mean and individual L1 errors and a 2.6% improvement in VAD error over our baseline  [22, 24] . Thus, our results effectively shows that while only using pose or facial landmarks might lead to noisy gradients, especially in images with unreliable/occluded body or face, adding cues from both facial and body pose features where available lead to better emotional context. We further note that our proposed Cont-In Blocks are effective in reasoning about emotion context when comparing PERI with recent methods that use both body pose and facial landmarks  [32] .",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Qualitative Results",
      "text": "In order to understand the results further, we look at several visual examples, a subset of which are shown in Fig.  3 . We choose Kosti et al.  [22, 24]  and Huang et al.  [19]  as our baselines as they are the closest SOTA methods. We derive several key insights pertaining to our results. In comparison to Kosti et al.  [22, 24]  [19] and Huang et al., PERI fares better on examples where the face is clearly visible. This is expected as PERI specifically brings greater attention to facial features. Interestingly, our model also performs better for images where either the face or the body is partially visible (occluded/blurred). This supports our hypothesis that partial body poses as well as partial facial landmarks can supplement one another using our PAS image representation.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Ablation Study",
      "text": "As shown in Tab. 3, we conduct a series of ablation experiments to create an optimal part-aware representation (PAS) and use the information in our base model effectively. For all experiments, we treat the implementation from Kosti et al.  [22, 24]  as our base network and build upon it.\n\nPAS images. To get the best PAS representation, we vary the standard deviation (σ) of the Gaussian kernel applied to our PAS image. We show that Experimenting with Cont-In blocks. To show the effectiveness of Cont-In blocks, we compare its performance with early and late fusion in Tab. 3. For early fusion, we concatenate the PAS image as an additional channel to the body-crop image in the body feature extraction stream. For late fusion, we concatenate the fused output of the body and image feature extraction streams with the downsampled PAS image. As opposed to PERI, we see a decline in performance for both mAP and VAD error when considering early and late fusion. From this we conclude that context infusion at intermediate blocks is important for accurate emotion recognition.\n\nAdditionally, we considered concatenating the PAS images directly to the intermediate features instead of using a Cont-In block. However, feature concatenation in the intermediate layers changes the backbone ResNet architecture, severely limiting gains from ImageNet  [36]  pretraining. This is apparent in the decrease in performance from early fusion, which may be explained, in part, by the inability to load ImageNet weights in the input layer of the backbone network. In contrast, Cont-In block are fully compatible with any emotion recognition network and do not alter the network backbone.\n\nIn the final experiment, we added Cont-In blocks to both the image feature extraction stream and the body feature extraction stream. Here we discovered that if we regulate the intermediate features of both streams as opposed to just the body stream the performance declines. A possible reason could be that contextual information from a single person does generalise well to the entire image with multiple people.\n\nPERI. From our ablation experiments, we found that PERI works best overall. It has the highest mAP among the ablation experiments as well as a lowest mean L1 error for VAD. While there are other hyper-parameters that have better L1 errors for Valence, Arousal, and Dominance independently, (different Gaussian standard deviations (σ k )), these hyper-parameters tend to perform worse overall compared to PERI.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Conclusion",
      "text": "Existing methods for in the wild emotion recognition primarily focus on either face or body, resulting in failures under challenging scenarios such as low resolution, occlusion, blur etc. To address these issues, we introduce PERI, a method that effectively represents body poses and facial landmarks together in a pixel-aligned part aware contextual image representation, PAS. We argue that using both features results in complementary information which is effective in challenging scenarios. Consequently, we show that PAS allows better emotion recognition not just in examples with fully visible face and body features, but also when one of the two features sets are missing, unreliable or partially available.\n\nTo seamlessly integrate the PAS images with a baseline emotion recognition network, we introduce a novel method for modulating intermediate features with the part-aware spatial (PAS) context by using context infusion (Cont-In) blocks. We demonstrate that using Cont-In blocks works better than a simple early or late fusion. PERI significantly outperforms the baseline emotion recognition network of Kosti et al.  [23, 24] . PERI also improves upon existing state-of-the-art methods on both the mAP and VAD error metrics.\n\nWhile our method is robust towards multiple in-the-wild challenging scenarios, we do not model multiple-human scenes and dynamic environments. In the future, we wish to further extend Cont-In blocks to utilise the PAS context better. Instead of modeling explicit attention using PAS images, it might be interesting to learn part-attention implicitly using self and cross-attention blocks, but we leave this for future work. Additionally, we also seek to explore multi-modal input beyond images, such as depth, text and audio.    [22, 24] ), baseline 2 (Huang et al.  [19] ) and the ground-truth value respectively. For the predicted categories we highlight the category in green if they are present in the ground-truth and orange if they aren't.\n\nThe ground-truth categories associated with each example are written as a list below the predictions.",
      "page_start": 13,
      "page_end": 14
    }
  ],
  "figures": [
    {
      "caption": "Figure 2: ). The PAS images are agnostic to occlusion and blur and take",
      "page": 3
    },
    {
      "caption": "Figure 1: The overall architecture that consists of a two-stream Resnet-18 network",
      "page": 6
    },
    {
      "caption": "Figure 1: shows the overall framework that",
      "page": 6
    },
    {
      "caption": "Figure 2: (Left) An input image along with the mask (B′) created by applying a",
      "page": 7
    },
    {
      "caption": "Figure 1: In order to demonstrate our idea, we stick with the basic",
      "page": 7
    },
    {
      "caption": "Figure 2: shows the architecture of a",
      "page": 8
    },
    {
      "caption": "Figure 3: We choose Kosti et al. [22, 24] and",
      "page": 11
    },
    {
      "caption": "Figure 3: The figure shows visual results on selected examples. Here each example is",
      "page": 14
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Conv layer": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Anticipation Happiness Engagement\nPERI\n0.171\n0.251\n0.556\nAnticipation Happiness Engagement\nBaseline 1\n0.176\n0.253\n0.539\nAnticipation Happiness Engagement\nBaseline 2\n0.172\n0.258\n0.535\nEngagement, Esteem, Excitement, Happiness,\nSympathy\nAnnoyance Disapproval Anger\nPERI\n0.347\n0.367\n0.422\nAnticipation Happiness\nEngagement\nBaseline 1\n0.169\n0.279\n0.532\nAnticipation Happiness\nEngagement\nBaseline 2\n0.142\n0.283\n0.413\nAnger, Annoyance, Anticipation, Disapproval,\nDisquietment, Doubt/Confusion, Engagement, Pain,\nSurprise": "Happiness\nAnger\nEngagement\nPERI\n0.234\n0.269\n0.334\nAnticipation Happiness\nEngagement\nBaseline 1\n0.159\n0.294\n0.527\nAnticipation Happiness\nEngagement\nBaseline 2\n0.171\n0.258\n0.512\nAnger, Annoyance, Aversion, Disapproval,\nDisquietment, Doubt/Confusion, Engagement",
          "Anticipation Happiness Engagement\nPERI\n0.210\n0.216\n0.611\nAnticipation Happiness Engagement\nBaseline 1\n0.173\n0.262\n0.537\nAnticipation Happiness Engagement\nBaseline 2\n0.180\n0.244\n0.550\nEngagement, Esteem, Excitement, Happiness,\nPleasure\nAnnoyance Engagement Anger\nPERI\n0.314\n0.321\n0.338\nAnticipation Happiness\nEngagement\nBaseline 1\n0.161\n0.289\n0.529\nAnticipation Happiness\nEngagement\nBaseline 2\n0.177\n0.248\n0.537\nAnger, Annoyance, Disapproval, Doubt/Confusion,\nFatigue, Pain": "Pleasure\nHappiness\nEngagement\nPERI\n0.162\n0.438\n0.467\nAnticipation Happiness\nEngagement\nBaseline 1\n0.159\n0.296\n0.527\nAnticipation Happiness\nEngagement\nBaseline 2\n0.169\n0.251\n0.541\nAffection, Engagement, Happiness, Pleasure,\nSympathy",
          "Excitement\nHappiness Engagement\nPERI\n0.264\n0.395\n0.489\nAnticipation Happiness Engagement\nBaseline 1\n0.176\n0.253\n0.539\nAnticipation Happiness Engagement\nBaseline 2\n0.155\n0.287\n0.517\nEngagement, Excitement, Happiness, Peace,\nPleasure, Surprise\nDisapproval Annoyance Anger\nPERI\n0.507\n0.510\n0.603\nAnticipation\nHappiness\nEngagement\nBaseline 1\n0.146\n0.326\n0.518\nAnnoyance\nHappiness\nEngagement\nBaseline 2\n0.166\n0.285\n0.355\nAnger, Annoyance, Disapproval, Excitement, Pain,\nSensitivity": "Confidence Anticipation Engagement\nPERI\n0.365\n0.455\n0.612\nAnticipation Excitement\nEngagement\nBaseline 1\n0.355\n0.299\n0.577\nAnticipation Excitement\nEngagement\nBaseline 2\n0.333\n0.278\n0.567\nAnticipation, Confidence, Engagement, Excitement,\nHappiness, Yearning",
          "Excitement\nHappiness Engagement\nPERI\n0.247\n0.290\n0.564\nAnticipation Happiness Engagement\nBaseline 1\n0.174\n0.257\n0.538\nAnticipation Happiness Engagement\nBaseline 2\n0.200\n0.235\n0.545\nEngagement, Excitement, Happiness, Pleasure\nExcitement\nAnticipation\nEngagement\nPERI\n0.650\n0.656\n0.632\nExcitement\nAnticipation\nEngagement\nBaseline 1\n0.318\n0.377\n0.582\nExcitement\nAnticipation\nEngagement\nBaseline 2\n0.370\n0.413\n0.563\nAnticipation, Confidence, Engagement, Excitement,\nPleasure": "Anticipation Happiness Engagement\nPERI\n0.151\n0.234\n0.550\nAnticipation Happiness Engagement\nBaseline 1\n0.176\n0.253\n0.539\nAnticipation Happiness Engagement\nBaseline 2\n0.183\n0.244\n0.542\nAffection, Annoyance, Anticipation, Disapproval,\nDisconnection, Disquietment, Engagement, Happiness,\nPain, Peace, Sadness, Surprise, Sympathy"
        }
      ],
      "page": 14
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion recognition from body movement",
      "authors": [
        "F Ahmed",
        "A Bari",
        "M Gavrilova"
      ],
      "year": "2020",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2019.2963113"
    },
    {
      "citation_id": "2",
      "title": "Emotionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild",
      "authors": [
        "C Benitez-Quiroz",
        "R Srinivasan",
        "A Martinez"
      ],
      "year": "2016",
      "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR.2016.600"
    },
    {
      "citation_id": "3",
      "title": "What do we express without knowing?: Emotion in gesture",
      "authors": [
        "G Castillo",
        "M Neff"
      ],
      "year": "2019",
      "venue": "AAMAS"
    },
    {
      "citation_id": "4",
      "title": "Selective transfer machine for personalized facial expression analysis",
      "authors": [
        "W Chu",
        "F De La Torre",
        "J Cohn"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "doi": "10.1109/TPAMI.2016.2547397"
    },
    {
      "citation_id": "5",
      "title": "Attributing emotion to static body postures: Recognition accuracy, confusions, and viewpoint dependence",
      "authors": [
        "M Coulson"
      ],
      "year": "2004",
      "venue": "Journal of nonverbal behavior"
    },
    {
      "citation_id": "6",
      "title": "Body expression recognition from animated 3d skeleton",
      "authors": [
        "A Crenn",
        "R Khan",
        "A Meyer",
        "S Bouakaz"
      ],
      "year": "2016",
      "venue": "2016 International Conference on 3D Imaging (IC3D)",
      "doi": "10.1109/IC3D.2016.7823448"
    },
    {
      "citation_id": "7",
      "title": "The bodily expressive action stimulus test (beast). construction and validation of a stimulus basis for measuring perception of whole body expression of emotions",
      "authors": [
        "B De Gelder",
        "J Van Den Stock"
      ],
      "year": "2011",
      "venue": "Frontiers in psychology"
    },
    {
      "citation_id": "8",
      "title": "Facial emotion recognition in real time",
      "authors": [
        "D Duncan",
        "G Shine",
        "C English"
      ],
      "year": "2016",
      "venue": "Computer Science"
    },
    {
      "citation_id": "9",
      "title": "Constants across cultures in the face and emotion",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1971",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "10",
      "title": "Facial action coding system: a technique for the measurement of facial movement",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1978",
      "venue": "Facial action coding system: a technique for the measurement of facial movement"
    },
    {
      "citation_id": "11",
      "title": "Discriminative shared gaussian processes for multiview and view-invariant facial expression recognition",
      "authors": [
        "S Eleftheriadis",
        "O Rudovic",
        "M Pantic"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Image Processing",
      "doi": "10.1109/TIP.2014.2375634"
    },
    {
      "citation_id": "12",
      "title": "Joint facial action unit detection and feature fusion: A multi-conditional learning approach",
      "authors": [
        "S Eleftheriadis",
        "O Rudovic",
        "M Pantic"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Image Processing",
      "doi": "10.1109/TIP.2016.2615288"
    },
    {
      "citation_id": "13",
      "title": "Towards the neurobiology of emotional body language",
      "authors": [
        "B De Gelder"
      ],
      "year": "2006",
      "venue": "Nature Reviews Neuroscience"
    },
    {
      "citation_id": "14",
      "title": "Effort-shape and kinematic assessment of bodily expression of emotion during gait",
      "authors": [
        "M Gross",
        "E Crane",
        "B Fredrickson"
      ],
      "year": "2012",
      "venue": "Human movement science"
    },
    {
      "citation_id": "15",
      "title": "Bi-modal emotion recognition from expressive face and body gestures",
      "authors": [
        "H Gunes",
        "M Piccardi"
      ],
      "year": "2007",
      "venue": "J. Netw. Comput. Appl"
    },
    {
      "citation_id": "16",
      "title": "An attention model for group-level emotion recognition",
      "authors": [
        "A Gupta",
        "D Agrawal",
        "H Chauhan",
        "J Dolz",
        "M Pedersoli"
      ],
      "year": "2018",
      "venue": "Proceedings of the 20th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "17",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2015",
      "venue": "Deep residual learning for image recognition"
    },
    {
      "citation_id": "18",
      "title": "Video facial emotion recognition based on local enhanced motion history image and cnn-ctslstm networks",
      "authors": [
        "M Hu",
        "H Wang",
        "X Wang",
        "J Yang",
        "R Wang"
      ],
      "year": "2019",
      "venue": "Journal of Visual Communication and Image Representation"
    },
    {
      "citation_id": "19",
      "title": "Emotion recognition based on body and context fusion in the wild",
      "authors": [
        "Y Huang",
        "H Wen",
        "L Qing",
        "R Jin",
        "L Xiao"
      ],
      "year": "2021",
      "venue": "2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)",
      "doi": "10.1109/ICCVW54120.2021.00403"
    },
    {
      "citation_id": "20",
      "title": "Recognizing affective dimensions from body posture",
      "authors": [
        "A Kleinsmith",
        "N Bianchi-Berthouze"
      ],
      "year": "2007",
      "venue": "International conference on affective computing and intelligent interaction"
    },
    {
      "citation_id": "21",
      "title": "A brief review of facial emotion recognition based on visual information",
      "authors": [
        "B Ko"
      ],
      "year": "2018",
      "venue": "sensors"
    },
    {
      "citation_id": "22",
      "title": "Context based emotion recognition using emotic dataset",
      "authors": [
        "R Kosti",
        "J Alvarez",
        "A Recasens",
        "A Lapedriza"
      ],
      "year": "2020",
      "venue": "Context based emotion recognition using emotic dataset",
      "arxiv": "arXiv:2003.13401"
    },
    {
      "citation_id": "23",
      "title": "Emotic: Emotions in context dataset",
      "authors": [
        "R Kosti",
        "J Álvarez",
        "A Recasens",
        "À Lapedriza"
      ],
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"
    },
    {
      "citation_id": "24",
      "title": "Emotion recognition in context",
      "authors": [
        "R Kosti",
        "J Álvarez",
        "A Recasens",
        "À Lapedriza"
      ],
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "25",
      "title": "Context-aware emotion recognition networks",
      "authors": [
        "J Lee",
        "S Kim",
        "S Kim",
        "J Park",
        "K Sohn"
      ],
      "year": "2019",
      "venue": "IEEE/CVF International Conference on Computer Vision (ICCV)",
      "doi": "10.1109/ICCV.2019.01024"
    },
    {
      "citation_id": "26",
      "title": "Facial-component-based bag of words and phog descriptor for facial expression recognition",
      "authors": [
        "Z Li",
        "J Imai",
        "M Kaneko"
      ],
      "year": "2009",
      "venue": "2009 IEEE International Conference on Systems, Man and Cybernetics",
      "doi": "10.1109/ICSMC.2009.5346254"
    },
    {
      "citation_id": "27",
      "title": "Microsoft COCO: common objects in context",
      "authors": [
        "T Lin",
        "M Maire",
        "S Belongie",
        "L Bourdev",
        "R Girshick",
        "J Hays",
        "P Perona",
        "D Ramanan",
        "P Dollár",
        "C Zitnick"
      ],
      "year": "2014",
      "venue": "Microsoft COCO: common objects in context"
    },
    {
      "citation_id": "28",
      "title": "Mediapipe: A framework for building perception pipelines",
      "authors": [
        "C Lugaresi",
        "J Tang",
        "H Nash",
        "C Mcclanahan",
        "E Uboweja",
        "M Hays",
        "F Zhang",
        "C Chang",
        "M Yong",
        "J Lee",
        "W Chang",
        "W Hua",
        "M Georg",
        "M Grundmann"
      ],
      "year": "2019",
      "venue": "Mediapipe: A framework for building perception pipelines"
    },
    {
      "citation_id": "29",
      "title": "Facial emotion recognition using convolutional neural networks (ferc)",
      "authors": [
        "N Mehendale"
      ],
      "year": "2020",
      "venue": "SN Applied Sciences"
    },
    {
      "citation_id": "30",
      "title": "Framework for a comprehensive description and measurement of emotional states. Genetic, social, and general psychology monographs 121 3",
      "authors": [
        "A Mehrabian"
      ],
      "year": "1995",
      "venue": "Framework for a comprehensive description and measurement of emotional states. Genetic, social, and general psychology monographs 121 3"
    },
    {
      "citation_id": "31",
      "title": "Facial emotion recognition using deep learning: review and insights",
      "authors": [
        "W Mellouk",
        "W Handouzi"
      ],
      "year": "2020",
      "venue": "Procedia Computer Science"
    },
    {
      "citation_id": "32",
      "title": "Emoticon: Context-aware multimodal emotion recognition using frege's principle",
      "authors": [
        "T Mittal",
        "P Guhan",
        "U Bhattacharya",
        "R Chandra",
        "A Bera",
        "D Manocha"
      ],
      "year": "2020",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "33",
      "title": "Facial expressions as conditioned stimuli for electrodermal responses: a case of\" preparedness",
      "authors": [
        "A Öhman",
        "U Dimberg"
      ],
      "year": "1978",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "34",
      "title": "Expert system for automatic analysis of facial expression",
      "authors": [
        "M Pantic",
        "L Rothkrantz"
      ],
      "year": "2000",
      "venue": "Image and Vision Computing",
      "doi": "10.1016/S0262-8856(00)00034-2"
    },
    {
      "citation_id": "35",
      "title": "Facial emotion recognition using deep convolutional neural network",
      "authors": [
        "E Pranav",
        "S Kamal",
        "C Chandran",
        "M Supriya"
      ],
      "year": "2020",
      "venue": "2020 6th International conference on advanced computing and communication Systems (ICACCS)"
    },
    {
      "citation_id": "36",
      "title": "ImageNet Large Scale Visual Recognition Challenge",
      "authors": [
        "O Russakovsky",
        "J Deng",
        "H Su",
        "J Krause",
        "S Satheesh",
        "S Ma",
        "Z Huang",
        "A Karpathy",
        "A Khosla",
        "M Bernstein",
        "A Berg",
        "L Fei-Fei"
      ],
      "year": "2015",
      "venue": "International Journal of Computer Vision (IJCV)",
      "doi": "10.1007/s11263-015-0816-y"
    },
    {
      "citation_id": "37",
      "title": "Multidimensional scaling of emotional facial expressions: similarity from preschoolers to adults",
      "authors": [
        "J Russell",
        "M Bullock"
      ],
      "year": "1985",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "38",
      "title": "Emotion recognition based on multi-view body gestures",
      "authors": [
        "Z Shen",
        "J Cheng",
        "X Hu",
        "Q Dong"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Image Processing",
      "doi": "10.1109/ICIP.2019.8803460"
    },
    {
      "citation_id": "39",
      "title": "Facial emotion recognition on a dataset using convolutional neural network",
      "authors": [
        "V Tümen",
        "Ö Söylemez",
        "B Ergen"
      ],
      "year": "2017",
      "venue": "2017 International Artificial Intelligence and Data Processing Symposium (IDAP)"
    },
    {
      "citation_id": "40",
      "title": "A generalized zero-shot framework for emotion recognition from body gestures",
      "authors": [
        "J Wu",
        "Y Zhang",
        "X Zhao",
        "W Gao"
      ],
      "year": "2020",
      "venue": "A generalized zero-shot framework for emotion recognition from body gestures",
      "doi": "10.48550/ARXIV.2010.06362"
    },
    {
      "citation_id": "41",
      "title": "Automatic emotion recognition based on body movement analysis: a survey",
      "authors": [
        "H Zacharatos",
        "C Gatzoulis",
        "Y Chrysanthou"
      ],
      "year": "2014",
      "venue": "IEEE computer graphics and applications"
    },
    {
      "citation_id": "42",
      "title": "Context-aware affective graph reasoning for emotion recognition",
      "authors": [
        "M Zhang",
        "Y Liang",
        "H Ma"
      ],
      "year": "2009",
      "venue": "IEEE International Conference on Multimedia and Expo (ICME"
    },
    {
      "citation_id": "43",
      "title": "Scene parsing through ade20k dataset",
      "authors": [
        "B Zhou",
        "H Zhao",
        "X Puig",
        "S Fidler",
        "A Barriuso",
        "A Torralba"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    }
  ]
}