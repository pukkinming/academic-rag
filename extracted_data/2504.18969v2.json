{
  "paper_id": "2504.18969v2",
  "title": "Advancing Face-To-Face Emotion Communication: A Multimodal Dataset (Affec)",
  "published": "2025-04-26T16:33:31Z",
  "authors": [
    "Meisam J. Sekiavandi",
    "Laurits Dixen",
    "Jostein Fimland",
    "Sree Keerthi Desu",
    "Antonia-Bianca Zserai",
    "Ye Sul Lee",
    "Maria Barrett",
    "Paolo Burelli"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition has the potential to play a pivotal role in enhancing human-computer interaction by enabling systems to accurately interpret and respond to human affect. Yet, capturing emotions in face-to-face contexts remains challenging due to subtle nonverbal cues, variations in personal traits, and the real-time dynamics of genuine interactions. Existing emotion recognition datasets often rely on limited modalities or controlled conditions, thereby missing the richness and variability found in realworld scenarios. In this work, we introduce Advancing Face-to-Face Emotion Communication (AFFEC), a multimodal dataset designed to address these gaps. AFFEC encompasses 84 simulated emotional dialogues across six distinct emotions, recorded from 73 participants over 5,000+ trials and annotated with more than 20,000 labels. It integrates electroencephalography (EEG), eye-tracking, galvanic skin response (GSR), facial videos, and Big Five personality assessments. Crucially, AFFEC explicitly distinguishes between felt emotions (the participant's internal affect) and perceived emotions (the observer's interpretation of the stimulus). Baseline analyses-spanning unimodal features and straightforward multimodal fusion-demonstrate that even minimal processing yields classification performance significantly above chance, especially for arousal. Incorporating personality traits further improves predictions of felt emotions, highlighting the importance of individual differences. By bridging controlled experimentation with more realistic face-to-face stimuli, AFFEC offers a unique resource for researchers aiming to develop context-sensitive, adaptive, and personalized emotion recognition models.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Human communication is inherently emotional, relying on both verbal and non-verbal cues to convey meaning and build connections. With the rapid rise of Human-Agent Interaction (HAI) and Social Robotics (SR), there is an increasing demand for agents capable of understanding and responding to human emotions. Such systems are critical for enhancing interactions in healthcare, education, customer service, and entertainment  [1] . The ultimate goal is to develop socially adept agents that consider both the cognitive and emotional states of their human counterparts, ensuring safe, engaging, and adaptive interactions.\n\nDespite significant advancements, current emotion recognition systems struggle to capture the full complexity of real-world human-agent interactions. Many approaches rely heavily on verbal cues while neglecting subtle non-verbal signals-such as facial expressions, eye movements, and physiological responses-that are essential for accurately interpreting emotions  [2] . Moreover, existing datasets are often constrained by static or artificial stimuli, limited modality diversity, and a lack of consideration for idiosyncratic dispositions (e.g. as expressed by personality traits) that critically influence emotional expression and perception  [3] [4] [5] [6] [7] . The importance of emotions in HAI is underscored by studies demonstrating that users respond more positively to virtual agents that recognise and adapt to their emotional states  [8, 9] . To achieve this, it is necessary to differentiate among Expressed Emotions (E e ), Perceived Emotions (E p ), and Felt Emotions (E f ). When encountering emotional stimuli, individuals not only perceive the emotion (E p ) but also experience an internal affective response (E f ). Figure  1  illustrates the interplay among these categories  [10, 11] .\n\nTo address these challenges, we introduce the Advancing Face-to-Face Emotion Communication (AFFEC) dataset. AFFEC is designed to capture the dynamic complexity of face-to-face emotional interactions by integrating multiple modalities-EEG, eye-tracking, GSR, facial movements, and personality assessments-with explicit labels for both perceived and felt emotions. This rich dataset not only facilitates a more comprehensive understanding of emotional processes but also enables the exploration of how individual differences modulate affective experiences.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Contributions Of This Work",
      "text": "The main contributions of this work are as follows:\n\n• Introduction of the AFFEC Dataset: A multimodal dataset capturing 84 simulated dialogues in six distinct emotions from 73 participants, comprising over 5,000 trials and 20,000+ labels, including EEG, eye tracking, GSR, facial, and personality data.\n\n• Explicit Differentiation of Emotional Categories: AFFEC uniquely distinguishes between expressed (E e ), perceived (E p ), and felt emotions (E f ), enabling nuanced analysis of human-agent emotional exchanges.\n\n• Baseline Analyses: We present baseline models using unimodal and multimodal features, demonstrating that even minimal processing yields performance levels well above chance, particularly for arousal prediction.\n\n• Incorporation of Individual Differences: By integrating personality assessments, AF-FEC supports the exploration of how stable individual traits influence emotional perception and expression, paving the way for personalized affective computing.\n\n• Advancing HAI and SR: AFFEC provides a robust foundation for developing virtual agents and robots that recognize and respond to the dynamic, context-dependent nature of human emotions.\n\nOverall, AFFEC addresses critical gaps in emotion recognition research and offers a valuable resource for advancing the fields of affective computing, human-agent interaction, and social robotics.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Emotion recognition is a central topic in affective computing, with the goal of enabling machines to detect, interpret, and adapt to human emotions. Over the past decades, extensive research has explored various modalities-such as facial expressions, speech, and physiological signals-and a range of models to improve emotion detection accuracy and robustness. However, many existing studies rely on simplified emotion models and controlled settings, limiting their ecological validity and personalization.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emotion Models",
      "text": "Emotion recognition research typically employs discrete or dimensional models. Discrete models categorise emotions into fundamental classes-often using the Big Six (anger, disgust, fear, joy, sadness, surprise)  [12] -while dimensional models represent emotions continuously along axes like arousal and valence (and sometimes dominance)  [13] . Although discrete models offer intuitive classifications, they may oversimplify the richness of spontaneous expressions in the real world  [14] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Multimodal Emotion Recognition",
      "text": "Recent work has increasingly focused on multimodal emotion recognition to address the limitations of unimodal approaches. For example, Kollias et al.  [15]  employed a multi-task learning framework to jointly predict valence, arousal, facial expressions, and action units from audio-visual inputs, demonstrating significant performance gains.\n\nReviews by Li et al.  [16]  and Zhang et al.  [17]  further underscore that combining facial, speech, and physiological signals outperforms single-modality systems. However, many studies focus primarily on arousal and valence metrics, potentially neglecting the full complexity of human emotion.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Stimuli, Participant, And Temporal Considerations",
      "text": "Traditional emotion recognition research often relies on emotionally charged video clips, music, or data from actors  [18, 19] . Although these controlled elicitation methods yield reproducible ground truths, they may fail to capture the subtlety and spontaneity inherent in everyday interactions. Moreover, many studies use static or shortduration stimuli that overlook the temporal evolution of emotions  [20, 21] . As noted by Wang et al.  [22] , integrating temporal dynamics is essential for accurately modeling how emotions unfold over time.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Personality And Emotion Recognition",
      "text": "Individual differences, particularly personality traits, play a significant role in shaping emotional perception and expression. For instance, neuroticism is associated with heightened negative affect and arousal  [23] , whereas extraversion correlates with increased positive emotions  [24]  and open-ness enhances receptivity to diverse emotional stimuli  [25] . Incorporating personality measures into emotion recognition systems can lead to more personalised and adaptive models.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Notable Emotion Recognition Datasets",
      "text": "Several multimodal datasets have advanced the field:\n\n• DECAF  [26]  integrates MEG, hEOG, ECG, tEMG, video, and audio from music videos and film clips, capturing valence, arousal, and dominance.\n\n• SEED-VII  [27]  uses EEG and eye tracking to record responses to video clips designed to evoke six basic emotions and a neutral state.\n\n• DREAMER  [28]  employs low-cost EEG and ECG sensors to capture affective responses to film clips.\n\n• EAV  [29]  combines EEG, audio, and video recordings in scripted conversational settings.\n\n• AMIGOS  [30]  collects EEG, ECG, GSR, and audiovisual data during individual and group emotional experiences.\n\n• AffectNet  [31]  provides over 400,000 annotated images for facial expression analysis.\n\n• K-EmoCon  [32]  captures continuous emotional states during naturalistic conversations using EEG, ECG, physiological signals, and video.\n\n• WESAD  [33]  focuses on wearable stress and affect detection via multiple physiological signals.\n\n• MAHNOB-HCI  [34]  integrates facial videos, audio, eye gaze, and various physiological signals.\n\n• ASCERTAIN  [35]  combines EEG, ECG, and GSR with self-reports and personality assessments.\n\n• MGEED  [36]  provides rich annotations from EEG, ECG, facial optomyography (OMG), video and depth maps.\n\n• DEAP  [37]  integrates EEG, GSR, and facial videos from music videos.\n\n• SEMAINE  [31]  focusses on textual and dyadic conversational scenarios.\n\nTable ?? summarises these datasets alongside AF-FEC, highlighting AFFEC's unique integration of multimodal signals and its explicit differentiation between felt and perceived emotions.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Limitations And The Affec Contribution",
      "text": "While existing datasets have advanced the field, they often suffer from:\n\n• Limited Multimodal Diversity: Many exclude subtle non-verbal cues such as gaze and personality assessments.\n\n• Static or Contrived Stimuli: Reliance on staged expressions limits ecological validity.\n\n• Lack of Emotion Differentiation: Few datasets distinguish between felt and perceived emotions.\n\n• Participant Homogeneity: Limited demographic diversity undermines generalisability.\n\n• Neglected Temporal Dynamics: Most datasets fail to capture the evolution of emotions over time.\n\nAFFEC directly addresses these gaps by integrating EEG, eye tracking, GSR, facial movements, and personality assessments. It uniquely differentiates between expressed, perceived, and felt emotions using naturalistic stimuli from non-actor participants and continuous recordings, thereby providing a more comprehensive foundation for developing adaptive, context-aware emotion recognition models.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Affec Dataset Design And Collection",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Data Access And Structure",
      "text": "The AFFEC dataset is openly available on Zenodo under at https://doi.org/10.5281/zenodo.\n\n14794876  [38] . It follows the Brain Imaging Data Structure (BIDS)  [39]  convention and organizes each participant's recordings (EEG, eye tracking, GSR, facial videos, and behavioural logs) into sub-<subject id> folders with corresponding annotation files. At the root level, users will find:\n\n• dataset description.json -High-level dataset metadata.\n\n• participants.tsv and participants.json -Demographic data and Big Five personality scores.\n\n• task-fer events.json -Global event annotations for the emotion-recognition (FER) task.\n\n• README.md -Detailed documentation on file formats, usage, and data collection.\n\nEach subject folder is further divided into subdirectories such as eeg/, beh/, and events/:\n\n• EEG Data (eeg/): .edf files sampled at 256 Hz from 63 channels (10-20 layout), accompanied by JSON sidecar files containing channel information.\n\n• Eye-Tracking, GSR, and Facial Data (beh/): JSON or TSV files capturing gaze coordinates, pupil diameter, galvanic skin response, and facial action units.\n\n• Event Files (* events.tsv): Trial timing, stimulus onsets/offsets, and emotional labels for perceived and felt emotions.\n\nAll recordings comply with the BIDS specification, ensuring compatibility with common neuroimaging and physiological signal-processing tools. Researchers can download the dataset from Zenodo at the link above.\n\nIn addition, code for preprocessing, analysis, and baseline modeling is available at https: //github.com/itubrainlab/AFFEC/tree/main. The GitHub repository includes usage guidelines and sample notebooks, enabling quick integration of AFFEC data into diverse workflows.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Participants",
      "text": "We recruited 73 participants (21 females; mean age 27.4 ± 6 years) spanning high-school through PhD educational levels. All participants had normal or corrected-to-normal vision and were neurotypical. Informed consent was obtained from each participant in accordance with institutional ethical standards.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experimental Design And Procedure",
      "text": "To simulate the listening component of a conversational setting, participants were exposed to dynamic, emotionally expressive video clips. A total of 88 trials (4 practice trials plus 84 main trials) were presented in random order. We selected 84 video clips from the CREMA-D dataset  [40] , covering 91 actors (48 males, 43 females) aged 20-74 who portrayed six basic emotions (Anger, Disgust, Fear, Happy, Neutral, Sad) at varying intensities. This selection balanced emotional content and actor demographics to ensure broad coverage and generalizability.\n\nTo enhance ecological validity, each video was preceded by a brief textual scenario offering context for the emotion depicted (Fig.  2 ). These scenarios were designed to prime the emotional responses of participants, mimicking realistic conversational cues. Figure  3  shows a sample frame from one of the stimuli.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Data Collection Modalities",
      "text": "To enable a comprehensive assessment of emotional responses, multiple data streams were recorded:\n\n• Eye-Tracking: Gazepoint GP3 (150 Hz) capturing gaze positions and pupil size, indicative of attention patterns and arousal.\n\n• EEG: g.tec hiamp 64-channel system to record neural activity associated with cognitive and emotional processes.\n\n• GSR: Shimmer 3 finger-mounted sensors tracking changes in skin conductance reflective of emotional arousal.\n\n• Facial Movements: A USB camera recorded participants' facial expressions throughout each trial.\n\n• Personality Assessments: The BFI-44 questionnaire  [41]  measured the five personality traits (Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism), adding context for individual variability in emotional perception and expression.\n\nFigure  5  shows the overall experimental setup, conducted under controlled lighting and temperature to minimize confounds.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Pre-Experiment Calibration",
      "text": "Before the main trials, the following calibrations ensured data integrity:\n\n• A 9-point grid for eye-tracker calibration.\n\n• EEG and GSR sensor checks for optimal placement and stable signals.\n\n• Practice trials to familiarize participants with the ratings and experimental procedure.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Emotion Labels And Ground Truth",
      "text": "Participants provided separate ratings for perceived and felt emotions following each video clip. This dual-labeling approach supports nuanced analyses of interactions between the stimulus's expressed emotion (E p ) and the participant's internal affective response (E f ). Arousal (1 = very calm; 9 = very excited) and valence (1 = very negative; 9 = very positive) ratings were subsequently discretized into three categories-low/negative, medium/neutral, and high/positive-using an optimal binning strategy. This approach equalizes class distributions (approximately 33% per category) and facilitates downstream classification. Figure  6  illustrates the frequency distributions and corresponding bin boundaries for perceived and felt arousal/valence.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Discretization Process And Insights",
      "text": "We adopted the following steps:\n\n• Label Categories: Continuous ratings mapped into low/negative, medium/neutral, or high/positive.\n\n• Optimal Binning: Boundaries were chosen to minimize divergence from a balanced distribution (∼ 33% per class).\n\n• Visualization: Figure  6  shows the frequency distributions with vertical dashed lines marking bin thresholds.\n\nNotable findings:\n\n1. Near-Balanced Classes: Each of the three categories captured roughly one-third of the data.\n\n4. Model Compatibility: These categories offer a robust foundation for classification tasks.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Data Splits And Evaluation",
      "text": "To ensure consistent comparisons across modalities (e.g., GSR, eye-tracking, EEG), we enforced a uniform data split (60%/20%/20% for train/validation/test) with label stratification. Each experiment used 5-fold cross-validation on the training set for hyperparameter tuning, and the best model was selected based on macro F1-score. We report mean ± standard deviation across folds to reflect variance.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Dataset Statistics",
      "text": "Finally, we summarize the primary dataset features:\n\n• Eye Tracking: 16 channels at 62 Hz (fixations, gaze coordinates, etc.) across 5,632 trials.\n\n• Pupil Data: 21 channels at 149 Hz (pupil diameter, eye position), also 5,632 trials.\n\n• Cursor Data: 4 channels at 62 Hz (cursor X/Y, state) over 5,632 trials.\n\n• Facial Analysis: 200+ channels at 40 Hz (2D/3D landmarks, gaze detection, action units) for 5,680 trials.\n\n• GSR & Other Physiological: 40 channels at 50 Hz (skin conductance, temperature, accelerometer) across 5,438 trials.\n\n• EEG: 63 channels at 256 Hz (10-20 scheme; reference: left earlobe) spanning 5,632 trials.\n\n• Self-Annotations: 5,807 trials, each with 4 emotion labels (perceived/felt arousal/valence).\n\n• Events: Onset time, duration, trial's type, and emotion labeling markers.\n\nThis multimodal richness makes AFFEC well suited for advanced affective modeling, offering a more complete view of real-time, face-to-face emotional communication.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Eye Data Analysis",
      "text": "We analyzed eye-tracking data to investigate whether visual attention patterns and pupil dynamics can predict self-reported emotional states.\n\nIn particular, we focused on four target variables-perceived arousal, perceived valence, felt arousal, and felt valence-each discretized into three ordinal categories (low, medium, high) as described in Section 3.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Modeling Approach",
      "text": "For each target variable, we trained a separate classifier using features derived from the eye-tracking data. The key features extracted per trial include:\n\n• Gaze Position: Horizontal and vertical coordinates.\n\n• Fixation Metrics: Duration and count of fixations.\n\n• Pupil Diameter: Statistical measures capturing pupil size variations.\n\nFor each trial, we computed summary statistics (mean, standard deviation, minimum, and maximum) over the time-series data for these features. The dataset was then stratified and split into 60% training, 20% validation, and 20% test sets to ensure balanced class distributions. To address class imbalance, class weights were applied during model training.\n\nRandom Forest classifiers were selected for their robustness, and model performance was evaluated using 5-fold cross-validation. The evaluation metrics include per-class F1-scores, macro-averaged F1-scores, and overall accuracy.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Results",
      "text": "Table  6  summarises the performance of the best models using eye-tracking features. Overall, the models achieve moderate predictive accuracy for arousal-related measures. However, the prediction of valence-particularly in the high category-remains challenging. These results indicate that while eye-tracking data capture important aspects of visual attention and arousal, additional modalities may be required to robustly predict emotional valence.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Modeling Approach",
      "text": "To demonstrate the suitability of AFFEC's EEG data for emotion recognition for more advanced modelling approaches, we employed a baseline deep learning model using FBSCPNet  [42] , a very popular ConvNet. This model consists of two 1dimensional convolutional layers, one for the temporal dimension and one for the spatial (channels), with 40 kernels each. For regularisation batch normalization and dropout of 0.5 is used, followed by the output classification layer. All hyperparameters were kept as presented in the original paper and for all four targets. The model was then trained for 5000 epochs and tested on a 5-fold crossvalidation split. We stress here that no hyperparameter tuning was performed, and no upsampling was applied, contrary to common practices in EEG emotion recognition to preserve the natural data distribution. Positive results from this hands-off approach show that the data can be used effectively with as much of an off-the-shelf method as can be found in deep learning modeling of EEG signals.\n\nFuture work may explore data augmentation tech-niques (e.g., adding noise or windowing) to further boost performance.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Results",
      "text": "Table  3  shows the classification performance of our CNN model on the four target variables, evaluated via 5-fold cross-validation. All metrics are well above the chance level (0.33). Notably, the model achieved its best performance for Felt Arousal (F1 = 0.50) while Perceived Valence scored the lowest (F1 = 0.40). These results show that the EEG data collected can indeed be used to successfully classify the labels in a robust manner with minimal data preprocessing and model tuning.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Gsr Signal Analysis",
      "text": "Galvanic Skin Response (GSR) signals provide valuable physiological indices of emotional arousal.\n\nIn this work, we leverage GSR data recorded during exposure to emotionally expressive face videos, along with participants' self-reported perceived and felt emotions. Our aim is to assess how GSRderived features relate to both categorical emotional stimuli and continuous affect measures.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Data Preprocessing And Feature Extraction",
      "text": "Raw GSR signals were processed using the neurokit module, which decomposes each signal into phasic (rapid fluctuations due to sudomotor activity) and tonic (slowly varying baseline) components. For each trial (from stimulus onset to 10 seconds post-offset), we extracted Skin Conduc- • Number of Peaks\n\n• SCR Onsets: mean, median, minimum, maximum, and standard deviation.\n\n• SCR Amplitude: mean, median, minimum, maximum, and standard deviation.\n\n• SCR Height: mean, median, minimum, maximum, and standard deviation.\n\n• SCR Rise Time: mean, median, minimum, maximum, and standard deviation.\n\n• SCR Recovery Time: mean, median, minimum, maximum, and standard deviation.\n\nThese metrics form a comprehensive feature set (see Table  4 ), and Figure  7  illustrates an example of the GSR signal decomposition.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Classification Of Self-Reported Emotional Responses",
      "text": "Beyond examining statistical relationships, we evaluated the predictive capacity of the GSR features for classifying self-reported emotional responses into three ordinal categories (low, medium, high) for each of the four targets: perceived arousal, perceived valence, felt arousal, and felt valence.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Experimental Setup",
      "text": "Continuous ratings for each target were discretised into the three classes as described in Section 3. After removing missing values and applying this mapping, the extracted SCR metrics were used as input features. We applied an 80/20 train-test split with stratification and trained a\n\nLGBMClassifier with class weights to address the imbalance. Model performance was evaluated using precision, recall, and F1-scores, averaged over 5-fold cross-validation.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Results Using Only Gsr Peak Metrics",
      "text": "Table  5  presents the classification performance (mean ± standard deviation) for each target. Although the F1 scores remain modest, the perfor-mance for targets such as perceived arousal and felt valence is slightly higher compared to the others.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Summary",
      "text": "GSR-derived features demonstrate sensitivity to emotional arousal, yet the complexity and subjectivity inherent in self-reported affect result in modest classification performance when using GSR data alone. Future work may improve predictive accuracy by integrating additional modalities or employing more advanced modelling techniques.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Facial Videos Analysis",
      "text": "In addition to physiological and eye-tracking data, we examined participants' facial expressions recorded during the experiment. Our analysis focuses on quantifying the alignment between participants' facial expressions and the emotional content of the stimuli, as well as assessing the utility of facial features for emotion recognition.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Variables And Measures",
      "text": "To investigate the relationship between facial expressions and emotional processing, we defined the following measures:\n\n• Sympathy: The Euclidean distance between perceived and felt emotion ratings in the valence-arousal space. Lower sympathy values indicate closer alignment between the internal (felt) and external (perceived) emotional states.\n\n• Facial Mimicry Similarity: The average Dynamic Time Warping (DTW) score computed across all Action Units (AUs) within each trial. Higher DTW scores reflect stronger temporal alignment between participants' facial expressions and those exhibited in the stimuli.\n\n• Emotion Recognition Performance: Evaluated using the Davies-Bouldin Index (DBI) to assess the compactness and separability of emotion clusters derived from facial features. Lower DBI values suggest better-defined clusters.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Results",
      "text": "Table  6  presents the classification performance (F1 scores) obtained using facial action unit features for predicting four target emotional variables: perceived arousal, perceived valence, felt arousal, and felt valence. The results, computed via 5-fold crossvalidation, indicate that facial features contribute valuable information for emotion recognition, although performance varies across targets. Notably, the best model for predicting felt valence (Random-Forest) achieved relatively high F1 scores compared to the other targets. These results suggest that facial video data, when analysed via action unit features, provides meaningful cues for emotion recognition. Integrating these features with other modalities may further enhance performance.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Personality Analysis",
      "text": "Personality traits play a critical role in shaping how individuals perceive, experience, and express emotions. In AFFEC, we measured the Big Five traits-Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism-using the BFI-44 questionnaire  [41] .\n\nThis section presents both the main findings of how personality correlates with perceived and felt emotions, as well as the detailed scale checks, normality tests, and correlation matrices that were originally placed in the appendix.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Measurement Scales",
      "text": "Table  7  presents the range of possible scores for each of the Big Five traits (scaled from 0 to 50 in our questionnaire) and the corresponding ranges for the emotional response ratings (from 1 to 9 on arousal and valence).",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Normality Tests",
      "text": "While large-sample data often fail formal tests of normality, we nonetheless conducted Shapiro-Wilk tests on each personality trait and emotional response variable to gauge distribution shapes (Table 8). Each p-value was below 0.05, which is not unusual given our sample size. We thus relied on nonparametric correlations (Pearson or Spearman,",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Personality-Emotion Correlations",
      "text": "Table  9  provides the complete set of pairwise Pearson correlations between each personality trait and the four emotional response variables (perceived arousal, perceived valence, felt arousal, felt valence). Although many correlations are small, a subset stand out as both significant and theoretically consistent with prior literature.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Key Findings",
      "text": "Although Table  9  enumerates all of the pairwise correlations, several stand out in size and significance:\n\n• Openness is positively correlated with felt arousal (r ≈ 0.138, p < 10 -25 ), suggesting that individuals high in openness tend to experience more intense internal emotional states.\n\n• Extraversion also shows a positive association with felt arousal (r ≈ 0.122, p < 10 -20 ).\n\n• Conscientiousness and Agreeableness show modest positive correlations with felt valence, suggesting a slight tilt toward more positive emotional states in higher scorers.\n\n• Neuroticism is negatively correlated with both felt arousal and felt valence, consistent with a propensity for distress and more negative affect.\n\nWe collect several of these strongest associations in Table  10 .",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Implications And Future Directions",
      "text": "These personality-emotion correlations demonstrate that trait-level differences systematically modulate affective responses. In practice, integrating personality measures into affective computing pipelines can enable more adaptive, user-specific emotion recognition systems. For example, highly conscientious and agreeable users may exhibit more positive valence overall, whereas individuals high in neuroticism may require special modeling to handle more frequent negative affect. Future work might investigate sub-facets of each Big Five trait (e.g., anxiety vs. depression within neuroticism) or adopt mixed-effects models that account for repeated measures. In conjunction with the multimodal signals of AFFEC, these personality insights can help researchers build robust, personalized affective computing models that reflect the interplay of stable traits and transient emotional states.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Multimodal Analysis",
      "text": "In this section, we evaluate the performance of emotion classification for baseline models using multimodal features. Our approach fuses signals from eye-tracking, facial action units (extracted from facial videos), and GSR. We further examine the effect of incorporating personality characteristics into the multimodal representation. Note that these baselines are based on minimal data processing and straightforward feature extraction. 9.1 Results with Multimodal Features (Eye, Facial Action Units, GSR)\n\nTable  6  summarises the 5-fold cross-validation performance when combining eye-tracking, facial action unit, and GSR features. The reported F1 scores (along with standard deviations) are provided for each emotion target.\n\n• Perceived Arousal: The best model (Light-GBM) achieves a macro-average F1 score of 0.4384 ± 0.0113, with individual class performance ranging from 0.3408 (medium) to 0.5183 (low).\n\n• Perceived Valence: The best model (Neu-ralNetFastAI) reaches a macro-average F1 score of 0.4230 ± 0.0150.\n\n• Felt Arousal: With XGBoost as the best model, the macro-average F1 score is 0.4619 ± 0.0119.\n\n• Felt Valence: Using NeuralNetTorch, the macro-average F1 score is 0.4527 ± 0.0159.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Results With Personality Features",
      "text": "Table ?? presents the classification performance when personality features are integrated with the multimodal data (eye, facial action units, and GSR). Comparing these results with those in Section 9, we observe that:\n\n• For Perceived Arousal and Perceived Valence, the macroaverage F1 scores remain comparable (0.4377 ± 0.0080 and 0.4242 ± 0.0152, respectively).\n\n• For Felt Arousal and Felt Valence, incorporating personality results in modest improvements, yielding macro-average F1 scores of 0.4778 ± 0.0142 and 0.4600 ± 0.0194, respectively.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Discussion",
      "text": "Comparing the performance of each modality and the combined (multimodal) configurations highlights several important takeaways. First, eyetracking yields moderate success in predicting arousal but struggles more with high valence. This suggests that gaze patterns and pupil dynamics capture aspects of emotional intensity but may not fully account for an emotion's positivity or negativity. In contrast, EEG shows robust performance across targets, especially for felt arousal, despite minimal artefact removal and no hyperparameter tuning; this underscores the rich signal captured by neural measures. GSR data demonstrate moderate predictive capacity, aligning well with arousalrelated states but showing limited effectiveness for valence on their own. Facial action units derived from video recordings offer meaningful cues for distinguishing different emotional states, although accuracy again tends to be higher for arousal. When combining eye-tracking, GSR, and facial modalities, we observe complementary gains across all emotional targets. The macro-average F1 scores in the multimodal setting range from 0.43 to 0.46, confirming that data fusion enhances recognition and suggesting that each sensor taps unique aspects of emotional expression. Furthermore, introducing personality traits yields modest but consistent improvements, particularly for felt emotions, a finding that underscores the role of stable individual differences in affective experiences.\n\nBecause these results are derived from baseline methods with simple feature extraction and no extensive optimisation, they serve primarily as evidence that the AFFEC data set contains discriminative signals for emotion recognition. Future work can build upon these findings by pursuing more advanced feature engineering, refined artefact handling (especially for EEG and GSR), and sophisticated fusion strategies (e.g., attention-based deep learning). These refinements could further uncover the nuanced temporal and cross-modal relationships that underlie human emotional experiences. Overall, the baseline models confirm AFFEC's potential to drive research in multimodal affective computing, while leaving ample room for improved approaches and deeper insights.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Discussion And Future Directions",
      "text": "Across all modalities considered in this study-eyetracking, EEG, GSR, and facial video-our data-splitting and evaluation protocols remained consistent. Each modality was divided into 60% training, 20% validation, and 20% testing sets with label stratification, and we trained baseline models under a uniform 5-fold cross-validation regime. These choices aimed to ensure fair modality comparisons, mitigate overfitting, and provide a transparent demonstration of the dataset's quality.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Baseline Findings And Modalities",
      "text": "The primary motivation behind these baseline experiments was to validate that the AFFEC dataset contains meaningful signals for emotion recognition, rather than to optimize model performance fully. Indeed, our results show:\n\n• Eye-Tracking: Gaze position, fixation metrics, and pupil dynamics reliably capture aspects of arousal but struggle with valence prediction.\n\n• EEG: Even with minimal artefact removal and no hyperparameter tuning, a simple CNN achieves performance well above chance on both perceived and felt emotion categories.\n\n• GSR: Phasic and tonic skin conductance data exhibit moderate predictive power, particularly for arousal.\n\n• Facial Expressions: Facial action units extracted via OpenFace provide valuable cues; however, accuracy varies across different emotion types.\n\n• Personality Traits: Incorporating the Big Five scales yields modest but consistent improvements in classifying felt arousal and valence, highlighting the role of stable individual differences.\n\nWhen multiple data streams (e.g., eye-tracking, GSR, and facial AUs) are fused, performance improves, underscoring the complementary nature of these signals. Yet, valence predictions remain more challenging than arousal, matching broader trends in affective computing. Overall, these results confirm that AFFEC provides discriminative features across diverse modalities.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Building On Affec: Advanced Methods And Insights",
      "text": "Though our baselines use straightforward feature extraction and modeling, two recent efforts have adopted more sophisticated approaches on this dataset:\n\n• Advanced Facial Mimicry Analysis  [43] :\n\nOne study devoted effort to facial action unit processing employing dynamic time warping (DTW) to align participant AUs with those expressed in the stimuli. By meticulously segmenting and synchronizing the AU patterns, the authors probed how closely the facial expressions of the participants mirrored the video content, revealing richer insights into the interaction between perceived and felt responses. Their results indicated that finegrained temporal alignment could discriminate subtle mimicry differences, particularly among emotions like fear or anger, and suggested that personality traits modulate the degree of alignment.\n\n• Personality and Eye-Tracking Fusion  [44] : Another work concentrated on neuralnetwork-based pipelines that integrated personality assessments, high-resolution eyetracking metrics, and temporal modelling.\n\nAfter extensive data preprocessing-such as more refined pupil-baseline corrections and gaze-region segmentations-the authors used multi-branch architectures to fuse these features. Notably, they reported higher macro-F1 scores than our baselines for both felt and perceived valence, indicating that deeper personalization and advanced sequence modelling can further enhance emotion prediction.\n\nBoth endeavors confirm that AFFEC can accommodate more detailed data preprocessing steps (e.g., improved noise filtering, advanced feature engineering) and model designs (e.g., neural networks with multi-branch temporal fusion). They also illustrate how personalization-either via personality traits or by modelling individual temporal patterns-can unlock deeper insights into the differences between internal and externally perceived affect.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Key Takeaways And Future Directions",
      "text": "• AFFEC as a Foundation: The dataset's comprehensive signals across EEG, eyetracking, GSR, facial analysis, and personality questionnaires make it a valuable platform for exploring more advanced or specialised approaches.\n\n• Multimodal Fusion Potential: Our results and recent extensions highlight the gains from combining modalities. Attention-based architectures, transformer models, or advanced temporal alignment techniques (e.g., DTW) may capture the sequential nature of emotional expression more effectively.\n\n• Personality-Informed Models: Empirical gains from personality integration suggest that personalisation is a promising avenue. Future work could refine trait-based or mixed-effects models to handle inter-participant variability more precisely.\n\n• Extended Scenarios: AFFEC's structure also invites expansions such as live, real-time systems or investigations into how contextual factors (like conversation flow) alter the mapping between physiological cues and subjective emotion labels.\n\nUltimately, these baseline evaluations confirm the rich signal content of AFFEC while leaving ample scope for innovative modelling and analysis. By incorporating deeper temporal dynamics, refined personalization, or sophisticated fusion methods, future research can push the boundaries of emotionaware technology and better characterize the subtleties of face-to-face affective communication.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Conclusion",
      "text": "AFFEC provides a multimodal dataset uniquely suited for studying face-to-face emotive communication in both perceived and felt contexts. Our baseline analyses, covering EEG, eye-tracking, GSR, facial expression data, and personality measures, confirm the presence of discriminative signals for emotion recognition and underscore that:\n\n• Arousal is more tractable than valence across most modalities, though multimodal fusion narrows the gap.\n\n• Eye-tracking, EEG, and GSR each offer complementary information about emotional states, and combining them yields notable performance gains.\n\n• Personality traits contribute valuable insight into individual differences, especially for felt emotions.\n\nThe dataset's comprehensive structure, BIDS formatting, and public availability encourage broad adoption and reproducibility. Moreover, two subsequent studies leveraging AFFEC have demonstrated its flexibility: from employing temporal alignment for in-depth facial mimicry analysis to integrating personality-driven gaze models for improved internal state prediction. We anticipate future work will explore advanced multimodal fusion strategies (e.g., attention mechanisms, graph-based architectures) and examine interactive paradigms or larger demographic samples to further elucidate how personality and context shape emotional communication. Overall, the AFFEC dataset represents a significant step forward for research in affective computing, human-agent interaction, and social robotics, supporting the design of more accurate, adaptive, and empathetic emotion-aware technologies.",
      "page_start": 18,
      "page_end": 18
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: illustrates the in-",
      "page": 2
    },
    {
      "caption": "Figure 1: An illustrative overview of the three types",
      "page": 2
    },
    {
      "caption": "Figure 2: ). These scenar-",
      "page": 5
    },
    {
      "caption": "Figure 3: shows a sample frame from one of",
      "page": 5
    },
    {
      "caption": "Figure 2: Example scenario presented before the",
      "page": 5
    },
    {
      "caption": "Figure 3: Representative frame from a CREMA-D",
      "page": 5
    },
    {
      "caption": "Figure 4: The 9-point scales used for describing the",
      "page": 5
    },
    {
      "caption": "Figure 5: shows the overall experimental setup,",
      "page": 6
    },
    {
      "caption": "Figure 5: Participant in the experimental setup",
      "page": 6
    },
    {
      "caption": "Figure 6: Frequency distributions and correspond-",
      "page": 6
    },
    {
      "caption": "Figure 6: illustrates",
      "page": 6
    },
    {
      "caption": "Figure 6: shows the frequency",
      "page": 6
    },
    {
      "caption": "Figure 7: illustrates an example of the",
      "page": 9
    },
    {
      "caption": "Figure 7: Decomposition of GSR signals into phasic",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1IT University of Copenhagen, 2Technical University of Denmark": ""
        },
        {
          "1IT University of Copenhagen, 2Technical University of Denmark": "experimentation with more\nrealistic\nface-to-face"
        },
        {
          "1IT University of Copenhagen, 2Technical University of Denmark": "stimuli,\nAFFEC\noffers\na\nunique\nresource\nfor"
        },
        {
          "1IT University of Copenhagen, 2Technical University of Denmark": "researchers\naiming\nto\ndevelop\ncontext-sensitive,"
        },
        {
          "1IT University of Copenhagen, 2Technical University of Denmark": ""
        },
        {
          "1IT University of Copenhagen, 2Technical University of Denmark": "adaptive,\nand\npersonalized\nemotion\nrecognition"
        },
        {
          "1IT University of Copenhagen, 2Technical University of Denmark": ""
        },
        {
          "1IT University of Copenhagen, 2Technical University of Denmark": "models."
        },
        {
          "1IT University of Copenhagen, 2Technical University of Denmark": ""
        },
        {
          "1IT University of Copenhagen, 2Technical University of Denmark": ""
        },
        {
          "1IT University of Copenhagen, 2Technical University of Denmark": ""
        },
        {
          "1IT University of Copenhagen, 2Technical University of Denmark": "1\nIntroduction"
        },
        {
          "1IT University of Copenhagen, 2Technical University of Denmark": ""
        },
        {
          "1IT University of Copenhagen, 2Technical University of Denmark": ""
        },
        {
          "1IT University of Copenhagen, 2Technical University of Denmark": "Human communication is inherently emotional, re-"
        },
        {
          "1IT University of Copenhagen, 2Technical University of Denmark": ""
        },
        {
          "1IT University of Copenhagen, 2Technical University of Denmark": "lying on both verbal and non-verbal cues to convey"
        },
        {
          "1IT University of Copenhagen, 2Technical University of Denmark": ""
        },
        {
          "1IT University of Copenhagen, 2Technical University of Denmark": "meaning and build connections. With the\nrapid"
        },
        {
          "1IT University of Copenhagen, 2Technical University of Denmark": ""
        },
        {
          "1IT University of Copenhagen, 2Technical University of Denmark": "rise of Human-Agent Interaction (HAI) and Social"
        },
        {
          "1IT University of Copenhagen, 2Technical University of Denmark": ""
        },
        {
          "1IT University of Copenhagen, 2Technical University of Denmark": "Robotics\n(SR),\nthere is an increasing demand for"
        },
        {
          "1IT University of Copenhagen, 2Technical University of Denmark": ""
        },
        {
          "1IT University of Copenhagen, 2Technical University of Denmark": "agents capable of understanding and responding to"
        },
        {
          "1IT University of Copenhagen, 2Technical University of Denmark": ""
        },
        {
          "1IT University of Copenhagen, 2Technical University of Denmark": "human emotions. Such systems are critical\nfor en-"
        },
        {
          "1IT University of Copenhagen, 2Technical University of Denmark": ""
        },
        {
          "1IT University of Copenhagen, 2Technical University of Denmark": "hancing interactions in healthcare, education, cus-"
        },
        {
          "1IT University of Copenhagen, 2Technical University of Denmark": ""
        },
        {
          "1IT University of Copenhagen, 2Technical University of Denmark": "tomer service, and entertainment [1]. The ultimate"
        },
        {
          "1IT University of Copenhagen, 2Technical University of Denmark": ""
        },
        {
          "1IT University of Copenhagen, 2Technical University of Denmark": "goal\nis\nto develop socially adept agents\nthat con-"
        },
        {
          "1IT University of Copenhagen, 2Technical University of Denmark": ""
        },
        {
          "1IT University of Copenhagen, 2Technical University of Denmark": "sider both the\ncognitive\nand emotional\nstates\nof"
        },
        {
          "1IT University of Copenhagen, 2Technical University of Denmark": ""
        },
        {
          "1IT University of Copenhagen, 2Technical University of Denmark": "their human counterparts, ensuring safe, engaging,"
        },
        {
          "1IT University of Copenhagen, 2Technical University of Denmark": ""
        },
        {
          "1IT University of Copenhagen, 2Technical University of Denmark": "and adaptive interactions."
        },
        {
          "1IT University of Copenhagen, 2Technical University of Denmark": ""
        },
        {
          "1IT University of Copenhagen, 2Technical University of Denmark": "Despite\nsignificant advancements,\ncurrent\nemo-"
        },
        {
          "1IT University of Copenhagen, 2Technical University of Denmark": ""
        },
        {
          "1IT University of Copenhagen, 2Technical University of Denmark": "tion recognition systems struggle to capture the full"
        },
        {
          "1IT University of Copenhagen, 2Technical University of Denmark": ""
        },
        {
          "1IT University of Copenhagen, 2Technical University of Denmark": "complexity of real-world human-agent interactions."
        },
        {
          "1IT University of Copenhagen, 2Technical University of Denmark": ""
        },
        {
          "1IT University of Copenhagen, 2Technical University of Denmark": "Many approaches rely heavily on verbal cues while"
        },
        {
          "1IT University of Copenhagen, 2Technical University of Denmark": ""
        },
        {
          "1IT University of Copenhagen, 2Technical University of Denmark": "neglecting\nsubtle non-verbal\nsignals—such as\nfa-"
        },
        {
          "1IT University of Copenhagen, 2Technical University of Denmark": "cial expressions, eye movements, and physiological"
        },
        {
          "1IT University of Copenhagen, 2Technical University of Denmark": "responses—that are essential\nfor accurately inter-"
        },
        {
          "1IT University of Copenhagen, 2Technical University of Denmark": "preting emotions\n[2]. Moreover,\nexisting datasets"
        },
        {
          "1IT University of Copenhagen, 2Technical University of Denmark": "are often constrained by static or artificial stimuli,"
        },
        {
          "1IT University of Copenhagen, 2Technical University of Denmark": "limited modality diversity, and a lack of considera-"
        },
        {
          "1IT University of Copenhagen, 2Technical University of Denmark": "tion for idiosyncratic dispositions (e.g. as expressed"
        },
        {
          "1IT University of Copenhagen, 2Technical University of Denmark": "by personality traits) that critically influence emo-"
        },
        {
          "1IT University of Copenhagen, 2Technical University of Denmark": "tional expression and perception [3–7]."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "The\nimportance\nof\nemotions\nin HAI\nis under-": "scored by studies demonstrating that users respond",
          "and expression, paving the way for personal-": "ized affective computing."
        },
        {
          "The\nimportance\nof\nemotions\nin HAI\nis under-": "more positively to virtual agents that recognise and",
          "and expression, paving the way for personal-": ""
        },
        {
          "The\nimportance\nof\nemotions\nin HAI\nis under-": "",
          "and expression, paving the way for personal-": "• Advancing HAI and SR: AFFEC provides"
        },
        {
          "The\nimportance\nof\nemotions\nin HAI\nis under-": "adapt\nto their emotional\nstates\n[8, 9].\nTo achieve",
          "and expression, paving the way for personal-": ""
        },
        {
          "The\nimportance\nof\nemotions\nin HAI\nis under-": "",
          "and expression, paving the way for personal-": "a\nrobust\nfoundation\nfor\ndeveloping\nvirtual"
        },
        {
          "The\nimportance\nof\nemotions\nin HAI\nis under-": "this,\nit\nis\nnecessary\nto\ndifferentiate\namong Ex-",
          "and expression, paving the way for personal-": ""
        },
        {
          "The\nimportance\nof\nemotions\nin HAI\nis under-": "",
          "and expression, paving the way for personal-": "agents and robots that recognize and respond"
        },
        {
          "The\nimportance\nof\nemotions\nin HAI\nis under-": "pressed Emotions\n(Ee), Perceived Emotions\n(Ep),",
          "and expression, paving the way for personal-": ""
        },
        {
          "The\nimportance\nof\nemotions\nin HAI\nis under-": "",
          "and expression, paving the way for personal-": "to the dynamic,\ncontext-dependent nature of"
        },
        {
          "The\nimportance\nof\nemotions\nin HAI\nis under-": "and Felt Emotions (Ef ). When encountering emo-",
          "and expression, paving the way for personal-": ""
        },
        {
          "The\nimportance\nof\nemotions\nin HAI\nis under-": "",
          "and expression, paving the way for personal-": "human emotions."
        },
        {
          "The\nimportance\nof\nemotions\nin HAI\nis under-": "tional\nstimuli,\nindividuals not\nonly perceive\nthe",
          "and expression, paving the way for personal-": ""
        },
        {
          "The\nimportance\nof\nemotions\nin HAI\nis under-": "emotion (Ep) but also experience an internal af-",
          "and expression, paving the way for personal-": ""
        },
        {
          "The\nimportance\nof\nemotions\nin HAI\nis under-": "",
          "and expression, paving the way for personal-": "Overall, AFFEC addresses critical gaps in emo-"
        },
        {
          "The\nimportance\nof\nemotions\nin HAI\nis under-": "Figure 1 illustrates\nthe in-\nfective response (Ef ).",
          "and expression, paving the way for personal-": ""
        },
        {
          "The\nimportance\nof\nemotions\nin HAI\nis under-": "",
          "and expression, paving the way for personal-": "tion recognition research and offers a valuable re-"
        },
        {
          "The\nimportance\nof\nemotions\nin HAI\nis under-": "terplay among these categories [10, 11].",
          "and expression, paving the way for personal-": ""
        },
        {
          "The\nimportance\nof\nemotions\nin HAI\nis under-": "",
          "and expression, paving the way for personal-": "source for advancing the fields of affective comput-"
        },
        {
          "The\nimportance\nof\nemotions\nin HAI\nis under-": "To\naddress\nthese\nchallenges, we\nintroduce\nthe",
          "and expression, paving the way for personal-": ""
        },
        {
          "The\nimportance\nof\nemotions\nin HAI\nis under-": "",
          "and expression, paving the way for personal-": "ing, human-agent interaction, and social robotics."
        },
        {
          "The\nimportance\nof\nemotions\nin HAI\nis under-": "Advancing Face-to-Face Emotion Communication",
          "and expression, paving the way for personal-": ""
        },
        {
          "The\nimportance\nof\nemotions\nin HAI\nis under-": "(AFFEC) dataset.\nAFFEC is designed to\ncap-",
          "and expression, paving the way for personal-": ""
        },
        {
          "The\nimportance\nof\nemotions\nin HAI\nis under-": "ture the dynamic complexity of\nface-to-face emo-",
          "and expression, paving the way for personal-": ""
        },
        {
          "The\nimportance\nof\nemotions\nin HAI\nis under-": "tional\ninteractions by integrating multiple modal-",
          "and expression, paving the way for personal-": ""
        },
        {
          "The\nimportance\nof\nemotions\nin HAI\nis under-": "ities—EEG, eye-tracking, GSR,\nfacial movements,",
          "and expression, paving the way for personal-": ""
        },
        {
          "The\nimportance\nof\nemotions\nin HAI\nis under-": "and personality assessments—with explicit\nlabels",
          "and expression, paving the way for personal-": ""
        },
        {
          "The\nimportance\nof\nemotions\nin HAI\nis under-": "for both perceived and felt\nemotions.\nThis\nrich",
          "and expression, paving the way for personal-": ""
        },
        {
          "The\nimportance\nof\nemotions\nin HAI\nis under-": "dataset not only facilitates a more comprehensive",
          "and expression, paving the way for personal-": ""
        },
        {
          "The\nimportance\nof\nemotions\nin HAI\nis under-": "understanding of emotional processes but also en-",
          "and expression, paving the way for personal-": ""
        },
        {
          "The\nimportance\nof\nemotions\nin HAI\nis under-": "ables the exploration of how individual differences",
          "and expression, paving the way for personal-": ""
        },
        {
          "The\nimportance\nof\nemotions\nin HAI\nis under-": "modulate affective experiences.",
          "and expression, paving the way for personal-": ""
        },
        {
          "The\nimportance\nof\nemotions\nin HAI\nis under-": "",
          "and expression, paving the way for personal-": "Figure 1: An illustrative overview of the three types"
        },
        {
          "The\nimportance\nof\nemotions\nin HAI\nis under-": "1.1\nContributions of this Work",
          "and expression, paving the way for personal-": ""
        },
        {
          "The\nimportance\nof\nemotions\nin HAI\nis under-": "",
          "and expression, paving the way for personal-": "emotions\nexamined\nin\nthis\nstudy:\nExpressed"
        },
        {
          "The\nimportance\nof\nemotions\nin HAI\nis under-": "The main contributions of this work are as follows:",
          "and expression, paving the way for personal-": "and Felt\nemotion (Ee), Perceived emotion (Ep),"
        },
        {
          "The\nimportance\nof\nemotions\nin HAI\nis under-": "",
          "and expression, paving the way for personal-": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 1: An illustrative overview of the three types": ""
        },
        {
          "Figure 1: An illustrative overview of the three types": "of\nemotions\nexamined\nin\nthis\nstudy:\nExpressed"
        },
        {
          "Figure 1: An illustrative overview of the three types": "and Felt\nemotion (Ee), Perceived emotion (Ep),"
        },
        {
          "Figure 1: An illustrative overview of the three types": "emotion (Ef )."
        },
        {
          "Figure 1: An illustrative overview of the three types": ""
        },
        {
          "Figure 1: An illustrative overview of the three types": ""
        },
        {
          "Figure 1: An illustrative overview of the three types": ""
        },
        {
          "Figure 1: An illustrative overview of the three types": "2\nRelated Work"
        },
        {
          "Figure 1: An illustrative overview of the three types": ""
        },
        {
          "Figure 1: An illustrative overview of the three types": ""
        },
        {
          "Figure 1: An illustrative overview of the three types": "Emotion\nrecognition\nis\na\ncentral\ntopic\nin\naffec-"
        },
        {
          "Figure 1: An illustrative overview of the three types": ""
        },
        {
          "Figure 1: An illustrative overview of the three types": "tive computing, with the goal of enabling machines"
        },
        {
          "Figure 1: An illustrative overview of the three types": "to\ndetect,\ninterpret,\nand\nadapt\nto\nhuman\nemo-"
        },
        {
          "Figure 1: An illustrative overview of the three types": ""
        },
        {
          "Figure 1: An illustrative overview of the three types": "tions.\nOver\nthe past decades,\nextensive\nresearch"
        },
        {
          "Figure 1: An illustrative overview of the three types": ""
        },
        {
          "Figure 1: An illustrative overview of the three types": "has explored various modalities—such as facial ex-"
        },
        {
          "Figure 1: An illustrative overview of the three types": ""
        },
        {
          "Figure 1: An illustrative overview of the three types": "pressions, speech, and physiological signals—and a"
        },
        {
          "Figure 1: An illustrative overview of the three types": ""
        },
        {
          "Figure 1: An illustrative overview of the three types": "range of models to improve emotion detection accu-"
        },
        {
          "Figure 1: An illustrative overview of the three types": ""
        },
        {
          "Figure 1: An illustrative overview of the three types": "racy and robustness. However, many existing stud-"
        },
        {
          "Figure 1: An illustrative overview of the three types": "ies rely on simplified emotion models and controlled"
        },
        {
          "Figure 1: An illustrative overview of the three types": ""
        },
        {
          "Figure 1: An illustrative overview of the three types": "settings,\nlimiting their ecological validity and per-"
        },
        {
          "Figure 1: An illustrative overview of the three types": ""
        },
        {
          "Figure 1: An illustrative overview of the three types": "sonalization."
        },
        {
          "Figure 1: An illustrative overview of the three types": ""
        },
        {
          "Figure 1: An illustrative overview of the three types": ""
        },
        {
          "Figure 1: An illustrative overview of the three types": ""
        },
        {
          "Figure 1: An illustrative overview of the three types": "2.1\nEmotion Models"
        },
        {
          "Figure 1: An illustrative overview of the three types": "Emotion recognition research typically employs dis-"
        },
        {
          "Figure 1: An illustrative overview of the three types": "crete or dimensional models. Discrete models cate-"
        },
        {
          "Figure 1: An illustrative overview of the three types": "gorise emotions into fundamental classes—often us-"
        },
        {
          "Figure 1: An illustrative overview of the three types": "ing the Big Six (anger, disgust,\nfear,\njoy,\nsadness,"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "surprise) [12]—while dimensional models represent": "emotions continuously along axes like arousal and",
          "ness enhances receptivity to diverse emotional stim-": "uli\n[25].\nIncorporating personality measures\ninto"
        },
        {
          "surprise) [12]—while dimensional models represent": "valence (and sometimes dominance) [13]. Although",
          "ness enhances receptivity to diverse emotional stim-": "emotion recognition systems can lead to more per-"
        },
        {
          "surprise) [12]—while dimensional models represent": "discrete models offer\nintuitive classifications,\nthey",
          "ness enhances receptivity to diverse emotional stim-": "sonalised and adaptive models."
        },
        {
          "surprise) [12]—while dimensional models represent": "may oversimplify the\nrichness of\nspontaneous\nex-",
          "ness enhances receptivity to diverse emotional stim-": ""
        },
        {
          "surprise) [12]—while dimensional models represent": "pressions in the real world [14].",
          "ness enhances receptivity to diverse emotional stim-": ""
        },
        {
          "surprise) [12]—while dimensional models represent": "",
          "ness enhances receptivity to diverse emotional stim-": "2.5\nNotable\nEmotion\nRecognition"
        },
        {
          "surprise) [12]—while dimensional models represent": "",
          "ness enhances receptivity to diverse emotional stim-": "Datasets"
        },
        {
          "surprise) [12]—while dimensional models represent": "2.2\nMultimodal\nEmotion Recogni-",
          "ness enhances receptivity to diverse emotional stim-": ""
        },
        {
          "surprise) [12]—while dimensional models represent": "",
          "ness enhances receptivity to diverse emotional stim-": "Several multimodal\ndatasets\nhave\nadvanced\nthe"
        },
        {
          "surprise) [12]—while dimensional models represent": "tion",
          "ness enhances receptivity to diverse emotional stim-": ""
        },
        {
          "surprise) [12]—while dimensional models represent": "",
          "ness enhances receptivity to diverse emotional stim-": "field:"
        },
        {
          "surprise) [12]—while dimensional models represent": "Recent work\nhas\nincreasingly\nfocused\non multi-",
          "ness enhances receptivity to diverse emotional stim-": ""
        },
        {
          "surprise) [12]—while dimensional models represent": "",
          "ness enhances receptivity to diverse emotional stim-": "• DECAF [26]\nintegrates MEG, hEOG, ECG,"
        },
        {
          "surprise) [12]—while dimensional models represent": "modal emotion recognition to address\nthe limita-",
          "ness enhances receptivity to diverse emotional stim-": ""
        },
        {
          "surprise) [12]—while dimensional models represent": "",
          "ness enhances receptivity to diverse emotional stim-": "tEMG,\nvideo,\nand audio\nfrom music\nvideos"
        },
        {
          "surprise) [12]—while dimensional models represent": "tions of unimodal approaches. For example, Kollias",
          "ness enhances receptivity to diverse emotional stim-": ""
        },
        {
          "surprise) [12]—while dimensional models represent": "",
          "ness enhances receptivity to diverse emotional stim-": "and film clips, capturing valence, arousal, and"
        },
        {
          "surprise) [12]—while dimensional models represent": "et al.\n[15]\nemployed a multi-task learning frame-",
          "ness enhances receptivity to diverse emotional stim-": ""
        },
        {
          "surprise) [12]—while dimensional models represent": "",
          "ness enhances receptivity to diverse emotional stim-": "dominance."
        },
        {
          "surprise) [12]—while dimensional models represent": "work to jointly predict valence, arousal,\nfacial ex-",
          "ness enhances receptivity to diverse emotional stim-": ""
        },
        {
          "surprise) [12]—while dimensional models represent": "pressions,\nand action units\nfrom audio-visual\nin-",
          "ness enhances receptivity to diverse emotional stim-": "• SEED-VII\n[27] uses EEG and eye\ntracking"
        },
        {
          "surprise) [12]—while dimensional models represent": "puts, demonstrating significant performance gains.",
          "ness enhances receptivity to diverse emotional stim-": "to record responses to video clips designed to"
        },
        {
          "surprise) [12]—while dimensional models represent": "Reviews by Li et al.\n[16] and Zhang et al.\n[17]\nfur-",
          "ness enhances receptivity to diverse emotional stim-": "evoke six basic emotions and a neutral state."
        },
        {
          "surprise) [12]—while dimensional models represent": "ther underscore that combining facial, speech, and",
          "ness enhances receptivity to diverse emotional stim-": ""
        },
        {
          "surprise) [12]—while dimensional models represent": "",
          "ness enhances receptivity to diverse emotional stim-": "• DREAMER [28] employs low-cost EEG and"
        },
        {
          "surprise) [12]—while dimensional models represent": "physiological\nsignals\noutperforms\nsingle-modality",
          "ness enhances receptivity to diverse emotional stim-": ""
        },
        {
          "surprise) [12]—while dimensional models represent": "",
          "ness enhances receptivity to diverse emotional stim-": "ECG sensors to capture affective responses to"
        },
        {
          "surprise) [12]—while dimensional models represent": "systems. However, many studies focus primarily on",
          "ness enhances receptivity to diverse emotional stim-": ""
        },
        {
          "surprise) [12]—while dimensional models represent": "",
          "ness enhances receptivity to diverse emotional stim-": "film clips."
        },
        {
          "surprise) [12]—while dimensional models represent": "arousal and valence metrics, potentially neglecting",
          "ness enhances receptivity to diverse emotional stim-": ""
        },
        {
          "surprise) [12]—while dimensional models represent": "the full complexity of human emotion.",
          "ness enhances receptivity to diverse emotional stim-": ""
        },
        {
          "surprise) [12]—while dimensional models represent": "",
          "ness enhances receptivity to diverse emotional stim-": "• EAV [29]\ncombines EEG,\naudio,\nand video"
        },
        {
          "surprise) [12]—while dimensional models represent": "",
          "ness enhances receptivity to diverse emotional stim-": "recordings in scripted conversational settings."
        },
        {
          "surprise) [12]—while dimensional models represent": "2.3\nStimuli, Participant,\nand Tem-",
          "ness enhances receptivity to diverse emotional stim-": ""
        },
        {
          "surprise) [12]—while dimensional models represent": "",
          "ness enhances receptivity to diverse emotional stim-": "• AMIGOS [30] collects EEG, ECG, GSR, and"
        },
        {
          "surprise) [12]—while dimensional models represent": "poral Considerations",
          "ness enhances receptivity to diverse emotional stim-": "audiovisual data during individual and group"
        },
        {
          "surprise) [12]—while dimensional models represent": "",
          "ness enhances receptivity to diverse emotional stim-": "emotional experiences."
        },
        {
          "surprise) [12]—while dimensional models represent": "Traditional emotion recognition research often re-",
          "ness enhances receptivity to diverse emotional stim-": ""
        },
        {
          "surprise) [12]—while dimensional models represent": "lies\non\nemotionally\ncharged\nvideo\nclips, music,",
          "ness enhances receptivity to diverse emotional stim-": "• AffectNet\n[31] provides over 400,000 anno-"
        },
        {
          "surprise) [12]—while dimensional models represent": "or\ndata\nfrom actors\n[18, 19].\nAlthough\nthese",
          "ness enhances receptivity to diverse emotional stim-": "tated images for facial expression analysis."
        },
        {
          "surprise) [12]—while dimensional models represent": "controlled\nelicitation methods\nyield\nreproducible",
          "ness enhances receptivity to diverse emotional stim-": ""
        },
        {
          "surprise) [12]—while dimensional models represent": "",
          "ness enhances receptivity to diverse emotional stim-": "• K-EmoCon [32]\ncaptures\ncontinuous\nemo-"
        },
        {
          "surprise) [12]—while dimensional models represent": "ground truths,\nthey may fail\nto capture the sub-",
          "ness enhances receptivity to diverse emotional stim-": ""
        },
        {
          "surprise) [12]—while dimensional models represent": "",
          "ness enhances receptivity to diverse emotional stim-": "tional states during naturalistic conversations"
        },
        {
          "surprise) [12]—while dimensional models represent": "tlety and spontaneity inherent in everyday interac-",
          "ness enhances receptivity to diverse emotional stim-": ""
        },
        {
          "surprise) [12]—while dimensional models represent": "",
          "ness enhances receptivity to diverse emotional stim-": "using EEG, ECG, physiological\nsignals,\nand"
        },
        {
          "surprise) [12]—while dimensional models represent": "tions. Moreover, many studies use static or short-",
          "ness enhances receptivity to diverse emotional stim-": ""
        },
        {
          "surprise) [12]—while dimensional models represent": "",
          "ness enhances receptivity to diverse emotional stim-": "video."
        },
        {
          "surprise) [12]—while dimensional models represent": "duration stimuli\nthat overlook the\ntemporal\nevo-",
          "ness enhances receptivity to diverse emotional stim-": ""
        },
        {
          "surprise) [12]—while dimensional models represent": "lution of emotions\n[20, 21]. As noted by Wang et",
          "ness enhances receptivity to diverse emotional stim-": ""
        },
        {
          "surprise) [12]—while dimensional models represent": "",
          "ness enhances receptivity to diverse emotional stim-": "• WESAD [33]\nfocuses on wearable stress and"
        },
        {
          "surprise) [12]—while dimensional models represent": "al.\n[22],\nintegrating temporal dynamics is essential",
          "ness enhances receptivity to diverse emotional stim-": ""
        },
        {
          "surprise) [12]—while dimensional models represent": "",
          "ness enhances receptivity to diverse emotional stim-": "affect detection via multiple physiological sig-"
        },
        {
          "surprise) [12]—while dimensional models represent": "for accurately modeling how emotions unfold over",
          "ness enhances receptivity to diverse emotional stim-": ""
        },
        {
          "surprise) [12]—while dimensional models represent": "",
          "ness enhances receptivity to diverse emotional stim-": "nals."
        },
        {
          "surprise) [12]—while dimensional models represent": "time.",
          "ness enhances receptivity to diverse emotional stim-": ""
        },
        {
          "surprise) [12]—while dimensional models represent": "",
          "ness enhances receptivity to diverse emotional stim-": "• MAHNOB-HCI [34] integrates facial videos,"
        },
        {
          "surprise) [12]—while dimensional models represent": "",
          "ness enhances receptivity to diverse emotional stim-": "audio, eye gaze, and various physiological sig-"
        },
        {
          "surprise) [12]—while dimensional models represent": "2.4\nPersonality and Emotion Recog-",
          "ness enhances receptivity to diverse emotional stim-": ""
        },
        {
          "surprise) [12]—while dimensional models represent": "",
          "ness enhances receptivity to diverse emotional stim-": "nals."
        },
        {
          "surprise) [12]—while dimensional models represent": "nition",
          "ness enhances receptivity to diverse emotional stim-": ""
        },
        {
          "surprise) [12]—while dimensional models represent": "",
          "ness enhances receptivity to diverse emotional stim-": "• ASCERTAIN [35] combines EEG, ECG, and"
        },
        {
          "surprise) [12]—while dimensional models represent": "Individual\ndifferences,\nparticularly\npersonality",
          "ness enhances receptivity to diverse emotional stim-": ""
        },
        {
          "surprise) [12]—while dimensional models represent": "",
          "ness enhances receptivity to diverse emotional stim-": "GSR with self-reports and personality assess-"
        },
        {
          "surprise) [12]—while dimensional models represent": "traits, play a significant role in shaping emotional",
          "ness enhances receptivity to diverse emotional stim-": ""
        },
        {
          "surprise) [12]—while dimensional models represent": "",
          "ness enhances receptivity to diverse emotional stim-": "ments."
        },
        {
          "surprise) [12]—while dimensional models represent": "perception and expression.\nFor\ninstance, neuroti-",
          "ness enhances receptivity to diverse emotional stim-": ""
        },
        {
          "surprise) [12]—while dimensional models represent": "cism is associated with heightened negative affect",
          "ness enhances receptivity to diverse emotional stim-": "• MGEED [36] provides rich annotations from"
        },
        {
          "surprise) [12]—while dimensional models represent": "and arousal\n[23], whereas\nextraversion correlates",
          "ness enhances receptivity to diverse emotional stim-": "EEG, ECG,\nfacial\noptomyography\n(OMG),"
        },
        {
          "surprise) [12]—while dimensional models represent": "with increased positive\nemotions\n[24]\nand open-",
          "ness enhances receptivity to diverse emotional stim-": "video and depth maps."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "videos from music videos.",
          "It follows the Brain Imaging Data\n14794876 [38].": "Structure\n(BIDS)\n[39]\nconvention\nand\norganizes"
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "",
          "It follows the Brain Imaging Data\n14794876 [38].": "each participant’s\nrecordings\n(EEG, eye tracking,"
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "• SEMAINE\n[31]\nfocusses\non\ntextual\nand",
          "It follows the Brain Imaging Data\n14794876 [38].": ""
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "",
          "It follows the Brain Imaging Data\n14794876 [38].": "GSR,\nfacial\nvideos,\nand\nbehavioural\nlogs)\ninto"
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "dyadic conversational scenarios.",
          "It follows the Brain Imaging Data\n14794876 [38].": ""
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "",
          "It follows the Brain Imaging Data\n14794876 [38].": "sub-<subject id> folders with corresponding an-"
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "",
          "It follows the Brain Imaging Data\n14794876 [38].": "notation files. At the root level, users will find:"
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "Table ?? summarises these datasets alongside AF-",
          "It follows the Brain Imaging Data\n14794876 [38].": ""
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "FEC, highlighting AFFEC’s unique integration of",
          "It follows the Brain Imaging Data\n14794876 [38].": ""
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "multimodal\nsignals and its explicit differentiation",
          "It follows the Brain Imaging Data\n14794876 [38].": "—\nHigh-level\n• dataset description.json"
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "between felt and perceived emotions.",
          "It follows the Brain Imaging Data\n14794876 [38].": "dataset metadata."
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "",
          "It follows the Brain Imaging Data\n14794876 [38].": "and\n• participants.tsv\nparticipants.json"
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "2.6\nLimitations\nand\nthe\nAFFEC",
          "It follows the Brain Imaging Data\n14794876 [38].": ""
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "",
          "It follows the Brain Imaging Data\n14794876 [38].": "— Demographic data and Big Five personality"
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "Contribution",
          "It follows the Brain Imaging Data\n14794876 [38].": ""
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "",
          "It follows the Brain Imaging Data\n14794876 [38].": "scores."
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "While\nexisting datasets have\nadvanced the field,",
          "It follows the Brain Imaging Data\n14794876 [38].": ""
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "they often suffer from:",
          "It follows the Brain Imaging Data\n14794876 [38].": "event\nan-\n• task-fer events.json — Global"
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "",
          "It follows the Brain Imaging Data\n14794876 [38].": "notations\nfor\nthe\nemotion-recognition (FER)"
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "• Limited Multimodal Diversity: Many ex-",
          "It follows the Brain Imaging Data\n14794876 [38].": ""
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "",
          "It follows the Brain Imaging Data\n14794876 [38].": "task."
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "clude subtle non-verbal cues such as gaze and",
          "It follows the Brain Imaging Data\n14794876 [38].": ""
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "personality assessments.",
          "It follows the Brain Imaging Data\n14794876 [38].": ""
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "",
          "It follows the Brain Imaging Data\n14794876 [38].": "• README.md — Detailed documentation on file"
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "",
          "It follows the Brain Imaging Data\n14794876 [38].": "formats, usage, and data collection."
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "• Static or Contrived Stimuli: Reliance on",
          "It follows the Brain Imaging Data\n14794876 [38].": ""
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "staged expressions limits ecological validity.",
          "It follows the Brain Imaging Data\n14794876 [38].": ""
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "",
          "It follows the Brain Imaging Data\n14794876 [38].": "Each subject\nfolder\nis\nfurther divided into sub-"
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "• Lack\nof Emotion Differentiation:\nFew",
          "It follows the Brain Imaging Data\n14794876 [38].": ""
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "",
          "It follows the Brain Imaging Data\n14794876 [38].": "directories such as eeg/, beh/, and events/:"
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "datasets distinguish between felt and perceived",
          "It follows the Brain Imaging Data\n14794876 [38].": ""
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "emotions.",
          "It follows the Brain Imaging Data\n14794876 [38].": ""
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "",
          "It follows the Brain Imaging Data\n14794876 [38].": "• EEG Data\nsampled at\n(eeg/):\n.edf files"
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "",
          "It follows the Brain Imaging Data\n14794876 [38].": "256 Hz\nfrom 63 channels\n(10–20 layout),\nac-"
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "• Participant Homogeneity: Limited demo-",
          "It follows the Brain Imaging Data\n14794876 [38].": ""
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "",
          "It follows the Brain Imaging Data\n14794876 [38].": "companied by JSON sidecar files\ncontaining"
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "graphic diversity undermines generalisability.",
          "It follows the Brain Imaging Data\n14794876 [38].": ""
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "",
          "It follows the Brain Imaging Data\n14794876 [38].": "channel\ninformation."
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "• Neglected\nTemporal Dynamics:\nMost",
          "It follows the Brain Imaging Data\n14794876 [38].": ""
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "datasets fail to capture the evolution of emo-",
          "It follows the Brain Imaging Data\n14794876 [38].": "• Eye-Tracking, GSR,\nand\nFacial Data"
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "tions over time.",
          "It follows the Brain Imaging Data\n14794876 [38].": "JSON or TSV files\ncapturing\ngaze\n(beh/):"
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "",
          "It follows the Brain Imaging Data\n14794876 [38].": "coordinates, pupil diameter, galvanic skin re-"
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "AFFEC directly addresses\nthese\ngaps by inte-",
          "It follows the Brain Imaging Data\n14794876 [38].": ""
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "",
          "It follows the Brain Imaging Data\n14794876 [38].": "sponse, and facial action units."
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "grating EEG, eye tracking, GSR, facial movements,",
          "It follows the Brain Imaging Data\n14794876 [38].": ""
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "and personality assessments.\nIt uniquely differen-",
          "It follows the Brain Imaging Data\n14794876 [38].": ""
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "",
          "It follows the Brain Imaging Data\n14794876 [38].": "• Event Files\nTrial\ntiming,\n(* events.tsv):"
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "tiates between expressed, perceived, and felt emo-",
          "It follows the Brain Imaging Data\n14794876 [38].": ""
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "",
          "It follows the Brain Imaging Data\n14794876 [38].": "stimulus onsets/offsets,\nand emotional\nlabels"
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "tions using naturalistic stimuli from non-actor par-",
          "It follows the Brain Imaging Data\n14794876 [38].": ""
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "",
          "It follows the Brain Imaging Data\n14794876 [38].": "for perceived and felt emotions."
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "ticipants and continuous\nrecordings,\nthereby pro-",
          "It follows the Brain Imaging Data\n14794876 [38].": ""
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "viding a more comprehensive foundation for devel-",
          "It follows the Brain Imaging Data\n14794876 [38].": ""
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "",
          "It follows the Brain Imaging Data\n14794876 [38].": "All\nrecordings\ncomply with\nthe BIDS\nspecifi-"
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "oping adaptive, context-aware emotion recognition",
          "It follows the Brain Imaging Data\n14794876 [38].": ""
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "",
          "It follows the Brain Imaging Data\n14794876 [38].": "cation,\nensuring compatibility with common neu-"
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "models.",
          "It follows the Brain Imaging Data\n14794876 [38].": ""
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "",
          "It follows the Brain Imaging Data\n14794876 [38].": "roimaging and physiological signal-processing tools."
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "",
          "It follows the Brain Imaging Data\n14794876 [38].": "Researchers can download the dataset from Zenodo"
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "3\nAFFEC Dataset Design and",
          "It follows the Brain Imaging Data\n14794876 [38].": "at the link above."
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "",
          "It follows the Brain Imaging Data\n14794876 [38].": "In\naddition,\ncode\nfor\npreprocessing,\nanalysis,"
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "Collection",
          "It follows the Brain Imaging Data\n14794876 [38].": ""
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "",
          "It follows the Brain Imaging Data\n14794876 [38].": "and\nbaseline modeling\nis\navailable\nat\nhttps:"
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "",
          "It follows the Brain Imaging Data\n14794876 [38].": "//github.com/itubrainlab/AFFEC/tree/main."
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "3.1\nData Access and Structure",
          "It follows the Brain Imaging Data\n14794876 [38].": ""
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "",
          "It follows the Brain Imaging Data\n14794876 [38].": "The GitHub repository includes usage guidelines"
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "The AFFEC dataset\nis openly available on Zen-",
          "It follows the Brain Imaging Data\n14794876 [38].": "and sample notebooks, enabling quick integration"
        },
        {
          "• DEAP [37]\nintegrates EEG, GSR, and facial": "odo under at https://doi.org/10.5281/zenodo.",
          "It follows the Brain Imaging Data\n14794876 [38].": "of AFFEC data into diverse workflows."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3.2\nParticipants": "We recruited 73 participants (21 females; mean age"
        },
        {
          "3.2\nParticipants": "27.4 ± 6 years) spanning high-school through PhD"
        },
        {
          "3.2\nParticipants": "educational\nlevels. All participants had normal or"
        },
        {
          "3.2\nParticipants": "corrected-to-normal vision and were neurotypical."
        },
        {
          "3.2\nParticipants": "Informed consent was obtained from each partici-"
        },
        {
          "3.2\nParticipants": "pant in accordance with institutional ethical stan-"
        },
        {
          "3.2\nParticipants": "dards."
        },
        {
          "3.2\nParticipants": ""
        },
        {
          "3.2\nParticipants": ""
        },
        {
          "3.2\nParticipants": "3.3\nExperimental Design and Proce-"
        },
        {
          "3.2\nParticipants": ""
        },
        {
          "3.2\nParticipants": "dure"
        },
        {
          "3.2\nParticipants": "To simulate the listening component of a conver-"
        },
        {
          "3.2\nParticipants": "sational\nsetting, participants were exposed to dy-"
        },
        {
          "3.2\nParticipants": "namic, emotionally expressive video clips. A total"
        },
        {
          "3.2\nParticipants": "of 88 trials\n(4 practice trials plus 84 main trials)"
        },
        {
          "3.2\nParticipants": "were presented in random order. We\nselected 84"
        },
        {
          "3.2\nParticipants": "video clips from the CREMA-D dataset [40], cover-"
        },
        {
          "3.2\nParticipants": "ing 91 actors (48 males, 43 females) aged 20–74 who"
        },
        {
          "3.2\nParticipants": "portrayed six basic emotions (Anger, Disgust, Fear,"
        },
        {
          "3.2\nParticipants": "Happy, Neutral, Sad) at varying intensities. This"
        },
        {
          "3.2\nParticipants": "selection balanced emotional content and actor de-"
        },
        {
          "3.2\nParticipants": "mographics to ensure broad coverage and general-"
        },
        {
          "3.2\nParticipants": ""
        },
        {
          "3.2\nParticipants": "izability."
        },
        {
          "3.2\nParticipants": ""
        },
        {
          "3.2\nParticipants": "To enhance\necological validity,\neach video was"
        },
        {
          "3.2\nParticipants": "preceded by a brief textual scenario offering context"
        },
        {
          "3.2\nParticipants": "for\nthe\nemotion depicted (Fig. 2).\nThese\nscenar-"
        },
        {
          "3.2\nParticipants": ""
        },
        {
          "3.2\nParticipants": "ios were designed to prime the emotional responses"
        },
        {
          "3.2\nParticipants": ""
        },
        {
          "3.2\nParticipants": "of participants, mimicking realistic conversational"
        },
        {
          "3.2\nParticipants": ""
        },
        {
          "3.2\nParticipants": "cues. Figure 3 shows a sample frame from one of"
        },
        {
          "3.2\nParticipants": "the stimuli."
        },
        {
          "3.2\nParticipants": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "To enable a comprehensive assessment of emotional": "responses, multiple data streams were recorded:"
        },
        {
          "To enable a comprehensive assessment of emotional": "• Eye-Tracking: Gazepoint GP3 (150 Hz) cap-"
        },
        {
          "To enable a comprehensive assessment of emotional": "turing gaze positions and pupil size,\nindicative"
        },
        {
          "To enable a comprehensive assessment of emotional": "of attention patterns and arousal."
        },
        {
          "To enable a comprehensive assessment of emotional": "• EEG: g.tec hiamp 64-channel system to record"
        },
        {
          "To enable a comprehensive assessment of emotional": "neural activity associated with cognitive and"
        },
        {
          "To enable a comprehensive assessment of emotional": "emotional processes."
        },
        {
          "To enable a comprehensive assessment of emotional": "• GSR:\nShimmer\n3\nfinger-mounted\nsensors"
        },
        {
          "To enable a comprehensive assessment of emotional": ""
        },
        {
          "To enable a comprehensive assessment of emotional": "tracking changes in skin conductance reflective"
        },
        {
          "To enable a comprehensive assessment of emotional": ""
        },
        {
          "To enable a comprehensive assessment of emotional": "of emotional arousal."
        },
        {
          "To enable a comprehensive assessment of emotional": ""
        },
        {
          "To enable a comprehensive assessment of emotional": "• Facial Movements: A USB camera recorded"
        },
        {
          "To enable a comprehensive assessment of emotional": "participants’\nfacial\nexpressions\nthroughout"
        },
        {
          "To enable a comprehensive assessment of emotional": "each trial."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "• Personality\nAssessments:\nThe\nBFI-44": "questionnaire\n[41] measured the five person-",
          "• Practice trials to familiarize participants with": "the ratings and experimental procedure."
        },
        {
          "• Personality\nAssessments:\nThe\nBFI-44": "ality traits (Openness, Conscientiousness, Ex-",
          "• Practice trials to familiarize participants with": ""
        },
        {
          "• Personality\nAssessments:\nThe\nBFI-44": "traversion, Agreeableness,\nand Neuroticism),",
          "• Practice trials to familiarize participants with": ""
        },
        {
          "• Personality\nAssessments:\nThe\nBFI-44": "",
          "• Practice trials to familiarize participants with": "3.6\nEmotion\nLabels\nand\nGround"
        },
        {
          "• Personality\nAssessments:\nThe\nBFI-44": "adding\ncontext\nfor\nindividual\nvariability\nin",
          "• Practice trials to familiarize participants with": ""
        },
        {
          "• Personality\nAssessments:\nThe\nBFI-44": "",
          "• Practice trials to familiarize participants with": "Truth"
        },
        {
          "• Personality\nAssessments:\nThe\nBFI-44": "emotional perception and expression.",
          "• Practice trials to familiarize participants with": ""
        },
        {
          "• Personality\nAssessments:\nThe\nBFI-44": "",
          "• Practice trials to familiarize participants with": "Participants\nprovided\nseparate\nratings\nfor\nper-"
        },
        {
          "• Personality\nAssessments:\nThe\nBFI-44": "Figure 5 shows\nthe overall\nexperimental\nsetup,",
          "• Practice trials to familiarize participants with": ""
        },
        {
          "• Personality\nAssessments:\nThe\nBFI-44": "",
          "• Practice trials to familiarize participants with": "ceived\nand\nfelt\nemotions\nfollowing\neach\nvideo"
        },
        {
          "• Personality\nAssessments:\nThe\nBFI-44": "conducted under controlled lighting and tempera-",
          "• Practice trials to familiarize participants with": ""
        },
        {
          "• Personality\nAssessments:\nThe\nBFI-44": "",
          "• Practice trials to familiarize participants with": "clip. This dual-labeling approach supports nuanced"
        },
        {
          "• Personality\nAssessments:\nThe\nBFI-44": "ture to minimize confounds.",
          "• Practice trials to familiarize participants with": ""
        },
        {
          "• Personality\nAssessments:\nThe\nBFI-44": "",
          "• Practice trials to familiarize participants with": "analyses of\ninteractions between the stimulus’s ex-"
        },
        {
          "• Personality\nAssessments:\nThe\nBFI-44": "",
          "• Practice trials to familiarize participants with": "pressed emotion (Ep) and the participant’s internal"
        },
        {
          "• Personality\nAssessments:\nThe\nBFI-44": "",
          "• Practice trials to familiarize participants with": "affective response (Ef ). Arousal (1 = very calm; 9"
        },
        {
          "• Personality\nAssessments:\nThe\nBFI-44": "",
          "• Practice trials to familiarize participants with": "= very excited) and valence (1 = very negative; 9 ="
        },
        {
          "• Personality\nAssessments:\nThe\nBFI-44": "",
          "• Practice trials to familiarize participants with": "very positive) ratings were subsequently discretized"
        },
        {
          "• Personality\nAssessments:\nThe\nBFI-44": "",
          "• Practice trials to familiarize participants with": "into three categories—low/negative, medium/neu-"
        },
        {
          "• Personality\nAssessments:\nThe\nBFI-44": "",
          "• Practice trials to familiarize participants with": "tral, and high/positive—using an optimal binning"
        },
        {
          "• Personality\nAssessments:\nThe\nBFI-44": "",
          "• Practice trials to familiarize participants with": "strategy.\nThis approach equalizes\nclass distribu-"
        },
        {
          "• Personality\nAssessments:\nThe\nBFI-44": "",
          "• Practice trials to familiarize participants with": "tions (approximately 33% per category) and facili-"
        },
        {
          "• Personality\nAssessments:\nThe\nBFI-44": "",
          "• Practice trials to familiarize participants with": "tates downstream classification. Figure 6 illustrates"
        },
        {
          "• Personality\nAssessments:\nThe\nBFI-44": "Figure\n5:\nParticipant\nin the\nexperimental\nsetup",
          "• Practice trials to familiarize participants with": "the frequency distributions and corresponding bin"
        },
        {
          "• Personality\nAssessments:\nThe\nBFI-44": "with EEG, GSR, and eye-tracking sensors, view-",
          "• Practice trials to familiarize participants with": "boundaries for perceived and felt arousal/valence."
        },
        {
          "• Personality\nAssessments:\nThe\nBFI-44": "ing stimuli on a monitor.",
          "• Practice trials to familiarize participants with": ""
        },
        {
          "• Personality\nAssessments:\nThe\nBFI-44": "",
          "• Practice trials to familiarize participants with": "3.6.1\nDiscretization Process and Insights"
        },
        {
          "• Personality\nAssessments:\nThe\nBFI-44": "",
          "• Practice trials to familiarize participants with": "We adopted the following steps:"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 6: summarises the performance of the best",
      "data": [
        {
          "4. Model Compatibility: These categories of-": "fer a robust foundation for classification tasks.",
          "4\nEye Data Analysis": ""
        },
        {
          "4. Model Compatibility: These categories of-": "",
          "4\nEye Data Analysis": "We\nanalyzed\neye-tracking\ndata\nto\ninvestigate"
        },
        {
          "4. Model Compatibility: These categories of-": "",
          "4\nEye Data Analysis": "whether\nvisual\nattention patterns\nand pupil dy-"
        },
        {
          "4. Model Compatibility: These categories of-": "3.6.2\nData Splits and Evaluation",
          "4\nEye Data Analysis": ""
        },
        {
          "4. Model Compatibility: These categories of-": "",
          "4\nEye Data Analysis": "namics can predict\nself-reported emotional\nstates."
        },
        {
          "4. Model Compatibility: These categories of-": "To\nensure\nconsistent\ncomparisons\nacross modali-",
          "4\nEye Data Analysis": "In\nparticular,\nwe\nfocused\non\nfour\ntarget\nvari-"
        },
        {
          "4. Model Compatibility: These categories of-": "ties\n(e.g., GSR,\neye-tracking, EEG), we\nenforced",
          "4\nEye Data Analysis": "ables—perceived\narousal,\nperceived\nvalence,\nfelt"
        },
        {
          "4. Model Compatibility: These categories of-": "a uniform data split (60%/20%/20% for train/val-",
          "4\nEye Data Analysis": "arousal,\nand\nfelt\nvalence—each\ndiscretized\ninto"
        },
        {
          "4. Model Compatibility: These categories of-": "idation/test) with label stratification. Each exper-",
          "4\nEye Data Analysis": "three ordinal categories (low, medium, high) as de-"
        },
        {
          "4. Model Compatibility: These categories of-": "iment used 5-fold cross-validation on the training",
          "4\nEye Data Analysis": "scribed in Section 3."
        },
        {
          "4. Model Compatibility: These categories of-": "set for hyperparameter tuning, and the best model",
          "4\nEye Data Analysis": ""
        },
        {
          "4. Model Compatibility: These categories of-": "was selected based on macro F1-score. We report",
          "4\nEye Data Analysis": ""
        },
        {
          "4. Model Compatibility: These categories of-": "",
          "4\nEye Data Analysis": "4.1\nModeling Approach"
        },
        {
          "4. Model Compatibility: These categories of-": "mean ± standard deviation across\nfolds\nto reflect",
          "4\nEye Data Analysis": ""
        },
        {
          "4. Model Compatibility: These categories of-": "",
          "4\nEye Data Analysis": "For each target variable, we trained a separate clas-"
        },
        {
          "4. Model Compatibility: These categories of-": "variance.",
          "4\nEye Data Analysis": ""
        },
        {
          "4. Model Compatibility: These categories of-": "",
          "4\nEye Data Analysis": "sifier using features derived from the eye-tracking"
        },
        {
          "4. Model Compatibility: These categories of-": "",
          "4\nEye Data Analysis": "data. The key features extracted per trial\ninclude:"
        },
        {
          "4. Model Compatibility: These categories of-": "3.7\nDataset Statistics",
          "4\nEye Data Analysis": ""
        },
        {
          "4. Model Compatibility: These categories of-": "",
          "4\nEye Data Analysis": "• Gaze Position: Horizontal and vertical coor-"
        },
        {
          "4. Model Compatibility: These categories of-": "Finally, we\nsummarize\nthe\nprimary\ndataset\nfea-",
          "4\nEye Data Analysis": ""
        },
        {
          "4. Model Compatibility: These categories of-": "",
          "4\nEye Data Analysis": "dinates."
        },
        {
          "4. Model Compatibility: These categories of-": "tures:",
          "4\nEye Data Analysis": ""
        },
        {
          "4. Model Compatibility: These categories of-": "",
          "4\nEye Data Analysis": "• Fixation Metrics: Duration and count of fix-"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 6: summarises the performance of the best",
      "data": [
        {
          "3.7\nDataset Statistics": ""
        },
        {
          "3.7\nDataset Statistics": "Finally, we\nsummarize\nthe\nprimary\ndataset\nfea-"
        },
        {
          "3.7\nDataset Statistics": ""
        },
        {
          "3.7\nDataset Statistics": "tures:"
        },
        {
          "3.7\nDataset Statistics": ""
        },
        {
          "3.7\nDataset Statistics": "• Eye Tracking:\n16 channels at 62 Hz\n(fixa-"
        },
        {
          "3.7\nDataset Statistics": "tions, gaze coordinates, etc.)\nacross 5,632 tri-"
        },
        {
          "3.7\nDataset Statistics": ""
        },
        {
          "3.7\nDataset Statistics": "als."
        },
        {
          "3.7\nDataset Statistics": ""
        },
        {
          "3.7\nDataset Statistics": "• Pupil Data: 21 channels at 149 Hz (pupil di-"
        },
        {
          "3.7\nDataset Statistics": ""
        },
        {
          "3.7\nDataset Statistics": "ameter, eye position), also 5,632 trials."
        },
        {
          "3.7\nDataset Statistics": ""
        },
        {
          "3.7\nDataset Statistics": ""
        },
        {
          "3.7\nDataset Statistics": "• Cursor Data:\n4\nchannels\nat\n62 Hz\n(cursor"
        },
        {
          "3.7\nDataset Statistics": ""
        },
        {
          "3.7\nDataset Statistics": "X/Y, state) over 5,632 trials."
        },
        {
          "3.7\nDataset Statistics": ""
        },
        {
          "3.7\nDataset Statistics": "• Facial Analysis:\n200+ channels\nat\n40 Hz"
        },
        {
          "3.7\nDataset Statistics": "(2D/3D landmarks,\ngaze\ndetection,\naction"
        },
        {
          "3.7\nDataset Statistics": "units) for 5,680 trials."
        },
        {
          "3.7\nDataset Statistics": ""
        },
        {
          "3.7\nDataset Statistics": "• GSR & Other Physiological:\n40 channels"
        },
        {
          "3.7\nDataset Statistics": ""
        },
        {
          "3.7\nDataset Statistics": "at 50 Hz (skin conductance,\ntemperature, ac-"
        },
        {
          "3.7\nDataset Statistics": ""
        },
        {
          "3.7\nDataset Statistics": "celerometer) across 5,438 trials."
        },
        {
          "3.7\nDataset Statistics": ""
        },
        {
          "3.7\nDataset Statistics": ""
        },
        {
          "3.7\nDataset Statistics": "• EEG: 63 channels at 256 Hz (10–20 scheme;"
        },
        {
          "3.7\nDataset Statistics": "reference:\nleft earlobe) spanning 5,632 trials."
        },
        {
          "3.7\nDataset Statistics": ""
        },
        {
          "3.7\nDataset Statistics": "• Self-Annotations:\n5,807\ntrials,\neach with"
        },
        {
          "3.7\nDataset Statistics": ""
        },
        {
          "3.7\nDataset Statistics": "4\nemotion\nlabels\n(perceived/felt\narousal/va-"
        },
        {
          "3.7\nDataset Statistics": ""
        },
        {
          "3.7\nDataset Statistics": "lence)."
        },
        {
          "3.7\nDataset Statistics": ""
        },
        {
          "3.7\nDataset Statistics": ""
        },
        {
          "3.7\nDataset Statistics": "• Events:\nOnset\ntime, duration,\ntrial’s\ntype,"
        },
        {
          "3.7\nDataset Statistics": ""
        },
        {
          "3.7\nDataset Statistics": "and emotion labeling markers."
        },
        {
          "3.7\nDataset Statistics": ""
        },
        {
          "3.7\nDataset Statistics": "This multimodal\nrichness makes AFFEC well"
        },
        {
          "3.7\nDataset Statistics": "suited for advanced affective modeling, offering a"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 2: Classification Performance (F1 Score) Using Eye-Tracking Features (5-Fold Cross-Validation)",
      "data": [
        {
          "Table 2: Classification Performance (F1 Score) Using Eye-Tracking Features (5-Fold Cross-Validation)": ""
        },
        {
          "Table 2: Classification Performance (F1 Score) Using Eye-Tracking Features (5-Fold Cross-Validation)": "Best Model"
        },
        {
          "Table 2: Classification Performance (F1 Score) Using Eye-Tracking Features (5-Fold Cross-Validation)": "High"
        },
        {
          "Table 2: Classification Performance (F1 Score) Using Eye-Tracking Features (5-Fold Cross-Validation)": "Medium"
        },
        {
          "Table 2: Classification Performance (F1 Score) Using Eye-Tracking Features (5-Fold Cross-Validation)": "Low"
        },
        {
          "Table 2: Classification Performance (F1 Score) Using Eye-Tracking Features (5-Fold Cross-Validation)": "Macro Avg"
        },
        {
          "Table 2: Classification Performance (F1 Score) Using Eye-Tracking Features (5-Fold Cross-Validation)": "Accuracy"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 2: Classification Performance (F1 Score) Using Eye-Tracking Features (5-Fold Cross-Validation)",
      "data": [
        {
          "Macro Avg\n0.3996 ± 0.0120": "Accuracy\n0.4123 ± 0.0143",
          "0.3451 ± 0.0116\n0.4849 ± 0.0116\n0.3825 ± 0.0304": "0.4576 ± 0.0126\n0.5501 ± 0.0135\n0.4642 ± 0.0304"
        },
        {
          "Macro Avg\n0.3996 ± 0.0120": "5\nEEG Analysis",
          "0.3451 ± 0.0116\n0.4849 ± 0.0116\n0.3825 ± 0.0304": "niques (e.g., adding noise or windowing) to further"
        },
        {
          "Macro Avg\n0.3996 ± 0.0120": "",
          "0.3451 ± 0.0116\n0.4849 ± 0.0116\n0.3825 ± 0.0304": "boost performance."
        },
        {
          "Macro Avg\n0.3996 ± 0.0120": "5.1\nPreprocessing",
          "0.3451 ± 0.0116\n0.4849 ± 0.0116\n0.3825 ± 0.0304": ""
        },
        {
          "Macro Avg\n0.3996 ± 0.0120": "To standardise data recorded under varying config-",
          "0.3451 ± 0.0116\n0.4849 ± 0.0116\n0.3825 ± 0.0304": "5.3\nResults"
        },
        {
          "Macro Avg\n0.3996 ± 0.0120": "urations, minimal preprocessing was applied using",
          "0.3451 ± 0.0116\n0.4849 ± 0.0116\n0.3825 ± 0.0304": ""
        },
        {
          "Macro Avg\n0.3996 ± 0.0120": "",
          "0.3451 ± 0.0116\n0.4849 ± 0.0116\n0.3825 ± 0.0304": "Table 3 shows the classification performance of our"
        },
        {
          "Macro Avg\n0.3996 ± 0.0120": "MNE v.\n1.8.0. All EEG signals were resampled",
          "0.3451 ± 0.0116\n0.4849 ± 0.0116\n0.3825 ± 0.0304": ""
        },
        {
          "Macro Avg\n0.3996 ± 0.0120": "",
          "0.3451 ± 0.0116\n0.4849 ± 0.0116\n0.3825 ± 0.0304": "CNN model\non the\nfour\ntarget\nvariables,\nevalu-"
        },
        {
          "Macro Avg\n0.3996 ± 0.0120": "to 200 Hz and bandpass filtered between 1 and 80",
          "0.3451 ± 0.0116\n0.4849 ± 0.0116\n0.3825 ± 0.0304": ""
        },
        {
          "Macro Avg\n0.3996 ± 0.0120": "",
          "0.3451 ± 0.0116\n0.4849 ± 0.0116\n0.3825 ± 0.0304": "ated via 5-fold cross-validation. All metrics are well"
        },
        {
          "Macro Avg\n0.3996 ± 0.0120": "Hz. Epochs were extracted from video onset with",
          "0.3451 ± 0.0116\n0.4849 ± 0.0116\n0.3825 ± 0.0304": ""
        },
        {
          "Macro Avg\n0.3996 ± 0.0120": "",
          "0.3451 ± 0.0116\n0.4849 ± 0.0116\n0.3825 ± 0.0304": "above the chance level (0.33). Notably, the model"
        },
        {
          "Macro Avg\n0.3996 ± 0.0120": "a fixed duration of 3 seconds. For coarse artefact",
          "0.3451 ± 0.0116\n0.4849 ± 0.0116\n0.3825 ± 0.0304": ""
        },
        {
          "Macro Avg\n0.3996 ± 0.0120": "",
          "0.3451 ± 0.0116\n0.4849 ± 0.0116\n0.3825 ± 0.0304": "achieved its best performance for Felt Arousal\n(F1"
        },
        {
          "Macro Avg\n0.3996 ± 0.0120": "removal, values above 100 µV and below -100 µV",
          "0.3451 ± 0.0116\n0.4849 ± 0.0116\n0.3825 ± 0.0304": ""
        },
        {
          "Macro Avg\n0.3996 ± 0.0120": "",
          "0.3451 ± 0.0116\n0.4849 ± 0.0116\n0.3825 ± 0.0304": "= 0.50) while Perceived Valence scored the lowest"
        },
        {
          "Macro Avg\n0.3996 ± 0.0120": "were clipped, and a channel-wise standard scaling",
          "0.3451 ± 0.0116\n0.4849 ± 0.0116\n0.3825 ± 0.0304": ""
        },
        {
          "Macro Avg\n0.3996 ± 0.0120": "",
          "0.3451 ± 0.0116\n0.4849 ± 0.0116\n0.3825 ± 0.0304": "(F1 = 0.40). These results show that the EEG data"
        },
        {
          "Macro Avg\n0.3996 ± 0.0120": "was then performed.",
          "0.3451 ± 0.0116\n0.4849 ± 0.0116\n0.3825 ± 0.0304": ""
        },
        {
          "Macro Avg\n0.3996 ± 0.0120": "",
          "0.3451 ± 0.0116\n0.4849 ± 0.0116\n0.3825 ± 0.0304": "collected can indeed be used to successfully classify"
        },
        {
          "Macro Avg\n0.3996 ± 0.0120": "",
          "0.3451 ± 0.0116\n0.4849 ± 0.0116\n0.3825 ± 0.0304": "the labels\nin a robust manner with minimal data"
        },
        {
          "Macro Avg\n0.3996 ± 0.0120": "5.2\nModeling Approach",
          "0.3451 ± 0.0116\n0.4849 ± 0.0116\n0.3825 ± 0.0304": "preprocessing and model tuning."
        },
        {
          "Macro Avg\n0.3996 ± 0.0120": "To demonstrate\nthe\nsuitability of AFFEC’s EEG",
          "0.3451 ± 0.0116\n0.4849 ± 0.0116\n0.3825 ± 0.0304": ""
        },
        {
          "Macro Avg\n0.3996 ± 0.0120": "data\nfor\nemotion recognition for more\nadvanced",
          "0.3451 ± 0.0116\n0.4849 ± 0.0116\n0.3825 ± 0.0304": ""
        },
        {
          "Macro Avg\n0.3996 ± 0.0120": "",
          "0.3451 ± 0.0116\n0.4849 ± 0.0116\n0.3825 ± 0.0304": "6\nGSR Signal Analysis"
        },
        {
          "Macro Avg\n0.3996 ± 0.0120": "modelling approaches, we employed a baseline deep",
          "0.3451 ± 0.0116\n0.4849 ± 0.0116\n0.3825 ± 0.0304": ""
        },
        {
          "Macro Avg\n0.3996 ± 0.0120": "learning model using FBSCPNet [42], a very pop-",
          "0.3451 ± 0.0116\n0.4849 ± 0.0116\n0.3825 ± 0.0304": ""
        },
        {
          "Macro Avg\n0.3996 ± 0.0120": "",
          "0.3451 ± 0.0116\n0.4849 ± 0.0116\n0.3825 ± 0.0304": "Galvanic\nSkin Response\n(GSR)\nsignals\nprovide"
        },
        {
          "Macro Avg\n0.3996 ± 0.0120": "ular ConvNet.\nThis model\nconsists\nof\ntwo\n1-",
          "0.3451 ± 0.0116\n0.4849 ± 0.0116\n0.3825 ± 0.0304": ""
        },
        {
          "Macro Avg\n0.3996 ± 0.0120": "",
          "0.3451 ± 0.0116\n0.4849 ± 0.0116\n0.3825 ± 0.0304": "valuable physiological\nindices of emotional arousal."
        },
        {
          "Macro Avg\n0.3996 ± 0.0120": "dimensional convolutional\nlayers, one for the tem-",
          "0.3451 ± 0.0116\n0.4849 ± 0.0116\n0.3825 ± 0.0304": ""
        },
        {
          "Macro Avg\n0.3996 ± 0.0120": "",
          "0.3451 ± 0.0116\n0.4849 ± 0.0116\n0.3825 ± 0.0304": "In this work, we leverage GSR data recorded dur-"
        },
        {
          "Macro Avg\n0.3996 ± 0.0120": "poral dimension and one for the spatial (channels),",
          "0.3451 ± 0.0116\n0.4849 ± 0.0116\n0.3825 ± 0.0304": ""
        },
        {
          "Macro Avg\n0.3996 ± 0.0120": "",
          "0.3451 ± 0.0116\n0.4849 ± 0.0116\n0.3825 ± 0.0304": "ing exposure to emotionally expressive face videos,"
        },
        {
          "Macro Avg\n0.3996 ± 0.0120": "with 40 kernels each. For regularisation batch nor-",
          "0.3451 ± 0.0116\n0.4849 ± 0.0116\n0.3825 ± 0.0304": ""
        },
        {
          "Macro Avg\n0.3996 ± 0.0120": "",
          "0.3451 ± 0.0116\n0.4849 ± 0.0116\n0.3825 ± 0.0304": "along with participants’ self-reported perceived and"
        },
        {
          "Macro Avg\n0.3996 ± 0.0120": "malization and dropout of 0.5 is used,\nfollowed by",
          "0.3451 ± 0.0116\n0.4849 ± 0.0116\n0.3825 ± 0.0304": ""
        },
        {
          "Macro Avg\n0.3996 ± 0.0120": "",
          "0.3451 ± 0.0116\n0.4849 ± 0.0116\n0.3825 ± 0.0304": "felt\nemotions.\nOur\naim is\nto\nassess how GSR-"
        },
        {
          "Macro Avg\n0.3996 ± 0.0120": "the\noutput\nclassification layer.\nAll hyperparam-",
          "0.3451 ± 0.0116\n0.4849 ± 0.0116\n0.3825 ± 0.0304": ""
        },
        {
          "Macro Avg\n0.3996 ± 0.0120": "",
          "0.3451 ± 0.0116\n0.4849 ± 0.0116\n0.3825 ± 0.0304": "derived\nfeatures\nrelate\nto\nboth\ncategorical\nemo-"
        },
        {
          "Macro Avg\n0.3996 ± 0.0120": "eters were\nkept\nas presented in the\noriginal pa-",
          "0.3451 ± 0.0116\n0.4849 ± 0.0116\n0.3825 ± 0.0304": ""
        },
        {
          "Macro Avg\n0.3996 ± 0.0120": "",
          "0.3451 ± 0.0116\n0.4849 ± 0.0116\n0.3825 ± 0.0304": "tional stimuli and continuous affect measures."
        },
        {
          "Macro Avg\n0.3996 ± 0.0120": "per and for all\nfour\ntargets. The model was\nthen",
          "0.3451 ± 0.0116\n0.4849 ± 0.0116\n0.3825 ± 0.0304": ""
        },
        {
          "Macro Avg\n0.3996 ± 0.0120": "trained for 5000 epochs and tested on a 5-fold cross-",
          "0.3451 ± 0.0116\n0.4849 ± 0.0116\n0.3825 ± 0.0304": ""
        },
        {
          "Macro Avg\n0.3996 ± 0.0120": "validation split. We stress here that no hyperpa-",
          "0.3451 ± 0.0116\n0.4849 ± 0.0116\n0.3825 ± 0.0304": ""
        },
        {
          "Macro Avg\n0.3996 ± 0.0120": "",
          "0.3451 ± 0.0116\n0.4849 ± 0.0116\n0.3825 ± 0.0304": "6.1\nData Preprocessing and Feature"
        },
        {
          "Macro Avg\n0.3996 ± 0.0120": "rameter tuning was performed, and no upsampling",
          "0.3451 ± 0.0116\n0.4849 ± 0.0116\n0.3825 ± 0.0304": ""
        },
        {
          "Macro Avg\n0.3996 ± 0.0120": "",
          "0.3451 ± 0.0116\n0.4849 ± 0.0116\n0.3825 ± 0.0304": "Extraction"
        },
        {
          "Macro Avg\n0.3996 ± 0.0120": "was applied, contrary to common practices in EEG",
          "0.3451 ± 0.0116\n0.4849 ± 0.0116\n0.3825 ± 0.0304": ""
        },
        {
          "Macro Avg\n0.3996 ± 0.0120": "emotion recognition to preserve\nthe natural data",
          "0.3451 ± 0.0116\n0.4849 ± 0.0116\n0.3825 ± 0.0304": "Raw\nGSR\nsignals\nwere\nprocessed\nusing\nthe"
        },
        {
          "Macro Avg\n0.3996 ± 0.0120": "distribution.\nPositive\nresults\nfrom this hands-off",
          "0.3451 ± 0.0116\n0.4849 ± 0.0116\n0.3825 ± 0.0304": "each signal\nneurokit module, which decomposes"
        },
        {
          "Macro Avg\n0.3996 ± 0.0120": "approach show that the data can be used effectively",
          "0.3451 ± 0.0116\n0.4849 ± 0.0116\n0.3825 ± 0.0304": "into phasic\n(rapid fluctuations due\nto sudomotor"
        },
        {
          "Macro Avg\n0.3996 ± 0.0120": "with as much of an off-the-shelf method as can be",
          "0.3451 ± 0.0116\n0.4849 ± 0.0116\n0.3825 ± 0.0304": "activity) and tonic (slowly varying baseline) com-"
        },
        {
          "Macro Avg\n0.3996 ± 0.0120": "found in deep learning modeling of EEG signals.",
          "0.3451 ± 0.0116\n0.4849 ± 0.0116\n0.3825 ± 0.0304": "ponents.\nFor\neach trial\n(from stimulus\nonset\nto"
        },
        {
          "Macro Avg\n0.3996 ± 0.0120": "Future work may explore data augmentation tech-",
          "0.3451 ± 0.0116\n0.4849 ± 0.0116\n0.3825 ± 0.0304": "10 seconds post-offset), we extracted Skin Conduc-"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 3: Classification Performance (F1 Score) CNN on EEG signals (5-Fold Cross-Validation)",
      "data": [
        {
          "Table 3: Classification Performance (F1 Score) CNN on EEG signals (5-Fold Cross-Validation)": "Perceived Arousal"
        },
        {
          "Table 3: Classification Performance (F1 Score) CNN on EEG signals (5-Fold Cross-Validation)": "0.4650 ± 0.0249"
        },
        {
          "Table 3: Classification Performance (F1 Score) CNN on EEG signals (5-Fold Cross-Validation)": "0.3840 ± 0.0222"
        },
        {
          "Table 3: Classification Performance (F1 Score) CNN on EEG signals (5-Fold Cross-Validation)": "0.4544 ± 0.0229"
        },
        {
          "Table 3: Classification Performance (F1 Score) CNN on EEG signals (5-Fold Cross-Validation)": "0.4345 ± 0.0067"
        },
        {
          "Table 3: Classification Performance (F1 Score) CNN on EEG signals (5-Fold Cross-Validation)": "0.4368 ± 0.0068"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 3: Classification Performance (F1 Score) CNN on EEG signals (5-Fold Cross-Validation)",
      "data": [
        {
          "Low\n0.4544 ± 0.0229": "Macro Avg\n0.4345 ± 0.0067",
          "0.3374 ± 0.0141\n0.5049 ± 0.0222\n0.3794 ± 0.0467": "0.3961 ± 0.0106\n0.4962 ± 0.0135\n0.4434 ± 0.0177"
        },
        {
          "Low\n0.4544 ± 0.0229": "Accuracy\n0.4368 ± 0.0068",
          "0.3374 ± 0.0141\n0.5049 ± 0.0222\n0.3794 ± 0.0467": "0.4099 ± 0.0106\n0.4972 ± 0.0122\n0.4674 ± 0.0176"
        },
        {
          "Low\n0.4544 ± 0.0229": "tance Response (SCR) peaks and computed the fol-",
          "0.3374 ± 0.0141\n0.5049 ± 0.0222\n0.3794 ± 0.0467": "Table 4: Extracted SCR Peak Metrics Per Trial"
        },
        {
          "Low\n0.4544 ± 0.0229": "lowing metrics:",
          "0.3374 ± 0.0141\n0.5049 ± 0.0222\n0.3794 ± 0.0467": ""
        },
        {
          "Low\n0.4544 ± 0.0229": "",
          "0.3374 ± 0.0141\n0.5049 ± 0.0222\n0.3794 ± 0.0467": "Peak Metrics"
        },
        {
          "Low\n0.4544 ± 0.0229": "• Number of Peaks",
          "0.3374 ± 0.0141\n0.5049 ± 0.0222\n0.3794 ± 0.0467": ""
        },
        {
          "Low\n0.4544 ± 0.0229": "",
          "0.3374 ± 0.0141\n0.5049 ± 0.0222\n0.3794 ± 0.0467": "Number of Peaks"
        },
        {
          "Low\n0.4544 ± 0.0229": "",
          "0.3374 ± 0.0141\n0.5049 ± 0.0222\n0.3794 ± 0.0467": "SCR Onsets (mean, median, min, max, STD)"
        },
        {
          "Low\n0.4544 ± 0.0229": "• SCR Onsets: mean, median, minimum, max-",
          "0.3374 ± 0.0141\n0.5049 ± 0.0222\n0.3794 ± 0.0467": ""
        },
        {
          "Low\n0.4544 ± 0.0229": "",
          "0.3374 ± 0.0141\n0.5049 ± 0.0222\n0.3794 ± 0.0467": "SCR Amplitude (mean, median, min, max, STD)"
        },
        {
          "Low\n0.4544 ± 0.0229": "imum, and standard deviation.",
          "0.3374 ± 0.0141\n0.5049 ± 0.0222\n0.3794 ± 0.0467": ""
        },
        {
          "Low\n0.4544 ± 0.0229": "",
          "0.3374 ± 0.0141\n0.5049 ± 0.0222\n0.3794 ± 0.0467": "SCR Height (mean, median, min, max, STD)"
        },
        {
          "Low\n0.4544 ± 0.0229": "",
          "0.3374 ± 0.0141\n0.5049 ± 0.0222\n0.3794 ± 0.0467": "SCR RiseTime (mean, median, min, max, STD)"
        },
        {
          "Low\n0.4544 ± 0.0229": "• SCR Amplitude: mean, median, minimum,",
          "0.3374 ± 0.0141\n0.5049 ± 0.0222\n0.3794 ± 0.0467": ""
        },
        {
          "Low\n0.4544 ± 0.0229": "",
          "0.3374 ± 0.0141\n0.5049 ± 0.0222\n0.3794 ± 0.0467": "SCR RecoveryTime (mean, median, min, max, STD)"
        },
        {
          "Low\n0.4544 ± 0.0229": "maximum, and standard deviation.",
          "0.3374 ± 0.0141\n0.5049 ± 0.0222\n0.3794 ± 0.0467": ""
        },
        {
          "Low\n0.4544 ± 0.0229": "• SCR Height: mean, median, minimum, max-",
          "0.3374 ± 0.0141\n0.5049 ± 0.0222\n0.3794 ± 0.0467": ""
        },
        {
          "Low\n0.4544 ± 0.0229": "",
          "0.3374 ± 0.0141\n0.5049 ± 0.0222\n0.3794 ± 0.0467": "6.2\nClassification\nof\nSelf-Reported"
        },
        {
          "Low\n0.4544 ± 0.0229": "imum, and standard deviation.",
          "0.3374 ± 0.0141\n0.5049 ± 0.0222\n0.3794 ± 0.0467": ""
        },
        {
          "Low\n0.4544 ± 0.0229": "",
          "0.3374 ± 0.0141\n0.5049 ± 0.0222\n0.3794 ± 0.0467": "Emotional Responses"
        },
        {
          "Low\n0.4544 ± 0.0229": "• SCR Rise Time: mean, median, minimum,",
          "0.3374 ± 0.0141\n0.5049 ± 0.0222\n0.3794 ± 0.0467": "Beyond examining statistical relationships, we eval-"
        },
        {
          "Low\n0.4544 ± 0.0229": "maximum, and standard deviation.",
          "0.3374 ± 0.0141\n0.5049 ± 0.0222\n0.3794 ± 0.0467": "uated the predictive capacity of the GSR features"
        },
        {
          "Low\n0.4544 ± 0.0229": "",
          "0.3374 ± 0.0141\n0.5049 ± 0.0222\n0.3794 ± 0.0467": "for\nclassifying\nself-reported\nemotional\nresponses"
        },
        {
          "Low\n0.4544 ± 0.0229": "• SCR Recovery Time: mean, median, mini-",
          "0.3374 ± 0.0141\n0.5049 ± 0.0222\n0.3794 ± 0.0467": "into three ordinal categories\n(low, medium, high)"
        },
        {
          "Low\n0.4544 ± 0.0229": "mum, maximum, and standard deviation.",
          "0.3374 ± 0.0141\n0.5049 ± 0.0222\n0.3794 ± 0.0467": "for each of the four targets: perceived arousal, per-"
        },
        {
          "Low\n0.4544 ± 0.0229": "",
          "0.3374 ± 0.0141\n0.5049 ± 0.0222\n0.3794 ± 0.0467": "ceived valence,\nfelt arousal, and felt valence."
        },
        {
          "Low\n0.4544 ± 0.0229": "These metrics form a comprehensive feature set (see",
          "0.3374 ± 0.0141\n0.5049 ± 0.0222\n0.3794 ± 0.0467": ""
        },
        {
          "Low\n0.4544 ± 0.0229": "Table 4), and Figure 7 illustrates an example of the",
          "0.3374 ± 0.0141\n0.5049 ± 0.0222\n0.3794 ± 0.0467": ""
        },
        {
          "Low\n0.4544 ± 0.0229": "",
          "0.3374 ± 0.0141\n0.5049 ± 0.0222\n0.3794 ± 0.0467": "6.2.1\nExperimental Setup"
        },
        {
          "Low\n0.4544 ± 0.0229": "GSR signal decomposition.",
          "0.3374 ± 0.0141\n0.5049 ± 0.0222\n0.3794 ± 0.0467": ""
        },
        {
          "Low\n0.4544 ± 0.0229": "",
          "0.3374 ± 0.0141\n0.5049 ± 0.0222\n0.3794 ± 0.0467": "Continuous ratings for each target were discretised"
        },
        {
          "Low\n0.4544 ± 0.0229": "",
          "0.3374 ± 0.0141\n0.5049 ± 0.0222\n0.3794 ± 0.0467": "into the three classes as described in Section 3. Af-"
        },
        {
          "Low\n0.4544 ± 0.0229": "",
          "0.3374 ± 0.0141\n0.5049 ± 0.0222\n0.3794 ± 0.0467": "ter removing missing values and applying this map-"
        },
        {
          "Low\n0.4544 ± 0.0229": "",
          "0.3374 ± 0.0141\n0.5049 ± 0.0222\n0.3794 ± 0.0467": "ping, the extracted SCR metrics were used as input"
        },
        {
          "Low\n0.4544 ± 0.0229": "",
          "0.3374 ± 0.0141\n0.5049 ± 0.0222\n0.3794 ± 0.0467": "features. We applied an 80/20 train-test split with"
        },
        {
          "Low\n0.4544 ± 0.0229": "",
          "0.3374 ± 0.0141\n0.5049 ± 0.0222\n0.3794 ± 0.0467": "stratification and trained a LGBMClassifier with"
        },
        {
          "Low\n0.4544 ± 0.0229": "",
          "0.3374 ± 0.0141\n0.5049 ± 0.0222\n0.3794 ± 0.0467": "class weights to address the imbalance. Model per-"
        },
        {
          "Low\n0.4544 ± 0.0229": "",
          "0.3374 ± 0.0141\n0.5049 ± 0.0222\n0.3794 ± 0.0467": "formance was evaluated using precision, recall, and"
        },
        {
          "Low\n0.4544 ± 0.0229": "",
          "0.3374 ± 0.0141\n0.5049 ± 0.0222\n0.3794 ± 0.0467": "F1-scores, averaged over 5-fold cross-validation."
        },
        {
          "Low\n0.4544 ± 0.0229": "",
          "0.3374 ± 0.0141\n0.5049 ± 0.0222\n0.3794 ± 0.0467": "6.2.2\nResults Using Only GSR Peak Met-"
        },
        {
          "Low\n0.4544 ± 0.0229": "",
          "0.3374 ± 0.0141\n0.5049 ± 0.0222\n0.3794 ± 0.0467": "rics"
        },
        {
          "Low\n0.4544 ± 0.0229": "Figure 7: Decomposition of GSR signals into phasic",
          "0.3374 ± 0.0141\n0.5049 ± 0.0222\n0.3794 ± 0.0467": ""
        },
        {
          "Low\n0.4544 ± 0.0229": "",
          "0.3374 ± 0.0141\n0.5049 ± 0.0222\n0.3794 ± 0.0467": "Table\n5\npresents\nthe\nclassification\nperformance"
        },
        {
          "Low\n0.4544 ± 0.0229": "and tonic components.",
          "0.3374 ± 0.0141\n0.5049 ± 0.0222\n0.3794 ± 0.0467": ""
        },
        {
          "Low\n0.4544 ± 0.0229": "",
          "0.3374 ± 0.0141\n0.5049 ± 0.0222\n0.3794 ± 0.0467": "(mean ± standard deviation)\nfor each target. Al-"
        },
        {
          "Low\n0.4544 ± 0.0229": "",
          "0.3374 ± 0.0141\n0.5049 ± 0.0222\n0.3794 ± 0.0467": "though the F1 scores\nremain modest,\nthe perfor-"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 7: presents the range of possible scores for",
      "data": [
        {
          "mance for targets such as perceived arousal and felt": "valence is slightly higher compared to the others.",
          "7.2\nResults": ""
        },
        {
          "mance for targets such as perceived arousal and felt": "",
          "7.2\nResults": "Table 6 presents the classification performance (F1"
        },
        {
          "mance for targets such as perceived arousal and felt": "",
          "7.2\nResults": "scores) obtained using facial action unit\nfeatures"
        },
        {
          "mance for targets such as perceived arousal and felt": "6.2.3\nSummary",
          "7.2\nResults": ""
        },
        {
          "mance for targets such as perceived arousal and felt": "",
          "7.2\nResults": "for predicting four target emotional variables: per-"
        },
        {
          "mance for targets such as perceived arousal and felt": "",
          "7.2\nResults": "ceived arousal, perceived valence,\nfelt arousal, and"
        },
        {
          "mance for targets such as perceived arousal and felt": "GSR-derived\nfeatures\ndemonstrate\nsensitivity",
          "7.2\nResults": ""
        },
        {
          "mance for targets such as perceived arousal and felt": "",
          "7.2\nResults": "felt valence. The results, computed via 5-fold cross-"
        },
        {
          "mance for targets such as perceived arousal and felt": "emotional arousal, yet the complexity and subjec-",
          "7.2\nResults": ""
        },
        {
          "mance for targets such as perceived arousal and felt": "",
          "7.2\nResults": "validation,\nindicate that\nfacial\nfeatures contribute"
        },
        {
          "mance for targets such as perceived arousal and felt": "tivity inherent in self-reported affect result in mod-",
          "7.2\nResults": ""
        },
        {
          "mance for targets such as perceived arousal and felt": "",
          "7.2\nResults": "valuable\ninformation for\nemotion recognition,\nal-"
        },
        {
          "mance for targets such as perceived arousal and felt": "est classification performance when using GSR data",
          "7.2\nResults": ""
        },
        {
          "mance for targets such as perceived arousal and felt": "",
          "7.2\nResults": "though performance varies across targets. Notably,"
        },
        {
          "mance for targets such as perceived arousal and felt": "alone. Future work may improve predictive accu-",
          "7.2\nResults": ""
        },
        {
          "mance for targets such as perceived arousal and felt": "",
          "7.2\nResults": "the best model for predicting felt valence (Random-"
        },
        {
          "mance for targets such as perceived arousal and felt": "racy by integrating\nadditional modalities\nor",
          "7.2\nResults": ""
        },
        {
          "mance for targets such as perceived arousal and felt": "",
          "7.2\nResults": "Forest) achieved relatively high F1 scores compared"
        },
        {
          "mance for targets such as perceived arousal and felt": "ploying more advanced modelling techniques.",
          "7.2\nResults": ""
        },
        {
          "mance for targets such as perceived arousal and felt": "",
          "7.2\nResults": "to the other targets."
        },
        {
          "mance for targets such as perceived arousal and felt": "",
          "7.2\nResults": "These\nresults\nsuggest\nthat\nfacial\nvideo\ndata,"
        },
        {
          "mance for targets such as perceived arousal and felt": "7\nFacial Videos Analysis",
          "7.2\nResults": "when analysed via action unit\nfeatures, provides"
        },
        {
          "mance for targets such as perceived arousal and felt": "",
          "7.2\nResults": "meaningful cues for emotion recognition.\nIntegrat-"
        },
        {
          "mance for targets such as perceived arousal and felt": "In\naddition\nto\nphysiological\nand",
          "7.2\nResults": "ing these features with other modalities may fur-"
        },
        {
          "mance for targets such as perceived arousal and felt": "data, we examined participants’",
          "7.2\nResults": "ther enhance performance."
        },
        {
          "mance for targets such as perceived arousal and felt": "recorded during the experiment. Our analysis",
          "7.2\nResults": ""
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 7: presents the range of possible scores for",
      "data": [
        {
          "To investigate\nthe\nrelationship between facial\nex-": ""
        },
        {
          "To investigate\nthe\nrelationship between facial\nex-": "pressions and emotional processing, we defined the"
        },
        {
          "To investigate\nthe\nrelationship between facial\nex-": ""
        },
        {
          "To investigate\nthe\nrelationship between facial\nex-": "following measures:"
        },
        {
          "To investigate\nthe\nrelationship between facial\nex-": ""
        },
        {
          "To investigate\nthe\nrelationship between facial\nex-": ""
        },
        {
          "To investigate\nthe\nrelationship between facial\nex-": "• Sympathy: The Euclidean distance between"
        },
        {
          "To investigate\nthe\nrelationship between facial\nex-": ""
        },
        {
          "To investigate\nthe\nrelationship between facial\nex-": "perceived and felt emotion ratings\nin the va-"
        },
        {
          "To investigate\nthe\nrelationship between facial\nex-": ""
        },
        {
          "To investigate\nthe\nrelationship between facial\nex-": "lence–arousal\nspace.\nLower\nsympathy values"
        },
        {
          "To investigate\nthe\nrelationship between facial\nex-": "indicate\ncloser\nalignment between the\ninter-"
        },
        {
          "To investigate\nthe\nrelationship between facial\nex-": "nal\n(felt) and external\n(perceived)\nemotional"
        },
        {
          "To investigate\nthe\nrelationship between facial\nex-": "states."
        },
        {
          "To investigate\nthe\nrelationship between facial\nex-": ""
        },
        {
          "To investigate\nthe\nrelationship between facial\nex-": "• Facial Mimicry Similarity:\nThe\naverage"
        },
        {
          "To investigate\nthe\nrelationship between facial\nex-": "Dynamic Time Warping\n(DTW)\nscore\ncom-"
        },
        {
          "To investigate\nthe\nrelationship between facial\nex-": "puted across\nall Action Units\n(AUs) within"
        },
        {
          "To investigate\nthe\nrelationship between facial\nex-": "each trial. Higher DTW scores reflect stronger"
        },
        {
          "To investigate\nthe\nrelationship between facial\nex-": "temporal alignment between participants’\nfa-"
        },
        {
          "To investigate\nthe\nrelationship between facial\nex-": "cial\nexpressions\nand\nthose\nexhibited\nin\nthe"
        },
        {
          "To investigate\nthe\nrelationship between facial\nex-": ""
        },
        {
          "To investigate\nthe\nrelationship between facial\nex-": "stimuli."
        },
        {
          "To investigate\nthe\nrelationship between facial\nex-": ""
        },
        {
          "To investigate\nthe\nrelationship between facial\nex-": "• Emotion Recognition Performance: Eval-"
        },
        {
          "To investigate\nthe\nrelationship between facial\nex-": "uated using the Davies–Bouldin Index (DBI)"
        },
        {
          "To investigate\nthe\nrelationship between facial\nex-": "to assess the compactness and separability of"
        },
        {
          "To investigate\nthe\nrelationship between facial\nex-": "emotion clusters derived from facial\nfeatures."
        },
        {
          "To investigate\nthe\nrelationship between facial\nex-": "Lower DBI values suggest better-defined clus-"
        },
        {
          "To investigate\nthe\nrelationship between facial\nex-": "ters."
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 5: Classification Performance (F1 Score) Using GSR Peak Metrics (5-Fold Cross-Validation)",
      "data": [
        {
          "Table 5: Classification Performance (F1 Score) Using GSR Peak Metrics (5-Fold Cross-Validation)": "Perceived Arousal"
        },
        {
          "Table 5: Classification Performance (F1 Score) Using GSR Peak Metrics (5-Fold Cross-Validation)": "F1 Score (Mean ± Std)"
        },
        {
          "Table 5: Classification Performance (F1 Score) Using GSR Peak Metrics (5-Fold Cross-Validation)": "LightGBM"
        },
        {
          "Table 5: Classification Performance (F1 Score) Using GSR Peak Metrics (5-Fold Cross-Validation)": "0.3719 ± 0.0343"
        },
        {
          "Table 5: Classification Performance (F1 Score) Using GSR Peak Metrics (5-Fold Cross-Validation)": "0.4453 ± 0.0330"
        },
        {
          "Table 5: Classification Performance (F1 Score) Using GSR Peak Metrics (5-Fold Cross-Validation)": "0.3056 ± 0.0398"
        },
        {
          "Table 5: Classification Performance (F1 Score) Using GSR Peak Metrics (5-Fold Cross-Validation)": "0.3834 ± 0.0127"
        },
        {
          "Table 5: Classification Performance (F1 Score) Using GSR Peak Metrics (5-Fold Cross-Validation)": "0.3743 ± 0.0177"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 5: Classification Performance (F1 Score) Using GSR Peak Metrics (5-Fold Cross-Validation)",
      "data": [
        {
          "Table\n6:": "Validation)",
          "Classification Performance": "",
          "(F1": "",
          "Score) Using Facial Action Unit Features": "",
          "(5-Fold Cross-": ""
        },
        {
          "Table\n6:": "",
          "Classification Performance": "Perceived Arousal",
          "(F1": "",
          "Score) Using Facial Action Unit Features": "Felt Arousal",
          "(5-Fold Cross-": "Felt Valence"
        },
        {
          "Table\n6:": "Best Model",
          "Classification Performance": "LightGBMXT",
          "(F1": "",
          "Score) Using Facial Action Unit Features": "NeuralNetFastAI",
          "(5-Fold Cross-": "RandomForest"
        },
        {
          "Table\n6:": "High",
          "Classification Performance": "0.4436 ± 0.0248",
          "(F1": "",
          "Score) Using Facial Action Unit Features": "0.2344 ± 0.0362",
          "(5-Fold Cross-": "0.4516 ± 0.0594"
        },
        {
          "Table\n6:": "Medium",
          "Classification Performance": "0.2986 ± 0.0267",
          "(F1": "",
          "Score) Using Facial Action Unit Features": "0.4778 ± 0.0348",
          "(5-Fold Cross-": "0.2481 ± 0.0583"
        },
        {
          "Table\n6:": "Low",
          "Classification Performance": "0.4860 ± 0.0167",
          "(F1": "",
          "Score) Using Facial Action Unit Features": "0.6176 ± 0.0196",
          "(5-Fold Cross-": "0.6584 ± 0.0059"
        },
        {
          "Table\n6:": "Macro Avg",
          "Classification Performance": "0.4094 ± 0.0039",
          "(F1": "",
          "Score) Using Facial Action Unit Features": "0.4433 ± 0.0213",
          "(5-Fold Cross-": "0.4527 ± 0.0313"
        },
        {
          "Table\n6:": "Accuracy",
          "Classification Performance": "0.4229 ± 0.0072",
          "(F1": "",
          "Score) Using Facial Action Unit Features": "0.5157 ± 0.0211",
          "(5-Fold Cross-": "0.5340 ± 0.0115"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 5: Classification Performance (F1 Score) Using GSR Peak Metrics (5-Fold Cross-Validation)",
      "data": [
        {
          "Table 7: Measurement Scale": "Emotional Responses",
          "for Personality and": "",
          "Table 8: Shapiro–Wilk Normality Test Results": ""
        },
        {
          "Table 7: Measurement Scale": "",
          "for Personality and": "",
          "Table 8: Shapiro–Wilk Normality Test Results": "W Statistic"
        },
        {
          "Table 7: Measurement Scale": "Variable",
          "for Personality and": "Max",
          "Table 8: Shapiro–Wilk Normality Test Results": ""
        },
        {
          "Table 7: Measurement Scale": "",
          "for Personality and": "",
          "Table 8: Shapiro–Wilk Normality Test Results": "0.9772"
        },
        {
          "Table 7: Measurement Scale": "Openness",
          "for Personality and": "49.0",
          "Table 8: Shapiro–Wilk Normality Test Results": "0.9795"
        },
        {
          "Table 7: Measurement Scale": "Conscientiousness",
          "for Personality and": "42.0",
          "Table 8: Shapiro–Wilk Normality Test Results": "0.9702"
        },
        {
          "Table 7: Measurement Scale": "Extraversion",
          "for Personality and": "38.0",
          "Table 8: Shapiro–Wilk Normality Test Results": "0.9736"
        },
        {
          "Table 7: Measurement Scale": "Agreeableness",
          "for Personality and": "45.0",
          "Table 8: Shapiro–Wilk Normality Test Results": "0.9778"
        },
        {
          "Table 7: Measurement Scale": "Neuroticism",
          "for Personality and": "35.0",
          "Table 8: Shapiro–Wilk Normality Test Results": "0.9538"
        },
        {
          "Table 7: Measurement Scale": "Perceived Arousal",
          "for Personality and": "9",
          "Table 8: Shapiro–Wilk Normality Test Results": "0.9203"
        },
        {
          "Table 7: Measurement Scale": "Perceived Valence",
          "for Personality and": "9",
          "Table 8: Shapiro–Wilk Normality Test Results": "0.9617"
        },
        {
          "Table 7: Measurement Scale": "Felt Arousal",
          "for Personality and": "9",
          "Table 8: Shapiro–Wilk Normality Test Results": "0.9583"
        },
        {
          "Table 7: Measurement Scale": "Felt Valence",
          "for Personality and": "9",
          "Table 8: Shapiro–Wilk Normality Test Results": ""
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 5: Classification Performance (F1 Score) Using GSR Peak Metrics (5-Fold Cross-Validation)",
      "data": [
        {
          "Perceived Valence\n1\n9": "Felt Arousal\n1\n9",
          "Felt Arousal": "Felt Valence",
          "0.9617\n1.30 × 10−36": "0.9583\n7.02 × 10−38"
        },
        {
          "Perceived Valence\n1\n9": "Felt Valence\n1\n9",
          "Felt Arousal": "",
          "0.9617\n1.30 × 10−36": ""
        },
        {
          "Perceived Valence\n1\n9": "",
          "Felt Arousal": "8.3",
          "0.9617\n1.30 × 10−36": "Personality-Emotion\nCorrela-"
        },
        {
          "Perceived Valence\n1\n9": "",
          "Felt Arousal": "",
          "0.9617\n1.30 × 10−36": "tions"
        },
        {
          "Perceived Valence\n1\n9": "",
          "Felt Arousal": "",
          "0.9617\n1.30 × 10−36": "Table 9 provides the complete set of pairwise Pear-"
        },
        {
          "Perceived Valence\n1\n9": "",
          "Felt Arousal": "",
          "0.9617\n1.30 × 10−36": "son correlations between each personality trait and"
        },
        {
          "Perceived Valence\n1\n9": "",
          "Felt Arousal": "the\nfour",
          "0.9617\n1.30 × 10−36": "emotional\nresponse\nvariables\n(perceived"
        },
        {
          "Perceived Valence\n1\n9": "",
          "Felt Arousal": "arousal,",
          "0.9617\n1.30 × 10−36": "perceived\nvalence,\nfelt\narousal,\nfelt\nva-"
        },
        {
          "Perceived Valence\n1\n9": "",
          "Felt Arousal": "lence).",
          "0.9617\n1.30 × 10−36": "Although many correlations are\nsmall,\na"
        },
        {
          "Perceived Valence\n1\n9": "depending on subsequent checks) to assess relation-",
          "Felt Arousal": "subset",
          "0.9617\n1.30 × 10−36": "stand out as both significant and theoreti-"
        },
        {
          "Perceived Valence\n1\n9": "ships.",
          "Felt Arousal": "cally consistent with prior literature.",
          "0.9617\n1.30 × 10−36": ""
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 9: Full Personality–Emotion Pearson Corre- Table 10: Key Personality–Emotion Correlations",
      "data": [
        {
          "Table 9: Full Personality–Emotion Pearson Corre-": "lations",
          "Table 10: Key Personality–Emotion Correlations": "(Selected from Table 9)"
        },
        {
          "Table 9: Full Personality–Emotion Pearson Corre-": "Personality",
          "Table 10: Key Personality–Emotion Correlations": "Personality\nEmotion\nr\np-value"
        },
        {
          "Table 9: Full Personality–Emotion Pearson Corre-": "Openness",
          "Table 10: Key Personality–Emotion Correlations": "3.80 × 10−11\nFelt Arousal\n0.138\n< 10−25"
        },
        {
          "Table 9: Full Personality–Emotion Pearson Corre-": "Openness",
          "Table 10: Key Personality–Emotion Correlations": "Extraversion\nFelt Arousal\n0.122\n< 10−20"
        },
        {
          "Table 9: Full Personality–Emotion Pearson Corre-": "Openness",
          "Table 10: Key Personality–Emotion Correlations": "3.68 × 10−26\nConscientiousness\nFelt Valence\n0.060\n< 10−5"
        },
        {
          "Table 9: Full Personality–Emotion Pearson Corre-": "Openness",
          "Table 10: Key Personality–Emotion Correlations": "Agreeableness\nFelt Valence\n0.083\n< 10−9"
        },
        {
          "Table 9: Full Personality–Emotion Pearson Corre-": "Conscientiousness",
          "Table 10: Key Personality–Emotion Correlations": "Neuroticism\nFelt Arousal\n-0.068\n< 10−6"
        },
        {
          "Table 9: Full Personality–Emotion Pearson Corre-": "Conscientiousness",
          "Table 10: Key Personality–Emotion Correlations": "Neuroticism\nFelt Valence\n-0.088\n< 10−10"
        },
        {
          "Table 9: Full Personality–Emotion Pearson Corre-": "Conscientiousness",
          "Table 10: Key Personality–Emotion Correlations": "0.418"
        },
        {
          "Table 9: Full Personality–Emotion Pearson Corre-": "Conscientiousness",
          "Table 10: Key Personality–Emotion Correlations": "4.05 × 10−6"
        },
        {
          "Table 9: Full Personality–Emotion Pearson Corre-": "Extraversion",
          "Table 10: Key Personality–Emotion Correlations": "0.277"
        },
        {
          "Table 9: Full Personality–Emotion Pearson Corre-": "Extraversion",
          "Table 10: Key Personality–Emotion Correlations": "8.5\nImplications\nand Future Direc-"
        },
        {
          "Table 9: Full Personality–Emotion Pearson Corre-": "Extraversion",
          "Table 10: Key Personality–Emotion Correlations": "1.04 × 10−20"
        },
        {
          "Table 9: Full Personality–Emotion Pearson Corre-": "",
          "Table 10: Key Personality–Emotion Correlations": "tions"
        },
        {
          "Table 9: Full Personality–Emotion Pearson Corre-": "Extraversion",
          "Table 10: Key Personality–Emotion Correlations": "0.904"
        },
        {
          "Table 9: Full Personality–Emotion Pearson Corre-": "Agreeableness",
          "Table 10: Key Personality–Emotion Correlations": "0.220\nThese\npersonality–emotion\ncorrelations\ndemon-"
        },
        {
          "Table 9: Full Personality–Emotion Pearson Corre-": "Agreeableness",
          "Table 10: Key Personality–Emotion Correlations": "0.947\nstrate\ntrait-level\ndifferences\nsystematically"
        },
        {
          "Table 9: Full Personality–Emotion Pearson Corre-": "Agreeableness",
          "Table 10: Key Personality–Emotion Correlations": "1.94 × 10−12\nmodulate affective responses.\nIn practice,\nintegrat-"
        },
        {
          "Table 9: Full Personality–Emotion Pearson Corre-": "Agreeableness",
          "Table 10: Key Personality–Emotion Correlations": "2.97 × 10−10\ning personality measures\ninto affective computing"
        },
        {
          "Table 9: Full Personality–Emotion Pearson Corre-": "Neuroticism",
          "Table 10: Key Personality–Emotion Correlations": "0.0087\npipelines\ncan enable more\nadaptive, user-specific"
        },
        {
          "Table 9: Full Personality–Emotion Pearson Corre-": "Neuroticism",
          "Table 10: Key Personality–Emotion Correlations": "emotion recognition systems. For example, highly"
        },
        {
          "Table 9: Full Personality–Emotion Pearson Corre-": "Neuroticism",
          "Table 10: Key Personality–Emotion Correlations": "2.34 × 10−7\nconscientious and agreeable users may exhibit more"
        },
        {
          "Table 9: Full Personality–Emotion Pearson Corre-": "Neuroticism",
          "Table 10: Key Personality–Emotion Correlations": "1.61 × 10−11\npositive valence overall, whereas individuals high in"
        },
        {
          "Table 9: Full Personality–Emotion Pearson Corre-": "",
          "Table 10: Key Personality–Emotion Correlations": "neuroticism may require special modeling to handle"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table 9: Full Personality–Emotion Pearson Corre- Table 10: Key Personality–Emotion Correlations",
      "data": [
        {
          "Although Table 9 enumerates all of\nthe pairwise": "correlations,\nseveral\nstand out\nin size and signifi-",
          "Big Five trait\n(e.g., anxiety vs. depression within": "neuroticism) or adopt mixed-effects models that ac-"
        },
        {
          "Although Table 9 enumerates all of\nthe pairwise": "cance:",
          "Big Five trait\n(e.g., anxiety vs. depression within": "count\nfor repeated measures.\nIn conjunction with"
        },
        {
          "Although Table 9 enumerates all of\nthe pairwise": "",
          "Big Five trait\n(e.g., anxiety vs. depression within": "the multimodal signals of AFFEC, these personal-"
        },
        {
          "Although Table 9 enumerates all of\nthe pairwise": "• Openness\nis\npositively\ncorrelated with\nfelt",
          "Big Five trait\n(e.g., anxiety vs. depression within": ""
        },
        {
          "Although Table 9 enumerates all of\nthe pairwise": "",
          "Big Five trait\n(e.g., anxiety vs. depression within": "ity insights can help researchers build robust, per-"
        },
        {
          "Although Table 9 enumerates all of\nthe pairwise": "arousal\n(r ≈ 0.138, p < 10−25),\nsuggesting",
          "Big Five trait\n(e.g., anxiety vs. depression within": ""
        },
        {
          "Although Table 9 enumerates all of\nthe pairwise": "",
          "Big Five trait\n(e.g., anxiety vs. depression within": "sonalized affective\ncomputing models\nthat\nreflect"
        },
        {
          "Although Table 9 enumerates all of\nthe pairwise": "that individuals high in openness tend to expe-",
          "Big Five trait\n(e.g., anxiety vs. depression within": ""
        },
        {
          "Although Table 9 enumerates all of\nthe pairwise": "",
          "Big Five trait\n(e.g., anxiety vs. depression within": "the\ninterplay of\nstable\ntraits\nand transient\nemo-"
        },
        {
          "Although Table 9 enumerates all of\nthe pairwise": "rience more intense internal emotional states.",
          "Big Five trait\n(e.g., anxiety vs. depression within": ""
        },
        {
          "Although Table 9 enumerates all of\nthe pairwise": "",
          "Big Five trait\n(e.g., anxiety vs. depression within": "tional states."
        },
        {
          "Although Table 9 enumerates all of\nthe pairwise": "• Extraversion also shows a positive associa-",
          "Big Five trait\n(e.g., anxiety vs. depression within": ""
        },
        {
          "Although Table 9 enumerates all of\nthe pairwise": "tion with felt arousal\n(r ≈ 0.122, p < 10−20).",
          "Big Five trait\n(e.g., anxiety vs. depression within": ""
        },
        {
          "Although Table 9 enumerates all of\nthe pairwise": "• Conscientiousness\nand\nAgreeableness",
          "Big Five trait\n(e.g., anxiety vs. depression within": "9\nMultimodal Analysis"
        },
        {
          "Although Table 9 enumerates all of\nthe pairwise": "show modest\npositive\ncorrelations with\nfelt",
          "Big Five trait\n(e.g., anxiety vs. depression within": ""
        },
        {
          "Although Table 9 enumerates all of\nthe pairwise": "valence,\nsuggesting a slight\ntilt\ntoward more",
          "Big Five trait\n(e.g., anxiety vs. depression within": ""
        },
        {
          "Although Table 9 enumerates all of\nthe pairwise": "",
          "Big Five trait\n(e.g., anxiety vs. depression within": "In this section, we evaluate the performance of emo-"
        },
        {
          "Although Table 9 enumerates all of\nthe pairwise": "positive emotional states in higher scorers.",
          "Big Five trait\n(e.g., anxiety vs. depression within": ""
        },
        {
          "Although Table 9 enumerates all of\nthe pairwise": "",
          "Big Five trait\n(e.g., anxiety vs. depression within": "tion classification for baseline models using multi-"
        },
        {
          "Although Table 9 enumerates all of\nthe pairwise": "",
          "Big Five trait\n(e.g., anxiety vs. depression within": "modal\nfeatures. Our approach fuses\nsignals\nfrom"
        },
        {
          "Although Table 9 enumerates all of\nthe pairwise": "• Neuroticism is\nnegatively\ncorrelated with",
          "Big Five trait\n(e.g., anxiety vs. depression within": ""
        },
        {
          "Although Table 9 enumerates all of\nthe pairwise": "",
          "Big Five trait\n(e.g., anxiety vs. depression within": "eye-tracking, facial action units (extracted from fa-"
        },
        {
          "Although Table 9 enumerates all of\nthe pairwise": "both felt arousal\nand felt\nvalence,\nconsistent",
          "Big Five trait\n(e.g., anxiety vs. depression within": ""
        },
        {
          "Although Table 9 enumerates all of\nthe pairwise": "",
          "Big Five trait\n(e.g., anxiety vs. depression within": "cial videos), and GSR. We further examine the ef-"
        },
        {
          "Although Table 9 enumerates all of\nthe pairwise": "with a propensity for distress and more nega-",
          "Big Five trait\n(e.g., anxiety vs. depression within": ""
        },
        {
          "Although Table 9 enumerates all of\nthe pairwise": "",
          "Big Five trait\n(e.g., anxiety vs. depression within": "fect of incorporating personality characteristics into"
        },
        {
          "Although Table 9 enumerates all of\nthe pairwise": "tive affect.",
          "Big Five trait\n(e.g., anxiety vs. depression within": ""
        },
        {
          "Although Table 9 enumerates all of\nthe pairwise": "",
          "Big Five trait\n(e.g., anxiety vs. depression within": "the multimodal\nrepresentation.\nNote\nthat\nthese"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table 9: 2 Results with Personality Fea- individual differences in affective experiences.",
      "data": [
        {
          "9.1\nResults with Multimodal\nFea-": "",
          "arousal but struggles more with high valence. This": "suggests\nthat\ngaze patterns\nand pupil dynamics"
        },
        {
          "9.1\nResults with Multimodal\nFea-": "tures (Eye, Facial Action Units,",
          "arousal but struggles more with high valence. This": ""
        },
        {
          "9.1\nResults with Multimodal\nFea-": "",
          "arousal but struggles more with high valence. This": "capture aspects of emotional\nintensity but may not"
        },
        {
          "9.1\nResults with Multimodal\nFea-": "GSR)",
          "arousal but struggles more with high valence. This": ""
        },
        {
          "9.1\nResults with Multimodal\nFea-": "",
          "arousal but struggles more with high valence. This": "fully account for an emotion’s positivity or negativ-"
        },
        {
          "9.1\nResults with Multimodal\nFea-": "Table 6 summarises the 5-fold cross-validation per-",
          "arousal but struggles more with high valence. This": "ity.\nIn contrast, EEG shows\nrobust performance"
        },
        {
          "9.1\nResults with Multimodal\nFea-": "formance when combining eye-tracking,\nfacial ac-",
          "arousal but struggles more with high valence. This": "across\ntargets,\nespecially for\nfelt arousal, despite"
        },
        {
          "9.1\nResults with Multimodal\nFea-": "tion unit,\nand GSR features.\nThe\nreported F1",
          "arousal but struggles more with high valence. This": "minimal artefact\nremoval and no hyperparameter"
        },
        {
          "9.1\nResults with Multimodal\nFea-": "scores\n(along with standard deviations)\nare pro-",
          "arousal but struggles more with high valence. This": "tuning; this underscores the rich signal captured by"
        },
        {
          "9.1\nResults with Multimodal\nFea-": "vided for each emotion target.",
          "arousal but struggles more with high valence. This": "neural measures. GSR data demonstrate moder-"
        },
        {
          "9.1\nResults with Multimodal\nFea-": "",
          "arousal but struggles more with high valence. This": "ate predictive capacity, aligning well with arousal-"
        },
        {
          "9.1\nResults with Multimodal\nFea-": "• Perceived Arousal: The best model (Light-",
          "arousal but struggles more with high valence. This": ""
        },
        {
          "9.1\nResults with Multimodal\nFea-": "",
          "arousal but struggles more with high valence. This": "related states but showing limited effectiveness for"
        },
        {
          "9.1\nResults with Multimodal\nFea-": "GBM) achieves a macro-average F1 score of",
          "arousal but struggles more with high valence. This": ""
        },
        {
          "9.1\nResults with Multimodal\nFea-": "",
          "arousal but struggles more with high valence. This": "valence on their own.\nFacial action units derived"
        },
        {
          "9.1\nResults with Multimodal\nFea-": "0.4384 ± 0.0113, with\nindividual\nclass\nper-",
          "arousal but struggles more with high valence. This": ""
        },
        {
          "9.1\nResults with Multimodal\nFea-": "",
          "arousal but struggles more with high valence. This": "from video recordings offer meaningful cues for dis-"
        },
        {
          "9.1\nResults with Multimodal\nFea-": "formance\nranging\nfrom 0.3408\n(medium)\nto",
          "arousal but struggles more with high valence. This": ""
        },
        {
          "9.1\nResults with Multimodal\nFea-": "",
          "arousal but struggles more with high valence. This": "tinguishing different emotional states, although ac-"
        },
        {
          "9.1\nResults with Multimodal\nFea-": "0.5183 (low).",
          "arousal but struggles more with high valence. This": ""
        },
        {
          "9.1\nResults with Multimodal\nFea-": "",
          "arousal but struggles more with high valence. This": "curacy again tends to be higher for arousal."
        },
        {
          "9.1\nResults with Multimodal\nFea-": "• Perceived Valence: The best model\n(Neu-",
          "arousal but struggles more with high valence. This": "When combining eye-tracking, GSR, and fa-"
        },
        {
          "9.1\nResults with Multimodal\nFea-": "ralNetFastAI)\nreaches\na macro-average\nF1",
          "arousal but struggles more with high valence. This": "cial modalities, we observe\ncomplementary gains"
        },
        {
          "9.1\nResults with Multimodal\nFea-": "score of 0.4230 ± 0.0150.",
          "arousal but struggles more with high valence. This": "across all emotional targets. The macro-average F1"
        },
        {
          "9.1\nResults with Multimodal\nFea-": "",
          "arousal but struggles more with high valence. This": "scores in the multimodal setting range from 0.43 to"
        },
        {
          "9.1\nResults with Multimodal\nFea-": "• Felt Arousal: With XGBoost\nas\nthe best",
          "arousal but struggles more with high valence. This": ""
        },
        {
          "9.1\nResults with Multimodal\nFea-": "",
          "arousal but struggles more with high valence. This": "0.46, confirming that data fusion enhances\nrecog-"
        },
        {
          "9.1\nResults with Multimodal\nFea-": "model,\nthe macro-average F1 score\nis 0.4619",
          "arousal but struggles more with high valence. This": ""
        },
        {
          "9.1\nResults with Multimodal\nFea-": "",
          "arousal but struggles more with high valence. This": "nition and suggesting that each sensor taps unique"
        },
        {
          "9.1\nResults with Multimodal\nFea-": "± 0.0119.",
          "arousal but struggles more with high valence. This": ""
        },
        {
          "9.1\nResults with Multimodal\nFea-": "",
          "arousal but struggles more with high valence. This": "aspects of emotional expression. Furthermore,\nin-"
        },
        {
          "9.1\nResults with Multimodal\nFea-": "• Felt Valence:\nUsing NeuralNetTorch,\nthe",
          "arousal but struggles more with high valence. This": "troducing personality traits yields modest but"
        },
        {
          "9.1\nResults with Multimodal\nFea-": "macro-average F1 score is 0.4527 ± 0.0159.",
          "arousal but struggles more with high valence. This": "consistent improvements, particularly for felt emo-"
        },
        {
          "9.1\nResults with Multimodal\nFea-": "",
          "arousal but struggles more with high valence. This": "tions, a finding that underscores the role of stable"
        },
        {
          "9.1\nResults with Multimodal\nFea-": "",
          "arousal but struggles more with high valence. This": "individual differences in affective experiences."
        },
        {
          "9.1\nResults with Multimodal\nFea-": "9.2\nResults\nwith\nPersonality\nFea-",
          "arousal but struggles more with high valence. This": ""
        },
        {
          "9.1\nResults with Multimodal\nFea-": "",
          "arousal but struggles more with high valence. This": "Because these results are derived from baseline"
        },
        {
          "9.1\nResults with Multimodal\nFea-": "tures",
          "arousal but struggles more with high valence. This": ""
        },
        {
          "9.1\nResults with Multimodal\nFea-": "",
          "arousal but struggles more with high valence. This": "methods with simple feature extraction and no ex-"
        },
        {
          "9.1\nResults with Multimodal\nFea-": "Table\n?? presents\nthe\nclassification performance",
          "arousal but struggles more with high valence. This": "tensive optimisation,\nthey serve primarily as\nevi-"
        },
        {
          "9.1\nResults with Multimodal\nFea-": "when personality features are integrated with the",
          "arousal but struggles more with high valence. This": "dence that the AFFEC data set contains discrimi-"
        },
        {
          "9.1\nResults with Multimodal\nFea-": "multimodal\ndata\n(eye,\nfacial\naction\nunits,\nand",
          "arousal but struggles more with high valence. This": "native signals for emotion recognition. Future work"
        },
        {
          "9.1\nResults with Multimodal\nFea-": "GSR). Comparing these results with those in Sec-",
          "arousal but struggles more with high valence. This": "can build upon these findings by pursuing more"
        },
        {
          "9.1\nResults with Multimodal\nFea-": "tion 9, we observe that:",
          "arousal but struggles more with high valence. This": "advanced feature engineering, refined artefact han-"
        },
        {
          "9.1\nResults with Multimodal\nFea-": "",
          "arousal but struggles more with high valence. This": "dling (especially for EEG and GSR), and sophisti-"
        },
        {
          "9.1\nResults with Multimodal\nFea-": "• For Perceived Arousal and Perceived Va-",
          "arousal but struggles more with high valence. This": ""
        },
        {
          "9.1\nResults with Multimodal\nFea-": "",
          "arousal but struggles more with high valence. This": "cated fusion strategies\n(e.g., attention-based deep"
        },
        {
          "9.1\nResults with Multimodal\nFea-": "lence,\nthe macroaverage F1\nscores\nremain",
          "arousal but struggles more with high valence. This": ""
        },
        {
          "9.1\nResults with Multimodal\nFea-": "",
          "arousal but struggles more with high valence. This": "learning). These refinements could further uncover"
        },
        {
          "9.1\nResults with Multimodal\nFea-": "comparable\n(0.4377 ± 0.0080\nand 0.4242 ±",
          "arousal but struggles more with high valence. This": ""
        },
        {
          "9.1\nResults with Multimodal\nFea-": "",
          "arousal but struggles more with high valence. This": "the nuanced temporal\nand cross-modal\nrelation-"
        },
        {
          "9.1\nResults with Multimodal\nFea-": "0.0152, respectively).",
          "arousal but struggles more with high valence. This": ""
        },
        {
          "9.1\nResults with Multimodal\nFea-": "",
          "arousal but struggles more with high valence. This": "ships\nthat underlie human emotional experiences."
        },
        {
          "9.1\nResults with Multimodal\nFea-": "• For Felt Arousal and Felt Valence, incorpo-",
          "arousal but struggles more with high valence. This": "Overall, the baseline models confirm AFFEC’s po-"
        },
        {
          "9.1\nResults with Multimodal\nFea-": "rating personality results\nin modest\nimprove-",
          "arousal but struggles more with high valence. This": "tential\nto drive\nresearch in multimodal\naffective"
        },
        {
          "9.1\nResults with Multimodal\nFea-": "ments,\nyielding macro-average F1\nscores\nof",
          "arousal but struggles more with high valence. This": "computing, while leaving ample room for improved"
        },
        {
          "9.1\nResults with Multimodal\nFea-": "0.4778 ± 0.0142 and 0.4600 ± 0.0194, respec-",
          "arousal but struggles more with high valence. This": "approaches and deeper insights."
        },
        {
          "9.1\nResults with Multimodal\nFea-": "tively.",
          "arousal but struggles more with high valence. This": ""
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "splitting and evaluation protocols remained consis-": "tent.\nEach modality was divided into 60% train-",
          "10.2\nBuilding on AFFEC: Advanced": ""
        },
        {
          "splitting and evaluation protocols remained consis-": "",
          "10.2\nBuilding on AFFEC: Advanced": "Methods and Insights"
        },
        {
          "splitting and evaluation protocols remained consis-": "ing,\n20% validation,\nand\n20% testing\nsets with",
          "10.2\nBuilding on AFFEC: Advanced": ""
        },
        {
          "splitting and evaluation protocols remained consis-": "label\nstratification, and we trained baseline mod-",
          "10.2\nBuilding on AFFEC: Advanced": "Though our baselines use\nstraightforward feature"
        },
        {
          "splitting and evaluation protocols remained consis-": "els under a uniform 5-fold cross-validation regime.",
          "10.2\nBuilding on AFFEC: Advanced": "extraction and modeling,\ntwo recent\nefforts have"
        },
        {
          "splitting and evaluation protocols remained consis-": "These choices aimed to ensure fair modality com-",
          "10.2\nBuilding on AFFEC: Advanced": "adopted more\nsophisticated\napproaches\non\nthis"
        },
        {
          "splitting and evaluation protocols remained consis-": "parisons, mitigate overfitting, and provide a trans-",
          "10.2\nBuilding on AFFEC: Advanced": "dataset:"
        },
        {
          "splitting and evaluation protocols remained consis-": "parent demonstration of the dataset’s quality.",
          "10.2\nBuilding on AFFEC: Advanced": ""
        },
        {
          "splitting and evaluation protocols remained consis-": "",
          "10.2\nBuilding on AFFEC: Advanced": "• Advanced Facial Mimicry Analysis [43]:"
        },
        {
          "splitting and evaluation protocols remained consis-": "",
          "10.2\nBuilding on AFFEC: Advanced": "One study devoted effort to facial action unit"
        },
        {
          "splitting and evaluation protocols remained consis-": "10.1\nBaseline Findings and Modali-",
          "10.2\nBuilding on AFFEC: Advanced": ""
        },
        {
          "splitting and evaluation protocols remained consis-": "",
          "10.2\nBuilding on AFFEC: Advanced": "processing\nemploying dynamic\ntime warping"
        },
        {
          "splitting and evaluation protocols remained consis-": "ties",
          "10.2\nBuilding on AFFEC: Advanced": ""
        },
        {
          "splitting and evaluation protocols remained consis-": "",
          "10.2\nBuilding on AFFEC: Advanced": "(DTW)\nto align participant AUs with those"
        },
        {
          "splitting and evaluation protocols remained consis-": "",
          "10.2\nBuilding on AFFEC: Advanced": "expressed\nin\nthe\nstimuli.\nBy meticulously"
        },
        {
          "splitting and evaluation protocols remained consis-": "The primary motivation behind these baseline ex-",
          "10.2\nBuilding on AFFEC: Advanced": ""
        },
        {
          "splitting and evaluation protocols remained consis-": "",
          "10.2\nBuilding on AFFEC: Advanced": "segmenting\nand\nsynchronizing\nthe AU pat-"
        },
        {
          "splitting and evaluation protocols remained consis-": "periments was to validate that the AFFEC dataset",
          "10.2\nBuilding on AFFEC: Advanced": ""
        },
        {
          "splitting and evaluation protocols remained consis-": "",
          "10.2\nBuilding on AFFEC: Advanced": "terns,\nthe authors probed how closely the fa-"
        },
        {
          "splitting and evaluation protocols remained consis-": "contains meaningful\nsignals\nfor\nemotion recogni-",
          "10.2\nBuilding on AFFEC: Advanced": ""
        },
        {
          "splitting and evaluation protocols remained consis-": "",
          "10.2\nBuilding on AFFEC: Advanced": "cial\nexpressions\nof\nthe participants mirrored"
        },
        {
          "splitting and evaluation protocols remained consis-": "tion,\nrather\nthan to optimize model performance",
          "10.2\nBuilding on AFFEC: Advanced": ""
        },
        {
          "splitting and evaluation protocols remained consis-": "",
          "10.2\nBuilding on AFFEC: Advanced": "the video content, revealing richer insights into"
        },
        {
          "splitting and evaluation protocols remained consis-": "fully.\nIndeed, our results show:",
          "10.2\nBuilding on AFFEC: Advanced": ""
        },
        {
          "splitting and evaluation protocols remained consis-": "",
          "10.2\nBuilding on AFFEC: Advanced": "the interaction between perceived and felt re-"
        },
        {
          "splitting and evaluation protocols remained consis-": "• Eye-Tracking: Gaze position, fixation met-",
          "10.2\nBuilding on AFFEC: Advanced": "sponses.\nTheir\nresults\nindicated\nthat\nfine-"
        },
        {
          "splitting and evaluation protocols remained consis-": "rics, and pupil dynamics\nreliably capture as-",
          "10.2\nBuilding on AFFEC: Advanced": "grained temporal alignment could discriminate"
        },
        {
          "splitting and evaluation protocols remained consis-": "pects of arousal but struggle with valence pre-",
          "10.2\nBuilding on AFFEC: Advanced": "subtle mimicry differences, particularly among"
        },
        {
          "splitting and evaluation protocols remained consis-": "diction.",
          "10.2\nBuilding on AFFEC: Advanced": "emotions like fear or anger, and suggested that"
        },
        {
          "splitting and evaluation protocols remained consis-": "",
          "10.2\nBuilding on AFFEC: Advanced": "personality traits modulate the degree of align-"
        },
        {
          "splitting and evaluation protocols remained consis-": "• EEG: Even with minimal\nartefact\nremoval",
          "10.2\nBuilding on AFFEC: Advanced": "ment."
        },
        {
          "splitting and evaluation protocols remained consis-": "and no hyperparameter tuning, a simple CNN",
          "10.2\nBuilding on AFFEC: Advanced": ""
        },
        {
          "splitting and evaluation protocols remained consis-": "achieves\nperformance well\nabove\nchance\non",
          "10.2\nBuilding on AFFEC: Advanced": "• Personality\nand\nEye-Tracking\nFusion"
        },
        {
          "splitting and evaluation protocols remained consis-": "both perceived and felt emotion categories.",
          "10.2\nBuilding on AFFEC: Advanced": "[44]: Another work concentrated on neural-"
        },
        {
          "splitting and evaluation protocols remained consis-": "",
          "10.2\nBuilding on AFFEC: Advanced": "network-based pipelines\nthat\nintegrated per-"
        },
        {
          "splitting and evaluation protocols remained consis-": "• GSR: Phasic and tonic skin conductance data",
          "10.2\nBuilding on AFFEC: Advanced": ""
        },
        {
          "splitting and evaluation protocols remained consis-": "",
          "10.2\nBuilding on AFFEC: Advanced": "sonality\nassessments,\nhigh-resolution\neye-"
        },
        {
          "splitting and evaluation protocols remained consis-": "exhibit moderate\npredictive\npower,\nparticu-",
          "10.2\nBuilding on AFFEC: Advanced": ""
        },
        {
          "splitting and evaluation protocols remained consis-": "",
          "10.2\nBuilding on AFFEC: Advanced": "tracking metrics,\nand\ntemporal modelling."
        },
        {
          "splitting and evaluation protocols remained consis-": "larly for arousal.",
          "10.2\nBuilding on AFFEC: Advanced": ""
        },
        {
          "splitting and evaluation protocols remained consis-": "",
          "10.2\nBuilding on AFFEC: Advanced": "After\nextensive data preprocessing—such as"
        },
        {
          "splitting and evaluation protocols remained consis-": "",
          "10.2\nBuilding on AFFEC: Advanced": "more\nrefined\npupil-baseline\ncorrections\nand"
        },
        {
          "splitting and evaluation protocols remained consis-": "• Facial Expressions: Facial action units ex-",
          "10.2\nBuilding on AFFEC: Advanced": ""
        },
        {
          "splitting and evaluation protocols remained consis-": "",
          "10.2\nBuilding on AFFEC: Advanced": "gaze-region segmentations—the authors used"
        },
        {
          "splitting and evaluation protocols remained consis-": "tracted via OpenFace provide valuable\ncues;",
          "10.2\nBuilding on AFFEC: Advanced": ""
        },
        {
          "splitting and evaluation protocols remained consis-": "",
          "10.2\nBuilding on AFFEC: Advanced": "multi-branch architectures\nto fuse\nthese\nfea-"
        },
        {
          "splitting and evaluation protocols remained consis-": "however, accuracy varies across different emo-",
          "10.2\nBuilding on AFFEC: Advanced": ""
        },
        {
          "splitting and evaluation protocols remained consis-": "",
          "10.2\nBuilding on AFFEC: Advanced": "tures.\nNotably,\nthey reported higher macro-"
        },
        {
          "splitting and evaluation protocols remained consis-": "tion types.",
          "10.2\nBuilding on AFFEC: Advanced": ""
        },
        {
          "splitting and evaluation protocols remained consis-": "",
          "10.2\nBuilding on AFFEC: Advanced": "F1 scores than our baselines for both felt and"
        },
        {
          "splitting and evaluation protocols remained consis-": "",
          "10.2\nBuilding on AFFEC: Advanced": "perceived valence,\nindicating that deeper per-"
        },
        {
          "splitting and evaluation protocols remained consis-": "• Personality Traits:\nIncorporating the Big",
          "10.2\nBuilding on AFFEC: Advanced": ""
        },
        {
          "splitting and evaluation protocols remained consis-": "",
          "10.2\nBuilding on AFFEC: Advanced": "sonalization and advanced sequence modelling"
        },
        {
          "splitting and evaluation protocols remained consis-": "Five\nscales yields modest but\nconsistent\nim-",
          "10.2\nBuilding on AFFEC: Advanced": ""
        },
        {
          "splitting and evaluation protocols remained consis-": "",
          "10.2\nBuilding on AFFEC: Advanced": "can further enhance emotion prediction."
        },
        {
          "splitting and evaluation protocols remained consis-": "provements\nin classifying felt arousal and va-",
          "10.2\nBuilding on AFFEC: Advanced": ""
        },
        {
          "splitting and evaluation protocols remained consis-": "lence, highlighting the role of stable individual",
          "10.2\nBuilding on AFFEC: Advanced": ""
        },
        {
          "splitting and evaluation protocols remained consis-": "differences.",
          "10.2\nBuilding on AFFEC: Advanced": "Both\nendeavors\nconfirm that AFFEC can\nac-"
        },
        {
          "splitting and evaluation protocols remained consis-": "",
          "10.2\nBuilding on AFFEC: Advanced": "commodate more detailed data preprocessing steps"
        },
        {
          "splitting and evaluation protocols remained consis-": "When multiple data streams (e.g., eye-tracking,",
          "10.2\nBuilding on AFFEC: Advanced": "(e.g.,\nimproved noise filtering, advanced feature en-"
        },
        {
          "splitting and evaluation protocols remained consis-": "GSR, and facial AUs) are fused, performance im-",
          "10.2\nBuilding on AFFEC: Advanced": "gineering) and model designs (e.g., neural networks"
        },
        {
          "splitting and evaluation protocols remained consis-": "proves, underscoring the complementary nature of",
          "10.2\nBuilding on AFFEC: Advanced": "with multi-branch temporal\nfusion). They also il-"
        },
        {
          "splitting and evaluation protocols remained consis-": "these signals. Yet, valence predictions remain more",
          "10.2\nBuilding on AFFEC: Advanced": "lustrate how personalization—either via personal-"
        },
        {
          "splitting and evaluation protocols remained consis-": "challenging than arousal, matching broader trends",
          "10.2\nBuilding on AFFEC: Advanced": "ity traits or by modelling individual temporal pat-"
        },
        {
          "splitting and evaluation protocols remained consis-": "in affective computing. Overall, these results con-",
          "10.2\nBuilding on AFFEC: Advanced": "terns—can unlock deeper\ninsights\ninto the differ-"
        },
        {
          "splitting and evaluation protocols remained consis-": "firm that AFFEC provides discriminative features",
          "10.2\nBuilding on AFFEC: Advanced": "ences between internal and externally perceived af-"
        },
        {
          "splitting and evaluation protocols remained consis-": "across diverse modalities.",
          "10.2\nBuilding on AFFEC: Advanced": "fect."
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "10.3\nKey Takeaways and Future Di-": "",
          "• Arousal\nis more tractable than valence": "across most modalities, though multimodal fu-"
        },
        {
          "10.3\nKey Takeaways and Future Di-": "rections",
          "• Arousal\nis more tractable than valence": ""
        },
        {
          "10.3\nKey Takeaways and Future Di-": "",
          "• Arousal\nis more tractable than valence": "sion narrows the gap."
        },
        {
          "10.3\nKey Takeaways and Future Di-": "• AFFEC as\na Foundation:\nThe dataset’s",
          "• Arousal\nis more tractable than valence": ""
        },
        {
          "10.3\nKey Takeaways and Future Di-": "comprehensive\nsignals\nacross\nEEG,\neye-",
          "• Arousal\nis more tractable than valence": "• Eye-tracking, EEG, and GSR each offer"
        },
        {
          "10.3\nKey Takeaways and Future Di-": "tracking, GSR,\nfacial analysis, and personal-",
          "• Arousal\nis more tractable than valence": "complementary information about\nemotional"
        },
        {
          "10.3\nKey Takeaways and Future Di-": "ity questionnaires make it a valuable platform",
          "• Arousal\nis more tractable than valence": "states, and combining them yields notable per-"
        },
        {
          "10.3\nKey Takeaways and Future Di-": "for exploring more advanced or specialised ap-",
          "• Arousal\nis more tractable than valence": "formance gains."
        },
        {
          "10.3\nKey Takeaways and Future Di-": "proaches.",
          "• Arousal\nis more tractable than valence": ""
        },
        {
          "10.3\nKey Takeaways and Future Di-": "",
          "• Arousal\nis more tractable than valence": "• Personality\ntraits\ncontribute\nvaluable\nin-"
        },
        {
          "10.3\nKey Takeaways and Future Di-": "• Multimodal Fusion Potential: Our results",
          "• Arousal\nis more tractable than valence": "sight into individual differences, especially for"
        },
        {
          "10.3\nKey Takeaways and Future Di-": "and recent extensions highlight the gains from",
          "• Arousal\nis more tractable than valence": "felt emotions."
        },
        {
          "10.3\nKey Takeaways and Future Di-": "combining modalities.\nAttention-based\nar-",
          "• Arousal\nis more tractable than valence": ""
        },
        {
          "10.3\nKey Takeaways and Future Di-": "chitectures,\ntransformer models, or advanced",
          "• Arousal\nis more tractable than valence": "The\ndataset’s\ncomprehensive\nstructure, BIDS"
        },
        {
          "10.3\nKey Takeaways and Future Di-": "temporal\nalignment\ntechniques\n(e.g., DTW)",
          "• Arousal\nis more tractable than valence": "formatting, and public availability encourage broad"
        },
        {
          "10.3\nKey Takeaways and Future Di-": "may capture the sequential nature of emotional",
          "• Arousal\nis more tractable than valence": "adoption and reproducibility. Moreover,\ntwo sub-"
        },
        {
          "10.3\nKey Takeaways and Future Di-": "expression more effectively.",
          "• Arousal\nis more tractable than valence": "sequent\nstudies\nleveraging AFFEC have demon-"
        },
        {
          "10.3\nKey Takeaways and Future Di-": "",
          "• Arousal\nis more tractable than valence": "strated\nits\nflexibility:\nfrom employing\ntemporal"
        },
        {
          "10.3\nKey Takeaways and Future Di-": "• Personality-Informed Models:\nEmpirical",
          "• Arousal\nis more tractable than valence": ""
        },
        {
          "10.3\nKey Takeaways and Future Di-": "",
          "• Arousal\nis more tractable than valence": "alignment\nfor\nin-depth facial mimicry analysis\nto"
        },
        {
          "10.3\nKey Takeaways and Future Di-": "gains from personality integration suggest that",
          "• Arousal\nis more tractable than valence": ""
        },
        {
          "10.3\nKey Takeaways and Future Di-": "",
          "• Arousal\nis more tractable than valence": "integrating personality-driven gaze models for im-"
        },
        {
          "10.3\nKey Takeaways and Future Di-": "personalisation is a promising avenue. Future",
          "• Arousal\nis more tractable than valence": ""
        },
        {
          "10.3\nKey Takeaways and Future Di-": "",
          "• Arousal\nis more tractable than valence": "proved internal state prediction. We anticipate fu-"
        },
        {
          "10.3\nKey Takeaways and Future Di-": "work could refine trait-based or mixed-effects",
          "• Arousal\nis more tractable than valence": ""
        },
        {
          "10.3\nKey Takeaways and Future Di-": "",
          "• Arousal\nis more tractable than valence": "ture work will explore advanced multimodal\nfusion"
        },
        {
          "10.3\nKey Takeaways and Future Di-": "models\nto handle inter-participant variability",
          "• Arousal\nis more tractable than valence": ""
        },
        {
          "10.3\nKey Takeaways and Future Di-": "",
          "• Arousal\nis more tractable than valence": "strategies (e.g., attention mechanisms, graph-based"
        },
        {
          "10.3\nKey Takeaways and Future Di-": "more precisely.",
          "• Arousal\nis more tractable than valence": ""
        },
        {
          "10.3\nKey Takeaways and Future Di-": "",
          "• Arousal\nis more tractable than valence": "architectures) and examine\ninteractive paradigms"
        },
        {
          "10.3\nKey Takeaways and Future Di-": "",
          "• Arousal\nis more tractable than valence": "or larger demographic samples to further elucidate"
        },
        {
          "10.3\nKey Takeaways and Future Di-": "• Extended\nScenarios:\nAFFEC’s\nstructure",
          "• Arousal\nis more tractable than valence": ""
        },
        {
          "10.3\nKey Takeaways and Future Di-": "",
          "• Arousal\nis more tractable than valence": "how personality and context shape emotional com-"
        },
        {
          "10.3\nKey Takeaways and Future Di-": "also invites expansions\nsuch as\nlive,\nreal-time",
          "• Arousal\nis more tractable than valence": ""
        },
        {
          "10.3\nKey Takeaways and Future Di-": "",
          "• Arousal\nis more tractable than valence": "munication.\nOverall,\nthe AFFEC dataset\nrepre-"
        },
        {
          "10.3\nKey Takeaways and Future Di-": "systems or investigations into how contextual",
          "• Arousal\nis more tractable than valence": ""
        },
        {
          "10.3\nKey Takeaways and Future Di-": "",
          "• Arousal\nis more tractable than valence": "sents a significant step forward for research in affec-"
        },
        {
          "10.3\nKey Takeaways and Future Di-": "factors (like conversation flow) alter the map-",
          "• Arousal\nis more tractable than valence": ""
        },
        {
          "10.3\nKey Takeaways and Future Di-": "",
          "• Arousal\nis more tractable than valence": "tive computing, human-agent\ninteraction, and so-"
        },
        {
          "10.3\nKey Takeaways and Future Di-": "ping between physiological cues and subjective",
          "• Arousal\nis more tractable than valence": ""
        },
        {
          "10.3\nKey Takeaways and Future Di-": "",
          "• Arousal\nis more tractable than valence": "cial\nrobotics,\nsupporting the design of more accu-"
        },
        {
          "10.3\nKey Takeaways and Future Di-": "emotion labels.",
          "• Arousal\nis more tractable than valence": ""
        },
        {
          "10.3\nKey Takeaways and Future Di-": "",
          "• Arousal\nis more tractable than valence": "rate, adaptive, and empathetic emotion-aware tech-"
        },
        {
          "10.3\nKey Takeaways and Future Di-": "",
          "• Arousal\nis more tractable than valence": "nologies."
        },
        {
          "10.3\nKey Takeaways and Future Di-": "Ultimately,\nthese\nbaseline\nevaluations\nconfirm",
          "• Arousal\nis more tractable than valence": ""
        },
        {
          "10.3\nKey Takeaways and Future Di-": "the rich signal content of AFFEC while leaving am-",
          "• Arousal\nis more tractable than valence": ""
        },
        {
          "10.3\nKey Takeaways and Future Di-": "ple scope for innovative modelling and analysis. By",
          "• Arousal\nis more tractable than valence": ""
        },
        {
          "10.3\nKey Takeaways and Future Di-": "",
          "• Arousal\nis more tractable than valence": "Acknowledgments"
        },
        {
          "10.3\nKey Takeaways and Future Di-": "incorporating deeper\ntemporal dynamics,\nrefined",
          "• Arousal\nis more tractable than valence": ""
        },
        {
          "10.3\nKey Takeaways and Future Di-": "personalization,\nor\nsophisticated fusion methods,",
          "• Arousal\nis more tractable than valence": ""
        },
        {
          "10.3\nKey Takeaways and Future Di-": "",
          "• Arousal\nis more tractable than valence": "We would like to thank all participants and the re-"
        },
        {
          "10.3\nKey Takeaways and Future Di-": "future research can push the boundaries of emotion-",
          "• Arousal\nis more tractable than valence": ""
        },
        {
          "10.3\nKey Takeaways and Future Di-": "",
          "• Arousal\nis more tractable than valence": "search team involved in data collection and process-"
        },
        {
          "10.3\nKey Takeaways and Future Di-": "aware technology and better characterize the sub-",
          "• Arousal\nis more tractable than valence": ""
        },
        {
          "10.3\nKey Takeaways and Future Di-": "",
          "• Arousal\nis more tractable than valence": "ing. This work was\nsupported in part by Pioneer"
        },
        {
          "10.3\nKey Takeaways and Future Di-": "tleties of\nface-to-face affective communication.",
          "• Arousal\nis more tractable than valence": ""
        },
        {
          "10.3\nKey Takeaways and Future Di-": "",
          "• Arousal\nis more tractable than valence": "Centre for Artificial Intelligence∗."
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "the\nSocial\nFunction\nof\nEye\nGaze\nin",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": "Fear and empathy while reading poe’s “the pit"
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "Social\nInteraction,”\nin\n2023\nSymposium",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": "and the pendulum”,” in Negative Emotions in"
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "on\nEye\nTracking\nResearch\nand\nApplica-",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": "the Reception of Fictional Narratives.\nBrill"
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "tions.\nTubingen\nGermany:\nACM, May",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": "mentis, 2022, pp. 21–42."
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "2023,\npp.\n1–3.\n[Online]. Available:\nhttps:",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": ""
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": "[11] H. Saarimaki, L. Nummenmaa, S. Volynets,"
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "//dl.acm.org/doi/10.1145/3588015.3589513",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": ""
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": "S.\nSantavirta, A. Aksiuto, M.\nSams,\nI. P."
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "[3] G. Mohammadi and P. Vuilleumier, “A multi-",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": "Jaaskelainen, and J. Lahnakoski, “Cerebral to-"
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "componential approach to emotion recognition",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": "pographies\nof perceived and felt\nemotions,”"
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "and the effect of personality,” IEEE Transac-",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": "bioRxiv, pp. 2023–02, 2023."
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "tions on Affective Computing, vol. 13, no. 3,",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": ""
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": "[12] P. Ekman, “An argument for basic emotions,”"
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "pp. 1127–1139, 2020.",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": ""
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": "Nature: The nature of human nature, vol. 2,"
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": "p. 294, 2005."
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "[4] M.\nS.\nK.\nHosseini,\nS. M.\nFiroozabadi,",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": ""
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "K. Badie,\nand P. Azadfallah,\n“Personality-",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": ""
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": "[13] A. Mehrabian,\n“Pleasure-arousal-dominance:"
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "based emotion recognition using\neeg\nsignals",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": ""
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": "A general\nframework for describing and mea-"
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "with\na\ncnn-lstm network,” Brain\nSciences,",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": ""
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": "suring individual differences in temperament,”"
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "vol. 13, no. 6, p. 947, 2023.",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": ""
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": "Current\nPsychology,\nvol.\n14,\npp.\n261–292,"
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": "1996."
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "[5] S. K. Khare, V. Blanes-Vidal, E. S. Nadimi,",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": ""
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "and U. R. Acharya,\n“Emotion\nrecognition",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": ""
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": "[14]\nI. Siegert, R. B¨ock, B. Vlasenko, D. Philippou-"
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "and artificial intelligence: A systematic review",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": ""
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": "H¨ubner,\nand A. Wendemuth,\n“Appropriate"
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "(2014–2023) and research recommendations,”",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": ""
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": "emotional\nlabelling of non-acted speech using"
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "Information fusion, vol. 102, p. 102019, 2024.",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": ""
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": "basic emotions, geneva emotion wheel and self"
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": "assessment manikins,”\nin 2011\nIEEE Inter-"
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "[6] W. Liu,\nJ.-L. Qiu, W.-L. Zheng,\nand B.-L.",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": ""
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": "national Conference on Multimedia and Expo."
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "Lu, “Comparing recognition performance and",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": ""
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": "IEEE, 2011, pp. 1–6."
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "robustness of multimodal deep learning mod-",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": ""
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "els for multimodal emotion recognition,” IEEE",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": ""
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": "[15] D. Kollias,\nP.\nTzirakis, M.\nA.\nNicolaou,"
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "Transactions on Cognitive and Developmental",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": ""
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": "A. Papaioannou, G. Zhao, B. Schuller,\nand"
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "Systems, vol. 14, no. 2, pp. 715–729, 2022.",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": ""
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": "S.\nZafeiriou,\n“Abaw:\nValence-arousal\nesti-"
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": "mation,\nexpression\nrecognition,\naction\nunit"
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "[7] C. Y.\nPark,\nN. Cha,\nS. Kang,\nA. Kim,",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": ""
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": "detection & multi-task\nlearning\nchallenges,”"
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "A. H. Khandoker, L. Hadjileontiadis, A. Oh,",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": ""
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": "in Proceedings of\nthe IEEE/CVF Conference"
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "Y. Jeong,\nand U. Lee,\n“K-emocon,\na multi-",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": ""
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": "on Computer Vision\nand Pattern Recogni-"
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "modal\nsensor dataset\nfor continuous emotion",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": ""
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": "tion Workshops (CVPRW), 2022, pp. 10 790–"
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "recognition in naturalistic conversations,” Sci-",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": ""
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": "10 800."
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "entific Data, vol. 7, no. 1, p. 293, 2020.",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": ""
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": "[16] X. Li and Y. Chen, “Emotion recognition us-"
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "[8] H. Yang\nand\nJ.\nShen,\n“Emotion\ndynam-",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": ""
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": "ing different\nsensors,\nemotion models, meth-"
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "ics modeling\nvia\nbert,”\nin\n2021\nInterna-",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": ""
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": "ods, and datasets: A comprehensive review,”"
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "tional Joint Conference\non Neural Networks",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": ""
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": "Frontiers\nin Neuroergonomics,\nvol.\n5,\nno."
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "(IJCNN).\nIEEE, 2021, pp. 1–8.",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": ""
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": "1338243, 2024."
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "[9] H. Cao and F. Elliott, “Analysis of Eye Fixa-",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": ""
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": "[17] L. Zhang and M. Wang, “Survey of deep emo-"
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "tions During Emotion Recognition in Talking",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": ""
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": "tion recognition in dynamic data using facial,"
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "Faces,” in 2021 9th International Conference",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": ""
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": "speech, and textual\ncues,” Frontiers\nin Psy-"
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "on Affective Computing\nand\nIntelligent\nIn-",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": ""
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": "chology, vol. 14, p. 10978716, 2023."
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "teraction (ACII).\nNara, Japan:\nIEEE, Sep.",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": ""
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "2021,\npp.\n1–7.\n[Online]. Available:\nhttps:",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": "[18] S. Park and J. Lee, “Modality effects on emo-"
        },
        {
          "[2] M.\n¸ akır\nand\nA.\nHuckauf,\n“Reviewing": "//ieeexplore.ieee.org/document/9597440/",
          "[10] M. Horv´ath, “Felt versus perceived emotions:": "tion perception in english by chinese l2 english"
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "users: An eye-tracking study,” in Proceedings": "of the North American Chapter of the Associa-",
          "Transactions on Affective Computing, vol. 6,": "no. 3, pp. 209–222, 2015."
        },
        {
          "users: An eye-tracking study,” in Proceedings": "tion for Computational Linguistics (NAACL),",
          "Transactions on Affective Computing, vol. 6,": ""
        },
        {
          "users: An eye-tracking study,” in Proceedings": "",
          "Transactions on Affective Computing, vol. 6,": "[27] W.-B. Jiang, X.-H. Liu, W.-L. Zheng, and B.-"
        },
        {
          "users: An eye-tracking study,” in Proceedings": "2024.",
          "Transactions on Affective Computing, vol. 6,": ""
        },
        {
          "users: An eye-tracking study,” in Proceedings": "",
          "Transactions on Affective Computing, vol. 6,": "L. Lu, “Seed-vii: A multimodal dataset of six"
        },
        {
          "users: An eye-tracking study,” in Proceedings": "[19] R. Chen and Q. Liu, “esee-d: Emotional state",
          "Transactions on Affective Computing, vol. 6,": "basic emotions with continuous labels for emo-"
        },
        {
          "users: An eye-tracking study,” in Proceedings": "estimation\nbased\non\neye-tracking\ndataset,”",
          "Transactions on Affective Computing, vol. 6,": "tion recognition,” IEEE Transactions on Af-"
        },
        {
          "users: An eye-tracking study,” in Proceedings": "ArXiv Preprint, vol. arXiv:2403.11590, 2024.",
          "Transactions on Affective Computing, vol. 6,": "fective Computing, pp. 1–16, 2024."
        },
        {
          "users: An eye-tracking study,” in Proceedings": "",
          "Transactions on Affective Computing, vol. 6,": "[28] S. Katsigiannis\nand N. Ramzan,\n“Dreamer:"
        },
        {
          "users: An eye-tracking study,” in Proceedings": "[20] M. J. Seikavandi and M. J. Barret, “Gaze re-",
          "Transactions on Affective Computing, vol. 6,": ""
        },
        {
          "users: An eye-tracking study,” in Proceedings": "",
          "Transactions on Affective Computing, vol. 6,": "A database\nfor\nemotion recognition through"
        },
        {
          "users: An eye-tracking study,” in Proceedings": "veals emotion perception:\nInsights from mod-",
          "Transactions on Affective Computing, vol. 6,": ""
        },
        {
          "users: An eye-tracking study,” in Proceedings": "",
          "Transactions on Affective Computing, vol. 6,": "eeg and ecg signals from wireless low-cost off-"
        },
        {
          "users: An eye-tracking study,” in Proceedings": "elling naturalistic\nface viewing,” in 2023 In-",
          "Transactions on Affective Computing, vol. 6,": ""
        },
        {
          "users: An eye-tracking study,” in Proceedings": "",
          "Transactions on Affective Computing, vol. 6,": "the-shelf devices,” IEEE Journal of Biomedi-"
        },
        {
          "users: An eye-tracking study,” in Proceedings": "ternational Conference on Machine Learning",
          "Transactions on Affective Computing, vol. 6,": ""
        },
        {
          "users: An eye-tracking study,” in Proceedings": "",
          "Transactions on Affective Computing, vol. 6,": "cal and Health Informatics, vol. 22, no. 1, pp."
        },
        {
          "users: An eye-tracking study,” in Proceedings": "and Applications (ICMLA).\nIEEE, 2023, pp.",
          "Transactions on Affective Computing, vol. 6,": ""
        },
        {
          "users: An eye-tracking study,” in Proceedings": "",
          "Transactions on Affective Computing, vol. 6,": "98–107, 2018."
        },
        {
          "users: An eye-tracking study,” in Proceedings": "2022–2025.",
          "Transactions on Affective Computing, vol. 6,": ""
        },
        {
          "users: An eye-tracking study,” in Proceedings": "",
          "Transactions on Affective Computing, vol. 6,": "[29] M.-H.\nLee,\nA.\nShomanov,\nB.\nBegim,"
        },
        {
          "users: An eye-tracking study,” in Proceedings": "[21] M. J. Seikavandi, M. Barrett, and P. Burelli,",
          "Transactions on Affective Computing, vol. 6,": ""
        },
        {
          "users: An eye-tracking study,” in Proceedings": "",
          "Transactions on Affective Computing, vol. 6,": "Z. Kabidenova,\nA.\nNyssanbay,\nA.\nYazici,"
        },
        {
          "users: An eye-tracking study,” in Proceedings": "“Modeling face emotion perception from nat-",
          "Transactions on Affective Computing, vol. 6,": ""
        },
        {
          "users: An eye-tracking study,” in Proceedings": "",
          "Transactions on Affective Computing, vol. 6,": "and S.-W. Lee, “Eav: Eeg-audio-video dataset"
        },
        {
          "users: An eye-tracking study,” in Proceedings": "uralistic face viewing:\nInsights from fixational",
          "Transactions on Affective Computing, vol. 6,": ""
        },
        {
          "users: An eye-tracking study,” in Proceedings": "",
          "Transactions on Affective Computing, vol. 6,": "for\nemotion\nrecognition\nin\nconversational"
        },
        {
          "users: An eye-tracking study,” in Proceedings": "events\nand\ngaze\nstrategies,”\nin Recent Ad-",
          "Transactions on Affective Computing, vol. 6,": ""
        },
        {
          "users: An eye-tracking study,” in Proceedings": "",
          "Transactions on Affective Computing, vol. 6,": "contexts,” Scientific Data, vol. 11, 2024."
        },
        {
          "users: An eye-tracking study,” in Proceedings": "vances\nin Deep Learning Applications:\nNew",
          "Transactions on Affective Computing, vol. 6,": ""
        },
        {
          "users: An eye-tracking study,” in Proceedings": "Techniques and Practical Examples.\nTaylor",
          "Transactions on Affective Computing, vol. 6,": ""
        },
        {
          "users: An eye-tracking study,” in Proceedings": "",
          "Transactions on Affective Computing, vol. 6,": "[30] J. A. Miranda-Correa, M. K. Abadi, N. Sebe,"
        },
        {
          "users: An eye-tracking study,” in Proceedings": "& Francis, 2024.",
          "Transactions on Affective Computing, vol. 6,": ""
        },
        {
          "users: An eye-tracking study,” in Proceedings": "",
          "Transactions on Affective Computing, vol. 6,": "and I. Patras, “Amigos: A dataset for affect,"
        },
        {
          "users: An eye-tracking study,” in Proceedings": "",
          "Transactions on Affective Computing, vol. 6,": "personality and mood research on individuals"
        },
        {
          "users: An eye-tracking study,” in Proceedings": "[22] Y. Wang and S. Gao,\n“Emotion recognition",
          "Transactions on Affective Computing, vol. 6,": ""
        },
        {
          "users: An eye-tracking study,” in Proceedings": "",
          "Transactions on Affective Computing, vol. 6,": "and groups,” IEEE Transactions on Affective"
        },
        {
          "users: An eye-tracking study,” in Proceedings": "in adaptive virtual reality settings: Challenges",
          "Transactions on Affective Computing, vol. 6,": ""
        },
        {
          "users: An eye-tracking study,” in Proceedings": "",
          "Transactions on Affective Computing, vol. 6,": "Computing, vol. 12, no. 2, pp. 479–493, 2021."
        },
        {
          "users: An eye-tracking study,” in Proceedings": "and opportunities,” IEEE Transactions on Af-",
          "Transactions on Affective Computing, vol. 6,": ""
        },
        {
          "users: An eye-tracking study,” in Proceedings": "fective Computing, 2023.",
          "Transactions on Affective Computing, vol. 6,": "[31] A. Mollahosseini, B. Hasani, and M. H. Ma-"
        },
        {
          "users: An eye-tracking study,” in Proceedings": "",
          "Transactions on Affective Computing, vol. 6,": "hoor, “Affectnet: A database for facial expres-"
        },
        {
          "users: An eye-tracking study,” in Proceedings": "[23] E. G. Kehoe, J. M. Toomey, J. H. Balsters, and",
          "Transactions on Affective Computing, vol. 6,": ""
        },
        {
          "users: An eye-tracking study,” in Proceedings": "",
          "Transactions on Affective Computing, vol. 6,": "sion,\nvalence,\nand arousal\ncomputing in the"
        },
        {
          "users: An eye-tracking study,” in Proceedings": "A. L. Bokde, “Personality modulates\nthe\nef-",
          "Transactions on Affective Computing, vol. 6,": ""
        },
        {
          "users: An eye-tracking study,” in Proceedings": "",
          "Transactions on Affective Computing, vol. 6,": "wild,” IEEE Transactions on Affective Com-"
        },
        {
          "users: An eye-tracking study,” in Proceedings": "fects of emotional arousal and valence on brain",
          "Transactions on Affective Computing, vol. 6,": ""
        },
        {
          "users: An eye-tracking study,” in Proceedings": "",
          "Transactions on Affective Computing, vol. 6,": "puting, vol. 10, no. 1, pp. 18–31, 2019."
        },
        {
          "users: An eye-tracking study,” in Proceedings": "activation,” Social cognitive and affective neu-",
          "Transactions on Affective Computing, vol. 6,": ""
        },
        {
          "users: An eye-tracking study,” in Proceedings": "roscience, vol. 7, no. 7, pp. 858–870, 2012.",
          "Transactions on Affective Computing, vol. 6,": "[32] C. Y.\nPark,\nN. Cha,\nS. Kang,\nA. Kim,"
        },
        {
          "users: An eye-tracking study,” in Proceedings": "",
          "Transactions on Affective Computing, vol. 6,": "A. H. Khandoker, L. Hadjileontiadis, A. Oh,"
        },
        {
          "users: An eye-tracking study,” in Proceedings": "[24] A. J. Zautra, G. G. Affleck, H. Tennen, J. W.",
          "Transactions on Affective Computing, vol. 6,": ""
        },
        {
          "users: An eye-tracking study,” in Proceedings": "",
          "Transactions on Affective Computing, vol. 6,": "Y. Jeong,\nand U. Lee,\n“K-emocon,\na multi-"
        },
        {
          "users: An eye-tracking study,” in Proceedings": "Reich, and M. C. Davis, “Dynamic approaches",
          "Transactions on Affective Computing, vol. 6,": ""
        },
        {
          "users: An eye-tracking study,” in Proceedings": "",
          "Transactions on Affective Computing, vol. 6,": "modal\nsensor dataset\nfor continuous emotion"
        },
        {
          "users: An eye-tracking study,” in Proceedings": "to emotions and stress\nin everyday life: Bol-",
          "Transactions on Affective Computing, vol. 6,": ""
        },
        {
          "users: An eye-tracking study,” in Proceedings": "",
          "Transactions on Affective Computing, vol. 6,": "recognition in naturalistic conversations,” Sci-"
        },
        {
          "users: An eye-tracking study,” in Proceedings": "ger and zuckerman reloaded with positive as",
          "Transactions on Affective Computing, vol. 6,": ""
        },
        {
          "users: An eye-tracking study,” in Proceedings": "",
          "Transactions on Affective Computing, vol. 6,": "entific Data, vol. 7, 2020."
        },
        {
          "users: An eye-tracking study,” in Proceedings": "well as negative affects,” Journal of personal-",
          "Transactions on Affective Computing, vol. 6,": ""
        },
        {
          "users: An eye-tracking study,” in Proceedings": "ity, vol. 73, no. 6, pp. 1511–1538, 2005.",
          "Transactions on Affective Computing, vol. 6,": "[33] P. Schmidt, A. Reiss, R. Duerichen, C. Mar-"
        },
        {
          "users: An eye-tracking study,” in Proceedings": "",
          "Transactions on Affective Computing, vol. 6,": "berger,\nand K. Van Laerhoven,\n“Introduc-"
        },
        {
          "users: An eye-tracking study,” in Proceedings": "[25] P. T. Costa\nand R. R. McCrae,\n“Influence",
          "Transactions on Affective Computing, vol. 6,": "ing wesad, a multimodal dataset for wearable"
        },
        {
          "users: An eye-tracking study,” in Proceedings": "of\nextraversion\nand\nneuroticism on\nsubjec-",
          "Transactions on Affective Computing, vol. 6,": "stress and affect detection,” in Proceedings of"
        },
        {
          "users: An eye-tracking study,” in Proceedings": "tive well-being:\nhappy and unhappy people.”",
          "Transactions on Affective Computing, vol. 6,": "the\n20th ACM International Conference\non"
        },
        {
          "users: An eye-tracking study,” in Proceedings": "Journal of personality and social psychology,",
          "Transactions on Affective Computing, vol. 6,": "Multimodal Interaction, 2018, pp. 400–408."
        },
        {
          "users: An eye-tracking study,” in Proceedings": "vol. 38, no. 4, p. 668, 1980.",
          "Transactions on Affective Computing, vol. 6,": ""
        },
        {
          "users: An eye-tracking study,” in Proceedings": "",
          "Transactions on Affective Computing, vol. 6,": "[34] M. Soleymani,\nJ. Lichtenauer, T. Pun,\nand"
        },
        {
          "users: An eye-tracking study,” in Proceedings": "[26] M. K. Abadi, R. Subramanian,\nS. M. Kia,",
          "Transactions on Affective Computing, vol. 6,": "M. Pantic,\n“A multimodal database\nfor\naf-"
        },
        {
          "users: An eye-tracking study,” in Proceedings": "P. Avesani,\nI. Patras,\nand N. Sebe,\n“Decaf:",
          "Transactions on Affective Computing, vol. 6,": "fect\nrecognition and implicit\ntagging,” IEEE"
        },
        {
          "users: An eye-tracking study,” in Proceedings": "Meg-based multimodal\ndatabase\nfor\ndecod-",
          "Transactions on Affective Computing, vol. 6,": "Transactions on Affective Computing, vol. 3,"
        },
        {
          "users: An eye-tracking study,” in Proceedings": "ing\naffective physiological\nresponses,”\nIEEE",
          "Transactions on Affective Computing, vol. 6,": "no. 1, pp. 42–55, 2012."
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[35] R.\nSubramanian,\nJ. Wache, M. K. Abadi,": "R. L. Vieriu, S. Winkler, and N. Sebe, “As-",
          "[Online].\nAvailable:\nhttps://onlinelibrary.": "wiley.com/doi/abs/10.1002/hbm.23730"
        },
        {
          "[35] R.\nSubramanian,\nJ. Wache, M. K. Abadi,": "certain: Emotion and personality recognition",
          "[Online].\nAvailable:\nhttps://onlinelibrary.": ""
        },
        {
          "[35] R.\nSubramanian,\nJ. Wache, M. K. Abadi,": "",
          "[Online].\nAvailable:\nhttps://onlinelibrary.": "[43] M. J. Seikavandi, J. Fimland, M. Barrett, and"
        },
        {
          "[35] R.\nSubramanian,\nJ. Wache, M. K. Abadi,": "using commercial sensors,” IEEE Transactions",
          "[Online].\nAvailable:\nhttps://onlinelibrary.": ""
        },
        {
          "[35] R.\nSubramanian,\nJ. Wache, M. K. Abadi,": "",
          "[Online].\nAvailable:\nhttps://onlinelibrary.": "P. Burelli, “Exploring the temporal dynamics"
        },
        {
          "[35] R.\nSubramanian,\nJ. Wache, M. K. Abadi,": "on Affective Computing, vol. 9, no. 2, pp. 147–",
          "[Online].\nAvailable:\nhttps://onlinelibrary.": ""
        },
        {
          "[35] R.\nSubramanian,\nJ. Wache, M. K. Abadi,": "",
          "[Online].\nAvailable:\nhttps://onlinelibrary.": "of\nfacial mimicry in emotion processing using"
        },
        {
          "[35] R.\nSubramanian,\nJ. Wache, M. K. Abadi,": "160, 2018.",
          "[Online].\nAvailable:\nhttps://onlinelibrary.": ""
        },
        {
          "[35] R.\nSubramanian,\nJ. Wache, M. K. Abadi,": "",
          "[Online].\nAvailable:\nhttps://onlinelibrary.": "action units,” in IEEE International Confer-"
        },
        {
          "[35] R.\nSubramanian,\nJ. Wache, M. K. Abadi,": "[36] Y. Wang,\nH. Yu, W. Gao,\nY. Xia,\nand",
          "[Online].\nAvailable:\nhttps://onlinelibrary.": "ence on Automatic Face and Gesture, 2025."
        },
        {
          "[35] R.\nSubramanian,\nJ. Wache, M. K. Abadi,": "C. Nduka,\n“Mgeed:\nA multimodal\ngenuine",
          "[Online].\nAvailable:\nhttps://onlinelibrary.": ""
        },
        {
          "[35] R.\nSubramanian,\nJ. Wache, M. K. Abadi,": "",
          "[Online].\nAvailable:\nhttps://onlinelibrary.": "[44] M. J. Seikavandi, J. Fimland, M. Barrett, and"
        },
        {
          "[35] R.\nSubramanian,\nJ. Wache, M. K. Abadi,": "emotion and expression detection database,”",
          "[Online].\nAvailable:\nhttps://onlinelibrary.": ""
        },
        {
          "[35] R.\nSubramanian,\nJ. Wache, M. K. Abadi,": "",
          "[Online].\nAvailable:\nhttps://onlinelibrary.": "P. Burelli, “Modelling emotions in face-to-face"
        },
        {
          "[35] R.\nSubramanian,\nJ. Wache, M. K. Abadi,": "IEEE Transactions\non Affective Computing,",
          "[Online].\nAvailable:\nhttps://onlinelibrary.": ""
        },
        {
          "[35] R.\nSubramanian,\nJ. Wache, M. K. Abadi,": "",
          "[Online].\nAvailable:\nhttps://onlinelibrary.": "setting: The interplay of eye-tracking, person-"
        },
        {
          "[35] R.\nSubramanian,\nJ. Wache, M. K. Abadi,": "vol. 15, no. 2, pp. 606–619, 2024.",
          "[Online].\nAvailable:\nhttps://onlinelibrary.": ""
        },
        {
          "[35] R.\nSubramanian,\nJ. Wache, M. K. Abadi,": "",
          "[Online].\nAvailable:\nhttps://onlinelibrary.": "ality, and temporal dynamics,” arXiv preprint"
        },
        {
          "[35] R.\nSubramanian,\nJ. Wache, M. K. Abadi,": "[37] S. Koelstra, C. Muhl, M.\nSoleymani,\nJ.-S.",
          "[Online].\nAvailable:\nhttps://onlinelibrary.": "arXiv:2503.16532, 2025."
        },
        {
          "[35] R.\nSubramanian,\nJ. Wache, M. K. Abadi,": "Lee, A. Yazdani, T. Ebrahimi, T. Pun, A. Ni-",
          "[Online].\nAvailable:\nhttps://onlinelibrary.": ""
        },
        {
          "[35] R.\nSubramanian,\nJ. Wache, M. K. Abadi,": "jholt, and I. Patras, “Deap: A database\nfor",
          "[Online].\nAvailable:\nhttps://onlinelibrary.": ""
        },
        {
          "[35] R.\nSubramanian,\nJ. Wache, M. K. Abadi,": "emotion analysis using physiological\nsignals,”",
          "[Online].\nAvailable:\nhttps://onlinelibrary.": ""
        },
        {
          "[35] R.\nSubramanian,\nJ. Wache, M. K. Abadi,": "IEEE Transactions\non Affective Computing,",
          "[Online].\nAvailable:\nhttps://onlinelibrary.": ""
        },
        {
          "[35] R.\nSubramanian,\nJ. Wache, M. K. Abadi,": "vol. 3, no. 1, pp. 18–31, 2012.",
          "[Online].\nAvailable:\nhttps://onlinelibrary.": ""
        },
        {
          "[35] R.\nSubramanian,\nJ. Wache, M. K. Abadi,": "[38] M.\nJ.\nSeikavandi,\nL. Dixen,\nJ.\nFimland,",
          "[Online].\nAvailable:\nhttps://onlinelibrary.": ""
        },
        {
          "[35] R.\nSubramanian,\nJ. Wache, M. K. Abadi,": "S. K. Desu,\nY.\nZserai,\nS.\nLee,\nAntonia-",
          "[Online].\nAvailable:\nhttps://onlinelibrary.": ""
        },
        {
          "[35] R.\nSubramanian,\nJ. Wache, M. K. Abadi,": "Bianca, M. Barrett,\nand P. Burelli,\n“Affec",
          "[Online].\nAvailable:\nhttps://onlinelibrary.": ""
        },
        {
          "[35] R.\nSubramanian,\nJ. Wache, M. K. Abadi,": "multimodal\ndataset,”\n2025,\nzenodo\nrecord.",
          "[Online].\nAvailable:\nhttps://onlinelibrary.": ""
        },
        {
          "[35] R.\nSubramanian,\nJ. Wache, M. K. Abadi,": "[Online]. Available:\nhttps://doi.org/10.5281/",
          "[Online].\nAvailable:\nhttps://onlinelibrary.": ""
        },
        {
          "[35] R.\nSubramanian,\nJ. Wache, M. K. Abadi,": "zenodo.14794876",
          "[Online].\nAvailable:\nhttps://onlinelibrary.": ""
        },
        {
          "[35] R.\nSubramanian,\nJ. Wache, M. K. Abadi,": "[39] K.\nJ. Gorgolewski,\nT. Auer,\nV. D.\nCal-",
          "[Online].\nAvailable:\nhttps://onlinelibrary.": ""
        },
        {
          "[35] R.\nSubramanian,\nJ. Wache, M. K. Abadi,": "houn, R. C. Craddock,\nS. Das, E. P. Duff,",
          "[Online].\nAvailable:\nhttps://onlinelibrary.": ""
        },
        {
          "[35] R.\nSubramanian,\nJ. Wache, M. K. Abadi,": "G. Flandin, S. S. Ghosh, T. Glatard, Y. O.",
          "[Online].\nAvailable:\nhttps://onlinelibrary.": ""
        },
        {
          "[35] R.\nSubramanian,\nJ. Wache, M. K. Abadi,": "Halchenko\net\nal.,\n“The\nbrain\nimaging\ndata",
          "[Online].\nAvailable:\nhttps://onlinelibrary.": ""
        },
        {
          "[35] R.\nSubramanian,\nJ. Wache, M. K. Abadi,": "structure, a format for organizing and describ-",
          "[Online].\nAvailable:\nhttps://onlinelibrary.": ""
        },
        {
          "[35] R.\nSubramanian,\nJ. Wache, M. K. Abadi,": "ing\noutputs\nof\nneuroimaging\nexperiments,”",
          "[Online].\nAvailable:\nhttps://onlinelibrary.": ""
        },
        {
          "[35] R.\nSubramanian,\nJ. Wache, M. K. Abadi,": "Scientific data, vol. 3, no. 1, pp. 1–9, 2016.",
          "[Online].\nAvailable:\nhttps://onlinelibrary.": ""
        },
        {
          "[35] R.\nSubramanian,\nJ. Wache, M. K. Abadi,": "[40] H. Cao, D. G. Cooper, M. K. Keutmann, R. C.",
          "[Online].\nAvailable:\nhttps://onlinelibrary.": ""
        },
        {
          "[35] R.\nSubramanian,\nJ. Wache, M. K. Abadi,": "Gur, A. Nenkova, and R. Verma, “Crema-d:",
          "[Online].\nAvailable:\nhttps://onlinelibrary.": ""
        },
        {
          "[35] R.\nSubramanian,\nJ. Wache, M. K. Abadi,": "Crowd-sourced emotional multimodal\nactors",
          "[Online].\nAvailable:\nhttps://onlinelibrary.": ""
        },
        {
          "[35] R.\nSubramanian,\nJ. Wache, M. K. Abadi,": "dataset,” IEEE transactions on affective com-",
          "[Online].\nAvailable:\nhttps://onlinelibrary.": ""
        },
        {
          "[35] R.\nSubramanian,\nJ. Wache, M. K. Abadi,": "puting, vol. 5, no. 4, pp. 377–390, 2014.",
          "[Online].\nAvailable:\nhttps://onlinelibrary.": ""
        },
        {
          "[35] R.\nSubramanian,\nJ. Wache, M. K. Abadi,": "[41] O. P. John, E. M. Donahue, and R. L. Ken-",
          "[Online].\nAvailable:\nhttps://onlinelibrary.": ""
        },
        {
          "[35] R.\nSubramanian,\nJ. Wache, M. K. Abadi,": "tle, “Big five inventory,” Journal of personality",
          "[Online].\nAvailable:\nhttps://onlinelibrary.": ""
        },
        {
          "[35] R.\nSubramanian,\nJ. Wache, M. K. Abadi,": "and social psychology, 1991.",
          "[Online].\nAvailable:\nhttps://onlinelibrary.": ""
        },
        {
          "[35] R.\nSubramanian,\nJ. Wache, M. K. Abadi,": "[42] R.\nT.\nSchirrmeister,\nJ.\nT.\nSpringen-",
          "[Online].\nAvailable:\nhttps://onlinelibrary.": ""
        },
        {
          "[35] R.\nSubramanian,\nJ. Wache, M. K. Abadi,": "berg,\nL.\nD.\nJ.\nFiederer, M.\nGlasstetter,",
          "[Online].\nAvailable:\nhttps://onlinelibrary.": ""
        },
        {
          "[35] R.\nSubramanian,\nJ. Wache, M. K. Abadi,": "K. Eggensperger, M. Tangermann, F. Hutter,",
          "[Online].\nAvailable:\nhttps://onlinelibrary.": ""
        },
        {
          "[35] R.\nSubramanian,\nJ. Wache, M. K. Abadi,": "W. Burgard,\nand T. Ball,\n“Deep\nlearning",
          "[Online].\nAvailable:\nhttps://onlinelibrary.": ""
        },
        {
          "[35] R.\nSubramanian,\nJ. Wache, M. K. Abadi,": "with convolutional neural networks\nfor EEG",
          "[Online].\nAvailable:\nhttps://onlinelibrary.": ""
        },
        {
          "[35] R.\nSubramanian,\nJ. Wache, M. K. Abadi,": "decoding\nand\nvisualization,” Human Brain",
          "[Online].\nAvailable:\nhttps://onlinelibrary.": ""
        },
        {
          "[35] R.\nSubramanian,\nJ. Wache, M. K. Abadi,": "Mapping, vol. 38, no. 11, pp. 5391–5420, 2017.",
          "[Online].\nAvailable:\nhttps://onlinelibrary.": ""
        }
      ],
      "page": 18
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A survey on emotion recognition for human robot interaction",
      "authors": [
        "S Mohammed",
        "A Hassan"
      ],
      "year": "2020",
      "venue": "Journal of computing and information technology"
    },
    {
      "citation_id": "2",
      "title": "Reviewing the Social Function of Eye Gaze in Social Interaction",
      "authors": [
        "A Huckauf"
      ],
      "year": "2023",
      "venue": "2023 Symposium on Eye Tracking Research and Applications",
      "doi": "10.1145/3588015.3589513"
    },
    {
      "citation_id": "3",
      "title": "A multicomponential approach to emotion recognition and the effect of personality",
      "authors": [
        "G Mohammadi",
        "P Vuilleumier"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "4",
      "title": "Personalitybased emotion recognition using eeg signals with a cnn-lstm network",
      "authors": [
        "M Hosseini",
        "S Firoozabadi",
        "K Badie",
        "P Azadfallah"
      ],
      "year": "2023",
      "venue": "Brain Sciences"
    },
    {
      "citation_id": "5",
      "title": "Emotion recognition and artificial intelligence: A systematic review (2014-2023) and research recommendations",
      "authors": [
        "S Khare",
        "V Blanes-Vidal",
        "E Nadimi",
        "U Acharya"
      ],
      "year": "2024",
      "venue": "Information fusion"
    },
    {
      "citation_id": "6",
      "title": "Comparing recognition performance and robustness of multimodal deep learning models for multimodal emotion recognition",
      "authors": [
        "W Liu",
        "J.-L Qiu",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "7",
      "title": "K-emocon, a multimodal sensor dataset for continuous emotion recognition in naturalistic conversations",
      "authors": [
        "C Park",
        "N Cha",
        "S Kang",
        "A Kim",
        "A Khandoker",
        "L Hadjileontiadis",
        "A Oh",
        "Y Jeong",
        "U Lee"
      ],
      "year": "2020",
      "venue": "Scientific Data"
    },
    {
      "citation_id": "8",
      "title": "Emotion dynamics modeling via bert",
      "authors": [
        "H Yang",
        "J Shen"
      ],
      "year": "2021",
      "venue": "2021 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "9",
      "title": "Analysis of Eye Fixations During Emotion Recognition in Talking Faces",
      "authors": [
        "H Cao",
        "F Elliott"
      ],
      "year": "2021",
      "venue": "2021 9th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "10",
      "title": "Felt versus perceived emotions: Fear and empathy while reading poe's \"the pit and the pendulum",
      "authors": [
        "M Horváth"
      ],
      "year": "2022",
      "venue": "Felt versus perceived emotions: Fear and empathy while reading poe's \"the pit and the pendulum"
    },
    {
      "citation_id": "11",
      "title": "Cerebral topographies of perceived and felt emotions",
      "authors": [
        "H Saarimaki",
        "L Nummenmaa",
        "S Volynets",
        "S Santavirta",
        "A Aksiuto",
        "M Sams",
        "I Jaaskelainen",
        "J Lahnakoski"
      ],
      "year": "2023",
      "venue": "bioRxiv"
    },
    {
      "citation_id": "12",
      "title": "Nature: The nature of human nature",
      "authors": [
        "P Ekman"
      ],
      "year": "2005",
      "venue": "Nature: The nature of human nature"
    },
    {
      "citation_id": "13",
      "title": "Pleasure-arousal-dominance: A general framework for describing and measuring individual differences in temperament",
      "authors": [
        "A Mehrabian"
      ],
      "year": "1996",
      "venue": "Current Psychology"
    },
    {
      "citation_id": "14",
      "title": "Appropriate emotional labelling of non-acted speech using basic emotions, geneva emotion wheel and self assessment manikins",
      "authors": [
        "I Siegert",
        "R Böck",
        "B Vlasenko",
        "D Philippou-Hübner",
        "A Wendemuth"
      ],
      "year": "2011",
      "venue": "2011 IEEE International Conference on Multimedia and Expo"
    },
    {
      "citation_id": "15",
      "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & multi-task learning challenges",
      "authors": [
        "D Kollias",
        "P Tzirakis",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"
    },
    {
      "citation_id": "16",
      "title": "Emotion recognition using different sensors, emotion models, methods, and datasets: A comprehensive review",
      "authors": [
        "X Li",
        "Y Chen"
      ],
      "year": "2024",
      "venue": "Frontiers in Neuroergonomics"
    },
    {
      "citation_id": "17",
      "title": "Survey of deep emotion recognition in dynamic data using facial, speech, and textual cues",
      "authors": [
        "L Zhang",
        "M Wang"
      ],
      "year": "2023",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "18",
      "title": "Modality effects on emotion perception in english by chinese l2 english users: An eye-tracking study",
      "authors": [
        "S Park",
        "J Lee"
      ],
      "venue": "Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL)"
    },
    {
      "citation_id": "19",
      "title": "esee-d: Emotional state estimation based on eye-tracking dataset",
      "authors": [
        "R Chen",
        "Q Liu"
      ],
      "year": "2024",
      "venue": "ArXiv Preprint",
      "arxiv": "arXiv:2403.11590"
    },
    {
      "citation_id": "20",
      "title": "Gaze reveals emotion perception: Insights from modelling naturalistic face viewing",
      "authors": [
        "M Seikavandi",
        "M Barret"
      ],
      "year": "2023",
      "venue": "2023 International Conference on Machine Learning and Applications (ICMLA)"
    },
    {
      "citation_id": "21",
      "title": "Modeling face emotion perception from naturalistic face viewing: Insights from fixational events and gaze strategies",
      "authors": [
        "M Seikavandi",
        "M Barrett",
        "P Burelli"
      ],
      "year": "2024",
      "venue": "Recent Advances in Deep Learning Applications: New Techniques and Practical Examples"
    },
    {
      "citation_id": "22",
      "title": "Emotion recognition in adaptive virtual reality settings: Challenges and opportunities",
      "authors": [
        "Y Wang",
        "S Gao"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "23",
      "title": "Personality modulates the effects of emotional arousal and valence on brain activation",
      "authors": [
        "E Kehoe",
        "J Toomey",
        "J Balsters",
        "A Bokde"
      ],
      "year": "2012",
      "venue": "Social cognitive and affective neuroscience"
    },
    {
      "citation_id": "24",
      "title": "Dynamic approaches to emotions and stress in everyday life: Bolger and zuckerman reloaded with positive as well as negative affects",
      "authors": [
        "A Zautra",
        "G Affleck",
        "H Tennen",
        "J Reich",
        "M Davis"
      ],
      "year": "2005",
      "venue": "Journal of personality"
    },
    {
      "citation_id": "25",
      "title": "Influence of extraversion and neuroticism on subjective well-being: happy and unhappy people",
      "authors": [
        "P Costa",
        "R Mccrae"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "26",
      "title": "Decaf: Meg-based multimodal database for decoding affective physiological responses",
      "authors": [
        "M Abadi",
        "R Subramanian",
        "S Kia",
        "P Avesani",
        "I Patras",
        "N Sebe"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "27",
      "title": "Seed-vii: A multimodal dataset of six basic emotions with continuous labels for emotion recognition",
      "authors": [
        "W.-B Jiang",
        "X.-H Liu",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "28",
      "title": "Dreamer: A database for emotion recognition through eeg and ecg signals from wireless low-cost offthe-shelf devices",
      "authors": [
        "S Katsigiannis",
        "N Ramzan"
      ],
      "year": "2018",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "29",
      "title": "Eav: Eeg-audio-video dataset for emotion recognition in conversational contexts",
      "authors": [
        "M.-H Lee",
        "A Shomanov",
        "B Begim",
        "Z Kabidenova",
        "A Nyssanbay",
        "A Yazici",
        "S.-W Lee"
      ],
      "year": "2024",
      "venue": "Scientific Data"
    },
    {
      "citation_id": "30",
      "title": "Amigos: A dataset for affect, personality and mood research on individuals and groups",
      "authors": [
        "J Miranda-Correa",
        "M Abadi",
        "N Sebe",
        "I Patras"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "31",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "32",
      "title": "K-emocon, a multimodal sensor dataset for continuous emotion recognition in naturalistic conversations",
      "authors": [
        "C Park",
        "N Cha",
        "S Kang",
        "A Kim",
        "A Khandoker",
        "L Hadjileontiadis",
        "A Oh",
        "Y Jeong",
        "U Lee"
      ],
      "year": "2020",
      "venue": "Scientific Data"
    },
    {
      "citation_id": "33",
      "title": "Introducing wesad, a multimodal dataset for wearable stress and affect detection",
      "authors": [
        "P Schmidt",
        "A Reiss",
        "R Duerichen",
        "C Marberger",
        "K Van Laerhoven"
      ],
      "year": "2018",
      "venue": "Proceedings of the 20th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "34",
      "title": "A multimodal database for affect recognition and implicit tagging",
      "authors": [
        "M Soleymani",
        "J Lichtenauer",
        "T Pun",
        "M Pantic"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "35",
      "title": "Ascertain: Emotion and personality recognition using commercial sensors",
      "authors": [
        "R Subramanian",
        "J Wache",
        "M Abadi",
        "R Vieriu",
        "S Winkler",
        "N Sebe"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "36",
      "title": "Mgeed: A multimodal genuine emotion and expression detection database",
      "authors": [
        "Y Wang",
        "H Yu",
        "W Gao",
        "Y Xia",
        "C Nduka"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "37",
      "title": "Deap: A database for emotion analysis using physiological signals",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani",
        "J.-S Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "38",
      "title": "Affec multimodal dataset",
      "authors": [
        "M Seikavandi",
        "L Dixen",
        "J Fimland",
        "S Desu",
        "Y Zserai",
        "S Lee",
        "M Antonia-Bianca",
        "P Barrett",
        "Burelli"
      ],
      "venue": "2025, zenodo record",
      "doi": "10.5281/zenodo.14794876"
    },
    {
      "citation_id": "39",
      "title": "The brain imaging data structure, a format for organizing and describing outputs of neuroimaging experiments",
      "authors": [
        "K Gorgolewski",
        "T Auer",
        "V Calhoun",
        "R Craddock",
        "S Das",
        "E Duff",
        "G Flandin",
        "S Ghosh",
        "T Glatard",
        "Y Halchenko"
      ],
      "year": "2016",
      "venue": "Scientific data"
    },
    {
      "citation_id": "40",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "41",
      "title": "Big five inventory",
      "authors": [
        "O John",
        "E Donahue",
        "R Kentle"
      ],
      "year": "1991",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "42",
      "title": "Deep learning with convolutional neural networks for EEG decoding and visualization",
      "authors": [
        "R Schirrmeister",
        "J Springenberg",
        "L Fiederer",
        "M Glasstetter",
        "K Eggensperger",
        "M Tangermann",
        "F Hutter",
        "W Burgard",
        "T Ball"
      ],
      "year": "2017",
      "venue": "Human Brain Mapping",
      "doi": "10.1002/hbm.23730"
    },
    {
      "citation_id": "43",
      "title": "Exploring the temporal dynamics of facial mimicry in emotion processing using action units",
      "authors": [
        "M Seikavandi",
        "J Fimland",
        "M Barrett",
        "P Burelli"
      ],
      "year": "2025",
      "venue": "IEEE International Conference on Automatic Face and Gesture"
    },
    {
      "citation_id": "44",
      "title": "Modelling emotions in face-to-face setting: The interplay of eye-tracking, personality, and temporal dynamics",
      "authors": [
        "M Seikavandi",
        "J Fimland",
        "M Barrett",
        "P Burelli"
      ],
      "year": "2025",
      "venue": "Modelling emotions in face-to-face setting: The interplay of eye-tracking, personality, and temporal dynamics",
      "arxiv": "arXiv:2503.16532"
    }
  ]
}