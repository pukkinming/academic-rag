{
  "paper_id": "2104.01978v2",
  "title": "Acted Vs. Improvised: Domain Adaptation For Elicitation Approaches In Audio-Visual Emotion Recognition",
  "published": "2021-04-05T15:59:31Z",
  "authors": [
    "Haoqi Li",
    "Yelin Kim",
    "Cheng-Hao Kuo",
    "Shrikanth Narayanan"
  ],
  "keywords": [
    "emotion recognition",
    "emotion elicitation",
    "domain adaptation",
    "domain transfer learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Key challenges in developing generalized automatic emotion recognition systems include scarcity of labeled data and lack of gold-standard references. Even for the cues that are labeled as the same emotion category, the variability of associated expressions can be high depending on the elicitation context e.g., emotion elicited during improvised conversations vs. acted sessions with predefined scripts. In this work, we regard the emotion elicitation approach as domain knowledge, and explore domain transfer learning techniques on emotional utterances collected under different emotion elicitation approaches, particularly with limited labeled target samples. Our emotion recognition model combines the gradient reversal technique with an entropy loss function as well as the softlabel loss, and the experiment results show that domain transfer learning methods can be employed to alleviate the domain mismatch between different elicitation approaches. Our work provides new insights into emotion data collection, particularly the impact of its elicitation strategies, and the importance of domain adaptation in emotion recognition aiming for generalized systems.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Human emotion expressions can provide clues about individual needs, preferences, and attitudes; emotion-related cues can be used to create more natural and human-centered interactive technology. Recent deep learning techniques have shown promising improvements in automatic emotion recognition  [1, 2] ; however, building an accurate emotion recognition system is still challenging due to the shortage of labeled data, and the lack of gold-standard references. Moreover, the performance of emotion recognition system is often sensitive to the variations across different domains. These domain variations can be potentially caused by many factors such as the speaker characteristics  [3]  and recording conditions  [4] , emotion elicitation approaches  [5, 6, 7, 8]  or even socio-cultural context  [9, 10] . Hence, emotion recognition systems that are only evaluated within a single domain condition may not be robust to these variations.\n\nIn automated emotion recognition (ER)  [11, 12, 13] , transfer learning has been employed to alleviate the domain mismatch in the cross-corpora  [14, 15, 16, 17, 18] , cross-language  [16, 19, 20] , cross-speaker  [21, 22]  and cross-modality  [23, 24]  scenarios. Besides those above, emotion elicitation approaches can be another important mismatch factor in ER. Existing works  [5, 6]  already show utterances with the same emotion label can have different expressed behaviors in natural conversations, * Work performed while interning at Amazon. scripted acted talks and improvised conversations. However, related work of quantitative inferential capacity across different emotion elicitation approaches is still very limited.\n\nThe elicitation approaches for most emotion recognition corpora can be divided into two categories: (1) read-speech with fixed lexical scripts and (2) spontaneous speech collected from improvised interaction sessions. In this work, we analyze the domain mismatch and inferential capacity across utterances generated from different emotion elicitation approaches. We explore two hypotheses: H1: Improvised utterances as a source domain can help the emotion recognition of the scripted utterances and H2: Using scripted utterances as a source domain can improve the emotion recognition of the improvised utterances.\n\nH1 is motivated by real human-agent interactions. Many widely-deployed speech agents (e.g. at home, cars etc.) are likely used in specific target domains (e.g. \"weather\", \"music\" etc.), in which most commands have relatively fixed and specialized content. Thus, the potential of leveraging a larger corpus with free content (e.g. public emotional utterances collected from Youtube) for target domain adaptation is explored under source-target setting in H1. Currently, a large number of emotional utterances can be easily collected from online resources, such as online videos  [25]  and call recordings  [26, 27] , however, copyright issues, privacy, and lack of controlled recording settings (background noise, etc.) might cause serious issues in commercial products or usage. Thus, H2 is motivated in that the many available curated emotion datasets are collected in carefully designed settings with controlled conditions, including with user consent, and leveraging these data for improving performances on natural sessions can be highly impactful.\n\nTo alleviate the domain mismatch, traditional transfer learning approaches, such as fine-tuning on target domain, often require hundreds of labeled samples, which can be challenging in the ER area considering the scarcity and cost of labeled data. Recently, domain adversarial training  [28]  has been employed in ER to remove domain information  [21, 29, 30] . In our work, to verify the two proposed hypotheses above, we build an emotion recognition model with the combination of adversarial iterative training strategy and softlabel loss from work  [21]  and  [31] . The adversarial iterative training strategy, on the one hand, is to maximize the capture of emotion-related information; on the other hand, is to remove domain information (the difference between various emotion elicitation approaches). The introduction of softlabel loss ensures that domain transfer learning can consider both domains and emotion categories simultaneously. This is motivated by constraints in real-world use settings, where it is not feasible to have enough target domain labeled data to conduct supervised tuning for each customer or each new usage scenario.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methodology",
      "text": "Our work aims to (1) investigate domain adaptation across utterances obtained through different emotion elicitation approaches and (2) apply domain adversarial learning and softlabel loss with limited target samples. The goal of adversarial iterative training strategy is to predict emotion class labels while simultaneously finding representations that make the domains as similar as possible. Technically, a max-entropy adversarial network model from  [21]  is utilized by optimizing over a loss which includes both classification error on the labeled samples as well as a combined gradient reversal technique with an entropy loss function to make the domains indistinguishable.\n\nHowever, while maximizing domain confusion pulls the marginal distributions of the domains together, it doesn't necessarily align the different emotion categories of the target with those in the source. The correlations across different emotions are related to the properties of human emotion expression and perception. For example, from the acoustic expression aspect, happiness and anger often result in a similar higher pitch. In addition, in emotion perception, sadness can be easily confused with neutral states regardless of utterance type as shown in  [6] . Motivated by work  [31] , besides a single emotion label, we also employ softlabel loss to perform the emotion category adaptation. We explicitly transfer the similarity of emotion categories from the source to the target and further optimize our representation to produce the same structure in the target domain using a few target labeled examples as reference points.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Max-Entropy Adversarial Network",
      "text": "Our proposed model takes acoustic and visual features as inputs, and contains three modules: the representation encoder (ENC), the domain classifier (DC), and the emotion classifier (EC) as shown in Figure  1 .\n\nFor the acoustic ENC, we use stacked 1D convolutional layers and followed by sequential GRU layers  [32]  to obtain the acoustic representation for each utterance. For the visual ENC, we extract the intermediate embedding from a pre-trained inception-ResNet  [33]  model on a face recognition task, then use this embedding as the input feature to another GRU model to generate the visual representation. Then, we concatenate the acoustic and visual representations and add fully connected layers to generate the emotion representation. This fixed dimension representation, as the output of the ENC, is further connected to two classification modules: the emotion classifier (EC) and the domain classifier (DC). We name the parameters of each component as θenc, θEC and θDC , respectively.\n\nThe model takes as input the labeled source data {xS, yS}, and a few target labeled data {xT , yT }. The goal is to train an emotion category classifier that operates on the emotion repre-sentation enc(x), and can correctly classify the target samples during the evaluation. The training strategy has the iterative update process: On the one hand, we attempt to optimize the DC to estimate the domain information; on the other hand, we attempt to correctly estimate the emotion label and remove the domain information for ENC output.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Training Of Dc",
      "text": "For a given representation v, the elicitation domain classifier (DC) can be regarded as a \"discriminator\", which is trained to distinguish the domain identity. Mathematically, this can be done by optimizing a cross entropy objective function as follow:\n\nwhere d is the domain label for sample x, and P (d|enc(x)) is the softmax output of domain classifier. In this training step, only the parameters of DC, i.e. θDC , are optimized.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Training Of Enc And Ec",
      "text": "For the training of ENC and EC, we need to optimize the model to improve the emotion classification accuracy. Simultaneously, we need to optimize ENC to increase the uncertainty or randomness of DC's outputs. Mathematically, we maximize the entropy value of DC's output  [21]  to promote the equal likelihood for all domains.\n\n(2) where P (d|enc(x)) is the softmax output of domain classifier, d refers to two emotion elicitation approaches considered in our work (acted/scripted vs. natural/spontaneous). Only the parameters of ENC are optimized based on this loss function.\n\nIn addition, the performance of the emotion classifier is optimized by minimizing cross entropy loss from EC's output:\n\nwhere the P is the softmax output of emotion classifier and k is the emotion label of the training sample (x, y) ∈ (xS, yS) or (xT , yT ).\n\nTo combine these two objective functions together, we flip the sign of L conf to do a gradient reversal and minimize the weighted overall loss sums:    4 ) given the parameters from the previous iteration rather than globally optimizing all the parameters.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Softlabel Loss",
      "text": "Although the adversarial domain confusion training tries to align the marginal distribution across different domains, the alignment of classes between each domain is not guaranteed. Thus, we further use the softlabel loss to align the source and target emotion classes, in which we try to ensure the relationship between emotion categories is preserved across source and target. This relationship originates from the human expression regardless of the elicitation approaches.\n\nThe softlabel is generated by averaging the per-category activation of source training examples using the source model. Thus, if in total we have K emotion classes, for each emotion category k, we have one softlabel l (k) with K dimensions to depict relative similarity across all emotion categories. The softlabel loss is defined as the cross-entropy loss between the soft activation of a target sample and the soft label corresponding to that emotion category of the sample.\n\nwhere Pi is the softmax output of emotion classifier with a high temperature of τ to make sure related emotion classes have enough probability mass to calculate the softlabel loss.\n\nLastly, we include this softlabel loss (5) to the total loss (4) to formulate the updated total loss during the training of ENC and EC as follows:\n\nL total (xS, yS, xT , yT , θDC ; θenc, θEC ) = Lemo(x, y; θenc, θEC ) -λ conf L conf (xs, xT , θDC ; θenc) +λ sof t L sof t (xT , yT ; θDC , θenc)",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Data And Feature Extraction",
      "text": "The MSP-IMPROV  [6]  corpus is a multi-modality emotional database, which is designed to have a controlled data collection process over lexical content and emotion while also promoting naturalness in the recordings. It includes four types of utterances under different emotion elicitation scenarios as shown in Tables  1  and 2 . Within MSP-IMPROV, utterances are classified as four types based on different elicitation scenarios: Natural Interaction (NI), Other-Improvised (OI), Target-Improvised (TI) and Target-Read (TR). To the best of our knowledge, currently, the MSP-IMPROV is the only dataset that was designed with such different emotion elicitation strategies for audio-visual emotion recognition. In addition, the \"emotion elicitation approach\" category label was provided for each utterance. For each utterance, we extract 40 dimensions of Mel-filter bank coefficients and energy as acoustic features. For visual features, we use an embedding of 512 dimensions from a pretrained face recognition model, which has a similar structure of inception-ResNet  [33] . The extracted embedding captures the facial information and is used for emotion recognition.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiment Setup",
      "text": "We use utterances combined from different categories as source and target domain to verify our hypotheses, and further investigate the possibility of domain transfer learning with limited samples of target domain. In terms of the utterance content, TR and TI contain the designed scripts, while NI and OI are mostly from natural interactions and improvisation sessions. To verify our proposed hypotheses, we design the source and target split with the following combinations:",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Using Ni And Oi As Source Domain",
      "text": "This split is used to test hypothesis (H1). Under this combination, source domain mainly contains utterances from natural or improvised conversations while for the target domain, lexical content of the utterances is fixed.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Using Tr And Ti As Source Domain",
      "text": "This split is used to test hypothesis (H2). Under this setting, the lexical content is fixed in the source domain while in the target domain, the lexical content can be variable.\n\nTo alleviate the emotion class imbalance issue, we utilize a weighted balanced data sampler to make sure samples from less representative categories have larger weights. In addition, within each training batch, we ensure the number of samples is balanced between the source and target domains. As we mentioned before, in a real usage scenario, the amount of labeled data of the target domain is often limited. Considering this situation, domain adaptation is performed with a very limited number of labeled target samples under our experimental setting. For samples of the target domain, we use only 10% as the training set, and 40% is used as a development set and the remaining is for evaluation.\n\nConsidering the imbalance across different emotions, we report the average recall value, which is obtained by computing the per-class classification accuracy independently and then averaging over 4 different emotion categories. For each experiment, we perform the experiments five times with exclusive training samples, and report the average 4-class emotion recognition recall value. As a comparison, we also conduct the following experiments:\n\nSource only: the model is trained using source domain data only, and using the target domain data for evaluation.\n\nSource + Target: The model is trained using samples both from source and target domains without any adversarial techniques.\n\nThe configuration of our model is shown in Table  3 .\n\nTraining details:\n\nAdam optimizer (lr=0.001) + L2 regularization (weight=0.01) batch size=32; epochs=100; τ = 2.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Enc",
      "text": "Acoustic part: Conv1D(in ch=41,out ch=64, kernel size=10, stride=2), PReLU Conv1D(in ch=64,out ch=128, kernel size=5, stride=2), PReLU GRU(in size=128, hidden size=128, num layers=1) Visual part: GRU(in size=512, hidden size=512, num layers=1) Merge part: Linear(in=128+512, out=128), PReLU, Dropout Linear(in=128, out=128) EC Linear(in=128, out=32) PReLU Linear(in=32, out=10) PReLU Linear(in=10, out=4) DC Linear(in=128, out=32) PReLU Linear(in=32, out=10) PReLU Linear(in=10, out=2)  The experiment results are shown in Table  4 . We first notice the domain mismatch exists between NI+OI and TR: the performance of \"source only\" model has the lowest performance among all experiments. The \"Source + Target\" experiment shows that adding target domain training samples helps in model domain adaptation as expected. Even though only a few samples (10% of the samples from the target domain) are employed during training, the performance improves from 0.5005 to 0.5837 directly. More importantly, the results of our proposed model show that the domain adversarial loss can help the target domain adaptation. The softlabel loss, considering the relationships across different emotion categories, also contributes to the domain adaptation under limited target domain samples with the highest performance of 0.6339.\n\nHowever, in the experiment of NI+OI → TI, there is no improvement by adding target training samples. Surprisingly, in contrast to the previous findings, the domain adversarial loss and softlabel loss even worsen the performance. A closer inspection reveals that many samples within OI are exactly the same scripted utterances spoken in an improvised way as those in TI. As described in Table  1 , OI and TI were both recorded during the improvisation sessions. We find the actors often practiced for a while and repeated scripted utterances multiple times before they were ready for the Target-improvised (TI) recording in many improvisation sessions. During dataset creation, those practicing utterances were largely kept and included in the group of OI.\n\nThus, OI utterances overlap with TI utterances in terms of the emotion elicitation manner and content information, which might consequently lead to the fact that adding new samples from target domain doesn't provide extra information of target domain. Moreover, this indistinct boundary between source and target can potentially lead to the domain confusion loss back propagating unintended bias during adaptation training, which can lead to a lower performance after adding the adversarial loss. In addition, we observe a large absolute increase of accuracy from using TR to TI as target domain (from 0.5005 to 0.7330), which is likely caused due to the existing overlap between OI and TI.\n\nApart from this domain overlapping issue caused by the data collection process, the results from our experiment verify the first proposed hypothesis: improvised utterances can be used as source domain to help the emotion recognition for the utterances with the scripted utterances.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Using Tr And Ti As Source Domain",
      "text": "We further reverse the source and target domain: TR and TI are used as the source domain and we select NI or OI as target domain. Under this setting, we are investigating whether we can apply the domain transfer learning from the acted dataset with fixed lexical content to those in natural conversation with variable lexical content. Compared with the combinations in Sec. 5.1, we have fewer samples in the source domain, but more samples in the target domain.    5 , similarly, we notice the domain mismatch between source and target from the lower performance of \"Source only\" models. A better emotion classification accuracy of \"Source+Target\" model indicates that extra target training samples can help the model adaptation for the target domain. For both experiments, we observe the domain adversarial loss and softlabel loss can help the model to achieve better performance with limited training samples of the target domain. By comparing the results across two target domains, we notice NI has a lower classification accuracy. A reasonable explanation is that NI has a larger domain mismatch since both emotion elicitation manners and content information are different from the source.\n\nAll results show that we can improve the emotion recognition for daily natural conversation by leveraging domain adversarial loss and softlabel loss on the content fixed corpus. The results verify the hypotheses we proposed in the beginning.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "The goal of this work is to focus on the domain adaptation across emotion utterances under different elicitation approaches. We proposed two hypotheses and investigated transfer learning between utterances with fixed scripted lexical content and utterances collected from spontaneous, improvised production, particularly with limited labeled target samples. Our adversarial training framework extracts domain invariant emotion representations. Moreover, we utilize the softlabel loss to take the correlations across different emotion categories into consideration. Our results confirm our proposed hypotheses and further provide new insights into elicitation approaches in emotion data collection, as well as the importance of domain adaptation in emotion recognition.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Our model includes the representation encoder (ENC), one domain classiﬁer (DC), and one emotion classiﬁer (EC). We use",
      "page": 2
    },
    {
      "caption": "Figure 1: For the acoustic ENC, we use stacked 1D convolutional",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Training\ndetails:": "ENC",
          "Adam optimizer (lr=0.001) + L2 regularization (weight=0.01)\nbatch size=32; epochs=100; τ = 2.": "Acoustic part:\nConv1D(in ch=41,out ch=64, kernel size=10, stride=2), PReLU\nConv1D(in ch=64,out ch=128, kernel size=5, stride=2), PReLU\nGRU(in size=128, hidden size=128, num layers=1)\nVisual part:\nGRU(in size=512, hidden size=512, num layers=1)\nMerge part:\nLinear(in=128+512, out=128), PReLU, Dropout\nLinear(in=128, out=128)"
        },
        {
          "Training\ndetails:": "EC",
          "Adam optimizer (lr=0.001) + L2 regularization (weight=0.01)\nbatch size=32; epochs=100; τ = 2.": "Linear(in=128, out=32) PReLU\nLinear(in=32, out=10) PReLU\nLinear(in=10, out=4)"
        },
        {
          "Training\ndetails:": "DC",
          "Adam optimizer (lr=0.001) + L2 regularization (weight=0.01)\nbatch size=32; epochs=100; τ = 2.": "Linear(in=128, out=32) PReLU\nLinear(in=32, out=10) PReLU\nLinear(in=10, out=2)"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Source Domain → Target Domain": "NI+OI → TR"
        },
        {
          "Source Domain → Target Domain": "0.5005 (0.021)\n0.5837 (0.031)\n0.6225 (0.018)\n0.6339 (0.023)"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Source Domain → Target Domain": "TI+TR → NI"
        },
        {
          "Source Domain → Target Domain": "0.4296(0.028)\n0.4440 (0.032)\n0.4601 (0.013)\n0.4653 (0.023)"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Speech emotion recognition: Two decades in a nutshell, benchmarks, and ongoing trends",
      "authors": [
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "3",
      "title": "End-to-end multimodal emotion recognition using deep neural networks",
      "authors": [
        "P Tzirakis",
        "G Trigeorgis",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2017",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "4",
      "title": "Speaker normalization based on frequency warping",
      "authors": [
        "P Zhan",
        "M Westphal"
      ],
      "year": "1997",
      "venue": "1997 IEEE International Conference on Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "5",
      "title": "Emotion recognition in the noise applying large acoustic feature sets",
      "authors": [
        "B Schuller",
        "D Arsic",
        "F Wallhoff",
        "G Rigoll"
      ],
      "year": "2006",
      "venue": "Proc. Speech Prosody"
    },
    {
      "citation_id": "6",
      "title": "Scripted dialogs versus improvisation: Lessons learned about emotional elicitation techniques from the IEMOCAP database",
      "authors": [
        "C Busso",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Proceedings of InterSpeech"
    },
    {
      "citation_id": "7",
      "title": "Msp-improv: An acted corpus of dyadic interactions to study emotion perception",
      "authors": [
        "C Busso",
        "S Parthasarathy",
        "A Burmania",
        "M Abdelwahab",
        "N Sadoughi",
        "E Provost"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "8",
      "title": "A novel emotion elicitation index using frontal brain asymmetry for enhanced eegbased emotion recognition",
      "authors": [
        "P Petrantonakis",
        "L Hadjileontiadis"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on information technology in biomedicine"
    },
    {
      "citation_id": "9",
      "title": "Demos: An italian emotional speech corpus",
      "authors": [
        "E Parada-Cabaleiro",
        "G Costantini",
        "A Batliner",
        "M Schmitt",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "10",
      "title": "Cultural differences in emotion: differences in emotional arousal level between the east and the west",
      "authors": [
        "N Lim"
      ],
      "year": "2016",
      "venue": "Integrative medicine research"
    },
    {
      "citation_id": "11",
      "title": "Adversarial domain adaption for multi-cultural dimensional emotion recognition in dyadic interactions",
      "authors": [
        "J Zhao",
        "R Li",
        "J Liang",
        "S Chen",
        "Q Jin"
      ],
      "year": "2019",
      "venue": "Proceedings of the 9th International on Audio/Visual Emotion Challenge and Workshop"
    },
    {
      "citation_id": "12",
      "title": "Survey on audiovisual emotion recognition: databases, features, and data fusion strategies",
      "authors": [
        "C.-H Wu",
        "J.-C Lin",
        "W.-L Wei"
      ],
      "year": "2014",
      "venue": "APSIPA transactions on signal and information processing"
    },
    {
      "citation_id": "13",
      "title": "A review of generalizable transfer learning in automatic emotion recognition",
      "authors": [
        "K Feng",
        "T Chaspari"
      ],
      "year": "2020",
      "venue": "Frontiers in Computer Science"
    },
    {
      "citation_id": "14",
      "title": "Deep learning techniques for speech emotion recognition, from databases to models",
      "authors": [
        "B Abbaschian",
        "D Sierra-Sosa",
        "A Elmaghraby"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "15",
      "title": "A crosscorpus study on speech emotion recognition",
      "authors": [
        "R Milner",
        "M Jalal",
        "R Ng",
        "T Hain"
      ],
      "year": "2019",
      "venue": "2019 IEEE Automatic Speech Recognition and Understanding Workshop"
    },
    {
      "citation_id": "16",
      "title": "Transfer linear subspace learning for cross-corpus speech emotion recognition",
      "authors": [
        "P Song"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "17",
      "title": "Transfer learning for improving speech emotion classification accuracy",
      "authors": [
        "S Latif",
        "R Rana",
        "S Younis",
        "J Qadir",
        "J Epps"
      ],
      "year": "2018",
      "venue": "Transfer learning for improving speech emotion classification accuracy",
      "arxiv": "arXiv:1801.06353"
    },
    {
      "citation_id": "18",
      "title": "Improving crosscorpus speech emotion recognition with adversarial discriminative domain generalization (addog)",
      "authors": [
        "J Gideon",
        "M Mcinnis",
        "E Provost"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "19",
      "title": "Domain generalization with triplet network for crosscorpus speech emotion recognition",
      "authors": [
        "S.-W Lee"
      ],
      "year": "2021",
      "venue": "2021 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "20",
      "title": "Speech emotion recognition with cross-lingual databases",
      "authors": [
        "B.-C Chiou",
        "C.-P Chen"
      ],
      "year": "2014",
      "venue": "Fifteenth Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "21",
      "title": "Unsupervised adversarial domain adaptation for cross-lingual speech emotion recognition",
      "authors": [
        "S Latif",
        "J Qadir",
        "M Bilal"
      ],
      "year": "2019",
      "venue": "2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "22",
      "title": "Speakerinvariant affective representation learning via adversarial training",
      "authors": [
        "H Li",
        "M Tu",
        "J Huang",
        "S Narayanan",
        "P Georgiou"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "23",
      "title": "Speaker-invariant adversarial domain adaptation for emotion recognition",
      "authors": [
        "Y Yin",
        "B Huang",
        "Y Wu",
        "M Soleymani"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "24",
      "title": "Exploring cross-modality affective reactions for audiovisual emotion recognition",
      "authors": [
        "S Mariooryad",
        "C Busso"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on affective computing"
    },
    {
      "citation_id": "25",
      "title": "Generating fmri-enriched acoustic vectors using a crossmodality adversarial network for emotion recognition",
      "authors": [
        "G.-Y Chao",
        "C.-M Chang",
        "J.-L Li",
        "Y.-T Wu",
        "C.-C Lee"
      ],
      "year": "2018",
      "venue": "Proceedings of the 20th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "26",
      "title": "Towards multimodal sentiment analysis: Harvesting opinions from the web",
      "authors": [
        "L.-P Morency",
        "R Mihalcea",
        "P Doshi"
      ],
      "year": "2011",
      "venue": "Proceedings of the 13th international conference on multimodal interfaces"
    },
    {
      "citation_id": "27",
      "title": "Toward detecting emotions in spoken dialogs",
      "authors": [
        "C Lee",
        "S Narayanan"
      ],
      "year": "2005",
      "venue": "IEEE transactions on speech and audio processing"
    },
    {
      "citation_id": "28",
      "title": "Real-life emotions detection with lexical and paralinguistic cues on human-human call center dialogs",
      "authors": [
        "L Devillers",
        "L Vidrascu"
      ],
      "year": "2006",
      "venue": "Ninth International Conference on Spoken Language Processing"
    },
    {
      "citation_id": "29",
      "title": "Domainadversarial training of neural networks",
      "authors": [
        "Y Ganin",
        "E Ustinova",
        "H Ajakan",
        "P Germain",
        "H Larochelle",
        "F Laviolette",
        "M Marchand",
        "V Lempitsky"
      ],
      "year": "2016",
      "venue": "The Journal of Machine Learning Research"
    },
    {
      "citation_id": "30",
      "title": "Domain adversarial for acoustic emotion recognition",
      "authors": [
        "M Abdelwahab",
        "C Busso"
      ],
      "year": "2018",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "31",
      "title": "An empirical analysis of information encoded in disentangled neural speaker representations",
      "authors": [
        "R Peri",
        "H Li",
        "K Somandepalli",
        "A Jati",
        "S Narayanan"
      ],
      "year": "2020",
      "venue": "Proc. Odyssey 2020 The Speaker and Language Recognition Workshop"
    },
    {
      "citation_id": "32",
      "title": "Simultaneous deep transfer across domains and tasks",
      "authors": [
        "E Tzeng",
        "J Hoffman",
        "T Darrell",
        "K Saenko"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "33",
      "title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
      "authors": [
        "K Cho",
        "B Van Merriënboer",
        "C Gulcehre",
        "D Bahdanau",
        "F Bougares",
        "H Schwenk",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
      "arxiv": "arXiv:1406.1078"
    },
    {
      "citation_id": "34",
      "title": "Inception-v4, inception-resnet and the impact of residual connections on learning",
      "authors": [
        "C Szegedy",
        "S Ioffe",
        "V Vanhoucke",
        "A Alemi"
      ],
      "year": "2017",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    }
  ]
}