{
  "paper_id": "2203.04443v3",
  "title": "Estimating The Uncertainty In Emotion Class Labels With Utterance-Specific Dirichlet Priors",
  "published": "2022-03-08T23:30:01Z",
  "authors": [
    "Wen Wu",
    "Chao Zhang",
    "Xixin Wu",
    "Philip C. Woodland"
  ],
  "keywords": [
    "Emotion Recognition",
    "Uncertainty",
    "Dirichlet Prior Distributions",
    "IEMOCAP"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition is a key attribute for artificial intelligence systems that need to naturally interact with humans. However, the task definition is still an open problem due to the inherent ambiguity of emotions. In this paper, a novel Bayesian training loss based on per-utterance Dirichlet prior distributions is proposed for verbal emotion recognition, which models the uncertainty in one-hot labels created when human annotators assign the same utterance to different emotion classes. An additional metric is used to evaluate the performance by detecting test utterances with high labelling uncertainty. This removes a major limitation that emotion classification systems only consider utterances with labels where the majority of annotators agree on the emotion class. Furthermore, a frequentist approach is studied to leverage the continuous-valued \"soft\" labels obtained by averaging the one-hot labels. We propose a two-branch model structure for emotion classification on a per-utterance basis, which achieves state-of-the-art classification results on the widely used IEMOCAP dataset. Based on this, uncertainty estimation experiments were performed. The best performance in terms of the area under the precision-recall curve when detecting utterances with high uncertainty was achieved by interpolating the Bayesian training loss with the Kullback-Leibler divergence training loss for the soft labels. The generality of the proposed approach was verified using the MSP-Podcast dataset which yielded the same pattern of results.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "A UTOMATIC emotion recognition (AER), as a key com- ponent of affective computing, has attracted much attention due to its wide range of potential applications in conversational AI, driver monitoring, and mental health analysis etc. Despite significant progress in recent years  [1] ,  [2] ,  [3] , AER is still a challenging problem and does not even have a single widely accepted task definition.\n\nA straightforward definition of AER is to classify each utterance (i.e. a data sample of emotions with verbal evidence) into a set of pre-defined discrete emotional states (referred to as emotion classes in this paper, such as happy, sad, and frustrated etc.)  [4] ,  [5] ,  [6] ,  [7] ,  [8] ,  [9] ,  [10] ,  [11] . However, since emotion is inherently ambiguous, complex, and highly personal, disagreements exist among human annotators who perceive the emotions and then label the data. Based on subjective judgements, different one-hot labels can be assigned to the same utterance by different annotators, and even by the same annotator  [7] , which leads to uncertainty in emotion labelling. Statistics show that some emotion classes are positively correlated (e.g. sad & frustrated) or inversely correlated (e.g. sad & happy)  [7] ,  [9] ,  [12] . These issues make the labels created by single annotators less reliable. As a solution, multiple annotators are often employed to label each utterance and majority • W.  Wu   voting is used to remove the uncertainty arising from difference among the assigned labels. However, this strategy not only causes training utterances without majority agreed labels to be unused, which is not ideal since emotional data are highly valuable, but also effectively assumes that such utterances would either not be encountered or not be evaluated at test-time. In fact, mixtures of emotions are commonly observed in human interaction  [13] .\n\nIn this paper, AER with one-hot labels is studied and the classification-based task definition revisited. Instead of removing the uncertainty in labels with majority voting, we propose modelling such uncertainty with a novel Bayesian training loss based on utterance-specific Dirichlet prior distributions. In this approach, the one-hot labels provided by different annotators of each utterance are considered as categorical distributions sampled from an utterance-specific Dirichlet prior distribution. A separate prior distribution is estimated for each utterance, which is achieved by an improved Dirichlet prior network (DPN)  [14] . The DPN is trained by minimising the negative log likelihood of sampling the original one-hot labels from their relevant utterance-specific Dirichlet priors. Alternatively, from a frequentist perspective, \"soft\" labels can be obtained by averaging the categorical distributions relevant to the one-hot labels which can be viewed as maximum likelihood estimate (MLE) of the label for each utterance. An AER model can be trained by minimising the Kullback-Leibler (KL) divergence between the soft labels and its output distributions. The DPN and KL divergence training losses can be combined by simple linear interpolation.\n\nTo evaluate the proposed approach, a state-of-the-art neural network model architecture proposed in  [15]  is adopted for emotion classification, which consists of a time synchronous branch (TSB) that focuses on modelling the temporal correlations of multimodal features and a time asynchronous branch (TAB) that takes sentence embeddings as input to facilitate modelling meanings embedded in the text transcriptions. Experimental results on the widely used IEMOCAP dataset  [7]  show that the TSB-TAB structure achieves state-of-the-art classification results in 4-way classification (happy, sad, angry & neutral) when evaluated with all of the commonly used speaker-independent test setups. The 4-way classification is then extended to 5-way classification by including an extra emotion class \"others\" which represents all the other types of emotion labelled in IEMOCAP but ignored in the 4-way setup. Next, we redefine the task from classification to distribution modelling by representing the emotions using a 5-dimensional distribution rather than a single hard label. This allows utterances without majority agreed labels to also be considered. Uncertainty estimation is performed by training the model with soft labels and the DPN training loss. Classification accuracy is no longer an appropriate evaluation metric when considering uncertainty in emotion labelling. Instead, we propose evaluating the model performance in uncertainty estimation in terms of the area under the precision-recall curve (AUPR) when detecting utterances without majority unique labels at test-time. This also provides a more general two-step test procedure for AER, which can detect utterances with high uncertainty in emotion for further processing and classify the remainder of them into one of the emotion classes. Further experiments on the MSP-Podcast dataset, a larger speech corpus with natural emotions, demonstrate that our proposed uncertainty estimation approach generalises to handling realistic emotion data.\n\nThe rest of the paper is organised as follows. Section 2 discusses work on objective emotion quantification and related methods that address the inconsistency of emotion perception among human annotators. Section 3 presents an analysis of the IEMOCAP database and revisits the definition of emotion classification task with IEMOCAP. Section 4 introduces the soft-label-based and DPN-based approaches to emotion distribution modelling and uncertainty analysis. The model structure and experiment setup are shown in Section 5. Section 6 presents the results and analysis for emotion classification and distribution modelling on IEMOCAP. The proposed approach is further verified with experiments on the MSP-Podcast dataset in Section 7, followed by conclusions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "The inherent ambiguity of emotion, resulting from mixed emotions and personal variations in emotion expression etc., makes it still an open question as how to define emotion for easier quantification and objective analysis. Discrete emotion theory classifies emotion into several basic categories (e.g. happy, sad, fear, anger etc.)  [16] ,  [17] ,  [18]  while psychologists have also observed that these distinct emotion categories overlap and have blurred boundaries between them  [19] ,  [20] . Alternative methods were developed to characterise emotional states by several fundamental continuous-valued or multi-valued bipolar dimensions that are more suitable to be evaluated independently  [21] ,  [22] .\n\nDespite the commonly assumed dimensions such as valencearousal and approach-avoidance  [21] ,  [22] , there's still a debate about the proper dimensional scheme and the orthogonality of the dimensions  [23] ,  [24] ,  [25] .\n\nThe subjectivity of emotional perception further complicates the problem of designing AER datasets. Despite the efforts of psychologists to de-correlate emotion dimensions, creating intensity labels with continuous values or multiple discrete values can still be highly subjective and also lead to a high degree of uncertainty in the data. In response to this problem, most datasets were created using the strategy of having multiple human annotators to provide multiple labels for each utterance. The \"ground truth\" is then commonly defined as the majority vote for discrete labels  [7] ,  [9] ,  [12] ,  [26]  or the mean for dimensional labels  [7] ,  [9] ,  [27] . When using the mean dimensional labels, the discrepancies between annotators are ignored. Several approaches have been proposed to characterise the subjective property of emotion perception by modelling the inter-annotator disagreement level as the standard deviation of the dimensional labels, such as including a separate task to predict the standard deviation in a multi-task framework  [28] ,  [29] , or predicting such values using Gaussian mixture regression models  [30] ,  [31] . Recently, alternative methods including Gaussian processes  [32] , generative variational auto-encoders  [33] , and Monte-Carlo dropout  [34]  have been applied to the problem without using the standard deviation of dimensional emotion labels as additional training labels.\n\nWhen using a majority vote to obtain the ground truth for discrete class labelling, the data without ground truth due to annotators' disagreement are usually discarded in classification-based AER  [35] ,  [36] ,  [37] . AER researchers have proposed various methods to address the uncertainty in emotion labelling caused by the inconsistency of emotion perception among human annotators. Nediyanchath et al.  [38]  used multitask learning with gender or speaker classification to model the variations in personal aspects of emotional expression. Lotfian et al.  [39]  proposed a multitask learning framework to recognise the primary emotional class by leveraging extra information provided in the evaluations about secondary emotions. Ando et al.  [40]  estimated the existence of multi-label emotions as an auxiliary task to improve the recognition of the dominant emotions. Another type of commonly used method is to train AER models with soft labels, which are derived as the mean of the hard labels and can be interpreted as the intensities of the emotion classes  [41] . Fayek et al.  [42]  incorporated inter-annotator variabilities by training a separate model based on the hard labels produced by each annotator, and it was shown that an ensemble of such models performed similarly to a single model trained using the soft labels. Although these approaches improved training with soft labels, at test-time the evaluations were only based on emotion classification accuracy, which results in a major inconsistency between training and evaluation  [43] .\n\nThis paper focuses on classification-based AER. Rather than trying to remove the uncertainty in the emotion representation or to make emotion classes more separable, we acknowledge the ambiguity in emotion expression and subjectivity in emotion perception and evaluations, and model the resulted uncertainty in emotion class labels using a novel Bayesian training loss.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Emotion Classification With Iemocap",
      "text": "The IEMOCAP  [7]  corpus is the primary corpus used in this paper. It is one of the most widely used datasets for verbal emotion classification and is designed with a typical data annotation procedure. It consists of 5 dyadic conversational sessions performed by 10 professional actors and the data includes three different modalities including the spoken audio, text transcription and the facial movements. There are in total 10,039 utterances and approximately 12 hours of data, with an average duration of 4.5 seconds per utterance. Each utterance was annotated by three human annotators for categorical labels (neutral, happiness, sadness, and anger etc.). Each annotator was allowed to tag more than one emotion category for each sentence if they perceived a mixture of emotions. The ground truth labels are determined by majority voting. However, since only 7,532 utterances have majority unique 1 hard labels , the remaining 25% of the utterances are normally discarded. This issue and its solutions will be discussed later in detail in Sections 3.3 and 4. Although our analysis is primarily performed on IEMOCAP, the issues also existed in many other commonly used AER datasets, such as MSP-Podcast  [10]  whose analyses are provided in Section 7.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "4-Way Classification",
      "text": "To be consistent and comparable with previous studies  [1] ,  [35] ,  [36] ,  [37] ,  [44] ,  [45] ,  [46] , only utterances with majority unique labels belonging to \"angry\", \"happy\", \"excited\", \"sad\", and \"neutral\" were used for our 4-way classification experiments. The \"excited\" class was merged with \"happy\" to better balance the size of each emotion class, which results in a total of 5,531 utterances with 1,636, 1,103, 1,084, and 1,708 utterances for happy, angry, sad and neutral respectively. This approach is widely used but it discards 44% of the data.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "5-Way Classification",
      "text": "As shown in Table  1 , although 7,532 utterances in IEMO-CAP have majority unique labels, only 5,531 of them are used in the 4-way classification setup given in Section 3.1. This excludes all the other classes of emotions, including \"frustration\" which is the largest emotion class taking 25% of the dataset, and therefore this partition only tackles part of the AER problem. To resolve this issue, we investigated the use of an alternative 5-way classification setup. An extra target of \"others\" was included as the 5-th class, to represent all of the other emotions that exist in IEMOCAP including utterances labelled as \"frustration\", \"fear\", \"surprise\", 'disgust\", and \"other\". All 7,532 utterances with majority unique labels were used for training and test in 5-way classification.\n\nAs in most previous studies, our 4-way and 5-way classification systems can be evaluated based on classification accuracy with the utterances with majority unique labels.\n\n1. Emotion category with highest votes was unique (notice that the evaluators were allowed to tag more than one emotion category). However, for the utterances without majority unique labels, that comprise 25% of IEMOCAP, classification accuracy against a single reference cannot be used 2 . In fact, the utterances without majority unique labels often include complex and meaningful emotions, and are potentially important for training AER systems. Furthermore, similar data will be encountered at test time and the system should also be evaluated on these type of data. In order to understand the problem better, the following section performs a data analysis.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Data Analysis",
      "text": "Recall that in IEMOCAP, each utterance was labelled by three annotators and each annotator was allowed to give multiple different labels to each utterance. Table  2  shows examples of typical situations for the hard labels provided by the annotators and Table  3  summarizes some of the statistics of IEMOCAP. There are 1,272 evaluations that have more than one label, indicating the annotators were uncertain about the emotions when evaluating the utterance. When labels from different annotators are considered, all annotators agreed on the same emotion class label (e.g. 'AAA') for only 24% (2,383 out of 10,309) of the utterances, which we denote as 3/3 agreement or Ω 3/3 utterances. The rest of the utterances consist of those with agreement from two annotators, denoted as 2/3 agreement or Ω 2/3 utterances (e.g. 'AAB'), and those without any agreement, referred to as no agreement or Ω ≤1/3 utterances (e.g. 'ABC'). Note the case shown in the last row of Table  2 . Although the labels have majority 'AB', the majority is not unique. Thus, the sentence belongs to Ω ≤1/3 .\n\nWhen majority voting is applied, both 3/3 agreement and 2/3 agreement utterances result in the same majority unique ground truth label (e.g. 'A'), despite that an annotator assigned different labels to the 2/3 agreement utterances, which causes a loss of the complexity and uncertainty in emotion annotation. This problem is more severe when there isn't a majority label and they are ignored completely 2. Multiple reference annotations could be counted as correct for scoring purposes. But it is clearly unsatisfactory when the number of annotators increases while using a small number of emotion classes.\n\nin both training and test. Considering the fact that 51% of the utterances are 2/3 agreement and 25% have no majority unique label, the strategy to use majority voting on labels significantly changes the true emotion distribution. To resolve these problems, in the next section, we propose representing and modelling the emotion using a distribution rather than a single hard label.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Table 2",
      "text": "Examples of typical situations for IEMOCAP annotations. \"e1\", \"e2\", and \"e3\" are three annotators for an utterance. \"A\" \"B\" \"C\" are three different emotion classes. Ω 3/3 , Ω 2/3 , and Ω ≤1/3 refer to the cases that the annotations achieve 3/3, 2/3, and no unique agreement. 'AB' for e2 denotes the second evaluator gave two annotations 'A' and 'B' to the utterance.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Uncertainty Estimation In Emotions",
      "text": "To model the uncertainty caused by subjective emotion perception and evaluation, we propose revising the training target for each utterance from a single one-hot hard label (or categorical or 1-of-K label) to a continuous-valued categorical distribution over the emotion classes. For Ω ≤1/3 utterances, this allows the consideration of scenarios where majority unique labels do not exist (e.g. the 'ABC' and 'AABBC' cases in Table  2 ). For Ω 2/3 utterances, it avoids the problem that not all original hard labels are represented by majority voting (e.g. the 'AAB' and 'AABC' cases in Table 2). To learn such a categorical distribution, a frequentist approach is used to obtain a \"soft\" label for training using the MLE of the distribution for each utterance. Next, an alternative Bayesian approach is proposed that estimates a separate Dirichlet prior for each utterance. These two approaches can be combined by interpolating the KL loss with the DPN loss. Finally, a method to evaluate the performance for uncertainty estimation of the 5-way AER systems is proposed.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Soft Labels",
      "text": "Denote a categorical distribution [p(ω 1 |µ), . . . , p(ω K |µ)] T = µ as the emotion distribution of an utterance x, where K is the number of emotion classes and\n\nn , . . . , µ\n\n} N n=1 be a dataset with N utterances where x n is the input features of the n-th utterance and µ (1) n , . . . , µ (Mn) n are its M n labels provided by all annotators. In this paper, µ (m) n is a one-hot vector since hard labels are annotated as in IEMOCAP. Such hard labels can be considered as samples drawn from the underlying true emotion distribution p tr (µ|x). For brevity, the subscript n is dropped and the following analysis is based on a single utterance x and its associated M labels {µ (1) , . . . , µ (M ) }.\n\nOne way to obtain the target emotion distribution µ for each utterance is to use the MLE:\n\nThe k th element of μ, μk = p(ω k | μ), can be obtained for the MLE as the relative frequency of the ω k hard labels:\n\nwhere N k is the number of occurrences of ω k in {µ (1) , . . . , µ (M ) }. Such an MLE-based distribution is referred to as a soft label, which consists of the proportion of each emotion class. For instance, if the three 1-of-3 hard labels of an 'AAB' utterance are [1,0,0], [1,0,0], and [0,1,0], the soft label is [0.67,0.33,0], whereas the majority unique label obtained by majority voting is [1,0,0]. This comparison shows that a soft label can preserve some uncertainty information derived from the original hard labels. A 5way classification system introduced in Section 3.2 can be trained using soft labels instead of majority unique labels by minimising the KL divergence between the soft labels and the predictions, which is abbreviated as a \"soft\" system in this paper. Denoting the softmax output of the neural network model as y\n\nwhere Λ is the collection of model parameters and y is the predicted emotion distribution, the soft system training loss is the KL divergence between μ and y given by",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Dirichlet Prior Network",
      "text": "In Section 4.1, the average of the observed labels of an utterance is used as the approximation to the true target emotion distribution. The MLE converges to the true target distributions when there is an extremely large number of labels available. However, this condition cannot be satisfied in real-world AER since often only a small number of annotators (i.e. three for IEMOCAP) can be employed for emotion data labelling due to the task complexity and cost. This issue can be alleviated from the Bayesian perspective by introducing a prior distribution for the categorical distribution.\n\nIn this section, we introduce the Dirichlet prior network (DPN)  [14] ,  [47] , a neural network model which models p(µ|x, Λ) by predicting the parameters of its Dirichlet prior distribution.\n\nThe Dirichlet distribution, as the conjugate prior of the categorical distribution, is parameterised by its concentration parameters α = [α 1 , . . . , α K ] T . The Dirichlet distribution Dir(µ|α) is defined as\n\nwhere Γ(•) is the gamma function defined as\n\nHence, as shown in Fig.  1 , given the concentration parameter α, the categorical distribution µ is a sample drawn from Dir(µ|α), and a 1-of-K hard label relevant to the emotion class ω k is a sample drawn from µ. Here µ models the distribution over K emotion classes. Dir(µ|α) models the distribution of the emotion distribution µ. For AER, utterance-specific priors derived for each utterance separately are more suitable than a \"global\" prior shared by all utterances, since emotions produced by different speakers in different contexts should not have the same prior. A DPN predicts the concentration parameters α as the output of the network α = f Λ (x). Here the prior distributions are \"utterance-specific\" as they are predicted separately for each utterance.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Training",
      "text": "Labels provided by different annotators of an utterance can be regarded as categorical distribution samples drawn from the utterance-specific prior. Given a training utterance x and M categorical emotion distributions {µ (1) , . . . , µ (M ) } provided by the annotators, a DPN is trained to maximise the likelihood p(µ|x, Λ) = Dir(µ|α), which is equivalent to minimising the negative log likelihood loss function:\n\nwhere Dir(µ (m) |f Λ (x)) is defined in Eqn. (3) and µ (m) is an one-hot hard label. L dpn is referred to as the DPN loss in the rest of the paper. When using the DPN loss, label smoothing  [48]  that converts µ (m) into a \"softer\" label μ(m) = [μ\n\nwas found necessary to stabilise training, where ε 1 > 0 is a small constant  [14] . It was also observed that it is important to increase each α k predicted by the model by another small constant ε 2 > 0 when calculating Dir( μ(m) |f Λ (x)) based on Eqn. (3)  [14] . Comparing Eqn.  (5)  to Eqn. (2), each label µ (m) is taken into account separately when training a DPN, while only the averaged label μ is considered when training a soft label system. For example, two observations 'A','B','C' and 'ABC','ABC','ABC' yield the same soft label loss but different DPN losses. The latter case shows that all three annotators are uncertain about the emotion, indicating the emotion of the utterance might have high degree of inherent uncertainty. DPN training preserves the number of occurrences of each emotion class and allows an estimate of the confidence of the uncertainty.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Inference",
      "text": "The predictive distribution of the DPN for an input x is given by marginalising over all possible categorical distributions, which is equivalent to the expected categorical distribution under the conditional Dirichlet prior:\n\nwhich is equal to\n\n, where z k is the logit of kth output unit of the neural network model. This makes the expected posterior probability of an emotion class ω k be the value of the k-th output of the softmax. In this way, a standard DNN classifier with a softmax output function can be viewed as predicting the expected categorical distribution under a Dirichlet prior  [14]  while the mean is insensitive to arbitrary scaling of α k . This means that the precision α 0 , which controls the sharpness of the Dirichlet prior distribution, is degenerate if the classifier is trained with the KL divergence loss instead of the DPN loss.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Combining Dpn With Soft Labels",
      "text": "In this section, we propose to combine the DPN loss L dpn with the KL loss L kl for soft labels by\n\nwhere λ is a scalar coefficient. Compared to the gradients produced by L kl , it was observed empirically that those produced by L dpn are highly sparse and have a much larger dynamic range. Interpolating L dpn with L kl reduces the sparsity of the gradients values, and the value of λ (e.g. 20) is set manually to ensure that the dynamic ranges of the gradients of the L dpn and L kl terms are similar. In practice, it was found in our experiments that using L dpn-kl instead of L dpn not only stabilised DPN training by removing the need for the two smoothing constants ε 1 and ε 2 defined in Section 4.2.1, but also improved the AER system performance with all evaluation criteria. Another motivation for using L dpn-kl lies in the connection between L dpn and L kl when an unlimited number of labels is available for each utterance. Let y = [p(ω 1 |x, Λ), . . . , p(ω K |x, Λ)] T be the distribution of x belonging to each of the K emotion classes estimated by the neural network model, when M → ∞, the DPN loss can be rewritten as\n\nwhere I(µ ∈ ω k ) is an indicator function and equals one when µ is a hard label of ω k . H [p tr (µ|x)] is the entropy of p tr (µ|x) and is a constant term in the loss. p tr (µ|x) is the underlying true emotion distribution of x, and L ∞ kl = KL [p tr (µ|x) y] is the KL loss when M → ∞. Thus both L ∞ dpn and L ∞ kl reach the same optimum. With a finite number of labels for each utterance, L dpn and L kl approximate L ∞ dpn and L ∞ kl separately from different perspectives:\n\nL dpn approximates the expectation with respect to the true distribution by the the empirical average of the likelihood while L kl approximates the true distribution by the sample average. When only a finite number of hard labels is available, L dpn-kl can achieve a better approximation by leveraging the complementarity of L dpn and L kl . It's worth mentioning that our proposed combined loss could be applicable to other perception and understanding tasks which rely on subjective evaluations and uncertain labels from human annotators.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Evaluation Of Uncertainty Estimation",
      "text": "In the previous sections, a Bayesian approach has been proposed to model the uncertainty in the training labels. However, the most appropriate way to handle the uncertainty in the test data remains an issue. From Table  3 , at least 25% of the utterances in IEMOCAP do not have majority unique labels, which makes it impossible to evaluate by classification with a single reference class. This indicates classification accuracy is no longer a suitable evaluation metric when considering more general AER applications, which can encounter test utterances with ambiguous emotional content. Therefore, in this section, we propose using the area under the precision-recall curve (AUPR) with different measures as alternative metrics for AER.\n\nThe AUPR is the average of precision across all recall values computed as the area under the precision-recall (PR) curve. To compute AUPR, a binary task is first defined which, in this paper, is detecting utterances without majority agreed labels (i.e. Ω ≤1/3 utterances in IEMOCAP). An uncertainty measure is then defined to measure the uncertainty for each predicted distribution. Two measures are used in this paper:\n\n• The probability of the predicted class or max probability (Max.P) that measures the confidence in the prediction  [49] ,  [50] . Max.P is defined as\n\n• The entropy of the predictive distribution (Ent.) that has been used in  [49] ,  [51] . It behaves similarly to Max.P, but represents the confidence encapsulated in the entire output distribution. That is,\n\nA decision threshold is set based on the uncertainty measure which determines whether the test sample belongs to the positive or negative class. For example, utterances with Max.P lower than the threshold (or Ent. higher than the threshold) are predicted as Ω ≤1/3 . A PR curve is obtained by calculating the precision and recall for different decision thresholds where the x-axis of a PR curve is the recall, the yaxis is the precision and the decision thresholds are implicit and are not shown as a separate axis. The area under the PR curve is computed as AUPR. Compared to classification accuracy, AUPR can not only be applied to any test utterance but also quantify the model's ability to estimate uncertainty. In Section 6.3, experiments that assess uncertainty estimation ability by detecting utterances without majority agreed labels are reported. This detects whether a test utterance belongs to Ω ≤1/3 based on the model output distributions with either Max.P or Ent. In the detection experiments, Ω ≤1/3 is chosen as the negative class, while Ω 2/3 and Ω 3/3 are chosen as the positive classes. Detecting Ω ≤1/3 was selected as the binary task for AUPR since this simulates such a real application case: if an Ω ≤1/3 utterance is detected, the utterance may include ambiguous emotions that should be evaluated by further models or humans. Otherwise the utterance belongs to Ω 2/3 or Ω 3/3 where a majority unique label exists and emotion classification can be applied.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Experimental Setup",
      "text": "Our experimental setup is given in this section, including feature representations and model structures.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Feature Representations",
      "text": "The audio representation used for speech-based AER often includes log-mel filterbank features (FBKs)  [52] . In this paper, 40-dimensional (-d) FBK with a 10 ms frame shift and 25 ms frame length are used, which is denoted FBK 25 . An additional type of long-term FBK feature is also used, which extracted in the same way as FBK 25 apart from an long 250 ms frame length, and this is denoted FBK 250 . FBK features contain information about the short-term spectrum but don't explicitly contain pitch information that can be important in describing emotional speech  [53]  and is often complementary to FBK features  [54] ,  [55] . Following  [56] , log pitch frequency features with probability-of-voicingweighted mean subtraction over a 1.5 second window are used along with FBK features.\n\nText features are also included in our models. Pre-trained 50-d GloVe embeddings are used to encode word-level transcriptions  [57] , while the pre-trained BERT-based model without fine-tuning is used to encode the transcription of each single utterance into a 768-d vector  [58] . Following prior work  [35] ,  [36] ,  [37] , the reference transcriptions from IEMOCAP were used for the text modality.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Model Structure",
      "text": "The proposed model structure is shown in Fig.  2 , which consists of a time synchronous branch (TSB) that fuses the audio features with the corresponding text information at each time step, and a time asynchronous branch (TAB) that captures the text information embedded across the transcriptions of a number of consecutive utterances. In the TSB, the audio features and the corresponding GloVe-based word embeddings are combined at each time step with a simple concatenation operation. The TSB structure is similar to that which is often used for speaker embedding extraction  [59] .\n\nIt uses a five-head self-attentive layer  [60]  to pool the framelevel vectors across time in the input window, and a time delay neural network with residual connections  [61]  is used as the encoder to derive the frame-level vectors.\n\nWhile the TSB includes modelling the temporal correlations between different modalities, the TAB focuses on capturing text information including meaning from the speech transcriptions. The BERT-derived sentence embeddings of the utterance transcriptions are used as the input vectors to the TAB. The embeddings for a number of consecutive utterances were used as the TAB input since the emotion of each utterance is often strongly related to its context in a spoken dialogue  [62] . A shared fully-connected (FC) layer is used to reduce the dimension of each input BERT embedding, and the resulting vectors are then integrated by another five-head self-attentive layer. Finally, output vectors from both branches are fused using an FC layer for emotion classification. The hidden and output activation functions are ReLU and softmax respectively, and a large-margin softmax loss function is used for better regularization  [63] .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experiments On Iemocap",
      "text": "Since the test sets are slightly imbalanced between different emotion categories, both the weighted accuracy (WA) and unweighted accuracy (UA) are reported. WA corresponds to the overall accuracy while UA corresponds to the average class-wise accuracy. Models were implemented using HTK  [64]  in combination with PyTorch. The newbob learning rate scheduler with an initial learning rate of 5 × 10 -5 was used throughout training.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "4-Way Classification And Cross Comparisons",
      "text": "To compare to previously published results on IEMOCAP, the system was evaluated with all of the commonly used training and test setups on IEMOCAP: training on Sessions 1-4 and testing on Session 5; 5-fold cross validation (CV) that leaves one of the 5 sessions out of training and used for testing at each fold, and 10-fold CV that leaves one of the 10 speakers out at each fold. These test setups show whether the model is able to learn reliable and speaker independent features with the limited amount of training and test data in IEMOCAP. The results and modalities used in previous related work are summarised and compared in Table  4 , which shows that our 4-way classification system achieved state-of-the-art results on IEMOCAP when evaluated with all of the three test settings. More detailed experiments and results can be found in  [15] .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "5-Way Classification",
      "text": "As shown in Table  5 , the classification accuracy of the 5way system using 5-fold CV on the previous four emotions (happy, sad, neutral, angry) was 72.47% WA and 74.29% UA. A 4.65% decrease was observed compared with the results of the 4-way system. On the other hand, since the 4-way system cannot correctly classify examples from the \"others\" class, the overall classification accuracy of the 4way system drops dramatically to 57.02% WA and 62.72% UA when tested on the 5-way data.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Uncertainty Estimation Experiments",
      "text": "Four systems were tested:\n\n• \"hard\": A 5-way emotion classification system described in Section 3.2;\n\n• \"soft\": A soft label system trained by minimising L kl between soft labels and the prediction in Section 4.1; • \"dpn\": A standard DPN system described in Section 4.  2  and trained with L dpn with ε 1 = 1 × 10 -2 and ε 2 = 1 × 10 -8 ; • \"dpn-kl\": A system trained on L dpn-kl described in Section 4.3 where ε 1 and ε 2 are set to 0, and the weight λ to scale the L kl term is set to 20.0. The \"hard\" system was trained on 5-way classification using the training data belonging to Ω 2/3 and Ω 3/3 , while the other systems were all trained using all the utterances in the training set. All systems were first evaluated using the 5way classification accuracy on Ω 2/3 and Ω 3/3 test utterances, and then evaluated on all test utterances using the average KL divergence, entropy, and AUPR (Max.P) and AUPR (Ent.).  The average of 5-fold CV results on IEMOCAP are shown in Table  6 . Compared to the \"hard\" system that is trained for better emotion classification accuracy, the \"soft\", \"dpn\", and \"dpn-kl\" systems were all trained to better model the uncertainty among the different emotion classes. Therefore it is expected that the hard system has the best UA and WA among all systems. However, as discussed in Section 4.4, classification accuracy is not suitable here as it can not be applied to the Ω ≤1/3 test utterances. It is also expected that the hard system has the lowest entropy (sharper output distributions) as it is trained to learn 0-1 distributions. It is widely known  [65] ,  [66]  by deep learning researchers that such deep model hard label classification systems are often \"over-confident\", meaning that the model can have poor uncertainty estimation ability as indicated by the AUPR metrics. Similarly, it is reasonable that the soft system has the best KL divergence as it is trained to minimise such an objective. However, the KL divergence is also not the most suitable metric here as it does not distinguish between the 'AB' and 'AAABBB' cases as discussed in Section 4.2.1. Comparing \"dpn\" to \"soft\", \"dpn\" ranks better on AUPR (Max.P) while \"soft\" is better on AUPR (Ent.). \"dpnkl\" however, outperforms \"dpn\" on all evaluation metrics and produces the highest AUPR among all systems. It also achieves a balance between the \"hard\" and \"soft\" systems, yielding higher UA and WA than \"soft\" and a smaller KL divergence than \"hard\". The standard deviation across folds are shown in Fig.  3 . Although the error bars of AUPR value contain overlap among the systems, the \"dpn-kl\" system consistently outperforms the others in all folds.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Further Experiments For Analysis",
      "text": "For the convenience of visualisation, we took one fold (trained on Session 1-4 and tested on Session 5) as an example and performed further analysis. The number of sentences in each group is listed in Table  7 . This section first presents the effect of replacing single categorical hard labels with emotion distributions on the uncertainty of emotion prediction for different data groups by comparing the \"hard\" system to the \"soft\" system. Then the performance of all four systems on detecting the test utterances with high labelling uncertainty is presented. Data group Ω (  5 ) Ω\n\n# of sentences 2,170 479 1,171 520",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Comparisons Between \"Hard\" And \"Soft\" Systems",
      "text": "The performance of the \"hard\" and \"soft\" systems on different data groups are given in Table  8  and illustrated in Fig.  4 . As utterances in Ω\n\n1/3 don't have \"ground truth\" hard labels, only the KL divergence and entropy are reported for that group.\n\nAs shown in Fig.  4 (b), \"soft\" has a higher entropy than \"hard\" in all cases as the soft labels retain some uncertainty in the labels and are trained to produce flatter distributions. The uncertainty in the estimated emotion distribution increases when fewer annotators reach an agreement. The label distribution becomes flatter and the entropy of the distribution predicted by \"soft\" increases. Furthermore, as shown in Fig.  4 (a), for Ω (5) 3/3 , which are utterances that all three annotators agree on the same emotion label, it is easier for \"hard\" to learn the target 0-1 distributions and has smaller KL divergence. For the Ω   improves the matching between the distributions and has considerably smaller KL divergence values.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Evaluating The Uncertainty Estimation In Emotion Prediction -Ω 1/3 Detection",
      "text": "As discussed in Section 4.4, Ω 1/3 detection experiments were conducted to assess the models' ability to estimate uncertainty. Both Max.P and Ent. were used as thresholds for AUPR measurement. The precision-recall (PR) curves for all four systems are shown in Fig.  5 . From the graphs, using Max.P and Ent. as thresholds yield similar trends. The dpn-kl system consistently outperforms all the other systems based on both measures of uncertainty in Ω 1/3 detection performance. The average value of Max.P and Ent. for different data groups are reported in Table  9  and illustrated in Fig.  6 . In general, when the emotion in the utterance gets more complex (with fewer annotators agreeing on the same emotion label), the average Max.P decreases and the average Ent. increases, showing that the systems predicts higher uncertainty of the emotion distribution. The prediction from \"hard\" has the least uncertainty as it is trained with one-hot labels. The high Max.P and low Ent. values of Ω 1/3 produced by \"hard\" indicate that this system can give incorrect predictions with high confidence when it encounters test utterances with complex emotions. The standard dpn system exhibits the most uncertainty by producing the lowest Max.P and the highest Ent., indicating that the Dirichlet prior predicted based on only about 3 hard labels introduces a large amount of uncertainty   in the estimation. However, such uncertainty plays a key role in the AER problem as it is difficult and expensive to have many reference labels for each utterance and a small number of labels are often insufficient to reflect the true underlying emotion distribution. The uncertainty is reduced by smoothing the Dirichlet samples with the MLE by incorporating an additional L kl term in the loss function.\n\nIt is worth noting that although \"dpn\" has higher average Ent. than \"dpn-kl\", its AUPR (Ent.) is still worse than \"dpnkl\".",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Further Analysis On Emotion Data Uncertainty",
      "text": "To understand the influence of the uncertainty in labels, a modified dpn-kl system (referred to as \"dpn-kl2\") was trained using a modified label setting. The labels of each Ω 2/3 and Ω 3/3 utterance were replaced by the same number of their corresponding majority unique hard label while the hard labels of Ω 1/3 utterances were kept the same (referred to as vote-and-replace). An example of this modified label setting is shown in Table  10 , and the results are given in Table  11 .\n\nWith vote-and-replace, the UA and WA of \"dpn-kl2\" increase and are closer to those of \"hard\". Compared to \"dpn-kl\", the entropy after voting decreases significantly. The entropy of \"dpn-kl2\" is even smaller than that of \"hard\", possibly because the number of the majority agreed labels remain unchanged in \"dpn-kl2\" while only one label is kept by \"hard\". Taking the first example in Table  10 , the label of \"dpn-kl2\" is 'AAAAA' while the label of \"hard\" is 'A'. Using the same label multiple times to represent the levels of confidence is an advantage of the DPN loss. Voteand-replace removes the uncertainty from the Ω 2/3 and Ω 3/3 utterances. This validates our motivation that the majority voting strategy considerably changes the uncertainty properties of the resulting model and should be avoided when constructing AER systems aimed at more general settings.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Table 10",
      "text": "Example of the vote-and-replace operation of the labels used for dpn-kl2 system. \"A\", \"B\", \"C\" denotes different emotion categories.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Original Label For Dpn-Kl:",
      "text": "A A A B C Majority:\n\nA Modified label for dpn-kl2: A A A A A Original label for dpn-kl:\n\nA B C Majority:\n\n--Modified label for dpn-kl2:\n\nA B C TABLE 11 Performance of dpn-kl2 system using labels after modified by the vote-and-replace operation. The tests were performed on Session 5 of IEMOCAP. \"↑\" denotes the higher the better, \"↓\" denotes the lower the better.  The distribution of emotion classes in MSP-Podcast is shown in Fig.  7 . Nearly 20% of the data doesn't have majority agreed labels, and the distribution is imbalanced among the emotion classes with neutral being the largest emotion class. These are common characteristics of datasets with natural emotions. To counteract the effect of class imbalance, the data was up-sampled by varying the overlap between the input windows. The standard splits for training, validation and testing were used in the experiments.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "System",
      "text": "The TSB-TAB structure was modified to be suitable for MSP-Podcast. Since reference transcriptions were not provided by MSP-Podcast, automatic speech recognition (ASR) results were used instead. We used an open public wav2vec 2.0 large model 3 to generate the speech transcriptions for MSP-Podcast, which has a word error rate (WER) of 2.2% on clean test set of Librispeech  [67]  (for regular speech) and 39.0% on IEMOCAP (for emotional speech) 4 . GloVe embeddings were not used here since the end-to-end ASR used here doesn't directly generate word alignments. Furthermore, the BERT-derived sentence embeddings were not used for the context utterances since each emotional utterance is presented separately without providing their surrounding context in MSP-Podcast. The TAB was therefore simplified to an FC layer with ReLU activation. This is a suitable setup for our proposed method that can be used when reference transcriptions and context utterances are not available.\n\nEmotion classification results are compared in Table  12  5 for both our setups and some from the literature. In the 4-way setups, only utterances with majority agreed label belonging to \"happy\", \"sad\", \"angry\", \"neutral\" were used. Emotion class \"disgust\" was included in the 5-way setups.\n\n3. https://huggingface.co/facebook/wav2vec2-large-960h-lv60 4. For comparison, the commercial Google Cloud Speech API has a WER of 44.6% on IEMOCAP.\n\n5. Note that the results are not directly comparable as different versions of MSP-Podcast dataset were used: papers  [55] ,  [69] ,  [70]  used release 1.4, papers  [68]  used release 1.7, and paper  [71]  used release 1.9. All utterances with majority label were used in the 9-way setup while \"other\" was excluded in the 8-way setup  [71] .\n\nAlthough our results are not directly comparable to those in the literature as different versions of MSP-Podcast dataset were used, our model produced competitive performance.\n\nThe degradation of classification results as the number of classes increases indicates the difficulty of fine-grained AER. The 9-way setup was used in the uncertainty estimation experiments which uses all emotion data. Label grouping was not performed here in order to retain the original labels without modification 6 . Emotion was then represented by a 9-dim categorical distribution. The experiments in Table  6  were repeated on MSP-Podcast, and the results shown in Table  13 . AUPR was computed by detecting utterances without majority agreed labels. Since each utterance in MSP-Podcast can be labelled by a varying number of annotators, this demonstrates the application of the AUPR metric in a general situation. Table  13  shows similar trend to Table  6  with \"hard\" system producing the highest classification accuracy and the \"soft\" system giving the smallest KL divergence. The \"dpn-kl\" system again produces the highest AUPR among all systems, showing its superior ability in emotion uncertainty estimation. Thus we validated the generality of our proposed methods to handle challenging realistic emotion data.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Conclusion",
      "text": "The paper proposes resolving the problem of disagreement in annotated hard labels for emotion classification from the perspective of Bayesian statistics. Instead of using majority voting to achieve a majority hard label that is used to build a classifier, the Dirichlet prior network training loss is applied 6. Grouping sentences doesn't affect labelling much in IEMOCAP as in IEMOCAP \"others\" is dominated by \"frustration\".\n\nto the task to better model the distribution of emotions. It preserves label uncertainty by maximising the likelihood of sampling all hard labels with inconsistent emotion classes from an utterance-specific Dirichlet distribution, which is predicted separately for each utterance with a neural network model. Given the fact that a large proportion of emotion data (e.g. in the IEMOCAP dataset) has significant interannotator disagreement, the proposed Bayesian framework also allows the detection of test utterances without majority unique labels based on two uncertainty estimation metrics, which is a more general setup than simply ignoring such data as in the traditional emotion classification framework. A novel combined loss function that interpolates the DPN loss with Kullback-Leibler loss has also been proposed, which not only has a more stable training performance but also results in improved uncertainty estimates. The findings were further validated using a larger real-life emotion dataset. Beyond emotion recognition, label uncertainty is a common issue in many human perception and understanding tasks, since golden references are often not well-defined due to the subjective evaluation of annotators. The proposed method could be applicable to other such tasks to handle the uncertainty in labels.\n\nWen Wu Wen Wu received the B.E degree from Fudan University and the MPhil degree from University of Cambridge. She is currently a PhD student at University of Cambridge supervised by Prof. Phil Woodland. Her research interests include audio-visual emotion recognition and Bayesian uncertainty estimation.\n\nDr. Chang Zhang Chao Zhang received his BE and MSc degrees in 2009 and 2012 both from the Department of Computer Science and Technology, Tsinghua University, and his PhD degree in 2017 from Cambridge University Engineering Department (CUED). He is currently an Assistant Professor at the Department of Electronic Engineering, Tsinghua University. Before that, he was a Senior Research Scientist at Google, a Research Associate at CUED, and an advisor and speech team co-leader of JD.com. His research interests include spoken language processing, machine learning, and cognitive neuroscience. He has published 70 peer-reviewed speech and language processing papers and received multiple paper awards. He is also a Visiting Fellow at CUED, and an Associate Member of the IEEE Speech and Language Processing Technical Committee.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Dr. Xixin Wu",
      "text": "",
      "page_start": 12,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , given the concentration parame-",
      "page": 5
    },
    {
      "caption": "Figure 1: Illustration of the DPN process. µ is a categorical distribution over",
      "page": 5
    },
    {
      "caption": "Figure 2: Proposed two-branch model structure.",
      "page": 7
    },
    {
      "caption": "Figure 3: Error bars showing the standard deviation across 5-fold for",
      "page": 8
    },
    {
      "caption": "Figure 3: Although the error bars of AUPR value",
      "page": 8
    },
    {
      "caption": "Figure 4: As utterances in Ω(5)",
      "page": 8
    },
    {
      "caption": "Figure 4: (b), “soft” has a higher entropy than",
      "page": 8
    },
    {
      "caption": "Figure 4: (a), for Ω(5)",
      "page": 8
    },
    {
      "caption": "Figure 4: Comparison of the “hard” system and the “soft” system in terms",
      "page": 9
    },
    {
      "caption": "Figure 6: In general, when the emotion in the utterance gets more",
      "page": 9
    },
    {
      "caption": "Figure 5: PR curves for the four systems using (a) Max.P and (b) Ent. as",
      "page": 9
    },
    {
      "caption": "Figure 6: Comparison of average Max.P (a) and Ent. (b) of different data",
      "page": 9
    },
    {
      "caption": "Figure 7: The distribution of emotion classes based on majority agreed",
      "page": 10
    },
    {
      "caption": "Figure 7: Nearly 20% of the data doesn’t have",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "synchronous branch (TSB)\nthat\nfocuses on modelling the",
          "2": "Despite the commonly assumed dimensions such as valence-"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "temporal\ncorrelations of multimodal\nfeatures\nand a\ntime",
          "2": "arousal and approach–avoidance [21], [22], there’s still a debate"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "asynchronous branch (TAB) that takes sentence embeddings",
          "2": "about the proper dimensional scheme and the orthogonality"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "as input\nto facilitate modelling meanings embedded in the",
          "2": "of the dimensions [23], [24], [25]."
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "text transcriptions. Experimental results on the widely used",
          "2": "The subjectivity of emotional perception further compli-"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "IEMOCAP dataset\n[7]\nshow that\nthe TSB-TAB structure",
          "2": "cates the problem of designing AER datasets. Despite the"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "achieves state-of-the-art classiﬁcation results in 4-way clas-",
          "2": "efforts of psychologists to de-correlate emotion dimensions,"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "siﬁcation (happy,\nsad,\nangry & neutral) when evaluated",
          "2": "creating intensity labels with continuous values or multiple"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "with all of\nthe\ncommonly used speaker-independent\ntest",
          "2": "discrete values can still be highly subjective and also lead"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "setups. The 4-way classiﬁcation is then extended to 5-way",
          "2": "to a high degree of uncertainty in the data.\nIn response to"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "classiﬁcation by including an extra emotion class “others”",
          "2": "this problem, most datasets were created using the strategy"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "which represents all\nthe other types of emotion labelled in",
          "2": "of having multiple human annotators to provide multiple"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "IEMOCAP but\nignored in the 4-way setup. Next, we rede-",
          "2": "labels for each utterance. The “ground truth” is then com-"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "ﬁne the task from classiﬁcation to distribution modelling by",
          "2": "monly deﬁned as the majority vote for discrete labels [7],"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "representing the emotions using a 5-dimensional distribu-",
          "2": "[9], [12], [26] or the mean for dimensional labels [7], [9], [27]."
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "tion rather than a single hard label. This allows utterances",
          "2": "When using the mean dimensional labels, the discrepancies"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "without majority agreed labels to also be considered. Uncer-",
          "2": "between annotators are ignored. Several approaches have"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "tainty estimation is performed by training the model with",
          "2": "been proposed to characterise\nthe\nsubjective property of"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "soft labels and the DPN training loss. Classiﬁcation accuracy",
          "2": "emotion perception by modelling the inter-annotator dis-"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "is no longer an appropriate evaluation metric when consid-",
          "2": "agreement\nlevel as\nthe\nstandard deviation of\nthe dimen-"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "ering uncertainty in emotion labelling. Instead, we propose",
          "2": "sional\nlabels,\nsuch as\nincluding a separate task to predict"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "evaluating the model performance in uncertainty estimation",
          "2": "the\nstandard deviation\nin\na multi-task\nframework\n[28],"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "in terms of the area under the precision-recall curve (AUPR)",
          "2": "[29],\nor predicting\nsuch\nvalues using Gaussian mixture"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "when detecting utterances without majority unique labels",
          "2": "regression models [30],\n[31]. Recently, alternative methods"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "at test-time. This also provides a more general two-step test",
          "2": "including Gaussian processes\n[32], generative variational"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "procedure for AER, which can detect utterances with high",
          "2": "auto-encoders [33], and Monte-Carlo dropout [34] have been"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "uncertainty in emotion for\nfurther processing and classify",
          "2": "applied to the problem without using the standard deviation"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "the\nremainder of\nthem into one of\nthe\nemotion classes.",
          "2": "of dimensional emotion labels as additional training labels."
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "Further experiments on the MSP-Podcast dataset, a larger",
          "2": "When using a majority vote to obtain the ground truth"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "speech corpus with natural emotions, demonstrate that our",
          "2": "for discrete class labelling,\nthe data without ground truth"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "proposed uncertainty estimation approach generalises\nto",
          "2": "due to annotators’ disagreement are usually discarded in"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "handling realistic emotion data.",
          "2": "classiﬁcation-based AER [35],\n[36],\n[37]. AER researchers"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "The rest of\nthe paper is organised as follows. Section 2",
          "2": "have proposed various methods to address the uncertainty"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "discusses work\non\nobjective\nemotion\nquantiﬁcation\nand",
          "2": "in emotion labelling caused by the inconsistency of emo-"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "related methods that address the inconsistency of emotion",
          "2": "tion perception among human annotators. Nediyanchath et"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "perception among human annotators. Section 3 presents an",
          "2": "al.\n[38] used multitask learning with gender\nor\nspeaker"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "analysis of\nthe IEMOCAP database and revisits the deﬁni-",
          "2": "classiﬁcation to model\nthe variations in personal aspects of"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "tion of emotion classiﬁcation task with IEMOCAP. Section 4",
          "2": "emotional expression. Lotﬁan et al. [39] proposed a multitask"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "introduces the soft-label-based and DPN-based approaches",
          "2": "learning\nframework\nto\nrecognise\nthe primary\nemotional"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "to emotion distribution modelling and uncertainty analysis.",
          "2": "class by leveraging extra information provided in the evalu-"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "The model structure and experiment setup are shown in Sec-",
          "2": "ations about secondary emotions. Ando et al. [40] estimated"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "tion 5. Section 6 presents the results and analysis for emotion",
          "2": "the existence of multi-label emotions as an auxiliary task to"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "classiﬁcation and distribution modelling on IEMOCAP. The",
          "2": "improve the recognition of the dominant emotions. Another"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "proposed approach is further veriﬁed with experiments on",
          "2": "type of commonly used method is to train AER models with"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "the MSP-Podcast dataset\nin Section 7,\nfollowed by conclu-",
          "2": "soft labels, which are derived as the mean of the hard labels"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "sions.",
          "2": "and can be\ninterpreted as\nthe\nintensities of\nthe\nemotion"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "2": "al.\nclasses\n[41]. Fayek et\n[42]\nincorporated inter-annotator"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "2": "variabilities\nby\ntraining\na\nseparate model\nbased on the"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "2\nRELATED WORK",
          "2": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "2": "hard labels produced by each annotator, and it was shown"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "The inherent ambiguity of emotion,\nresulting from mixed",
          "2": "that an ensemble of such models performed similarly to a"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "emotions\nand personal variations\nin emotion expression",
          "2": "single model\ntrained using the soft\nlabels. Although these"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "etc., makes\nit\nstill\nan\nopen\nquestion\nas\nhow to\ndeﬁne",
          "2": "approaches improved training with soft\nlabels, at\ntest-time"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "emotion for\neasier\nquantiﬁcation and objective\nanalysis.",
          "2": "the evaluations were only based on emotion classiﬁcation"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "Discrete emotion theory classiﬁes emotion into several basic",
          "2": "accuracy, which results\nin a major\ninconsistency between"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "categories\n(e.g. happy,\nsad,\nfear, anger\netc.)\n[16],\n[17],\n[18]",
          "2": "training and evaluation [43]."
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "while psychologists have also observed that\nthese distinct",
          "2": "This paper focuses on classiﬁcation-based AER. Rather"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "emotion categories overlap and have blurred boundaries be-",
          "2": "than trying to remove the uncertainty in the emotion rep-"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "tween them [19], [20]. Alternative methods were developed",
          "2": "resentation or to make emotion classes more separable, we"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "to\ncharacterise\nemotional\nstates\nby\nseveral\nfundamental",
          "2": "acknowledge the ambiguity in emotion expression and sub-"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "continuous-valued or multi-valued bipolar dimensions that",
          "2": "jectivity in emotion perception and evaluations, and model"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "are more suitable to be evaluated independently\n[21], [22].",
          "2": "the resulted uncertainty in emotion class labels using a novel"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 1: , although 7,532 utterances in IEMO- ‘AAA’) for only 24% (2,383 out of 10,309) of the utterances,",
      "data": [
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "Bayesian training loss.",
          "3": "TABLE 1"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "3": "Number of utterances associated with different classes of ground truth"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "3": "(majority unique) labels in IEMOCAP. The emotion class “excited” is"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "3": "merged into “happy”."
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "3\nEMOTION CLASSIFICATION WITH IEMOCAP",
          "3": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "The IEMOCAP [7] corpus is the primary corpus used in this",
          "3": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "3": "Anger\nHappiness\nSadness\nNeutral\nFrustration"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "paper. It is one of the most widely used datasets for verbal",
          "3": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "3": "1,103\n1,636\n1,084\n1,708\n1,849"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "emotion classiﬁcation and is designed with a typical data",
          "3": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "3": "Surprise\nFear\nDisgust\nOther\nTotal"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "annotation procedure. It consists of 5 dyadic conversational",
          "3": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "3": "107\n40\n2\n3\n7,532"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "sessions performed by 10 professional actors and the data",
          "3": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "includes\nthree different modalities\nincluding the\nspoken",
          "3": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "3": "Anger"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "audio,\ntext\ntranscription and the facial movements. There",
          "3": "15%\nFear"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "3": "Happiness"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "3": "Sadness\n0%"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "are in total 10,039 utterances and approximately 12 hours of",
          "3": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "3": "22%"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "3": "14%"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "data, with an average duration of 4.5 seconds per utterance.",
          "3": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "3": "Other"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "Each utterance was annotated by three human annotators",
          "3": "0%"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "3": "1%"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "3": "Disgust"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "for categorical labels (neutral, happiness, sadness, and anger",
          "3": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "3": "0%"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "etc.). Each annotator was allowed to tag more than one emo-",
          "3": "Neutral\nFrustration"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "3": "Surprise"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "tion category for each sentence if they perceived a mixture of",
          "3": "23%\n25%\n1%"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "emotions. The ground truth labels are determined by majority",
          "3": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "voting. However, since only 7,532 utterances have majority",
          "3": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "unique1 hard labels ,\nthe remaining 25% of\nthe utterances",
          "3": "However, for the utterances without majority unique labels,"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "are normally discarded. This issue and its solutions will be",
          "3": "that\ncomprise\n25% of\nIEMOCAP,\nclassiﬁcation\naccuracy"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "discussed later in detail in Sections 3.3 and 4. Although our",
          "3": "against a single reference cannot be used2. In fact, the utter-"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "analysis\nis primarily performed on IEMOCAP,\nthe issues",
          "3": "ances without majority unique labels often include complex"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "also existed in many other commonly used AER datasets,",
          "3": "and meaningful emotions, and are potentially important for"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "such as MSP-Podcast\n[10] whose analyses are provided in",
          "3": "training AER systems. Furthermore,\nsimilar data will be"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "Section 7.",
          "3": "encountered at\ntest\ntime\nand the\nsystem should also be"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "3": "evaluated on these\ntype of data.\nIn order\nto understand"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "3": "the problem better,\nthe following section performs a data"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "3.1\n4-way classiﬁcation",
          "3": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "3": "analysis."
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "To be consistent and comparable with previous studies [1],",
          "3": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "[35], [36], [37], [44], [45], [46], only utterances with majority",
          "3": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "3": "3.3\nData analysis"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "unique\nlabels belonging to “angry”,\n“happy”,\n“excited”,",
          "3": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "“sad”, and “neutral” were used for our 4-way classiﬁcation",
          "3": "Recall\nthat\nin IEMOCAP,\neach utterance was\nlabelled by"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "experiments. The “excited” class was merged with “happy”",
          "3": "three annotators and each annotator was allowed to give"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "to better balance the size of each emotion class, which results",
          "3": "multiple different labels to each utterance. Table 2 shows ex-"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "in a total of 5,531 utterances with 1,636, 1,103, 1,084, and",
          "3": "amples of typical situations for the hard labels provided by"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "1,708 utterances for happy, angry, sad and neutral\nrespec-",
          "3": "the annotators and Table 3 summarizes some of the statistics"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "tively. This approach is widely used but\nit discards 44% of",
          "3": "of\nIEMOCAP. There are 1,272 evaluations that have more"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "the data.",
          "3": "than one\nlabel,\nindicating the\nannotators were uncertain"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "3": "about the emotions when evaluating the utterance."
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "3": "When labels\nfrom different annotators are considered,"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "3.2\n5-way classiﬁcation",
          "3": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "3": "all annotators agreed on the same emotion class label\n(e.g."
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "As shown in Table 1, although 7,532 utterances in IEMO-",
          "3": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "3": "‘AAA’) for only 24% (2,383 out of 10,309) of the utterances,"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "CAP have majority unique labels, only 5,531 of\nthem are",
          "3": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "3": "which we denote\nas\n3/3\nagreement\nutterances.\nor Ω3/3"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "used in the 4-way classiﬁcation setup given in Section 3.1.",
          "3": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "3": "The rest of\nthe utterances consist of\nthose with agreement"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "This excludes all\nthe other classes of emotions,\nincluding",
          "3": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "3": "from two annotators, denoted as 2/3 agreement or Ω2/3"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "“frustration” which is the largest emotion class taking 25%",
          "3": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "3": "utterances (e.g.\n‘AAB’), and those without any agreement,"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "of the dataset, and therefore this partition only tackles part",
          "3": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "3": "referred to as no agreement or Ω≤1/3 utterances (e.g. ‘ABC’)."
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "of\nthe AER problem. To resolve this issue, we investigated",
          "3": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "3": "Note the case shown in the last row of Table 2. Although the"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "the use of an alternative 5-way classiﬁcation setup. An extra",
          "3": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "3": "labels have majority ‘AB’, the majority is not unique. Thus,"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "target of “others” was included as the 5-th class, to represent",
          "3": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "3": "the sentence belongs to Ω≤1/3."
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "all of\nthe other emotions\nthat exist\nin IEMOCAP includ-",
          "3": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "3": "When majority voting is applied, both 3/3 agreement"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "ing utterances labelled as “frustration”, “fear”, “surprise”,",
          "3": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "3": "and 2/3 agreement utterances result\nin the same majority"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "‘disgust”, and “other”. All 7,532 utterances with majority",
          "3": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "3": "unique ground truth label (e.g. ‘A’), despite that an annotator"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "unique\nlabels were used for\ntraining and test\nin 5-way",
          "3": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "3": "assigned different\nlabels\nto the 2/3 agreement utterances,"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "classiﬁcation.",
          "3": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "3": "which causes a loss of\nthe complexity and uncertainty in"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "As in most previous studies, our 4-way and 5-way clas-",
          "3": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "3": "emotion annotation. This problem is more\nsevere when"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "siﬁcation systems can be evaluated based on classiﬁcation",
          "3": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "3": "there isn’t a majority label and they are ignored completely"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "accuracy with the utterances with majority unique labels.",
          "3": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "3": "2. Multiple\nreference\nannotations\ncould be\ncounted as\ncorrect\nfor"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "1. Emotion category with highest votes was unique (notice that\nthe",
          "3": "scoring purposes. But\nit\nis clearly unsatisfactory when the number of"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "evaluators were allowed to tag more than one emotion category).",
          "3": "annotators increases while using a small number of emotion classes."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 2: ). For Ω 2/3 utterances, it avoids In Section 4.1, the average of the observed labels of an",
      "data": [
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "in both training and test. Considering the\nfact\nthat\n51%",
          "4": "K is\nthe number of\nemotion classes\nis\nthe k-th\nand ωk"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "of\nthe utterances\nare\n2/3\nagreement\nand 25% have\nno",
          "4": ", . . . , µ(Mn)\n}N\nclass. Let D = {xn, µ(1)\nn=1 be a dataset with"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "majority unique label, the strategy to use majority voting on",
          "4": "is\nthe\ninput\nfeatures of\nthe n-th\nN utterances where xn"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "labels\nsigniﬁcantly changes\nthe true emotion distribution.",
          "4": ", . . . , µ(Mn)\nutterance and µ(1)"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "4": "are its Mn labels provided by"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "To resolve these problems,\nin the next section, we propose",
          "4": "all annotators.\nIn this paper, µ(m)\nis a one-hot vector since"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "representing and modelling the emotion using a distribution",
          "4": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "4": "hard labels are annotated as in IEMOCAP. Such hard labels"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "rather than a single hard label.",
          "4": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "4": "can be considered as samples drawn from the underlying"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "4": "true emotion distribution ptr(µ|x). For brevity, the subscript"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "TABLE 2",
          "4": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "4": "n is dropped and the following analysis is based on a single"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "Examples of typical situations for IEMOCAP annotations. “e1”, “e2”,",
          "4": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "and “e3” are three annotators for an utterance. “A” “B” “C” are three",
          "4": "utterance x and its associated M labels {µ(1), . . . , µ(M )}."
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "different emotion classes. Ω3/3, Ω2/3, and Ω≤1/3 refer to the cases",
          "4": "One way to obtain the target emotion distribution µ for"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "that the annotations achieve 3/3, 2/3, and no unique agreement.\n‘AB’",
          "4": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "4": "each utterance is to use the MLE:"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "for e2 denotes the second evaluator gave two annotations ‘A’ and ‘B’\nto",
          "4": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "the utterance.",
          "4": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: ). For Ω 2/3 utterances, it avoids In Section 4.1, the average of the observed labels of an",
      "data": [
        {
          "TABLE 3": ""
        },
        {
          "TABLE 3": "utterance. An evaluation can include more than one labels if"
        },
        {
          "TABLE 3": "annotator is uncertain about\nthe emotions."
        },
        {
          "TABLE 3": ""
        },
        {
          "TABLE 3": ""
        },
        {
          "TABLE 3": "Number of total utterances\n10,039"
        },
        {
          "TABLE 3": ""
        },
        {
          "TABLE 3": "Number of total evaluations\n30,117"
        },
        {
          "TABLE 3": ""
        },
        {
          "TABLE 3": "Evaluation with more than one label\n1,272"
        },
        {
          "TABLE 3": "Utterance with more than three labels\n1,186"
        },
        {
          "TABLE 3": "Average number of labels per utterance\n3.12"
        },
        {
          "TABLE 3": "2,383\nNumber of Ω3/3 utterances"
        },
        {
          "TABLE 3": "5,149\nNumber of Ω2/3 utterances"
        },
        {
          "TABLE 3": "2,507\nNumber of Ω≤1/3 utterances"
        },
        {
          "TABLE 3": ""
        },
        {
          "TABLE 3": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "parameters α = [α1, . . . , αK]T. The Dirichlet distribution",
          "5": "to increase each αk predicted by the model by another small"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "Dir(µ|α) is deﬁned as",
          "5": "constant ε2 > 0 when calculating Dir( ˆµ(m)|fΛ(x)) based on"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "5": "Eqn. (3) [14]."
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "Γ (α0)",
          "5": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "Dir(µ|α) =\n,\nµαk−1",
          "5": "Comparing Eqn.\n(5)\nto Eqn.\n(2),\neach\nlabel µ(m)\nis"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "K(cid:89) k\n(3)\nk",
          "5": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "(cid:81)K",
          "5": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "k=1 Γ (αk)\n=1",
          "5": "taken into account separately when training a DPN, while"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "5": "only the averaged label\nµ is\nconsidered when training a"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "K(cid:88) k\nαk,\nαk > 0,\nα0 =",
          "5": "soft\nlabel system. For example,\ntwo observations ‘A’,‘B’,‘C’"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "=1",
          "5": "and ‘ABC’,‘ABC’,‘ABC’ yield the same soft\nlabel\nloss but"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "5": "different DPN losses. The latter case shows\nthat all\nthree"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "where Γ(·) is the gamma function deﬁned as",
          "5": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "5": "annotators are uncertain about\nthe emotion,\nindicating the"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "(cid:90) ∞",
          "5": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "5": "emotion of the utterance might have high degree of inherent"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "zαk−1e−z dz.\n(4)\nΓ (αk) =",
          "5": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "0",
          "5": "uncertainty. DPN training preserves the number of occur-"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "Hence, as shown in Fig. 1, given the concentration parame-",
          "5": "rences of each emotion class and allows an estimate of\nthe"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "ter α, the categorical distribution µ is a sample drawn from",
          "5": "conﬁdence of the uncertainty."
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "Dir(µ|α), and a 1-of-K hard label relevant\nto the emotion",
          "5": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "5": "4.2.2\nInference"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "is a sample drawn from µ. Here µ models\nthe\nclass ωk",
          "5": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "distribution over K emotion classes. Dir(µ|α) models the",
          "5": "The predictive distribution of\nthe DPN for an input x is"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "distribution of the emotion distribution µ.",
          "5": "given by marginalising over\nall possible\ncategorical dis-"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "5": "tributions, which is equivalent\nto the expected categorical"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "5": "distribution under the conditional Dirichlet prior:"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "5": "(cid:90)"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "but represents the conﬁdence encapsulated in the entire": ""
        },
        {
          "but represents the conﬁdence encapsulated in the entire": ""
        },
        {
          "but represents the conﬁdence encapsulated in the entire": "output distribution. That is,"
        },
        {
          "but represents the conﬁdence encapsulated in the entire": ""
        },
        {
          "but represents the conﬁdence encapsulated in the entire": "(cid:88)K"
        },
        {
          "but represents the conﬁdence encapsulated in the entire": "H [p (µ|x, Λ)] = −"
        },
        {
          "but represents the conﬁdence encapsulated in the entire": "(12)\np (ωk|x, Λ) ln p (ωk|x, Λ) ."
        },
        {
          "but represents the conﬁdence encapsulated in the entire": "k=1"
        },
        {
          "but represents the conﬁdence encapsulated in the entire": ""
        },
        {
          "but represents the conﬁdence encapsulated in the entire": ""
        },
        {
          "but represents the conﬁdence encapsulated in the entire": "A decision threshold is set based on the uncertainty measure"
        },
        {
          "but represents the conﬁdence encapsulated in the entire": "which determines whether\nthe test sample belongs to the"
        },
        {
          "but represents the conﬁdence encapsulated in the entire": "positive\nor negative\nclass. For\nexample, utterances with"
        },
        {
          "but represents the conﬁdence encapsulated in the entire": ""
        },
        {
          "but represents the conﬁdence encapsulated in the entire": "Max.P lower\nthan the threshold (or Ent. higher\nthan the"
        },
        {
          "but represents the conﬁdence encapsulated in the entire": ""
        },
        {
          "but represents the conﬁdence encapsulated in the entire": "threshold) are predicted as Ω≤1/3. A PR curve is obtained"
        },
        {
          "but represents the conﬁdence encapsulated in the entire": ""
        },
        {
          "but represents the conﬁdence encapsulated in the entire": "by calculating the precision and recall for different decision"
        },
        {
          "but represents the conﬁdence encapsulated in the entire": ""
        },
        {
          "but represents the conﬁdence encapsulated in the entire": "thresholds where the x-axis of a PR curve is the recall, the y-"
        },
        {
          "but represents the conﬁdence encapsulated in the entire": ""
        },
        {
          "but represents the conﬁdence encapsulated in the entire": "axis is the precision and the decision thresholds are implicit"
        },
        {
          "but represents the conﬁdence encapsulated in the entire": ""
        },
        {
          "but represents the conﬁdence encapsulated in the entire": ""
        },
        {
          "but represents the conﬁdence encapsulated in the entire": "and are not shown as a separate axis. The area under\nthe"
        },
        {
          "but represents the conﬁdence encapsulated in the entire": ""
        },
        {
          "but represents the conﬁdence encapsulated in the entire": "PR curve is computed as AUPR. Compared to classiﬁcation"
        },
        {
          "but represents the conﬁdence encapsulated in the entire": ""
        },
        {
          "but represents the conﬁdence encapsulated in the entire": "accuracy, AUPR can not only be applied to any test utterance"
        },
        {
          "but represents the conﬁdence encapsulated in the entire": ""
        },
        {
          "but represents the conﬁdence encapsulated in the entire": "but also quantify the model’s ability to estimate uncertainty."
        },
        {
          "but represents the conﬁdence encapsulated in the entire": "In Section 6.3, experiments\nthat assess uncertainty es-"
        },
        {
          "but represents the conﬁdence encapsulated in the entire": ""
        },
        {
          "but represents the conﬁdence encapsulated in the entire": ""
        },
        {
          "but represents the conﬁdence encapsulated in the entire": "timation ability by detecting utterances without majority"
        },
        {
          "but represents the conﬁdence encapsulated in the entire": ""
        },
        {
          "but represents the conﬁdence encapsulated in the entire": "agreed labels\nare\nreported. This detects whether\na\ntest"
        },
        {
          "but represents the conﬁdence encapsulated in the entire": ""
        },
        {
          "but represents the conﬁdence encapsulated in the entire": ""
        },
        {
          "but represents the conﬁdence encapsulated in the entire": "utterance\nbelongs\nbased on the model\noutput\nto Ω≤1/3"
        },
        {
          "but represents the conﬁdence encapsulated in the entire": "distributions with either Max.P or Ent.\nIn the detection"
        },
        {
          "but represents the conﬁdence encapsulated in the entire": ""
        },
        {
          "but represents the conﬁdence encapsulated in the entire": "is chosen as\nthe negative class, while\nexperiments, Ω≤1/3"
        },
        {
          "but represents the conﬁdence encapsulated in the entire": ""
        },
        {
          "but represents the conﬁdence encapsulated in the entire": "Ω2/3 and Ω3/3 are chosen as the positive classes. Detecting"
        },
        {
          "but represents the conﬁdence encapsulated in the entire": ""
        },
        {
          "but represents the conﬁdence encapsulated in the entire": "Ω≤1/3 was selected as the binary task for AUPR since this"
        },
        {
          "but represents the conﬁdence encapsulated in the entire": ""
        },
        {
          "but represents the conﬁdence encapsulated in the entire": "simulates such a real application case: if an Ω≤1/3 utterance"
        },
        {
          "but represents the conﬁdence encapsulated in the entire": ""
        },
        {
          "but represents the conﬁdence encapsulated in the entire": "is detected, the utterance may include ambiguous emotions"
        },
        {
          "but represents the conﬁdence encapsulated in the entire": ""
        },
        {
          "but represents the conﬁdence encapsulated in the entire": "that\nshould be\nevaluated by further models or humans."
        },
        {
          "but represents the conﬁdence encapsulated in the entire": ""
        },
        {
          "but represents the conﬁdence encapsulated in the entire": "Otherwise the utterance belongs to Ω2/3 or Ω3/3 where a"
        },
        {
          "but represents the conﬁdence encapsulated in the entire": ""
        },
        {
          "but represents the conﬁdence encapsulated in the entire": "majority unique label exists and emotion classiﬁcation can"
        },
        {
          "but represents the conﬁdence encapsulated in the entire": ""
        },
        {
          "but represents the conﬁdence encapsulated in the entire": "be applied."
        },
        {
          "but represents the conﬁdence encapsulated in the entire": ""
        },
        {
          "but represents the conﬁdence encapsulated in the entire": "5\nEXPERIMENTAL SETUP"
        },
        {
          "but represents the conﬁdence encapsulated in the entire": "Our experimental setup is given in this section,\nincluding"
        },
        {
          "but represents the conﬁdence encapsulated in the entire": "feature representations and model structures."
        },
        {
          "but represents the conﬁdence encapsulated in the entire": ""
        },
        {
          "but represents the conﬁdence encapsulated in the entire": ""
        },
        {
          "but represents the conﬁdence encapsulated in the entire": "5.1\nFeature representations"
        },
        {
          "but represents the conﬁdence encapsulated in the entire": ""
        },
        {
          "but represents the conﬁdence encapsulated in the entire": "The audio representation used for speech-based AER often"
        },
        {
          "but represents the conﬁdence encapsulated in the entire": ""
        },
        {
          "but represents the conﬁdence encapsulated in the entire": "includes\nlog-mel ﬁlterbank\nfeatures\n(FBKs)\n[52].\nIn\nthis"
        },
        {
          "but represents the conﬁdence encapsulated in the entire": ""
        },
        {
          "but represents the conﬁdence encapsulated in the entire": "paper, 40-dimensional\n(-d) FBK with a 10 ms\nframe shift"
        },
        {
          "but represents the conﬁdence encapsulated in the entire": ""
        },
        {
          "but represents the conﬁdence encapsulated in the entire": "and 25 ms frame length are used, which is denoted FBK25."
        },
        {
          "but represents the conﬁdence encapsulated in the entire": ""
        },
        {
          "but represents the conﬁdence encapsulated in the entire": "An additional\ntype of\nlong-term FBK feature is also used,"
        },
        {
          "but represents the conﬁdence encapsulated in the entire": ""
        },
        {
          "but represents the conﬁdence encapsulated in the entire": "from an\nwhich extracted in the same way as FBK25 apart"
        },
        {
          "but represents the conﬁdence encapsulated in the entire": ""
        },
        {
          "but represents the conﬁdence encapsulated in the entire": "long 250 ms frame length, and this is denoted FBK250. FBK"
        },
        {
          "but represents the conﬁdence encapsulated in the entire": ""
        },
        {
          "but represents the conﬁdence encapsulated in the entire": "features contain information about the short-term spectrum"
        },
        {
          "but represents the conﬁdence encapsulated in the entire": ""
        },
        {
          "but represents the conﬁdence encapsulated in the entire": "but don’t explicitly contain pitch information that can be"
        },
        {
          "but represents the conﬁdence encapsulated in the entire": ""
        },
        {
          "but represents the conﬁdence encapsulated in the entire": "important\nin describing emotional speech [53] and is often"
        },
        {
          "but represents the conﬁdence encapsulated in the entire": ""
        },
        {
          "but represents the conﬁdence encapsulated in the entire": "complementary to FBK features\n[54],\n[55]. Following [56],"
        },
        {
          "but represents the conﬁdence encapsulated in the entire": ""
        },
        {
          "but represents the conﬁdence encapsulated in the entire": "log pitch frequency\nfeatures with probability-of-voicing-"
        },
        {
          "but represents the conﬁdence encapsulated in the entire": ""
        },
        {
          "but represents the conﬁdence encapsulated in the entire": "weighted mean subtraction over a 1.5 second window are"
        },
        {
          "but represents the conﬁdence encapsulated in the entire": ""
        },
        {
          "but represents the conﬁdence encapsulated in the entire": "used along with FBK features."
        },
        {
          "but represents the conﬁdence encapsulated in the entire": ""
        },
        {
          "but represents the conﬁdence encapsulated in the entire": "Text features are also included in our models. Pre-trained"
        },
        {
          "but represents the conﬁdence encapsulated in the entire": ""
        },
        {
          "but represents the conﬁdence encapsulated in the entire": "50-d GloVe\nembeddings\nare used to\nencode word-level"
        },
        {
          "but represents the conﬁdence encapsulated in the entire": ""
        },
        {
          "but represents the conﬁdence encapsulated in the entire": "transcriptions [57], while the pre-trained BERT-based model"
        },
        {
          "but represents the conﬁdence encapsulated in the entire": ""
        },
        {
          "but represents the conﬁdence encapsulated in the entire": "without ﬁne-tuning is used to encode the transcription of"
        },
        {
          "but represents the conﬁdence encapsulated in the entire": ""
        },
        {
          "but represents the conﬁdence encapsulated in the entire": "each single utterance\ninto a 768-d vector\n[58]. Following"
        },
        {
          "but represents the conﬁdence encapsulated in the entire": ""
        },
        {
          "but represents the conﬁdence encapsulated in the entire": "prior work [35], [36], [37], the reference transcriptions from"
        },
        {
          "but represents the conﬁdence encapsulated in the entire": "IEMOCAP were used for the text modality."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 5: , the classification accuracy of the 5-",
      "data": [
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "7": "6.1\n4-way classiﬁcation and cross comparisons"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "Time synchronous branch\nTime asynchronous branch",
          "7": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "b(u+1)\nh(1) h(2)\nb(u)\nh(T)\nb(u-1)",
          "7": "To compare to previously published results on IEMOCAP,"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "Audio",
          "7": "the system was evaluated with all of\nthe commonly used"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "…",
          "7": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "BERT",
          "7": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "7": "training and test setups on IEMOCAP: training on Sessions"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "Glove",
          "7": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "7": "1-4 and testing on Session 5; 5-fold cross validation (CV)"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "7": "that leaves one of the 5 sessions out of training and used for"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "FC\nFC\nFC\nResTDNN",
          "7": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "7": "testing at each fold, and 10-fold CV that leaves one of the 10"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "7": "speakers out at each fold. These test setups show whether"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "Self-attentive\nSelf-attentive",
          "7": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "7": "the model\nis able to learn reliable and speaker independent"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "7": "features with the limited amount of\ntraining and test data"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "7": "in IEMOCAP. The results and modalities used in previous"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "7": "related work are\nsummarised and compared in Table\n4,"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "FCs",
          "7": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "7": "which shows that our 4-way classiﬁcation system achieved"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "7": "state-of-the-art results on IEMOCAP when evaluated with"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "7": "all of the three test settings. More detailed experiments and"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "7": "results can be found in [15]."
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "Fig. 2. Proposed two-branch model structure.",
          "7": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "7": "TABLE 4"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "7": "Summary of 4-way classiﬁcation results on IEMOCAP in the literature."
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "5.2\nModel structure",
          "7": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "7": "“A”, “T”, and “V” refer to the audio,\ntext, and video modalities"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "7": "respectively."
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "The proposed model\nstructure is\nshown in Fig. 2, which",
          "7": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "consists of a time synchronous branch (TSB)\nthat\nfuses the",
          "7": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 5: , the classification accuracy of the 5-",
      "data": [
        {
          "Summary of 4-way classiﬁcation results on IEMOCAP in the literature.": ""
        },
        {
          "Summary of 4-way classiﬁcation results on IEMOCAP in the literature.": "“A”, “T”, and “V” refer to the audio,\ntext, and video modalities"
        },
        {
          "Summary of 4-way classiﬁcation results on IEMOCAP in the literature.": "respectively."
        },
        {
          "Summary of 4-way classiﬁcation results on IEMOCAP in the literature.": ""
        },
        {
          "Summary of 4-way classiﬁcation results on IEMOCAP in the literature.": ""
        },
        {
          "Summary of 4-way classiﬁcation results on IEMOCAP in the literature.": "Paper\nModality\nTest Setting\nWA (%)\nUA (%)"
        },
        {
          "Summary of 4-way classiﬁcation results on IEMOCAP in the literature.": "[35]\nA+T+V\nSession 5\n71.04\n–"
        },
        {
          "Summary of 4-way classiﬁcation results on IEMOCAP in the literature.": "[36]\nA+T+V\nSession 5\n76.5\n–"
        },
        {
          "Summary of 4-way classiﬁcation results on IEMOCAP in the literature.": "[44]\nA+T\n5-fold CV\n72.39\n70.08"
        },
        {
          "Summary of 4-way classiﬁcation results on IEMOCAP in the literature.": ""
        },
        {
          "Summary of 4-way classiﬁcation results on IEMOCAP in the literature.": "[45]\nA+T\n5-fold CV\n71.06\n72.05"
        },
        {
          "Summary of 4-way classiﬁcation results on IEMOCAP in the literature.": ""
        },
        {
          "Summary of 4-way classiﬁcation results on IEMOCAP in the literature.": "[46]\nA+T\n5-fold CV\n73.0\n73.5"
        },
        {
          "Summary of 4-way classiﬁcation results on IEMOCAP in the literature.": "[37]\nA+T+V\n10-fold CV\n76.1\n–"
        },
        {
          "Summary of 4-way classiﬁcation results on IEMOCAP in the literature.": ""
        },
        {
          "Summary of 4-way classiﬁcation results on IEMOCAP in the literature.": "83.08\n83.22\nours\nA+T\nSession 5"
        },
        {
          "Summary of 4-way classiﬁcation results on IEMOCAP in the literature.": "77.57±3.89\n78.41±3.14\nours\nA+T\n5-fold CV"
        },
        {
          "Summary of 4-way classiﬁcation results on IEMOCAP in the literature.": "77.76±4.94\n78.30±3.97\nours\nA+T\n10-fold CV"
        },
        {
          "Summary of 4-way classiﬁcation results on IEMOCAP in the literature.": ""
        },
        {
          "Summary of 4-way classiﬁcation results on IEMOCAP in the literature.": ""
        },
        {
          "Summary of 4-way classiﬁcation results on IEMOCAP in the literature.": ""
        },
        {
          "Summary of 4-way classiﬁcation results on IEMOCAP in the literature.": "6.2\n5-way classiﬁcation"
        },
        {
          "Summary of 4-way classiﬁcation results on IEMOCAP in the literature.": ""
        },
        {
          "Summary of 4-way classiﬁcation results on IEMOCAP in the literature.": "As\nshown in Table 5,\nthe classiﬁcation accuracy of\nthe 5-"
        },
        {
          "Summary of 4-way classiﬁcation results on IEMOCAP in the literature.": ""
        },
        {
          "Summary of 4-way classiﬁcation results on IEMOCAP in the literature.": "way system using 5-fold CV on the previous four emotions"
        },
        {
          "Summary of 4-way classiﬁcation results on IEMOCAP in the literature.": ""
        },
        {
          "Summary of 4-way classiﬁcation results on IEMOCAP in the literature.": "(happy,\nsad, neutral, angry) was 72.47% WA and 74.29%"
        },
        {
          "Summary of 4-way classiﬁcation results on IEMOCAP in the literature.": ""
        },
        {
          "Summary of 4-way classiﬁcation results on IEMOCAP in the literature.": "UA. A 4.65% decrease was observed compared with the"
        },
        {
          "Summary of 4-way classiﬁcation results on IEMOCAP in the literature.": ""
        },
        {
          "Summary of 4-way classiﬁcation results on IEMOCAP in the literature.": "results of\nthe 4-way system. On the other hand, since the"
        },
        {
          "Summary of 4-way classiﬁcation results on IEMOCAP in the literature.": ""
        },
        {
          "Summary of 4-way classiﬁcation results on IEMOCAP in the literature.": "4-way system cannot correctly classify examples from the"
        },
        {
          "Summary of 4-way classiﬁcation results on IEMOCAP in the literature.": ""
        },
        {
          "Summary of 4-way classiﬁcation results on IEMOCAP in the literature.": "“others” class,\nthe overall classiﬁcation accuracy of\nthe 4-"
        },
        {
          "Summary of 4-way classiﬁcation results on IEMOCAP in the literature.": ""
        },
        {
          "Summary of 4-way classiﬁcation results on IEMOCAP in the literature.": "way system drops dramatically to 57.02% WA and 62.72%"
        },
        {
          "Summary of 4-way classiﬁcation results on IEMOCAP in the literature.": ""
        },
        {
          "Summary of 4-way classiﬁcation results on IEMOCAP in the literature.": "UA when tested on the 5-way data."
        },
        {
          "Summary of 4-way classiﬁcation results on IEMOCAP in the literature.": ""
        },
        {
          "Summary of 4-way classiﬁcation results on IEMOCAP in the literature.": ""
        },
        {
          "Summary of 4-way classiﬁcation results on IEMOCAP in the literature.": "TABLE 5"
        },
        {
          "Summary of 4-way classiﬁcation results on IEMOCAP in the literature.": "Comparison of 4-way and 5-way classiﬁcation system using 5-fold CV"
        },
        {
          "Summary of 4-way classiﬁcation results on IEMOCAP in the literature.": ""
        },
        {
          "Summary of 4-way classiﬁcation results on IEMOCAP in the literature.": "on IEMOCAP."
        },
        {
          "Summary of 4-way classiﬁcation results on IEMOCAP in the literature.": ""
        },
        {
          "Summary of 4-way classiﬁcation results on IEMOCAP in the literature.": ""
        },
        {
          "Summary of 4-way classiﬁcation results on IEMOCAP in the literature.": "4-way system\nWA(%)\nUA(%)"
        },
        {
          "Summary of 4-way classiﬁcation results on IEMOCAP in the literature.": ""
        },
        {
          "Summary of 4-way classiﬁcation results on IEMOCAP in the literature.": "results of 4 classes\n77.57±3.89\n78.41±3.14"
        },
        {
          "Summary of 4-way classiﬁcation results on IEMOCAP in the literature.": ""
        },
        {
          "Summary of 4-way classiﬁcation results on IEMOCAP in the literature.": "results of 5 classes\n57.02±4.23\n62.72±2.97"
        },
        {
          "Summary of 4-way classiﬁcation results on IEMOCAP in the literature.": "5-way system\nWA(%)\nUA(%)"
        },
        {
          "Summary of 4-way classiﬁcation results on IEMOCAP in the literature.": ""
        },
        {
          "Summary of 4-way classiﬁcation results on IEMOCAP in the literature.": "results of 4 classes\n72.47±5.12\n74.29±4.90"
        },
        {
          "Summary of 4-way classiﬁcation results on IEMOCAP in the literature.": "results of 5 classes\n77.67±4.29\n77.74±4.18"
        },
        {
          "Summary of 4-way classiﬁcation results on IEMOCAP in the literature.": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 7: This section",
      "data": [
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "•\n“soft”: A soft\nlabel system trained by minimising Lkl",
          "8": "(sharper output distributions) as\nit\nis\ntrained to learn 0-1"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "between soft labels and the prediction in Section 4.1;",
          "8": "distributions. It is widely known [65], [66] by deep learning"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "•\n“dpn”: A standard DPN system described in Section 4.2",
          "8": "researchers that such deep model hard label classiﬁcation"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "and trained with Ldpn with ε1 = 1 × 10−2 and ε2 =",
          "8": "systems are often “over-conﬁdent”, meaning that the model"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "1 × 10−8;",
          "8": "can have poor uncertainty estimation ability as indicated by"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "•\ndescribed in\n“dpn-kl”: A system trained on Ldpn-kl",
          "8": "the AUPR metrics. Similarly,\nit\nis\nreasonable that\nthe soft"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "Section 4.3 where ε1 and ε2 are set to 0, and the weight",
          "8": "system has the best KL divergence as it\nis trained to min-"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "term is set to 20.0.\nλ to scale the Lkl",
          "8": "imise such an objective. However, the KL divergence is also"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "8": "not\nthe most suitable metric here as it does not distinguish"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "The “hard” system was trained on 5-way classiﬁcation using",
          "8": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "8": "between the ‘AB’ and ‘AAABBB’ cases as discussed in Sec-"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "the\nthe\ntraining data belonging to Ω2/3\nand Ω3/3, while",
          "8": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "8": "tion 4.2.1. Comparing “dpn” to “soft”, “dpn” ranks better on"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "other\nsystems were all\ntrained using all\nthe utterances\nin",
          "8": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "8": "AUPR (Max.P) while “soft” is better on AUPR (Ent.). “dpn-"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "the training set. All systems were ﬁrst evaluated using the 5-",
          "8": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "8": "kl” however, outperforms “dpn” on all evaluation metrics"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "way classiﬁcation accuracy on Ω2/3 and Ω3/3 test utterances,",
          "8": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "8": "and produces the highest AUPR among all systems. It also"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "and then evaluated on all test utterances using the average",
          "8": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "8": "achieves a balance between the “hard” and “soft” systems,"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "KL divergence,\nentropy,\nand AUPR (Max.P)\nand AUPR",
          "8": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "8": "yielding higher UA and WA than “soft” and a smaller KL"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "(Ent.).",
          "8": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "8": "divergence than “hard”. The standard deviation across folds"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "TABLE 6",
          "8": "are shown in Fig. 3. Although the error bars of AUPR value"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "The average of 5-fold CV AER results for uncertainty estimation",
          "8": "contain overlap among the systems,\nthe “dpn-kl” system"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "experiments on IEMOCAP. The WA and UA classiﬁcation accuracy",
          "8": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "8": "consistently outperforms the others in all folds."
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "the other\nresults were obtained using Ω≥2/3 data in the test sets. All",
          "8": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "results were computed on the whole test sets. “↑” denotes the higher",
          "8": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "the better, “↓” denotes the lower the better.",
          "8": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "8": "6.4\nFurther experiments for analysis"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 7: This section",
      "data": [
        {
          "results were obtained using Ω≥2/3 data in the test sets. All": "",
          "the other": ""
        },
        {
          "results were obtained using Ω≥2/3 data in the test sets. All": "the better, “↓” denotes the lower the better.",
          "the other": ""
        },
        {
          "results were obtained using Ω≥2/3 data in the test sets. All": "",
          "the other": ""
        },
        {
          "results were obtained using Ω≥2/3 data in the test sets. All": "",
          "the other": ""
        },
        {
          "results were obtained using Ω≥2/3 data in the test sets. All": "hard",
          "the other": "dpn-kl"
        },
        {
          "results were obtained using Ω≥2/3 data in the test sets. All": "",
          "the other": ""
        },
        {
          "results were obtained using Ω≥2/3 data in the test sets. All": "77.67",
          "the other": "75.78"
        },
        {
          "results were obtained using Ω≥2/3 data in the test sets. All": "",
          "the other": ""
        },
        {
          "results were obtained using Ω≥2/3 data in the test sets. All": "77.74",
          "the other": "74.76"
        },
        {
          "results were obtained using Ω≥2/3 data in the test sets. All": "",
          "the other": ""
        },
        {
          "results were obtained using Ω≥2/3 data in the test sets. All": "0.7634",
          "the other": "0.5580"
        },
        {
          "results were obtained using Ω≥2/3 data in the test sets. All": "0.7065",
          "the other": "1.0957"
        },
        {
          "results were obtained using Ω≥2/3 data in the test sets. All": "0.7973",
          "the other": "0.8468"
        },
        {
          "results were obtained using Ω≥2/3 data in the test sets. All": "",
          "the other": ""
        },
        {
          "results were obtained using Ω≥2/3 data in the test sets. All": "0.7950",
          "the other": "0.8563"
        },
        {
          "results were obtained using Ω≥2/3 data in the test sets. All": "",
          "the other": ""
        },
        {
          "results were obtained using Ω≥2/3 data in the test sets. All": "",
          "the other": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 7: This section",
      "data": [
        {
          "labels with emotion distributions on the uncertainty of emo-": ""
        },
        {
          "labels with emotion distributions on the uncertainty of emo-": "tion prediction for different data groups by comparing the"
        },
        {
          "labels with emotion distributions on the uncertainty of emo-": "“hard” system to the “soft” system. Then the performance"
        },
        {
          "labels with emotion distributions on the uncertainty of emo-": "of all four systems on detecting the test utterances with high"
        },
        {
          "labels with emotion distributions on the uncertainty of emo-": ""
        },
        {
          "labels with emotion distributions on the uncertainty of emo-": "labelling uncertainty is presented."
        },
        {
          "labels with emotion distributions on the uncertainty of emo-": ""
        },
        {
          "labels with emotion distributions on the uncertainty of emo-": ""
        },
        {
          "labels with emotion distributions on the uncertainty of emo-": ""
        },
        {
          "labels with emotion distributions on the uncertainty of emo-": ""
        },
        {
          "labels with emotion distributions on the uncertainty of emo-": ""
        },
        {
          "labels with emotion distributions on the uncertainty of emo-": ""
        },
        {
          "labels with emotion distributions on the uncertainty of emo-": ""
        },
        {
          "labels with emotion distributions on the uncertainty of emo-": ""
        },
        {
          "labels with emotion distributions on the uncertainty of emo-": ""
        },
        {
          "labels with emotion distributions on the uncertainty of emo-": ""
        },
        {
          "labels with emotion distributions on the uncertainty of emo-": ""
        },
        {
          "labels with emotion distributions on the uncertainty of emo-": ""
        },
        {
          "labels with emotion distributions on the uncertainty of emo-": ""
        },
        {
          "labels with emotion distributions on the uncertainty of emo-": ""
        },
        {
          "labels with emotion distributions on the uncertainty of emo-": ""
        },
        {
          "labels with emotion distributions on the uncertainty of emo-": "Data group"
        },
        {
          "labels with emotion distributions on the uncertainty of emo-": ""
        },
        {
          "labels with emotion distributions on the uncertainty of emo-": ""
        },
        {
          "labels with emotion distributions on the uncertainty of emo-": "# of sentences"
        },
        {
          "labels with emotion distributions on the uncertainty of emo-": ""
        },
        {
          "labels with emotion distributions on the uncertainty of emo-": ""
        },
        {
          "labels with emotion distributions on the uncertainty of emo-": ""
        },
        {
          "labels with emotion distributions on the uncertainty of emo-": ""
        },
        {
          "labels with emotion distributions on the uncertainty of emo-": ""
        },
        {
          "labels with emotion distributions on the uncertainty of emo-": "The performance of"
        },
        {
          "labels with emotion distributions on the uncertainty of emo-": ""
        },
        {
          "labels with emotion distributions on the uncertainty of emo-": "ferent data groups are given in Table 8 and illustrated in"
        },
        {
          "labels with emotion distributions on the uncertainty of emo-": "Fig. 4. As utterances in Ω(5)"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 9: and illustrated in Fig. 6.",
      "data": [
        {
          "TABLE 8": ""
        },
        {
          "TABLE 8": ""
        },
        {
          "TABLE 8": ""
        },
        {
          "TABLE 8": ""
        },
        {
          "TABLE 8": ""
        },
        {
          "TABLE 8": ""
        },
        {
          "TABLE 8": "Entropy"
        },
        {
          "TABLE 8": ""
        },
        {
          "TABLE 8": ""
        },
        {
          "TABLE 8": "0.6030"
        },
        {
          "TABLE 8": "0.9670"
        },
        {
          "TABLE 8": ""
        },
        {
          "TABLE 8": "Entropy"
        },
        {
          "TABLE 8": ""
        },
        {
          "TABLE 8": ""
        },
        {
          "TABLE 8": "0.7449"
        },
        {
          "TABLE 8": "1.0489"
        },
        {
          "TABLE 8": ""
        },
        {
          "TABLE 8": "Entropy"
        },
        {
          "TABLE 8": ""
        },
        {
          "TABLE 8": "0.6355"
        },
        {
          "TABLE 8": ""
        },
        {
          "TABLE 8": "1.1359"
        },
        {
          "TABLE 8": ""
        },
        {
          "TABLE 8": ""
        },
        {
          "TABLE 8": ""
        },
        {
          "TABLE 8": ""
        },
        {
          "TABLE 8": ""
        },
        {
          "TABLE 8": ""
        },
        {
          "TABLE 8": ""
        },
        {
          "TABLE 8": ""
        },
        {
          "TABLE 8": ""
        },
        {
          "TABLE 8": ""
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 9: and illustrated in Fig. 6.",
      "data": [
        {
          "considerably smaller KL divergence values.": ""
        },
        {
          "considerably smaller KL divergence values.": ""
        },
        {
          "considerably smaller KL divergence values.": "6.4.2\nEvaluating the uncertainty estimation in emotion pre-"
        },
        {
          "considerably smaller KL divergence values.": "diction – Ω1/3 detection"
        },
        {
          "considerably smaller KL divergence values.": ""
        },
        {
          "considerably smaller KL divergence values.": ""
        },
        {
          "considerably smaller KL divergence values.": "As discussed in Section 4.4, Ω1/3 detection experiments"
        },
        {
          "considerably smaller KL divergence values.": ""
        },
        {
          "considerably smaller KL divergence values.": "were\nconducted to assess\nthe models’ ability to estimate"
        },
        {
          "considerably smaller KL divergence values.": ""
        },
        {
          "considerably smaller KL divergence values.": "uncertainty. Both Max.P and Ent. were used as conﬁdence"
        },
        {
          "considerably smaller KL divergence values.": ""
        },
        {
          "considerably smaller KL divergence values.": "thresholds\nfor AUPR measurement.\nThe\nprecision-recall"
        },
        {
          "considerably smaller KL divergence values.": "(PR) curves for all\nfour systems are shown in Fig. 5. From"
        },
        {
          "considerably smaller KL divergence values.": ""
        },
        {
          "considerably smaller KL divergence values.": "the graphs, using Max.P and Ent. as thresholds yield similar"
        },
        {
          "considerably smaller KL divergence values.": "trends. The dpn-kl system consistently outperforms all\nthe"
        },
        {
          "considerably smaller KL divergence values.": "other systems based on both measures of uncertainty in Ω1/3"
        },
        {
          "considerably smaller KL divergence values.": ""
        },
        {
          "considerably smaller KL divergence values.": "detection performance."
        },
        {
          "considerably smaller KL divergence values.": ""
        },
        {
          "considerably smaller KL divergence values.": "The average value of Max.P and Ent.\nfor different data"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 10: , and the results are given in",
      "data": [
        {
          "Each utterance was labelled by at least 5 human annotators": "and has an average of 6.7 annotations per utterance."
        },
        {
          "Each utterance was labelled by at least 5 human annotators": ""
        },
        {
          "Each utterance was labelled by at least 5 human annotators": ""
        },
        {
          "Each utterance was labelled by at least 5 human annotators": ""
        },
        {
          "Each utterance was labelled by at least 5 human annotators": "19.4%"
        },
        {
          "Each utterance was labelled by at least 5 human annotators": ""
        },
        {
          "Each utterance was labelled by at least 5 human annotators": ""
        },
        {
          "Each utterance was labelled by at least 5 human annotators": ""
        },
        {
          "Each utterance was labelled by at least 5 human annotators": "5.6%"
        },
        {
          "Each utterance was labelled by at least 5 human annotators": ""
        },
        {
          "Each utterance was labelled by at least 5 human annotators": ""
        },
        {
          "Each utterance was labelled by at least 5 human annotators": "3.6%"
        },
        {
          "Each utterance was labelled by at least 5 human annotators": ""
        },
        {
          "Each utterance was labelled by at least 5 human annotators": "1.1%"
        },
        {
          "Each utterance was labelled by at least 5 human annotators": ""
        },
        {
          "Each utterance was labelled by at least 5 human annotators": ""
        },
        {
          "Each utterance was labelled by at least 5 human annotators": ""
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 10: , and the results are given in",
      "data": [
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "in the estimation. However,\nsuch uncertainty plays a key",
          "10": "7\nEXPERIMENTS ON MSP-PODCAST"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "role\nin the AER problem as\nit\nis difﬁcult and expensive",
          "10": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "10": "This section presents our experiments on MSP-Podcast [10],"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "to have many reference\nlabels\nfor\neach utterance\nand a",
          "10": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "10": "a\nlarger dataset with natural\nemotional\nspeech data,\nto"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "small number of\nlabels are often insufﬁcient\nto reﬂect\nthe",
          "10": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "10": "validate the generalisation ability of our proposed method"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "true underlying emotion distribution. The uncertainty is",
          "10": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "10": "and the reliability of our ﬁndings. MSP-Podcast\ncontains"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "reduced by smoothing the Dirichlet samples with the MLE",
          "10": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "10": "natural English speech from podcast recordings. Release 1.8"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "by incorporating an additional Lkl term in the loss function.",
          "10": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "10": "was used in this paper which contains 73,042 utterances"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "It\nis worth noting that although “dpn” has higher average",
          "10": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "10": "from 1,285 speakers amounting to more than 110 hours of"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "Ent. than “dpn-kl”, its AUPR (Ent.) is still worse than “dpn-",
          "10": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "10": "speech. The corpus was annotated using crowd-sourcing."
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "kl”.",
          "10": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "10": "Each utterance was labelled by at least 5 human annotators"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "10": "and has an average of 6.7 annotations per utterance."
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "6.4.3\nFurther analysis on emotion data uncertainty",
          "10": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "10": "4.4%"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "To understand the\ninﬂuence of\nthe uncertainty in labels,",
          "10": "5.0%"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "10": "19.4%"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "10": "3.6%"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "a modiﬁed dpn-kl\nsystem (referred to as “dpn-kl2”) was",
          "10": "Angry\n Contempt\n2.1%"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "trained using a modiﬁed label\nsetting. The labels of each",
          "10": "Disgust\n Fear"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "10": "5.6%"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "Ω2/3 and Ω3/3 utterance were replaced by the same number",
          "10": "Happy\n Neutral"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "10": "19.6%"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "10": "3.6%"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "of their corresponding majority unique hard label while the",
          "10": "Other\n Sad"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "10": "1.1%"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "hard labels of Ω1/3 utterances were kept the same (referred",
          "10": "Surprise\nNo agreement"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "10": "35.6%"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "to as vote-and-replace). An example of\nthis modiﬁed label",
          "10": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "setting is\nshown in Table 10, and the results are given in",
          "10": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "Table 11.",
          "10": "Fig. 7. The distribution of emotion classes based on majority agreed"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "10": "labels in MSP-Podcast."
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "With vote-and-replace,\nthe UA and WA of\n“dpn-kl2”",
          "10": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "increase and are\ncloser\nto those of “hard”. Compared to",
          "10": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "10": "The distribution of emotion classes\nin MSP-Podcast\nis"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "“dpn-kl”,\nthe entropy after voting decreases\nsigniﬁcantly.",
          "10": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "10": "shown\nin Fig.\n7. Nearly\n20% of\nthe data doesn’t have"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "The\nentropy\nof\n“dpn-kl2”\nis\neven\nsmaller\nthan\nthat\nof",
          "10": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "10": "majority agreed labels, and the distribution is imbalanced"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "“hard”, possibly because the number of the majority agreed",
          "10": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "10": "among the emotion classes with neutral being the largest"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "labels remain unchanged in “dpn-kl2” while only one label",
          "10": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "10": "emotion class. These are common characteristics of datasets"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "is kept by “hard”. Taking the ﬁrst example in Table 10,\nthe",
          "10": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "10": "with natural\nemotions. To\ncounteract\nthe\neffect\nof\nclass"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "label of “dpn-kl2” is\n‘AAAAA’ while the label of “hard”",
          "10": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "10": "imbalance, the data was up-sampled by varying the overlap"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "is ‘A’. Using the same label multiple times to represent\nthe",
          "10": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "10": "between the input windows. The standard splits for train-"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "levels of conﬁdence is an advantage of the DPN loss. Vote-",
          "10": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "10": "ing, validation and testing were used in the experiments."
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "and-replace removes the uncertainty from the Ω2/3 and Ω3/3",
          "10": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "10": "The TSB-TAB structure was modiﬁed to\nbe\nsuitable"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "utterances. This validates our motivation that\nthe majority",
          "10": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "10": "for MSP-Podcast. Since\nreference\ntranscriptions were not"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "voting strategy considerably changes the uncertainty prop-",
          "10": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "10": "provided by MSP-Podcast,\nautomatic\nspeech recognition"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "erties of\nthe resulting model and should be avoided when",
          "10": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "10": "(ASR)\nresults were used instead. We used an open public"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "constructing AER systems aimed at more general settings.",
          "10": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "10": "wav2vec 2.0 large model3\nto generate the speech transcrip-"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "10": "tions for MSP-Podcast, which has a word error rate (WER)"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 6: Wen Wu is supported by a Cambridge International Schol-",
      "data": [
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "TABLE 12",
          "11": "to the task to better model\nthe distribution of emotions.\nIt"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "AER results for classiﬁcation experiments on MSP-Podcast.",
          "11": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "11": "preserves label uncertainty by maximising the likelihood of"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "11": "sampling all hard labels with inconsistent emotion classes"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "Paper\nSetting\nModality\nWA(%)\nUA (%)\nWeighted F1",
          "11": "from an utterance-speciﬁc Dirichlet distribution, which is"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "[68]\n4-way\nA\n59.7\n48.6\n-",
          "11": "predicted separately for each utterance with a neural net-"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "[69]\n4-way\nA+T\n-\n59.1\n-",
          "11": "work model. Given the fact that a large proportion of emo-"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "[70]\n5-way\nA\n-\n-\n0.5868",
          "11": "tion data (e.g. in the IEMOCAP dataset) has signiﬁcant inter-"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "[55]\n5-way\nA\n-\n-\n0.5846",
          "11": "annotator disagreement, the proposed Bayesian framework"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "[71]\n8-way\nA\n-\n15.6\n0.347",
          "11": "also allows the detection of test utterances without majority"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "11": "unique labels based on two uncertainty estimation metrics,"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "ours\n4-way\nA+T\n61.85\n60.63\n0.6409",
          "11": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "ours\n5-way\nA+T\n55.43\n51.97\n0.6066",
          "11": "which is a more general setup than simply ignoring such"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "ours\n9-way\nA+T\n36.93\n32.84\n0.3889",
          "11": "data as in the traditional emotion classiﬁcation framework."
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "11": "A novel combined loss function that\ninterpolates the DPN"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "11": "loss with Kullback-Leibler\nloss has\nalso\nbeen proposed,"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "All utterances with majority label were used in the 9-way",
          "11": "which not only has a more stable training performance but"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "setup while “other” was excluded in the 8-way setup [71].",
          "11": "also results\nin improved uncertainty estimates. The ﬁnd-"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "Although our results are not directly comparable to those in",
          "11": "ings were further validated using a larger real-life emotion"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "the literature as different versions of MSP-Podcast dataset",
          "11": "dataset. Beyond emotion recognition,\nlabel uncertainty is a"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "were used, our model produced competitive performance.",
          "11": "common issue in many human perception and understand-"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "The degradation of classiﬁcation results as the number of",
          "11": "ing tasks, since golden references are often not well-deﬁned"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "classes increases indicates the difﬁculty of ﬁne-grained AER.",
          "11": "due to the subjective evaluation of annotators. The proposed"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "The 9-way setup was used in the uncertainty estimation",
          "11": "method could be applicable to other such tasks to handle the"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "experiments which uses all emotion data. Label grouping",
          "11": "uncertainty in labels."
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 6: Wen Wu is supported by a Cambridge International Schol-",
      "data": [
        {
          "Dallas.": ""
        },
        {
          "Dallas.": ""
        },
        {
          "Dallas.": ""
        },
        {
          "Dallas.": "REFERENCES"
        },
        {
          "Dallas.": "[1]"
        },
        {
          "Dallas.": ""
        },
        {
          "Dallas.": ""
        },
        {
          "Dallas.": ""
        },
        {
          "Dallas.": ""
        },
        {
          "Dallas.": "[2]"
        },
        {
          "Dallas.": ""
        },
        {
          "Dallas.": ""
        },
        {
          "Dallas.": ""
        },
        {
          "Dallas.": ""
        },
        {
          "Dallas.": ""
        },
        {
          "Dallas.": "[3]"
        },
        {
          "Dallas.": ""
        },
        {
          "Dallas.": ""
        },
        {
          "Dallas.": ""
        },
        {
          "Dallas.": ""
        },
        {
          "Dallas.": ""
        },
        {
          "Dallas.": "[4]"
        },
        {
          "Dallas.": ""
        },
        {
          "Dallas.": ""
        },
        {
          "Dallas.": "[5]"
        },
        {
          "Dallas.": ""
        },
        {
          "Dallas.": ""
        },
        {
          "Dallas.": ""
        },
        {
          "Dallas.": "[6]"
        },
        {
          "Dallas.": ""
        },
        {
          "Dallas.": ""
        },
        {
          "Dallas.": ""
        },
        {
          "Dallas.": ""
        },
        {
          "Dallas.": "[7]"
        },
        {
          "Dallas.": ""
        },
        {
          "Dallas.": ""
        },
        {
          "Dallas.": ""
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "[8]\nS. Abrilian, L. Devillers, S. Buisine, and J.-C. Martin, “EmoTV1:",
          "12": "cies of emotion uncertainty using Kalman ﬁlters,” in Proc. ICASSP,"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "Annotation of\nreal-life\nemotions\nfor\nthe\nspeciﬁcation of multi-",
          "12": "Calgary, Canada, 2018, pp. 4929–4933."
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "modal affective interfaces,” in Proc. HCI\nInternational, Las Vegas,",
          "12": "[32] M. Atcheson, V. Sethu, and J. Epps, “Using Gaussian processes"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "Nevada, US, 2005, pp. 407–408.",
          "12": "with LSTM neural networks to predict continuous-time, dimen-"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "[9]\nC.\nBusso,\nS.\nParthasarathy,\nA.\nBurmania, M.\nAbdelWahab,",
          "12": "sional emotion in ambiguous speech,” in Proc. ACII, Cambridge,"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "N. Sadoughi, and E. M. Provost, “MSP-IMPROV: An acted corpus",
          "12": "UK, 2019, pp. 718–724."
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "of dyadic interactions to study emotion perception,” IEEE Trans on",
          "12": "[33] K. Sridhar, W.-C. Lin, and C. Busso, “Generative approach using"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "Affective Computing, vol. 8, no. 1, pp. 67–80, 2017.",
          "12": "soft-labels to learn uncertainty in predicting emotional attributes,”"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "[10] R. Lotﬁan and C. Busso, “Building naturalistic emotionally bal-",
          "12": "in Proc. ACII, Conference held virtually, 2021, pp. 1–8."
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "anced speech corpus by retrieving emotional speech from existing",
          "12": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "12": "[34] K. Sridhar\nand C. Busso,\n“Modeling uncertainty in predicting"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "podcast\nrecordings,” IEEE Trans\non Affective Computing, vol. 10,",
          "12": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "12": "emotional attributes from spontaneous speech,” in Proc.\nICASSP,"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "no. 4, pp. 471–483, 2019.",
          "12": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "12": "Barcelona, Spain, 2020, pp. 4929–4933."
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "[11] A. Bagher Zadeh, P. P. Liang,\nS. Poria, E. Cambria,\nand L.-",
          "12": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "12": "[35]\nS. Tripathi, S. Tripathi, and H. Beigi, “Multi-modal emotion recog-"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "P\n. Morency, “Multimodal\nlanguage analysis\nin the wild: CMU-",
          "12": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "12": "nition on IEMOCAP dataset using deep learning,” arXiv preprint"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "MOSEI dataset and interpretable dynamic fusion graph,” in Proc.",
          "12": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "12": "1804.05788, 2018."
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "ACL, Melbourne, Australia, 2018, pp. 2236–2246.",
          "12": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "12": "[36] N. Majumder, D. Hazarika, A. Gelbukh, E. Cambria, and S. Poria,"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "[12]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and",
          "12": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "12": "“Multimodal\nsentiment\nanalysis using hierarchical\nfusion with"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "R. Mihalcea, “MELD: A multimodal multi-party dataset for emo-",
          "12": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "12": "context modeling,” Knowledge-Based Systems, vol. 161, pp. 124–133,"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "tion recognition in conversations,” in Proc. ACL, Florence,\nItaly,",
          "12": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "12": "2018."
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "2019, pp. 527–536.",
          "12": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "12": "[37]\nS. Poria, N. Majumder, D. Hazarika, E. Cambria, A. Gelbukh,"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "[13] L. Devillers, L. Vidrascu,\nand L. Lamel,\n“Challenges\nin\nreal-",
          "12": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "12": "and A. Hussain, “Multimodal sentiment analysis: Addressing key"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "life emotion annotation and machine learning based detection,”",
          "12": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "12": "IEEE Intelligent\nissues\nand setting up the\nbaselines,”\nSystems,"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "Neural Networks, vol. 18, no. 4, pp. 407–422, 2005.",
          "12": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "12": "vol. 33, no. 6, pp. 17–25, 2018."
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "[14] A. Malinin and M. Gales, “Predictive uncertainty estimation via",
          "12": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "12": "[38] A. Nediyanchath, P. Paramasivam, and P. Yenigalla, “Multi-head"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "prior networks,” in Proc. NeurIPS, Montr´eal, Canada, 2018.",
          "12": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "12": "attention for speech emotion recognition with auxiliary learning"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "[15] W. Wu, C. Zhang,\nand P. Woodland,\n“Emotion recognition by",
          "12": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "12": "of gender\nrecognition,” in Proc.\nICASSP, Barcelona, Spain, 2020,"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "fusing time synchronous and time asynchronous representations,”",
          "12": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "12": "pp. 7179–7183."
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "in Proc. ICASSP, Toronto, Canada, 2021, pp. 6269–6273.",
          "12": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "12": "[39] R. Lotﬁan and C. Busso,\n“Predicting\ncategorical\nemotions\nby"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "imagery\nconsciousness: Volume\nI: The\npositive\n[16]\nS. Tomkins, Affect",
          "12": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "12": "jointly learning primary and secondary emotions\nthrough mul-"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "affects.\nSpringer publishing company, 1962.",
          "12": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "12": "titask learning,” in Proc.\nInterspeech, Hyderabad,\nIndia, 2018, pp."
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "[17] P. Ekman,\n“Facial\nexpressions of\nemotion: New ﬁndings, new",
          "12": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "12": "951–955."
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "questions,” Psychological Science, vol. 3, pp. 34 – 38, 1992.",
          "12": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "12": "[40] A. Ando, R. Masumura, H. Kamiyama,\nS. Kobashikawa,\nand"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "[18] P. Ekman,\n“Emotions\nrevealed:\nrecognising facial\nexpressions,”",
          "12": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "12": "Y. Aono, “Speech emotion recognition based on multi-label emo-"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "BMJ, vol. 12, pp. 140–142, 2004.",
          "12": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "12": "tion existence model,” in Proc. Interspeech, Graz, Austria, 2019, pp."
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "[19] B. Fehr and J. A. Russell, “Concept of emotion viewed from a",
          "12": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "12": "2818–2822."
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "prototype perspective.” Journal of experimental psychology: General,",
          "12": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "12": "[41] A. Ando, S. Kobashikawa, H. Kamiyama, R. Masumura, Y. Ijima,"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "vol. 113, no. 3, p. 464, 1984.",
          "12": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "12": "and Y. Aono,\n“Soft-target\ntraining with ambiguous\nemotional"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "[20] A.\nS. Cowen and D. Keltner,\n“Self-report\ncaptures\n27 distinct",
          "12": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "12": "utterances for DNN-based speech emotion classiﬁcation,” in Proc."
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "categories of\nemotion bridged by continuous gradients,” Proc.",
          "12": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "12": "ICASSP, Brighton, UK, 2018, pp. 4964–4968."
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "National Academy of Sciences, vol. 114, no. 38, pp. E7900–E7909,",
          "12": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "12": "[42] H. Fayek, M. Lech, and L. Cavedon, “Modeling subjectiveness in"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "2017.",
          "12": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "12": "emotion recognition with deep neural networks: Ensembles vs soft"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "[21]\nJ. A. Russell, “A circumplex model of affect.” Journal of personality",
          "12": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "12": "labels,” in Proc. IJCNN, Vancouver, Canada, 2016, pp. 566–570."
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "and social psychology, vol. 39, no. 6, p. 1161, 1980.",
          "12": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "12": "[43] E. Mower, A. Metallinou, C. chun Lee, A. Kazemzadeh, C. Busso,"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "[22] M. Grimm, K. Kroschel, E. Mower, and S. Narayanan, “Primitives-",
          "12": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "12": "S. Lee,\nand S. Narayanan,\n“Interpreting ambiguous\nemotional"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "based evaluation and estimation of emotions in speech,” Speech",
          "12": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "12": "expressions,” in Proc. ACII, Amsterdam, Netherlands, 2009, pp."
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "communication, vol. 49, no. 10-11, pp. 787–800, 2007.",
          "12": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "12": "1–8."
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "[23] T. C. Schneirla, “An evolutionary and developmental\ntheory of",
          "12": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "12": "[44] P. Liu, K. Li, and H. Meng, “Group gated fusion on attention-based"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "biphasic processes underlying approach and withdrawal.” Ne-",
          "12": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "12": "bidirectional alignment\nfor multimodal emotion recognition,” in"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "braska symposium on motivation, pp. 1–42, 1959.",
          "12": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "12": "Proc. Interspeech, Shanghai, China, 2020, pp. 379–383."
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "[24] P.\nJ. Lang, M. M. Bradley, and B. N. Cuthbert, “Motivated atten-",
          "12": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "12": "[45] M. Chen and X. Zhao, “A multi-scale fusion framework for bi-"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "tion: Affect, activation, and action,” Attention and orienting: Sensory",
          "12": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "12": "modal speech emotion recognition,” in Proc. Interspeech, Shanghai,"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "and motivational processes, vol. 97, p. 135, 1997.",
          "12": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "12": "China, 2020, pp. 374–378."
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "[25] D. Watson, D. Wiese, J. Vaidya, and A. Tellegen, “The two general",
          "12": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "12": "[46] M. R. Makiuchi, K. Uto, and K. Shinoda, “Multimodal emotion"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "activation systems of affect: Structural ﬁndings, evolutionary con-",
          "12": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "12": "recognition with high-level\nspeech and text\nfeatures,”\nin Proc."
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "siderations, and psychobiological evidence.” Journal of personality",
          "12": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "12": "ASRU, Cartagena, Colombia, 2021, pp. 350–357."
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "and social psychology, vol. 76, no. 5, p. 820, 1999.",
          "12": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "12": "[47] A. Malinin and M.\nJ. F. Gales, “Reverse KL-divergence training"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "[26] Y. Li,\nJ. Tao, B. Schuller, S. Shan, D.\nJiang, and J.\nJia, “MEC 2017:",
          "12": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "12": "of prior networks:\nImproved uncertainty and adversarial robust-"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "Multimodal emotion recognition challenge,” in Proc. ACII Asia,",
          "12": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "12": "ness,” in Proc. NeurIPS, Vancouver, Canada, 2019."
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "Beijing, China, 2018, pp. 1–5.",
          "12": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "12": "[48] C. Szegedy, V. Vanhoucke, S.\nIoffe,\nJ. Shlens, and Z. Wojna, “Re-"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "[27]\nF. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne, “Introducing",
          "12": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "12": "thinking the inception architecture for computer vision,” in Proc."
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "the RECOLA multimodal\ncorpus\nof\nremote\ncollaborative\nand",
          "12": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "12": "CVPR, Las Vegas, Nevada, US, 2016, pp. 2818–2826."
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "affective interactions,” in Proc. FG13, Shanghai, China, 2013, pp.",
          "12": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "1–8.",
          "12": "[49] B. Lakshminarayanan, A. Pritzel, and C. Blundell, “Simple and"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "12": "scalable predictive uncertainty estimation using deep ensembles,”"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "[28]\nJ. Han, Z. Zhang, Z. Ren, and B. Schuller, “Exploring perception",
          "12": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "12": "in Proc. NeurIPS, Long Beach, US, 2017."
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "uncertainty for emotion recognition in dyadic conversation and",
          "12": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "music listening,” Cognitive Computation, vol. 13, no. 2, pp. 231–240,",
          "12": "[50] K. Lee, H. Lee, K. Lee, and J. Shin, “Training conﬁdence-calibrated"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "2021.",
          "12": "classiﬁers for detecting out-of-distribution samples,” in Proc. ICLR,"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "12": "Vancouver, Canada, 2018."
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "[29]\nJ. Han, Z. Zhang, M. Schmitt, M. Pantic, and B. Schuller, “From",
          "12": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "hard to soft: Towards more human-like emotion recognition by",
          "12": "[51] Y. Gal and Z. Ghahramani, “Dropout as a Bayesian approximation:"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "modelling the perception uncertainty,” in Proc. ACM Multimedia,",
          "12": "Representing model uncertainty in deep learning,” in Proc. ICML,"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "Mountain View, US, 2017, pp. 890–897.",
          "12": "New York City, US, 2016, pp. 1050–1059."
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "[30] T. Dang, V. Sethu, J. Epps, and E. Ambikairajah, “An investigation",
          "12": "[52] C. Busso, S. Lee, and S. Narayanan, “Using neutral speech models"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "of emotion prediction uncertainty using gaussian mixture regres-",
          "12": "for emotional speech analysis,” in Proc. Interspeech, Antwerp, 2007."
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "sion.” in Proc. Interspeech, Stockholm, Sweden, 2017, pp. 1248–1252.",
          "12": "[53] E. Rodero, “Intonation and emotion: Inﬂuence of pitch levels and"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "[31] T. Dang, V. Sethu, and E. Ambikairajah, “Dynamic multi-rater",
          "12": "contour type on creating emotions,” Journal of Voice: Ofﬁcial Journal"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "Gaussian mixture regression incorporating temporal dependen-",
          "12": "of the Voice Foundation, vol. 25, pp. e25–e34, 2011."
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "[54] P. Dumouchel, N. Dehak, Y. Attabi, R. Dehak, and N. Boufaden,",
          "13": "Dr. Chang Zhang Chao Zhang received his BE and MSc degrees in"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "“Cepstral and long-term features for emotion recognition,” in Proc.",
          "13": "2009 and 2012 both from the Department of Computer Science and"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "Interspeech, Brighton, UK, 2009, pp. 344–347.",
          "13": "Technology, Tsinghua University, and his PhD degree in 2017 from"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "[55] R. Pappagari, T. Wang,\nJ. Villalba, N. Chen, and N. Dehak, “X-",
          "13": "Cambridge University Engineering Department (CUED). He is currently"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "vectors meet emotions: A study on dependencies between emotion",
          "13": "an Assistant Professor at\nthe Department of Electronic Engineering,"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "and speaker recognition,” in Proc. ICASSP, Barcelona, Spain, 2020,",
          "13": "Tsinghua University. Before that, he was a Senior Research Scien-"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "pp. 7169–7173.",
          "13": "tist at Google, a Research Associate at CUED, and an advisor and"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "[56] P. Ghahremani, B. BabaAli, D. Povey, K. Riedhammer,\nJ. Trmal,",
          "13": "speech team co-leader of JD.com. His research interests include spoken"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "and S. Khudanpur, “A pitch extraction algorithm tuned for auto-",
          "13": "language processing, machine learning, and cognitive neuroscience."
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "matic speech recognition,” in Proc.\nICASSP, Florence,\nItaly, 2014,",
          "13": "He has published 70 peer-reviewed speech and language processing"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "pp. 2494–2498.",
          "13": "papers and received multiple paper awards. He is also a Visiting Fellow"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "[57]\nJ. Pennington, R. Socher, and C. Manning, “GloVe: Global vectors",
          "13": "at CUED, and an Associate Member of the IEEE Speech and Language"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "for word representation,” in Proc. EMNLP, Doha, Qatar, 2014, pp.",
          "13": "Processing Technical Committee."
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "1532–1543.",
          "13": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "[58]\nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-",
          "13": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "training of deep bidirectional Transformers for\nlanguage under-",
          "13": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "standing,” in Proc. NAACL-HLT, Minneapolis, US, 2019, pp. 4171–",
          "13": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "4186.",
          "13": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "[59] G. Sun, C. Zhang, and P. Woodland, “Speaker diarisation using",
          "13": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "2D self-attentive combination of embeddings,” in Proc.\nICASSP,",
          "13": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "Brighton, UK, 2019, pp. 5801–5805.",
          "13": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "[60] Z. Lin, M. Feng, C. dos Santos, Y. Mo, X. Bing, B. Zhou, and",
          "13": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "Y. Bengio, “A structured self-attentive sentence embedding,” in",
          "13": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "Proc. ICLR, Toulon, France, 2017.",
          "13": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "[61]\nF. Kreyssig, C. Zhang,\nand P. Woodland,\n“Improved TDNNs",
          "13": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "using deep kernels and frequency dependent Grid-RNNs,” in Proc.",
          "13": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "ICASSP, Calgary, Canada, 2018, pp. 4864–4868.",
          "13": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "[62]\nF. Eyben,\nS. Buchholz, N. Braunschweiler,\nJ. Latorre, V. Wan,",
          "13": "IEEE)\nreceived his B.S. degree from\nDr. Xixin Wu Xixin Wu (Member,"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "M. Gales, and K. Knill, “Unsupervised clustering of emotion and",
          "13": "Beihang University, Beijing, China, his M.S. degree from Tsinghua Uni-"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "voice\nstyles\nfor\nexpressive\ntts,” in Proc.\nICASSP, Kyoto,\nJapan,",
          "13": "versity, Beijing, China, and his Ph.D. degree from The Chinese Univer-"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "2012, pp. 4009–4012.",
          "13": "sity of Hong Kong, Hong Kong. He is currently a Research Assistant"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "[63] W. Liu, Y. Wen, Z. Yu, M. Li, B. Raj, and L. Song, “SphereFace: Deep",
          "13": "Professor with the Stanley Ho Big Data Decision Analytics Research"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "hypersphere\nembedding\nfor\nface\nrecognition,”\nin Proc. CVPR,",
          "13": "Centre, The Chinese University of Hong Kong. Before this, he worked"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "Hawaii, US, 2017, pp. 212–220.",
          "13": "as a Research Associate with the Machine Intelligence Laboratory,"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "[64]\nS. Young, G. E., M. Gales, T. Hain, D. Kershaw, X. Liu, G. Moore,",
          "13": "Cambridge University Engineering Department. His research interests"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "J. Odell, D. Ollason, D. Povey, A. Ragni, V. Valtchev, P. Woodland,",
          "13": "include speech synthesis and recognition,\nspeaker\nveriﬁcation, and"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "and C. Zhang, The HTK Book (version 3.5a).\nUniversity of Cam-",
          "13": "neural network uncertainty."
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "bridge, 2015.",
          "13": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "[65] A. Nguyen,\nJ. Yosinski,\nand J. Clune,\n“Deep neural networks",
          "13": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "are easily fooled: High conﬁdence predictions for unrecognizable",
          "13": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "images,” in Proc. CVPR, Boston, US, 2015, pp. 427–436.",
          "13": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "[66] M. Hein, M. Andriushchenko,\nand J. Bitterwolf,\n“Why ReLu",
          "13": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "networks yield high-conﬁdence predictions\nfar\naway from the",
          "13": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "training data and how to mitigate the problem,” in Proc. CVPR,",
          "13": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "Long Beach, US, 2019, pp. 41–50.",
          "13": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "[67] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Librispeech:",
          "13": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "An ASR corpus based on public domain audio books,” in Proc.",
          "13": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "ICASSP, South Brisbane, Australia, 2015, pp. 5206–5210.",
          "13": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "[68] A. Ando, R. Masumura, H. Sato, T. Moriya, T. Ashihara, Y. Ijima,",
          "13": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "and T. Toda, “Speech emotion recognition based on listener adap-",
          "13": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "tive models,” in Proc.\nICASSP, Toronto, Canada, 2021, pp. 6274–",
          "13": ""
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "6278.",
          "13": "In-\nProf. Philip C. Woodland Philip C. Woodland is a Professor of"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "[69] L. Pepino, P. Riera, L. Ferrer, and A. Gravano, “Fusion approaches",
          "13": "formation Engineering in the Engineering Department, University of"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "for emotion recognition from speech using acoustic and text-based",
          "13": "Cambridge, Cambridge, U.K., where he is the Head of\nthe Machine"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "features,” in Proc. ICASSP, Barcelona, Spain, 2020, pp. 6484–6488.",
          "13": "Intelligence Laboratory and a Professorial Fellow of Peterhouse. After"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "˙\n[70] R. Pappagari, J. Villalba, P.\nZelasko, L. Moro-Velazquez, and N. De-",
          "13": "working at British Telecom Research Labs for three years, he returned"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "hak, “CopyPaste: An augmentation method for speech emotion",
          "13": "to a Lectureship at Cambridge in 1989 and became a (Full) Professor in"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "recognition,” in Proc.\nICASSP, Toronto, Canada, 2021, pp. 6324–",
          "13": "2002. He has published more than 250 papers in the area of speech and"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "6328.",
          "13": "language technology with a major focus on speech recognition systems."
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "[71] H.-C. Chou, C.-C. Lee, and C. Busso, “Exploiting co-occurrence",
          "13": "He has received a number of best paper awards including for work on"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "frequency of emotions in perceptual evaluations to train a speech",
          "13": "speaker adaptation and discriminative training. He is one of\nthe original"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "emotion classiﬁer,” in Proc.\nInterspeech,\nIncheon, Korea, 2022, pp.",
          "13": "coauthors of\nthe HTK toolkit and has continued to play a major\nrole in"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "161–165.",
          "13": "its development. He was a member of\nthe editorial board of Computer"
        },
        {
          "SUBMITTED TO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, 2022": "",
          "13": "Speech and Language (1994–2009) and is currently a member of\nthe"
        }
      ],
      "page": 13
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Deep learning for robust feature generation in audiovisual emotion recognition",
      "authors": [
        "Y Kim",
        "H Lee",
        "E Provost"
      ],
      "year": "2013",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "2",
      "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
      "authors": [
        "S Poria",
        "E Cambria",
        "R Bajpai",
        "A Hussain"
      ],
      "year": "2017",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "3",
      "title": "End-to-end multimodal emotion recognition using deep neural networks",
      "authors": [
        "P Tzirakis",
        "G Trigeorgis",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2017",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "4",
      "title": "A database of German emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "5",
      "title": "CHEAVD: a Chinese natural emotional audio-visual database",
      "authors": [
        "Y Li",
        "J Tao",
        "L Chao",
        "W Bao",
        "Y Liu"
      ],
      "year": "2016",
      "venue": "Journal of Ambient Intelligence and Humanized Computing"
    },
    {
      "citation_id": "6",
      "title": "Emotion recognition in speech using cross-modal transfer in the wild",
      "authors": [
        "S Albanie",
        "A Nagrani",
        "A Vedaldi",
        "A Zisserman"
      ],
      "year": "2018",
      "venue": "Proc. ACM Multimedia"
    },
    {
      "citation_id": "7",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Provost",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "8",
      "title": "EmoTV1: Annotation of real-life emotions for the specification of multimodal affective interfaces",
      "authors": [
        "S Abrilian",
        "L Devillers",
        "S Buisine",
        "J.-C Martin"
      ],
      "year": "2005",
      "venue": "Proc. HCI International"
    },
    {
      "citation_id": "9",
      "title": "MSP-IMPROV: An acted corpus of dyadic interactions to study emotion perception",
      "authors": [
        "C Busso",
        "S Parthasarathy",
        "A Burmania",
        "M Abdelwahab",
        "N Sadoughi",
        "E Provost"
      ],
      "year": "2017",
      "venue": "IEEE Trans on Affective Computing"
    },
    {
      "citation_id": "10",
      "title": "Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2019",
      "venue": "IEEE Trans on Affective Computing"
    },
    {
      "citation_id": "11",
      "title": "Multimodal language analysis in the wild: CMU-MOSEI dataset and interpretable dynamic fusion graph",
      "authors": [
        "A Zadeh",
        "P Liang",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Proc. ACL"
    },
    {
      "citation_id": "12",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "Proc. ACL"
    },
    {
      "citation_id": "13",
      "title": "Challenges in reallife emotion annotation and machine learning based detection",
      "authors": [
        "L Devillers",
        "L Vidrascu",
        "L Lamel"
      ],
      "year": "2005",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "14",
      "title": "Predictive uncertainty estimation via prior networks",
      "authors": [
        "A Malinin",
        "M Gales"
      ],
      "year": "2018",
      "venue": "Proc. NeurIPS"
    },
    {
      "citation_id": "15",
      "title": "Emotion recognition by fusing time synchronous and time asynchronous representations",
      "authors": [
        "W Wu",
        "C Zhang",
        "P Woodland"
      ],
      "year": "2021",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "16",
      "title": "Affect imagery consciousness: Volume I: The positive affects",
      "authors": [
        "S Tomkins"
      ],
      "year": "1962",
      "venue": "Affect imagery consciousness: Volume I: The positive affects"
    },
    {
      "citation_id": "17",
      "title": "Facial expressions of emotion: New findings, new questions",
      "authors": [
        "P Ekman"
      ],
      "year": "1992",
      "venue": "Psychological Science"
    },
    {
      "citation_id": "18",
      "title": "Emotions revealed: recognising facial expressions",
      "authors": [
        "P Ekman"
      ],
      "year": "2004",
      "venue": "BMJ"
    },
    {
      "citation_id": "19",
      "title": "Concept of emotion viewed from a prototype perspective",
      "authors": [
        "B Fehr",
        "J Russell"
      ],
      "year": "1984",
      "venue": "Journal of experimental psychology: General"
    },
    {
      "citation_id": "20",
      "title": "Self-report captures 27 distinct categories of emotion bridged by continuous gradients",
      "authors": [
        "A Cowen",
        "D Keltner"
      ],
      "year": "2017",
      "venue": "Proc. National Academy of Sciences"
    },
    {
      "citation_id": "21",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "22",
      "title": "Primitivesbased evaluation and estimation of emotions in speech",
      "authors": [
        "M Grimm",
        "K Kroschel",
        "E Mower",
        "S Narayanan"
      ],
      "year": "2007",
      "venue": "Speech communication"
    },
    {
      "citation_id": "23",
      "title": "An evolutionary and developmental theory of biphasic processes underlying approach and withdrawal",
      "authors": [
        "T Schneirla"
      ],
      "year": "1959",
      "venue": "An evolutionary and developmental theory of biphasic processes underlying approach and withdrawal"
    },
    {
      "citation_id": "24",
      "title": "Attention and orienting: Sensory and motivational processes",
      "authors": [
        "P Lang",
        "M Bradley",
        "B Cuthbert"
      ],
      "year": "1997",
      "venue": "Attention and orienting: Sensory and motivational processes"
    },
    {
      "citation_id": "25",
      "title": "The two general activation systems of affect: Structural findings, evolutionary considerations, and psychobiological evidence",
      "authors": [
        "D Watson",
        "D Wiese",
        "J Vaidya",
        "A Tellegen"
      ],
      "year": "1999",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "26",
      "title": "MEC 2017: Multimodal emotion recognition challenge",
      "authors": [
        "Y Li",
        "J Tao",
        "B Schuller",
        "S Shan",
        "D Jiang",
        "J Jia"
      ],
      "year": "2018",
      "venue": "Proc. ACII Asia"
    },
    {
      "citation_id": "27",
      "title": "Introducing the RECOLA multimodal corpus of remote collaborative and affective interactions",
      "authors": [
        "F Ringeval",
        "A Sonderegger",
        "J Sauer",
        "D Lalanne"
      ],
      "year": "2013",
      "venue": "Proc. FG13"
    },
    {
      "citation_id": "28",
      "title": "Exploring perception uncertainty for emotion recognition in dyadic conversation and music listening",
      "authors": [
        "J Han",
        "Z Zhang",
        "Z Ren",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "Cognitive Computation"
    },
    {
      "citation_id": "29",
      "title": "From hard to soft: Towards more human-like emotion recognition by modelling the perception uncertainty",
      "authors": [
        "J Han",
        "Z Zhang",
        "M Schmitt",
        "M Pantic",
        "B Schuller"
      ],
      "year": "2017",
      "venue": "Proc. ACM Multimedia"
    },
    {
      "citation_id": "30",
      "title": "An investigation of emotion prediction uncertainty using gaussian mixture regression",
      "authors": [
        "T Dang",
        "V Sethu",
        "J Epps",
        "E Ambikairajah"
      ],
      "year": "2017",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "31",
      "title": "Dynamic multi-rater Gaussian mixture regression incorporating temporal dependen-cies of emotion uncertainty using Kalman filters",
      "authors": [
        "T Dang",
        "V Sethu",
        "E Ambikairajah"
      ],
      "year": "2018",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "32",
      "title": "Using Gaussian processes with LSTM neural networks to predict continuous-time, dimensional emotion in ambiguous speech",
      "authors": [
        "M Atcheson",
        "V Sethu",
        "J Epps"
      ],
      "year": "2019",
      "venue": "Proc. ACII"
    },
    {
      "citation_id": "33",
      "title": "Generative approach using soft-labels to learn uncertainty in predicting emotional attributes",
      "authors": [
        "K Sridhar",
        "W.-C Lin",
        "C Busso"
      ],
      "year": "2021",
      "venue": "Proc. ACII, Conference held virtually"
    },
    {
      "citation_id": "34",
      "title": "Modeling uncertainty in predicting emotional attributes from spontaneous speech",
      "authors": [
        "K Sridhar",
        "C Busso"
      ],
      "year": "2020",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "35",
      "title": "Multi-modal emotion recognition on IEMOCAP dataset using deep learning",
      "authors": [
        "S Tripathi",
        "S Tripathi",
        "H Beigi"
      ],
      "year": "2018",
      "venue": "Multi-modal emotion recognition on IEMOCAP dataset using deep learning"
    },
    {
      "citation_id": "36",
      "title": "Multimodal sentiment analysis using hierarchical fusion with context modeling",
      "authors": [
        "N Majumder",
        "D Hazarika",
        "A Gelbukh",
        "E Cambria",
        "S Poria"
      ],
      "year": "2018",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "37",
      "title": "Multimodal sentiment analysis: Addressing key issues and setting up the baselines",
      "authors": [
        "S Poria",
        "N Majumder",
        "D Hazarika",
        "E Cambria",
        "A Gelbukh",
        "A Hussain"
      ],
      "year": "2018",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "38",
      "title": "Multi-head attention for speech emotion recognition with auxiliary learning of gender recognition",
      "authors": [
        "A Nediyanchath",
        "P Paramasivam",
        "P Yenigalla"
      ],
      "year": "2020",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "39",
      "title": "Predicting categorical emotions by jointly learning primary and secondary emotions through multitask learning",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "40",
      "title": "Speech emotion recognition based on multi-label emotion existence model",
      "authors": [
        "A Ando",
        "R Masumura",
        "H Kamiyama",
        "S Kobashikawa",
        "Y Aono"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "41",
      "title": "Soft-target training with ambiguous emotional utterances for DNN-based speech emotion classification",
      "authors": [
        "A Ando",
        "S Kobashikawa",
        "H Kamiyama",
        "R Masumura",
        "Y Ijima",
        "Y Aono"
      ],
      "year": "2018",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "42",
      "title": "Modeling subjectiveness in emotion recognition with deep neural networks: Ensembles vs soft labels",
      "authors": [
        "H Fayek",
        "M Lech",
        "L Cavedon"
      ],
      "year": "2016",
      "venue": "Proc. IJCNN"
    },
    {
      "citation_id": "43",
      "title": "Interpreting ambiguous emotional expressions",
      "authors": [
        "E Mower",
        "A Metallinou",
        "C Chun Lee",
        "A Kazemzadeh",
        "C Busso",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2009",
      "venue": "Proc. ACII"
    },
    {
      "citation_id": "44",
      "title": "Group gated fusion on attention-based bidirectional alignment for multimodal emotion recognition",
      "authors": [
        "P Liu",
        "K Li",
        "H Meng"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "45",
      "title": "A multi-scale fusion framework for bimodal speech emotion recognition",
      "authors": [
        "M Chen",
        "X Zhao"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "46",
      "title": "Multimodal emotion recognition with high-level speech and text features",
      "authors": [
        "M Makiuchi",
        "K Uto",
        "K Shinoda"
      ],
      "year": "2021",
      "venue": "Proc. ASRU"
    },
    {
      "citation_id": "47",
      "title": "Reverse KL-divergence training of prior networks: Improved uncertainty and adversarial robustness",
      "authors": [
        "A Malinin",
        "M Gales"
      ],
      "year": "2019",
      "venue": "Proc. NeurIPS"
    },
    {
      "citation_id": "48",
      "title": "Rethinking the inception architecture for computer vision",
      "authors": [
        "C Szegedy",
        "V Vanhoucke",
        "S Ioffe",
        "J Shlens",
        "Z Wojna"
      ],
      "year": "2016",
      "venue": "Proc. CVPR"
    },
    {
      "citation_id": "49",
      "title": "Simple and scalable predictive uncertainty estimation using deep ensembles",
      "authors": [
        "B Lakshminarayanan",
        "A Pritzel",
        "C Blundell"
      ],
      "year": "2017",
      "venue": "Proc. NeurIPS"
    },
    {
      "citation_id": "50",
      "title": "Training confidence-calibrated classifiers for detecting out-of-distribution samples",
      "authors": [
        "K Lee",
        "H Lee",
        "K Lee",
        "J Shin"
      ],
      "year": "2018",
      "venue": "Proc. ICLR"
    },
    {
      "citation_id": "51",
      "title": "Dropout as a Bayesian approximation: Representing model uncertainty in deep learning",
      "authors": [
        "Y Gal",
        "Z Ghahramani"
      ],
      "year": "2016",
      "venue": "Proc. ICML"
    },
    {
      "citation_id": "52",
      "title": "Using neutral speech models for emotional speech analysis",
      "authors": [
        "C Busso",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2007",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "53",
      "title": "Intonation and emotion: Influence of pitch levels and contour type on creating emotions",
      "authors": [
        "E Rodero"
      ],
      "year": "2011",
      "venue": "Journal of Voice: Official Journal of the Voice Foundation"
    },
    {
      "citation_id": "54",
      "title": "Cepstral and long-term features for emotion recognition",
      "authors": [
        "P Dumouchel",
        "N Dehak",
        "Y Attabi",
        "R Dehak",
        "N Boufaden"
      ],
      "year": "2009",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "55",
      "title": "Xvectors meet emotions: A study on dependencies between emotion and speaker recognition",
      "authors": [
        "R Pappagari",
        "T Wang",
        "J Villalba",
        "N Chen",
        "N Dehak"
      ],
      "year": "2020",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "56",
      "title": "A pitch extraction algorithm tuned for automatic speech recognition",
      "authors": [
        "P Ghahremani",
        "B Babaali",
        "D Povey",
        "K Riedhammer",
        "J Trmal",
        "S Khudanpur"
      ],
      "year": "2014",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "57",
      "title": "GloVe: Global vectors for word representation",
      "authors": [
        "J Pennington",
        "R Socher",
        "C Manning"
      ],
      "year": "2014",
      "venue": "Proc. EMNLP"
    },
    {
      "citation_id": "58",
      "title": "BERT: Pretraining of deep bidirectional Transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Proc. NAACL-HLT"
    },
    {
      "citation_id": "59",
      "title": "Speaker diarisation using 2D self-attentive combination of embeddings",
      "authors": [
        "G Sun",
        "C Zhang",
        "P Woodland"
      ],
      "year": "2019",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "60",
      "title": "A structured self-attentive sentence embedding",
      "authors": [
        "Z Lin",
        "M Feng",
        "C Santos",
        "Y Mo",
        "X Bing",
        "B Zhou",
        "Y Bengio"
      ],
      "year": "2017",
      "venue": "Proc. ICLR"
    },
    {
      "citation_id": "61",
      "title": "Improved TDNNs using deep kernels and frequency dependent Grid-RNNs",
      "authors": [
        "F Kreyssig",
        "C Zhang",
        "P Woodland"
      ],
      "year": "2018",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "62",
      "title": "Unsupervised clustering of emotion and voice styles for expressive tts",
      "authors": [
        "F Eyben",
        "S Buchholz",
        "N Braunschweiler",
        "J Latorre",
        "V Wan",
        "M Gales",
        "K Knill"
      ],
      "year": "2012",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "63",
      "title": "SphereFace: Deep hypersphere embedding for face recognition",
      "authors": [
        "W Liu",
        "Y Wen",
        "Z Yu",
        "M Li",
        "B Raj",
        "L Song"
      ],
      "year": "2017",
      "venue": "Proc. CVPR"
    },
    {
      "citation_id": "64",
      "title": "",
      "authors": [
        "S Young",
        "M Gales",
        "T Hain",
        "D Kershaw",
        "X Liu",
        "G Moore",
        "J Odell",
        "D Ollason",
        "D Povey",
        "A Ragni",
        "V Valtchev",
        "P Woodland",
        "C Zhang"
      ],
      "year": "2015",
      "venue": ""
    },
    {
      "citation_id": "65",
      "title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images",
      "authors": [
        "A Nguyen",
        "J Yosinski",
        "J Clune"
      ],
      "year": "2015",
      "venue": "Proc. CVPR"
    },
    {
      "citation_id": "66",
      "title": "Why ReLu networks yield high-confidence predictions far away from the training data and how to mitigate the problem",
      "authors": [
        "M Hein",
        "M Andriushchenko",
        "J Bitterwolf"
      ],
      "year": "2019",
      "venue": "Proc. CVPR"
    },
    {
      "citation_id": "67",
      "title": "Librispeech: An ASR corpus based on public domain audio books",
      "authors": [
        "V Panayotov",
        "G Chen",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2015",
      "venue": "Proc. ICASSP, South"
    },
    {
      "citation_id": "68",
      "title": "Speech emotion recognition based on listener adaptive models",
      "authors": [
        "A Ando",
        "R Masumura",
        "H Sato",
        "T Moriya",
        "T Ashihara",
        "Y Ijima",
        "T Toda"
      ],
      "year": "2021",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "69",
      "title": "Fusion approaches for emotion recognition from speech using acoustic and text-based features",
      "authors": [
        "L Pepino",
        "P Riera",
        "L Ferrer",
        "A Gravano"
      ],
      "year": "2020",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "70",
      "title": "CopyPaste: An augmentation method for speech emotion recognition",
      "authors": [
        "R Pappagari",
        "J Villalba",
        "P Żelasko",
        "L Moro-Velazquez",
        "N Dehak"
      ],
      "year": "2021",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "71",
      "title": "Exploiting co-occurrence frequency of emotions in perceptual evaluations to train a speech emotion classifier",
      "authors": [
        "H.-C Chou",
        "C.-C Lee",
        "C Busso"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech"
    }
  ]
}