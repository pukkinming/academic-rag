{
  "paper_id": "2410.22506v1",
  "title": "Affectnet+: A Database For Enhancing Facial Expression Recognition With Soft-Labels",
  "published": "2024-10-29T19:57:10Z",
  "authors": [
    "Ali Pourramezan Fard",
    "Mohammad Mehdi Hosseini",
    "Timothy D. Sweeny",
    "Mohammad H. Mahoor"
  ],
  "keywords": [
    "Facial Expression Recognition",
    "Affective Computing",
    "AffectNet Dataset",
    "AffectNet+ Dataset",
    "Soft-Label-Based FER {Ali.Pourramezanfard",
    "MohammadMehdi.Hosseini",
    "Moham-"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Automated Facial Expression Recognition (FER) is challenging due to intra-class variations and inter-class similarities. FER can be especially difficult when facial expressions reflect a mixture of various emotions (aka compound expressions). Existing FER datasets, such as AffectNet, provide discrete emotion labels (hard-labels), where a single category of emotion is assigned to an expression. To alleviate inter-and intra-class challenges, as well as provide a better facial expression descriptor, we propose a new approach to create FER datasets through a labeling method in which an image is labeled with more than one emotion (called softlabels), each with different confidences. Specifically, we introduce the notion of soft-labels for facial expression datasets, a new approach to affective computing for more realistic recognition of facial expressions. To achieve this goal, we propose a novel methodology to accurately calculate soft-labels: a vector representing the extent to which multiple categories of emotion are simultaneously present within a single facial expression. Finding smoother decision boundaries, enabling multi-labeling, and mitigating bias and imbalanced data are some of the advantages of our proposed method. Building upon AffectNet, we introduce AffectNet+, the next-generation facial expression dataset. This dataset contains soft-labels, three categories of data complexity subsets, and additional metadata such as age, gender, ethnicity, head pose, facial landmarks, valence, and arousal. AffectNet+ will be made publicly accessible to researchers.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Facial expressions are essential non-verbal communication channels utilized by both humans and animals  [1] . Facial expressions result from facial muscle movements and provide a window into the emotions, feelings, and psychological states humans experience  [2] . The discrete/categorical theory of emotions defines six basic (potentially universally shared) emotions expressed by facial expressions Happy, Sad, Surprise, Fear, Disgust, and Anger  [3] ,  [4] . Contempt, which is the feeling of dislike for and superiority (usually morally) over another person, was later added to this list of basic emotions  [5] . Recognition and analysis of emotional facial expressions have many applications including emotion regulation, cultural influences, health care, and human-computer interaction (HCI). While manual measurement of facial expressions is a laborintensive task, the development of automated Facial Expression Recognition (FER) using machine learning (ML) algorithms has garnered significant attention in the realms of computer vision over the past few decades. Considerable FER advancements have been made in recent years by employing robust deep learning methods, such as Convolutional Neural Networks (CNNs)  [6] -  [8] , and Vision-Transformers  [9] ,  [10] . Specifically, in comparison with the traditional ML methods, deep learning-based models have better success in dealing with images collected in uncontrolled environments (aka wild settings) where we can witness a vast variation in scene lighting, camera view, image resolution, and subject's head pose, gender, and ethnicity.\n\nCreating a robust and accurate FER model using machine learning necessitates a substantial dataset of annotated facial images. Annotating facial expressions in images poses challenges due to intrinsic intra-class variations and inter-class similarities  [7]  among facial expressions. Intra-class variations reflect the diverse range of expressions observed within a single emotion category. For example, sadness can manifest to various degrees with distinct facial muscle movements  [11] . Similarly, happiness can be perceived across a range of different smiles (e.g., Duchenne smile vs non-Duchenne smile  [12] ). Inter-class similarities refer to the overlap in activation of facial musculature across different emotion categories, especially evident in subtle expressions. For instance, the high correlation between muscle movements associated with Happy and Contempt expressions causes confusion when distinguishing subtle variations between these emotions.\n\nFurther complicating the situation is the fact that due to the dynamic changes over the facial muscles  [13] -  [15] , some facial expressions may not exclusively convey a single emotion, with individuals expressing mixed emotions in different emotional states-referred to by some researchers as compound expressions  [16] -  [22] . Fig.  1  illustrates the combination of two expressions in a single facial image. In this figure, Happy-Contempt, Disgust-Anger, Sad-Neutral, and Fear-Surprise are jointly mixed in a facial image. Cultural differences represent another significant factor impacting emotional facial expressions and their perception, particularly among individuals from diverse cultural backgrounds  [5] ,  [23] -  [25] . Additionally, expressing facial expressions is a dynamic, time-varying behavior, and in wild facial datasets, we capture only a snapshot of a person's evolving expression of emotion in still images. Consequently, it becomes challenging for humans to consistently and accurately judge the facial expressions. Hence, human annotators may not unanimously agree on emotion labels of others in complex, real-world environments. As a result, labels assigned to facial images in well-known datasets collected in wild settings, such as AffectNet  [26] , RAF-DB  [17] , and FER2013  [27] , are often noisy and unreliable.\n\nAs mentioned above, there is often disagreement between humans when annotating facial expressions, explicitly affecting the existing FER datasets  [17] ,  [26] ,  [27]  and, ultimately, the automated FER models. While crowd-sourcing  [17]  (using multiple trained human annotators) can alleviate this issue, the reported agreement between annotators is usually less than 68%  [26] . This issue stems from the fact that assigning a single label (emotion) to an image might not be the right approach for annotating expressions, as some facial images express compound emotions. To address this concern, we propose an alternative approach where an image is annotated with more than one emotion label (which we refer to as soft-labels), each with different degrees of confidence.\n\nAffectNet  [26]  is the largest publicly available in-the-wild facial expression dataset, containing both categorical  [3]  and dimensional (valence and arousal  [28] ) labels. Despite its extensive use and application by researchers, AffectNet has several shortcomings and limitations that require further consideration. Firstly, although 450K out of one million images in AffectNet are annotated by human experts, the labels are noisy. In fact, each image is labeled only by one annotator, significantly detracting from the reliability of the labels. Hence, the potential noisy labels in AffectNet may have adversely contributed to the accuracy of FER models trained on AffectNet thus far. Secondly, only one label per image is given to AffectNet images, and as discussed before  [26] , the dataset is collected by crawling the web, often producing images that contain compound emotions. Furthermore, the metadata (such as facial landmark points) released with AffectNet is noisy, as the algorithm used to extract facial landmark points has significantly improved in recent years. Additionally, the dataset lacks other metadata such as age, race, gender, and head pose, which are crucial in various affective computing applications.\n\nTo address these issues, this paper introduces AffectNet+, a revised version of AffectNet, which will be publicly available to the research community 1 . Although the concept of soft-label is used in affective computing, there is no dataset covering this feature. AffectNet+ provides a novel approach to facial expression datasets, termed soft-labeling. In contrast to the traditional method of assigning a single hard-label to a facial image, soft-labeling involves allocating multiple labels with varying degrees of confidence. In other words, a probability score is assigned to each of the seven emotion labels (plus an additional score for a Neutral label) that may be perceived when observing an image. Following this approach, we provide a new annotation vector named soft-label, containing eight independent probability scores corresponding to each emotion for every facial image in AffectNet+. Fig.  1  illustrates examples of facial expressions with soft-labels, where an image conveys two emotions with a high probability. Moreover, AffectNet+ categorizes the AffectNet images into three exclusive subsets based on the difficulty of recognizing facial expressions. These categories, denoted as Easy, Challenging, and Difficult, are applied to both the training and validation sets.\n\nTo create the soft-labels for AffectNet+, we utilize a subset of AffectNet dataset containing 36K facial images, annotated by at least two human annotators. This subset provides more reliable labels compared to the single-annotator AffectNet training and validation sets. This subset is referred to as multi-annotated-set (MAS). Table  1  describes the MAS and AffectNet dataset. We propose two methods for creating soft-labels: 1-Ensemble of binary classifiers, and 2-Action unit (AU)-based classifier. The \"ensemble of binary classifiers\" approach consists of training a set of binary classifiers (see Fig.  2 ), each designed to predict the probability score of a specific facial expression given an image (e.g., a model predicting the probability score of Happy versus all other facial expressions). The \"AU-based classifier\" leverages the overlap of AUs associated with facial expressions, defined by the Emotional Facial Action Coding System (EMFACS)  [11] . Specifically, for each emotion class, we train a binary classifier to jointly learn an AU-based representation vector as well as a binary class label (see Fig.  3 ).\n\nBy calculating the probability vectors of the aforementioned classifiers, we designate a soft-label vector to each image. Then, we compare the achieved soft-label vector with the class label assigned by the annotator and categorize all the images in the AffectNet datset into Easy, Challenging, or Difficult subsets.\n\nThe contributions of our approach are summarized as follows:\n\n•\n\nWe introduce the notion of soft-labels for facial expressions datasets, which could provide more realistic description of facial expressions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "•",
      "text": "We propose an automatic method to sub-categorize Af-fectNet into three subsets based on the level of difficulty of recognizing expressions in each image.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "•",
      "text": "We introduce AffectNet+, the next-generation of facial expression dataset, which contains soft-labels and other metadata, including age, gender, ethnicity, valence, arousal, head pose, and facial landmark points.\n\nIn the remainder of this paper Sec. 2 reviews the related works. Sec. 3 describes the proposed methodology for creating soft-",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Related Works",
      "text": "In this section, we review the major studies on the AffectNet database problems, as well as the researches focused on the compound datasets and soft-labeling concepts in FER.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Fer Using Affectnet",
      "text": "The AffectNet database is the largest in-the-wild dataset in existence today, includes 1 million images. In reviewing the SOTA papers that used AffectNet, we recognized three main challenges that researchers mainly dealt with 1) uncertainty in the emotion labels, 2) imbalanced data, and 3) lack of data diversity. These challenges stem from the nature of the data distribution and the nature of the images posted on the web as the main source used to collect the images and create AffectNet. In the following, we explain these difficulties and some of the provided solutions. Uncertainty in emotion labels: Uncertainty in FER occurs when it is difficult for annotator (human or model) to determine the precise expression for a given facial image (see Fig.  1 ). Deep metric learning-based methods  [6] ,  [7] , self-learning  [29] , latent space analysis  [30] , and label-smoothing  [31]  are the most notable approaches proposed to deal with label uncertainty.\n\nTo handle the problem originated by label uncertainty, Gera et al.  [32]  utilized a lightweight network structure to combine the attention area with the local-global features to alleviate the noisy data. By considering the overlap between the expressions, Lang et al.  [33]  offered a three-step deep learning approach to group similar features, extract intra-class distribution, and finally distinguish similar expressions. Other approaches took advantage of AUs to deal with label uncertainty  [34] ,  [35] . Liu et al.  [34]  used AUs to find the most reliable image data, while Savchenko  [35]  combined AUs with valence-arousal to deal with noisy samples. Hasani et al.  [8]  changed the shortcut passing method of the ResNet  [36]  model to a trainable transformer, to extract less correlated features. The relation between the accuracy of the model and the data distribution was studied by Dominguez-Caten et al.  [37] . They concluded that the balance between other facial attributes, such as gender and race, can improve the accuracy of the model. Another study by Su et al.  [38] , as well as Heidari and Iosifidis  [39] , showed the importance of compositional information between adjacent pixels in extracting robust features. Inspired by control theory, Wang et al.  [40]  developed transmitters for making a feedback cycle between regular one-hot label predictors and probabilistic label predictors, to generate soft-labels for the images. To cope with label uncertainty, soft-labeling was studied by Zhang et al.  [41] .\n\nImbalanced data distribution: This problem in FER originates from inequality between the number of samples per class. Table  1  illustrates the distribution of various emotions in Affect-Net. As this table shows, AffectNet is an imbalanced database. For instance, 32% of the images in AffectNet are labeled Happy, while only 2% of them are labeled Fear. Data manipulation and model generalization  [39] ,  [42] -  [45]  are among the most common approaches to tackle imbalanced data in AffectNet.\n\nA) Data manipulation refers to up-sampling, down-sampling, and data knowledge sharing. For instance, Gao et al.  [42]  extracted a subcategory for each expression before feeding their neural network. Lang et al.  [33]  considered only a third of the whole training set in the AffectNet dataset. In contrast, Gera et al.  [32]  upsampled the data through regular augmentation methods. Some research leveraged unsupervised and semi-supervised data to solve imbalanced data problems. While Jiang et al.  [45]  approached the imbalance data using semi-supervised learning, Zeng et al.  [44]  combined unsupervised face recognition data with supervised AffectNet images to make a feedback-based adaptive network.\n\nB) Model generalization approaches focus mainly on the objective functions to minimize the prediction error. Gong et al.  [46]  combined Focal Smoothing (FS) and Aggregation-Separation (AS) loss functions as EAFR loss. Similar study, by Li et al.  [47] , proposed a loss function for extracting basic facial expressions. Another method for confronting the imbalanced data was the weighted regularization method  [34] . Ma et al.  [43]  designed a cascade feature-augmentation method to preserve geometrical features and improve model generality by maximizing intra-sample and minimizing inter-sample similarities.\n\nLack of data diversity: This challenge in FER refers to the unevenness of demographic factors in a dataset, such as race, age, and gender, as well as some extrinsic factors, like head pose, occlusion, and illumination. This bias is problematic even in the AffectNet dataset despite its very large size. For instance, the number of images of males is nearly double that of images of females. We reported the data distribution over all the demographic factors in the Supplementary Materials. Researchers offered approaches to address this problem, such as focusing on regions of interest, ensemble learning, and domain adaptation.\n\nA) Focusing on the regions of interest, i.e., exploring the most relevant parts of the facial image, is a solution to cope with the lack of data diversity. Zhang and Yu  [48]  turned to find a unique pattern map that transfers all the data of a specific class to a single pattern, different from the other classes. Another study considered the attention area problem as a multi-dimensional issue  [42] . They combined spatial and spectral information and then extracted the relation between the AUs. Landmark detection and pyramid image scaling were other approaches for concentrating on the attention area  [49] -  [51] . Zheng et al.  [50]  suggested a crossfusion transformer to take advantage of the landmarks to force the model to focus on the most related areas. On the other hand, Liu et al.  [49]  created a hierarchical attention map, where they cropped the attention area and skipped the rest of the image.\n\nB) Recent ensemble learning methods mainly provide parallel convolutional neural networks to extract robust features to address the lack of diversity. To alleviate this problem, Zia et al.  [52]  combined the features extracted by three VGG-19  [53] , Inceptuion-V3  [54] , and ResNet-50  [36]  models to make a majority voting decision over the expressions. OANet  [55]  was an oriented attention network structure that utilizes different networks in parallel and series, for diverse feature extraction and expression recognition.\n\nC) Vision transformers were another approach to tackle the lack of diversity in AffectNet dataset. TransFER  [56]  model explored the relationship between different facial features. Dresvyanskiy et al.  [57]  used an LSTM-RNN model alongside two different modalities of audio and video to transfer and fuse their knowledge. Rescigno et al.  [58]  presented a combination of valence-arousal and facial features to exploit more robust features. Schiller et al.  [59]  utilized an encoder-decoder to extract the saliencies on the expression and then fed the masked version of the input samples to the model to mitigate the lack of diversity.\n\nAlthough the aforementioned methods mitigate the AffectNet dataset limitations, they are not a certain solution for the AffectNet complexity. How can we look at the data more realistically? Are the facial expressions explicitly separable? Are the facial AUs unique for any facial expression? What if we rethink the facial expressions in a way that any facial image can convey a portion of multiple expressions, simultaneously? The solution to these questions could be find in compund labeling and soft-labeling.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Compound Fer Datasets And Soft-Labeling",
      "text": "Most facial expression recognition datasets are annotated with six basic facial expression labels and Neutral  [27] ,  [60] -  [64] . However, in some datasets, Contempt is added as the seventh basic expression  [26] . It is argued that sometimes these expressions are not explicitly separable (i.e., the uncertainty problem, discussed in Section 2.1). In other words, there are many cases in which more than one expression is included in a facial image. Some researchers created compound datasets to deal with this problem. They included at most two expression labels for the images, but without considering the intensity of each expression. On the other hand, some researchers have worked on the idea of softlabeling, where they calculate the intensity of expressions in each facial image, but apply just one label to the facial image. However, to the best of our knowledge, no dataset exists with multiple labels with different intensities assigned to the facial images.  [17]  is a manually annotated dataset, including six basic expressions, accompanied by twelve compound expressions, such as Happily-Surprised and Fearfully-Disgusted. FER+  [65]  is the new version of FER-2013  [27]  dataset. This dataset includes eight expressions in the form of single and compund label expressions. EmotioNet  [18]  is another FER in-the-wild dataset with compound labels. They considered 23 basic expressions as descriptors of the dataset, where fourteen of them were compound (pair) expressions. C-EXPR-DB  [19]  is a manually annotated in-the-wild dataset, annotated by 12 compound expressions, including 400 videos (200K frames). In 2022, Liu et al.  [20]  released the MAFW compound multi-modal dataset, containing more than 10K video clips, accompanied by audio and text descriptors. Barsoum et al.  [63]  worked on the dataset FER-2013  [27]  and re-labeled this dataset in a compound labeling format.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Compound Datasets Raf-Db",
      "text": "In addition to these in-the-wild datasets, there are two compound lab-controlled datasets. Du et al.  [21]  created a dataset, including 21 compound expressions of 230 subjects. This dataset includes the expressions and the intensity of the AUs. As the second compound dataset, iCV-MEFED  [22] , containing 31250 facial images, targeted 125 subjects in a controlled environment and assigned 49 compound expressions to the facial images (plus Neutral). For any subject, they defined 50 compound expressions and captured 5 images per person-expression.\n\nThe aforementioned datasets highlight the essence of paying more attention to the compound expressions in FER. However, it is notable that all the reviewed datasets provide neither more than one combination of the labels, nor the intensity of each expression. For more information about the datasets in FER, we refer our readers to the Supplementary Materials.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Soft-Labeling",
      "text": "Some research in expression recognition has recently focused on extracting soft-labels rather than hard-labels  [40] ,  [66] -  [71] . Gan et al.  [69]  proposed a model to discover the co-occurrence of multiple expressions in a single image. They initially trained a model to generate a probability vector over the expressions. These probabilities were then perturbed to generate soft-labels. In the last step, the soft-labels were used in another model to find the intrinsic relation between the expressions in an image. In another line of research, Liu et al.  [70]  studied non-verbal behavior in schools, using infrared images. They initially extracted the similarity between different expressions and then fed the data into their CDLLNet model to learn the Cauchy distribution over the expressions. This method enabled them to have multiple expressions with different intensities for a single image. To relax the effect of noisy samples, Lukov et al.  [71]  developed a Soft Label Smoothing (SLS) model to smooth the logits. In this model, instead of labeling the facial expressions, a probability vector was generated to show the correlation of the expressions in an image.\n\nAll these models worked on soft-labeling, but they generated their soft-labels with different methods and had no evaluation set to evaluate or compare their approach. Therefore, having a dataset including soft-labels could provide more general and robust models. Soft-labeling methods and the aforementioned compound FER datasets highlight the necessity of paying attention to the soft-labeled facial expression recognition datasets. To cover this essence, this paper introduces the AffectNet+ dataset, including soft-labels, three categorizations of the data, and some useful metadata, that could open new perspectives toward FER studies.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Methodology",
      "text": "In this section, we first explain our novel Soft-FER and the process of creating soft-labels. Afterward, we introduce the AffectNet+ database and its Easy, Challenging, and Difficult subsets. Finally, we explain the metadata we updated or added to AffectNet+.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Soft-Fer",
      "text": "Facial expressions are the result of facial muscle movements, which can be coded in terms of action units (AUs). EMFACS  [11]  describes many combinations of facial muscle movements related to each expression. According to EMFACS, for almost all basic facial expressions, there exists more than one combination of AUs. For instance, Happy expression can be shown by the activation of specific AUs, such as AU6 and AU12, or solely AU12. These",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Happy Classifier",
      "text": "Sad Classifier Fig.  2 . Architecture of ensemble of binary classifiers (EBC model), as the initial step of the soft-labeling process. It contains ensemble of three ResNet-50  [36] , EfficientNet-B3  [72] , and XceptionNet  [73]  classifiers, for any expression. There are eight instances of this network architecture, trained for each expression in a binary one-vs-rest method. Finally, their output aggregates to make the expression vector.\n\ncombinations illustrate the intra-class variation in FER. Likewise, EMFACS shows a high correlation in AUs for specific emotions. For example, action units 6, 12, and 25 correspond to Happy emotion, while AU12 and AU14 correspond to Contempt. This correlation between the action units highlights inter-class similarities in FER. Tables  2  and 3  show the AUs for each emotion class and the correlation between them, respectively. Hence, people should potentially perceive more than one specific facial expression from a facial image in many cases. In fact, by assigning only one emotional label to a facial image we are ignoring the valuable information that can be utilized to provide a more comprehensive explanation of facial expression. We argued that widely used Hard-FER, where we assign one label to an image, needs further consideration, and accordingly, we proposed Soft-FER as a solution. In our proposed Soft-FER, we measured the probability score of the existence of all the facial expressions for each image as follows in Eq. 1:\n\nTABLE 2 Action units for different expressions  [11] ,  [74] . Different subsets of the corresponding AUs will create an expression. For instance, AU1, AU4, AU15, AU17 create Sad expression, while another combination could be AU1, AU4, AU6, AU11, AU15. where EMOTIONS = {Neutral, Happy, Sad, Surprise, Fear, Disgust, Anger, and Contempt}, i ∈ {0, 1, .., 7} indicating the i th expression, k ∈ {0, 1, ..., N }, and N is the number of images in the dataset. We used a neural network to estimate the corresponding probability P . Using Eq. 1, we defined a soft-label vector, SL k , corresponding to img k as follow in Eq. 2:",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Action Units",
      "text": "SL k := {P 0k , P 1k , ..., P 7k }.\n\n(2)\n\nAs Fig  1  shows, soft-labels are more explanatory compared to hard-labels as they explicitly present the similarity between a facial image img k and all the emotions in EM OT ION S set. In fact, hard-label does not consider the variation within an emotion class. For example, very happy versus slightly happy can be potentially confused with the Neutral expression. It also distorts the similarity between different emotion classes. It means that a facial image can be perceived as both Anger and Fear, as there is a high correlation between the AUs corresponding to such emotions. On the contrary, soft-labels do not have these drawbacks as it considers the probability score of the existence of all the emotion TABLE  3  Correlation between the action units, regarding each emotion class. Each value shows the number of common action units between two expressions, using EMFACS  [11] .\n\nclasses for a facial image. Consequently, the machine learning model would learn the different variations of a specific emotion class, as well as the similarities between different classes.\n\nTo the best of our knowledge, there exists no FER dataset providing soft-labels. Creating such labels necessitates training annotators in accordance with Soft-FER methodology, which demands a significant investment of both time and financial resources. Hence, in AffectNet+ we attempt to automatically generate soft-labels using deep learning-based methods.\n\nIn order to generate soft-labels automatically for both the training and validation sets of AffectNet, we used our multi-annotated set (MAS). For more detail on the MAS refer to Supplementary Materials. We divided MAS into training and test sets. For each emotion, we selected 100 images with the most obvious facial expression as the test set, called test-MAS. To clarify, if all the human annotators agreed on a facial expression the respective image was a candidate for our test set. The rest of the images in MAS were considered as the training set, which we refer to as train-MAS. Table  1  shows the training and test set configuration created from the multi-annotated set (MAS).\n\nIn the next step, we designed and utilized two solutions to calculate soft-labels for each image in the training and validation set of the AfectNet dataset. particularly, this paper introduces AffectNet+ by adding soft-labels, three level of data complexity, as well as a set of additional metadata, to the AffectNet dataset. To assign soft-label to each image, we calculated the probability score of all the emotions. Accordingly, we proposed the following methods: 1-Ensemble of binary classifiers, and 2-AU-based classifier. In the following, we explain each method.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Ensemble Of Binary Classifiers (Ebc)",
      "text": "Categorical state-of-the-art models  [6] -  [8]  face a high confusion rate while distinguishing between emotions that exhibit significant similarities, such as Neutral and Contempt. To alleviate this challenge, we proposed 8 binary classifiers, each trained to detect one facial expression in a one-vs-rest way. In fact, instead of using a convolutional neural network to predict the probability score of all the facial expressions at once, we introduced 8 different CNNs, each trained to detect only one facial expression. Moreover, to increase the confidence of the prediction, we utilized an ensemble of binary classifiers by the following CNNs: ResNet-50  [36] , EfficientNet-B3  [72] , and XceptionNet  [73] . Fig.  2  demonstrates the architecture of our binary classifier. We ensembled three of these binary classifiers, with different network architectures, to achieve more robust results.\n\nTraining: Training binary classifiers using train-MAS needed first choosing a set of positive and negative samples. Assume we train a binary classifier to predict Sad emotion, all the images in the training set annotated as Sad are taken as the positive samples, and the rest can be chosen as the negative samples. One naive approach is to choose all the images labeled as desired facial expressions as positive and the rest as negative samples, resulting in an imbalanced training set, and accordingly a biased classifier. Thus, we proposed a novel positive-negative selection strategy to ensure the high accuracy of the classifiers.\n\nWe utilized the correlation between the AUs corresponding to different emotions to choose the ratio of the negative samples. For training a binary classifier, to detect the facial expression of emotion emo i , we selected the maximum number of negative samples from the images annotated as emo j , where emo j has the highest AU correlation with emo i . As certain emotions may not share any similar AUs, we always chose 20% of the negative samples randomly to ensure a uniform distribution from all the other emotions. The remaining negative samples were allocated proportionally based on the similarity ratio of the corresponding AUs between emo i and the emotions that share similar AUs. Table  2  shows the AUs associated with each emotion class.\n\nConfidence Score Calculation: We introduced the term confidence score to indicate the level of trustworthiness in the prediction of each binary classifier model. For each binary classifier, the confidence score is defined as the average per-class accuracy. Since we followed a one-vs-rest training approach, the number of negative samples was far more than the number of positive samples. To tackle this imbalanced distribution, we defined the confidence score of each emotion class emo i as follows in Eq. 3:\n\n).\n\n(\n\nWe used the confidence score for each binary classifier in the inference, for adjusting the probability score assigned to a facial image considering each emotion class. Table  4  shows the confidence scores of each binary classifier. It is also notable that we report this score as the average accuracy, Acc, in Sec. 4.\n\nInference: We also leveraged the semantic score associated with the ensemble of the binary classifiers, called SC EB . This score indicates the existence of the emotion emo i ∈ EM OT ION S in an arbitrary facial image img k . It is calculated using the multiplication of the corresponding probability (P ) and the confidence score (CS EB ), as follows in Eq. 4:\n\nIn the ensemble of binary classifiers model, for each emotion class, we have three P functions, with their corresponding confidence scores. For an emotion class emo i , we calculated the ensemble of the semantic scores as the average score of three binary classifiers as follows in Eq. 5:\n\nIn this equation, RN , EN , and XN refer to ResNet-50  [36] , EfficientNet-B3  [72] , and XceptionNet  [73] , respectively. Table  6  shows per-class confidence scores for each classifier (indicated as Acc). We will use these scores in Sec. 3.4 to calculate soft-labels.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Action Unit (Au)-Based Classifier",
      "text": "In this section for each emotion class emo i we trained a model to learn the corresponding AU-based representation vector. We proposed a novel algorithm that utilizes the representation vector (AU vector), generated by each model to estimate the probability of the corresponding facial expression. In contrast to the ensemble of binary classifiers, which utilized hard-labels for training, the AU-based classifier used an AU-based representation of each emotion, resulting in a fine-grained analysis of facial expressions. Unlike the previous studies  [60] ,  [75] , where neural networks were trained to learn AUs specifically for FER or valence-arousal estimation, our novel method leveraged AUs only as a more comprehensive representation. We proposed a deep neural network that tends to learn the AUs presented in a given image.\n\nAs Table  2  shows, for each emotion class, there exists a set of AUs, which can be used as a representation vector. The AUbased representation vector can explicitly convey the inter-class similarity. Thus, training a CNN model, to learn and capture the unique AU-based representation vector of each emotion class, can potentially assist the neural network to better learn facial expressions from facial images.\n\nTraining: To train the AU-based classifier, we first defined the representation vector for each emotion class. We used 21 different AUs to model 7 basic facial expressions (refer to Table  2 ). Hence, the length of the representation vector was 21. We showed the set of AUs as follows in Eq. 6:\n\nFor each emotion emo i ∈ EM OT ION S, we first referred to Table  2  to identify the corresponding set of action units, denoted as AU set i . Then, we constructed an AU-based representation vector AU i for emo i , where all indices were initialized to zero except for those corresponding to the action units in AU set i , which were set to 1. This resulted in a sparse vector where the majority of values were zero, indicating the absence of the corresponding AUs, while the non-zero values (ones) indicated the presence of the specific AUs associated with emo i .\n\nFor each emotion emo i ∈ EM OT ION S, we trained a model to learn the corresponding AU-based representation vector AU i . As the synergy between two related tasks can improve the overall performance of the models  [76] , we designed our models to simultaneously generate the representation vectors, as well as performing a binary classification task.\n\nWe followed the approach described in Sec. 3.2 for choosing the positive and negative samples. As Fig.  3  illustrates, the multihead model ResNet-50  [36]  that we used for our AU-based classifier, consisted of two fully connected (FC) layers. Each head was responsible for a specific task. A binary classifier head focused on labeling the input sample as a negative or positive sample. Another head extracted the AU-based representation vector.\n\nOn the one hand, to train the binary classifier head, we used a Softmax activation function after the last fully connected (FC) layer, and binary cross entropy (CE) as the loss function. Thus, the output of the binary classification task was a 2 dimensional vector called Binary Probability Vector (BPV).\n\nOn the other hand, to generate the AU-based representation vector, we utilized the Sigmoid activation function following the final FC layer. Using the Sigmoid function, we forced the model to learn the value of each element of the representation vector. Further, we used the multi-label cross entropy (CE) as the loss function. The output of this head was an AU-based representation vector with the size of 21. This vector later helped us to score each emotion based on its corresponding action units.\n\nWe created two one-hot weight maps, ω pos and ω neg , where ω pos showed the active AUs for an image, and ω neg indicated its inactive AUs. The length of this positive and negative weight maps was equal to the length of the AU k  (21) . Finally, we defined our multi-label cross entropy loss as Eq. 7:\n\nwhere AU k and ÂU k are the ground truth and the generated AUbased representation vectors, respectively, and N is the number of training set samples. To explain more, we considered each element in ÂU k as a binary classification task.\n\nConfidence Score Calculation: We introduced the confidence score as a metric to track the accuracy of the AU-based classifier. Since the AU-based classifier performs two tasks (binary classification, as well as generating an AU-based representation vector), we derived the prediction by taking the average of the probability scores associated with each task. Then we used the average accuracy as the confidence score.\n\nFor the AU-based representation vector, we proposed a novel algorithm to assess the similarity between the predicted representation vector and the corresponding ground truth. We proposed a weighting strategy based on the ratio of the presence of an action unit in the emotion set EM OT ION S, and accordingly, assigned a score to each AU. We defined the score for each AU to be inversely proportional to the frequency of its presence within the emotion set. Hence, the less frequently an AU appears in the emotion set, the greater its score will be. To illustrate, AU14 exclusively appears in Contempt, while AU25 appears in four expressions, Happy, Fear, Disgust, and Anger. Hence, we assigned a score of 1 to the former action unit (AU 14) and 1  4 to the latter (AU 25). In Eq. 8, we defined the score vector of the AUs, known as AU S k1×n , such that its i th element represents the score corresponding to the i th element of the AU. AU S := {0.33, 0.5, 0.33, 0.33, 0.5, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 0.33, 1.0, 1.0, 1.0, 1.0, 0.25, 0.25, 0.5}.\n\nFor an image img k , we introduced the similarity vector SV k1×8 in Eq. 9, such that j th element represents the similarity between generated and ground truth AU-based representations.\n\nAU emoj is the AU-based representation vector for the emotion class emo j ∈ EM OT ION S, while ÂU shows the generated representation vector. AU i emoj and ÂU i are the i th elements of AU emoj and ÂU , respectively. Likewise, sim emoj is the weighted sum of non-zero elements in the generated ÂU and ground truth AU of emo j . It is notable that for Neutral, where all the elements in AU N eutral are zero, we define sim N eutral = 0.25 as a hyper-parameter.\n\nIn the next step, we introduced the corresponding binary similarity vector, BSV k1×2 , as follows in Eq. 10:\n\nHappy Classifier Sad Classifier Fig.  3 . Architecture of the AU-based classifier for each expression, as the second model of the soft-labeling process. For each emotion class, a multi-head ResNet-50  [36]  classifier is trained to simultaneously learn the features in the AUs and the expressions. Each model is trained to find the relation between the expressions and AUs. There are eight instances of this network architecture, trained for each expression. Similar to the initial model (EBC), each expression is trained in a binary one-vs-rest way, and their output aggregates to make the expression vector.\n\nwhere gt ∈ {0, ..., 7} is the index of the ground truth emotion class. In fact, BSV k means that for the img k , we calculated the score of the expected expression versus the average of the other expressions. Afterward, we calculated the AU-based binary probability vector AP V k1×2 using the corresponding similarity vector BSV k as follows in Eq. 11:\n\nIn addition, for the binary classification task in Fig.  3 , we defined BP V k1×2 as the binary probability vector associated with img k . Finally, the element-wise sum between BP V k and AP V k is used for the ultimate classification.\n\nWe followed the approach described in Sec. 3.2, and used the average accuracy as the confidence score. Table  4  shows the confidence scores of each expression.\n\nInference: For any image in the training set of AffectNet, we measured the probability of the presence of the emo i ∈ EM OT ION S, following the approach explained in Sec. 3.3, using Eq. 12. We measured the AU-based Semantic Score (SC AU ) as follows in Eq. 13:\n\nSC AU (emo i , img k ) := CS AU (emo i )P (emo i |img k ),  (13)  where CS AU is the confidence score of the AU-based classifier, calculated by Eq. 3, and P is the AU-based binary probability vector introduced in Eq. 12. See Table  7  for per-class evaluation scores associated with the AU-based classifier.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Creating Soft-Labels",
      "text": "For any image in the training and validation sets of AffectNet, we introduced the soft-labels using SC EB , the semantic scores of the ensemble of the binary classifiers, and SC AU , the semantic scores of AU-based classifier, as follows in Eq. 14:\n\nAs Eq. 14 expresses, we defined soft-labels as a set containing the average of the SC EB , and SC AU for each emotion class.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Proposed Affectnet+ Dataset",
      "text": "The AffectNet+ database is similar to its ancestor, AffectNet, regarding the images in both training and validation sets. Moreover, the original hard-labels assigned by human annotators have been retained without modification. By introducing soft-labels for each image in the original AffectNet dataset, we propose AffectNet+, which also includes three distinct subsets and supplementary metadata for each image. The distribution of AffectNet+ train set over different subsets of Easy, Challenging, and Difficult. The Easy subset determines the set of images that the model and the annotator agree on their expression. The Challenging subset refers to the images that the annotator and the model do not agree on, but their label is in the model's top-3 predictions. The Difficult subset determines the samples their label is out of the model's top-3 predictions.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Neutral",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "The Affectnet+ Subsets",
      "text": "For both the training and validation sets of AffectNet, we introduced 3 different subsets (Easy, Challenging, and Difficult) using the relation between the soft-labels and the hard-labels.\n\nWe defined the Easy subset as the group of images where the emotion with the highest probability in the soft-label matches the hard-label. Since the highest probability in the soft-label aligns with the hard-label, it suggests that the facial expression in these images is clear and vivid. As a result, the images in the Easy subset are likely to exhibit distinct and easily recognizable facial expressions.\n\nNext, we introduced the Challenging subset, consisting of the images where the emotion class associated with the hard-label, falls within the second or the third-ranked highest probability in the corresponding soft-label. To put it simply, although the humanassigned labels (hard-labels) may not be the highest probability option for the corresponding soft-labels, they still hold a relatively high ranking. Consequently, recognizing the facial expression might be more difficult compared to the images in the Easy subset as the images within this set exhibit complexities or variations that make it less straightforward to identify the primary perceived emotion.\n\nFinally, any images not belonging to either the Easy or the Challenging subsets categorized the Difficult subset. The humanlabeled annotations (hard-labels) for these images are different from the facial expressions that can be perceived from the corresponding soft-labels, indicating the fact that these images represent the most complex and ambiguous cases in terms of recognition of facial expressions.\n\nProviding the Easy, Challenging, and Difficult subsets allows for the development of different FER models. To illustrate, a classifier trained over the Easy subset can perform more accurately where the facial expressions in the images are high intensity and clearly distinct, while it may face difficulties and confusion in subtle facial images. An ensemble of 3 classifiers, each trained on one of the AffectNet+ subsets, would eventually improve the performance of FER applications specifically in-the-wild settings.\n\nAccording to Table  5 , which shows a per-class distribution of the AffectNet+ subsets, a significant portion of Happy and Neutral emotions, accounting for 86.25% and 68.67%, respectively, are within the Easy set, indicating that the facial expressions associated with these two classes are more obvious compared to other classes. On the contrary, only 12.72% of Contempt, the least within all the emotion classes, falls under the Easy subset, indicating a high degree of ambiguity associated with this class. In the Easy subset, there exists the maximum number of Happy and Neutral facial images, while the Contempt and Disgust expressions are the least represented images. Likewise, this trend exists in the Challenging and Difficult sets. The imbalanced training set causes challenges for training Hard-FER models, addressed using a combination of up-sampling and weighted loss. However, Soft-FER models require no such adjustments during training. In addition, the class imbalance is apparent in the validation set, highlighting the need for reporting metrics like F-1 score and average accuracy, alongside accuracy. Overall, for most emotions, over two-thirds of the images fall within the Challenging and Difficult sets, highlighting complexities in interpreting facial expressions. Consequently, while the traditional hard-label struggles to represent the full expression spectrum, the proposed soft-labels provide information regarding the combination of various expressions with different intensities.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Per-Subset Analysis",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Per-Expression Analysis",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Affectnet+ Metadata",
      "text": "To further enrich the AffectNet+ dataset, for each image in the training and validation set, we provided gender, age, ethnicity, two new sets of facial landmark points (68-point and 28-point), and head pose as metadata, using pre-trained deep learning-based models. For gender classification, we used the model proposed by Rothe et al.  [77] . For both age, and ethnicity classification,",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Experimental Results",
      "text": "In this section, we first elaborate on the ensemble of binary classifiers, explain the implementation detail and evaluation method, and analyze the models' performance. Then, we assess the performance of our proposed AU-based classifier and review the details of its implementation. Finally, we introduce new baseline models for both Hard-FER and Soft-FER on each subset of the AffectNet+ dataset.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Ensemble Of Binary Classifiers Results",
      "text": "Training: We selected ResNet-50  [36] , EfficientNet-B3  [72] , and XceptionNet  [73]  as our backbone models. We trained each model for every emotion class individually (one-vs-rest) using the train-MAS subset. With these three backbone models and eight expressions, we generated a total of 24 different decision-makers. For the training step, we followed the methodology described in Sec. 3.2. To this end, we split data into the positive and negative samples. Positives were the samples with a specific label (like Happy), and negatives were the rest. To train each model, our method re-scaled each image to the size of 224 × 224 and utilized the Adam optimizer  [80]  with learning -rate = 10 -3 , β 1 = 0.9, β 2 = 0.999, and decay = 10 -5 , for 25 epochs with a batch size of 50. We implemented our models using TensorFlow and ran them on Nvidia GPUs. Test: To evaluate the performance of our trained binary classifiers, we leveraged the test-MAS subset. This set included 800 uniform samples, therefore for every binary classifier (like Happy), we had 100 positive and 700 negative samples. As mentioned earlier, we ensembled three models, including ResNet-50  [36] , EfficientNet-B3  [72] , and XceptionNet  [73]  models. Different popular metrics, including precision, recall, F-1 score, accuracy, as well as average accuracy (See Eq. 3), were used for evaluating the ensemble of binary classifier (EBC) models. A summary of these metrics is shown in Table  6 , while the full table is provided in Supplementary Materials.\n\nThe reported accuracy (shown by Acc) in Table  6  depicts that in our one-vs-rest model training, we could reach high accuracies for all the expressions, which is an indication of our models' robustness. All three classifiers significantly boosted the accuracy of the least provided samples (like Contempt). This table highlights that the Acc varied between 75% and 92% for all the classifier models, per expression. Meanwhile, the standard deviation of the classifiers' Acc for all the expressions was 3.95%, 4.07%, and 3.72%, for ResNet-50  [36] , EfficientNet-B3  [72] , and XceptionNet  [73] , respectively. The Acc in the last section of this table demonstrates the results of an ensemble of three aforementioned models, where the Acc per expression changed in the higher range of 87% to 93%, and the standard deviation was lower than each of the three models (3.36%). On the other hand, the average accuracy (shown by Acc) tried to highlight the impact of the imbalance distribution of the validation set (100 positive samples versus 700 negative samples). The difference between Acc and Acc of the Neutral, Happy, Sad, Surprise, Fear, and Anger expressions was not eyecatching. This fact, bolds the low impact of the imbalance data distribution on our training method. Notably, the highest impact of the imbalance distribution was shown on Disgust and Contempt expressions on the three ResNet-50  [36] , EfficientNet-B3  [72] , and XceptionNet  [73]  models. However, even the reported Acc on these two expressions was considerable for all of the models. Thanks to the method we utilized for the ensemble of a binary classifier, we boosted Acc of Disgust and Contempt expressions to 86.56% and 78.51%, respectively. This fact demonstrates the effect of our ensemble model on the imbalanced data. In summary, analyzing Table  6  illustrates the reliability of the proposed ensemble of binary classifiers (EBC) model, with high accuracies and low standard deviations for all the expressions, useful for the expression classification and soft-labeling.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Action Unit (Au)-Based Classifier Results",
      "text": "Training: We selected ResNet-50  [36]  as the backbone of our AU-based classifier. As described in Sec. 3.3, we modified the last layer of ResNet-50  [36] , such that the model has two outputs, the binary probabilities, and the AU-based representation vector. For each emotion class, we trained the corresponding model individually, using the train-MAS subset of images. The binary probability refers to the probability distribution over different classes, while the AU-based representation indicates the intensity of an AU in an image. We trained the models using the images with a size of 224 × 224 pixels. We used the Adam optimizer  [80]  with learning-rate = 10 -4 , β 1 = 0.9, β 2 = 0.999, and decay = 10 -6 , for 40 epochs with a batch size of 50. We implemented these models in Tensorflow with the same GPU used for binary classifiers.\n\nTest: As described in Sec. 3.3, we first utilized a postprocessing algorithm to convert the AU-based representation vector to a binary probability vector. Next, we took the average of the probability vectors of the binary classification task and the AU-based representation vector task, as the final decision of each model.\n\nSimilar to the ensemble of binary classifiers, we evaluated our AU-based classifier over test-MAS. Precision, recall, F-1 score, accuracy, and average accuracy were the selected metrics for this analysis. Table  7  shows the accuracy and average accuracy of the AU-based classifier. To see the full results, refer to Supplementary Materials. AU-based classifier worked well for the expressions Neutral, Happy, Sad, Surprise, Disgust, and Contempt. The accuracy (shown by Acc) over these expressions was in the range of 67% to 85%. However, the accuracy for two expressions, Fear and Anger, was lower than the other expressions. The high number of common action units between the Fear expression and other expressions was the reason for its lowest accuracy among all the expressions. There were 4 common action units between the two expressions Fear and Anger, which were highly activated in both the Fear and Anger facial samples. Fear also had 5 common action units with Surprise, which were less activated in the facial samples, and affected the accuracy of the Fear expression. On the other hand, the average accuracy (shown by Acc) of the AU-based classifiers was higher than 60% for all the expressions. This table demonstrates that with a subtle analysis of the facial expressions, using their action units, we could extract valuable information for the expression classification and soft-labeling.\n\nTo evaluate the role of the AU-based classifier in the final decision-making, we conducted an experiment. We trained a model (ResNet-50  [36] ) to label images without and with an AU-based classifier. This experiment showed that for all the expressions the accuracy increased when we added an AU-based classifier to our baseline model. The progress over the average accuracy was eye-catching (up to 10%). To see the table of this experiment refer to Supplementary Materials.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Baseline Models For Affectnet+",
      "text": "In this section, we provide a set of new baselines for both Hard-FER and Soft-FER methods on AffectNet+. We used ResNet-50  [36]  as the backbone for both methods. We trained the baseline models on the training set of AffectNet+, and assessed their accuracy and performance on the validation set of AffectNet+.\n\nBaselines for Hard-FER: For Hard-FER, we trained our baseline model using the hard-labels, provided by the human annotators. For each subset of AffectNet+, we trained one baseline model and evaluated its accuracy and performance on its corresponding subset in the validation set. Table  8  shows the accuracy and the average accuracy of Hard-FER baseline models.\n\nAccording to Table  8 , the baseline model achieved the highest accuracy (85.86%) on the Easy subset, by far greater than the accuracy on the Challenging and Difficult subsets, 51.62% and 34.34% respectively. These results are expected since as elaborated in Sec. 3.5.1, the subsets within AffectNet+ vary in terms of facial expression intensity and ambiguity, which directly influences the accuracy of FER. The images within the Easy subset tend to have high-intensity facial expressions, while the faces in the Challenging and Difficult sets tend to have less intense, more ambiguous expressions.\n\nTable  9  shows precision, recall, and F-1 score for each subset of AffectNet+. This table reveals that, over all the sample data, the baseline model achieved the highest F-1 score for the Happy class (66.05%), and the lowest for Contempt (25.32%). We witnessed a similar pattern for the Easy subset, where the F-1 scores for Happy and Contempt classes are 95.55% and 51.94%, respectively. However, for the Challenging and Difficult subsets, the lowest F-1 score was achieved for the Neutral and Happy classes. It can be concluded that although Happy and Neutral were among the most obvious and less ambiguous emotions for FER, in subtle cases can still be extremely difficult to recognize these emotions. Overall, as we expected, the F-1 score reduced on the Challenging and Difficult subsets, in comparison with the Easy subset.\n\nFig.  5  shows the confusion matrices of the baseline model for every subset of AffectNet+. The model faced the least confusion on the Easy set, while the highest level of confusion occurred on the Difficult set. This figure indicates that images within the    [36] ) for every subset of AffectNet+ (Easy, Challenging, and Difficult). The baseline model is trained over any subset, separately. Then, the models are evaluated over all the samples in the evaluation set, regardless of their subset.\n\nformer set include obvious facial expressions that are unambiguous and easy to recognize, whereas the latter comprises images with less distinct facial expressions. Furthermore, considering all the subsets, the highest degree of confusion happened between the facial expression of Contempt and either Neutral or Happy, and the second highest level of confusion was between Disgust and Anger.\n\nFor the Challenging and Difficult sets, we observed a high level of confusion between the facial expressions of Sad and Anger, as well as between Surprise and Fear. The high level of confusion between facial expressions associated with specific emotions, indicating the intra-class variations, as well as inter-class similarities, clearly explains how Hard-FER results in an inaccurate FER model and illustrates the effectiveness of our proposed Soft-FER method.\n\nBaselines for Soft-FER As described in Sec. 3.1, in our proposed Soft-FER methodology, the neural network trained to predict the probability scores for facial expressions associated with each individual emotion class. Since the prediction of softlabels is a regression task, we utilized mean error, failure rate, and Area Under the Cumulative Errors Distribution curve  [81]  as the evaluation metrics.\n\nTo better evaluate the model performance, we proposed a weighted error mechanism to measure the error between the ground truth and the generated soft-labels. We assigned a weight to each element of an arbitrary ground truth soft-label, based on its relative magnitudes. To clarify, the weight associated with the i th element is proportional to its relative magnitudes, such that the largest element will be receiving a weight of 1, the second largest element, a weight of 1  2 , and so on (the weight 1 8 will be assigned to the smallest element). The weighting mechanism ensured that the elements with higher values in a ground truth soft-label are considered more important compared to the elements with lower values. We calculated the Weighted Mean Absolute Error (W-MAE) as follows in Eq. 14:\n\nwhere N is the number of images in the validation set, n is the number of emotions in the EM OT ION S set, SL i k and Ŝ L i k are the i th elements of the ground truth, and the predicted softlabels, respectively, associated with the k th image. Finally, w i k is the weight of the i th element of the soft-label corresponding to the k th image.\n\nBuilding upon W-MAE, we proposed the Weighted Failure Rate (W-FR), a metric to show the robustness of the models. To calculate the W-FR, first, we defined a threshold, called ϵ. Then, an individual prediction was considered a failure if the weighted error between the ground truth and its corresponding predicted soft-label was greater than ϵ = 0.3. W-FR is defined as the portion of these failures among all predictions.\n\nTable  10  shows the W-MAE and W-FR for each subset of AffectNet+. Similar to Hard-FER, W-MAE, and W-FR are small for the Easy subset (17.30% and 10.58%, respectively), and large for the Difficult subset (21.21% and 18.66%, respectively), representing the degree of difficulty of facial expression recognition for each subset.\n\nIn Table  11 , we provided a per-emotion analysis of the performance of the baseline model in Soft-FER. Overall, for the Easy set, we observed the lowest W-FR and W-MAE values. The highest values were observed in the Difficult set. Considering all the samples in the validation set (marked as All in Table  11 ), the baseline model performed the best in terms of recognizing Happy expression, and the worst in terms of recognizing Fear and Disgust expressions. For the Easy set, the baseline model achieved the lowest W-FR and W-MAE on Happy, Neutral, and Contempt. Contrary to Hard-FER, where the baseline model has a high confusion rate between the Neutral and the Contempt expressions, Soft-FER showed an improved performance. This occurred because Soft-FER considered each emotion class individually and predicted the probability scores associated with each class given a facial image.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Subjective Evaluation Of Soft-Lables",
      "text": "The concept of soft-labeling offers a more nuanced representation of data and helps soften the classification boundaries in models. It could also provide insights into compound labeling, as noted by many recent studies. In addition to model-based evaluations, human assessment is crucial for evaluating the potential benefits of soft-labeling. We thus conducted an experiment with human participants to compare the utility of soft and hard labels for accurately reflecting people's subjective perception of emotion on others' faces.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Subjective Test Design",
      "text": "There are two key questions about the soft-labeling approach compared to the traditional hard-labeling approach: from a human perspective, 1) which approach is more informative for explaining the expressions of a facial image, and 2) how accurately can softlabel describe the expressions of a facial image. We selected 6 students from a diverse pool of candidates, ensuring a range of ages, genders, and racial backgrounds. Their task was to review a large set of facial images from the AffectNet+ evaluation set and respond to two key questions. The evaluation set consisted of 500 images for each of the eight expression categories, totaling 4000 images. Since there were 2 questions per image, this resulted in 8000 questions. Additionally, 30% of these questions were repeated for reliability: 20% involved selfevaluation, and 10% were for circular user agreement, where each user was compared with 2 other users. In total, we had 10,406 questions, randomly and equally distributed among the experimenters.\n\nFor the first experiment, we showed each experimenter a random facial image and asked him/her to select the best facial image descriptor among hard-labels, soft-labels, both, and none. In the other experiment, we showed each experimenter a facial image accompanied by two soft-labels and asked him/her to select the soft-label related to the facial image, among the related and a randomly selected soft-label. The soft-labels were created using the approach described in Section 3.4, while the hard-labels were the original human annotations from AffectNet. To find more details on these two experiments refer to Supplementary Materials. All participants were students from the University of Denver, aged 20-45 years. The study was conducted under an approved IRB, and the students provided consent. The group included 4 males and 2 females, representing diverse racial backgrounds: Asians (2 students), Hispanic-Latino, Caucasian, Middle-Eastern, and White-Asian. We organized a training session for all participants to review universal facial expressions and their associated facial indicators and appearances. To ensure they were adequately trained, we asked each participant to label 40 images from our dataset and evaluated their performance against the image labels. A 75% agreement with the labels was required to qualify, and all participants passed this exam before beginning the main experiment. Each participant was then randomly assigned approximately 1,735 images and given 7 days to complete the task.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Subjective Test Analysis",
      "text": "Fig.  6  shows the results of two experiments, and the percentage of agreement between subjects. As Fig.  6 -a demonstrates, on average, human subjects preferred soft-labels in 65% of the first experiment, compared to only 22% for hard-labels. Additionally, 5 out of 6 participants selected soft-labels as the best descriptor overall.\n\nThe results indicate that, on average, humans preferred soft-labels over traditional hard-labels as the better image descriptor.\n\nFig.  6 -b evaluates the reliability of soft-labels, where in some test cases only the intensity of the expression posed a challenge for the experimenters. The results illustrate tht, given an accurate versus a random soft-label, participants could identify the accurate soft-label in 81% of the questions. The accuracy of each participant in the second experiment varied between 77% and 84%.\n\nFinally, Fig.  6 -c examines how accurately the participants answered the questions. We asked 30% of the images more than once to evaluate the self-agreement and pairwise agreement between the participants. The average accuracy across all the participants was 80%, with a maximum of 90% and a minimum of 64%. These results show that the users' agreement was also notable, as participants largely agreed on the common questions, with an average agreement of 69%, which is high for FER tasks.\n\nThe results of these experiments confirmed that, from a human perspective, the concept of soft-labeling provides a more accurate and intuitive description of facial expressions. This is particularly relevant as many facial images convey more than one distinct expression with varying intensities.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Future Research Direction",
      "text": "Mixed facial expressions are common in real-life emotional displays, making them important to consider when studying both human and computer-based recognition of facial affect. Many recent studies in FER thus focus on compound-labeling and soft-labeling. Applying soft-labels allows for more nuanced FER, more flexibly responding to the true complexity of facial expressions as they are produced in the wild, with subtlety and sometimes multiple emotions conveyed at the same time. AffectNet+ can open some windows to the problems that need compound-labeling or softlabeling. The complexity of FER datasets can be originated from extrinsic and intrinsic challenges. Extrinsic challenges originate Fig.  6 . The results of the subjective tests highlight the importance of the soft-labeling approach from a human perspective. Subfigure (a) demonstrates that subjects preferred soft-labels over hard-labels to describe the images. Subfigure (b) shows that subjects were able to distinguish between accurate and random soft-labels for individual images. Subfigure (c) depicts the self-agreement of each participant and the inter-agreement between them. Self-agreement is indicated by the nodes, while the edges represent pair-wise inter-agreement. . from extrinsic factors such as illumination, camera quality, and query type (for in-the-wild datasets). Besides, intrinsic challenges occur because of noisy labels, relative relation of expressions, intensity of expressions, diversity of the facial samples, head pose, eye movements, etc. AffectNet+ provides the opportunity to focus on some of these issues. continuing, we introduce some of the future research directions AffectNet+.\n\n• AffectNet+ is a source for research on quantifying uncertainty in soft-label prediction.\n\n• Multi-labeling is another feature proposed by AffectNet+, where it allows FER models to predict even more than two expressions from a facial image.\n\n• Soft-labels provided in AffectNet+ can help future studies to reduce the effect of noisy labels.\n\n• Using AffectNet+ we can find smoother decision boundaries. Therefore, studying generalization over soft-labels and comparing them with hard-labels could be another future research direction using this dataset.",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "•",
      "text": "AffectNet+ can be a source to study imbalanced data and provide solutions to this challenge. This dataset can also be considered a multi-expression dataset, where the data distribution is less imbalanced. • AffectNet+ provides the intensity of soft-label expressions; therefore, designing FER models and loss functions that consider the labels and their intensity during training is effective in FER studies.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "•",
      "text": "The three subsets of AffectNet+ provide the opportunity to train and combine different models for each subset with various loss functions and regularizers.\n\n• Metadata provided in AffectNet+ is a valuable source for coping with imbalanced data in FER. Additionally, this dataset is practical for data augmentation and selfsupervised learning.\n\nThe aforementioned research problems highlight the importance and essence of AffectNet+ over facial expression recognition tasks. Best of our knowledge, AffectNet+ is the largest humanannotated in-the-wild dataset, accompanied by soft-labels and metadata, for the next studies on facial expression recognition.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Conclusion",
      "text": "Automated Facial Expression Recognition plays a crucial role in understanding human emotions and has diverse applications in healthcare, autonomous driving, and education. The advent of deep learning techniques, such as Convolutional Neural Networks and Vision-Transformers, has significantly improved the accuracy of FER methods. However, FER remains challenging due to intraclass variations, inter-class similarities, and cultural differences in perceiving and judging facial expressions. The existing FER datasets suffer from limited annotations, noisy labels, and biased models, hindering the development of robust and reliable FER systems.\n\nTo alleviate these challenges, we proposed Soft-FER, a novel approach for FER, alongside the traditional (Hard-FER). We introduced the concept of soft-labels in the FER datasets, which provides the probability score of facial expression existence for each emotion class in an arbitrary image. Compared to the traditional hard-labels, where we assign only one label to images, soft-labels are more explanatory, enabling a more comprehensive and nuanced representation of emotions and resulting in the development of more accurate aromatic FER solutions. We proposed two novel methods, an ensemble of binary classifiers and an AUbased classifier, for an accurate calculation of soft-labels for each image in AffectNet.\n\nBuilding upon AffectNet, we proposed the AffectNet+ dataset, by adding soft-labels to each image and providing additional metadata. Moreover, we introduced 3 new subsets (i.e., Easy, Challenging, and Difficult subsets) to AffectNet+, based on the difficulty of the recognition of facial expressions. AffectNet+ has the potential to be utilized to enhance the performance and robustness of FER systems, resulting in a better interpretation of human facial expressions.",
      "page_start": 14,
      "page_end": 15
    },
    {
      "section_name": "Ii Multi-Annotated-Set (Mas)",
      "text": "AffectNet contains a non-released subset, referred to as multiannotated-set (MAS), which includes more than 36K images. The images are annotated by at least two, and at most five, well trained human annotators. Since every image within the MAS is annotated with at least two well-trained human annotators, this subset is less noisy compared to the AffectNet public subsets.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Name",
      "text": "Attributes AffectNet  [26]  I: ∼1M, Exp: 8, W, V, A, MD, MA: ∼440K RAF-DB  [17]  I: ∼30K, Exp: 19, W, MA, MD, CP CK+  [60]  VS: 593, Exp: 7, C, P, AU: 30, MA: 327 Aff-Wild  [101]  VS: 500, I: 10K, AU: 16, S, W, V, A Aff-Wild2  [102]  VS: 260 (+ Aff-Wild), W, V, A FER-Wild  [61]  I: ∼120K, Exp: 7, W, MD, MA: 24K MultiPie  [103]  I: ∼750K, Exp: 6, C, P MMI  [104]  VS + I: ∼1.5K, Exp: 6, AU: 31, C, P, MD DISFA  [105]  VS: 27, AU: 12, C, S, MD RECOLA  [106]  VS: 46, Exp: 5, S, V, A, MD AM-FED  [107]  VS: 242, AU: 16, S, MD DEAP  [108]  VS: 32, C, S, V, A, MD AFEW  [62]  VS:1426, Exp: 7, W, MD SFEW  [109]  I: 700, Exp: 7, W, MD FER-2013  [27]  I: ∼36K, Exp: 7, W EmotioNet  [18]  I: 1M, Exp: 23, AU: 15, W, AA, CP FERG  [110]  I: ∼56K, Exp: 7, Synthesized Cartoon Images Oulu-CASIA  [111]  I: ∼3K, Exp: 6, C, P, NIR AR Face  [112]  I: ∼4K, Exp: 4, C, P JAFFE  [113]  I: 219, Exp: 7, C, P, Japanese Females GFT  [114]  VS: 96, AU: 20, C, S B4PD  [115]  VS: 41, Exp: 6, AU: 32, C, S, MD, 2D/3D B4PD+  [116]  VS:140, AU: 32, C, S, MD, 2D/3D, NIR 4DFAB  [117]  I: 1.8M Mesh, Exp: 6, P, S, MD, 3D/4D Belfast  [118]  VS: 1400, Exp: 3-5-7, C, S, V, MD DAiSEE  [119]  VS: ∼9K, Exp: 4, W, CP FER+  [63]  I: ∼36K, Exp: 8, W, CP ExpW  [64]  I: ∼90K, Exp: 7, W FEAFA+  [120]  VS: 150, AU: 24, C, P, S, W KDEF  [121]  I: 490, Exp: 7, C, P, A C-EXPR  [19]  VS: 400, Exp: 13, AU: 17, W, V, A, MA, MD MAFW  [20]  VS: 10K, Exp: 43, W, MA, MD CFEE  [21]  I: ∼5K, Exp: 22, AU: 17, C, P, AA, CP iCV-MEFED  [22]  I: ∼30K, Exp: 50, C, P, MA, CP Eight emotion classes in the MAS are Neutral, Happy, Sad, Surprise, Fear, Disgust, Anger, and Contempt, accompanied by three non-expression labels, including None, Uncertain, and Non-Face. None label expresses that the facial expression of the corresponding image was none of the eight emotions. Uncertain means the annotator was uncertain of the facial expression. Likewise, Non-Face indicates that the corresponding image was not a human facial image. Another remarkable point about the MAS is labeling expression to an image based on the majority voting between annotators. When there was a tie, e.g., two annotators labeled an image, one as Happy and the other as Surprise, the final label was selected as the keyword used for querying that image on the web. For more details on image collection and annotation process over AffectNet refer to  [26] .\n\nSince the MAS is annotated by more than one annotator, it is more reliable than the publicly available training and validation sets of AffectNet. Hence, to increase the performance of our proposed models, we used the MAS for training and test purposes. We split the MAS subset two subsets in order to train and test our models. We first created the test set, which we refer to as test-MAS, by randomly selecting 100 images from the MAS for each emotion. The only restriction we imposed for choosing images within the test-MAS was that all the annotators agreed on a specific facial expression. Hence, test-MAS was the most reliable subset, containing images with the most clear (least ambiguous) facial expressions. Based on this clarity, we used test-MAS to assess the accuracy and performance of our proposed models. Then, we chose the rest as the training set and called it train-MAS. These two sets were only used for the training and testing of our proposed ensemble of binary classifiers (EBC) and AUbased classifier.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Iii Additional Experimental Results",
      "text": "We utilized EfficientNet-B3  [72]  as our secondary baseline model.\n\nIn this section, we provided the experimental results regarding the performance of Hard-FER and Soft-FER models. Overall, the results of the secondary baseline model demonstrated better performance compared to the initial baseline model (ResNet-50  [36] ).",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "Iii.1 Hard-Fer Secondary Baseline",
      "text": "Table  IV  shows the accuracy and average accuracy of Hard-FER secondary baseline model. This model achieved the highest accuracy (87.06%) on the Easy subset, by far higher than the accuracy on the Challenging and Difficult subsets, 57.13% and 41.36%, respectively. Table VII illustrates precision, recall, and F-1 score for each subset of AffectNet+, on the secondary baseline model.\n\nEfficientNet-B3  [72]  reached to the highest F-1 score on the Happy expression (68.27%), and the lowest on Contempt (34.16%). We witnessed a similar pattern for the Easy subset, where F-1 score for Happy and Contempt was 97.16% and 59.09%, respectively. However, for the Challenging and Difficult subsets, the lowest F-1 score was obtained for the Neutral and Happy classes, similar to the scores reported for the main baseline model (ResNet-50  [36] ).",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "Iii.2 Soft-Fer Secondary Baseline",
      "text": "Table  V  shows W-MAE and W-FR for each subset of AffectNet+. Similar to Hard-FER, the minimum W-MAE, and W-FR belonged",
      "page_start": 22,
      "page_end": 22
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: illustrates the combination of two expressions in",
      "page": 1
    },
    {
      "caption": "Figure 1: Unlike the traditional approaches, where a single emotion label",
      "page": 2
    },
    {
      "caption": "Figure 1: illustrates examples of facial expressions with soft-labels, where",
      "page": 2
    },
    {
      "caption": "Figure 2: ), each designed to predict the",
      "page": 2
    },
    {
      "caption": "Figure 2: Architecture of ensemble of binary classifiers (EBC model), as the initial step of the soft-labeling process. It contains ensemble of three",
      "page": 5
    },
    {
      "caption": "Figure 1: shows, soft-labels are more explanatory compared",
      "page": 5
    },
    {
      "caption": "Figure 2: demonstrates",
      "page": 6
    },
    {
      "caption": "Figure 3: illustrates, the multi-",
      "page": 7
    },
    {
      "caption": "Figure 3: Architecture of the AU-based classifier for each expression, as the second model of the soft-labeling process. For each emotion class, a",
      "page": 8
    },
    {
      "caption": "Figure 4: depicts the distribution of emotion classes within Affect-",
      "page": 9
    },
    {
      "caption": "Figure 4: also shows the distribution of images in the AffectNet+",
      "page": 9
    },
    {
      "caption": "Figure 4: Distribution of the AffectNet+ sets, including training and validation sets, over different Easy, Challenging, and Difficult subsets.",
      "page": 10
    },
    {
      "caption": "Figure 5: shows the confusion matrices of the baseline model for",
      "page": 11
    },
    {
      "caption": "Figure 5: Confusion matrix of the baseline model (ResNet-50 [36]) for every subset of AffectNet+ (Easy, Challenging, and Difficult). The baseline",
      "page": 12
    },
    {
      "caption": "Figure 6: shows the results of two experiments, and the percentage of",
      "page": 13
    },
    {
      "caption": "Figure 6: -a demonstrates, on average,",
      "page": 13
    },
    {
      "caption": "Figure 6: -b evaluates the reliability of soft-labels, where in some",
      "page": 13
    },
    {
      "caption": "Figure 6: -c examines how accurately the participants",
      "page": 13
    },
    {
      "caption": "Figure 6: The results of the subjective tests highlight the importance of the soft-labeling approach from a human perspective. Subfigure (a)",
      "page": 14
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "%96"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "%96\n%"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 10: shows the W-MAE and W-FR for each subset of",
      "data": [
        {
          "Column_1": "ueNpaHdaS\nruS\naeF\nsiD\ngnAnoC",
          "NeuHap SadSurFea DisAngCon": ""
        }
      ],
      "page": 12
    },
    {
      "caption": "Table 11: , we provided a per-emotion analysis of the per-",
      "data": [
        {
          "Column_1": "ueNpaHdaS\nruS\naeF\nsiD\ngnAnoC",
          "NeuHap SadSurFea DisAngCon": ""
        }
      ],
      "page": 12
    },
    {
      "caption": "Table 10: shows the W-MAE and W-FR for each subset of",
      "data": [
        {
          "Column_1": "ueNpaHdaS\nruS\naeF\nsiD\ngnAnoC",
          "NeuHap SadSurFea DisAngCon": ""
        }
      ],
      "page": 12
    },
    {
      "caption": "Table 10: shows the W-MAE and W-FR for each subset of",
      "data": [
        {
          "Column_1": "ueNpaHdaS\nruS\naeF\nsiD\ngnAnoC",
          "NeuHap SadSurFea DisAngCon": ""
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "ueNpaHdaS\nruS\naeF\nsiD\ngnAnoC",
          "Neu Hap SadSurFea DisAngCon": ""
        }
      ],
      "page": 20
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "ueNpaHdaS\nruS\naeF\nsiD\ngnAnoC",
          "Neu Hap SadSurFea DisAngCon": ""
        }
      ],
      "page": 20
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "ueNpaHdaS\nruS\naeF\nsiD\ngnAnoC",
          "Neu Hap SadSurFea DisAngCon": ""
        }
      ],
      "page": 20
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "ueNpaHdaS\nruS\naeF\nsiD\ngnAnoC",
          "Neu Hap SadSurFea DisAngCon": ""
        }
      ],
      "page": 20
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "lartueN yppaH daS esirpruS raeF tsugsiD regnA metnoC elaM elameF naisA naidnI kcalB etihW tsaE-M cinapsiH 51-0 23-61 35-33 45 revO",
          "Column_2": "",
          "Column_3": "",
          "Column_4": ""
        }
      ],
      "page": 23
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "lartueN yppaH daS esirpruS raeF tsugsiD regnA metnoC elaM elameF naisA naidnI kcalB etihW tsaE-M cinapsiH 51-0 23-61 35-33 45 revO",
          "Column_2": "",
          "Column_3": "",
          "Column_4": ""
        }
      ],
      "page": 24
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "The expression of the emotions in man and animals",
      "authors": [
        "C Darwin",
        "P Prodger"
      ],
      "year": "1998",
      "venue": "The expression of the emotions in man and animals"
    },
    {
      "citation_id": "2",
      "title": "Emotional expressions beyond facial muscle actions. a call for studying autonomic signals and their impact on social perception",
      "authors": [
        "M Kret"
      ],
      "year": "2015",
      "venue": "Frontiers in psychology"
    },
    {
      "citation_id": "3",
      "title": "Constants across cultures in the face and emotion",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1971",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "4",
      "title": "Handbook of cognition and emotion",
      "authors": [
        "P Ekman"
      ],
      "year": "1999",
      "venue": "Handbook of cognition and emotion"
    },
    {
      "citation_id": "5",
      "title": "Universals and cultural differences in the judgments of facial expressions of emotion",
      "authors": [
        "P Ekman",
        "W Friesen",
        "M O'sullivan",
        "A Chan",
        "I Diacoyanni-Tarlatzis",
        "K Heider",
        "R Krause",
        "W Lecompte",
        "T Pitcairn",
        "P Ricci-Bitti"
      ],
      "year": "1987",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "6",
      "title": "Facial expression recognition in the wild via deep attentive center loss",
      "authors": [
        "A Farzaneh",
        "X Qi"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF winter conference on applications of computer vision"
    },
    {
      "citation_id": "7",
      "title": "Ad-corre: Adaptive correlation-based loss for facial expression recognition in the wild",
      "authors": [
        "A Fard",
        "M Mahoor"
      ],
      "year": "2022",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "8",
      "title": "Breg-next: Facial affect computing using adaptive residual networks with bounded gradient",
      "authors": [
        "B Hasani",
        "S Negi",
        "M Mahoor"
      ],
      "year": "2006",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "9",
      "title": "Face-mask-aware facial expression recognition based on face parsing and vision transformer",
      "authors": [
        "B Yang",
        "J Wu",
        "K Ikeda",
        "G Hattori",
        "M Sugano",
        "Y Iwasawa",
        "Y Matsuo"
      ],
      "year": "2022",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "10",
      "title": "Vision transformer with attentive pooling for robust facial expression recognition",
      "authors": [
        "F Xue",
        "Q Wang",
        "Z Tan",
        "Z Ma",
        "G Guo"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "11",
      "title": "Environmental Psychology & Nonverbal Behavior",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1978",
      "venue": "Environmental Psychology & Nonverbal Behavior"
    },
    {
      "citation_id": "12",
      "title": "The mechanism of human facial expression",
      "authors": [
        "G.-B De Boulogne"
      ],
      "year": "1990",
      "venue": "The mechanism of human facial expression"
    },
    {
      "citation_id": "13",
      "title": "The timing of facial motion in posed and spontaneous smiles",
      "authors": [
        "J Cohn",
        "K Schmidt"
      ],
      "year": "2004",
      "venue": "International Journal of Wavelets, Multiresolution and Information Processing"
    },
    {
      "citation_id": "14",
      "title": "What the face reveals: Basic and applied studies of spontaneous expression using the Facial Action Coding System (FACS)",
      "authors": [
        "P Ekman",
        "E Rosenberg"
      ],
      "year": "1997",
      "venue": "What the face reveals: Basic and applied studies of spontaneous expression using the Facial Action Coding System (FACS)"
    },
    {
      "citation_id": "15",
      "title": "What the face reveals: Basic and applied studies of spontaneous expression using the facial action coding system (facs)",
      "authors": [
        "N Szajnberg"
      ],
      "venue": "What the face reveals: Basic and applied studies of spontaneous expression using the facial action coding system (facs)"
    },
    {
      "citation_id": "16",
      "title": "Compound facial expressions of emotion: from basic research to clinical applications",
      "authors": [
        "S Du",
        "A Martinez"
      ],
      "year": "2015",
      "venue": "Dialogues in clinical neuroscience"
    },
    {
      "citation_id": "17",
      "title": "Reliable crowdsourcing and deep localitypreserving learning for expression recognition in the wild",
      "authors": [
        "S Li",
        "W Deng",
        "J Du"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "18",
      "title": "Emotionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild",
      "authors": [
        "Fabian Benitez-Quiroz",
        "R Srinivasan",
        "A Martinez"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "19",
      "title": "Multi-label compound expression recognition: C-expr database & network",
      "authors": [
        "D Kollias"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "20",
      "title": "Mafw: A large-scale, multi-modal, compound affective database for dynamic facial expression recognition in the wild",
      "authors": [
        "Y Liu",
        "W Dai",
        "C Feng",
        "W Wang",
        "G Yin",
        "J Zeng",
        "S Shan"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "21",
      "title": "Compound facial expressions of emotion",
      "authors": [
        "S Du",
        "Y Tao",
        "A Martinez"
      ],
      "year": "2014",
      "venue": "Proceedings of the national academy of sciences"
    },
    {
      "citation_id": "22",
      "title": "Dominant and complementary emotion recognition from still images of faces",
      "authors": [
        "J Guo",
        "Z Lei",
        "J Wan",
        "E Avots",
        "N Hajarolasvadi",
        "B Knyazev",
        "A Kuharenko",
        "J Junior",
        "X Baró",
        "H Demirel"
      ],
      "year": "2018",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "23",
      "title": "The handbook of culture and psychology",
      "authors": [
        "D Matsumoto"
      ],
      "year": "2001",
      "venue": "The handbook of culture and psychology"
    },
    {
      "citation_id": "24",
      "title": "The handbook of culture and psychology",
      "authors": [
        "D Matsumoto",
        "H Hwang"
      ],
      "year": "2019",
      "venue": "The handbook of culture and psychology"
    },
    {
      "citation_id": "25",
      "title": "Understanding multimodal emotional expressions",
      "authors": [
        "D Keltner",
        "D Cordaro"
      ],
      "year": "2017",
      "venue": "The science of facial expression"
    },
    {
      "citation_id": "26",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "27",
      "title": "Challenges in representation learning: A report on three machine learning contests",
      "authors": [
        "I Goodfellow",
        "D Erhan",
        "P Carrier",
        "A Courville",
        "M Mirza",
        "B Hamner",
        "W Cukierski",
        "Y Tang",
        "D Thaler",
        "D.-H Lee"
      ],
      "year": "2013",
      "venue": "International conference on neural information processing"
    },
    {
      "citation_id": "28",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "29",
      "title": "Deep self-learning from noisy labels",
      "authors": [
        "J Han",
        "P Luo",
        "X Wang"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF international conference on computer vision"
    },
    {
      "citation_id": "30",
      "title": "Dive into ambiguity: Latent distribution mining and pairwise uncertainty estimation for facial expression recognition",
      "authors": [
        "J She",
        "Y Hu",
        "H Shi",
        "J Wang",
        "Q Shen",
        "T Mei"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "31",
      "title": "Delving deep into label smoothing",
      "authors": [
        "C.-B Zhang",
        "P.-T Jiang",
        "Q Hou",
        "Y Wei",
        "Q Han",
        "Z Li",
        "M.-M Cheng"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "32",
      "title": "Cern: Compact facial expression recognition net",
      "authors": [
        "D Gera",
        "S Balasubramanian",
        "A Jami"
      ],
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "33",
      "title": "Multi-stage and multi-branch network with similar expressions label distribution learning for facial expression recognition",
      "authors": [
        "J Lang",
        "X Sun",
        "J Li",
        "M Wang"
      ],
      "year": "2022",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "34",
      "title": "Uncertain label correction via auxiliary action unit graphs for facial expression recognition",
      "authors": [
        "Y Liu",
        "X Zhang",
        "J Kauttonen",
        "G Zhao"
      ],
      "year": "2022",
      "venue": "2022 26th International Conference on Pattern Recognition (ICPR)"
    },
    {
      "citation_id": "35",
      "title": "Video-based frame-level facial analysis of affective behavior on mobile devices using efficientnets",
      "authors": [
        "A Savchenko"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "36",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "37",
      "title": "Assessing demographic bias transfer from dataset to model: A case study in facial expression recognition",
      "authors": [
        "I Dominguez-Catena",
        "D Paternain",
        "M Galar"
      ],
      "year": "2022",
      "venue": "Assessing demographic bias transfer from dataset to model: A case study in facial expression recognition",
      "arxiv": "arXiv:2205.10049"
    },
    {
      "citation_id": "38",
      "title": "Using attention lsgb network for facial expression recognition",
      "authors": [
        "C Su",
        "J Wei",
        "D Lin",
        "L Kong"
      ],
      "year": "2022",
      "venue": "Pattern Analysis and Applications"
    },
    {
      "citation_id": "39",
      "title": "Learning diversified feature representations for facial expression recognition in the wild",
      "authors": [
        "N Heidari",
        "A Iosifidis"
      ],
      "year": "2022",
      "venue": "Learning diversified feature representations for facial expression recognition in the wild",
      "arxiv": "arXiv:2210.09381"
    },
    {
      "citation_id": "40",
      "title": "Bias-based soft label learning for facial expression recognition",
      "authors": [
        "S Wang",
        "H Shuai",
        "C Liu",
        "Q Liu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "41",
      "title": "Man: Mining ambiguity and noise for facial expression recognition in the wild",
      "authors": [
        "Z Zhang",
        "X Sun",
        "J Li",
        "M Wang"
      ],
      "year": "2022",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "42",
      "title": "Ssa-icl: Multi-domain adaptive attention with intra-dataset continual learning for facial expression recognition",
      "authors": [
        "H Gao",
        "M Wu",
        "Z Chen",
        "Y Li",
        "X Wang",
        "S An",
        "J Li",
        "C Liu"
      ],
      "year": "2023",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "43",
      "title": "Relation and context augmentation network for facial expression recognition",
      "authors": [
        "X Ma",
        "Y Ma"
      ],
      "year": "2022",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "44",
      "title": "Face2exp: Combating data biases for facial expression recognition",
      "authors": [
        "D Zeng",
        "Z Lin",
        "X Yan",
        "Y Liu",
        "F Wang",
        "B Tang"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "45",
      "title": "Boosting facial expression recognition by a semi-supervised progressive teacher",
      "authors": [
        "J Jiang",
        "W Deng"
      ],
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "46",
      "title": "Effective attention feature reconstruction loss for facial expression recognition in the wild",
      "authors": [
        "W Gong",
        "Y Fan",
        "Y Qian"
      ],
      "year": "2022",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "47",
      "title": "Separate loss for basic and compound facial expression recognition in the wild",
      "authors": [
        "Y Li",
        "Y Lu",
        "J Li",
        "G Lu"
      ],
      "year": "2019",
      "venue": "Asian conference on machine learning"
    },
    {
      "citation_id": "48",
      "title": "Improving the facial expression recognition and its interpretability via generating expression pattern-map",
      "authors": [
        "J Zhang",
        "H Yu"
      ],
      "year": "2022",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "49",
      "title": "Joint spatial and scale attention network for multi-view facial expression recognition",
      "authors": [
        "Y Liu",
        "J Peng",
        "W Dai",
        "J Zeng",
        "S Shan"
      ],
      "year": "2023",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "50",
      "title": "Poster: A pyramid cross-fusion transformer network for facial expression recognition",
      "authors": [
        "C Zheng",
        "M Mendieta",
        "C Chen"
      ],
      "year": "2022",
      "venue": "Poster: A pyramid cross-fusion transformer network for facial expression recognition",
      "arxiv": "arXiv:2204.04083"
    },
    {
      "citation_id": "51",
      "title": "Facetoponet: Facial expression recognition using face topology learning",
      "authors": [
        "M Kolahdouzi",
        "A Sepas-Moghaddam",
        "A Etemad"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Artificial Intelligence"
    },
    {
      "citation_id": "52",
      "title": "Emotion recognition from occluded facial images using deep ensemble model",
      "authors": [
        "Z Ullah",
        "M Mohmand",
        "S Rehman",
        "M Zubair",
        "M Driss",
        "W Boulila",
        "R Sheikh",
        "I Alwawi"
      ],
      "year": "2022",
      "venue": "Cmc-Computers Materials & Continua"
    },
    {
      "citation_id": "53",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for large-scale image recognition",
      "arxiv": "arXiv:1409.1556"
    },
    {
      "citation_id": "54",
      "title": "Rethinking the inception architecture for computer vision",
      "authors": [
        "C Szegedy",
        "V Vanhoucke",
        "S Ioffe",
        "J Shlens",
        "Z Wojna"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "55",
      "title": "Oaenet: Oriented attention ensemble for accurate facial expression recognition",
      "authors": [
        "Z Wang",
        "F Zeng",
        "S Liu",
        "B Zeng"
      ],
      "year": "2021",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "56",
      "title": "Transfer: Learning relation-aware facial expression representations with transformers",
      "authors": [
        "F Xue",
        "Q Wang",
        "G Guo"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "57",
      "title": "End-to-end modeling and transfer learning for audiovisual emotion recognition in-the-wild",
      "authors": [
        "D Dresvyanskiy",
        "E Ryumina",
        "H Kaya",
        "M Markitantov",
        "A Karpov",
        "W Minker"
      ],
      "year": "2022",
      "venue": "Multimodal Technologies and Interaction"
    },
    {
      "citation_id": "58",
      "title": "Personalized models for facial emotion recognition through transfer learning",
      "authors": [
        "M Rescigno",
        "M Spezialetti",
        "S Rossi"
      ],
      "year": "2020",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "59",
      "title": "Relevance-based data masking: a model-agnostic transfer learning approach for facial expression recognition",
      "authors": [
        "D Schiller",
        "T Huber",
        "M Dietz",
        "E André"
      ],
      "year": "2020",
      "venue": "Frontiers in Computer Science"
    },
    {
      "citation_id": "60",
      "title": "The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression",
      "authors": [
        "P Lucey",
        "J Cohn",
        "T Kanade",
        "J Saragih",
        "Z Ambadar",
        "I Matthews"
      ],
      "year": "2010",
      "venue": "2010 ieee computer society conference on computer vision and pattern recognition-workshops"
    },
    {
      "citation_id": "61",
      "title": "Facial expression recognition from world wild web",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Salvador",
        "H Abdollahi",
        "D Chan",
        "M Mahoor"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition workshops"
    },
    {
      "citation_id": "62",
      "title": "Collecting large, richly annotated facial-expression databases from movies",
      "authors": [
        "A Dhall",
        "R Goecke",
        "S Lucey",
        "T Gedeon"
      ],
      "year": "2012",
      "venue": "IEEE multimedia"
    },
    {
      "citation_id": "63",
      "title": "Training deep networks for facial expression recognition with crowd-sourced label distribution",
      "authors": [
        "E Barsoum",
        "C Zhang",
        "C Ferrer",
        "Z Zhang"
      ],
      "year": "2016",
      "venue": "ACM International Conference on Multimodal Interaction (ICMI)"
    },
    {
      "citation_id": "64",
      "title": "From facial expression recognition to interpersonal relation prediction",
      "authors": [
        "Z Zhang",
        "P Luo",
        "C Loy",
        "X Tang"
      ],
      "year": "2018",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "65",
      "title": "Training deep networks for facial expression recognition with crowd-sourced label distribution",
      "authors": [
        "E Barsoum",
        "C Zhang",
        "C Ferrer",
        "Z Zhang"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM international conference on multimodal interaction"
    },
    {
      "citation_id": "66",
      "title": "Soft label mining and average expression anchoring for facial expression recognition",
      "authors": [
        "H Ming",
        "W Lu",
        "W Zhang"
      ],
      "year": "2022",
      "venue": "Proceedings of the Asian Conference on Computer Vision"
    },
    {
      "citation_id": "67",
      "title": "Joint recognition of basic and compound facial expressions by mining latent soft labels",
      "authors": [
        "J Jiang",
        "M Wang",
        "B Xiao",
        "J Hu",
        "W Deng"
      ],
      "year": "2023",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "68",
      "title": "Transformer-augmented network with online label correction for facial expression recognition",
      "authors": [
        "F Ma",
        "B Sun",
        "S Li"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "69",
      "title": "Facial expression recognition boosted by soft label with a diverse ensemble",
      "authors": [
        "Y Gan",
        "J Chen",
        "L Xu"
      ],
      "year": "2019",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "70",
      "title": "Facial expression recognition method with multi-label distribution learning for non-verbal behavior understanding in the classroom",
      "authors": [
        "T Liu",
        "J Wang",
        "B Yang",
        "X Wang"
      ],
      "year": "2021",
      "venue": "Infrared Physics & Technology"
    },
    {
      "citation_id": "71",
      "title": "Teaching with soft label smoothing for mitigating noisy labels in facial expressions",
      "authors": [
        "T Lukov",
        "N Zhao",
        "G Lee",
        "S.-N Lim"
      ],
      "year": "2022",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "72",
      "title": "Efficientnet: Rethinking model scaling for convolutional neural networks",
      "authors": [
        "M Tan",
        "Q Le"
      ],
      "year": "1920",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "73",
      "title": "Xception: Deep learning with depthwise separable convolutions",
      "authors": [
        "F Chollet"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "74",
      "title": "Automatic analysis of facial actions: A survey",
      "authors": [
        "B Martinez",
        "M Valstar",
        "B Jiang",
        "M Pantic"
      ],
      "year": "2017",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "75",
      "title": "An emotion index estimation based on facial action unit prediction",
      "authors": [
        "X Tan",
        "Y Fan",
        "M Sun",
        "M Zhuang",
        "F Qu"
      ],
      "year": "2022",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "76",
      "title": "Asmnet: A lightweight deep neural network for face alignment and pose estimation",
      "authors": [
        "A Fard",
        "H Abdollahi",
        "M Mahoor"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "77",
      "title": "Deep expectation of real and apparent age from a single image without facial landmarks",
      "authors": [
        "R Rothe",
        "R Timofte",
        "L Gool"
      ],
      "year": "2018",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "78",
      "title": "Hyperextended lightface: A facial attribute analysis framework",
      "authors": [
        "S Serengil",
        "A Ozpinar"
      ],
      "year": "2021",
      "venue": "2021 International Conference on Engineering and Emerging Technologies (ICEET)"
    },
    {
      "citation_id": "79",
      "title": "Acr loss: Adaptive coordinate-based regression loss for face alignment",
      "authors": [
        "A Fard",
        "M Mahoor"
      ],
      "year": "2022",
      "venue": "2022 26th International Conference on Pattern Recognition (ICPR)"
    },
    {
      "citation_id": "80",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "81",
      "title": "An empirical study of recent face alignment methods",
      "authors": [
        "H Yang",
        "X Jia",
        "C Loy",
        "P Robinson"
      ],
      "year": "2015",
      "venue": "An empirical study of recent face alignment methods",
      "arxiv": "arXiv:1511.05049"
    },
    {
      "citation_id": "82",
      "title": "Hierarchical attention network with progressive feature fusion for facial expression recognition",
      "authors": [
        "H Tao",
        "Q Duan"
      ],
      "year": "2024",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "83",
      "title": "Fg-agr: Finegrained associative graph representation for facial expression recognition in the wild",
      "authors": [
        "C Li",
        "X Li",
        "X Wang",
        "D Huang",
        "Z Liu",
        "L Liao"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "84",
      "title": "Enhanced spatial-temporal learning network for dynamic facial expression recognition",
      "authors": [
        "W Gong",
        "Y Qian",
        "W Zhou",
        "H Leng"
      ],
      "year": "2024",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "85",
      "title": "A discriminatively deep fusion approach with improved conditional gan (im-cgan) for facial expression recognition",
      "authors": [
        "Z Sun",
        "H Zhang",
        "J Bai",
        "M Liu",
        "Z Hu"
      ],
      "year": "2023",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "86",
      "title": "Spatial-temporal graphs plus transformers for geometry-guided facial expression recognition",
      "authors": [
        "R Zhao",
        "T Liu",
        "Z Huang",
        "D Lun",
        "K.-M Lam"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "87",
      "title": "Classifying emotions and engagement in online learning based on a single facial expression recognition neural network",
      "authors": [
        "A Savchenko",
        "L Savchenko",
        "I Makarov"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "88",
      "title": "Hard sample-aware consistency for low-resolution facial expression recognition",
      "authors": [
        "B Lee",
        "K Ko",
        "J Hong",
        "H Ko"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision"
    },
    {
      "citation_id": "89",
      "title": "Multi-relations aware network for in-the-wild facial expression recognition",
      "authors": [
        "D Chen",
        "G Wen",
        "H Li",
        "R Chen",
        "C Li"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "90",
      "title": "Enhanced discriminative global-local feature learning with priority for facial expression recognition",
      "authors": [
        "Z Zhang",
        "X Tian",
        "Y Zhang",
        "K Guo",
        "X Xu"
      ],
      "year": "2023",
      "venue": "Information Sciences"
    },
    {
      "citation_id": "91",
      "title": "Uncertain facial expression recognition via multi-task assisted correction",
      "authors": [
        "Y Liu",
        "X Zhang",
        "J Kauttonen",
        "G Zhao"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "92",
      "title": "Probabilistic attribute tree structured convolutional neural networks for facial expression recognition in the wild",
      "authors": [
        "J Cai",
        "Z Meng",
        "A Khan",
        "Z Li",
        "J O'reilly",
        "Y Tong"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "93",
      "title": "Thin: Throwable information networks and application for facial expression recognition in the wild",
      "authors": [
        "E Arnaud",
        "A Dapogny",
        "K Bailly"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "94",
      "title": "Clipaware expressive feature learning for video-based facial expression recognition",
      "authors": [
        "Y Liu",
        "C Feng",
        "X Yuan",
        "L Zhou",
        "W Wang",
        "J Qin",
        "Z Luo"
      ],
      "year": "2022",
      "venue": "Information Sciences"
    },
    {
      "citation_id": "95",
      "title": "Coarse-to-fine cascaded networks with smooth predicting for video facial expression recognition",
      "authors": [
        "F Xue",
        "Z Tan",
        "Y Zhu",
        "Z Ma",
        "G Guo"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "96",
      "title": "Emotion recognition from facial images with simultaneous occlusion, pose and illumination variations using meta-learning",
      "authors": [
        "S Kuruvayil",
        "S Palaniswamy"
      ],
      "year": "2022",
      "venue": "Journal of King Saud University-Computer and Information Sciences"
    },
    {
      "citation_id": "97",
      "title": "Deep learning framework for facial emotion recognition using cnn architectures",
      "authors": [
        "R Borgalli",
        "S Surve"
      ],
      "year": "2022",
      "venue": "2022 International Conference on Electronics and Renewable Systems (ICEARS)"
    },
    {
      "citation_id": "98",
      "title": "E2-capsule neural networks for facial expression recognition using au-aware attention",
      "authors": [
        "S Cao",
        "Y Yao",
        "G An"
      ],
      "year": "2020",
      "venue": "IET Image Processing"
    },
    {
      "citation_id": "99",
      "title": "Windmill graph based feature descriptors for facial expression recognition",
      "authors": [
        "M Kartheek",
        "M Prasad",
        "R Bhukya"
      ],
      "year": "2022",
      "venue": "Optik"
    },
    {
      "citation_id": "100",
      "title": "Recognition of learners' cognitive states using facial expressions in e-learning environments",
      "authors": [
        "K Rao",
        "M Rao"
      ],
      "year": "2020",
      "venue": "Journal of University of Shanghai for Science and Technology"
    },
    {
      "citation_id": "101",
      "title": "Facial affect\"in-the-wild",
      "authors": [
        "S Zafeiriou",
        "A Papaioannou",
        "I Kotsia",
        "M Nicolaou",
        "G Zhao"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "102",
      "title": "Aff-wild2: Extending the aff-wild database for affect recognition",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2018",
      "venue": "Aff-wild2: Extending the aff-wild database for affect recognition",
      "arxiv": "arXiv:1811.07770"
    },
    {
      "citation_id": "103",
      "title": "Multi-pie",
      "authors": [
        "R Gross",
        "I Matthews",
        "J Cohn",
        "T Kanade",
        "S Baker"
      ],
      "year": "2008",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "104",
      "title": "Web-based database for facial expression analysis",
      "authors": [
        "M Pantic",
        "M Valstar",
        "R Rademaker",
        "L Maat"
      ],
      "year": "2005",
      "venue": "2005 IEEE international conference on multimedia and Expo"
    },
    {
      "citation_id": "105",
      "title": "Disfa: A spontaneous facial action intensity database",
      "authors": [
        "S Mavadati",
        "M Mahoor",
        "K Bartlett",
        "P Trinh",
        "J Cohn"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "106",
      "title": "Introducing the recola multimodal corpus of remote collaborative and affective interactions",
      "authors": [
        "F Ringeval",
        "A Sonderegger",
        "J Sauer",
        "D Lalanne"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE international conference and workshops on automatic face and gesture recognition (FG)"
    },
    {
      "citation_id": "107",
      "title": "Affectiva-mit facial expression dataset (am-fed): Naturalistic and spontaneous facial expressions collected",
      "authors": [
        "D Mcduff",
        "R Kaliouby",
        "T Senechal",
        "M Amr",
        "J Cohn",
        "R Picard"
      ],
      "year": "2013",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition workshops"
    },
    {
      "citation_id": "108",
      "title": "Deap: A database for emotion analysis; using physiological signals",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani",
        "J.-S Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "109",
      "title": "Static facial expression analysis in tough conditions: Data, evaluation protocol and benchmark",
      "authors": [
        "A Dhall",
        "R Goecke",
        "S Lucey",
        "T Gedeon"
      ],
      "year": "2011",
      "venue": "2011 IEEE international conference on computer vision workshops (ICCV workshops"
    },
    {
      "citation_id": "110",
      "title": "Modeling stylized character expressions via deep learning",
      "authors": [
        "D Aneja",
        "A Colburn",
        "G Faigin",
        "L Shapiro",
        "B Mones"
      ],
      "year": "2017",
      "venue": "Asian conference on computer vision"
    },
    {
      "citation_id": "111",
      "title": "Facial expression recognition from near-infrared videos",
      "authors": [
        "G Zhao",
        "X Huang",
        "M Taini",
        "S Li",
        "M Pietikäinen"
      ],
      "year": "2011",
      "venue": "Image and vision computing"
    },
    {
      "citation_id": "112",
      "title": "The ar face database: Cvc technical report",
      "authors": [
        "A Martinez",
        "R Benavente"
      ],
      "venue": "The ar face database: Cvc technical report"
    },
    {
      "citation_id": "113",
      "title": "Coding facial expressions with gabor wavelets",
      "authors": [
        "M Lyons",
        "S Akamatsu",
        "M Kamachi",
        "J Gyoba"
      ],
      "year": "1998",
      "venue": "Proceedings Third IEEE interna-tional conference on automatic face and gesture recognition"
    },
    {
      "citation_id": "114",
      "title": "Sayette group formation task (gft) spontaneous facial expression database",
      "authors": [
        "J Girard",
        "W.-S Chu",
        "L Jeni",
        "J Cohn"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "115",
      "title": "A high-resolution spontaneous 3d dynamic facial expression database",
      "authors": [
        "X Zhang",
        "L Yin",
        "J Cohn",
        "S Canavan",
        "M Reale",
        "A Horowitz",
        "P Liu"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE international conference and workshops on automatic face and gesture recognition (FG)"
    },
    {
      "citation_id": "116",
      "title": "Multimodal spontaneous emotion corpus for human behavior analysis",
      "authors": [
        "Z Zhang",
        "J Girard",
        "Y Wu",
        "X Zhang",
        "P Liu",
        "U Ciftci",
        "S Canavan",
        "M Reale",
        "A Horowitz",
        "H Yang"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "117",
      "title": "4dfab: A large scale 4d database for facial expression analysis and biometric applications",
      "authors": [
        "S Cheng",
        "I Kotsia",
        "M Pantic",
        "S Zafeiriou"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "118",
      "title": "The belfast induced natural emotion database",
      "authors": [
        "I Sneddon",
        "M Mcrorie",
        "G Mckeown",
        "J Hanratty"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "119",
      "title": "Daisee: Towards user engagement recognition in the wild",
      "authors": [
        "A Gupta",
        "A D'cunha",
        "K Awasthi",
        "V Balasubramanian"
      ],
      "year": "2016",
      "venue": "Daisee: Towards user engagement recognition in the wild",
      "arxiv": "arXiv:1609.01885"
    },
    {
      "citation_id": "120",
      "title": "Feafa+: an extended well-annotated dataset for facial expression analysis and 3d facial animation",
      "authors": [
        "W Gan",
        "J Xue",
        "K Lu",
        "Y Yan",
        "P Gao",
        "J Lyu"
      ],
      "year": "2022",
      "venue": "Fourteenth International Conference on Digital Image Processing"
    },
    {
      "citation_id": "121",
      "title": "Karolinska directed emotional faces",
      "authors": [
        "D Lundqvist",
        "A Flykt",
        "A Öhman"
      ],
      "year": "1998",
      "venue": "Cognition and Emotion"
    },
    {
      "citation_id": "122",
      "title": "TABLE VIII Precision, recall, F-1 score, accuracy and average accuracy of ensemble of binary classifiers (EBC), over test-MAS (in %)",
      "venue": "Neutral Happy Sad Surprise Fear Disgust Anger Contempt"
    }
  ]
}