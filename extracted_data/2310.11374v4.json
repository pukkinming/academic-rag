{
  "paper_id": "2310.11374v4",
  "title": "Dialoguellm: Context And Emotion Knowledge-Tuned Large Language Models For Emotion Recognition In Conversations",
  "published": "2023-10-17T16:15:34Z",
  "authors": [
    "Yazhou Zhang",
    "Mengyao Wang",
    "Youxi Wu",
    "Prayag Tiwari",
    "Qiuchi Li",
    "Benyou Wang",
    "Jing Qin"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Large language models (LLMs) and their variants have shown extraordinary efficacy across numerous downstream natural language processing (NLP) tasks, which has presented a new vision for the development of NLP. Despite their remarkable performance in natural language generating, LLMs lack a distinct focus on the emotion understanding domain. As a result, using LLMs for emotion recognition may lead to suboptimal and inadequate precision. Another limitation of the current emotion LLMs is that they are typical trained without leveraging multi-modal information. To overcome these limitations, we propose Dia-logueLLM, a context and emotion knowledge tuned LLM that is obtained by fine-tuning large language models with benchmarking multimodal (i.e., texts and videos) emotional dialogues. The visual information is considered as the supplementary knowledge to construct high-quality instructions. We offer a comprehensive evaluation of our proposed model on three benchmarking emotion recognition in conversations (ERC) datasets and compare the results against the state-of-the-art baselines and other state-of-the-art LLMs. Additionally, DialogueLLM-7B can be easily trained using LoRA on a 40GB A100 GPU in 5 hours, facilitating reproducibility for other researchers.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Scaling up language models has been proved to be an effective way to improve the performance and sample efficiency in downstream NLP tasks. The rise of instruction-following LLMs has garnered considerable attention from academy and industry, due to their outstanding performance in human instruction understanding and responsing. Language modeling has evolved from small language models (SLMs), e.g., GPT  [30] , BERT  [4] , RoBERTa  [20] , etc., to LLMs, e.g., ChatGPT 1  GPT-4  [23] , Claude 2  , etc.\n\nCompared with SLMs, LLMs are characterized by their enormous parameter size, typically reaching tens of billions or even more. They often have stronger generalization across various downstream tasks and unique emergent ability to tackle complex tasks. Despite that LLMs possess numerous commendable qualities, they also present a couple of limitations that deserve careful consideration and in-depth exploration:  (1)  the nonopen source status may restrict the development of LLMs community and (2) they are not specifically designed for emotion understanding task. Their broad domain knowledge frequently proves insufficient when tackling such specialized domains. For example, Zhang et al.  [41]  showed LLMs' unsatisfactory performance in many emotion recognition tasks without fine-tunning on emotional knowledge. Let et al.  [12]  presented a retreival based framework to improve the adaptability of LLMs to emotion recognition. Hence, the potential of LLMs in understanding emotional communication needs to be explored further.\n\nHuman communication is the process of exchanging information, thoughts, ideas, and feelings between individuals, which is naturally filled with the participant's subjective attitudes or emotions. Emotion recognition in conversations (ERC) aims to accurately detect the feelings and emotions expressed in the utterances. It has immense potential in dialogue understanding and intent analysis, and has been an active task in the recent literature  [19, 42, 45] . In general, there are two key factors that contribute the classification performance, i.e., multi-modal fusion and context dependency (also known as intra-and inter-speaker dependency)  [21] . Multi-modal fusion involves combining information from different sources or modalities, such as text, visual cues, to obtain a more comprehensive and accurate understanding of the emotional utterance. In view that emotions are influenced by the surrounding environment, the relationship between the participants, etc., context is a critical factor in accurately classifying emotions in conversations. The same utterance in different contexts might express different emotions. Fig.  1  illustrates an example to introduce the presence of both challenges.\n\nTo overcome the above-mentioned limitations, it's crucial to develop emotion-tailored LLMs that can better understand human-human conversation and take a further step towards emotion intelligence. In this paper, we present DialogueLLM, an emotion and context knowledge enhanced language model, which is specifically designed for ERC based on the open-source base models, namely LLaMA 2  [35] . By collecting diverse instruction conversational data based on emotional knowledge from five open-source benchmarking datasets (i.e., MELD  [27] , IEMOCAP  [1] , EmoryNLP  [39] .), we obtain 2411 multi-party dialogues, over 24,304 utterances. Meanwhile, the visual information (i.e., videos) will be forward into ERNIE Bot 3 to automatically generate the text descriptions, which will be considered as the supplementary knowledge to construct high-quality instructions. We adopt an end-to-end supervised instruction-finetuning ap-3 https://yiyan.baidu.com/ proach on the open-source LLaMA 2-7B base models. Additionally, DialogueLLM-7B can be easily trained using LoRA on a 40GB A100 GPU in 5 hours, facilitating reproducibility for other researchers.\n\nWe offer a comprehensive evaluation of our proposed DialogueLLM model across three ERC tasks and compare the results against 15 state-ofthe-art ERC baselines, including bc-LSTM  [46] , MTL  [15] , ICON  [7] , DialogXL  [32] , TOD-KAT  [49] , CoG-BART  [16] , DialogueGCN  [5] , RGAT  [11] , DAG-ERC  [33] , DialogueRNN  [22] , DialogueCRN  [10] , CauAIN  [48] , COIN  [40] , GraphCFC  [14] , SACL-LSTM  [9]  and three SOTA LLMs, i.e., LLaMA, Alpaca  4  and LLaMA 2. The experimental results show the effectiveness of Dia-logueLLM with the margin of 5.36%, 1.03% and 1.5% for three benchmarking ERC tasks. The study reveals that DialogueLLM significantly outperforms the SOTA baselines on ERC tasks requiring deeper understanding or conversational emotion information. A series of sub-experiments underscore how emotion and context knowledge enhanced LLMs deal with ERC tasks. The main innovations of the work are concluded as follows:\n\n• To the best of our knowledge, DialogueLLM\n\nis the first open source emotional LLM that is specifically designed for ERC tasks.\n\n• The visual information is proposed to construct high-quality instructions.\n\n• We show a comprehensive dataset of over 24K utterances to serve as a knowledge corpus, supporting the training and testing of emotional LLMs with accurate and domainspecific knowledge.\n\n• Our model achieves state-of-the-art performance on ERC tasks. We show that an open-sourced model finetuned with emotional knowledge has the potential to achieve even higher accuracy than SOTA.\n\nThe rest of this paper is organized as follows. Section 2 briefly outlines the related work. In Section 3, we describe the proposed DialogueLLM in detail. In Section 4, we report the empirical experiments and analyze the results. Section 5 concludes the paper and points out future research directions.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "We depict two lines of research that form the basis of this work: large language models and emotion recognition in conversations models.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Large Language Models",
      "text": "In recent years, significant advancements in natural language processing (NLP) have been attributed to the emergence of large language models. These models have showcased remarkable capabilities such as in-context learning, few-shot prompting, instruction following, etc. These dynamic abilities have greatly contributed to boosting the effectiveness of language models, thus enabling AI algorithms to achieve unparalleled levels of effectiveness and productivity. Typically, models like the transformer architecture-based LLMs are first pretrained using extensive datasets comprising diverse languages and domains  [47] .\n\nOpenAI has achieved significant milestones with the creation of two groundbreaking models: Chat-GPT and GPT-4. These models herald a new era in language processing. However, due to their proprietary nature, there has been a proliferation of LLM variants featuring tens or even hundreds of billions of parameters. Our aim is to categorize these LLMs into two distinct groups based on their specialization: general LLMs and specialized LLMs. General LLMs are designed for versatility across a wide spectrum of NLP tasks, including machine translation, language comprehension, and dialogue generation. Prominent examples of these models are GPT-4, Claude, ChatGPT, LLaMA, PanGu-Σ  [31] , Bard  5  , Falcon  [24] , etc. Such LLMs are not specifically optimized for any particular task. While they can perform well across a range of tasks, but their potentials in specific scenarios await further explore.\n\nIn contrast, specialized LLMs also known as task-specific LLMs, are fine-tuned for specific tasks via task-specific architectures and knowledge, allowing them to achieve higher or comparable performance against general LLMs with fewer parameters. For example, Wang et al.  [2]  released a large language model 'Phoenix' to meet the needs of multiple languages. Liu and Low  [18]  finetuned a Goat model based on LLaMA model to deal with arithmetic tasks. In view that LLMs have not yet performed optimally in medical domain tasks, a few Chinese and English medical knowledge enhanced LLMs have been proposed, such as HuaTuo  [36] , PMC-LLaMA  [38] , Dr. LLaMA  [6] , ChatDoctor  [17] . Different from the above-mentioned works, we aim to explore the potential of LLMs in emotion understanding domain and take a further step towards emotional intelligence.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Emotion Recognition In Conversations",
      "text": "Emotion recognition in conversation (ERC) has become a popular research topic. In this task, the conversational context dependency and multi-modal fusion have been considered through deep learning approaches. These efforts can be broadly categorized into methods based on sequences and those based on the Transformer architecture.\n\nSequence based approaches often use the sequential information in a dialogue to capture the contextual and emotional features. For example, Poria et al.  [26]  introduced an LSTM-based model that effectively captured conversational context from surrounding videos, thereby enhancing the classification process. Building upon this idea, Hazarika et al.  [8]  presented the conversational memory network (CMN), which harnessed contextual information from the conversation history to improve ERC. Another approach by Majumder et al.  [22]  introduced the DialogueRNN model, which meticulously monitored the states of individual participants throughout the conversation, utilizing this information for ERC. In terms of multimodal advancements, Poria et al.  [27]  played a pivotal role by crafting the first-ever multimodal conversational dataset named the multimodal emotionlines dataset (MELD). This dataset was instrumental in propelling the field of conversational sentiment analysis. Further innovation came from Zhang et al.  [43] , who devised the quantum-inspired interactive network (QIN) model for conversational emotion recognition, showcasing its effectiveness. Moreover, their research extended to the realm of multitask learning. Zhang et al.  [44]  devised a quantuminspired multi-task learning framework catering to both sarcasm detection and emotion recognition in conversations.\n\nTransformer based approaches often adopt the \"fine-tuning\" paradigm. They build the models upon the foundation of Transformer based pretrained language models. Then, such models are supervised-fine-tuned with labeled samples and are adapted to the specific task. For instance, Li et al.  [16]  used a supervised contrastive term and a response generation task to enhance BART's ability for ERC. Zhang et al.  [42]  proposed a multi-modal multi-task network based on BERT and graph attention network (GAT) to detect sentiment and emotion. They also proposed a quantum inspired multitask interactive Transformer to model sentiment and emotion  [19] . Chudasama et al.  [3]  presented a multi-modal fusion network (M2FNet) to learn emotion-relevant multi-modal features by revising the Transformer encoder. Qiao et al.  [29]  built a mutual-enhanced incongruity learning network upon RoBERTa and graph convolution networks to identify sarcasm. Pramanick  [28]  combined selfattention with BERT to model intra-modal correspondence and optimal transport for cross-modal correspondence, aiming to discover sarcasm and humor. Lei et al.  [12]  replaced the ERC task from a traditional discriminative model to a generative model, and proposed a simple but effective retrieval template modulem, named InstructERC to help the model explicitly integrate multi-granular dialog supervision information.\n\nCompared with them, DialogueLLM possesses the abilty to understand complex emotions without introducing any other components. In addition, our model would also benefit the development of task-specific LLMs.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Methodology",
      "text": "In this section, we detail the comprehensive pipeline for training DialogueLLM models, as shown in Fig.  2 .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Problem Formulation",
      "text": "Assume that there are N conversation instances in the instruction dataset, the i th conversation D i contains K multi-modal utterances, which is represented as\n\nHere, l T k and l V k denote the sequence length of textual and visual utterances, d T k and d V k represents the dimensions of the textual and visual features. Now, we summarize our research problem as: Given one multi-speaker conversation including K multi-modal utterances, how to detect their emotions? It could be written as:\n\nwhere Θ denotes the parameter set.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Base Model",
      "text": "The first key component is to select open-source and strong foundation language models. LLaMA is a collection of open source foundation language models ranging from 7B to 65B parameters, which is trained on trillions of tokens using publicly available datasets. It achieves state-of-the-art performance on numerous benchmarks, which has greatly promoted the research progress of LLMs. A considerable number of researchers choose to expand the capabilities of LLaMA models though instruction tuning, due to the lower computational costs. Furthermore, Meta AI has just developed and released LLaMA 2, which is an updated version of LLaMA 1. Compared with LLaMA 1, the training data used for LLaMA 2 was increased by 40% and the context length was doubled. LLaMA 2 also incorporated grouped query attention mechanisms. LLaMA 2 shows many behaviors similar to ChatGPT, but is also surprisingly small and easy to reproduce. Hence, we adopt LLaMA 2-7B model as our base model. Furthermore, LLaMA-7B have also been attempted and evaluated in the experiments. We use low-rank adaptation (LoRA) to finetune them with only 2.1 million trainable parameters.\n\nIn view that LLaMA 2 possess a powerful generative capability, we treat ERC as a conditional generative task, where the output Y k will be an emotion label. We first propose a general zero-shot prompt template, namely P rompt k erc that consists of the contextual, multi-modal utterances and the instruction I erc by merging them together:\n\n) where T extDescription (V k ) denotes the text description of the k th video produced by the ERNIE Bot. Then, we ask LLMs to generate an emotion label by providing the above-mentioned prompts:",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Emotion And Context Knowledge Based Instruction Dataset",
      "text": "Human conversation is filled with different emotions, such as neutral, anger, happiness, surprise, etc. To satisfy complex emotion recognition needs, DialogueLLM undergos an instruction-tuning step which involves training on supervised instruction/input/output data. Instruction tuning helps align DialogueLLM with human prompts, enabling precise customization for emotional domains. This allows DialogueLLM to become adaptable and proficient at generating accurate emotional responses.\n\nIn this paper, we create a high-quality instruction dataset by leveraging three widely used benchmarking ERC datasets. Since many potential shortcomings exist in automatic generation of samples using strong language models (e.g., ChatGPT), such as low quality, repetition, and lack of diversity, etc., different from the existing works  [18, 25] , we do not use ChatGPT to generate instances. The benchmarking ERC datasets have provided clean samples with precise annotations, which will be an optimal choice for creating instruction dataset. The training sets of three benchmarking datasets (i.e., MELD, IEMOCAP and EmoryNLP) are treated as the data source, altogether 2,411 multi-party dialogues, over 24,304 utterances are collected. In view that the labels are from different datasets, we first pre-process the lables. For example, \"joy\", \"happy\" and \"happiness\" will be normalized to be \"happiness\". The instructions are constructed based on the task definition and label space, e.g., \"Given the Video Description and Context, detect the emotion of the input, and assign an accuracy label from ['happiness', 'anger', 'fear', 'sadness', 'disgust', 'surprise', 'neutral'].\". The textual raw samples and the counterpart labels are normalized to the input/output pairs.\n\nIn view of the importance of the conversational context and multi-modal knowledge, the contextual utterances and the visual information are incorporated into instruction instances. Assume that there are z contextual utterances before the target utterance, we would list them before the input content. In this work, the default size is z = 1 (where the impact of varying size will be discussed in Sec. 4.9). In addition, the corresponding video is split into frames and forward them through the ERNIE Bot, to generate the descriptions of this video. Then, such descriptions are considered as the supplementary knowledge. More statistics of this dataset are presented in Fig.  3  and Fig.  4 . Notably, \"Neutral\" and \"Happiness\" accounted for the largest percentage of the total instances, about 31.1% and 15.4%, respectively. In contrast, \"Fear\", \"Powerful\", and \"Peaceful\" are represented at lower proportions. \"Fear\" comprises around 6.1% of the  dataset, and \"Powerful\" represents about 3.5% of the dataset. Similarly, \"Peaceful\" constitutes approximately 6.3% of the dataset, indicating a notable but still comparatively moderate occurrence of this particular emotion. \"Anger\", \"Sadness\", \"Frustration\", \"Surprise\", \"Excitement\" and \"Disgust\" collectively account for around 37.6% of the dataset. Specifically, \"Anger\" accounts for about 9.7%, \"Sadness\" for about 6.4%, \"Frustration\" for about 5.6%, \"Surprise\" is about 9.4%, \"Excitement\" is about 4.2%, and \"Disgust\" is about 2.3%.\n\nFinally, this instruction dataset is used for supervised fine-tuning. Notably, all the instances in the dataset are normalize as \"instruction/video descriptions/context/input/output\" pairs (see Fig.  2 ).",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Training And Implementation",
      "text": "The DialogueLLM-7B model is fine-tuned LLaMA 2-7B with the emotional knowledge based instruction data to acquire emotion recognition skills. Training a DialogueLLM-7B model will cost about 5 hours on a 40GB A100 GPU. The total approximate tokens seen during pre-training is approximately 22 billion tokens. We optimize our model with the AdamW optimizer with the following hyper-parameters: β 1 = 0.9, β 2 = 0.95. We use a cosine learning rate schedule, such that the final learning rate is equal to 10% of the maximal learning rate. The activation function is set to SwiGLU to improve performance. The target utterances are forward through DialogueLLM models to generate the emotion labels. The details of the algorithm are outlined in Algorithm 1. RQ2: Does modeling of the contextual dependency and multi-modal information help improve performance?\n\nRQ3: Does DialogueLLM has powerful incontext learning abilities?\n\nTo answer RQ1, we compare the proposed Dia-logueLLM with a wide range of state-of-the-art baselines and other LLMs on three benchmark datasets in Sec. 4.4. To answer RQ2, we conduct a ablation test by removing one component at one time in Sec. 4.5. To answer RQ3, we consider zeroshot and few-shot prompting setups, and report their results in Sec. 4.6.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Experimental Settings",
      "text": "Datasets. Three benchmark ERC datasets which include the textual and visual utterances with high quality emotion annotations, are selected as the experimental beds, viz. MELD 6    [27] , IEMOCAP  7  , and EmoryNLP  8  .\n\nMELD. It consists of 13,708 multi-modal utterances from 1,433 multi-party dialogues of Friends TV series. The utterances in each dialogue are annotated with one of three sentiments (positive, negative or neutral) and one of seven emotions (anger, disgust, fear, joy, neutral, sadness or surprise). The overall Fleiss' kappa score reaches 0.43. In this work, we only use textual and visual information.\n\nIEMOCAP. It is comprised of 151 recorded dialogue videos, encompassing a total of 302 videos across the entire dataset, each involving two speakers per session. The annotations for this dataset encompass 9 distinct emotions (anger, excitement, fear, sadness, surprise, frustration, happiness, disappointment, and neutrality). The recordings are distributed across five sessions, with each session featuring five pairs of speakers.\n\nEmoryNLP. consists of 97 episodes, 897 scenes, and 12,606 utterances, which is a textual corpus that comprises multi-party dialogue transcripts of the Friends TV show. Each utterance is annotated with one of seven emotions, i.e., sad, mad, scared, powerful, peaceful, joyful, and neutral. The detailed statistics are shown in Table  1 .\n\nEvaluation metrics. In line with the previous approaches, accuracy (Acc) and weighted-F1 (w-F1) are used as evaluation metrics. For each method, we run five random seeds and report the average result of the test sets.\n\nHyper-parameter. We report the detailed hyperparameter settings of DialogueLLM on three datasets in Table  2 . The maximum context length is set to 4,096. We use a weight decay of 0.1 and gradient clipping of 1.0. The batch size is set to 128.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Compared Baselines",
      "text": "A wide range of SOTA baselines are included for comparison including pre-trained language model (PLM) based and LLM based approaches. They are:\n\n• PLM based approaches:\n\n(1) bc-LSTM  [46]  implements an utterancelevel LSTM to capture contextual features.\n\n(2) ICON  [7]  hierarchically models the selfand inter-speaker emotional influences into global memories, and generates contextual summaries.\n\n(3) MTL  [15]  exploits speaker identification (SI) as an auxiliary task to enhance the utterance representation in conversations.\n\n(4) DialogXL  [32]  modifies the recurrence mechanism of XLNet to store longer historical context and dialog-aware self-attention to deal with the multi-party structures.\n\n(5) TODKAT  [49]  designs a transformerbased encoder-decoder architecture fuses the topical and commonsense information, and performs the emotion label sequence prediction.\n\n(6) CoG-BART  [16]  uses the pre-trained encoder-decoder model BART as the backbone model and utilizes an auxiliary response generation task to enhance the model's ability of handling context information.\n\n(7) DialogueRNN  [22]  designs a method based on recurrent neural networks (RNN) that keeps track of the individual party states throughout the conversation and uses this information for emotion classification.\n\n(8) DialogueGCN  [5]  leverages self and interspeaker dependency of the interlocutors to model conversational context for emotion recognition.\n\n(9) DialogueCRN  [10]  designs multi-turn reasoning modules to extract and integrate emotional clues.\n\n(10) RGAT  [11]  proposes relational position encodings to capture both the speaker dependency and the sequential information.\n\n(11) DAG-ERC  [5]  regards each conversation as a directed acyclic graph to model the conversation context.\n\n(12) CauAIN  [48]  retrieves causal clues provided by commonsense knowledge to guide the process of causal utterance traceback.\n\n(13) COIN  [40]  is a conversational interactive model to mitigate the problem of overlooking the immediate mutual interaction between different speakers by applying state mutual interaction within history contexts.\n\n(14) GraphCFC  [14]  is a module adept at modeling context and interaction information in ERC tasks with high efficiency. It leverages multiple extractors and PairCC strategies to effectively tackle the heterogeneity present in multimodal fusion.\n\n(15) SACL-LSTM  [9]  applies contrast-aware adversarial training to generate worst-case samples and uses a joint class-spread contrastive learning objective on both original and adversarial samples.\n\n• LLMs based approaches:\n\n(1) LLaMA  [34]  takes a sequence of words as an input and predicts a next word to recursively generate text.\n\n(2) Alpaca is a state-of-the-art finedtuning version of LLaMA, by using supervised learning  from a LLaMA 7B model on 52K instructionfollowing demonstrations.\n\n(3) LLaMA 2  [35]  is trained on 2 trillion tokens, and have double the context length than Llama 1, and outperforms other open source language models on many external benchmarks.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Results And Anlysis",
      "text": "The experimental performance of all baselines is shown in Table  3 . We divide these baselines into two categories, i.e., pre-trained language models and large language models without fine-tuning. We will conduct a detailed analysis of their classification performance.\n\nFor the PLM based baselines, we can observe that MTL demonstrates very poor performance compared to the other baseline models, with the worst classification accuracy on the MELD and EmoryNLP datasets. One possible reason is that MTL disregards modeling conversation-level interaction information. Without capturing contextual information, the model struggles to learn effectively, resulting in inaccurate classification outcomes. In contrast, DialogueCNN, DialogueGCN, and DialogueCRN aim to model the contextual information derived from the speaker. Compared to MTL, their performance substantially improves. This further verifies that incorporating contextual information is vital for ERC.\n\nThe SACL-LSTM model performs very well, being the second-best in terms of the average scores on three datasets. It may benefit from an architecture that combines the strengths of LSTM with self-attention mechanisms, enabling it to capture both long-term dependencies and subtle contextual cues within dialogues. Other strong models like TODKAT, CoG-BART, and DialogXL show competitive performance but do not reach the same level as SACL-LSTM across MELD and IEMO-CAP datasets. They do not achieve top scores, which could be attributed to the complexity of emotion recognition tasks that may not be entirely captured by the models' pre-training data or architecture. With 6.1M parameters, CauAIN demonstrates solid performance, especially on the MELD and IEMOCAP datasets. This model's architecture likely includes mechanisms that aid in capturing causal relationships within dialogues, which is a critical factor for understanding emotions. The COIN model, despite its smaller size of 0.6M parameters, still achieves competitive accuracy on the IEMOCAP dataset. But it was not evaluated on other datasets.\n\nIn addition, Table  3  shows that when LLMs are fine-tuned without using the proposed emotional knowledge, all of LLaMA-7B, Alpaca, and LLaMA2-7B perform very poor on three emotion recognition tasks. This proves that the general priori knowledge of LLMs are not sufficient to handle complex and subjective emotion understanding tasks. The emotion-specific knowledge is needed to further deepen their potential. In contrast, The proposed DialogueLLM model achieves the stateof-the-art performance across three datasets, which proves the effectiveness of fine-tuning LLMs with task specific knowledge.\n\nMELD. DialogueLLM achieves remarkable results on the MELD dataset, demonstrating its robustness with a leading F1 score of 71.90% and an accuracy of 71.96%. This is a significant uptick from the other strong contender on this dataset, SACL-LSTM, which registers an F1 score of 66.45% and an accuracy of 67.51%. The MELD dataset, known for its realistic conversational scenarios from a popular TV show, poses a challeng- ing benchmark due to its diverse emotional expressions and informal dialogue. DialogueLLM's performance here suggests its superior ability to decode nuanced emotional cues within a naturalistic dialogue setting. IEMOCAP. DialogueLLM showcases its prowess with an accuracy of 70.62% and an F1 score of 69.93%, outstripping the previously leading SACL-LSTM model, which had an accuracy of 69.08% and an F1 score of 69.22%. The IEMOCAP dataset is unique due to its focus on dyadic conversations with a rich set of emotional annotations, ranging from anger to happiness. The high performance of DialogueLLM on this dataset underscores its effectiveness in understanding and interpreting complex emotional dynamics in close-ended conversations.\n\nEmorynlp. DialogueLLM maintains a competitive edge, securing an F1 score of 40.05% and an accuracy of 41.88%. DialogueLLM consistently surpasses all of the 15 baselines. This performance is indicative of DialogueLLM's versatile capacity to capture emotional nuances across varied conversational contexts\n\nThe experimental results demonstrate the effectiveness of the DialogueLLM model in emotion recognition tasks across different datasets. It consistently achieves the highest F1 scores and accuracy, outperforming 15 state-of-the-art models. Notably, DiaologueLLM's performance is robust across various datasets, which underscores its versatility and reliability in handling different data sources and domains. This also suggests a trend where specialized pre-training on tasks closely related to the downstream application can yield significant benefits.\n\nTraining loss. The training loss is shown in Figure  5 . The loss result on MELD dataset shows a rapid initial decrease with some early fluctuations, eventually stabilizing at around a 0.2 loss value. The IEMOCAP dataset's training loss drops more abruptly than MELD, suggesting a faster learning rate or easier dataset for the model to learn, and levels off at a lower value near 0.1, indicating a more successful training outcome. Lastly, the EmoryNLP dataset's loss decreases smoothly without the volatility observed in the MELD graph, also stabilizing at a loss value just under 0.2. This smooth decrease may point to a stable learning process.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Ablation Test",
      "text": "The ablation study is conducted across three datasets, which will provide a structured insight into the contribution of different components to the model's performance. The term \"w/o\" indicates the model's performance without a specific feature, where the term \"w\" indicates the model's performance with a specific feature.\n\nFrom Table  4 , we have four observations: ( the DialogueLLM's performance decreases across all datasets when any component is removed, underscoring the integral role each part plays in the model's design for emotion recognition; (2) the performance drops with the removal of context on all datasets suggests that contextual information is important for emotion recognition, aligning with the premise that conversational emotion understanding is heavily reliant on context; (3) the removal of LoRA leads to a significant decrease in model performance, because the small training size leads to underfitting; (4) removing the visual information leads to a noticeable decrease in performance, suggesting that multimodal information may be beneficial for emotion recognition in dialogues. Notably, we do not use the visual information from IEMO-CAP, because the actors in this corpus are sitting on chairs for face-to-face conversations, and the descriptions of the image information are too similar, e.g., \"a man and a woman sitting on chairs for face-to-face exchanges\". Here, we have given the answer to RQ2.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Zero-Shot V/S Few-Shot Prompting",
      "text": "This paper also performs zero-shot and few-shot experiments to evaluate whether DialogueLLM can perform better when a limited number of cases are available for emotion recognition tasks. The results are shown in Table  5 . We design four H-shot settings: zero-shot, one-shot, five-shot, ten-shot. For each setting, we sample H = {0, 1, 5, 10} examples for emotion classification. These sampling examples serve as the learning samples for Dia-logueLLM.\n\nThe impact of adding shots varies with the number of shots. The performance gains are not significant or even decreased when adding too many shots. The change from zero-shot to one-shot results in a slight improvement in classification performance.\n\nWith the gradual increase in the number of shots, the performance drops down.\n\nThis could be attributed to misclassifications made by DialogueLLM, potentially arising from the model learning excessive redundant information when handling too long contextual data. This suggests that roughly increasing the number of extra shots does not necessaryly result in a stable performance improvement.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Error Analysis",
      "text": "The detailed error analysis is also conducted via the confusion matrices that are shown in Figure  6 . Each cell (i, j) represents the percentage of class i is classified to be class j. Upon reviewing the classification results produced by DialogueLLM on the three datasets, we discover that imbalanced emotion categories and the similarity across different emotions are the key factors contributing to misclassification.\n\nBy examining the diagonal elements of the matrices, DialogueLLM demonstrates effective truepositive categorization for most fine-grained emotions. However, it exhibits a tendency to misclassify the utterances to be \"neutral\", particularly in the EmoryNLP dataset. This misclassification is influenced by the dataset's imbalance, where the percentages of \"neutral\" utterances in EmoryNLP, MELD and IEMOCAP are 32.48%, 47.5%, and 22.23%, respectively. This highly unbalanced data distribution leads to the model's excessive preference for \"neutral\" emotions.\n\nAdditional, we show a few typical misclassification examples in Figure  7 . It is evident that Dia-logueLLM encounters challenges in distinguishing closely related pairs of emotions. In the confusion matrices, we observe a consistent misclassification of \"anger\" to be \"disgust\" on the MELD dataset, see the example B in Figure  7 . In this case, when Phoebe makes a negative comment to another person, it is challenging to discover whether the expressed emotion is \"disgust\" or \"anger\". A few pairs of emotions such as \"surprise\" vs \"excitement\", \"anger\" vs \"frustration\" and \"peaceful\" vs \"happiness\". The slight similarity across such emotions poses a challenge for the model to accurately distinguish them. This difficulty in discerning emotions may result in errors during emotion categorization.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "The Impact Of Epoch",
      "text": "In this section, the impact of the number of epoch on the classification performance is shown in Figure  8 . We can notice that the performance increases with iterative epoch on all three datasets.\n\nIn particular, there is a sharp leap in performance when epoch ranges from 1 to 3, and a slow increase in performance when epoch ranges from 3 to 10. However, the performance slightly decreases when epoch varies from 7 to 10 on the EmoryNLP dataset. One possible explanation is overfitting on the train- ing set due to the small size of this dataset. This result shows the importance of selecting an appropriate number of iteration rounds. Adding the number of epoch often leads to longer training times. Due to the limitation of our GPUs, we refrained from testing with a higher number of epochs.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Effect Of Context Range",
      "text": "This subsection aims to explore the impact of context length on the performance of DialogueLLM. We select and evaluate the context length from the pool {0, 1, 2, 3, All}, where All represents using all the contexts. The experimental results are shown in Figure  9 .\n\nIn general, longer context length will allow the model to access more information, thus making accurate prediction. We can notice that there is a slight improvement when the context length ranges from 0 to 2, and the performance slightly decreases when the context length ranges from 3 to All. This shows that too short can not provide supplementary knowledge where too long will introduce excessive amounts of noise. Hence, taking the previous two utterances before the target utterance into consideration may be a optimal choice. Additionally, processing long contextual information demands increased computational resources, thereby constraining the model's utility.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Analysis Of Dialoguellm'S Emotional Intelligence",
      "text": "Emotional intelligence (EQ) is the ability to manage both human emotions and understand the emotions of other people. A people's EQ affects his/her daily behavior and decision making. Since LLMs (including DialogueLLM) have shown strong emotion understanding ability, then evaluating their emotion intelligence will be the new target. We will answer the question: can DialogueLLM be as emotionally intelligent as humans?\n\nIn particular, we evaluate 12 LLMs' EQ via a benchmarking testing bed, namely SECEU  [37] . This test requires evaluating complex emotions (e.g., surprised, joyful, puzzled, proud) in realistic scenarios (e.g., despite feeling underperformed,  John surprisingly achieved a top score). According SECEU's requirement, we design a standard prompt to ask LLMs to do multi-choice questions in SECEU, where the prompt template is illustrated in Figure  10 .\n\nThe model is tasked with scoring the extent to which the protagonist experiences a given emotion on a scale from 0 to 10, with 10 indicating the highest possible level of that emotion. The cumulative score for the last four emotions must be constrained to 10. To standardize the performance of LLMs, we calculate the Euclidean distance between the LLMs' responses for the i th item (denoted as L i ) and the standard human scores (denoted as SS i ).\n\nWe then average all 40 distances and generate the SECEU score. We then normalize the SECEU scores to derive an EQ score designed to conform to a normal distribution with a mean of 100 and a standard deviation of 15. The calculating process-ing of EQ is written as:\n\nSECEU score = 1 40\n\nwhere M = 2.79 and SD = 15 represents the mean value and the standard deviation. The results of 12 LLMs' EQ scores are shown in Table  6  and Figure  11 . We can notice that the LLaMA base model cannot complete the EQ test, where our DialogueLLM achieves the second highest scores across 12 LLMs. GPT-4 achieves the highest EQ scores against ours (117 v/s 109). However, our DialogueLLM model has only 7 billion parameters, which is 1/257 of GPT-4 (1.8 trillion parameters). The training time required for Dia-logueLLM is considerably shorter than that of GPT-4. DialogueLLM performs the best among all the LLaMA series, and exhibits human-like response patterns, demonstrating a balanced mechanism for high emotion understanding proficiency.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Effects Of Emotional Stimuli",
      "text": "Psychological studies have shown that adding emotional stimuli related to expectations, confidence and social influence can have an impact on individual behavior. For example, real-life students who are taught using encouraging and positive words have a higher success rate than those who are not taught using these words. Therefore, we use the EmotionPrompt approach  [13]  to explore the performance of DialogueLLM in the face of emotional  stimuli. The prompt template for adding emotional stimuli is shown in Figure  12 , with no subsequent prompt included for brevity. The DialogueLLM using the proposed emotion prompt is called Stim-uliLLM.\n\nTable  7  shows the comparison Stim-uliLLM and the original DialogueLLM model on three ERC datasets. We can notice that the Emo-tionPrompt approach significantly boosts the performance of DialogueLLM (1.06% and 1.14% average improvement in terms of performance in zeroshot and one-shot settings).",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Limitations",
      "text": "Although DialogueLLM tries to accurately perform emotion classification by considering both conversational contexts and video descriptions of the utterances, this takes more computing power and train-  ing time. Additionally, the speaker information is also important for improving the performance since different speakers have their own characters. But DialogueLLM does not take it into consideration, due to the limit of the dataset. Also, technology for generating accurate video descriptions automatically still has room for improvement, and inaccurate descriptions can mislead the model's prediction. The issue of using multiple data sources like images and video to improve emotion classification in large language models isn't fully solved yet. Lastly, we train DialogueLLM using a specific approach that focuses on identifying emotions, rather than a general-purpose training method for all affects. Hence, our future work will collect a large scale knowledge corpus that contains over 1M subjective examples covering differnt types of affects, e.g., sentiment, emotion, sarcasm, humor, enthusiasm, etc.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Conclusion And Future Work",
      "text": "ERC presents an intriguing and challenging natural language processing endeavor. In this paper, inspired by the remarkable performance of LLMs and their variants in NLP tasks, we propose Di-alogueLLM, a context and emotion knowledge tuned LLM that is obtained by fine-tuning large language models with benchmarking multi-modal (i.e., texts and videos) emotional dialogues. We offer a comprehensive evaluation of our proposed model on three benchmarking ERC datasets and achieves the state-of-the-art results. This proves that fine-tuning LLMs with task-specific knowledge will yield significant improvement over other PLM based approaches. In future work, we plan to design and generate more precise video descriptions, incorporating multimodal information to further explore the potential of LLMs in the NLP domain. Additionally, considering the close connections between emotions, sarcasm, passion, and depression, we aim to design a multi-affect learning framework based on LLMs.",
      "page_start": 16,
      "page_end": 16
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Sample utterances in a multi-modal conversa-",
      "page": 1
    },
    {
      "caption": "Figure 1: illustrates an example to introduce the presence of",
      "page": 2
    },
    {
      "caption": "Figure 2: Overview of DialogueLLM fine-tuning and classification pipeline.",
      "page": 4
    },
    {
      "caption": "Figure 3: The distribution of three ERC datasets.",
      "page": 5
    },
    {
      "caption": "Figure 3: and Fig. 4. No-",
      "page": 5
    },
    {
      "caption": "Figure 4: The distribution of seven basic emotions",
      "page": 6
    },
    {
      "caption": "Figure 5: The training loss of DialogueLLM.",
      "page": 10
    },
    {
      "caption": "Figure 6: Each cell (i, j) represents the percentage of class",
      "page": 11
    },
    {
      "caption": "Figure 7: It is evident that Dia-",
      "page": 11
    },
    {
      "caption": "Figure 7: In this case,",
      "page": 11
    },
    {
      "caption": "Figure 6: The normalized confusion matrices for Dia-",
      "page": 11
    },
    {
      "caption": "Figure 9: In general, longer context length will allow the",
      "page": 11
    },
    {
      "caption": "Figure 7: A few typical misclassified examples on three datasets.",
      "page": 12
    },
    {
      "caption": "Figure 8: The impact of epoch on the classification performance in a zero-shot setting.",
      "page": 12
    },
    {
      "caption": "Figure 9: The experiments of the varying context lengths.",
      "page": 13
    },
    {
      "caption": "Figure 10: The proposed prompt for the emotional intel-",
      "page": 13
    },
    {
      "caption": "Figure 10: The model is tasked with scoring the extent to",
      "page": 13
    },
    {
      "caption": "Figure 11: We can notice that the",
      "page": 13
    },
    {
      "caption": "Figure 11: LLMs’ EQ. The y-axis indicates the EQ",
      "page": 14
    },
    {
      "caption": "Figure 12: , with no subsequent",
      "page": 14
    },
    {
      "caption": "Figure 12: Prompt template with emotional stimuli",
      "page": 14
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Meld": "Acc w-F1",
          "IEMOCAP": "Acc w-F1"
        },
        {
          "Meld": "71.91 71.81\n71.96 71.90\n71.92 71.63\n70.65 71.04",
          "IEMOCAP": "70.48 69.40\n70.62 69.93\n70.54 69.37\n69.94 68.82"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "\u0000\u001a\u0000\u0014\u0000\u0011\u0000\u001b\u0000\u0014\n\u0000\u0019\u0000\u001a\u0000\u0011\u0000\u001c\u0000\u0017 \u0000\u0019\u0000\u001c\u0000\u0011\u0000\u0013\u0000\u0015 \u0000\u0019\u0000\u001b\u0000\u0011\u0000\u0014\u0000\u0014 \u0000\u0019\u0000\u001c\u0000\u0011\u0000\u0017 \u0000\u0014\u0000\u0010\u0000(\u0000S\u0000R\u0000F\u0000K \u0000\u001a\u0000\u0010\u0000(\u0000S\u0000R\u0000F\u0000K": "\u0000\u0019\u0000\u0019\u0000\u0011\u0000\u001b\u0000\u0018\n\u0000\u0019\u0000\u0013\u0000\u0011\u0000\u0013\u0000\u0016",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "\u0000\u0019\u0000\u0019\u0000\u0011\u0000\u0019\u0000\u0015 \u0000\u0019\u0000\u001a\u0000\u0011\u0000\u0014\u0000\u001b\n\u0000\u0018\u0000\u001c\u0000\u0011\u0000\u0017\u0000\u001a",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "\u0000\u0016\u0000\u0010\u0000(\u0000S\u0000R\u0000F\u0000K \u0000\u0014\u0000\u0013\u0000\u0010\u0000(\u0000S\u0000R\u0000F\u0000K\n\u0000\u0018\u0000\u0010\u0000(\u0000S\u0000R\u0000F\u0000K",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "",
          "Column_17": "",
          "Column_18": "",
          "Column_19": ""
        },
        {
          "\u0000\u001a\u0000\u0014\u0000\u0011\u0000\u001b\u0000\u0014\n\u0000\u0019\u0000\u001a\u0000\u0011\u0000\u001c\u0000\u0017 \u0000\u0019\u0000\u001c\u0000\u0011\u0000\u0013\u0000\u0015 \u0000\u0019\u0000\u001b\u0000\u0011\u0000\u0014\u0000\u0014 \u0000\u0019\u0000\u001c\u0000\u0011\u0000\u0017 \u0000\u0014\u0000\u0010\u0000(\u0000S\u0000R\u0000F\u0000K \u0000\u001a\u0000\u0010\u0000(\u0000S\u0000R\u0000F\u0000K": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "\u0000\u0016\u0000\u001c\u0000\u0011\u0000\u0019\u0000\u001b",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "",
          "Column_17": "",
          "Column_18": "",
          "Column_19": ""
        },
        {
          "\u0000\u001a\u0000\u0014\u0000\u0011\u0000\u001b\u0000\u0014\n\u0000\u0019\u0000\u001a\u0000\u0011\u0000\u001c\u0000\u0017 \u0000\u0019\u0000\u001c\u0000\u0011\u0000\u0013\u0000\u0015 \u0000\u0019\u0000\u001b\u0000\u0011\u0000\u0014\u0000\u0014 \u0000\u0019\u0000\u001c\u0000\u0011\u0000\u0017 \u0000\u0014\u0000\u0010\u0000(\u0000S\u0000R\u0000F\u0000K \u0000\u001a\u0000\u0010\u0000(\u0000S\u0000R\u0000F\u0000K": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "\u0000\u0016\u0000\u001a\u0000\u0011\u0000\u0019\u0000\u0019\n\u0000\u0016\u0000\u0018\u0000\u0011\u0000\u0015\u0000\u0017\n\u0000\u0016\u0000\u0013\u0000\u0011\u0000\u0019\u0000",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "",
          "Column_17": "",
          "Column_18": "\u0000\u0016\u0000\u001b\u0000\u0011\u0000\u0017\u0000\u001a",
          "Column_19": ""
        },
        {
          "\u0000\u001a\u0000\u0014\u0000\u0011\u0000\u001b\u0000\u0014\n\u0000\u0019\u0000\u001a\u0000\u0011\u0000\u001c\u0000\u0017 \u0000\u0019\u0000\u001c\u0000\u0011\u0000\u0013\u0000\u0015 \u0000\u0019\u0000\u001b\u0000\u0011\u0000\u0014\u0000\u0014 \u0000\u0019\u0000\u001c\u0000\u0011\u0000\u0017 \u0000\u0014\u0000\u0010\u0000(\u0000S\u0000R\u0000F\u0000K \u0000\u001a\u0000\u0010\u0000(\u0000S\u0000R\u0000F\u0000K": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "\u00000\u0000H\u0000O\u0000G",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "\u0000,\u0000(\u00000\n\u0000'",
          "Column_10": "\u00002\u0000&\u0000$\n\u0000D\u0000W\u0000D\u0000V\u0000H",
          "Column_11": "\u00003\n\u0000W\u0000V",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "\u0000(\u0000P",
          "Column_16": "\u0000R\u0000U\u0000\\\u0000Q\u0000O\u0000S",
          "Column_17": "",
          "Column_18": "",
          "Column_19": ""
        },
        {
          "\u0000\u001a\u0000\u0014\u0000\u0011\u0000\u001b\u0000\u0014\n\u0000\u0019\u0000\u001a\u0000\u0011\u0000\u001c\u0000\u0017 \u0000\u0019\u0000\u001c\u0000\u0011\u0000\u0013\u0000\u0015 \u0000\u0019\u0000\u001b\u0000\u0011\u0000\u0014\u0000\u0014 \u0000\u0019\u0000\u001c\u0000\u0011\u0000\u0017 \u0000\u0014\u0000\u0010\u0000(\u0000S\u0000R\u0000F\u0000K \u0000\u001a\u0000\u0010\u0000(\u0000S\u0000R\u0000F\u0000K": "",
          "Column_2": "Figu",
          "Column_3": "re8:",
          "Column_4": "Thei",
          "Column_5": "mpac",
          "Column_6": "tofe",
          "Column_7": "",
          "Column_8": "nthe",
          "Column_9": "classi",
          "Column_10": "ficati",
          "Column_11": "onpe",
          "Column_12": "rform",
          "Column_13": "",
          "Column_14": "aze",
          "Column_15": "ro-sh",
          "Column_16": "otset",
          "Column_17": "ting.",
          "Column_18": "",
          "Column_19": ""
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "\u0000\u0014\u0000\u0010\u0000(\u0000S\u0000R\u0000F\u0000K \u0000\u001a\u0000\u0010\u0000(\u0000S\u0000R\u0000F\u0000K": "\u0000\u0016\u0000\u0010\u0000(\u0000S\u0000R\u0000F\u0000K \u0000\u0014\u0000\u0013\u0000\u0010\u0000(\u0000S\u0000R\u0000F\u0000K\n\u0000\u0018\u0000\u0010\u0000(\u0000S\u0000R\u0000F\u0000K"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table 6: and Figure 11. We can notice that the",
      "data": [
        {
          "Meld\nEmorynlp": "IEMOCAP",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": ""
        }
      ],
      "page": 13
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation"
    },
    {
      "citation_id": "2",
      "title": "Phoenix: Democratizing chatgpt across languages",
      "authors": [
        "Zhihong Chen",
        "Feng Jiang",
        "Junying Chen",
        "Tiannan Wang",
        "Fei Yu",
        "Guiming Chen",
        "Hongbo Zhang",
        "Juhao Liang",
        "Chen Zhang",
        "Zhiyi Zhang"
      ],
      "year": "2023",
      "venue": "Phoenix: Democratizing chatgpt across languages",
      "arxiv": "arXiv:2304.10453"
    },
    {
      "citation_id": "3",
      "title": "Pankaj Wasnik, and Naoyuki Onoe. 2022. M2fnet: Multi-modal fusion network for emotion recognition in conversation",
      "authors": [
        "Purbayan Vishal Chudasama",
        "Ashish Kar",
        "Nirmesh Gudmalwar",
        "Shah"
      ],
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "4",
      "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/N19-1423"
    },
    {
      "citation_id": "5",
      "title": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Soujanya Poria",
        "Niyati Chhaya",
        "Alexander Gelbukh"
      ],
      "year": "2019",
      "venue": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "arxiv": "arXiv:1908.11540"
    },
    {
      "citation_id": "6",
      "title": "Dr. llama: Improving small language models in domain-specific qa via generative data augmentation",
      "authors": [
        "Zhen Guo",
        "Peiqi Wang",
        "Yanwei Wang",
        "Shangdi Yu"
      ],
      "year": "2023",
      "venue": "Dr. llama: Improving small language models in domain-specific qa via generative data augmentation",
      "arxiv": "arXiv:2305.07804"
    },
    {
      "citation_id": "7",
      "title": "Icon: Interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Rada Mihalcea",
        "Erik Cambria",
        "Roger Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 conference on empirical methods in natural language processing"
    },
    {
      "citation_id": "8",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Amir Zadeh",
        "Erik Cambria",
        "Louis-Philippe Morency",
        "Roger Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the conference"
    },
    {
      "citation_id": "9",
      "title": "Supervised adversarial contrastive learning for emotion recognition in conversations",
      "authors": [
        "Dou Hu",
        "Yinan Bao",
        "Lingwei Wei",
        "Wei Zhou",
        "Songlin Hu"
      ],
      "year": "2023",
      "venue": "Supervised adversarial contrastive learning for emotion recognition in conversations",
      "arxiv": "arXiv:2306.01505"
    },
    {
      "citation_id": "10",
      "title": "Dialoguecrn: Contextual reasoning networks for emotion recognition in conversations",
      "authors": [
        "Dou Hu",
        "Lingwei Wei",
        "Xiaoyong Huai"
      ],
      "year": "2021",
      "venue": "Dialoguecrn: Contextual reasoning networks for emotion recognition in conversations",
      "arxiv": "arXiv:2106.01978"
    },
    {
      "citation_id": "11",
      "title": "Relation-aware graph attention networks with relational position encodings for emotion recognition in conversations",
      "authors": [
        "Taichi Ishiwatari",
        "Yuki Yasuda",
        "Taro Miyazaki",
        "Jun Goto"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)"
    },
    {
      "citation_id": "12",
      "title": "Instructerc: Reforming emotion recognition in conversation with a retrieval multi-task llms framework",
      "authors": [
        "Shanglin Lei",
        "Guanting Dong",
        "Xiaoping Wang",
        "Keheng Wang",
        "Sirui Wang"
      ],
      "year": "2023",
      "venue": "Instructerc: Reforming emotion recognition in conversation with a retrieval multi-task llms framework",
      "doi": "10.48550/ARXIV.2309.11911"
    },
    {
      "citation_id": "13",
      "title": "Large language models understand and can be enhanced by emotional stimuli",
      "authors": [
        "Cheng Li",
        "Jindong Wang",
        "Yixuan Zhang",
        "Kaijie Zhu",
        "Wenxin Hou",
        "Jianxun Lian",
        "Fang Luo",
        "Qiang Yang",
        "Xing Xie"
      ],
      "year": "2023",
      "venue": "Large language models understand and can be enhanced by emotional stimuli"
    },
    {
      "citation_id": "14",
      "title": "Graphcfc: A directed graph based crossmodal feature complementation approach for multimodal conversational emotion recognition",
      "authors": [
        "Jiang Li",
        "Xiaoping Wang",
        "Guoqing Lv",
        "Zhigang Zeng"
      ],
      "year": "2022",
      "venue": "Graphcfc: A directed graph based crossmodal feature complementation approach for multimodal conversational emotion recognition",
      "doi": "10.48550/ARXIV.2207.12261"
    },
    {
      "citation_id": "15",
      "title": "Multi-task learning with auxiliary speaker identification for conversational emotion recognition",
      "authors": [
        "Jingye Li",
        "Meishan Zhang",
        "Donghong Ji",
        "Yijiang Liu"
      ],
      "year": "2020",
      "venue": "Multi-task learning with auxiliary speaker identification for conversational emotion recognition"
    },
    {
      "citation_id": "16",
      "title": "Contrast and generation make bart a good dialogue emotion recognizer",
      "authors": [
        "Shimin Li",
        "Hang Yan",
        "Xipeng Qiu"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "17",
      "title": "Chatdoctor: A medical chat model fine-tuned on a large language model meta-ai (llama) using medical domain knowledge",
      "authors": [
        "Yunxiang Li",
        "Zihan Li",
        "Kai Zhang",
        "Ruilong Dan",
        "Steve Jiang",
        "You Zhang"
      ],
      "year": "2023",
      "venue": "Cureus"
    },
    {
      "citation_id": "18",
      "title": "Goat: Fine-tuned llama outperforms gpt-4 on arithmetic tasks",
      "authors": [
        "Tiedong Liu",
        "Bryan Kian",
        "Hsiang Low"
      ],
      "year": "2023",
      "venue": "Goat: Fine-tuned llama outperforms gpt-4 on arithmetic tasks",
      "arxiv": "arXiv:2305.14201"
    },
    {
      "citation_id": "19",
      "title": "A quantum probability driven framework for joint multi-modal sarcasm, sentiment and emotion analysis",
      "authors": [
        "Yaochen Liu",
        "Yazhou Zhang",
        "Dawei Song"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2023.3279145"
    },
    {
      "citation_id": "20",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "21",
      "title": "Moving from narrative to interactive multi-modal sentiment analysis: A survey",
      "authors": [
        "Junxia Ma",
        "Lu Rong",
        "Yazhou Zhang",
        "Prayag Tiwari"
      ],
      "year": "2023",
      "venue": "ACM Transactions on Asian and Low-Resource Language Information Processing"
    },
    {
      "citation_id": "22",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "Navonil Majumder",
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Rada Mihalcea",
        "Alexander Gelbukh",
        "Erik Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "23",
      "title": "Gpt-4 technical report",
      "authors": [
        "Openai"
      ],
      "year": "2023",
      "venue": "Gpt-4 technical report"
    },
    {
      "citation_id": "24",
      "title": "Ebtesam Almazrouei, and Julien Launay. 2023. The refinedweb dataset for falcon llm: Outperforming curated corpora with web data",
      "authors": [
        "Guilherme Penedo",
        "Quentin Malartic",
        "Daniel Hesslow",
        "Ruxandra Cojocaru",
        "Alessandro Cappelli",
        "Hamza Alobeidli",
        "Baptiste Pannier"
      ],
      "venue": "Ebtesam Almazrouei, and Julien Launay. 2023. The refinedweb dataset for falcon llm: Outperforming curated corpora with web data"
    },
    {
      "citation_id": "25",
      "title": "Instruction tuning with gpt-4",
      "authors": [
        "Baolin Peng",
        "Chunyuan Li",
        "Pengcheng He",
        "Michel Galley",
        "Jianfeng Gao"
      ],
      "year": "2023",
      "venue": "Instruction tuning with gpt-4",
      "arxiv": "arXiv:2304.03277"
    },
    {
      "citation_id": "26",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th annual meeting of the association for computational linguistics"
    },
    {
      "citation_id": "27",
      "title": "Navonil Majumder, Gautam Naik, Erik Cambria, and Rada Mihalcea",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika"
      ],
      "year": "2018",
      "venue": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "28",
      "title": "Multimodal learning using optimal transport for sarcasm and humor detection",
      "authors": [
        "Shraman Pramanick",
        "Aniket Roy",
        "M Vishal",
        "Patel"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision"
    },
    {
      "citation_id": "29",
      "title": "Mutualenhanced incongruity learning network for multimodal sarcasm detection",
      "authors": [
        "Yang Qiao",
        "Liqiang Jing",
        "Xuemeng Song",
        "Xiaolin Chen",
        "Lei Zhu",
        "Liqiang Nie"
      ],
      "year": "2023",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "30",
      "title": "Improving language understanding by generative pre-training",
      "authors": [
        "Alec Radford",
        "Karthik Narasimhan",
        "Tim Salimans",
        "Ilya Sutskever"
      ],
      "year": "2018",
      "venue": "Improving language understanding by generative pre-training"
    },
    {
      "citation_id": "31",
      "title": "Pangu-{\\Sigma}: Towards trillion parameter language model with sparse heterogeneous computing",
      "authors": [
        "Xiaozhe Ren",
        "Pingyi Zhou",
        "Xinfan Meng",
        "Xinjing Huang",
        "Yadao Wang",
        "Weichao Wang",
        "Pengfei Li",
        "Xiaoda Zhang",
        "Alexander Podolskiy",
        "Grigory Arshinov"
      ],
      "year": "2023",
      "venue": "Pangu-{\\Sigma}: Towards trillion parameter language model with sparse heterogeneous computing",
      "arxiv": "arXiv:2303.10845"
    },
    {
      "citation_id": "32",
      "title": "Dialogxl: All-in-one xlnet for multi-party conversation emotion recognition",
      "authors": [
        "Weizhou Shen",
        "Junqing Chen",
        "Xiaojun Quan",
        "Zhixian Xie"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "33",
      "title": "Directed acyclic graph network for conversational emotion recognition",
      "authors": [
        "Weizhou Shen",
        "Siyue Wu",
        "Yunyi Yang",
        "Xiaojun Quan"
      ],
      "year": "2021",
      "venue": "Directed acyclic graph network for conversational emotion recognition",
      "arxiv": "arXiv:2105.12907"
    },
    {
      "citation_id": "34",
      "title": "Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models",
      "authors": [
        "Hugo Touvron",
        "Thibaut Lavril",
        "Gautier Izacard",
        "Xavier Martinet",
        "Marie-Anne Lachaux",
        "Timothée Lacroix",
        "Baptiste Rozière",
        "Naman Goyal"
      ],
      "venue": "Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models",
      "arxiv": "arXiv:2302.13971"
    },
    {
      "citation_id": "35",
      "title": "Igor Molybog",
      "authors": [
        "Hugo Touvron",
        "Louis Martin",
        "Kevin Stone",
        "Peter Albert",
        "Amjad Almahairi",
        "Yasmine Babaei",
        "Nikolay Bashlykov",
        "Soumya Batra",
        "Prajjwal Bhargava",
        "Shruti Bhosale",
        "Dan Bikel",
        "Lukas Blecher",
        "Cristian Canton Ferrer",
        "Moya Chen",
        "Guillem Cucurull",
        "David Esiobu",
        "Jude Fernandes",
        "Jeremy Fu",
        "Wenyin Fu",
        "Brian Fuller",
        "Cynthia Gao",
        "Vedanuj Goswami",
        "Naman Goyal",
        "Anthony Hartshorn",
        "Saghar Hosseini",
        "Rui Hou",
        "Hakan Inan",
        "Marcin Kardas",
        "Viktor Kerkez",
        "Madian Khabsa",
        "Isabel Kloumann",
        "Artem Korenev",
        "Punit Singh Koura",
        "Marie-Anne Lachaux",
        "Thibaut Lavril",
        "Jenya Lee",
        "Diana Liskovich",
        "Yinghai Lu",
        "Yuning Mao",
        "Xavier Martinet",
        "Todor Mihaylov",
        "Pushkar Mishra"
      ],
      "venue": "Igor Molybog"
    },
    {
      "citation_id": "36",
      "title": "Huatuo: Tuning llama model with chinese medical knowledge",
      "authors": [
        "Haochun Wang",
        "Chi Liu",
        "Nuwa Xi",
        "Zewen Qiang",
        "Sendong Zhao",
        "Bing Qin",
        "Ting Liu"
      ],
      "year": "2023",
      "venue": "Huatuo: Tuning llama model with chinese medical knowledge",
      "arxiv": "arXiv:2304.06975"
    },
    {
      "citation_id": "37",
      "title": "Emotional intelligence of large language models",
      "authors": [
        "Xuena Wang",
        "Xueting Li",
        "Zi Yin",
        "Yue Wu",
        "Liu Jia"
      ],
      "year": "2023",
      "venue": "Emotional intelligence of large language models",
      "doi": "10.48550/ARXIV.2307.09042"
    },
    {
      "citation_id": "38",
      "title": "Pmc-llama: Further finetuning llama on medical papers",
      "authors": [
        "Chaoyi Wu",
        "Xiaoman Zhang",
        "Ya Zhang",
        "Yanfeng Wang",
        "Weidi Xie"
      ],
      "year": "2023",
      "venue": "Pmc-llama: Further finetuning llama on medical papers",
      "arxiv": "arXiv:2304.14454"
    },
    {
      "citation_id": "39",
      "title": "Emotion detection on tv show transcripts with sequencebased convolutional neural networks",
      "authors": [
        "M Sayyed",
        "Jinho D Zahiri",
        "Choi"
      ],
      "year": "2017",
      "venue": "Emotion detection on tv show transcripts with sequencebased convolutional neural networks",
      "arxiv": "arXiv:1708.04299"
    },
    {
      "citation_id": "40",
      "title": "Coin: Conversational interactive networks for emotion recognition in conversation",
      "authors": [
        "Haidong Zhang",
        "Yekun Chai"
      ],
      "year": "2021",
      "venue": "Proceedings of the Third Workshop on Multimodal Artificial Intelligence"
    },
    {
      "citation_id": "41",
      "title": "Sinno Jialin Pan, and Lidong Bing. 2023. Sentiment analysis in the era of large language models: A reality check",
      "authors": [
        "Wenxuan Zhang",
        "Yue Deng",
        "Bing Liu"
      ],
      "venue": "Sinno Jialin Pan, and Lidong Bing. 2023. Sentiment analysis in the era of large language models: A reality check",
      "arxiv": "arXiv:2305.15005"
    },
    {
      "citation_id": "42",
      "title": "M3gat: A multimodal multi-task interactive graph attention network for conversational sentiment analysis and emotion recognition",
      "authors": [
        "Yazhou Zhang",
        "Ao Jia",
        "Bo Wang",
        "Peng Zhang",
        "Dongming Zhao",
        "Pu Li",
        "Yuexian Hou",
        "Xiaojia Jin",
        "Dawei Song",
        "Jing Qin"
      ],
      "year": "2023",
      "venue": "ACM Transactions on Information Systems"
    },
    {
      "citation_id": "43",
      "title": "Quantum-inspired interactive networks for conversational sentiment analysis",
      "authors": [
        "Yazhou Zhang",
        "Qiuchi Li",
        "Dawei Song",
        "Peng Zhang",
        "Panpan Wang"
      ],
      "year": "2019",
      "venue": "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence IJCAI-19",
      "doi": "10.24963/ijcai.2019/755"
    },
    {
      "citation_id": "44",
      "title": "Cfn: a complexvalued fuzzy network for sarcasm detection in conversations",
      "authors": [
        "Yazhou Zhang",
        "Yaochen Liu",
        "Qiuchi Li",
        "Prayag Tiwari",
        "Benyou Wang",
        "Yuhua Li",
        "Hari Pandey",
        "Peng Zhang",
        "Dawei Song"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Fuzzy Systems"
    },
    {
      "citation_id": "45",
      "title": "A multitask learning model for multimodal sarcasm, sentiment and emotion recognition in conversations",
      "authors": [
        "Yazhou Zhang",
        "Jinglin Wang",
        "Yaochen Liu",
        "Lu Rong",
        "Qian Zheng",
        "Dawei Song",
        "Prayag Tiwari",
        "Jing Qin"
      ],
      "year": "2023",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "46",
      "title": "Learning multi-task commonness and uniqueness for multi-modal sarcasm detection and sentiment analysis in conversation",
      "authors": [
        "Yazhou Zhang",
        "Yang Yu",
        "Dongming Zhao",
        "Zuhe Li",
        "Bo Wang",
        "Yuexian Hou",
        "Prayag Tiwari",
        "Jing Qin"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Artificial Intelligence"
    },
    {
      "citation_id": "47",
      "title": "A survey of large language models",
      "authors": [
        "Kun Wayne Xin Zhao",
        "Junyi Zhou",
        "Tianyi Li",
        "Xiaolei Tang",
        "Yupeng Wang",
        "Yingqian Hou",
        "Beichen Min",
        "Junjie Zhang",
        "Zican Zhang",
        "Dong"
      ],
      "year": "2023",
      "venue": "A survey of large language models",
      "arxiv": "arXiv:2303.18223"
    },
    {
      "citation_id": "48",
      "title": "Cauain: Causal aware interaction network for emotion recognition in conversations",
      "authors": [
        "Weixiang Zhao",
        "Yanyan Zhao",
        "Xin Lu"
      ],
      "year": "2022",
      "venue": "Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI"
    },
    {
      "citation_id": "49",
      "title": "Topic-driven and knowledgeaware transformer for dialogue emotion detection",
      "authors": [
        "Lixing Zhu",
        "Gabriele Pergola",
        "Lin Gui",
        "Deyu Zhou",
        "Yulan He"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
      "doi": "10.18653/v1/2021.acl-long.125"
    }
  ]
}