{
  "paper_id": "2409.07589v2",
  "title": "Mimamba: Eeg-Based Emotion Recognition With Multi-Scale Inverted Mamba Models",
  "published": "2024-09-11T19:39:58Z",
  "authors": [
    "Xin Zhou",
    "Dawei Huang",
    "Xiaojing Peng",
    "Lijun Yin"
  ],
  "keywords": [
    "Electroencephalogram (EEG)",
    "emotion recognition",
    "multi-scale",
    "spatiotemporal feature"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "EEG-based emotion recognition holds significant potential in the field of brain-computer interfaces. A key challenge lies in extracting discriminative spatiotemporal features from electroencephalogram (EEG) signals. Existing studies often rely on domain-specific time-frequency features and analyze temporal dependencies and spatial characteristics separately, neglecting the interaction between local-global relationships and spatiotemporal dynamics. To address this, we propose a novel network called Multi-Scale Inverted Mamba (MS-iMamba), which consists of Multi-Scale Temporal Blocks (MSTB) and Temporal-Spatial Fusion Blocks (TSFB). Specifically, MSTBs are designed to capture both local details and global temporal dependencies across different scale subsequences. The TSFBs, implemented with an inverted Mamba structure, focus on the interaction between dynamic temporal dependencies and spatial characteristics. The primary advantage of MS-iMamba lies in its ability to leverage reconstructed multi-scale EEG sequences, exploiting the interaction between temporal and spatial features without the need for domain-specific time-frequency feature extraction. Experimental results on the DEAP, DREAMER, and SEED datasets demonstrate that MS-iMamba achieves classification accuracies of 94.86%, 94.94%, and 91.36%, respectively, using only four-channel EEG signals, outperforming state-of-the-art methods.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "E MOTION recognition is pivotal for enhancing human- computer interaction and intelligent systems. Accurately identifying emotional states enables systems to respond more appropriately to human needs, thereby improving interaction naturalness and efficiency. EEG, as a non-invasive method for physiological signal acquisition, offers superior temporal resolution and continuity compared to other signals like facial expressions or voice, allowing real-time capture of human's emotional dynamics. In practical applications, EEG-based emotion recognition can facilitate mental health monitoring and diagnostic support by detecting abnormal patterns related to emotional disorders, providing objective indicators for clinical diagnosis  [1] .\n\nExtracting and analyzing discriminative spatiotemporal features from EEG signals is a challenging task due to the brain's complex spatial topology. Traditional approaches often involve manual extraction of domain-specific time-frequency features such as differential entropy (DE)  [2] ,  [3] , power spectral density (PSD)  [4] ,  [5] , and functional connectivity  [6] . While these methods have advanced EEG emotion recognition, they are time-consuming, require extensive domain knowledge, and often lose valuable temporal information by compressing long time series into single single eigenvalue.\n\nTo address these limitations, deep learning methods have gained prominence for their end-to-end capabilities. For example, Cui et al.  [7]  utilized gated recurrent units combined with minimal class confusion for emotion recognition. Feng  [8]  and Li  [9]  integrated attention mechanisms into bidirectional long short-term memory (LSTM) modules to extract key temporal features from EEG sequences. Similarly, Du et al.  [10]  applied attention mechanisms with LSTM-generated feature vectors to automatically select appropriate EEG channels for emotion recognition. Other studies have framed physiological signal emotion recognition tasks as sequence-to-sequence multivariate time series prediction problems, employing advanced selfattention mechanisms to decompose signals into independent frequency and time-domain representations  [11] . These approaches effectively capture useful temporal dependencies. Given that individual EEG time steps lack semantic meaning  [12] , the appropriateness of iterating or calculating mutual correlations among them is questionable. Inspired by this, we segment EEG signals into patches of different scales, aggregating time steps into subsequence-level patches to enhance local details and capture global relationships that single time points cannot provide.\n\nOther deep learning methods have been utilized to construct spatial features from EEG signals, significantly enhancing emotion recognition accuracy. Typical spatial feature extraction methods include convolutional neural networks (CNNs)  [13] ,  [14] . For example, Li et al  [15] . employed a novel efficient convolutional block to reduce computational burden. Liu et al.  [16]  proposed a model named 3-D Convolutional Attention Neural Network (3DCANN), which consists of spatiotemporal feature extraction modules and EEG channel attention weight learning modules. This model effectively captures dynamic relationships between multi-channel EEG signals and the internal spatial relationships within these signals. Recent studies have shown that graph convolutions can effectively utilize brain topological structures for emotion recognition. Lin et al.  [17]  developed an improved graph convolution model combined with dynamic channel selection to simulate information transmission in the brain. This model combines the advantages of one-dimensional convolution and graph convolution, capturing intra-channel and inter-channel EEG features and further modeling inter-regional brain con-nectivity by adding functional connectivity. Feng et al  [8] . introduced a spatial graph convolution module that adaptively learns intrinsic connections between EEG channels using an adjacency matrix to extract spatial domain features. Additionally, researchers like Cui  [18]  and Deng et al.  [19]  explored the spatial information of adjacent and symmetrical channels from the perspective of whether EEG signals exhibit symmetrical emotional responses. These studies underscore the importance of understanding brain topological structures in EEG-based emotion recognition tasks.\n\nThe brain's complex structure results in EEG signals with time-varying spatial topology and temporal dependencies recorded through multiple electrodes. It is intuitive to use both temporal dependencies and spatial features as auxiliary information. For instance, a novel Attention-based Spatiotemporal Dual-Stream Fusion Network (ASTDF-Net)  [20]  has been employed to learn a joint latent subspace to capture the coupled spatiotemporal information in EEG signals.  Cheng and Feng et al.  proposed a hybrid model combining a Spatial-Graph Convolutional Network (SGCN) module and an attention-enhanced bidirectional Long Short-Term Memory (LSTM) module  [8] , and later designed a hybrid network comprising a Dynamic Graph Convolution (DGC) module and a Temporal Self-Attention Representation (TSAR) module, integrating spatial topology and temporal information  [21] . Gong et al.  [22]  used a novel Attention-based Convolutional Transformer Neural Network (ACTNN), effectively integrating key spatial, spectral, and temporal information of EEG signals and cascading CNNs with transformers for emotion recognition tasks. Shen et al.  [23]  utilized multi-scale temporal self-attention modules to learn temporal continuity information while employing dynamic graph convolution modules to capture spatial functional relationships between different EEG electrodes. Although these integrated models consider both temporal and spatial features, they typically use two separate branches to extract spatiotemporal information, lacking adequate interaction between them.\n\nGiven these challenges, this article proposes a spatiotemporal fusion network called Multi-Scale Inverted Mamba (MS-iMamba), combining Multi-Scale Temporal Blocks (MSTB) and Temporal-Spatial Fusion Blocks (TSFB). The primary advantage of the proposed MS-iMamba is its ability to simultaneously leverage local details and global relationships in EEG signals without the need for complex statistical feature extraction, enhancing emotion recognition performance through adequate spatiotemporal dependency interactions. Specifically, MSTB divides EEG signals into multiple scale patches, using small-scale patches to capture fine local details and coarse global relationships, thereby utilizing complementary predictive capabilities in multi-scale observations. TSFB embeds the temporal dimension rather than the spatial dimension of reconstructed multi-scale EEG signals into a token and uses a Selective State-Space Model (SSM) to model spatiotemporal information. This mechanism fully integrates the spatiotemporal dependencies of both modules to enhance EEG emotion recognition.\n\nThe main contributions of this study are as follows:\n\n• A plug-and-play MSTB is designed, which considers local details and global relationships without requiring traditional domain-specific time-frequency statistical feature extraction, providing a promising perspective for time dependency modeling in EEG emotion recognition. • The proposed TSFB reflects on the modeling approach of the spatiotemporal characteristics of EEG signals, adequately considering the interaction between temporal dependencies and spatial features, offering a method that simultaneously integrates temporal and spatial features for EEG emotion recognition. • Intra-subject and inter-subject experiments were conducted on three public datasets: DEAP, DREAMER, and SEED. The experimental results demonstrate that the proposed MS-iMamba outperforms various state-of-theart methods using only four-channel EEG data. The remaining sections of this article are organized as follows. Section II reviews the related work on multi-scale and spatiotemporal representation learning. Section III presents the pipeline of MS-iMmaba. Section IV details the procedure of the conducted experiments and experimental results. A more in-depth discussion is provided in Section V. Finally, the study is concluded in Section VI.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work A. Multi-Scale Representation Learning",
      "text": "Representing data in fine granularity has been widely adopted in time series prediction  [12]  and computer vision fields  [24] ,  [25] . EEG emotion recognition is essentially a time series prediction task, and considering an effective sequence representation approach is necessary. Nie et al.  [12]  argued that for time series data, single-point data lacks clear semantic information unlike words, making the computation of singletime-step correlations debatable. In natural language processing, it is also more effective to symbolize words in a sentence rather than each letter  [26] ,  [27] . This approach of aggregating single-point data into patches has been validated effectively in time series prediction tasks  [28] . For instance, Wu et al.  [29]  addressed the limitations of 1-D time series by segmenting the sequence into short and long periods representations. These representations were embedded into the columns and rows of a 2-D tensor to capture inter-and intra-periodic variations, respectively, allowing easy modeling of 2-D variations using 2-D convolutional kernels. Chen et al.  [30]  highlighted the difficulty of capturing features across multiple scales when modeling time series with limited or fixed scales. Their proposed Pathformer model achieved multi-scale modeling by integrating time resolution and time distance, dividing the time series into different time resolutions and performing dual attention mechanisms at each scale to capture global correlations and local details as temporal dependencies.\n\nMulti-scale representation learning has also been applied to EEG signal processing. Wang et al.  [31]  proposed the Multi-Scale Convolutional Neural Network-Dynamic Graph Convolutional Network (AMCNN-DGCN) model to avoid the cumbersome manual feature extraction process. Jiang et al.  [32]  designed a novel Attention Mechanism-Based Multi-Scale Feature Fusion Network (AM-MSFFN) that considers high-level features at different scales to enhance the model's generalization capability across different subjects. To extract a comprehensive range of multi-class features from multi-channel EEG time series for accurate understanding of brain activity, Li et al.  [33]  introduced a Multi-Scale Attention Mechanism Fusion Convolutional Neural Network (MS-AMF), which extracts spatiotemporal multi-scale features from signals representing multiple brain regions and employs a dense fusion strategy to retain maximum information flow. These studies underscore the importance of multi-scale representation learning in EEG temporal modeling and demonstrate its potential in the field of emotion recognition.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Spatiotemporal Representation Learning",
      "text": "In the field of multivariate time series prediction, the fusion of spatiotemporal features has become a popular strategy for improving prediction accuracy. Numerous scholars have focused on effectively integrating temporal continuity with spatial correlations  [34] ,  [35] . Grigsby et al. proposed Spacetimeformer  [36] , which transforms multivariate time series problems into a spatiotemporal sequence format. In this approach, each input token represents the value of a single variable at a given time step, allowing simultaneous learning of temporal and spatial relationships. Other works have modeled spatiotemporal relationships by transforming one-dimensional or multidimensional sequence data into tensors  [37] ,  [38] . Jin et al.  [39]  demonstrated that traditional methods, which process multichannel EEG signals into one-dimensional graphical features, limit the expressive capability of emotion recognition models. To address this issue, they introduced the G2G module, which transforms one-dimensional graphical data into two-dimensional grid data through numerical relationship encoding, using deep models like ResNet for subsequent tasks. Li et al.  [40]  employed dilated causal convolutional neural networks to extract nonlinear relationships between different time frames and used feature-level fusion to merge features from multiple channels, exploring potential complementary information between different views to enhance feature representation.\n\nRecently, the integration of attention mechanisms and graph neural networks for EEG spatiotemporal modeling has gained increasing attention. Cheng and Feng have conducted extensive research in this direction. Initially, they proposed a model combining a Spatial Graph Convolution Network (SGCN) module with an attention-enhanced bidirectional Long Short-Term Memory (LSTM) module. This model's main advantage is its consideration of each brain region's biological topology, extracting representative spatiotemporal features from multiple EEG channels  [8] . They subsequently designed a hybrid network comprising a Dynamic Graph Convolution (DGC) module and a Temporal Self-Attention Representation (TSAR) module, incorporating representative knowledge of spatial topology and temporal context into EEG emotion recognition tasks  [21] . Recently, they equipped the Dense Graph Convolutional Network (DGC) with Joint Cross-Attention (JCA) for multimodal emotion recognition tasks, termed DG-JCA  [41] . However, Zeng et al.  [42]  demonstrated that single-layer linear models unexpectedly outperformed complex Transformerbased models in time series prediction tasks. Liu et al.  [43]   reflected on this result, suggesting that for multivariate sequence data, points on different channels at the same time step record entirely different physical meanings or events, making embedding them into tokens inappropriate. They proposed the Inverted Transformer (iTransformer), which treats independent time series as tokens and captures multivariate correlations through self-attention to leverage spatiotemporal mutual information. These methods share a common goal of revealing and utilizing the spatiotemporal features of time series data from different perspectives to achieve higher prediction accuracy. Each method has its specific application scenarios and advantages, but they all highlight the importance of spatiotemporal feature fusion in current research, providing a wealth of technical options and research directions for the field of EEG emotion recognition.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Method",
      "text": "In this section, we formalize the MS-iMamba network for EEG emotion recognition. As illustrated in Figure  1 , the network consists of two primary modules: the Multi-Scale Temporal Block (MSTB) and the Temporal-Spatial Fusion Block (TSFB). Each module will be discussed in detail below.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Notations And Definitions",
      "text": "Let the EEG signals of each subject be represented by a 3-D matrix S ∈ R M ×T ×C , where M , T , and C denote the number of trials, sampling points, and channels, respectively. The matrix S is segmented into N samples of length L using a non-overlapping sliding window (thus, T = N × L). The segmented EEG samples are denoted as\n\nFor the same trial, all N segments share the same label. Each segmented sample is denoted as X 1D := X ij . The goal of EEG emotion recognition is to predict Y ij given X 1D .\n\nB. Multi-Scale Temporal Block (MSTB) 1) Multi-Scale Representation: Let X 1D denote an EEG signal of length L with C channels. Before representing this signal in a multi-scale format, we need to determine the patch sizes. To achieve this, we transform the original EEG signal into the frequency domain for analysis. Specifically, as shown in Equation  1 :\n\nwhere F F T denotes the Fast Fourier Transform (FFT), and A represents the amplitude calculation for each frequency. Since high-frequency regions often contain significant noise, we select only the top k frequencies with the highest amplitudes to avoid interference . The selected frequencies {f 1 , f 2 , . . . , f k } correspond to periods and amplitudes {p 1 , p 2 , . . . , p k } and {A f1 , A f2 , . . . , A f k }, respectively. The periods {p 1 , p 2 , . . . , p k } are used as the patch sizes for segmenting the EEG signal.\n\nAs illustrated in the left part of Figure  1 , the original EEG signal is transformed into the frequency domain using FFT, with the red dashed boxes indicating the k frequencies with the highest amplitudes. We then calculate the weights for each frequency using Equation  2 :\n\nNext, the signal X 1D is segmented into patches of varying sizes and reshaped into a 2-D format, as shown in Equation  3 :\n\nwhere the padding operation ensures the original sequence can be divided into integer patches. The reshaped EEG signal is represented in a multi-scale format, X i 2D ∈ R pi×fi×C , which denotes the i-th reshaped time series based on period p i . The vertical and horizontal directions represent intrapatch and inter-patch variations, respectively. These variations capture local details and global relationships. Consequently, we obtain a set of 2-D tensors derived from different patches {X 1 2D , X 2 2D , . . . , X k 2D }. This transformation facilitates capturing information at various distances, with larger p i capturing longer temporal dependencies. Additionally, the reshaped tensors allow for efficient feature extraction using convolutional operations. 2) Multi-Scale Perception: The reshaped tensors are processed by the Multi-Scale Perception (MSP) module, as shown in Equation  4 :\n\n) In this module, convolutional kernels of different sizes are employed. This mechanism allows the module to simultaneously perceive variations within the same patch and across patches with the same phase. After the convolution operations, we reshape the 2-D tensors back to the 1-D form X i 1D . To assign different levels of attention to features extracted from patches corresponding to different frequencies, we perform a weighted sum of these multi-scale signals to obtain the final reconstructed multi-scale representation, as shown in Equation  5 :\n\nThis approach ensures that the features from various scales are effectively combined, enhancing the overall representation of the EEG signal for emotion recognition.\n\nC. Temporal-Spatial Fusion Block (TSFB) 1) Inverted Embedding Representation: After obtaining the multi-scale representation of the EEG signal, we consider the interaction of temporal and spatial information. Generally, conventional methods embed data from different channels at the same time step into a single token. As illustrated in the upper part of Figure  2 , the conventional embedding method places points from different electrodes, each representing completely different events and physical meanings, into the same token. Specifically, at a certain time point, some channel data might be at a peak while others are at a trough. Embedding them into the same token not only fails to reveal valuable information due to the narrow focus of a single time point but also represents misaligned events as a single token.\n\nTherefore, we adopt an inverted embedding method, as shown in the lower part of Figure  2 . The inverted embedding method maps multiple time steps of the same channel into a single token. This event-driven representation not only considers information over longer time steps but also distinguishes data from different channels through separate tokens. The inverted embedding representation approach enhances the capacity to capture temporal dependencies and spatial relationships, ensuring a more comprehensive and meaningful interpretation of the EEG signals for emotion recognition.\n\nThis method is demonstrated through the following equations. Given a multi-scale EEG representation X 1D , we reshape it to consider the temporal and spatial interactions:\n\nHere, X1D represents that temporal steps and channels are reorganized to reflect the inverted embedding structure. To capture the dynamic interactions between temporal and spatial features, we apply a SSM to X1D . This inversion of the embedding representation and the fusion of temporal-spatial information using SSM enhance the ability to model the complex dependencies in EEG signals, leading to improved performance in emotion recognition tasks.\n\n2) iMamba: After the inverted embedding operation, X 1D ∈ R L×C is transformed into X1D ∈ R C×L . Next, we introduce the iMamba model, which consists of the inverted embedding mechanism and the SSM, specifically Mamba, to capture spatiotemporal correlations.\n\nMamba is inspired by continuous systems, mapping a 1-D sequence through a hidden state h(t) ∈ R N to x(t) ∈ R → y(t) ∈ R. As shown in Equation  7 , Mamba uses three parameter matrices A ∈ R d×d , B ∈ R d×1 , and C ∈ R 1×d (where d is the hidden dimension) to control this process. These parameters are analogous to the forget gate, input gate, and output gate mechanisms in LSTM. The parameter A controls how much information is ignored, B controls how the current input affects the hidden state, and C controls the output flow of information:\n\nTo adapt to discrete sequences, Mamba uses zero-order hold techniques, transforming A and B into their discrete versions via the time scale parameter ∆, as defined below:\n\nThe discrete version is redefined as follows:\n\nFor parallel processing, Mamba computes the output using the following convolution form:\n\nwhere L is the length of input sequence x and K is the structured convolution kernel. Due to the inverted embedding operation, iMamba can extract both temporal and spatial features from the input data, fully considering spatiotemporal interactions. iMamba receives the input X1D ∈ R C×L and produces the output prediction Ŷi through the following calculation:\n\nwhere f (•) is a linear classifier consisting of a Linear layer and a softmax operation. The cross-entropy loss is computed as follows:\n\nwhere n denotes the number of categories and 1 [i= Ŷi] equals 1 if the predicted class matches the true label, and 0 otherwise. Finally, the backpropagation algorithm is used to update the network parameters. The pseudocode of MS-iMamba is summarized in Algorithm 1.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Algorithm 1 Ms-Imamba For Eeg Emotion Recognition",
      "text": "Require: EEG signal S ∈ R M ×T ×C Ensure: Predicted label Ŷ 1: Preprocessing: 2: Slice S into non-overlapping windows to get samples I = {(X ij , Y ij )} 3: Multi-Scale Temporal Block (MSTB): 4: Transform X ij to frequency domain using FFT 5: A = A(FFT(X 1D )) 6: Select top k frequencies and their periods:",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iv. Experiment And Results Analysis A. Datasets",
      "text": "DEAP  [44] : The DEAP dataset comprises multimodal data collected from 32 participants. Each participant watched 40 music videos while 32-channel EEG data and 8-channel peripheral physiological signals were recorded. Participants rated the videos on a scale from 1 to 9 for valence, arousal, dominance, and liking. Each video contains 60 seconds of data (excluding a 3-second baseline signal), which was downsampled to 128 Hz and filtered using a 4-45 Hz band-pass filter. We classified each metric into high and low categories using a threshold of 5. To augment the dataset, we segmented each signal into 1-second non-overlapping segments. In our experiments, we used only the frontal polar region channels FP1, FP2, AF3, and AF4.\n\nDREAMER  [45] : The DREAMER dataset also contains multimodal data from 23 participants. Each participant watched 18 video clips (ranging from 65s to 393s, with an average duration of 199s), while 14-channel EEG and 2-channel Electrocardiograph (ECG) signals were recorded. Participants rated valence, arousal, and dominance on a scale from 1 to 5. The signals were sampled at 128 Hz and filtered to 4-45 Hz using a band-pass filter. The EEG signals were then segmented into 1-second non-overlapping segments to expand the dataset. For DREAMER, we used four channels from the frontal polar and frontal regions: AF3, AF4, F7, and F8. Each metric was classified into high and low categories using a threshold of 3.\n\nSEED  [46] : The SEED dataset includes data from 15 participants, with 62-channel EEG data collected according to the international 10-20 system. Each participant conducted three sessions approximately one week apart, during which they watched 15 different film clips (each lasting about 4 minutes). These films elicited positive, neutral, and negative emotions as experimental stimuli. The data were downsampled to 200 Hz and filtered to 0-75 Hz, then segmented into 1-second nonoverlapping segments. Only the frontal polar region channels FP1, FP2, AF3, and AF4 were selected for our experiments. Finally, to mitigate data drift across different channels, Z-score normalization was applied to all three datasets.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Training Protocol",
      "text": "In our experiments, we employed two different paradigms: intra-subject and inter-subject paradigm. For the intra-subject paradigm, we evaluated each participant's data individually, using 80% for training and 20% for testing. For the intersubject paradigm, we combined and shuffled the data from all participants, splitting it into training and testing sets in a 4:1 ratio. Due to the SEED dataset comprising data from three different sessions, which significantly impacts experimental results, we also used intra-session and inter-session evaluation methods. Our training configuration included a batch size of 32, the Adam optimizer with an initial learning rate of 1 × 10 -3 , and 10 epochs of training. An adaptive learning rate strategy was employed to reduce the learning rate as the training loss decreased. Other hyperparameters, such as the number of network layers and Top-k, were set to 1 and 2, respectively. All experiments were conducted on an Intel Xeon Silver 4210R CPU @ 2.40GHz (×2) and an NVIDIA RTX A6000 GPU.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "C. Baseline Model Selection",
      "text": "For our benchmark model selection, we chose several representative methods to compare against our model under the same experimental settings. These models are sourced from the Time Series Library (TSlib 1  ) and include the top three ranked models for classification tasks: TimesNet, Nonstationary Transformer (NTransformer), and Informer. Additionally, we included models characterized by linear structures and causal convolution structures, such as DLinear and TCN. Below is a brief introduction to these benchmark models:\n\n• iTransformer  [43] : iTransformer addresses the shortcomings of traditional Transformers in modeling spatiotemporal information by proposing an inverted Transformer structure that better considers spatiotemporal relationships.\n\n• DLinear  [42] : DLinear decomposes sequences into periodic and trend components, achieving impressive results in various time series tasks using a straightforward linear structure, outperforming many complex Transformer models and their variants.\n\n• TimesNet  [29] : TimesNet employs a multi-scale strategy to transform time series from 1-D to 2-D format, capturing both intra-period and inter-period variations.\n\n• NTransformer  [47] : This model designs non-stationary attention mechanisms to recover inherent non-stationary information in time dependencies through distinguishable attention learned from the raw sequences.\n\n• Informer  [48] : Informer utilizes sparse attention and a self-distillation mechanism to reduce the computational complexity of attention maps to logarithmic levels.\n\n• TCN  [49] : TCN introduces the concept of dilated causal convolutions, which are favored for expanding the receptive field without increasing computational burden. As shown in Table  I , MS-iMamba demonstrates outstanding performance on both the DEAP and DREAMER datasets, significantly outperforming other models in most metrics. Specifically, it achieves the highest accuracy in DEAP (valence) at 94.69%, DEAP (arousal) at 95.03%, and DREAMER (valence) at 94.54%. It also achieves the second-highest accuracy in DREAMER (arousal) at 95.34%, underscoring its robustness and effectiveness in emotion recognition tasks. This makes MS-iMamba an excellent choice for applications requiring high-precision valence and arousal detection from the DEAP and DREAMER datasets. Notably, the linear model DLinear also performs well in this context, second only to MS-iMamba, and even achieving the highest accuracy in DREAMER (arousal). Surprisingly, TCN surpasses several Transformerbased models, while TimesNet performs comparably to them. Table  II  presents the accuracy of different models across four sessions: inter-session, session 1, session 2, and session 3. MS-iMamba consistently maintains the highest accuracy in all sessions, demonstrating its strong performance and adaptability to various session conditions. The most significant improvement is observed in the inter-session scenario, where MS-iMamba outperforms the second-best model by approximately 22.39%. This substantial advantage highlights MS-iMamba's exceptional ability to generalize across different session data. In specific session scenarios, MS-iMamba surpasses the second-best model by 7.44%, 4.48%, and 6.60%, respectively. DLinear consistently ranks second in sessions 1, 2, and 3, indicating its reliability and effectiveness, although it lags noticeably in the inter-session scenario. TimesNet shows relatively high accuracy in the inter-session scenario (70.21%), but its performance declines in subsequent sessions, indicating potential limitations in session-specific contexts. Other models, such as iTransformer, NTransformer, and TCN, exhibit lower and more variable performance across sessions, indicating less consistency compared to MS-iMamba and DLinear. Overall, MS-iMamba demonstrates superior performance across all scenarios, significantly outperforming other models, particularly under inter-session conditions. This consistent and robust performance makes MS-iMamba an exceptional model for tasks requiring high accuracy under different session conditions. DLinear emerges as a strong contender, particularly effective in single-session scenarios, but falls short in terms of generalization compared to MS-iMamba.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "D. Intra-Subject Experiment Results",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "E. Inter-Subject Experiment Results",
      "text": "Table III presents the performance of various models under inter-subject conditions on the DEAP and DREAMER datasets. Compared to the intra-subject paradigm, the intersubject setting poses a greater challenge for model generalization. MS-iMamba consistently outperforms other models in both intra-subject and inter-subject conditions, demonstrating significant robustness and generalization capability. However, due to increased data variability, all models exhibit a performance drop when transitioning from intra-subject to intersubject conditions. Despite its excellent performance in intrasubject scenarios, DLinear shows a notable decline in intersubject settings, highlighting potential limitations in handling data from different subjects. TCN maintains relatively stable performance across both conditions, making it a reliable choice, albeit not the top-performing one. We also conducted inter-subject experiments on the SEED dataset, along with inter-session and intra-session experiments, with results presented in Table  IV . MS-iMamba outperforms the second-best model by 17.85%, 22.48%, 40.43%, and 38.11% in inter-session, session 1, session 2, and session 3, respectively. Although the accuracy of MS-iMamba in the inter-session experiment decreases by 6.5% compared to intra-subject conditions, its performance in intra-session experiments increases, while other models experience significant drops. Interestingly, while the DLinear model performs impressively in intra-subject experiments, it disappoints in inter-subject experiments, displaying the opposite pattern to TCN.\n\nThese results indicate that linear models are only suitable for scenarios with simple data structure distribution. Additionally, despite the multi-scale and inverted spatiotemporal structures used by TimesNet and iTransformer, their performance remains unsatisfactory. Overall, MS-iMamba consistently outperforms other models across nearly all datasets in both intrasubject and inter-subject experiments, showcasing superior robustness and generalization capabilities.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "F. Ablation Study",
      "text": "To validate the effectiveness of each component in MS-iMamba, we conducted ablation experiments using five different configurations, as shown in Table  V . We compared the performance of these five variants under both intra-subject and inter-subject conditions.\n\n1) Intra-subject Results: We visualized the performance of the five variants across three datasets, with the results illustrated in Figures  3  and 4 . Figures  3(a ) and 3(b) show the accuracy of each individual's data on valence and arousal in the DEAP and DREAMER datasets, respectively. From the figures, we observe that the Mamba (green) performs the worst, while the Mamba with MSTB (black) shows slight improvement. However, both are outperformed by the variant using only MSTB (pink). This indicates that MSTB can effectively extract temporal features for emotion classification but does not integrate well with Mamba. The iMamba variant with inverted embedding (red) exhibits significant improvement, closely approaching the performance of MS-iMamba. These results suggest that in intra-subject scenarios on the DEAP and DREAMER datasets, using inverted embedding to consider spatiotemporal interactions is more beneficial than using multiscale features. Figure  4  displays the performance of these variants in four different session modes on the SEED dataset. The results indicate that Mamba shows significant improvement with the addition of MSTB and inverted embedding, with the latter providing a more substantial effect. These findings validate the effectiveness of MSTB and inverted embedding across all three datasets.\n\n2) Inter-subject Results: We evaluated the performance of different MS-iMamba variants in inter-subject scenarios on the three datasets. Table  VI  shows that the combinations Mamba+Multi-Scale and iMamba+Multi-Scale, equipped with MSTB, achieve average accuracy improvements of 1.73% and 4.45%, respectively, compared to their counterparts without MSTB (Mamba and iMamba). The variants with inverted embedding (iMamba and iMamba+Multi-Scale) show average accuracy increases of 18.57% and 21.29%, respectively, compared to the Mamba and Mamba+Multi-Scale variants. The combined use of both mechanisms in iMamba+Multi-Scale (i.e., MS-iMamba) results in average accuracy improvements of 17.65% and 23.02% over the single-use Multi-Scale and Mamba variants. Overall, in inter-subject conditions, using MSTB, inverted embedding, or their combination leads to improved recognition performance on the DEAP, DREAMER, and SEED datasets.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "G. Comparison With State-Of-The-Art Methods",
      "text": "We compared MS-iMamba against state-of-the-art methods, and Table VII presents the average classification accuracies of various models on the DEAP, DREAMER, and SEED datasets. The input feature types include raw data (Raw), power spectral density (PSD), and differential entropy (DE). Among models utilizing feature extraction and all-channel EEG data, EESCN  [53] , V-IAG  [52] , and ATDD-LSTM  [10]  achieved the highest accuracies on the three datasets, with 94.81%, 92.96%, and 91.08%, respectively. TAE  [54] , masking 70% of the data and using the remaining 30%, reached an accuracy of 66.29% on DEAP, while CSGNN  [17] , retaining  only 20% of the channels, achieved accuracies of 83.39% on DEAP and 83.93% on SEED. These models performed poorly under conditions of incomplete data. In contrast, MS-iMamba, without using handcrafted features and relying on just four channels, achieved or exceeded the performance of these models. This demonstrates that our model effectively utilizes limited channel information to achieve high-precision classification. MS-iMamba consistently outperformed the state-ofthe-art models across all datasets, highlighting the advanced nature and efficacy of our feature extraction and classification algorithms.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "V. Discussion",
      "text": "In this study, we designed MS-iMamba for EEG-based emotion recognition, incorporating two main components: Multi-Scale Temporal Blocks (MSTB) and Temporal-Spatial Feature Blocks (TSFB). MSTB and TSFB are utilized to capture multi-scale temporal features and spatiotemporal interactions, respectively. We replaced traditional manual time-frequency feature extraction with MSTB and introduced a novel approach to handle spatiotemporal information. The proposed model was compared with numerous advanced models, demonstrating its effectiveness. This section delves deeper into the discussion.\n\nWe employed three popular public datasets, DEAP, DREAMER and SEED, and used only four-channel EEG signals from the frontal polar region as inputs. This choice was based on two considerations. First, previous research indicates that emotion-related EEG signals are predominantly found in the prefrontal lobe and lateral temporal lobe of the brain  [17] ,  [22] ,  [40] . Second, the frontal polar region has less hair, reducing the likelihood of EEG signal interference from hair. Achieving high recognition accuracy with fewer EEG channels is a valuable exploration. Additionally, manual feature extraction requires specific domain knowledge and can disrupt the temporal characteristics of the original EEG signals, adding to the workload and potentially diminishing the dataset's usability.\n\nWith the rise of deep learning, self-attention mechanisms have garnered attention across various fields. However, our experiments revealed that Transformer-based models did not perform as expected with limited channels. Properly considering spatiotemporal characteristics can not only enhance recognition performance but also improve the interpretability of the EEG's temporal dependencies and spatial topology. Our two plug-and-play modules, MSTB and TSFB, are suited for different scenarios. From the experimental results, TSFB offered more significant benefits than MSTB. In simple data distribution scenarios, MSTB's improvement was minimal, whereas in complex environments, MSTB proved to be a valuable addition. Combining both modules enhances the model's generalization and robustness.\n\nWhile MS-iMamba achieved impressive results using fewer channels, there are still several limitations. For instance, under the same experimental configuration, MS-iMamba's performance in cross-subject and cross-session scenarios was suboptimal. Given the challenges in acquiring large-scale EEG data, predicting unknown subjects' emotional categories using data from a few subjects remains challenging. However, our work suggests a potential method to preserve the data scale in EEG emotion recognition. In the future, we will continue exploring effective use of limited or incomplete data to improve MS-iMamba's performance in complex scenarios.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "This study introduces MS-iMamba, a novel model designed for EEG-based emotion recognition, integrating Multi-Scale Temporal Blocks (MSTB) and Temporal-Spatial Feature Blocks (TSFB). Our approach effectively captures multi-scale temporal features and spatiotemporal interactions, offering a robust alternative to traditional manual feature extraction methods. Comprehensive experiments conducted on three widelyused public datasets, DEAP, DREAMER, and SEED, demonstrate that MS-iMamba outperforms state-of-the-art models and achieves higher classification accuracy with fewer EEG channels.\n\nOur results highlight the model's robustness and generalization capabilities. Notably, the combination of MSTB and TSFB enhances the model's performance, providing significant improvements over individual components. Despite the challenges in cross-subject and cross-session contexts, MS-iMamba's ability to achieve high accuracy with limited data channels underscores its potential for practical applications in real-world settings.\n\nWhile MS-iMamba shows promise, it also faces limitations, particularly in handling the variability inherent in cross-subject and cross-session data. Future research will focus on further optimizing the model to handle these complexities and exploring the use of limited or incomplete data to enhance performance in more challenging scenarios.\n\nIn conclusion, MS-iMamba represents a significant advancement in EEG-based emotion recognition, offering a scalable, high-accuracy solution that balances the need for fewer data channels with robust performance. This work lays a foundation for future exploration in efficient and effective emotion recognition using EEG, with potential applications across various domains requiring precise emotional state detection.",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Architecture of the MS-iMamba network for EEG emotion recogni-",
      "page": 3
    },
    {
      "caption": "Figure 1: , the original EEG",
      "page": 4
    },
    {
      "caption": "Figure 2: Comparison between normal and inverted embedding mechanism.",
      "page": 4
    },
    {
      "caption": "Figure 2: , the conventional embedding method",
      "page": 4
    },
    {
      "caption": "Figure 2: The inverted embedding",
      "page": 5
    },
    {
      "caption": "Figure 3: Performance of different MS-iMamba variants on the DEAP and DREAMER datasets under intra-subject conditions.",
      "page": 8
    },
    {
      "caption": "Figure 4: Performance of different MS-iMamba variants on the SEED dataset across four session modes under intra-subject conditions.",
      "page": 8
    },
    {
      "caption": "Figure 4: displays the performance of these vari-",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "tential\nin the field of brain-computer interfaces. A key challenge",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": ""
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": "density (PSD)\n[4],\n[5], and functional connectivity [6]. While"
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "lies\nin\nextracting\ndiscriminative\nspatiotemporal\nfeatures\nfrom",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": ""
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": "these methods have advanced EEG emotion recognition,\nthey"
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "electroencephalogram (EEG)\nsignals. Existing studies often rely",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": ""
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": "are time-consuming, require extensive domain knowledge, and"
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "on domain-specific time-frequency features and analyze temporal",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": ""
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "dependencies and spatial characteristics separately, neglecting the",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": "often lose valuable temporal\ninformation by compressing long"
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "interaction between local-global relationships and spatiotemporal",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": "time series into single single eigenvalue."
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "dynamics. To address\nthis, we propose a novel network called",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": ""
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": "To address\nthese\nlimitations, deep learning methods have"
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "Multi-Scale\nInverted Mamba\n(MS-iMamba), which consists\nof",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": ""
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": "gained prominence for their end-to-end capabilities. For exam-"
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "Multi-Scale\nTemporal\nBlocks\n(MSTB)\nand\nTemporal-Spatial",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": ""
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": "ple, Cui et al. [7] utilized gated recurrent units combined with"
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "Fusion\nBlocks\n(TSFB).\nSpecifically, MSTBs\nare\ndesigned\nto",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": ""
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "capture\nboth\nlocal\ndetails\nand\nglobal\ntemporal\ndependencies",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": "minimal class confusion for emotion recognition. Feng [8] and"
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "across different\nscale\nsubsequences. The TSFBs,\nimplemented",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": "Li\n[9] integrated attention mechanisms into bidirectional\nlong"
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "with\nan\ninverted Mamba\nstructure,\nfocus\non\nthe\ninteraction",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": ""
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": "short-term memory (LSTM) modules to extract key temporal"
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "between dynamic temporal dependencies and spatial character-",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": ""
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": "features from EEG sequences. Similarly, Du et al. [10] applied"
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "istics. The primary advantage of MS-iMamba lies\nin its ability",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": ""
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": "attention mechanisms with LSTM-generated feature vectors"
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "to leverage reconstructed multi-scale EEG sequences, exploiting",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": ""
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "the\ninteraction between temporal and spatial\nfeatures without",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": "to automatically select appropriate EEG channels for emotion"
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "the need for domain-specific time-frequency feature extraction.",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": "recognition. Other\nstudies have\nframed physiological\nsignal"
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "Experimental\nresults\non\nthe DEAP, DREAMER,\nand\nSEED",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": ""
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": "emotion recognition tasks as sequence-to-sequence multivari-"
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "datasets\ndemonstrate\nthat MS-iMamba\nachieves\nclassification",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": ""
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": "ate time series prediction problems, employing advanced self-"
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "accuracies of 94.86%, 94.94%, and 91.36%,\nrespectively, using",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": ""
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": "attention mechanisms to decompose signals into independent"
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "only\nfour-channel EEG signals,\noutperforming\nstate-of-the-art",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": ""
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "methods.",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": "frequency\nand\ntime-domain\nrepresentations\n[11]. These\nap-"
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": "proaches\neffectively\ncapture\nuseful\ntemporal\ndependencies."
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "Index Terms—Electroencephalogram (EEG), emotion recogni-",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": ""
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": "Given that\nindividual EEG time steps lack semantic meaning"
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "tion, multi-scale, spatiotemporal\nfeature.",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": ""
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": "[12],\nthe\nappropriateness\nof\niterating\nor\ncalculating mutual"
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": "correlations among them is questionable. Inspired by this, we"
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "I.\nINTRODUCTION",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": ""
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": "segment EEG signals\ninto patches of different\nscales, aggre-"
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "for\nenhancing human-",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": "gating time steps\ninto subsequence-level patches\nto enhance"
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "interaction and intelligent systems. Accurately\nE MOTION recognition is pivotal",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": "local details and capture global\nrelationships that single time"
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "identifying emotional states enables systems to respond more",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": "points cannot provide."
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "appropriately to human needs,\nthereby improving interaction",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": "Other deep learning methods have been utilized to construct"
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "naturalness\nand\nefficiency. EEG,\nas\na\nnon-invasive method",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": "spatial\nfeatures\nfrom EEG signals,\nsignificantly\nenhancing"
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "for physiological\nsignal acquisition, offers\nsuperior\ntemporal",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": "emotion recognition accuracy. Typical\nspatial\nfeature extrac-"
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "resolution and continuity compared to other signals like facial",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": "tion methods\ninclude convolutional neural networks\n(CNNs)"
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "expressions or voice, allowing real-time capture of human’s",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": "[13],\n[14]. For\nexample, Li\net\nal\n[15].\nemployed\na\nnovel"
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "emotional\ndynamics.\nIn\npractical\napplications,\nEEG-based",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": "efficient convolutional block to reduce computational burden."
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "emotion recognition can facilitate mental health monitoring",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": "Liu et al.\n[16] proposed a model named 3-D Convolutional"
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "and diagnostic support by detecting abnormal patterns related",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": "Attention Neural Network\n(3DCANN), which\nconsists\nof"
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "to emotional disorders, providing objective indicators for clin-",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": "spatiotemporal\nfeature extraction modules and EEG channel"
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "ical diagnosis [1].",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": "attention weight\nlearning modules. This model\neffectively"
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "Extracting and analyzing discriminative spatiotemporal fea-",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": "captures dynamic\nrelationships between multi-channel EEG"
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "tures from EEG signals is a challenging task due to the brain’s",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": "signals\nand\nthe\ninternal\nspatial\nrelationships within\nthese"
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "complex spatial topology. Traditional approaches often involve",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": "signals. Recent\nstudies have\nshown that graph convolutions"
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "manual extraction of domain-specific time-frequency features",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": "can effectively utilize brain topological structures for emotion"
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": "recognition. Lin\net\nal.\n[17]\ndeveloped\nan\nimproved\ngraph"
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "Xin Zhou and Lijun Yin are with the Department of Computer Science,",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": ""
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": "convolution model combined with dynamic channel selection"
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "State University of New York at Binghamton, Binghamton, NY 13902 USA.",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": ""
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "E-mail:\n(xzhou11,\nlyin)@binghamton.edu",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": "to simulate information transmission in the brain. This model"
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "Dawei Huang and Xiaojiang Peng (Corresponding author)\nare with the",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": ""
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": "combines the advantages of one-dimensional convolution and"
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "College of Big Data\nand Internet, Shenzhen Technology University, Shen-",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": ""
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": "graph convolution, capturing intra-channel and inter-channel"
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "zhen, 518118, China. E-mail: huangdawei2023@email.szu.edu.cn, pengxiao-",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": ""
        },
        {
          "Abstract—EEG-based emotion recognition holds significant po-": "jiang@sztu.edu.cn",
          "such\nas\ndifferential\nentropy\n(DE)\n[2],\n[3],\npower\nspectral": "EEG features and further modeling inter-regional brain con-"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "nectivity by adding functional\nconnectivity. Feng et\nal\n[8].",
          "2": "local details\nand global\nrelationships without\nrequiring"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "introduced a spatial graph convolution module that adaptively",
          "2": "traditional domain-specific time-frequency statistical fea-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "learns\nintrinsic connections between EEG channels using an",
          "2": "ture\nextraction,\nproviding\na\npromising\nperspective\nfor"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "adjacency matrix to extract spatial domain features. Addition-",
          "2": "time dependency modeling in EEG emotion recognition."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ally, researchers like Cui [18] and Deng et al. [19] explored the",
          "2": "• The proposed TSFB reflects on the modeling approach"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "spatial information of adjacent and symmetrical channels from",
          "2": "of\nthe\nspatiotemporal\ncharacteristics\nof\nEEG signals,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the perspective of whether EEG signals exhibit\nsymmetrical",
          "2": "adequately considering the interaction between temporal"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "emotional responses. These studies underscore the importance",
          "2": "dependencies and spatial features, offering a method that"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "of understanding brain topological\nstructures\nin EEG-based",
          "2": "simultaneously integrates\ntemporal\nand spatial\nfeatures"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "emotion recognition tasks.",
          "2": "for EEG emotion recognition."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "The\nbrain’s\ncomplex\nstructure\nresults\nin\nEEG signals",
          "2": "•\nIntra-subject\nand\ninter-subject\nexperiments were\ncon-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "with time-varying spatial\ntopology and temporal dependencies",
          "2": "ducted on three public datasets: DEAP, DREAMER, and"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "recorded\nthrough multiple\nelectrodes.\nIt\nis\nintuitive\nto\nuse",
          "2": "SEED. The\nexperimental\nresults\ndemonstrate\nthat\nthe"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "both temporal dependencies and spatial\nfeatures as auxiliary",
          "2": "proposed MS-iMamba outperforms various\nstate-of-the-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "information. For instance, a novel Attention-based Spatiotem-",
          "2": "art methods using only four-channel EEG data."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "poral Dual-Stream Fusion Network\n(ASTDF-Net)\n[20]\nhas",
          "2": "The\nremaining\nsections\nof\nthis\narticle\nare\norganized\nas"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "been employed to learn a joint\nlatent subspace to capture the",
          "2": "follows. Section II reviews the related work on multi-scale and"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "coupled\nspatiotemporal\ninformation\nin EEG signals. Cheng",
          "2": "spatiotemporal representation learning. Section III presents the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "and\nFeng\net\nal.\nproposed\na\nhybrid model\ncombining\na",
          "2": "pipeline of MS-iMmaba. Section IV details the procedure of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Spatial-Graph Convolutional Network\n(SGCN) module\nand",
          "2": "the conducted experiments and experimental\nresults. A more"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "an attention-enhanced bidirectional Long Short-Term Memory",
          "2": "in-depth discussion is provided in Section V. Finally, the study"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "(LSTM) module\n[8],\nand\nlater\ndesigned\na\nhybrid\nnetwork",
          "2": "is concluded in Section VI."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "comprising a Dynamic Graph Convolution (DGC) module and",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "II. RELATED WORK"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "a Temporal Self-Attention Representation\n(TSAR) module,",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "A. Multi-Scale Representation Learning"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "integrating\nspatial\ntopology\nand\ntemporal\ninformation\n[21].",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Gong et al.\n[22] used a novel Attention-based Convolutional",
          "2": "Representing\ndata\nin\nfine\ngranularity\nhas\nbeen widely"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Transformer Neural Network (ACTNN), effectively integrat-",
          "2": "adopted in time\nseries prediction [12]\nand computer vision"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ing key spatial,\nspectral,\nand temporal\ninformation of EEG",
          "2": "fields [24], [25]. EEG emotion recognition is essentially a time"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "signals\nand cascading CNNs with transformers\nfor\nemotion",
          "2": "series prediction task, and considering an effective sequence"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "recognition tasks. Shen et al. [23] utilized multi-scale temporal",
          "2": "representation approach is necessary. Nie\net\nal.\n[12]\nargued"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "self-attention modules\nto learn temporal continuity informa-",
          "2": "that for time series data, single-point data lacks clear semantic"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "tion while employing dynamic graph convolution modules to",
          "2": "information unlike words, making the computation of single-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "capture spatial functional relationships between different EEG",
          "2": "time-step correlations debatable.\nIn natural\nlanguage process-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "electrodes. Although\nthese\nintegrated models\nconsider\nboth",
          "2": "ing,\nit\nis also more effective to symbolize words in a sentence"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "temporal and spatial\nfeatures,\nthey typically use two separate",
          "2": "rather than each letter [26], [27]. This approach of aggregating"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "branches\nto extract\nspatiotemporal\ninformation,\nlacking ade-",
          "2": "single-point data into patches has been validated effectively in"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "quate interaction between them.",
          "2": "time series prediction tasks [28]. For\ninstance, Wu et al.\n[29]"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Given these challenges,\nthis article proposes a spatiotempo-",
          "2": "addressed the limitations of 1-D time series by segmenting the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ral\nfusion network called Multi-Scale Inverted Mamba (MS-",
          "2": "sequence\ninto short\nand long periods\nrepresentations. These"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "iMamba),\ncombining Multi-Scale Temporal Blocks\n(MSTB)",
          "2": "representations were embedded into the columns and rows of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "and Temporal-Spatial Fusion Blocks\n(TSFB). The\nprimary",
          "2": "a 2-D tensor\nto capture\ninter-\nand intra-periodic variations,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "advantage of the proposed MS-iMamba is its ability to simulta-",
          "2": "respectively, allowing easy modeling of 2-D variations using"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "neously leverage local details and global relationships in EEG",
          "2": "2-D convolutional kernels. Chen et\nal.\n[30] highlighted the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "signals without\nthe need for\ncomplex statistical\nfeature\nex-",
          "2": "difficulty of\ncapturing features\nacross multiple\nscales when"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "traction, enhancing emotion recognition performance through",
          "2": "modeling\ntime\nseries with\nlimited\nor\nfixed\nscales.\nTheir"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "adequate spatiotemporal dependency interactions. Specifically,",
          "2": "proposed Pathformer model achieved multi-scale modeling by"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "MSTB divides EEG signals into multiple scale patches, using",
          "2": "integrating\ntime\nresolution\nand\ntime\ndistance,\ndividing\nthe"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "small-scale patches\nto capture fine\nlocal details\nand coarse",
          "2": "time\nseries\ninto\ndifferent\ntime\nresolutions\nand\nperforming"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "global\nrelationships,\nthereby utilizing complementary predic-",
          "2": "dual\nattention mechanisms\nat\neach\nscale\nto\ncapture\nglobal"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "tive\ncapabilities\nin multi-scale\nobservations. TSFB embeds",
          "2": "correlations and local details as temporal dependencies."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the temporal dimension rather\nthan the spatial dimension of",
          "2": "Multi-scale\nrepresentation learning has\nalso been applied"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "reconstructed multi-scale EEG signals into a token and uses a",
          "2": "to EEG signal\nprocessing. Wang\net\nal.\n[31]\nproposed\nthe"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Selective State-Space Model\n(SSM)\nto model\nspatiotemporal",
          "2": "Multi-Scale Convolutional Neural Network-Dynamic Graph"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "information. This mechanism fully integrates\nthe\nspatiotem-",
          "2": "Convolutional Network (AMCNN-DGCN) model\nto avoid the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "poral dependencies of both modules to enhance EEG emotion",
          "2": "cumbersome manual\nfeature\nextraction process.\nJiang et\nal."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "recognition.",
          "2": "[32]\ndesigned\na\nnovel Attention Mechanism-Based Multi-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "The main contributions of\nthis study are as follows:",
          "2": "Scale\nFeature\nFusion Network\n(AM-MSFFN)\nthat\nconsid-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "• A plug-and-play MSTB is\ndesigned, which\nconsiders",
          "2": "ers\nhigh-level\nfeatures\nat\ndifferent\nscales\nto\nenhance\nthe"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "model’s generalization capability across different subjects. To",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "extract\na\ncomprehensive\nrange of multi-class\nfeatures\nfrom",
          "3": "TSFB"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "Temporal"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "MSTB"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "multi-channel EEG time\nseries\nfor\naccurate\nunderstanding",
          "3": "Feature"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "of\nbrain\nactivity, Li\net\nal.\n[33]\nintroduced\na Multi-Scale",
          "3": "W1\nWk"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "Projection\nProjection"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Attention Mechanism Fusion Convolutional Neural Network",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "..."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "(MS-AMF), which extracts spatiotemporal multi-scale features",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "Convolution"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "from signals representing multiple brain regions and employs",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "a dense fusion strategy to retain maximum information flow.",
          "3": "Activation"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "These studies underscore the importance of multi-scale repre-",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "Selective"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "sentation learning in EEG temporal modeling and demonstrate",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "SSM"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "P1"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "its potential\nin the field of emotion recognition.",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "Pk"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "B. Spatiotemporal Representation Learning",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "F1, W1\n...\nFk, Wk"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "Add"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "In the field of multivariate time series prediction,\nthe fusion",
          "3": "Projection"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "Top K"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "of\nspatiotemporal\nfeatures\nhas\nbecome\na\npopular\nstrategy",
          "3": "Multiply"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "FFT"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "for\nimproving prediction accuracy. Numerous\nscholars have",
          "3": "Activation"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "Linear + Softmax"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "focused\non\neffectively\nintegrating\ntemporal\ncontinuity with",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "EEG"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "spatial correlations [34],\n[35]. Grigsby et al. proposed Space-",
          "3": "Classification"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "timeformer\n[36], which\ntransforms multivariate\ntime\nseries",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "problems\ninto a spatiotemporal\nsequence format.\nIn this ap-",
          "3": "Fig. 1.\nArchitecture of\nthe MS-iMamba network for EEG emotion recogni-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "tion. The network comprises\ntwo main modules:\nthe Multi-Scale Temporal"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "proach,\neach\ninput\ntoken\nrepresents\nthe\nvalue\nof\na\nsingle",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "Block (MSTB) and the Temporal-Spatial Fusion Block (TSFB). The MSTB"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "variable at a given time step, allowing simultaneous learning of",
          "3": "extracts multi-scale representations by converting the EEG signal into different"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "temporal and spatial relationships. Other works have modeled",
          "3": "frequency domain components and reshaping them into 2-D patches. These"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "patches\ncapture\nboth\nlocal\nand\nglobal\ndependencies\nthrough\nconvolution"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "spatiotemporal relationships by transforming one-dimensional",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "operations. The TSFB then integrates\ntemporal\nand spatial\ninformation by"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "or multidimensional sequence data into tensors [37], [38]. Jin",
          "3": "embedding multiple\ntime\nsteps of\nthe\nsame\nchannel\ninto tokens,\nenabling"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "et al.\n[39] demonstrated that\ntraditional methods, which pro-",
          "3": "effective feature extraction through the iMamba module, which combines a"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "reversed embedding mechanism with a selective spatial state model\n(SSM)."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "cess multichannel EEG signals into one-dimensional graphical",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "features,\nlimit\nthe\nexpressive\ncapability of\nemotion recogni-",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "tion models. To address\nthis\nissue,\nthey introduced the G2G",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "reflected on this\nresult,\nsuggesting that\nfor multivariate\nse-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "module, which\ntransforms\none-dimensional\ngraphical\ndata",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "quence data, points on different channels at the same time step"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "into two-dimensional grid data through numerical relationship",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "record entirely different physical meanings or events, making"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "encoding, using deep models like ResNet for subsequent tasks.",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "embedding them into tokens inappropriate. They proposed the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Li\net\nal.\n[40]\nemployed dilated causal\nconvolutional neural",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "Inverted Transformer (iTransformer), which treats independent"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "networks\nto extract nonlinear\nrelationships between different",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "time\nseries\nas\ntokens\nand captures multivariate\ncorrelations"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "time frames and used feature-level\nfusion to merge features",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "through self-attention to leverage\nspatiotemporal mutual\nin-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "from multiple\nchannels,\nexploring\npotential\ncomplementary",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "formation."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "information between different views\nto enhance feature rep-",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "These methods\nshare\na\ncommon\ngoal\nof\nrevealing\nand"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "resentation.",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "utilizing the spatiotemporal\nfeatures of\ntime series data from"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Recently, the integration of attention mechanisms and graph",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "different perspectives\nto achieve higher prediction accuracy."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "neural networks for EEG spatiotemporal modeling has gained",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "Each method has its specific application scenarios and advan-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "increasing attention. Cheng and Feng have conducted exten-",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "tages, but\nthey all highlight\nthe importance of spatiotemporal"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "sive research in this direction. Initially,\nthey proposed a model",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "feature\nfusion\nin\ncurrent\nresearch,\nproviding\na wealth\nof"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "combining\na\nSpatial Graph Convolution Network\n(SGCN)",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "technical options and research directions for the field of EEG"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "module with an attention-enhanced bidirectional Long Short-",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "emotion recognition."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Term Memory (LSTM) module. This model’s main advantage",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "is its consideration of each brain region’s biological\ntopology,",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "III. METHOD"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "extracting representative spatiotemporal features from multiple",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "In this section, we formalize the MS-iMamba network for"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "EEG channels\n[8].\nThey\nsubsequently\ndesigned\na\nhybrid",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "EEG emotion\nrecognition. As\nillustrated\nin\nFigure\n1,\nthe"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "network\ncomprising\na Dynamic Graph Convolution\n(DGC)",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "network\nconsists\nof\ntwo\nprimary modules:\nthe Multi-Scale"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "module and a Temporal Self-Attention Representation (TSAR)",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "Temporal Block\n(MSTB)\nand\nthe Temporal-Spatial Fusion"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "module,\nincorporating\nrepresentative\nknowledge\nof\nspatial",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "Block (TSFB). Each module will be discussed in detail below."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "topology and temporal context\ninto EEG emotion recognition",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "tasks [21]. Recently,\nthey equipped the Dense Graph Convo-",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "A. Notations and Definitions"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "lutional Network (DGC) with Joint Cross-Attention (JCA) for",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "multimodal emotion recognition tasks,\ntermed DG-JCA [41].",
          "3": "Let\nthe EEG signals of each subject be represented by a"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "However, Zeng et al.\n[42] demonstrated that single-layer\nlin-",
          "3": "3-D matrix S ∈ RM ×T ×C, where M , T , and C denote the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ear models unexpectedly outperformed complex Transformer-",
          "3": "number of\ntrials, sampling points, and channels,\nrespectively."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "based models\nin time series prediction tasks. Liu et al.\n[43]",
          "3": "The matrix S is segmented into N samples of length L using"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "a non-overlapping sliding window (thus, T = N × L). The",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "|",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "segmented EEG samples\nare denoted as I = {(Xij, Yij)",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "i = 1, 2, . . . , M ; j = 1, 2, . . . , N }, where Xij ∈ RL×C and",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Yij ∈ R represent the ground-truth label corresponding to Xij.",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "For the same trial, all N segments share the same label. Each",
          "4": "Embedding"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "segmented sample\n:= Xij. The goal of\nis denoted as X1D",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "EEG emotion recognition is to predict Yij given X1D.",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "B. Multi-Scale Temporal Block (MSTB)",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "1) Multi-Scale Representation:\ndenote\nan EEG\nLet X1D",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "signal of\nlength L with C channels. Before representing this",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "signal\nin a multi-scale format, we need to determine the patch",
          "4": "Channels"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "sizes. To achieve this, we transform the original EEG signal",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "into the frequency domain for analysis. Specifically, as shown",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "in Equation 1:",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "into the frequency domain for analysis. Specifically, as shown": "in Equation 1:",
          "Token": ""
        },
        {
          "into the frequency domain for analysis. Specifically, as shown": "",
          "Token": "Time"
        },
        {
          "into the frequency domain for analysis. Specifically, as shown": "A = A (F F T (X1D)) ,",
          "Token": ""
        },
        {
          "into the frequency domain for analysis. Specifically, as shown": "(1)\n{f1, f2, . . . , fk} = argTopk(A),",
          "Token": "Fig. 2.\nComparison between normal\nand inverted embedding mechanism."
        },
        {
          "into the frequency domain for analysis. Specifically, as shown": "",
          "Token": "The\ntop part\nillustrates\nthe\nconventional\nembedding approach, where data"
        },
        {
          "into the frequency domain for analysis. Specifically, as shown": "i ∈ {1, . . . , k},\npi = ⌈L/fi⌉ ,",
          "Token": ""
        },
        {
          "into the frequency domain for analysis. Specifically, as shown": "",
          "Token": "from different channels at\nthe same time step are mapped into a single token."
        },
        {
          "into the frequency domain for analysis. Specifically, as shown": "",
          "Token": "The bottom part depicts the reversed embedding method, where multiple time"
        },
        {
          "into the frequency domain for analysis. Specifically, as shown": "F F T\nwhere\ndenotes\nthe\nFast\nFourier\nTransform (FFT),",
          "Token": "steps of\nthe same channel are mapped into a single token."
        },
        {
          "into the frequency domain for analysis. Specifically, as shown": "and A represents\nthe\namplitude\ncalculation\nfor\neach\nfre-",
          "Token": ""
        },
        {
          "into the frequency domain for analysis. Specifically, as shown": "quency. Since high-frequency regions often contain significant",
          "Token": ""
        },
        {
          "into the frequency domain for analysis. Specifically, as shown": "",
          "Token": "2) Multi-Scale Perception: The reshaped tensors are pro-"
        },
        {
          "into the frequency domain for analysis. Specifically, as shown": "noise, we\nselect only the\ntop k frequencies with the high-",
          "Token": ""
        },
        {
          "into the frequency domain for analysis. Specifically, as shown": "",
          "Token": "cessed\nby\nthe Multi-Scale Perception\n(MSP) module,\nas"
        },
        {
          "into the frequency domain for analysis. Specifically, as shown": "est amplitudes\nto avoid interference\n. The\nselected frequen-",
          "Token": ""
        },
        {
          "into the frequency domain for analysis. Specifically, as shown": "",
          "Token": "shown in Equation 4:"
        },
        {
          "into the frequency domain for analysis. Specifically, as shown": "cies\ncorrespond\nto\nperiods\nand\namplitudes\n{f1, f2, . . . , fk}",
          "Token": ""
        },
        {
          "into the frequency domain for analysis. Specifically, as shown": "respectively. The\n{p1, p2, . . . , pk} and {Af1 , Af2, . . . , Afk },",
          "Token": ""
        },
        {
          "into the frequency domain for analysis. Specifically, as shown": "the patch sizes\nfor\nseg-\nperiods {p1, p2, . . . , pk} are used as",
          "Token": ""
        },
        {
          "into the frequency domain for analysis. Specifically, as shown": "",
          "Token": "X i\n(MSP(X i\ni ∈ {1, . . . , k}."
        },
        {
          "into the frequency domain for analysis. Specifically, as shown": "",
          "Token": "(4)\n2D)),\n1D = Reshape1,pi×fi"
        },
        {
          "into the frequency domain for analysis. Specifically, as shown": "menting the EEG signal.",
          "Token": ""
        },
        {
          "into the frequency domain for analysis. Specifically, as shown": "",
          "Token": "In this module,\nconvolutional kernels of different\nsizes\nare"
        },
        {
          "into the frequency domain for analysis. Specifically, as shown": "As illustrated in the left part of Figure 1,\nthe original EEG",
          "Token": ""
        },
        {
          "into the frequency domain for analysis. Specifically, as shown": "",
          "Token": "employed. This mechanism allows\nthe module to simultane-"
        },
        {
          "into the frequency domain for analysis. Specifically, as shown": "signal\nis\ntransformed into the frequency domain using FFT,",
          "Token": ""
        },
        {
          "into the frequency domain for analysis. Specifically, as shown": "",
          "Token": "ously perceive variations within the\nsame patch and across"
        },
        {
          "into the frequency domain for analysis. Specifically, as shown": "with the red dashed boxes\nindicating the k frequencies with",
          "Token": ""
        },
        {
          "into the frequency domain for analysis. Specifically, as shown": "",
          "Token": "patches with the same phase. After the convolution operations,"
        },
        {
          "into the frequency domain for analysis. Specifically, as shown": "the highest amplitudes. We then calculate the weights for each",
          "Token": ""
        },
        {
          "into the frequency domain for analysis. Specifically, as shown": "",
          "Token": "the 2-D tensors back to the 1-D form X i"
        },
        {
          "into the frequency domain for analysis. Specifically, as shown": "frequency using Equation 2:",
          "Token": "1D. To"
        },
        {
          "into the frequency domain for analysis. Specifically, as shown": "",
          "Token": "assign different\nlevels of attention to features extracted from"
        },
        {
          "into the frequency domain for analysis. Specifically, as shown": "",
          "Token": "patches corresponding to different\nfrequencies, we perform a"
        },
        {
          "into the frequency domain for analysis. Specifically, as shown": "(2)\nWfi = {Wf1, . . . , Wfk } = Softmax(Af1, . . . , Afk ).",
          "Token": "weighted sum of\nthese multi-scale signals to obtain the final"
        },
        {
          "into the frequency domain for analysis. Specifically, as shown": "",
          "Token": "reconstructed multi-scale representation, as shown in Equation"
        },
        {
          "into the frequency domain for analysis. Specifically, as shown": "Next,\nthe\nis\nsegmented into patches of varying\nsignal X1D",
          "Token": ""
        },
        {
          "into the frequency domain for analysis. Specifically, as shown": "",
          "Token": "5:"
        },
        {
          "into the frequency domain for analysis. Specifically, as shown": "sizes and reshaped into a 2-D format, as\nshown in Equation",
          "Token": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "might be at a peak while others are at a trough. Embedding",
          "5": "For parallel processing, Mamba computes the output using the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "them into the\nsame\ntoken not only fails\nto reveal valuable",
          "5": "following convolution form:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "information due to the narrow focus of a single time point but",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "(cid:16)\n(cid:17)\nL−1"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "also represents misaligned events as a single token.",
          "5": "K =\nCB, CAB, . . . , CA\nB\n,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "(10)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Therefore, we\nadopt\nan\ninverted\nembedding method,\nas",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "y = x ∗ K,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "shown in the lower part of Figure 2. The inverted embedding",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "method maps multiple\ntime\nsteps of\nthe\nsame\nchannel\ninto",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "ˆ"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "where L is\nthe\nlength\nof\ninput\nsequence x and\nK is\nthe"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "a\nsingle\ntoken.\nThis\nevent-driven\nrepresentation\nnot\nonly",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "structured convolution kernel."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "considers\ninformation over\nlonger\ntime steps but also distin-",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "Due\nto\nthe\ninverted\nembedding\noperation,\niMamba\ncan"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "guishes data from different channels through separate tokens.",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "extract both temporal and spatial features from the input data,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "The\ninverted\nembedding\nrepresentation\napproach\nenhances",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "fully considering spatiotemporal interactions. iMamba receives"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the\ncapacity\nto\ncapture\ntemporal\ndependencies\nand\nspatial",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "ˆ"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "the input\nX1D ∈ RC×L and produces the output prediction\nYi"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "relationships, ensuring a more comprehensive and meaningful",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "through the following calculation:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "interpretation of\nthe EEG signals for emotion recognition.",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "This method is demonstrated through the following equa-",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "(11)\nYi = f (iMamba( ˆX1D)),"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "tions. Given a multi-scale EEG representation X1D, we reshape",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "it\nto consider\nthe temporal and spatial\ninteractions:",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "where f (·)\nis a linear classifier consisting of a Linear\nlayer"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "and a softmax operation. The cross-entropy loss is computed"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ˆ",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "(6)\nX1D = ReshapeC,L(X1D).",
          "5": "as follows:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ˆ",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Here,\nrepresents\nthat\ntemporal\nsteps\nand\nchannels\nare\nX1D",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "(cid:17)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "1\n,\n(12)\n(cid:16) ˆYi"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "reorganized\nto\nreflect\nthe\ninverted\nembedding\nstructure. To",
          "5": "n(cid:88) i\nLcls = −\n[i= ˆYi] log"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "=1"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "capture the dynamic interactions between temporal and spatial",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ˆ",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "features, we\napply\na\nSSM to\ninversion\nof\nthe\nX1D. This",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "where n denotes the number of categories and 1"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "[i= ˆYi] equals 1"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "embedding representation and the fusion of\ntemporal-spatial",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "if\nthe predicted class matches the true label, and 0 otherwise."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "information\nusing\nSSM enhance\nthe\nability\nto model\nthe",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "Finally,\nthe\nbackpropagation\nalgorithm is\nused\nto\nupdate"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "complex dependencies\nin EEG signals,\nleading to improved",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "the network parameters. The pseudocode of MS-iMamba\nis"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "performance in emotion recognition tasks.",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "summarized in Algorithm 1."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "2)\niMamba:\nAfter\nthe\ninverted\nembedding\noperation,",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "transformed into\nX1D ∈ RL×C is\nX1D ∈ RC×L. Next, we",
          "5": "Algorithm 1 MS-iMamba for EEG Emotion Recognition"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "introduce the iMamba model, which consists of\nthe inverted",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "Require: EEG signal S ∈ RM ×T ×C"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "embedding mechanism and the SSM, specifically Mamba,\nto",
          "5": "ˆ"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "Y\nEnsure: Predicted label"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "capture spatiotemporal correlations.",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "1: Preprocessing:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Mamba\nis\ninspired by continuous\nsystems, mapping a 1-",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "2: Slice S into non-overlapping windows to get samples I ="
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "D sequence\nthrough\na\nhidden\nstate h(t) ∈ RN to x(t) ∈",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "{(Xij, Yij)}"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "R → y(t) ∈ R. As\nshown in Equation 7, Mamba uses\nthree",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "3: Multi-Scale Temporal Block (MSTB):"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "parameter matrices A ∈ Rd×d, B ∈ Rd×1, and C ∈ R1×d",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "to frequency domain using FFT\n4: Transform Xij"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "(where d is\nthe hidden dimension)\nto control\nthis process.",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "5: A = A(FFT(X1D))"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "These parameters are analogous to the forget gate,\ninput gate,",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "6: Select\ntop k frequencies and their periods:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "and\noutput\ngate mechanisms\nin LSTM. The\nparameter A",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "pi = ⌈L/fi⌉\n7: {f1, f2, . . . , fk} = argTopk(A),"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "controls how much information is\nignored, B controls how",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "8: Calculate weights:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the current\ninput affects the hidden state, and C controls the",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "9: Wfi = Softmax(Af1, Af2, . . . , Afk )"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "output flow of\ninformation:",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "10: Reshape X1D into 2-D patches:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "11: X i"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "2D = Reshapepi,fi"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "H ′(t) = Ah(t) + Bx(t),",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "12: Apply multi-scale inception:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "(7)",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "y(t) = Ch(t).",
          "5": "13: X i\n(MSP(X i"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "2D))\n1D = Reshape1,pi×fi"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "14: Combine multi-scale features:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "To adapt to discrete sequences, Mamba uses zero-order hold",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "15: X1D = (cid:80)k\n1D\ni=1 Wfi × X i"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "techniques,\ntransforming A and B into their discrete versions",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "16: Temporal-Spatial Fusion Block (TSFB):"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "via the time scale parameter ∆, as defined below:",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "17: Reverse embedding to reshape\nX1D ∈ RC×L"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "18:\niMamba:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "A = exp(∆A),",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "(8)",
          "5": "19: Apply iMamba to capture spatio-temporal correlation:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "B = (∆A)−1(exp(∆A) − I) · ∆B.",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "20:\nYi = f (iMamba( ˆX1D))"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "21: Calculate cross-entropy loss:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "The discrete version is redefined as follows:",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "22: Lcls = (cid:80)n\ni=1 log( ˆYi) · 1[i = ˆYi]"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "23: Update network parameters using backpropagation."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ht = Aht−1 + Bxt,",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "(9)",
          "5": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "filter. We classified each metric into high and low categories": "using a threshold of 5. To augment\nthe dataset, we segmented",
          "stationary Transformer": "",
          "(NTransformer), and Informer. Addi-": "tionally, we included models characterized by linear structures"
        },
        {
          "filter. We classified each metric into high and low categories": "each signal\ninto 1-second non-overlapping segments.\nIn our",
          "stationary Transformer": "",
          "(NTransformer), and Informer. Addi-": "and causal convolution structures, such as DLinear and TCN."
        },
        {
          "filter. We classified each metric into high and low categories": "experiments, we used only the frontal polar\nregion channels",
          "stationary Transformer": "Below is a brief",
          "(NTransformer), and Informer. Addi-": "introduction to these benchmark models:"
        },
        {
          "filter. We classified each metric into high and low categories": "FP1, FP2, AF3, and AF4.",
          "stationary Transformer": "•",
          "(NTransformer), and Informer. Addi-": "iTransformer [43]:\niTransformer addresses the shortcom-"
        },
        {
          "filter. We classified each metric into high and low categories": "DREAMER [45]: The DREAMER dataset\nalso\ncontains",
          "stationary Transformer": "",
          "(NTransformer), and Informer. Addi-": "ings of\ntraditional Transformers\nin modeling spatiotem-"
        },
        {
          "filter. We classified each metric into high and low categories": "multimodal\ndata\nfrom\n23\nparticipants.\nEach\nparticipant",
          "stationary Transformer": "",
          "(NTransformer), and Informer. Addi-": "poral\ninformation by proposing an inverted Transformer"
        },
        {
          "filter. We classified each metric into high and low categories": "watched\n18\nvideo\nclips\n(ranging\nfrom 65s\nto\n393s, with",
          "stationary Transformer": "",
          "(NTransformer), and Informer. Addi-": "structure\nthat\nbetter\nconsiders\nspatiotemporal\nrelation-"
        },
        {
          "filter. We classified each metric into high and low categories": "an\naverage\nduration\nof\n199s), while\n14-channel EEG and",
          "stationary Transformer": "",
          "(NTransformer), and Informer. Addi-": "ships."
        },
        {
          "filter. We classified each metric into high and low categories": "2-channel Electrocardiograph (ECG)\nsignals were\nrecorded.",
          "stationary Transformer": "• DLinear",
          "(NTransformer), and Informer. Addi-": "[42]: DLinear decomposes sequences into peri-"
        },
        {
          "filter. We classified each metric into high and low categories": "Participants rated valence, arousal, and dominance on a scale",
          "stationary Transformer": "",
          "(NTransformer), and Informer. Addi-": "odic and trend components, achieving impressive results"
        },
        {
          "filter. We classified each metric into high and low categories": "from 1 to 5. The signals were sampled at 128 Hz and filtered",
          "stationary Transformer": "",
          "(NTransformer), and Informer. Addi-": "in various\ntime series\ntasks using a straightforward lin-"
        },
        {
          "filter. We classified each metric into high and low categories": "to 4-45 Hz using a band-pass filter. The EEG signals were",
          "stationary Transformer": "",
          "(NTransformer), and Informer. Addi-": "ear structure, outperforming many complex Transformer"
        },
        {
          "filter. We classified each metric into high and low categories": "then segmented into 1-second non-overlapping segments\nto",
          "stationary Transformer": "",
          "(NTransformer), and Informer. Addi-": "models and their variants."
        },
        {
          "filter. We classified each metric into high and low categories": "expand the dataset. For DREAMER, we used four channels",
          "stationary Transformer": "• TimesNet",
          "(NTransformer), and Informer. Addi-": "[29]: TimesNet employs a multi-scale strategy"
        },
        {
          "filter. We classified each metric into high and low categories": "from the frontal polar and frontal regions: AF3, AF4, F7, and",
          "stationary Transformer": "",
          "(NTransformer), and Informer. Addi-": "to transform time series from 1-D to 2-D format, captur-"
        },
        {
          "filter. We classified each metric into high and low categories": "F8. Each metric was classified into high and low categories",
          "stationary Transformer": "",
          "(NTransformer), and Informer. Addi-": "ing both intra-period and inter-period variations."
        },
        {
          "filter. We classified each metric into high and low categories": "using a threshold of 3.",
          "stationary Transformer": "• NTransformer",
          "(NTransformer), and Informer. Addi-": "[47]: This model\ndesigns\nnon-stationary"
        },
        {
          "filter. We classified each metric into high and low categories": "SEED [46]: The SEED dataset\nincludes data from 15 par-",
          "stationary Transformer": "",
          "(NTransformer), and Informer. Addi-": "attention mechanisms\nto recover\ninherent non-stationary"
        },
        {
          "filter. We classified each metric into high and low categories": "ticipants, with 62-channel EEG data collected according to the",
          "stationary Transformer": "",
          "(NTransformer), and Informer. Addi-": "information in time dependencies through distinguishable"
        },
        {
          "filter. We classified each metric into high and low categories": "international 10-20 system. Each participant conducted three",
          "stationary Transformer": "",
          "(NTransformer), and Informer. Addi-": "attention learned from the raw sequences."
        },
        {
          "filter. We classified each metric into high and low categories": "sessions\napproximately\none week\napart,\nduring which\nthey",
          "stationary Transformer": "•",
          "(NTransformer), and Informer. Addi-": "Informer\n[48]:\nInformer utilizes\nsparse\nattention and a"
        },
        {
          "filter. We classified each metric into high and low categories": "watched 15 different film clips (each lasting about 4 minutes).",
          "stationary Transformer": "",
          "(NTransformer), and Informer. Addi-": "self-distillation mechanism to reduce the computational"
        },
        {
          "filter. We classified each metric into high and low categories": "These films elicited positive, neutral, and negative emotions",
          "stationary Transformer": "",
          "(NTransformer), and Informer. Addi-": "complexity of attention maps to logarithmic levels."
        },
        {
          "filter. We classified each metric into high and low categories": "as experimental\nstimuli. The data were downsampled to 200",
          "stationary Transformer": "",
          "(NTransformer), and Informer. Addi-": "• TCN [49]: TCN introduces the concept of dilated causal"
        },
        {
          "filter. We classified each metric into high and low categories": "Hz and filtered to 0-75 Hz, then segmented into 1-second non-",
          "stationary Transformer": "",
          "(NTransformer), and Informer. Addi-": "convolutions, which are favored for expanding the recep-"
        },
        {
          "filter. We classified each metric into high and low categories": "overlapping segments. Only the frontal polar\nregion channels",
          "stationary Transformer": "",
          "(NTransformer), and Informer. Addi-": "tive field without\nincreasing computational burden."
        },
        {
          "filter. We classified each metric into high and low categories": "FP1, FP2, AF3, and AF4 were selected for our experiments.",
          "stationary Transformer": "",
          "(NTransformer), and Informer. Addi-": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "DATASETS (INTRA-SUBJECT)"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "DEAP"
        },
        {
          "TABLE I": "(arousal)"
        },
        {
          "TABLE I": "81.35%"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "91.89%"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "88.05%"
        },
        {
          "TABLE I": "87.01%"
        },
        {
          "TABLE I": "88.39%"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "89.24%"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "95.03%"
        },
        {
          "TABLE I": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "IV. EXPERIMENT AND RESULTS ANALYSIS",
          "6": "respectively. All experiments were conducted on an Intel Xeon"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "Silver 4210R CPU @ 2.40GHz\n(×2)\nand an NVIDIA RTX"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "A. Datasets",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "A6000 GPU."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "DEAP [44]: The DEAP dataset comprises multimodal data",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "collected from 32 participants. Each participant watched 40",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "C. Baseline Model Selection"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "music\nvideos while\n32-channel\nEEG data\nand\n8-channel",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "peripheral\nphysiological\nsignals were\nrecorded. Participants",
          "6": "For\nour\nbenchmark model\nselection, we\nchose\nseveral"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "rated the videos on a scale from 1 to 9 for valence, arousal,",
          "6": "representative methods\nto compare against our model under"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "dominance,\nand liking. Each video contains 60 seconds of",
          "6": "the\nsame\nexperimental\nsettings. These models\nare\nsourced"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "data (excluding a 3-second baseline signal), which was down-",
          "6": "from the Time Series Library (TSlib1)\nand include\nthe\ntop"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "sampled to 128 Hz\nand filtered using a 4-45 Hz band-pass",
          "6": "three ranked models\nfor classification tasks: TimesNet, Non-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "filter. We classified each metric into high and low categories",
          "6": "stationary Transformer\n(NTransformer), and Informer. Addi-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "using a threshold of 5. To augment\nthe dataset, we segmented",
          "6": "tionally, we included models characterized by linear structures"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "each signal\ninto 1-second non-overlapping segments.\nIn our",
          "6": "and causal convolution structures, such as DLinear and TCN."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "experiments, we used only the frontal polar\nregion channels",
          "6": "Below is a brief\nintroduction to these benchmark models:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "FP1, FP2, AF3, and AF4.",
          "6": "•\niTransformer [43]:\niTransformer addresses the shortcom-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "DREAMER [45]: The DREAMER dataset\nalso\ncontains",
          "6": "ings of\ntraditional Transformers\nin modeling spatiotem-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "multimodal\ndata\nfrom\n23\nparticipants.\nEach\nparticipant",
          "6": "poral\ninformation by proposing an inverted Transformer"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "watched\n18\nvideo\nclips\n(ranging\nfrom 65s\nto\n393s, with",
          "6": "structure\nthat\nbetter\nconsiders\nspatiotemporal\nrelation-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "an\naverage\nduration\nof\n199s), while\n14-channel EEG and",
          "6": "ships."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "2-channel Electrocardiograph (ECG)\nsignals were\nrecorded.",
          "6": "• DLinear\n[42]: DLinear decomposes sequences into peri-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Participants rated valence, arousal, and dominance on a scale",
          "6": "odic and trend components, achieving impressive results"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "from 1 to 5. The signals were sampled at 128 Hz and filtered",
          "6": "in various\ntime series\ntasks using a straightforward lin-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "to 4-45 Hz using a band-pass filter. The EEG signals were",
          "6": "ear structure, outperforming many complex Transformer"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "then segmented into 1-second non-overlapping segments\nto",
          "6": "models and their variants."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "expand the dataset. For DREAMER, we used four channels",
          "6": "• TimesNet\n[29]: TimesNet employs a multi-scale strategy"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "from the frontal polar and frontal regions: AF3, AF4, F7, and",
          "6": "to transform time series from 1-D to 2-D format, captur-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "F8. Each metric was classified into high and low categories",
          "6": "ing both intra-period and inter-period variations."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "using a threshold of 3.",
          "6": "• NTransformer\n[47]: This model\ndesigns\nnon-stationary"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "SEED [46]: The SEED dataset\nincludes data from 15 par-",
          "6": "attention mechanisms\nto recover\ninherent non-stationary"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ticipants, with 62-channel EEG data collected according to the",
          "6": "information in time dependencies through distinguishable"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "international 10-20 system. Each participant conducted three",
          "6": "attention learned from the raw sequences."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "sessions\napproximately\none week\napart,\nduring which\nthey",
          "6": "•\nInformer\n[48]:\nInformer utilizes\nsparse\nattention and a"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "watched 15 different film clips (each lasting about 4 minutes).",
          "6": "self-distillation mechanism to reduce the computational"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "These films elicited positive, neutral, and negative emotions",
          "6": "complexity of attention maps to logarithmic levels."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "as experimental\nstimuli. The data were downsampled to 200",
          "6": "• TCN [49]: TCN introduces the concept of dilated causal"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Hz and filtered to 0-75 Hz, then segmented into 1-second non-",
          "6": "convolutions, which are favored for expanding the recep-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "overlapping segments. Only the frontal polar\nregion channels",
          "6": "tive field without\nincreasing computational burden."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "FP1, FP2, AF3, and AF4 were selected for our experiments.",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Finally, to mitigate data drift across different channels, Z-score",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "D.\nIntra-subject Experiment Results"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "normalization was applied to all\nthree datasets.",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "TABLE I"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "B. Training Protocol",
          "6": "PERFORMANCE COMPARISON OF MODELS ON DEAP AND DREAMER"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "DATASETS (INTRA-SUBJECT)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "In our experiments, we employed two different paradigms:",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "intra-subject and inter-subject paradigm. For\nthe intra-subject",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "Model Name\nDEAP\nDEAP\nDREAMER\nDREAMER"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "paradigm, we\nevaluated each participant’s data\nindividually,",
          "6": "(valence)\n(arousal)\n(valence)\n(arousal)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "iTransformer\n79.10%\n81.35%\n77.60%\n79.78%"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "using 80% for\ntraining and 20% for\ntesting. For\nthe\ninter-",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "95.47%\nDlinear\n90.77%\n91.89%\n93.73%"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "subject paradigm, we combined and shuffled the data from all",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "TimesNet\n87.32%\n88.05%\n84.69%\n88.47%"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "participants, splitting it\ninto training and testing sets in a 4:1",
          "6": "NTransformer\n85.01%\n87.01%\n84.51%\n86.25%"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "Informer\n87.27%\n88.39%\n86.48%\n89.47%"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ratio. Due\nto the SEED dataset\ncomprising data\nfrom three",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "TCN\n88.07%\n89.24%\n84.13%\n88.63%"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "different\nsessions, which\nsignificantly\nimpacts\nexperimental",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "94.69%\n95.03%\n94.54%\n95.34%\nMS-iMamba"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "results, we also used intra-session and inter-session evaluation",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "methods. Our\ntraining\nconfiguration\nincluded\na\nbatch\nsize",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "As shown in Table I, MS-iMamba demonstrates outstanding"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "of 32,\nthe Adam optimizer with an initial\nlearning rate of",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "performance on both the DEAP and DREAMER datasets, sig-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "1 × 10−3,\nand 10 epochs of\ntraining. An adaptive\nlearning",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "nificantly outperforming other models in most metrics. Specif-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "rate strategy was employed to reduce the learning rate as the",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "ically,\nit achieves the highest accuracy in DEAP (valence) at"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "training loss decreased. Other hyperparameters,\nsuch as\nthe",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "number of network layers\nand Top-k, were\nset\nto 1 and 2,",
          "6": "1TSLib: https://github.com/thuml/Time-Series-Library"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "94.69%, DEAP (arousal) at 95.03%, and DREAMER (valence)",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "at 94.54%.\nIt\nalso achieves",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "DREAMER (arousal) at 95.34%, underscoring its robustness",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "and effectiveness",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "DREAMER"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "MS-iMamba\nan\nexcellent",
          "7": "(arousal)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "74.29%"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "high-precision valence and arousal detection from the DEAP",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "78.21%"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "and DREAMER datasets. Notably,",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "75.65%"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "also performs well in this context, second only to MS-iMamba,",
          "7": "75.75%"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "74.82%"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "and\neven\nachieving\nthe",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "79.01%"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "(arousal). Surprisingly, TCN surpasses",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "87.04%"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "based models, while TimesNet performs comparably to them.",
          "7": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TCN": "",
          "75.80%": "",
          "78.87%": "",
          "72.24%": "",
          "79.01%": ""
        },
        {
          "TCN": "MS-iMamba",
          "75.80%": "86.04%",
          "78.87%": "85.94%",
          "72.24%": "81.90%",
          "79.01%": "87.04%"
        },
        {
          "TCN": "",
          "75.80%": "",
          "78.87%": "",
          "72.24%": "",
          "79.01%": ""
        },
        {
          "TCN": "",
          "75.80%": "",
          "78.87%": "",
          "72.24%": "",
          "79.01%": ""
        },
        {
          "TCN": "",
          "75.80%": "",
          "78.87%": "",
          "72.24%": "",
          "79.01%": ""
        },
        {
          "TCN": "both intra-subject and inter-subject conditions, demonstrating",
          "75.80%": "",
          "78.87%": "",
          "72.24%": "",
          "79.01%": ""
        },
        {
          "TCN": "",
          "75.80%": "",
          "78.87%": "",
          "72.24%": "",
          "79.01%": ""
        },
        {
          "TCN": "significant",
          "75.80%": "robustness and generalization capability. However,",
          "78.87%": "",
          "72.24%": "",
          "79.01%": ""
        },
        {
          "TCN": "due\nto\nincreased",
          "75.80%": "data",
          "78.87%": "variability,",
          "72.24%": "all models",
          "79.01%": "exhibit\na"
        },
        {
          "TCN": "formance drop when transitioning from intra-subject",
          "75.80%": "",
          "78.87%": "",
          "72.24%": "",
          "79.01%": ""
        },
        {
          "TCN": "",
          "75.80%": "",
          "78.87%": "",
          "72.24%": "",
          "79.01%": ""
        },
        {
          "TCN": "subject conditions. Despite its excellent performance in intra-",
          "75.80%": "",
          "78.87%": "",
          "72.24%": "",
          "79.01%": ""
        },
        {
          "TCN": "",
          "75.80%": "",
          "78.87%": "",
          "72.24%": "",
          "79.01%": ""
        },
        {
          "TCN": "subject",
          "75.80%": "scenarios, DLinear",
          "78.87%": "",
          "72.24%": "shows a notable decline in inter-",
          "79.01%": ""
        },
        {
          "TCN": "",
          "75.80%": "",
          "78.87%": "",
          "72.24%": "",
          "79.01%": ""
        },
        {
          "TCN": "subject settings, highlighting potential",
          "75.80%": "",
          "78.87%": "",
          "72.24%": "",
          "79.01%": "limitations in handling"
        },
        {
          "TCN": "",
          "75.80%": "",
          "78.87%": "",
          "72.24%": "",
          "79.01%": ""
        },
        {
          "TCN": "data from different",
          "75.80%": "",
          "78.87%": "subjects. TCN maintains",
          "72.24%": "",
          "79.01%": "relatively stable"
        },
        {
          "TCN": "",
          "75.80%": "",
          "78.87%": "",
          "72.24%": "",
          "79.01%": ""
        },
        {
          "TCN": "performance",
          "75.80%": "across\nboth",
          "78.87%": "",
          "72.24%": "conditions, making",
          "79.01%": "it\na"
        },
        {
          "TCN": "choice, albeit not",
          "75.80%": "",
          "78.87%": "the top-performing one.",
          "72.24%": "",
          "79.01%": ""
        },
        {
          "TCN": "",
          "75.80%": "",
          "78.87%": "",
          "72.24%": "",
          "79.01%": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3. MS-iMamba": "",
          "consistently maintains": "",
          "the": "",
          "highest\naccuracy": "",
          "TABLE IV": ""
        },
        {
          "3. MS-iMamba": "in\nall",
          "consistently maintains": "demonstrating\nits",
          "the": "strong",
          "highest\naccuracy": "performance\nand",
          "TABLE IV": ""
        },
        {
          "3. MS-iMamba": "",
          "consistently maintains": "",
          "the": "",
          "highest\naccuracy": "",
          "TABLE IV": ""
        },
        {
          "3. MS-iMamba": "adaptability to various session conditions. The most significant",
          "consistently maintains": "",
          "the": "",
          "highest\naccuracy": "",
          "TABLE IV": ""
        },
        {
          "3. MS-iMamba": "improvement",
          "consistently maintains": "is observed in the inter-session scenario, where",
          "the": "",
          "highest\naccuracy": "",
          "TABLE IV": ""
        },
        {
          "3. MS-iMamba": "",
          "consistently maintains": "",
          "the": "",
          "highest\naccuracy": "",
          "TABLE IV": "Session 1"
        },
        {
          "3. MS-iMamba": "MS-iMamba outperforms",
          "consistently maintains": "",
          "the": "the second-best model by approx-",
          "highest\naccuracy": "",
          "TABLE IV": ""
        },
        {
          "3. MS-iMamba": "",
          "consistently maintains": "",
          "the": "",
          "highest\naccuracy": "",
          "TABLE IV": "48.11%"
        },
        {
          "3. MS-iMamba": "imately",
          "consistently maintains": "substantial",
          "the": "advantage",
          "highest\naccuracy": "highlights MS-",
          "TABLE IV": ""
        },
        {
          "3. MS-iMamba": "",
          "consistently maintains": "",
          "the": "",
          "highest\naccuracy": "",
          "TABLE IV": "44.88%"
        },
        {
          "3. MS-iMamba": "iMamba’s",
          "consistently maintains": "ability\nto",
          "the": "generalize",
          "highest\naccuracy": "across\ndifferent",
          "TABLE IV": "57.88%"
        },
        {
          "3. MS-iMamba": "",
          "consistently maintains": "",
          "the": "",
          "highest\naccuracy": "",
          "TABLE IV": "53.04%"
        },
        {
          "3. MS-iMamba": "session data.",
          "consistently maintains": "In specific session scenarios, MS-iMamba sur-",
          "the": "",
          "highest\naccuracy": "",
          "TABLE IV": ""
        },
        {
          "3. MS-iMamba": "",
          "consistently maintains": "",
          "the": "",
          "highest\naccuracy": "",
          "TABLE IV": "50.60%"
        },
        {
          "3. MS-iMamba": "passes",
          "consistently maintains": "",
          "the": "",
          "highest\naccuracy": "",
          "TABLE IV": ""
        },
        {
          "3. MS-iMamba": "",
          "consistently maintains": "",
          "the": "",
          "highest\naccuracy": "",
          "TABLE IV": "71.23%"
        },
        {
          "3. MS-iMamba": "respectively. DLinear consistently ranks second in sessions 1,",
          "consistently maintains": "",
          "the": "",
          "highest\naccuracy": "",
          "TABLE IV": "93.71%"
        },
        {
          "3. MS-iMamba": "2, and 3,",
          "consistently maintains": "indicating its reliability and effectiveness, although it",
          "the": "",
          "highest\naccuracy": "",
          "TABLE IV": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE V": "CONFIGURATIONS OF MS-IMAMBA VARIANTS USED IN THE ABLATION",
          "results\nindicate\nthat Mamba\nshows\nsignificant\nimprovement": ""
        },
        {
          "TABLE V": "",
          "results\nindicate\nthat Mamba\nshows\nsignificant\nimprovement": "with\nthe\naddition\nof MSTB and\ninverted\nembedding, with"
        },
        {
          "TABLE V": "EXPERIMENTS",
          "results\nindicate\nthat Mamba\nshows\nsignificant\nimprovement": ""
        },
        {
          "TABLE V": "",
          "results\nindicate\nthat Mamba\nshows\nsignificant\nimprovement": "the latter providing a more substantial effect. These findings"
        },
        {
          "TABLE V": "",
          "results\nindicate\nthat Mamba\nshows\nsignificant\nimprovement": "validate the effectiveness of MSTB and inverted embedding"
        },
        {
          "TABLE V": "Variants\nMSTB\nMamba\nInverted Embedding",
          "results\nindicate\nthat Mamba\nshows\nsignificant\nimprovement": ""
        },
        {
          "TABLE V": "",
          "results\nindicate\nthat Mamba\nshows\nsignificant\nimprovement": "across all\nthree datasets."
        },
        {
          "TABLE V": "✓\nMulti-Scale",
          "results\nindicate\nthat Mamba\nshows\nsignificant\nimprovement": ""
        },
        {
          "TABLE V": "✓\nMamba",
          "results\nindicate\nthat Mamba\nshows\nsignificant\nimprovement": "2)\nInter-subject Results: We evaluated the performance of"
        },
        {
          "TABLE V": "✓\n✓",
          "results\nindicate\nthat Mamba\nshows\nsignificant\nimprovement": ""
        },
        {
          "TABLE V": "Multi-Scale+Mamba",
          "results\nindicate\nthat Mamba\nshows\nsignificant\nimprovement": "different MS-iMamba\nvariants\nin\ninter-subject\nscenarios\non"
        },
        {
          "TABLE V": "✓\n✓\niMamba",
          "results\nindicate\nthat Mamba\nshows\nsignificant\nimprovement": ""
        },
        {
          "TABLE V": "",
          "results\nindicate\nthat Mamba\nshows\nsignificant\nimprovement": "the\nthree\ndatasets. Table VI\nshows\nthat\nthe\ncombinations"
        },
        {
          "TABLE V": "✓\n✓\n✓\nMulti-Scale+iMamba",
          "results\nindicate\nthat Mamba\nshows\nsignificant\nimprovement": ""
        },
        {
          "TABLE V": "",
          "results\nindicate\nthat Mamba\nshows\nsignificant\nimprovement": "Mamba+Multi-Scale and iMamba+Multi-Scale, equipped with"
        },
        {
          "TABLE V": "",
          "results\nindicate\nthat Mamba\nshows\nsignificant\nimprovement": "MSTB, achieve average accuracy improvements of 1.73% and"
        },
        {
          "TABLE V": "",
          "results\nindicate\nthat Mamba\nshows\nsignificant\nimprovement": "4.45%,\nrespectively,\ncompared to their\ncounterparts without"
        },
        {
          "TABLE V": "F\n. Ablation Study",
          "results\nindicate\nthat Mamba\nshows\nsignificant\nimprovement": ""
        },
        {
          "TABLE V": "",
          "results\nindicate\nthat Mamba\nshows\nsignificant\nimprovement": "MSTB (Mamba\nand\niMamba). The\nvariants with\ninverted"
        },
        {
          "TABLE V": "",
          "results\nindicate\nthat Mamba\nshows\nsignificant\nimprovement": "embedding (iMamba and iMamba+Multi-Scale) show average"
        },
        {
          "TABLE V": "To validate\nthe\neffectiveness of\neach component\nin MS-",
          "results\nindicate\nthat Mamba\nshows\nsignificant\nimprovement": ""
        },
        {
          "TABLE V": "",
          "results\nindicate\nthat Mamba\nshows\nsignificant\nimprovement": "accuracy increases of 18.57% and 21.29%, respectively, com-"
        },
        {
          "TABLE V": "iMamba, we\nconducted ablation experiments using five dif-",
          "results\nindicate\nthat Mamba\nshows\nsignificant\nimprovement": ""
        },
        {
          "TABLE V": "",
          "results\nindicate\nthat Mamba\nshows\nsignificant\nimprovement": "pared to the Mamba\nand Mamba+Multi-Scale variants. The"
        },
        {
          "TABLE V": "ferent configurations, as shown in Table V. We compared the",
          "results\nindicate\nthat Mamba\nshows\nsignificant\nimprovement": ""
        },
        {
          "TABLE V": "",
          "results\nindicate\nthat Mamba\nshows\nsignificant\nimprovement": "combined use of both mechanisms\nin iMamba+Multi-Scale"
        },
        {
          "TABLE V": "performance of these five variants under both intra-subject and",
          "results\nindicate\nthat Mamba\nshows\nsignificant\nimprovement": ""
        },
        {
          "TABLE V": "",
          "results\nindicate\nthat Mamba\nshows\nsignificant\nimprovement": "(i.e., MS-iMamba)\nresults in average accuracy improvements"
        },
        {
          "TABLE V": "inter-subject conditions.",
          "results\nindicate\nthat Mamba\nshows\nsignificant\nimprovement": ""
        },
        {
          "TABLE V": "",
          "results\nindicate\nthat Mamba\nshows\nsignificant\nimprovement": "of 17.65% and 23.02% over\nthe\nsingle-use Multi-Scale\nand"
        },
        {
          "TABLE V": "1)\nIntra-subject Results: We\nvisualized\nthe\nperformance",
          "results\nindicate\nthat Mamba\nshows\nsignificant\nimprovement": ""
        },
        {
          "TABLE V": "",
          "results\nindicate\nthat Mamba\nshows\nsignificant\nimprovement": "Mamba\nvariants. Overall,\nin\ninter-subject\nconditions,\nusing"
        },
        {
          "TABLE V": "of\nthe\nfive\nvariants\nacross\nthree\ndatasets, with\nthe\nresults",
          "results\nindicate\nthat Mamba\nshows\nsignificant\nimprovement": ""
        },
        {
          "TABLE V": "",
          "results\nindicate\nthat Mamba\nshows\nsignificant\nimprovement": "MSTB,\ninverted\nembedding,\nor\ntheir\ncombination\nleads\nto"
        },
        {
          "TABLE V": "illustrated in Figures 3 and 4. Figures 3(a)\nand 3(b)\nshow",
          "results\nindicate\nthat Mamba\nshows\nsignificant\nimprovement": ""
        },
        {
          "TABLE V": "",
          "results\nindicate\nthat Mamba\nshows\nsignificant\nimprovement": "improved recognition performance on the DEAP, DREAMER,"
        },
        {
          "TABLE V": "the accuracy of each individual’s data on valence and arousal",
          "results\nindicate\nthat Mamba\nshows\nsignificant\nimprovement": ""
        },
        {
          "TABLE V": "",
          "results\nindicate\nthat Mamba\nshows\nsignificant\nimprovement": "and SEED datasets."
        },
        {
          "TABLE V": "in\nthe DEAP and DREAMER datasets,\nrespectively. From",
          "results\nindicate\nthat Mamba\nshows\nsignificant\nimprovement": ""
        },
        {
          "TABLE V": "the figures, we observe that\nthe Mamba (green) performs the",
          "results\nindicate\nthat Mamba\nshows\nsignificant\nimprovement": ""
        },
        {
          "TABLE V": "",
          "results\nindicate\nthat Mamba\nshows\nsignificant\nimprovement": "G. Comparison with State-of-the-Art Methods"
        },
        {
          "TABLE V": "worst, while\nthe Mamba with MSTB (black)\nshows\nslight",
          "results\nindicate\nthat Mamba\nshows\nsignificant\nimprovement": ""
        },
        {
          "TABLE V": "improvement. However, both are outperformed by the variant",
          "results\nindicate\nthat Mamba\nshows\nsignificant\nimprovement": "We compared MS-iMamba against state-of-the-art methods,"
        },
        {
          "TABLE V": "using only MSTB (pink). This indicates that MSTB can effec-",
          "results\nindicate\nthat Mamba\nshows\nsignificant\nimprovement": "and Table VII presents\nthe\naverage\nclassification accuracies"
        },
        {
          "TABLE V": "tively extract\ntemporal\nfeatures for emotion classification but",
          "results\nindicate\nthat Mamba\nshows\nsignificant\nimprovement": "of\nvarious models\non\nthe DEAP, DREAMER,\nand\nSEED"
        },
        {
          "TABLE V": "does not integrate well with Mamba. The iMamba variant with",
          "results\nindicate\nthat Mamba\nshows\nsignificant\nimprovement": "datasets. The\ninput\nfeature\ntypes\ninclude\nraw data\n(Raw),"
        },
        {
          "TABLE V": "inverted\nembedding\n(red)\nexhibits\nsignificant\nimprovement,",
          "results\nindicate\nthat Mamba\nshows\nsignificant\nimprovement": "power\nspectral density (PSD), and differential entropy (DE)."
        },
        {
          "TABLE V": "closely approaching the performance of MS-iMamba. These",
          "results\nindicate\nthat Mamba\nshows\nsignificant\nimprovement": "Among models\nutilizing\nfeature\nextraction\nand\nall-channel"
        },
        {
          "TABLE V": "results suggest that in intra-subject scenarios on the DEAP and",
          "results\nindicate\nthat Mamba\nshows\nsignificant\nimprovement": "EEG data, EESCN [53], V-IAG [52], and ATDD-LSTM [10]"
        },
        {
          "TABLE V": "DREAMER datasets, using inverted embedding to consider",
          "results\nindicate\nthat Mamba\nshows\nsignificant\nimprovement": "achieved the\nhighest\naccuracies\non\nthe\nthree\ndatasets, with"
        },
        {
          "TABLE V": "spatiotemporal interactions is more beneficial than using multi-",
          "results\nindicate\nthat Mamba\nshows\nsignificant\nimprovement": "94.81%, 92.96%, and 91.08%, respectively. TAE [54], masking"
        },
        {
          "TABLE V": "scale features. Figure 4 displays the performance of these vari-",
          "results\nindicate\nthat Mamba\nshows\nsignificant\nimprovement": "70% of\nthe data\nand using the\nremaining 30%,\nreached an"
        },
        {
          "TABLE V": "ants in four different session modes on the SEED dataset. The",
          "results\nindicate\nthat Mamba\nshows\nsignificant\nimprovement": "accuracy of 66.29% on DEAP, while CSGNN [17],\nretaining"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE VI": ""
        },
        {
          "TABLE VI": "DREAMER"
        },
        {
          "TABLE VI": ""
        },
        {
          "TABLE VI": "(arousal)"
        },
        {
          "TABLE VI": "77.84%"
        },
        {
          "TABLE VI": "74.76%"
        },
        {
          "TABLE VI": "74.62%"
        },
        {
          "TABLE VI": "81.21%"
        },
        {
          "TABLE VI": "87.04%"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE VII": "",
          "EEG channels is a valuable exploration. Additionally, manual": ""
        },
        {
          "TABLE VII": "",
          "EEG channels is a valuable exploration. Additionally, manual": "knowledge\nand"
        },
        {
          "TABLE VII": "",
          "EEG channels is a valuable exploration. Additionally, manual": ""
        },
        {
          "TABLE VII": "",
          "EEG channels is a valuable exploration. Additionally, manual": "the original EEG"
        },
        {
          "TABLE VII": "",
          "EEG channels is a valuable exploration. Additionally, manual": "adding to the workload and potentially diminishing"
        },
        {
          "TABLE VII": "Channels",
          "EEG channels is a valuable exploration. Additionally, manual": ""
        },
        {
          "TABLE VII": "",
          "EEG channels is a valuable exploration. Additionally, manual": ""
        },
        {
          "TABLE VII": "All",
          "EEG channels is a valuable exploration. Additionally, manual": ""
        },
        {
          "TABLE VII": "All",
          "EEG channels is a valuable exploration. Additionally, manual": "self-attention mechanisms"
        },
        {
          "TABLE VII": "All",
          "EEG channels is a valuable exploration. Additionally, manual": "have garnered attention across various fields. However, our"
        },
        {
          "TABLE VII": "All",
          "EEG channels is a valuable exploration. Additionally, manual": ""
        },
        {
          "TABLE VII": "",
          "EEG channels is a valuable exploration. Additionally, manual": "revealed that Transformer-based models did not"
        },
        {
          "TABLE VII": "All",
          "EEG channels is a valuable exploration. Additionally, manual": ""
        },
        {
          "TABLE VII": "All",
          "EEG channels is a valuable exploration. Additionally, manual": "Properly\ncon-"
        },
        {
          "TABLE VII": "30%",
          "EEG channels is a valuable exploration. Additionally, manual": "sidering spatiotemporal characteristics can not only enhance"
        },
        {
          "TABLE VII": "All",
          "EEG channels is a valuable exploration. Additionally, manual": ""
        },
        {
          "TABLE VII": "",
          "EEG channels is a valuable exploration. Additionally, manual": "recognition performance but also improve the interpretability"
        },
        {
          "TABLE VII": "20%",
          "EEG channels is a valuable exploration. Additionally, manual": ""
        },
        {
          "TABLE VII": "4",
          "EEG channels is a valuable exploration. Additionally, manual": "spatial\ntopology."
        },
        {
          "TABLE VII": "",
          "EEG channels is a valuable exploration. Additionally, manual": "Our two plug-and-play modules, MSTB and TSFB, are suited"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 2023: [18] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, and X. Chen, “Eeg-",
      "data": [
        {
          "in Guangzhou (2023B03J0172).": ""
        },
        {
          "in Guangzhou (2023B03J0172).": ""
        },
        {
          "in Guangzhou (2023B03J0172).": ""
        },
        {
          "in Guangzhou (2023B03J0172).": ""
        },
        {
          "in Guangzhou (2023B03J0172).": ""
        },
        {
          "in Guangzhou (2023B03J0172).": ""
        },
        {
          "in Guangzhou (2023B03J0172).": ""
        },
        {
          "in Guangzhou (2023B03J0172).": ""
        },
        {
          "in Guangzhou (2023B03J0172).": ""
        },
        {
          "in Guangzhou (2023B03J0172).": ""
        },
        {
          "in Guangzhou (2023B03J0172).": ""
        },
        {
          "in Guangzhou (2023B03J0172).": ""
        },
        {
          "in Guangzhou (2023B03J0172).": ""
        },
        {
          "in Guangzhou (2023B03J0172).": ""
        },
        {
          "in Guangzhou (2023B03J0172).": "[2]"
        },
        {
          "in Guangzhou (2023B03J0172).": ""
        },
        {
          "in Guangzhou (2023B03J0172).": ""
        },
        {
          "in Guangzhou (2023B03J0172).": ""
        },
        {
          "in Guangzhou (2023B03J0172).": ""
        },
        {
          "in Guangzhou (2023B03J0172).": ""
        },
        {
          "in Guangzhou (2023B03J0172).": ""
        },
        {
          "in Guangzhou (2023B03J0172).": ""
        },
        {
          "in Guangzhou (2023B03J0172).": "[3]"
        },
        {
          "in Guangzhou (2023B03J0172).": ""
        },
        {
          "in Guangzhou (2023B03J0172).": ""
        },
        {
          "in Guangzhou (2023B03J0172).": ""
        },
        {
          "in Guangzhou (2023B03J0172).": ""
        },
        {
          "in Guangzhou (2023B03J0172).": ""
        },
        {
          "in Guangzhou (2023B03J0172).": ""
        },
        {
          "in Guangzhou (2023B03J0172).": ""
        },
        {
          "in Guangzhou (2023B03J0172).": ""
        },
        {
          "in Guangzhou (2023B03J0172).": ""
        },
        {
          "in Guangzhou (2023B03J0172).": ""
        },
        {
          "in Guangzhou (2023B03J0172).": "[5] G. Zhang, M. Yu, Y.-J. Liu, G. Zhao, D. Zhang,"
        },
        {
          "in Guangzhou (2023B03J0172).": ""
        },
        {
          "in Guangzhou (2023B03J0172).": ""
        },
        {
          "in Guangzhou (2023B03J0172).": ""
        },
        {
          "in Guangzhou (2023B03J0172).": "[6]"
        },
        {
          "in Guangzhou (2023B03J0172).": ""
        },
        {
          "in Guangzhou (2023B03J0172).": ""
        },
        {
          "in Guangzhou (2023B03J0172).": ""
        },
        {
          "in Guangzhou (2023B03J0172).": "[7] H. Cui, A. Liu, X. Zhang, X. Chen,"
        },
        {
          "in Guangzhou (2023B03J0172).": ""
        },
        {
          "in Guangzhou (2023B03J0172).": ""
        },
        {
          "in Guangzhou (2023B03J0172).": ""
        },
        {
          "in Guangzhou (2023B03J0172).": "[8]"
        },
        {
          "in Guangzhou (2023B03J0172).": ""
        },
        {
          "in Guangzhou (2023B03J0172).": ""
        },
        {
          "in Guangzhou (2023B03J0172).": ""
        },
        {
          "in Guangzhou (2023B03J0172).": ""
        },
        {
          "in Guangzhou (2023B03J0172).": ""
        },
        {
          "in Guangzhou (2023B03J0172).": ""
        },
        {
          "in Guangzhou (2023B03J0172).": ""
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 2023: [18] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, and X. Chen, “Eeg-",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "challenges\nin cross-subject\nand cross-session contexts, MS-",
          "10": "[10] X. Du, C. Ma, G. Zhang, J. Li, Y.-K. Lai, G. Zhao, X. Deng, Y.-J. Liu,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "10": "and H. Wang, “An efficient\nlstm network for emotion recognition from"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "iMamba’s ability to achieve high accuracy with limited data",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "10": "multichannel eeg signals,” IEEE Transactions on Affective Computing,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "channels underscores its potential for practical applications in",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "10": "vol. 13, no. 3, pp. 1528–1540, 2020."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "real-world settings.",
          "10": "[11]\nZ. Yang and H. Cao, “Decompose time and frequency dependencies:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "10": "Multivariate time series physiological signal emotion recognition.”"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "While MS-iMamba shows promise,\nit also faces limitations,",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "10": "[12] Y. Nie, N. H. Nguyen, P. Sinthong, and J. Kalagnanam, “A time series"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "particularly in handling the variability inherent in cross-subject",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "10": "arXiv\nis worth\n64 words: Long-term forecasting with\ntransformers,”"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "and\ncross-session\ndata. Future\nresearch will\nfocus\non\nfur-",
          "10": "preprint arXiv:2211.14730, 2022."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "10": "[13] W. K. Ngai, H. Xie, D. Zou,\nand K.-L. Chou,\n“Emotion recognition"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ther optimizing the model\nto handle\nthese complexities\nand",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "10": "based on convolutional neural networks\nand heterogeneous bio-signal"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "exploring the use of\nlimited or\nincomplete data\nto enhance",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "10": "data sources,” Information Fusion, vol. 77, pp. 107–117, 2022."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "performance in more challenging scenarios.",
          "10": "[14] W. Tao, C. Li, R. Song, J. Cheng, Y. Liu, F. Wan, and X. Chen, “Eeg-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "10": "based emotion recognition via channel-wise attention and self attention,”"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "In conclusion, MS-iMamba represents a significant advance-",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "10": "IEEE Transactions on Affective Computing, vol. 14, no. 1, pp. 382–393,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ment\nin EEG-based emotion recognition, offering a scalable,",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "10": "2020."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "high-accuracy solution that balances\nthe need for\nfewer data",
          "10": "[15] C. Li, X. Lin, Y. Liu, R. Song,\nJ. Cheng,\nand X. Chen,\n“Eeg-based"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "channels with robust performance. This work lays a foundation",
          "10": "emotion\nrecognition\nvia\nefficient\nconvolutional\nneural\nnetwork\nand"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "10": "contrastive learning,” IEEE Sensors Journal, vol. 22, no. 20, pp. 19 608–"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "for future exploration in efficient and effective emotion recog-",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "10": "19 619, 2022."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "nition using EEG, with potential applications across various",
          "10": "[16]\nS. Liu, X. Wang, L. Zhao, B. Li, W. Hu,\nJ. Yu,\nand Y.-D. Zhang,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "domains requiring precise emotional state detection.",
          "10": "“3dcann: A spatio-temporal\nconvolution attention neural network for"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "10": "IEEE Journal\nof Biomedical\nand Health\neeg\nemotion\nrecognition,”"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "10": "Informatics, vol. 26, no. 11, pp. 5321–5331, 2021."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ACKNOWLEDGMENTS",
          "10": "[17] X. Lin, J. Chen, W. Ma, W. Tang, and Y. Wang, “Eeg emotion recog-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "10": "nition using improved graph neural network with channel\nselection,”"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "This work\nis\npartially\nsupported\nby\nthe National Nat-",
          "10": "Computer Methods and Programs in Biomedicine, vol. 231, p. 107380,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "10": "2023."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ural\nScience\nFoundation\nof China\n(62176165),\nthe\nStable",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "10": "[18] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang,\nand X. Chen,\n“Eeg-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Support\nProjects\nfor\nShenzhen Higher\nEducation\nInstitu-",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "10": "based\nemotion\nrecognition\nusing\nan\nend-to-end\nregional-asymmetric"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "tions\n(20220718110918001),\nthe Natural\nScience\nFounda-",
          "10": "convolutional neural network,” Knowledge-Based Systems, vol. 205, p."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "10": "106243, 2020."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "tion\nof\nTop\nTalent\nof\nSZTU (GDRC202131),\nthe\nBasic",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "10": "[19] X. Deng, J. Zhu, and S. Yang, “Sfe-net: Eeg-based emotion recognition"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "and Applied Basic Research Project of Guangdong Province",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "10": "the 29th\nwith symmetrical spatial feature extraction,” in Proceedings of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "(2022B1515130009), and the Special\nsubject on Agriculture",
          "10": "ACM international conference on multimedia, 2021, pp. 2391–2400."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "and Social Development, Key Research and Development Plan",
          "10": "[20]\nP. Gong, Z. Jia, P. Wang, Y. Zhou, and D. Zhang, “Astdf-net: Attention-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "10": "based spatial-temporal dual-stream fusion network for eeg-based emo-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "in Guangzhou (2023B03J0172).",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "10": "the 31st ACM International Con-\ntion recognition,” in Proceedings of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "10": "ference on Multimedia, 2023, pp. 883–892."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "10": "[21] C. Cheng, Z. Yu, Y. Zhang, and L. Feng, “Hybrid network using dynamic"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "REFERENCES",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "10": "graph\nconvolution\nand\ntemporal\nself-attention\nfor\neeg-based\nemotion"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "10": "IEEE Transactions\non Neural Networks\nand\nLearning\nrecognition,”"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[1] M. Jafari, A. Shoeibi, M. Khodatars, S. Bagherzadeh, A. Shalbaf, D. L.",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "10": "Systems, 2023."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Garc´ıa, J. M. Gorriz, and U. R. Acharya, “Emotion recognition in eeg",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "10": "[22]\nL. Gong, M. Li, T. Zhang, and W. Chen, “Eeg emotion recognition using"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "signals using deep learning methods: A review,” Computers in Biology",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "10": "attention-based convolutional\ntransformer neural network,” Biomedical"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "and Medicine, p. 107450, 2023.",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "10": "Signal Processing and Control, vol. 84, p. 104835, 2023."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[2]\nL.-C. Shi, Y.-Y.\nJiao,\nand B.-L. Lu,\n“Differential\nentropy feature\nfor",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "10": "[23]\nL. Shen, M. Sun, Q. Li, B. Li, Z. Pan, and J. Lei, “Multiscale temporal"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "2013\n35th Annual\nInternational\neeg-based\nvigilance\nestimation,”\nin",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "10": "self-attention and dynamical graph convolution hybrid network for eeg-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Conference of\nthe IEEE Engineering in Medicine and Biology Society",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "10": "based stereogram recognition,” IEEE Transactions on Neural Systems"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "(EMBC).\nIEEE, 2013, pp. 6627–6630.",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "10": "and Rehabilitation Engineering, vol. 30, pp. 1191–1202, 2022."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[3]\nT. Song, W. Zheng, P. Song, and Z. Cui, “Eeg emotion recognition using",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "10": "[24] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "dynamical graph convolutional neural networks,” IEEE Transactions on",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "10": "T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al.,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Affective Computing, vol. 11, no. 3, pp. 532–541, 2018.",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "10": "“An image is worth 16x16 words: Transformers for\nimage recognition"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[4] Y.-P. Lin, C.-H. Wang, T.-P. Jung, T.-L. Wu, S.-K. Jeng, J.-R. Duann, and",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "10": "at scale,” arXiv preprint arXiv:2010.11929, 2020."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "J.-H. Chen, “Eeg-based emotion recognition in music listening,” IEEE",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Transactions on Biomedical Engineering, vol. 57, no. 7, pp. 1798–1806,",
          "10": "[25]\nL. Zhu, B. Liao, Q. Zhang, X. Wang, W. Liu, and X. Wang, “Vision"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "2010.",
          "10": "mamba: Efficient visual\nrepresentation learning with bidirectional state"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[5] G. Zhang, M. Yu, Y.-J. Liu, G. Zhao, D. Zhang,\nand W. Zheng,",
          "10": "space model,” arXiv preprint arXiv:2401.09417, 2024."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "“Sparsedgcnn: Recognizing\nemotion\nfrom multichannel\neeg\nsignals,”",
          "10": "[26]\nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "IEEE Transactions on Affective Computing, vol. 14, no. 1, pp. 537–548,",
          "10": "of deep bidirectional\ntransformers\nfor\nlanguage understanding,” arXiv"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "2021.",
          "10": "preprint arXiv:1810.04805, 2018."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[6]\nP. Li, H. Liu, Y. Si, C. Li, F. Li, X. Zhu, X. Huang, Y. Zeng, D. Yao,",
          "10": "[27] M. Schuster and K. Nakajima, “Japanese and korean voice search,” in"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Y\n. Zhang et al., “Eeg based emotion recognition by combining functional",
          "10": "2012 IEEE international\nconference on acoustics,\nspeech and signal"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "IEEE\nTransactions\non\nconnectivity\nnetwork\nand\nlocal\nactivations,”",
          "10": "processing (ICASSP).\nIEEE, 2012, pp. 5149–5152."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Biomedical Engineering, vol. 66, no. 10, pp. 2869–2881, 2019.",
          "10": "[28]\nS. Wang, H. Wu, X. Shi, T. Hu, H. Luo, L. Ma,\nJ. Y. Zhang,\nand"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[7] H. Cui, A. Liu, X. Zhang, X. Chen,\nJ. Liu, and X. Chen, “Eeg-based",
          "10": "J. Zhou, “Timemixer: Decomposable multiscale mixing for\ntime series"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "subject-independent emotion recognition using gated recurrent unit and",
          "10": "forecasting,” arXiv preprint arXiv:2405.14616, 2024."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "minimum class confusion,” IEEE Transactions on Affective Computing,",
          "10": "[29] H. Wu, T. Hu, Y. Liu, H. Zhou,\nJ. Wang,\nand M. Long,\n“Timesnet:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "vol. 14, no. 4, pp. 2740–2750, 2022.",
          "10": "Temporal 2d-variation modeling for general\ntime series analysis,” arXiv"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[8]\nL. Feng, C. Cheng, M. Zhao, H. Deng,\nand Y. Zhang,\n“Eeg-based",
          "10": "preprint arXiv:2210.02186, 2022."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "emotion\nrecognition\nusing\nspatial-temporal\ngraph\nconvolutional\nlstm",
          "10": "[30]\nP. Chen, Y. Zhang, Y. Cheng, Y. Shu, Y. Wang, Q. Wen, B. Yang, and"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "IEEE Journal of Biomedical and Health\nwith attention mechanism,”",
          "10": "C. Guo, “Pathformer: Multi-scale transformers with adaptive pathways"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Informatics, vol. 26, no. 11, pp. 5406–5417, 2022.",
          "10": "for\ntime series forecasting,” arXiv preprint arXiv:2402.05956, 2024."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[9] C. Li, Z. Bao, L. Li, and Z. Zhao, “Exploring temporal\nrepresentations",
          "10": "[31] H. Wang, L. Xu, A. Bezerianos, C. Chen,\nand Z. Zhang,\n“Linking"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "by leveraging attention-based bidirectional\nlstm-rnns\nfor multi-modal",
          "10": "attention-based multiscale cnn with dynamical gcn for driving fatigue"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "emotion recognition,” Information Processing & Management, vol. 57,",
          "10": "IEEE Transactions\non\nInstrumentation\ndetection,”\nand Measurement,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "no. 3, p. 102185, 2020.",
          "10": "vol. 70, pp. 1–11, 2020."
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[32] Y.\nJiang, S. Xie, X. Xie, Y. Cui, and H. Tang, “Emotion recognition",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "via multiscale feature fusion network and attention mechanism,” IEEE",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Sensors Journal, vol. 23, no. 10, pp. 10 790–10 800, 2023.",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[33] D. Li,\nJ. Xu,\nJ. Wang, X.\nFang,\nand Y.\nJi,\n“A multi-scale\nfusion",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "convolutional\nneural\nnetwork\nbased\non\nattention mechanism for\nthe",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "visualization analysis of eeg signals decoding,” IEEE Transactions on",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Neural Systems and Rehabilitation Engineering, vol. 28, no. 12, pp.",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "2615–2626, 2020.",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[34]\nL. Bai, L. Yao, C. Li, X. Wang,\nand C. Wang,\n“Adaptive\ngraph",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "in\nconvolutional\nrecurrent\nnetwork\nfor\ntraffic\nforecasting,” Advances",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "neural information processing systems, vol. 33, pp. 17 804–17 815, 2020.",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[35] N.\nSesti,\nJ.\nJ. Garau-Luis,\nE.\nCrawley,\nand\nB.\nCameron,\n“Inte-",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "arXiv\npreprint\ngrating\nlstms\nand\ngnns\nfor\ncovid-19\nforecasting,”",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "arXiv:2108.10052, 2021.",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[36]\nJ. Grigsby,\nZ. Wang, N. Nguyen,\nand Y. Qi,\n“Long-range\ntrans-",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "arXiv\npreprint\nformers\nfor\ndynamic\nspatiotemporal\nforecasting,”",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "arXiv:2109.12218, 2021.",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[37] X. Chen and L. Sun, “Low-rank autoregressive tensor completion for",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "multivariate time series forecasting,” arXiv preprint arXiv:2006.10436,",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "2020.",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[38] A. Sharma and D. Kumar, “Classification with 2-d convolutional neural",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "networks for breast cancer diagnosis,” Scientific Reports, vol. 12, no. 1,",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "p. 21857, 2022.",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[39] M.\nJin and J. Li,\n“Graph to grid: Learning deep representations\nfor",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "of\nthe\n31st ACM\nmultimodal\nemotion\nrecognition,”\nin Proceedings",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "International Conference on Multimedia, 2023, pp. 5985–5993.",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[40] C.\nLi, N. Bian,\nZ.\nZhao, H. Wang,\nand B. W.\nSchuller,\n“Multi-",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "view domain-adaptive\nrepresentation\nlearning\nfor\neeg-based\nemotion",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "recognition,” Information Fusion, vol. 104, p. 102156, 2024.",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[41] C. Cheng, W. Liu, L. Feng, and Z. Jia, “Dense graph convolutional with",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "joint cross-attention network for multimodal emotion recognition,” IEEE",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Transactions on Computational Social Systems, 2024.",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[42] A. Zeng, M. Chen, L. Zhang, and Q. Xu, “Are transformers effective",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the AAAI conference on\nfor\ntime series forecasting?” in Proceedings of",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "artificial\nintelligence, vol. 37, no. 9, 2023, pp. 11 121–11 128.",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[43] Y. Liu, T. Hu, H. Zhang, H. Wu, S. Wang, L. Ma, and M. Long, “itrans-",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "former:\nInverted transformers are effective for\ntime series forecasting,”",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "arXiv preprint arXiv:2310.06625, 2023.",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[44]\nS. Koelstra, C. Muhl, M. Soleymani, J.-S. Lee, A. Yazdani, T. Ebrahimi,",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "T. Pun, A. Nijholt, and I. Patras, “Deap: A database for emotion analysis;",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "using physiological signals,” IEEE transactions on affective computing,",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "vol. 3, no. 1, pp. 18–31, 2011.",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[45]\nS. Katsigiannis\nand N. Ramzan,\n“Dreamer: A database\nfor\nemotion",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "recognition through eeg and ecg signals\nfrom wireless\nlow-cost off-",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the-shelf devices,” IEEE journal of biomedical and health informatics,",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "vol. 22, no. 1, pp. 98–107, 2017.",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[46] W.-L. Zheng and B.-L. Lu, “Investigating critical\nfrequency bands and",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "channels for eeg-based emotion recognition with deep neural networks,”",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "IEEE Transactions on autonomous mental development, vol. 7, no. 3,",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "pp. 162–175, 2015.",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[47] Y. Liu, H. Wu,\nJ. Wang, and M. Long, “Non-stationary transformers:",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Exploring the stationarity in time series forecasting,” Advances in Neural",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Information Processing Systems, vol. 35, pp. 9881–9893, 2022.",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[48] H. Zhou, S. Zhang, J. Peng, S. Zhang, J. Li, H. Xiong, and W. Zhang,",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "“Informer: Beyond efficient\ntransformer\nfor\nlong sequence time-series",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the AAAI conference on artificial\nintel-\nforecasting,” in Proceedings of",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ligence, vol. 35, no. 12, 2021, pp. 11 106–11 115.",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[49]\nS. Bai, J. Z. Kolter, and V. Koltun, “An empirical evaluation of generic",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "arXiv\nconvolutional\nand\nrecurrent\nnetworks\nfor\nsequence modeling,”",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "preprint arXiv:1803.01271, 2018.",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[50] C. Li, Y. Hou, R. Song, J. Cheng, Y. Liu, and X. Chen, “Multi-channel",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "eeg-based emotion recognition in the presence of noisy labels,” Science",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "China Information Sciences, vol. 65, no. 4, p. 140405, 2022.",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[51]\nT. Song, S. Liu, W. Zheng, Y. Zong, and Z. Cui, “Instance-adaptive graph",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the AAAI Conference\nfor eeg emotion recognition,” in Proceedings of",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "on Artificial\nIntelligence, vol. 34, no. 03, 2020, pp. 2701–2708.",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[52]\nT. Song, S. Liu, W. Zheng, Y. Zong, Z. Cui, Y. Li,\nand X. Zhou,",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "“Variational\ninstance-adaptive graph for eeg emotion recognition,” IEEE",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Transactions on Affective Computing, vol. 14, no. 1, pp. 343–356, 2021.",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[53]\nF. Xu, D. Pan, H. Zheng, Y. Ouyang, Z.\nJia,\nand H. Zeng,\n“Eescn:",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "A novel\nspiking neural network method for eeg-based emotion recog-",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "nition,” Computer methods and programs\nin biomedicine, vol. 243, p.",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "107927, 2024.",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[54] C. Cheng, W. Liu, Z. Fan, L. Feng, and Z.\nJia, “A novel\ntransformer",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "autoencoder for multi-modal emotion recognition with incomplete data,”",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Neural Networks, vol. 172, p. 106111, 2024.",
          "11": ""
        }
      ],
      "page": 11
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion recognition in eeg signals using deep learning methods: A review",
      "authors": [
        "M Jafari",
        "A Shoeibi",
        "M Khodatars",
        "S Bagherzadeh",
        "A Shalbaf",
        "D García",
        "J Gorriz",
        "U Acharya"
      ],
      "year": "2023",
      "venue": "Computers in Biology and Medicine"
    },
    {
      "citation_id": "2",
      "title": "Differential entropy feature for eeg-based vigilance estimation",
      "authors": [
        "L.-C Shi",
        "Y.-Y Jiao",
        "B.-L Lu"
      ],
      "year": "2013",
      "venue": "2013 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)"
    },
    {
      "citation_id": "3",
      "title": "Eeg emotion recognition using dynamical graph convolutional neural networks",
      "authors": [
        "T Song",
        "W Zheng",
        "P Song",
        "Z Cui"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "4",
      "title": "Eeg-based emotion recognition in music listening",
      "authors": [
        "Y.-P Lin",
        "C.-H Wang",
        "T.-P Jung",
        "T.-L Wu",
        "S.-K Jeng",
        "J.-R Duann",
        "J.-H Chen"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "5",
      "title": "Sparsedgcnn: Recognizing emotion from multichannel eeg signals",
      "authors": [
        "G Zhang",
        "M Yu",
        "Y.-J Liu",
        "G Zhao",
        "D Zhang",
        "W Zheng"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "6",
      "title": "Eeg based emotion recognition by combining functional connectivity network and local activations",
      "authors": [
        "P Li",
        "H Liu",
        "Y Si",
        "C Li",
        "F Li",
        "X Zhu",
        "X Huang",
        "Y Zeng",
        "D Yao",
        "Y Zhang"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "7",
      "title": "Eeg-based subject-independent emotion recognition using gated recurrent unit and minimum class confusion",
      "authors": [
        "H Cui",
        "A Liu",
        "X Zhang",
        "X Chen",
        "J Liu",
        "X Chen"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "8",
      "title": "Eeg-based emotion recognition using spatial-temporal graph convolutional lstm with attention mechanism",
      "authors": [
        "L Feng",
        "C Cheng",
        "M Zhao",
        "H Deng",
        "Y Zhang"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "9",
      "title": "Exploring temporal representations by leveraging attention-based bidirectional lstm-rnns for multi-modal emotion recognition",
      "authors": [
        "C Li",
        "Z Bao",
        "L Li",
        "Z Zhao"
      ],
      "year": "2020",
      "venue": "Information Processing & Management"
    },
    {
      "citation_id": "10",
      "title": "An efficient lstm network for emotion recognition from multichannel eeg signals",
      "authors": [
        "X Du",
        "C Ma",
        "G Zhang",
        "J Li",
        "Y.-K Lai",
        "G Zhao",
        "X Deng",
        "Y.-J Liu",
        "H Wang"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "11",
      "title": "Decompose time and frequency dependencies: Multivariate time series physiological signal emotion recognition",
      "authors": [
        "Z Yang",
        "H Cao"
      ],
      "venue": "Decompose time and frequency dependencies: Multivariate time series physiological signal emotion recognition"
    },
    {
      "citation_id": "12",
      "title": "A time series is worth 64 words: Long-term forecasting with transformers",
      "authors": [
        "Y Nie",
        "N Nguyen",
        "P Sinthong",
        "J Kalagnanam"
      ],
      "year": "2022",
      "venue": "A time series is worth 64 words: Long-term forecasting with transformers",
      "arxiv": "arXiv:2211.14730"
    },
    {
      "citation_id": "13",
      "title": "Emotion recognition based on convolutional neural networks and heterogeneous bio-signal data sources",
      "authors": [
        "W Ngai",
        "H Xie",
        "D Zou",
        "K.-L Chou"
      ],
      "year": "2022",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "14",
      "title": "Eegbased emotion recognition via channel-wise attention and self attention",
      "authors": [
        "W Tao",
        "C Li",
        "R Song",
        "J Cheng",
        "Y Liu",
        "F Wan",
        "X Chen"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "15",
      "title": "Eeg-based emotion recognition via efficient convolutional neural network and contrastive learning",
      "authors": [
        "C Li",
        "X Lin",
        "Y Liu",
        "R Song",
        "J Cheng",
        "X Chen"
      ],
      "year": "2022",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "16",
      "title": "3dcann: A spatio-temporal convolution attention neural network for eeg emotion recognition",
      "authors": [
        "S Liu",
        "X Wang",
        "L Zhao",
        "B Li",
        "W Hu",
        "J Yu",
        "Y.-D Zhang"
      ],
      "year": "2021",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "17",
      "title": "Eeg emotion recognition using improved graph neural network with channel selection",
      "authors": [
        "X Lin",
        "J Chen",
        "W Ma",
        "W Tang",
        "Y Wang"
      ],
      "year": "2023",
      "venue": "Computer Methods and Programs in Biomedicine"
    },
    {
      "citation_id": "18",
      "title": "Eegbased emotion recognition using an end-to-end regional-asymmetric convolutional neural network",
      "authors": [
        "H Cui",
        "A Liu",
        "X Zhang",
        "X Chen",
        "K Wang",
        "X Chen"
      ],
      "year": "2020",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "19",
      "title": "Sfe-net: Eeg-based emotion recognition with symmetrical spatial feature extraction",
      "authors": [
        "X Deng",
        "J Zhu",
        "S Yang"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th ACM international conference on multimedia"
    },
    {
      "citation_id": "20",
      "title": "Astdf-net: Attentionbased spatial-temporal dual-stream fusion network for eeg-based emotion recognition",
      "authors": [
        "P Gong",
        "Z Jia",
        "P Wang",
        "Y Zhou",
        "D Zhang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "21",
      "title": "Hybrid network using dynamic graph convolution and temporal self-attention for eeg-based emotion recognition",
      "authors": [
        "C Cheng",
        "Z Yu",
        "Y Zhang",
        "L Feng"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "22",
      "title": "Eeg emotion recognition using attention-based convolutional transformer neural network",
      "authors": [
        "L Gong",
        "M Li",
        "T Zhang",
        "W Chen"
      ],
      "year": "2023",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "23",
      "title": "Multiscale temporal self-attention and dynamical graph convolution hybrid network for eegbased stereogram recognition",
      "authors": [
        "L Shen",
        "M Sun",
        "Q Li",
        "B Li",
        "Z Pan",
        "J Lei"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "24",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly"
      ],
      "year": "2020",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "arxiv": "arXiv:2010.11929"
    },
    {
      "citation_id": "25",
      "title": "Vision mamba: Efficient visual representation learning with bidirectional state space model",
      "authors": [
        "L Zhu",
        "B Liao",
        "Q Zhang",
        "X Wang",
        "W Liu",
        "X Wang"
      ],
      "year": "2024",
      "venue": "Vision mamba: Efficient visual representation learning with bidirectional state space model",
      "arxiv": "arXiv:2401.09417"
    },
    {
      "citation_id": "26",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2018",
      "venue": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "27",
      "title": "Japanese and korean voice search",
      "authors": [
        "M Schuster",
        "K Nakajima"
      ],
      "year": "2012",
      "venue": "2012 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "28",
      "title": "Timemixer: Decomposable multiscale mixing for time series forecasting",
      "authors": [
        "S Wang",
        "H Wu",
        "X Shi",
        "T Hu",
        "H Luo",
        "L Ma",
        "J Zhang",
        "J Zhou"
      ],
      "year": "2024",
      "venue": "Timemixer: Decomposable multiscale mixing for time series forecasting",
      "arxiv": "arXiv:2405.14616"
    },
    {
      "citation_id": "29",
      "title": "Timesnet: Temporal 2d-variation modeling for general time series analysis",
      "authors": [
        "H Wu",
        "T Hu",
        "Y Liu",
        "H Zhou",
        "J Wang",
        "M Long"
      ],
      "year": "2022",
      "venue": "Timesnet: Temporal 2d-variation modeling for general time series analysis",
      "arxiv": "arXiv:2210.02186"
    },
    {
      "citation_id": "30",
      "title": "Pathformer: Multi-scale transformers with adaptive pathways for time series forecasting",
      "authors": [
        "P Chen",
        "Y Zhang",
        "Y Cheng",
        "Y Shu",
        "Y Wang",
        "Q Wen",
        "B Yang",
        "C Guo"
      ],
      "year": "2024",
      "venue": "Pathformer: Multi-scale transformers with adaptive pathways for time series forecasting",
      "arxiv": "arXiv:2402.05956"
    },
    {
      "citation_id": "31",
      "title": "Linking attention-based multiscale cnn with dynamical gcn for driving fatigue detection",
      "authors": [
        "H Wang",
        "L Xu",
        "A Bezerianos",
        "C Chen",
        "Z Zhang"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "32",
      "title": "Emotion recognition via multiscale feature fusion network and attention mechanism",
      "authors": [
        "Y Jiang",
        "S Xie",
        "X Xie",
        "Y Cui",
        "H Tang"
      ],
      "year": "2023",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "33",
      "title": "A multi-scale fusion convolutional neural network based on attention mechanism for the visualization analysis of eeg signals decoding",
      "authors": [
        "D Li",
        "J Xu",
        "J Wang",
        "X Fang",
        "Y Ji"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "34",
      "title": "Adaptive graph convolutional recurrent network for traffic forecasting",
      "authors": [
        "L Bai",
        "L Yao",
        "C Li",
        "X Wang",
        "C Wang"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "35",
      "title": "Integrating lstms and gnns for covid-19 forecasting",
      "authors": [
        "N Sesti",
        "J Garau-Luis",
        "E Crawley",
        "B Cameron"
      ],
      "year": "2021",
      "venue": "Integrating lstms and gnns for covid-19 forecasting",
      "arxiv": "arXiv:2108.10052"
    },
    {
      "citation_id": "36",
      "title": "Long-range transformers for dynamic spatiotemporal forecasting",
      "authors": [
        "J Grigsby",
        "Z Wang",
        "N Nguyen",
        "Y Qi"
      ],
      "year": "2021",
      "venue": "Long-range transformers for dynamic spatiotemporal forecasting",
      "arxiv": "arXiv:2109.12218"
    },
    {
      "citation_id": "37",
      "title": "Low-rank autoregressive tensor completion for multivariate time series forecasting",
      "authors": [
        "X Chen",
        "L Sun"
      ],
      "year": "2020",
      "venue": "Low-rank autoregressive tensor completion for multivariate time series forecasting",
      "arxiv": "arXiv:2006.10436"
    },
    {
      "citation_id": "38",
      "title": "Classification with 2-d convolutional neural networks for breast cancer diagnosis",
      "authors": [
        "A Sharma",
        "D Kumar"
      ],
      "year": "2022",
      "venue": "Scientific Reports"
    },
    {
      "citation_id": "39",
      "title": "Graph to grid: Learning deep representations for multimodal emotion recognition",
      "authors": [
        "M Jin",
        "J Li"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "40",
      "title": "Multiview domain-adaptive representation learning for eeg-based emotion recognition",
      "authors": [
        "C Li",
        "N Bian",
        "Z Zhao",
        "H Wang",
        "B Schuller"
      ],
      "year": "2024",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "41",
      "title": "Dense graph convolutional with joint cross-attention network for multimodal emotion recognition",
      "authors": [
        "C Cheng",
        "W Liu",
        "L Feng",
        "Z Jia"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Computational Social Systems"
    },
    {
      "citation_id": "42",
      "title": "Are transformers effective for time series forecasting",
      "authors": [
        "A Zeng",
        "M Chen",
        "L Zhang",
        "Q Xu"
      ],
      "year": "2023",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "43",
      "title": "itransformer: Inverted transformers are effective for time series forecasting",
      "authors": [
        "Y Liu",
        "T Hu",
        "H Zhang",
        "H Wu",
        "S Wang",
        "L Ma",
        "M Long"
      ],
      "year": "2023",
      "venue": "itransformer: Inverted transformers are effective for time series forecasting",
      "arxiv": "arXiv:2310.06625"
    },
    {
      "citation_id": "44",
      "title": "Deap: A database for emotion analysis; using physiological signals",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani",
        "J.-S Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "45",
      "title": "Dreamer: A database for emotion recognition through eeg and ecg signals from wireless low-cost offthe-shelf devices",
      "authors": [
        "S Katsigiannis",
        "N Ramzan"
      ],
      "year": "2017",
      "venue": "IEEE journal of biomedical and health informatics"
    },
    {
      "citation_id": "46",
      "title": "Investigating critical frequency bands and channels for eeg-based emotion recognition with deep neural networks",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on autonomous mental development"
    },
    {
      "citation_id": "47",
      "title": "Non-stationary transformers: Exploring the stationarity in time series forecasting",
      "authors": [
        "Y Liu",
        "H Wu",
        "J Wang",
        "M Long"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "48",
      "title": "Informer: Beyond efficient transformer for long sequence time-series forecasting",
      "authors": [
        "H Zhou",
        "S Zhang",
        "J Peng",
        "S Zhang",
        "J Li",
        "H Xiong",
        "W Zhang"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "49",
      "title": "An empirical evaluation of generic convolutional and recurrent networks for sequence modeling",
      "authors": [
        "S Bai",
        "J Kolter",
        "V Koltun"
      ],
      "year": "2018",
      "venue": "An empirical evaluation of generic convolutional and recurrent networks for sequence modeling",
      "arxiv": "arXiv:1803.01271"
    },
    {
      "citation_id": "50",
      "title": "Multi-channel eeg-based emotion recognition in the presence of noisy labels",
      "authors": [
        "C Li",
        "Y Hou",
        "R Song",
        "J Cheng",
        "Y Liu",
        "X Chen"
      ],
      "year": "2022",
      "venue": "Science China Information Sciences"
    },
    {
      "citation_id": "51",
      "title": "Instance-adaptive graph for eeg emotion recognition",
      "authors": [
        "T Song",
        "S Liu",
        "W Zheng",
        "Y Zong",
        "Z Cui"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "52",
      "title": "Variational instance-adaptive graph for eeg emotion recognition",
      "authors": [
        "T Song",
        "S Liu",
        "W Zheng",
        "Y Zong",
        "Z Cui",
        "Y Li",
        "X Zhou"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "53",
      "title": "Eescn: A novel spiking neural network method for eeg-based emotion recognition",
      "authors": [
        "F Xu",
        "D Pan",
        "H Zheng",
        "Y Ouyang",
        "Z Jia",
        "H Zeng"
      ],
      "year": "2024",
      "venue": "Computer methods and programs in biomedicine"
    },
    {
      "citation_id": "54",
      "title": "A novel transformer autoencoder for multi-modal emotion recognition with incomplete data",
      "authors": [
        "C Cheng",
        "W Liu",
        "Z Fan",
        "L Feng",
        "Z Jia"
      ],
      "year": "2024",
      "venue": "Neural Networks"
    }
  ]
}