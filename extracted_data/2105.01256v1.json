{
  "paper_id": "2105.01256v1",
  "title": "Self-Supervised Approach For Facial Movement Based Optical Flow",
  "published": "2021-05-04T02:38:11Z",
  "authors": [
    "Muhannad Alkaddour",
    "Usman Tariq",
    "Abhinav Dhall"
  ],
  "keywords": [
    "Optical flow",
    "deep learning",
    "micro-expression detection",
    "facial expression analysis"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Computing optical flow is a fundamental problem in computer vision. However, deep learning-based optical flow techniques do not perform well for non-rigid movements such as those found in faces, primarily due to lack of the training data representing the fine facial motion. We hypothesize that learning optical flow on face motion data will improve the quality of predicted flow on faces. The aim of this work is threefold: (1) exploring self-supervised techniques to generate optical flow ground truth for face images; (2) computing baseline results on the effects of using face data to train Convolutional Neural Networks (CNN) for predicting optical flow; and (3) using the learned optical flow in micro-expression recognition to demonstrate its effectiveness. We generate optical flow ground truth using facial key-points in the BP4D-Spontaneous dataset. The generated optical flow is used to train the FlowNetS architecture to test its performance on the generated dataset. The performance of FlowNetS trained on face images surpassed that of other optical flow CNN architectures, demonstrating its usefulness. Our optical flow features are further compared with other methods using the STSTNet micro-expression classifier, and the results indicate that the optical flow obtained using this work has promising applications in facial expression analysis.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "F ACIAL expressions are generated due to non-rigid move- ment in faces. From the perspective of automatic facial expression recognition (FER), the motion information has been well explored for the task of both micro and macro expression analysis. Optical flow is used to estimate the motion of sets of pixels across images. This information on faces can help characterize both micro and macro expressions, which are useful in expression recognition. A major motivation for using the motion information for FER is based on what is known as the facial feedback hypothesis  [1] , which, in summary, suggests that facial actions can both encode current emotions as well as induce or amplify emotions. An example of this would be that the furrowing of the brow could increase anger  [1] . It has also been demonstrated that some facial muscle movements are linked to the compound facial expression of negation  [2] . Also, the relation between motion information extracted from the eyes and mouth has been studied in its association with the facial expressions of psychopaths  [3] . Facial and head movements are also important in social contexts, such as head motion used to indicate particular social cues, or the famous twitching of the lip corners that may suggest lying  [4] .\n\nFaces have a peculiar structure. Hence, in this work, we focus on learning optical flow specialized for faces which we Manuscript received October X, 2020; revised XXXXX XX, 202X.\n\nwill attempt to constrain the algorithm to learn only lifelike expressions on faces. In doing so, we explore how well a deep network can perform in this task. We demonstrate that the proposed architecture will work well for faces compared to traditional optical flow algorithms. The results can serve as a precursor to designing motion-based features for supervised and unsupervised learning of facial expressions by drawing on existing research linking facial motion information to facial expression and emotion recognition. Several works document the use of facial optical flow features for facial expression recognition and action unit recognition tasks.\n\nWe use the BP4D-Spontaneous dataset  [5]  consisting of videos of 41 participants with different facial expressions to generate the ground-truth optical flow between every pair of consecutive frames in the dataset. The groundtruth optical flow is obtained using facial key-points and image warping with affine transformations. We then use this facial optical flow ground truth to train a convolutional autoencoder based architecture, FlowNetS  [6]  (specialized for optical flow estimation), to learn optical flow specialized for facial motions, meaning that the motion learned should exhibit local coherency as would be expected on faces. We also modify the architecture by adding a cyclic loss to help the network reconstruct the latter image in a given image pair using the optical flow predicted by the network. We argue that adding this reconstruction in the learning framework improves the predicted optical flow by guiding it using the structure of the image pairs. We perform an ablation study with different loss functions, and compare the performance of our network and other baseline optical flow CNNs. Finally, we test the usefulness of our network by using the learned optical flow predictions for microexpression detection using optical flow and the Shallow Triple Stream Three-dimensional CNN (STSTNet)  [7] .\n\nIntroduction of a \"noisy\" optical flow dataset for faces, making use of the peculiar structure of faces.\n\n• Learning a network for optical flow estimation, specialized for face movements. We then complement the structure with a cyclic loss. Our modified architecture outperforms several other networks used for optical flow estimation.\n\n• Exhibiting the usefulness of our trained network by applying it for micro-expression detection.\n\nThe remainder of the paper is organized as follows. Section 2 contains related literature in the relevant topics.Section 3 describes the details of the automatic dataset generation used in this paper, and details of the networks trained on the generated dataset are explained in Section 4. The results of the ablation study and micro-expression recognition are presented in Section 5. And finally, we present the concluding remarks and recommendations for improvement and future work in Section 6.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "First, we discuss works related to optical flow estimation using classical and deep learning techniques, along with some of the common challenges. We follow this up by a survey of optical flow methods as applied to faces in particular, and how optical flow is used in tasks such as micro-expression detection.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Optical Flow Estimation",
      "text": "Optical flow in images is used to estimate the motion of sets of pixels across images. Classical methods, such as in  [8]  and  [9] , use the intensity derivatives and energy methods to estimate the optical flow. However, there may be several challenges.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Optical Flow Challenges",
      "text": "Different methods have been proposed to compensate for typical problems that may arise in optical flow estimation. Chen et al.  [10]  developed a method using quaternions to deal with the possible inconsistency among the RGB channel intensities, Portz et al.  [11]  proposed an algorithm to compute optical flow in blurred environments, and Porikli et al.  [12]  modified the optical flow algorithm to deal particularly with low frame-rate applications. Finally, Zappella et al.  [13]  presented a comprehensive literature review and evaluation of motion tracking algorithms, including their advantages and applications.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Deep Learning For Optical Flow Estimation",
      "text": "With the surge and success of deep learning applications this decade, there has also been a rise in using convolutional neural networks to learn optical flow, beginning with the groundbreaking work of Fischer et al.  [6]  with their FlowNet CNN architecture. Building on the success of FlowNet, FlowNet2.0  [14]  was introduced a few years later to improve performance by stacking networks, scheduling the training data, and learning small-motion datasets. FlowNet3.0  [15]  was also proposed afterwards for scene flow estimation. For our experiments, we use the FlowNetS architecture adapted from  [6]  to train on our dataset. By demonstrating how we can adapt FlowNetS to perform well on datasets consisting of only faces, we can later improve even further by training the data with the more advanced architectures.\n\nWhile FlowNet is one of the most popular optical flow deep learning architectures, several other architectures have since been proposed to deal with certain challenges. Janai et al.  [16]  dealt with the problem of unsupervised learning of optical flow in occluded settings by considering a triplet instead of a pair of frames and a photometric loss to handle the occlusions. Ren et al.  [17]  and Meister et al.  [18]  also built on these concepts for unsupervised learning of optical flow. Sun et al.  [19]  used the pyramid-structure CNN architecture PWC-Net for optical flow prediction, which we use in this work to test on the face optical flow dataset as a benchmark implementation and compare with our performance. Another optical flow CNN we use for comparison in this work is LiteFlowNet by Hui et al.  [20] , which surpassed Flownet2.0's performance on the KITTI and Sintel final datasets.\n\nIn their pioneering work, Zhu et al.  [21]  developed the cycleGAN, which is a type of generative adversarial network (GAN), that implements a cyclic loss function which is used as a metric to evaluate the network's prediction as compared with one of the inputs. This loss function is also used in the context of optical flow learning. Yu et al.  [22]  used this cyclic loss, which they dub \"warp loss\", to train a Flownet architecture for optical flow learning. The cyclic loss is also adapted by Lai et al.  [23]  in the context of optical flow using a GAN. Both of the latter architectures used a differentiable spatial transformer layer with learnable parameters, adapted from Jaderberg et al.  [24] .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Optical Flow And Facial Expression Analysis",
      "text": "In this section, we discuss various convolutional neural network architectures for optical flow estimation as well as deep learning that uses optical flow for facial expression analysis.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Face Optical Flow Estimation",
      "text": "More relevant to our topic are motion tracking methods, which are used in facial expression analysis. One important work in learning optical flow for facial expressions by Snape et al. is Face Flow  [25] , which minimizes a proposed energy to learn the flow field for a sequence of frames consisting of facial expressions. Another relevant work is optical flow dataset generation done by Le et al.  [26]  who are also concerned with producing optical ground-truth data for general video sequences. According to them, little prior work exists on how the performance of CNNs is influenced by optical flow datasets, and their main focus is that of non-rigid motion. Our work can be considered to be a contribution to the study of optical flow's effects on CNNs, with the difference being that we focus on facial datasets instead. We attempt to learn optical flow from the face movements themselves. On a side note, a review of different optical flow techniques specialized for facial expression recognition can be found in  [27] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Face Optical Flow And Deep Learning",
      "text": "We mention a few implementations of deep learning in facial expression analysis using optical flow. Koujan et al.  [28]  recently proposed DeepFaceFlow, in which they construct a 3D optical flow dataset for faces from a large collection of videos and compare the performance of their U-net trained on their dataset with other CNN architectures for both 2D and 3D optical flow estimation. One key difference between our work and theirs is that we incorporate a cyclic loss to test how well the flow field reconstructs the second image in the pair. Additionally, the training data we generate is based upon the BP4D-Spontaneous dataset, which is specifically tuned to exhibit various emotions and thus more specialized for expression recognition tasks. We also test our network's performance on microexpression detection.\n\nSeveral works also use optical flow for action unit recognition. Ma et al.  [29]  proposed Action Unit (AU) R-CNN to improve AU recognition by using expert prior knowledge, which can be in the form of optical flow, to guide an R-CNN in locating the action region. Yang and Yin  [30]  learn both optical flow and facial action units for static images in one combined CNN architecture. Other works that use optical flow for action unit recognition can be found in  [31] ,  [32] , and  [33] .\n\nLiong et al.  [34]  exploit the optical flow in a video sequence between the frame with the highest intensity, called apex, and each of the rest of the frames, using the optical flow as input to a deep network for micro-expression detection. They also use apex and onset frames in  [7]  to compute optical flow along with an added feature, the optical strain, as input to STSTNet, which we adapt in this work to test for micro-expression recognition. Verburg and Menkovski  [35]  use optical flow histograms as feature inputs to a recurrent neural network for the recognition of micro-expressions. Li et al.  [36]  use a CNN to locate facial keypoints and FlowNet2.0 to compute optical flow, and the flow features are then used with a support vector machine for micro-expression detection.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Dataset Preparation",
      "text": "Our method is inspired by the progress in self supervised learning techniques for action recognition  [37]  and eye gaze prediction  [38] . We use the BP4D-Spontaneous dataset  [5] , which consists of 41 subjects with 8 video sequences each, containing videos of elicited emotions. The motivation for using BP4D-Spontaneous is its inclusion of both head and facial motion. While local non-rigid facial motion estimation is the primary focus of this work, it is also useful to capture this local facial flow in the presence of head motion. Since BP4D-Spontaneous is concerned with spontaneously elicited expression sequences and 3D encoding, more general motion is available. Other datasets, such as the Extended CK+  [39] , are more specialized for AU or micro-expression detection, and thereby are less suited for a more general motion framework. Moreover, this allows us to test how optical flow performs on micro-expression detection when trained on a dataset not specialized for micro-expression detection.\n\nFig.  1  shows the overall pipeline for a pair of frames and how they can be used for dataset generation and CNN training. 1 We introduce the notation that we'll use throughout this section to generate the optical flow ground truth from the BP4D-Spontaneous dataset  [5] . For a given sequence S in the dataset, we denote the frames contained in S by F = {f k } N f k=0 , where f k ∈ R H×W ×3 are the ordered frames. Our aim in this section is to compute a set of optical flow fields, U separately for every ordered set of frames, F , where\n\ncontains the optical flow fields u k : R H×W → R H×W ×2 for each frame except the final one in that sequence. The u k are vector-valued functions defined on the image grid.\n\nLandmarks P on the face in S are tracked for each frame using the open source OpenFace pipeline  [40] , which uses the Convolutional Experts Constrained Local Model  [41]  to obtain 68 landmarks per face. We note that, for this step, other facial keypoint detection techniques can also be used. We denote the facial landmarks tracked on the face in each frame f k of S by P k = (p 0 . . . p 68 ) T k ∈ R 68×2 (where, T denotes the transpose operation).\n\nNext, we completely partition the first face f 0 into a triangular mesh using Delaunay triangulation on P 0 using Scipy's Delaunay triangulation package. Theoretical background related to Delaunay triangulation can be found in  [42] . This mesh divides the face in f 0 into N t disjoint triangles T 0 = {t l } Nt l=0 , where each\n\nis the matrix with rows composed of vertices of triangle l. After triangulating f 0 , we use similar triangulation on the remaining frames in the sequence, yielding the set of triangulations {T k } N f k=0 on S. We use the triangulation T k-1 to capture the local motion on every triangle in the face partition from frame f k-1 to frame f k . Given the triangle t l k-1 , we infer an affine map A l k-1 ∈ R 3×3 that sends its vertices to the vertices in t l k . Specifying three mappings are sufficient to uniquely define an affine map  [43] . We can define t * = t, 1 3×1\n\nT ∈ R 3×3\n\nto be the matrix of homogeneous coordinates of each vertex.\n\nThen, for all triangles in f k-1 and\n\nThis gives the required matrix for the affine map. Note that if the triangle is degenerate, then t * k-1 will be singular. Once the correspondence between the two triangles across frames is known, A l k-1 also maps the interior of t l k-1 to the interior of t l k , since barycentric coordinates are invariant under affine maps  [43] .\n\nWe use the barycentric coordinates to compute the interiors of all the triangles in T 0 , and then learn each affine map A l 0 as described above to map all the triangle interiors from T 0 to T 1 . To compute the interior of the triangle using barycentric coordinates, an efficient algorithm from  [44]  can be used to test if an arbitrary point v is contained in a given triangle by taking the convex combination with the triangle vertices\n\n1. Our code implementing the algorithm in this section will be made publicly available. By taking the dot product of equation (  2 ) with v 1 -v 0 and v 2 -v 0 , a 2 × 2 system of equations can be solved for λ = λ 1 , λ 2 T , and\n\nV λ = b, where,\n\n, and\n\nand if the λ i ∈ [0, 1], then v lies in the closure of the triangle of interest, i.e. v is a convex combination of the columns of t. We test all points in this way using a rectangular discrete grid surrounding the triangle. Repeating this for all f k in the sequence is overall computationally expensive, so we only do it for triangles in the first frame of that video. By invariance of barycentric coordinates under the affine maps A l k-1 , this also determines the barycentric coordinates for all subsequent frames f k , k > 0.\n\nAfter determining the affine maps and mapping the triangles and their interior pixels v k-1 to v k , we compute the per-pixel optical flow vector ũk-1 by\n\nHowever, it is not guaranteed that the domain of any given affine map, A, will lie on an discrete grid Z + ×Z + . When the domain is not a discrete grid, the optical flow fields ũk are defined on points that are not necessarily pixel coordinates, which affects the frames after f 0 . The optical flow field ũk-1 from equation (  4 ) is defined on a discrete grid, but the pixels that are mapped from f 0 to f 1 will subsequently be mapped from f 1 to f 2 , in which case it is not guaranteed that they also lie on a discrete grid. To recover the optical flow field u k-1 on a discrete grid in the target image, we use bicubic spline interpolation over the irregular grid using ũk-1 . We only do this to define the optical flow field at each frame, but continue to learn the affine maps on the irregular grids, since we wish to preserve the same barycentric coordinates obtained in f 0 for all frames. The flow fields are stored in .flo formats for later use in the experiments.\n\nTogether with the resampling stage, this procedure gives us the ground-truth vector field for all pixels of frame f k-1 . The details can be summarized as follows:\n\n1) Starting from frame f 0 , determine the interiors of all triangles t l 0 , using barycentric coordinates.\n\n2) Learn the affine maps sending all t l 0 to t l 1 and transform the entire face to obtain the first optical flow field u 0 .\n\n3) For all frames starting from f 1 , again infer the affine maps sending all t l 1 to t l 2 and apply the transformation on all the pixels which have already been mapped from frame f 0 . This removes the need to expensively compute the triangle interiors for frame f 1 while still finding the optical flow field ũ1 . 4) From ũ1 , resample the flow field over a discrete grid to yield the ground-truth flow u 1 . 5) Repeat steps 3-4 for the remaining frames in a sequence {f k } N f k=2 , for all sequences and subjects. The total number of images in the generated dataset is 325720, and these were partitioned into 228171, 65130, and 32419 for training, validation, and test data respectively. The dataset generation was completed in a total of about five days using multicore CPU parallel processing with four parallel processes running at a time.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Baseline Networks",
      "text": "In this section, we describe the CNN architecture used to train the optical flow, followed by the training and ablation study details. These details include the different hyperparameters used in the different experimental setups, such as the choices of loss functions, the loss weights, and training/testing data split.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Cnn Architecture: Flownets",
      "text": "To test the effects of having a large, \"noisy\" ground-truth optical flow dataset specialized for faces on CNNs, the FlowNetS  [6]  architecture was used. FlowNetS is one of the pioneering CNNs on optical flow learning. While more sophisticated optical flow architectures have been developed, our purpose is to demonstrate the improvement of training a CNN with face data compared with some other datasets, e.g., the FlyingChairs dataset, as a proof of concept. Should we discover an improvement, in future, we can expand it to tackle other problems (e.g. robustness to occlusion).\n\nFlowNetS is a convolutional autoencoder architecture which accepts a pair of images as input and outputs the per-pixel optical flow from the first image to the second. It consists of a sequence of downsampling convolutional layers in the encoder followed by upsampling layers in the decoder, in addition to intermediate operations and concatenations. Another variant of FlowNet, which is FlowNetCorr, is characterized by a cross-correlation layer which fuses two input streams together, contrasted to FlowNetS which combines them with a simple concatenation. The difference in performance reported in  [6]  is not too significant, and including the cross-correlation layer during training resulted in the inconvenience of much longer training times.\n\nThe output resolutions of each of the flow predictions in our network are slightly different than the original FlowNetS. Specifically, the ratio of our flow prediction heights to theirs is 24:17, and our widths to theirs is 4:5. The reader is referred to  [6]  for specific details on the network architecture.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Cyclic Loss For Image Reconstruction",
      "text": "For some of the experiments described in the next section, a cyclic loss is implemented to minimize the difference between the output predicted using the flow prediction and the second input image. This resulted in an additional warping layer to the network that acts on the flow prediction with highest resolution. The warping layer uses the predicted per-pixel flow field vectors to warp the first input image, and the result is recovered using bilinear interpolation. We note that structures inherent only to the second input cannot be reproduced in the warped output, since the warping function only changes pixel locations from the first input, and does not contain any learnable parameters. Fig.  2  shows two examples of this phenomena from FlyingChairs and our face dataset, showing the original input image pair (X 1 , X 2 ), the image X 2 deformed using the flow field, and visualization of the flow field Y .\n\nThe dominant motion in the FlyingChairs image pair from the flow field is rightward motion of the left armchair. The location of the armchair in the warped image is correct, but the reconstruction of the warped portion is missing. This is also present in the smaller desk chair, making a copy of itself at the warped location during reconstruction. Due to these large differences in the images, adding a warping layer while training on the FlyingChairs dataset is likely to worsen the network's performance. However, this effect is much more subtle in our face dataset due to the higher frame rate of the sequences, which causes lower magnitude motion between every two consectuive frames. For the face example in Fig.  2 , the deformed image X 2 is perceptually similar to the actual X 2 , particularly in the upwards motion of the eyes and the slight rightward motion caused by the furrowing of the brow. Since the time difference between two frames is very small in the face dataset, it is very unlikely for new structures to be introduced in X 2 . A notable exception to this is the opening (closing) of the mouth due to revealing (hiding) teeth, which cannot be reproduced by pixel rearrangement alone. Another exception would be the squinting or widening of the eyes for the same reason, since the eyelid or eyeball would not be present in the first image. Although the artifacts caused by the warping produced a flawed image in the FlyingChairs dataset, we hypothesize and show that it still helps guide the directions of the predicted flow when training on faces since the undesirable effects are considerably less due to the lower amount of new structure.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Training And Ablation Studies Details",
      "text": "The training details of the aforementioned architecture are described in this section 2 . Ablation studies are performed on FlowNetS by training the network with different loss functions and their corresponding weights.\n\nWe denote by (X i , X i+1 ) the pair of successive input frames, where\n\nfor k ∈ {1, . . . , 5} in the decoder. ( Ŷi ) 1 is the largest flow prediction, as in the original FlowNetS output. Note that, in the following, we drop the added subscript and refer to it as Ŷi .\n\nSince we assume that the background is stationary, much of the ground-truth flow field outside of the boundaries defined by the key-points are zero vectors. To make the training more practical, we zoom on the box with vertices defined by the key-points with maximal and minimal coordinates plus some offset in the x and y directions. The cropped images and flow fields are then resized using bilinear interpolation. To preserve the units of the flow vectors as pixels, they are scaled accordingly in the horizontal and vertical directions.\n\nNext, we describe the different experimental setups used to train the networks.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experimental Setup 1: No Cyclic Loss",
      "text": "In this experiment, the architecture is used without the additional warping layer. The network was trained for 30, 40, and 400 epochs on the face, FlyingChairs, and Sintel datasets respectively, with 15000, 21592, and 870 training and 1000, 640, and 271 validation input image pairs each. The batch size used for training is 16 input pairs. The loss function is the average endpoint error (EPE), L i 1 (Y i , Ŷi ), defined for one output by,\n\nHere, the w k are loss weights for each intermediate flow prediction loss, given by w k = 2 -k . H k , W k are the sizes of the intermediate predictions and the y ij , ( ŷij ) k are the flow vectors for the jth pixel of ground-truth and kth predicted flow fields Y i and ( Ŷi ) k . The flow fields Y i are resized to compute the error for each intermediate prediction. The optimizer used is Adam, with β 1 = 0.9 and β 2 = 0.999 as in  [6] . This performs better than alternative optimizers. We initialized the learning rate α at 1e-4 for faces and 5e-5 for FlyingChairs and scheduled similar to  [6] .\n\nFor preliminary experimentation, we trained the network once on the face data for a 15k and 1k training 2. Code for the details described in this section will be made publicly available.\n\nFig.  2 : Effect of using flow field Y to warp X 1 to X 2 is demonstrated for images with large (top) and small (bottom) motion. and testing split, and once separately on the entire Fly-ingChairs dataset. We then tested each trained network on both datasets each, as well as the Sintel dataset  [45] . After the preliminary experiment, we trained the same network again from scratch on faces only for a 228k, 65k, and 32.5k train/val/test split, exactly as in the next two experiments, to make them comparable. The latter setup is referred to as Experiment 1, from here onwards.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Experimental Setup 2: With Cyclic Loss",
      "text": "When the warping layer  [23]  at the end of the network is included, it is necessary to define a cyclic loss function for the warped output Xi+1 and the second input X i+1 . We expect to see an improvement in the flow prediction due to the cyclic loss. For this experiment, we define the additional cyclic loss function L i 2 (X i+1 , Xi+1 ) for one output pair i as:\n\nwhich uses the Huber loss function x H1  [46] , a variant of the L 1 loss that is everywhere differentiable, since it is quadratic for small values of x. The x i+1,j,k , xi+1,j,k are values of the jth pixel of X i+1 , Xi+1 at color channel k. We also note that Xi+1 is a function of the first image of the input pair, X i , and Ŷi , which is the flow prediction with largest resolution.\n\nThe total loss function J(X, X, Y, Ŷ ) for all training pairs is then\n\n) with the L i 1 , L i 2 defined in equations (  5 ) and (  7 ) and λ 1 , λ 2 to be specified, averaged over all M training examples. In this experiment, we train the network on both faces and Fly-ingChairs datasets using two different sets of loss weights λ 1 , λ 2 . One network has more emphasis on reconstruction, with λ 2 = 0.6, λ 1 = 0.4. We refer to this as Case I. The other network has higher weight assigned to the EPE with λ 1 = 0.75, λ 2 = 0.25. We refer to this as Case II. Note that the w i in equation (  5 ) should sum to λ 1 . For both cases, we trained the network on faces for 15 epochs and 228160 training pairs. Learning rates were kept constant for these experiments throughout training, since scheduling them as previously done lead to very large gradients halfway through training. In Case I, the learning rates were 2.5e-6 and 1.25e-6 for faces and FlyingChairs respectively, and in Case II, they were both set to 2.5e-6. We then tested the trained networks on the test set of 32416 image pairs.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experimental Setup 3: With Cyclic Loss, Smoothness Constraint, And Average Angular Error",
      "text": "In this experiment, we added an additional loss function L i 3 ( Ŷi ). In Case I of this experiment, a smoothness constraint was imposed on the flow prediction by minimizing the flow gradients, defined as:\n\nwhere (û ij , vij ) are the components of the predicted flow vector ŷij at every pixel j.\n\nAnother common metric to quantify performance of optical flow algorithms  [47]  is the average angular error (AAE). The average angular error between two flow vectors is the average of the angle difference between every ground-truth and predicted flow vectors in the homogeneous coordinates, which are y * j = (u j , v j , 1) T and ŷ * j = (û j , vj , 1) T respectively. In Case II, the loss function L i 3 (Y i , Ŷi ) is defined as:\n\nThe total loss function is then a weighted sum of the loss functions,\n\nWe trained the network on only the faces dataset for 14 epochs and 228160 training pairs, with λ 1 = 0.3, λ 2 = 0.5, λ 3 = 0.2, and learning rate 2.5e-6. We initialized the weights from the results of Experiment 2 (Case I), to see if there is any improvement in flow prediction after adding L 3 . In the next sections, we will use abbreviations for experiment and case numbers in the discussions for brevity. For example, Experiment 2, Case II is referred to as Exp. 2II, and no Roman numerals mean we refer to both cases of that particular experiment.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Micro-Expression Detection",
      "text": "In this section, we describe how optical flow features are used for a micro-expression recognition task to demonstrate the efficacy of the optical flow generated using our method. The use of optical flow in micro-expression recognition has proven useful in several prior works, as described in section 2.2.2.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Cnn And Optical Flow Features",
      "text": "To train the optical flow features, we use the threedimensional lightweight CNN proposed by Liong et al.  [7] , named the \"Shallow Triple Stream Three-dimensional CNN\", or STSTNet, which shows improved results compared with their previous work and other deep networks for micro-expression recognition. Their algorithm is evaluated on the CASME II  [48] , SAMM  [49] , and SMIC  [50]  datasets, composed of videos containing micro-expressions that represent either negative, positive, or surprise emotions (threeclass classification). For each video sequence, they compute the optical flow between the onset the apex frames, and use this optical flow as input to train STSTNet classifier. The apex frames in SAMM are provided with the dataset, which is not the case with SMIC. The apex frames were also used for micro-expression recognition on SMIC dataset by Quang et al.  [51] . We make use of their labeling for the SMIC dataset. We crop the faces based on keypoints obtained using the OpenFace 2.0 toolbox  [40]  for SAMM. For SMIC, since OpenFace failed to detect the keypoints for some images, we instead use the dlib facial landmark detector  [52] , which is based on an ensemble of regression trees  [53] , and define the crop border at 15 pixels away from the maximum and minimum x and y image coordinates.\n\nWe follow their recommended approach to train the STSTNet. The optical flow from the onset to the apex frame is used to compute the optical flow strain (U ) for a given flow field, U = (u(x, y), v(x, y)). The strain is defined  [7]  by the symmetric matrix known as the strain tensor The strain of a planar displacement field (u, v) is wellknown in solid mechanics, consisting of normal strains xx , yy , which are the diagonal elements, and shear strains xy = yx , which are the off-diagonal elements  [7] . The strain values represent the type of local deformation that occurs at each point in the flow field. The optical strain norm || (u(x, y), v(x, y))|| s is then defined  [7]  as:\n\nThe optical strain feature V ∈ R H×W ×3 is an RGB image and, for a given pixel coordinate (x h , y h ), takes the value (u(x h , y h ), v(x h , y h ), || (u, v)|| s ) ∈ R 3 . Fig.  3  shows an example of the optical flow feature computed for an image pair, using the optical flow obtained from each network. In this example, the salient motion is an upwards curling of the lips plus a subtle leftwards shift in glance.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Micro-Expression Detection Experimental Setup",
      "text": "The authors of STSTNet  [7]  evaluate their model using leave-one-subject-out cross-validation (LOSOCV), and we do the same to train the micro-expression recognition networks. The SAMM and SMIC datasets were both used for the task. All optical flow networks described in Section 5.2 are used separately to train STSTNet. For every optical flow network, we train the network three times: once on SAMM, once on SMIC, and once on the combined dataset consisting of both. We use the publicly available code provided by the authors  [7] , and thus replicate the exact same network architecture, with a learning rate of 5e-5 and maximum epochs set to 500. We note that the RGB input images, described in Section 4.4, are resized to a resolution of 28 × 28 × 3. We also compute the TVL1 optical flow on SAMM and SMIC, as done in  [7] , to compare its performance with the optical flow features obtained from other networks.\n\nTo deal with the class imbalance, we use macro-averaged recall, precision, and F 1 -scores to evaluate the performance of every trained network. Additionally, the metrics specified by Yap et al.  [54]  are the micro-averaged F 1 -score and Unweighted Average Recall (UAR). The definition of UAR is equivalent to macro-averaged recall R M . UAR is also popular for imbalanced multiclass problems, such as in  [7] ,  [55] , and  [56] . The performance measures are defined as  [57] :\n\nThe subscripts M, µ denotes macro and micro-averaging, respectively, and for the F 1 -score, x ∈ {M, µ}. tp j i , f p j i , and f n j i denote the true positive, false positive, and false negative of class i, sample j, for a total of n (= 3) classes and m samples. Note that when the prediction for a given class is a true positive, this also counts as a true negative for each of the other two classes. The macro-averaged metrics tend to remove the bias caused by the imbalance degree, Fig.  3 : An example of the computed optical flow features used as inputs to train STSTNet for each network variant. Source: subject 03, SMIC  [50] . since it does not \"favor\" the classes with higher number of examples, as opposed to micro-averaging  [57] .\n\nMoreover, since LOSOCV is used, this yields one metric per subject. We will combine the metrics to a single scalar, which we will refer to as the aggregated metric. This has been done in other works such as  [58]  (following a different experimental setup), and the aggregation is done by taking the mean of the metric across all subjects for every iteration.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Results And Discussion",
      "text": "To evaluate the flow network, we first report preliminary results based on Experiment 1 and compare the flow predictions with networks trained on only the FlyingChairs or Sintel datasets. As indicated in section 4.3.1, the performance measure for this experiment is the average EPE. We then show the results of the ablation study for the experiments trained on the full dataset, and compare the networks using the average EPE and AAE. Next, we evaluate a number of other popular optical flow methods on the test set. These include FlowNet2.0  [14] , FlowNet3.0  [15] , LiteFlowNet  [20] , and PWC-Net  [19] , and the classic Gunnar-Farneback optical flow  [59] . Finally, the results of the micro-expression detection task using all networks are presented, which shows the usefulness of our method for a practical application.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Results For Ablation Studies",
      "text": "We first describe the initial results of Exp. 1, which comprise of the network trained and tested on faces, FlyingChairs, and Sintel dataset. We report the performance in terms of average EPE. Table  1  shows the first experiment's overall statistics for each network when tested on 3000 samples from our face dataset. We also compare the performance when trained and tested on other datasets. It is worth noting that the subjects that appear in the training set do not appear in the validation or test sets of our face data.\n\nThe error values in Table  1  are in pixels, averaged over each of the test sets. Row 1 shows the results when the network was trained on faces and tested on all three datasets. Similarly, rows 2 and 3 are trained on FlyingChairs and Sintel and tested on all three. From Table  1 , we observe that the network trained on our BP4D-derived face dataset performs best when tested on faces. This is likely due to  the nature of the dataset the network was trained on. The flow fields on our face dataset consist of small, non-rigid motions, especially when the head motion is lacking, whilst the motion fields in the FlyingChairs dataset have larger magnitude and is more rigid. The Sintel dataset is also different in nature than the face dataset, but has smaller overall motion, and thus it is likely that the network trained on FlyingChairs overestimates the motion on the face dataset. Note that the results in Table  1  are comparable to state of the art methods on the Sintel dataset, as can be seen in  [45] .\n\nAfter adding the cyclic loss and training for more data and epochs, we expect to observe a difference in performance compared with Exp. 1. Here, we train the setup for Exp 1 again, using the same data split as the other experiments, for comparison purposes. Now we show the results of the networks trained with cyclic loss as described in sections 4.3.2 and 4.3.3.\n\nTable  2  summarizes the statistics computed based on the results of Exp. 2 and Exp. 3. The statistics related to the flow fields (AAE and average EPE) are computed for all 32.5k image pairs in the test set. As outlined at the end of section 4.3, Exp. 2I and Exp. 2II represent, respectively, the higher and lower reconstruction weight experiments, while Exp. 3I and Exp. 3II represent the experiment with smoothness constraint and the experiment with average angular error.\n\nThere are several observations to be made from these results. Adding the cyclic loss but with lower reconstruction weights (Exp. 2II) improves the flow prediction compared to using only the EPE loss (Exp. 1), since both EPE and AAE decrease significantly. When there is higher weight on reconstruction loss (Exp. 2I), the network alters the predicted flow to improve the warped output's semblance to X 2 . However, the higher focus on reconstruction worsens the performance of the AAE and EPE. One reason could be that the noisy ground truth does not necessarily reconstruct X 2 from X 1 very well, i.e. the reconstruction capability of a predicted flow field is adversarial to the ground-truth flow EPE and AAE. Exp. 3 with the smoothness and AAE losses yields worse outcomes than the other two in terms of predicted flow, particularly compared to Exp. 2I. Note that Exp. 3 weights are initialized from the latter to test any change in performance. This could be due to the decreased weight in the EPE loss, which suggests that the EPE is a stronger indicator of flow performance than the AAE. The EPE encodes the direction in addition to the magnitude information. Another explanation would be that training data with angular error as a loss metric does not generalize well to test data, unlike the EPE. Exp. 3I exhibits the worst performance in both EPE and AAE amongst our network variants. This is likely due to the imposed smoothness constraints, which impose flow field values in the otherwise null regions outside the face boundary.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Comparison With Other Networks",
      "text": "We now compare the results with other notable optical flow implementations. Table  3  shows the flow statistics computed for the network variants described earlier.  3  In all cases, the networks trained on our automatic face dataset perform better in both metrics than PWC-Net  [19]  and LiteFlowNet  [20] , which are some of the popular CNN-based optical flow methods. PWC-Net demonstrates a notably high average EPE, but a more competitive AAE. This is likely due to an overestimation of the flow prediction magnitudes. FlowNet2.0 and FlowNet3.0-CSS, which are both state of the art improvements on FlowNetS, are both outperformed by all of our network variants with the exception of average EPE in Exp. 3I. The Gunnar-Farneback optical flow performs better than all methods in both average EPE and AAE, but is outperformed by Exp. 1 and Exp. 2I.\n\nTo investigate the type of flow produced by each of the networks on the facial images, Fig.  4  shows a sample subset of image pairs in the test set with their respective groundtruth and flow predictions from each network. The EPE and AAE for each prediction are also labeled. The saturation intensity in a given image is only representative of the intensity of that region relative to the other pixels of the same image. The same intensity in two images may have substantially different optical flow vector values. This is common practice in optical flow visualization, since it places emphasis on which motion is more salient for a given image. In images with small motion, as is the case in many frames in the BP4D dataset, using to-scale visualization would not convey important local motion information. We note that the following remarks for the remainder of this section are qualitative in nature and are based on a very small subset, but nevertheless yield some insight to accompany the statistics from Tables  2  and 3 .\n\nWe first observe the differences in flow predictions among the networks trained on our dataset. From these five, Exp. 1 shows the sparsest predictions, which is expected as it only minimizing the EPE from the sparse groundtruth flow. After introducing the cyclic loss in the other four experiments, denser optical features start to appear, caused by the added emphasis on image reconstruction. For example, this denser optical flow allowed the network to better predict the eye motion in rows 3 and 4 of Figure  4 . Thus, the EPE loss taught the network to predict well the regional directions and magnitudes, and the cyclic loss helped it further localize the motion in these regions.\n\nThere is higher motion variance across the face in Exp. 2I compared to Exp. 2II. Although both were trained with cyclic loss, there is higher emphasis on the loss in Exp. 2I than in Exp. 2II, which more clearly shows the effect of the cyclic loss, since no other losses were introduced in Exp. 2. The outputs of the networks with cyclic loss also show coarser representations compared to the outputs of FlowNet2.0 and FlowNet3.0-CSS, such as in rows 9 and 10 in Fig.  4 . When the smoothness constraints were imposed in Exp. 3I, the face segmentation learned by the network was affected, since the large values of the flow derivatives at the face boundaries enlargened the gradients in the smoothness loss function.\n\nBy both visual perception of these examples and the average EPE and AAE values from Table  3 , the Gunnar-Farneback optical flow shows similarity in both direction and magnitude. Since the method is unbiased by any training data, this similarity provides a degree of validation to the ground-truth optical flow. However, it still underestimates optical flow in some instances, such as the nearzero regions in rows 6, 8, and 13. The Gunnar-Farneback flow also segments the face, since the background has zero motion. This is in contrast to the outputs of the other four networks (FlowNet2.0, FlowNet3.0-CSS, LiteFlowNet, and PWC-Net). The outputs of FlowNet2.0, FlowNet3.0-CSS, and LiteFlowNet all tend to estimate background flow. The flow trend of their outputs from the examples of Fig.  4  can be matched with the outputs of the other networks, although some examples -especially those with global motion, such as in rows 4, 5, and 13 -show appreciable differences. PWC-Net demonstrates a more consistent flow pattern similar to our networks and Gunnar-Farneback. However, the EPE values in both the Fig.  4  examples and From the overall results of the experiments, our networks trained on the automatically generated face dataset are better-suited at predicting the optical flow on faces compared to other networks.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Results Of Micro-Expression Detection",
      "text": "We now report on the results of the aforementioned experiments for micro-expression detection. The micro-and macro-averaged metrics are shown for every network on each of the SAMM, SMIC, and combined datasets in Tables  4  and 5 . In these tables, the aggregation of the metrics across the subjects from the LOSOCV is the mean of the metric across the subjects. The results in Table  4  indicate that the performance of STSTNet trained on optical flow features from different networks also significantly depends on the dataset it is trained on. For each evaluation measure, the top three performing networks are indicated in bold. We note that the F 1 -scores are typically lower than the precision and TABLE 4: Results of the aggregated performance metrics for micro-expression recognition on the SAMM and SMIC datasets separately, using TVL1 optical flow as done in  [7] , our network with different variants, and the other optical flow CNN architectures.  recall since these are aggregated metrics, i.e. the F 1 -score averaged over all F 1 -scores in the LOSOCV, and is not the harmonic mean of the aggregated precision and recall. For macro-averaged precision P M , as well as the macro and micro-averaged F 1 -scores on the SAMM dataset, the top scores are achieved from Experiments 1, 2II, and 3II, followed closely by FlowNet3.0-CSS and TVL1. The higher F 1 -scores are more influenced by the precision values and less so by the recalls. Exp. 1 is the highest for these three metrics, while TVL1 scored highest in R M , and FlowNet3.0-CSS in geometric mean. This is one testimony to the complexity of capturing the overall classification performance with a single scalar metric for multi-class problems, since the proposed metrics can each emphasize different features of the classifier performances. Exp. 3II is the only variant which is consistently among the top 3 for all metrics, at either second or third.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Samm Smic Optical Flow Methods",
      "text": "The SMIC results allow for a more consistent inference on the performance of the classifiers. Across all metrics, the top three networks were FlowNet2.0, FlowNet3.0-CSS, and LiteFlowNet. Both the precision and recall, and consequently the F 1 -scores, follow more similar trends, in contrast with the SAMM and combined training protocols. For precision, recall, and F 1 -scores, the lowest three scores are interchanged amongst Exp. 1, PWC-Net, and Gunnar-Farneback. In fact, Gunnar-Farneback and PWC-Net are consistently the least performing across all three training protocols.\n\nBy comparing the results across the three training protocols, it is difficult to conclude that optical flow features computed from one specific method will be optimal for training the STSTNet classifier for micro-expression detection. Although the networks trained using our method performed well when trained and tested on SAMM, they were somewhat outperformed in the other two protocols. However, even in these cases, they were not as consistently behind when compared to Gunnar-Farneback and PWC-Net. This could be due to the sparse nature of the learned optical flow representations from our generated dataset. It is also plausible that the accuracy of the flow magnitude prediction is not a consistent predictor of its performance on micro-expression detection. We hypothesize that our method will overcome the performance difference in some of the results if we use a denser keypoint tracker during the optical flow training phase to generate the BP4D groundtruth. This will likely improve the network's ability to more consistently capture fine local facial motion which may otherwise have been missed in the current work. Furthermore, as previously discussed, we have used FlowNetS to train the face data to benchmark its efficacy compared to other networks, and thus using a better-designed CNN along with the denser keypoint ground-truth will likely further improve the performance.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Conclusion And Future Work",
      "text": "In this paper, we explore the possibility of using a facial expression dataset to learn optical flow representations based on a self-supervised technique. Motion information on faces has been shown to be useful in facial expression analysis in multi-modal techniques.\n\nThe dataset is generated by using the image sequences from the BP4D-Spontaneous dataset to compute the optical flow ground-truth. The OpenFace 2.0 toolbox, which uses a constrained local model, is used to locate the facial landmarks on every image. Delaunay triangulation is then used on the resulting set of points to form the face mesh and allow the computation of the optical flow for every pair of images using triangle-to-triangle affine maps to develop an automatic facial optical flow dataset. The generated dataset, with a total of nearly 324k image pairs, is used as a noisy ground-truth for optical flow to train the FlowNetS convolutional autoencoder architecture with 228k pairs in the training partition.\n\nIt was observed that training the FlowNetS architecture for optical flow on this automatically generated noisy ground-truth data improved the network's ability to predict optical flow on face data in particular. The learned representations also helped the network give good accuracy on the FlyingChairs and Sintel datasets. This demonstrates that the facial movements are nicely encoded in our data which enables the network to learn subtle movements that are useful on the challenging Sintel dataset as well. A cyclic loss was also added for optimization to help the network use the predicted flow to reconstruct the second image, and the flow results from different experimental setups are compared. It was seen that the flow predictions are best when there is less emphasis on reconstruction, due to denser representations learned with reconstruction that are not present in the ground-truth flow fields. Compared with other optical flow methods (Gunnar-Farneback, FlowNet2.0, FlowNet3.0-CSS, LiteFlowNet, and PWC-Net), it was shown that the networks trained on the generated dataset predict better flow representations, as quantified by the flow error metrics. This implies that a network trained on good face optical flow ground-truth have the propensity to outperform networks trained on other datasets.\n\nTo investigate the performance of the different optical flow network variants in an FER application, the optical flow features were used to train STSTNet for microexpression detection. The experimental results using different performance metrics were mixed, e.g. FlowNet3.0-CSS outperformed the other methods in a good proportion of the cases. However, our method also demonstrated promising results in some cases, and note that further improvements and extension to this baseline work can help improve its application to FER.\n\nFor further investigation and improvement, future work related to this work can include the following: 1) Use a denser tracker such as Zface  [60]  to track a higher number of key-points for a finer triangulation and denser optical flow ground-truth in our automatic data generation algorithm. 2) Use a more complex CNN architecture to train the denser optical flow ground-truth. 3) Train the optical flow network on faces with some head rotation, such as pan and tilt, to learn optical flow for non-frontal faces. 4) Tackle challenges in optical flow learning, such as in environments with occlusion and illumination, to increase the robustness of facial optical flow.\n\nIn addition to these improvements for optical flow learning, the empirical analysis can be extended to evaluate the performance of the face-trained optical flow CNN in other problems in facial expression analysis, such as action unit recognition.",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: shows the overall pipeline for a pair of frames",
      "page": 3
    },
    {
      "caption": "Figure 1: Overall pipeline for data generation and network training: Two examples of the afﬁne maps are shown for some",
      "page": 4
    },
    {
      "caption": "Figure 2: , the deformed image X′",
      "page": 5
    },
    {
      "caption": "Figure 2: Effect of using ﬂow ﬁeld Y to warp X1 to X′",
      "page": 6
    },
    {
      "caption": "Figure 3: An example of the computed optical ﬂow features used as inputs to train STSTNet for each network variant. Source:",
      "page": 8
    },
    {
      "caption": "Figure 4: shows a sample subset",
      "page": 9
    },
    {
      "caption": "Figure 4: Thus, the EPE loss taught the network to predict well",
      "page": 9
    },
    {
      "caption": "Figure 4: When the smoothness constraints were imposed in",
      "page": 9
    },
    {
      "caption": "Figure 4: can be matched with the outputs of the other networks,",
      "page": 9
    },
    {
      "caption": "Figure 4: examples and",
      "page": 9
    },
    {
      "caption": "Figure 4: Color-coded optical ﬂow predictions for a small subset of the test set for the networks trained in each of the",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table 1: shows the ﬁrst experiment’s overall",
      "page": 8
    },
    {
      "caption": "Table 1: are in pixels, averaged",
      "page": 8
    },
    {
      "caption": "Table 1: , we observe",
      "page": 8
    },
    {
      "caption": "Table 1: The average EPE for each network described in",
      "page": 8
    },
    {
      "caption": "Table 2: Flow performance for the ablation studies",
      "page": 8
    },
    {
      "caption": "Table 1: are comparable to state of",
      "page": 8
    },
    {
      "caption": "Table 2: summarizes the statistics computed based on the",
      "page": 8
    },
    {
      "caption": "Table 3: Comparing various optical ﬂow methods.",
      "page": 9
    },
    {
      "caption": "Table 3: shows the ﬂow statistics computed",
      "page": 9
    },
    {
      "caption": "Table 3: , the Gunnar-",
      "page": 9
    },
    {
      "caption": "Table 3: suggest that the network, perhaps, overestimates the",
      "page": 10
    },
    {
      "caption": "Table 4: indicate that the",
      "page": 10
    },
    {
      "caption": "Table 4: Results of the aggregated performance metrics for micro-expression recognition on the SAMM and SMIC",
      "page": 11
    },
    {
      "caption": "Table 5: Results of the aggregated performance metrics for",
      "page": 11
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "How the experience of emotion is modulated by facial feedback",
      "authors": [
        "K Ohlén",
        "U Dimberg"
      ],
      "year": "2018",
      "venue": "Journal of Nonverbal Behavior"
    },
    {
      "citation_id": "2",
      "title": "The not face: A grammaticalization of facial expressions of emotion",
      "authors": [
        "C Benitez-Quiroz",
        "R Wilbur",
        "A Martinez"
      ],
      "year": "2016",
      "venue": "Cognition"
    },
    {
      "citation_id": "3",
      "title": "Psychopathic traits are associated with reduced attention to the eyes of emotional faces among adult male non-offenders",
      "authors": [
        "S Gillespie",
        "P Rotshtein",
        "L Wells",
        "A Beech",
        "I Mitchell"
      ],
      "year": "2015",
      "venue": "Frontiers in Human Neuroscience",
      "doi": "10.3389/fnhum.2015.00552"
    },
    {
      "citation_id": "4",
      "title": "Psychological and Neural Perspectives on Human Face Recognition",
      "authors": [
        "A O'toole"
      ],
      "year": "2005",
      "venue": "Psychological and Neural Perspectives on Human Face Recognition"
    },
    {
      "citation_id": "5",
      "title": "Bp4d-spontaneous: A high-resolution spontaneous 3d dynamic facial expression database",
      "authors": [
        "X Zhang",
        "L Yin",
        "J Cohn",
        "S Canavan",
        "M Reale",
        "A Horowitz",
        "P Liu",
        "J Girard"
      ],
      "year": "2014",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "6",
      "title": "Flownet: Learning optical flow with convolutional networks",
      "authors": [
        "A Dosovitskiy",
        "P Fischer",
        "E Ilg",
        "P Häusser",
        "C Hazırbas",
        "V Golkov",
        "P Smagt",
        "D Cremers",
        "T Brox"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "7",
      "title": "Shallow triple stream three-dimensional cnn (ststnet) for micro-expression recognition",
      "authors": [
        "S Liong",
        "Y Gan",
        "J See",
        "H Khor",
        "Y Huang"
      ],
      "year": "2019",
      "venue": "2019 14th IEEE International Conference on Automatic Face Gesture Recognition"
    },
    {
      "citation_id": "8",
      "title": "An iterative image registration technique with an application to stereo vision",
      "authors": [
        "B Lucas",
        "T Kanade"
      ],
      "year": "1981",
      "venue": "Proceedings of the 7th International Joint Conference on Artifical Intelligence"
    },
    {
      "citation_id": "9",
      "title": "High performance implementation of the horn and schunck optical flow algorithm on fpga",
      "authors": [
        "M Balazadeh Bahar",
        "G Karimian"
      ],
      "year": "2012",
      "venue": "20th Iranian Conference on Electrical Engineering"
    },
    {
      "citation_id": "10",
      "title": "Quaternion based optical flow estimation for robust object tracking",
      "authors": [
        "E Chen",
        "Y Xu",
        "X Yang",
        "W Zhang"
      ],
      "year": "2013",
      "venue": "Digital Signal Processing"
    },
    {
      "citation_id": "11",
      "title": "Optical flow in the presence of spatially-varying motion blur",
      "authors": [
        "T Portz",
        "L Zhang",
        "H Jiang"
      ],
      "year": "2012",
      "venue": "2012 IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "12",
      "title": "Object tracking in low-frame-rate video",
      "authors": [
        "F Porikli",
        "O Tuzel"
      ],
      "year": "2005",
      "venue": "SPIE Conference on Image and Video Communications and Processing"
    },
    {
      "citation_id": "13",
      "title": "Motion segmentation: A review",
      "authors": [
        "L Zappella",
        "X Llad",
        "J Salvi"
      ],
      "year": "2008",
      "venue": "Proceedings of the 2008 Conference on Artificial Intelligence Research and Development: Proceedings of the 11th International Conference of the Catalan Association for Artificial Intelligence"
    },
    {
      "citation_id": "14",
      "title": "Flownet 2.0: Evolution of optical flow estimation with deep networks",
      "authors": [
        "E Ilg",
        "N Mayer",
        "T Saikia",
        "M Keuper",
        "A Dosovitskiy",
        "T Brox"
      ],
      "year": "2017",
      "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "15",
      "title": "Occlusions, motion and depth boundaries with a generic network for disparity, optical flow or scene flow estimation",
      "authors": [
        "E Ilg",
        "T Saikia",
        "M Keuper",
        "T Brox"
      ],
      "year": "2018",
      "venue": "European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "16",
      "title": "Unsupervised learning of multi-frame optical flow with occlusions",
      "authors": [
        "J Janai",
        "F Üney",
        "A Ranjan",
        "M Black",
        "A Geiger"
      ],
      "year": "2018",
      "venue": "Proceedings of the 15th European Conference on Computer Vision"
    },
    {
      "citation_id": "17",
      "title": "Unsupervised deep learning for optical flow estimation",
      "authors": [
        "Z Ren",
        "J Yan",
        "B Ni",
        "B Liu",
        "X Yang",
        "H Zha"
      ],
      "year": "2017",
      "venue": "Proceedings of the 31st AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "18",
      "title": "Unflow: Unsupervised learning of optical flow with a bidirectional census loss",
      "authors": [
        "S Meister",
        "J Hur",
        "S Roth"
      ],
      "year": "2017",
      "venue": "CoRR"
    },
    {
      "citation_id": "19",
      "title": "Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume",
      "authors": [
        "D Sun",
        "X Yang",
        "M Liu",
        "J Kautz"
      ],
      "year": "2017",
      "venue": "CoRR"
    },
    {
      "citation_id": "20",
      "title": "Liteflownet: A lightweight convolutional neural network for optical flow estimation",
      "authors": [
        "T.-W Hui",
        "X Tang",
        "C Loy"
      ],
      "year": "2018",
      "venue": "Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "21",
      "title": "Unpaired image-toimage translation using cycle-consistent adversarial networks",
      "authors": [
        "J Zhu",
        "T Park",
        "P Isola",
        "A Efros"
      ],
      "year": "2017",
      "venue": "CoRR"
    },
    {
      "citation_id": "22",
      "title": "Back to basics: Unsupervised learning of optical flow via brightness constancy and motion smoothness",
      "authors": [
        "J Yu",
        "A Harley",
        "K Derpanis"
      ],
      "year": "2016",
      "venue": "Computer Vision -ECCV 2016 Workshops"
    },
    {
      "citation_id": "23",
      "title": "Semi-supervised learning for optical flow with generative adversarial networks",
      "authors": [
        "W.-S Lai",
        "J.-B Huang",
        "M.-H Yang"
      ],
      "year": "2017",
      "venue": "Proceedings of the 31st International Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "24",
      "title": "Spatial transformer networks",
      "authors": [
        "M Jaderberg",
        "K Simonyan",
        "A Zisserman",
        "K Kavukcuoglu"
      ],
      "year": "2015",
      "venue": "CoRR"
    },
    {
      "citation_id": "25",
      "title": "Face flow",
      "authors": [
        "P Snape",
        "A Roussos",
        "Y Panagakis",
        "S Zafeiriou"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "26",
      "title": "Unsupervised generation of optical flow datasets from videos in the wild",
      "authors": [
        "H Le",
        "T Nimbhorkar",
        "T Mensink",
        "A Baslamisli",
        "S Karaoglu",
        "T Gevers"
      ],
      "year": "2018",
      "venue": "CoRR"
    },
    {
      "citation_id": "27",
      "title": "Optical flow techniques for facial expression analysis: Performance evaluation and improvements",
      "authors": [
        "B Allaert",
        "I Ward",
        "I Bilasco",
        "C Djeraba",
        "M Bennamoun"
      ],
      "year": "1904",
      "venue": "CoRR"
    },
    {
      "citation_id": "28",
      "title": "Deepfaceflow: In-the-wild dense 3d facial motion estimation",
      "authors": [
        "M Koujan",
        "A Roussos",
        "S Zafeiriou"
      ],
      "year": "2020",
      "venue": "Deepfaceflow: In-the-wild dense 3d facial motion estimation",
      "arxiv": "arXiv:2005.07298"
    },
    {
      "citation_id": "29",
      "title": "Au r-cnn: Encoding expert prior knowledge into r-cnn for action unit detection",
      "authors": [
        "C Ma",
        "L Chen",
        "J Yong"
      ],
      "year": "2019",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "30",
      "title": "Learning temporal information from a single image for au detection",
      "authors": [
        "H Yang",
        "L Yin"
      ],
      "year": "2019",
      "venue": "2019 14th IEEE International Conference on Automatic Face Gesture Recognition"
    },
    {
      "citation_id": "31",
      "title": "Multi-view dynamic facial action unit detection",
      "authors": [
        "A Romero",
        "J Le Ón",
        "P Arbeláez"
      ],
      "year": "2018",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "32",
      "title": "Learning spatial and temporal cues for multi-label facial action unit detection",
      "authors": [
        "W Chu",
        "F De La Torre",
        "J Cohn"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Automatic Face Gesture Recognition"
    },
    {
      "citation_id": "33",
      "title": "Spontaneous expression recognition using universal attribute model",
      "authors": [
        "N Perveen",
        "D Roy",
        "C Mohan"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "34",
      "title": "Off-apexnet on micro-expression recognition system",
      "authors": [
        "Y Gan",
        "S.-T Liong",
        "W.-C Yau",
        "Y.-C Huang",
        "L.-K Tan"
      ],
      "year": "2019",
      "venue": "Signal Processing: Image Communication"
    },
    {
      "citation_id": "35",
      "title": "Micro-expression detection in long videos using optical flow and recurrent neural networks",
      "authors": [
        "M Verburg",
        "V Menkovski"
      ],
      "year": "2019",
      "venue": "2019 14th IEEE International Conference on Automatic Face Gesture Recognition"
    },
    {
      "citation_id": "36",
      "title": "Micro-expression analysis by fusing deep convolutional neural network and optical flow",
      "authors": [
        "Q Li",
        "J Yu",
        "T Kurihara",
        "S Zhan"
      ],
      "year": "2018",
      "venue": "2018 5th International Conference on Control, Decision and Information Technologies"
    },
    {
      "citation_id": "37",
      "title": "Unsupervised learning of visual representations using videos",
      "authors": [
        "X Wang",
        "A Gupta"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 IEEE International Conference on Computer Vision",
      "doi": "10.1109/ICCV.2015.320"
    },
    {
      "citation_id": "38",
      "title": "Unsupervised learning of eye gaze representation from the web",
      "authors": [
        "N Dubey",
        "S Ghosh",
        "A Dhall"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Joint Conference on Neural Networks"
    },
    {
      "citation_id": "39",
      "title": "The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression",
      "authors": [
        "P Lucey",
        "J Cohn",
        "T Kanade",
        "J Saragih",
        "Z Ambadar",
        "I Matthews"
      ],
      "year": "2010",
      "venue": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition -Workshops"
    },
    {
      "citation_id": "40",
      "title": "Constrained local neural fields for robust facial landmark detection in the wild",
      "authors": [
        "T Baltrusaitis",
        "P Robinson",
        "L Morency"
      ],
      "year": "2013",
      "venue": "Proceedings of the 2013 IEEE International Conference on Computer Vision Workshops"
    },
    {
      "citation_id": "41",
      "title": "Deep constrained local models for facial landmark detection",
      "authors": [
        "A Zadeh",
        "T Baltrusaitis",
        "L Morency"
      ],
      "year": "2016",
      "venue": "CoRR"
    },
    {
      "citation_id": "42",
      "title": "Revent ós, Affine Maps, Euclidean Motions and Quadrics",
      "year": "2011",
      "venue": "Revent ós, Affine Maps, Euclidean Motions and Quadrics"
    },
    {
      "citation_id": "43",
      "title": "Geometric Methods and Applications: For Computer Science and Engineering",
      "authors": [
        "J Gallier"
      ],
      "year": "2013",
      "venue": "Geometric Methods and Applications: For Computer Science and Engineering"
    },
    {
      "citation_id": "44",
      "title": "Real-Time Collision Detection",
      "authors": [
        "C Ericson"
      ],
      "year": "2004",
      "venue": "Real-Time Collision Detection"
    },
    {
      "citation_id": "45",
      "title": "A naturalistic open source movie for optical flow evaluation",
      "authors": [
        "D Butler",
        "J Wulff",
        "G Stanley",
        "M Black"
      ],
      "year": "2012",
      "venue": "Proceedings of the 12th European Conference on Computer Vision"
    },
    {
      "citation_id": "46",
      "title": "Robust estimation of a location parameter",
      "authors": [
        "P Huber"
      ],
      "year": "1964",
      "venue": "Annals of Mathematical Statistics",
      "doi": "10.1214/aoms/1177703732"
    },
    {
      "citation_id": "47",
      "title": "A database and evaluation methodology for optical flow",
      "authors": [
        "S Baker",
        "D Scharstein",
        "J Lewis",
        "S Roth",
        "M Black",
        "R Szeliski"
      ],
      "year": "2011",
      "venue": "International Journal of Computer Vision",
      "doi": "10.1007/s11263-010-0390-2"
    },
    {
      "citation_id": "48",
      "title": "Casme ii: An improved spontaneous micro-expression database and the baseline evaluation",
      "authors": [
        "W.-J Yan",
        "X Li",
        "S.-J Wang",
        "G Zhao",
        "Y.-J Liu",
        "Y.-H Chen",
        "X Fu"
      ],
      "year": "2014",
      "venue": "PLoS ONE"
    },
    {
      "citation_id": "49",
      "title": "Samm: A spontaneous micro-facial movement dataset",
      "authors": [
        "A Davison",
        "C Lansley",
        "N Costen",
        "K Tan",
        "M Yap"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "50",
      "title": "A spontaneous micro-expression database: Inducement, collection and baseline",
      "authors": [
        "X Li",
        "T Pfister",
        "X Huang",
        "G Zhao",
        "M Pietikäinen"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "51",
      "title": "Automatic apex frame spotting in micro-expression database",
      "authors": [
        "S Liong",
        "J See",
        "K Wong",
        "A Le Ngo",
        "Y Oh",
        "R Phan"
      ],
      "year": "2015",
      "venue": "2015 3rd IAPR Asian Conference on Pattern Recognition (ACPR)"
    },
    {
      "citation_id": "52",
      "title": "Dlib-ml: A machine learning toolkit",
      "authors": [
        "D King"
      ],
      "year": "2009",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "53",
      "title": "One millisecond face alignment with an ensemble of regression trees",
      "authors": [
        "V Kazemi",
        "J Sullivan"
      ],
      "year": "2014",
      "venue": "2014 IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "54",
      "title": "Facial micro-expressions grand challenge 2018 summary",
      "authors": [
        "M Yap",
        "J See",
        "X Hong",
        "S Wang"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE International Conference on Automatic Face Gesture Recognition (FG 2018)"
    },
    {
      "citation_id": "55",
      "title": "We are not amused -but how do you know? user states in a multi-modal dialogue system",
      "authors": [
        "A Batliner",
        "V Zeißler",
        "C Frank",
        "J Adelhardt",
        "R Shi",
        "E Öth"
      ],
      "year": "2003",
      "venue": "We are not amused -but how do you know? user states in a multi-modal dialogue system"
    },
    {
      "citation_id": "56",
      "title": "Recognising realistic emotions and affect in speech: State of the art and lessons learnt from the first challenge",
      "authors": [
        "B Schuller",
        "A Batliner",
        "S Steidl",
        "D Seppi"
      ],
      "venue": "Speech Communication"
    },
    {
      "citation_id": "57",
      "title": "sensing Emotion and Affect -Facing Realism in Speech Processing",
      "year": "2011",
      "venue": "sensing Emotion and Affect -Facing Realism in Speech Processing"
    },
    {
      "citation_id": "58",
      "title": "A systematic analysis of performance measures for classification tasks",
      "authors": [
        "M Sokolova",
        "G Lapalme"
      ],
      "year": "2009",
      "venue": "Information Processing & Management"
    },
    {
      "citation_id": "59",
      "title": "Deep neural networks for human activity recognition with wearable sensors: Leave-one-subject-out cross-validation for model selection",
      "authors": [
        "D Gholamiangonabadi",
        "N Kiselov",
        "K Grolinger"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "60",
      "title": "Two-frame motion estimation based on polynomial expansion",
      "authors": [
        "G Farnebäck"
      ],
      "year": "2003",
      "venue": "Proceedings of the 13th Scandinavian Conference on Image Analysis, ser. SCIA'03"
    },
    {
      "citation_id": "61",
      "title": "Dense 3d face alignment from 2d video for real-time use",
      "authors": [
        "L Jeni",
        "J Cohn",
        "T Kanade"
      ],
      "year": "2017",
      "venue": "Image and Vision Computing"
    }
  ]
}